{"version": 2, "width": 160, "height": 40, "timestamp": 1762942281, "env": {"SHELL": "/bin/bash", "TERM": "screen"}}
[0.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[0.002, "i", "asciinema rec --stdin /logs/agent.cast\r"]
[0.004, "o", "asciinema rec --stdin /logs/agent.cast\r\n"]
[0.006, "o", "\u001b[?2004l\r\n\u001b[0;31masciinema: /logs/agent.cast already exists, aborting\u001b[0m\r\n\u001b[0;31masciinema: use --overwrite option if you want to overwrite existing recording\u001b[0m\r\n\u001b[0;31masciinema: use --append option if you want to append to existing recording\u001b[0m\r\n"]
[5.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[5.002, "i", "clear\r"]
[5.004, "o", "clear\r\n"]
[5.006, "o", "\u001b[?2004l\r\n\u001b[H\u001b[J\r\n"]
[10.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[10.002, "i", "ls -la\r"]
[10.004, "o", "ls -la\r\n"]
[10.006, "o", "\u001b[?2004l\r\ntotal 176\r\ndrwxr-xr-x 1 root root  4096 Oct  1 12:34 \u001b[0m\u001b[01;34m.\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 15 17:25 \u001b[01;34m..\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34m.binder\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34m.circleci\u001b[0m\r\n-rw-r--r-- 1 root root  1392 Sep 13 19:09 .cirrus.star\r\n-rw-r--r-- 1 root root   999 Sep 13 19:09 .codecov.yml\r\n-rw-r--r-- 1 root root   150 Sep 13 19:09 .coveragerc\r\ndrwxr-xr-x 1 root root  4096 Sep 15 17:27 \u001b[01;34m.git\u001b[0m\r\n-rw-r--r-- 1 root root  1000 Sep 13 19:09 .git-blame-ignore-revs\r\ndrwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34m.github\u001b[0m\r\n-rw-r--r-- 1 root root  2004 Sep 13 19:09 .gitignore\r\n-rw-r--r-- 1 root root  7263 Sep 13 19:08 .mailmap\r\n-rw-r--r-- 1 root root   872 Sep 13 19:09 .pre-commit-config.yaml\r\n-rw-r--r-- 1 root root   645 Sep 13 19:09 CODE_OF_CONDUCT.md\r\n-rw-r--r-- 1 root root  2109 Sep 13 19:09 CONTRIBUTING.md\r\n-rw-r--r-- 1 root root  1532 Sep 13 19:09 COPYING\r\n-rw-r--r-- 1 root root   971 Sep 13 19:09 MANIFEST.in\r\n-rw-r--r-- 1 root root  1491 Sep 13 19:09 Makefile\r\n-rw-r--r-- 1 root root  7604 Sep 13 19:09 README.rst\r\n-rw-r--r-- 1 root root   692 Sep 13 19:09 SECURITY.md\r\ndrwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34masv_benchmarks\u001b[0m\r\n-rw-r--r-- 1 root root 11978 Sep 13 19:09 azure-pipelines.yml\r\ndrwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34mbenchmarks\u001b[0m\r\ndrwxr-xr-x 4 root root  4096 Sep 15 14:32 \u001b[01;34mbuild\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34mbuild_tools\u001b[0m\r\n-rw-r--r-- 1 root root   388 Sep 13 19:09 conftest.py\r\ndrwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34mdoc\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34mexamples\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34mmaint_tools\u001b[0m\r\n-rw-r--r-- 1 root root  3838 Sep 13 19:09 pyproject.toml\r\ndrwxr-xr-x 1 root root  4096 Sep 15 14:32 \u001b[01;34mscikit_learn.egg-info\u001b[0m\r\n-rw-r--r-- 1 root root  2113 Sep 13 19:09 setup.cfg\r\n-rwxr-xr-x 1 root root 22566 Sep 13 19:09 \u001b[01;32msetup.py\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 15 14:38 \u001b[01;34msklearn\u001b[0m\r\n"]
[15.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[15.002, "i", "find . -name 'asv*.json' -o -name 'asv_benchmarks.txt'\r"]
[15.004, "o", "find . -name 'asv*.json' -o -name 'asv_benchmarks.txt'\r\n"]
[15.006, "o", "\u001b[?2004l\r\n./asv_benchmarks/asv.conf.json\r\n"]
[20.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[20.002, "i", "ls -la asv_benchmarks/\r"]
[20.004, "o", "ls -la asv_benchmarks/\r\n"]
[20.006, "o", "\u001b[?2004l\r\ntotal 24\r\ndrwxr-xr-x 1 root root 4096 Sep 13 19:09 \u001b[0m\u001b[01;34m.\u001b[0m\r\ndrwxr-xr-x 1 root root 4096 Oct  1 12:34 \u001b[01;34m..\u001b[0m\r\n-rw-r--r-- 1 root root   66 Sep 13 19:08 .gitignore\r\n-rw-r--r-- 1 root root 6754 Sep 13 19:09 asv.conf.json\r\ndrwxr-xr-x 1 root root 4096 Sep 13 19:09 \u001b[01;34mbenchmarks\u001b[0m\r\n"]
[25.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[25.002, "i", "ls -la asv_benchmarks/benchmarks/\r"]
[25.004, "o", "ls -la asv_benchmarks/benchmarks/\r\n"]
[25.006, "o", "\u001b[?2004l\r\ntotal 76\r\ndrwxr-xr-x 1 root root 4096 Sep 13 19:09 \u001b[0m\u001b[01;34m.\u001b[0m\r\ndrwxr-xr-x 1 root root 4096 Sep 13 19:09 \u001b[01;34m..\u001b[0m\r\n-rw-r--r-- 1 root root   49 Sep 13 19:08 __init__.py\r\n-rw-r--r-- 1 root root 2925 Sep 13 19:08 cluster.py\r\n-rw-r--r-- 1 root root 7341 Sep 13 19:09 common.py\r\n-rw-r--r-- 1 root root 1544 Sep 13 19:09 config.json\r\n-rw-r--r-- 1 root root 5176 Sep 13 19:09 datasets.py\r\n-rw-r--r-- 1 root root 2406 Sep 13 19:08 decomposition.py\r\n-rw-r--r-- 1 root root 2995 Sep 13 19:08 ensemble.py\r\n-rw-r--r-- 1 root root 6648 Sep 13 19:09 linear_model.py\r\n-rw-r--r-- 1 root root  820 Sep 13 19:08 manifold.py\r\n-rw-r--r-- 1 root root 1363 Sep 13 19:08 metrics.py\r\n-rw-r--r-- 1 root root 2371 Sep 13 19:08 model_selection.py\r\n-rw-r--r-- 1 root root 1140 Sep 13 19:08 neighbors.py\r\n-rw-r--r-- 1 root root  762 Sep 13 19:08 svm.py\r\n-rw-r--r-- 1 root root 1310 Sep 13 19:08 utils.py\r\n"]
[30.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[30.002, "i", "cat asv_benchmarks/benchmarks/decomposition.py\r"]
[30.004, "o", "cat asv_benchmarks/benchmarks/decomposition.py\r\n"]
[30.006, "o", "\u001b[?2004l\r\nfrom sklearn.decomposition import PCA, DictionaryLearning, MiniBatchDictionaryLearning\r\n\r\nfrom .common import Benchmark, Estimator, Transformer\r\nfrom .datasets import _mnist_dataset, _olivetti_faces_dataset\r\nfrom .utils import make_dict_learning_scorers, make_pca_scorers\r\n\r\n\r\nclass PCABenchmark(Transformer, Estimator, Benchmark):\r\n    \"\"\"\r\n    Benchmarks for PCA.\r\n    \"\"\"\r\n\r\n    param_names = [\"svd_solver\"]\r\n    params = ([\"full\", \"arpack\", \"randomized\"],)\r\n\r\n    def setup_cache(self):\r\n        super().setup_cache()\r\n\r\n    def make_data(self, params):\r\n        return _mnist_dataset()\r\n\r\n    def make_estimator(self, params):\r\n        (svd_solver,) = params\r\n\r\n        estimator = PCA(n_components=32, svd_solver=svd_solver, random_state=0)\r\n\r\n        return estimator\r\n\r\n    def make_scorers(self):\r\n        make_pca_scorers(self)\r\n\r\n\r\nclass DictionaryLearningBenchmark(Transformer, Estimator, Benchmark):\r\n    \"\"\"\r\n    Benchmarks for DictionaryLearning.\r\n    \"\"\"\r\n\r\n    param_names = [\"fit_algorithm\", \"n_jobs\"]\r\n    params = ([\"lars\", \"cd\"], Benchmark.n_jobs_vals)\r\n\r\n    def setup_cache(self):\r\n        super().setup_cache()\r\n\r\n    def make_data(self, params):\r\n        return _olivetti_faces_dataset()\r\n\r\n    def make_estimator(self, params):\r\n        fit_algorithm, n_jobs = params\r\n\r\n        estimator = DictionaryLearning(\r\n            n_components=15,\r\n            fit_algorithm=fit_algorithm,\r\n            alpha=0.1,\r\n            transform_alpha=1,\r\n            max_iter=20,\r\n            tol=1e-16,\r\n            random_state=0,\r\n            n_jobs=n_jobs,\r\n        )\r\n\r\n        return estimator\r\n\r\n    def make_scorers(self):\r\n        make_dict_learning_scorers(self)\r\n\r\n\r\nclass MiniBatchDictionaryLearningBenchmark(Transformer, Estimator, Benchmark):\r\n    \"\"\"\r\n    Benchmarks for MiniBatchDictionaryLearning\r\n    \"\"\"\r\n\r\n    param_names = [\"fit_algorithm\", \"n_jobs\"]\r\n    params = ([\"lars\", \"cd\"], Benchmark.n_jobs_vals)\r\n\r\n    def setup_cache(self):\r\n        super().setup_cache()\r\n\r\n    def make_data(self, params):\r\n        return _olivetti_faces_dataset()\r\n\r\n    def make_estimator(self, params):\r\n        fit_algorithm, n_jobs = params\r\n\r\n        estimator = MiniBatchDictionaryLearning(\r\n            n_components=15,\r\n            fit_algorithm=fit_algorithm,\r\n            alpha=0.1,\r\n            batch_size=3,\r\n            random_state=0,\r\n            n_jobs=n_jobs,\r\n        )\r\n\r\n        return estimator\r\n\r\n    def make_scorers(self):\r\n        make_dict_learning_scorers(self)\r\n"]
[35.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[35.002, "i", "ls -la sklearn/decomposition/\r"]
[35.004, "o", "ls -la sklearn/decomposition/\r\n"]
[35.006, "o", "\u001b[?2004l\r\ntotal 2776\r\ndrwxr-xr-x 1 root root    4096 Sep 15 14:39 \u001b[0m\u001b[01;34m.\u001b[0m\r\ndrwxr-xr-x 1 root root    4096 Sep 15 14:38 \u001b[01;34m..\u001b[0m\r\n-rw-r--r-- 1 root root    1296 Sep 13 19:09 __init__.py\r\ndrwxr-xr-x 2 root root    4096 Sep 15 14:39 \u001b[01;34m__pycache__\u001b[0m\r\n-rw-r--r-- 1 root root    5729 Sep 13 19:09 _base.py\r\n-rw-r--r-- 1 root root  890875 Sep 15 14:31 _cdnmf_fast.c\r\n-rwxr-xr-x 1 root root  218800 Sep 15 14:34 \u001b[01;32m_cdnmf_fast.cpython-310-x86_64-linux-gnu.so\u001b[0m\r\n-rw-r--r-- 1 root root    1118 Sep 13 19:09 _cdnmf_fast.pyx\r\n-rw-r--r-- 1 root root   83338 Sep 13 19:09 _dict_learning.py\r\n-rw-r--r-- 1 root root   15301 Sep 13 19:09 _factor_analysis.py\r\n-rw-r--r-- 1 root root   26099 Sep 13 19:09 _fastica.py\r\n-rw-r--r-- 1 root root   15792 Sep 13 19:09 _incremental_pca.py\r\n-rw-r--r-- 1 root root   21794 Sep 13 19:09 _kernel_pca.py\r\n-rw-r--r-- 1 root root   33064 Sep 13 19:09 _lda.py\r\n-rw-r--r-- 1 root root   82499 Sep 13 19:09 _nmf.py\r\n-rw-r--r-- 1 root root 1069160 Sep 15 14:31 _online_lda_fast.c\r\n-rwxr-xr-x 1 root root  262144 Sep 15 14:34 \u001b[01;32m_online_lda_fast.cpython-310-x86_64-linux-gnu.so\u001b[0m\r\n-rw-r--r-- 1 root root    2855 Sep 13 19:09 _online_lda_fast.pyx\r\n-rw-r--r-- 1 root root   25848 Sep 13 19:09 _pca.py\r\n-rw-r--r-- 1 root root   18249 Sep 13 19:09 _sparse_pca.py\r\n-rw-r--r-- 1 root root   11489 Sep 13 19:09 _truncated_svd.py\r\ndrwxr-xr-x 1 root root    4096 Sep 13 19:09 \u001b[01;34mtests\u001b[0m\r\n"]
[40.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[40.002, "i", "head -50 sklearn/decomposition/_dict_learning.py\r"]
[40.004, "o", "head -50 sklearn/decomposition/_dict_learning.py\r\n"]
[40.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n"]
[45.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[45.002, "i", "grep -n \"class.*DictionaryLearning\" sklearn/decomposition/_dict_learning.py\r"]
[45.004, "o", "grep -n \"class.*DictionaryLearning\" sklearn/decomposition/_dict_learning.py\r\n"]
[45.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K1242\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \"\"\"Base \u001b[01;31m\u001b[Kclass from SparseCoder and DictionaryLearning\u001b[m\u001b[K algorithms.\"\"\"\r\n\u001b[32m\u001b[K1524\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kclass DictionaryLearning\u001b[m\u001b[K(_BaseSparseCoding, BaseEstimator):\r\n\u001b[32m\u001b[K1867\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kclass MiniBatchDictionaryLearning\u001b[m\u001b[K(_BaseSparseCoding, BaseEstimator):\r\n"]
[50.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[50.002, "i", "sed -n '1867,1920p' sklearn/decomposition/_dict_learning.py\r"]
[50.004, "o", "sed -n '1867,1920p' sklearn/decomposition/_dict_learning.py\r\n"]
[50.006, "o", "\u001b[?2004l\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n"]
[55.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[55.002, "i", "sed -n '1920,1980p' sklearn/decomposition/_dict_learning.py\r"]
[55.004, "o", "sed -n '1920,1980p' sklearn/decomposition/_dict_learning.py\r\n"]
[55.006, "o", "\u001b[?2004l\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n"]
[60.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[60.002, "i", "grep -A 30 \"def __init__\" sklearn/decomposition/_dict_learning.py | grep -A 30 \"MiniBatch\" -B 5\r"]
[60.004, "o", "grep -A 30 \"def __init__\" sklearn/decomposition/_dict_learning.py | grep -A 30 \"MiniBatch\" -B 5\r\n"]
[60.006, "o", "\u001b[?2004l\r\n"]
[65.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[65.002, "i", "sed -n '1990,2050p' sklearn/decomposition/_dict_learning.py\r"]
[65.004, "o", "sed -n '1990,2050p' sklearn/decomposition/_dict_learning.py\r\n"]
[65.006, "o", "\u001b[?2004l\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        Components extracted from the data.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations over the full dataset.\r\n\r\n    n_steps_ : int\r\n        Number of mini-batches processed.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[70.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[70.002, "i", "awk '/class MiniBatchDictionaryLearning/,/def __init__/' sklearn/decomposition/_dict_learning.py | tail -20\r"]
[70.004, "o", "awk '/class MiniBatchDictionaryLearning/,/def __init__/' sklearn/decomposition/_dict_learning.py | tail -20\r\n"]
[70.006, "o", "\u001b[?2004l\r\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n        \"shuffle\": [\"boolean\"],\r\n        \"dict_init\": [None, np.ndarray],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"callback\": [None, callable],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n    }\r\n\r\n    def __init__(\r\n"]
[75.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[75.002, "i", "sed -n '2090,2130p' sklearn/decomposition/_dict_learning.py\r"]
[75.004, "o", "sed -n '2090,2130p' sklearn/decomposition/_dict_learning.py\r\n"]
[75.006, "o", "\u001b[?2004l\r\n            Hidden(StrOptions({\"deprecated\"})),\r\n        ],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n        \"n_jobs\": [None, Integral],\r\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n        \"shuffle\": [\"boolean\"],\r\n        \"dict_init\": [None, np.ndarray],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"callback\": [None, callable],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        n_iter=\"deprecated\",\r\n        max_iter=None,\r\n        fit_algorithm=\"lars\",\r\n        n_jobs=None,\r\n        batch_size=256,\r\n        shuffle=True,\r\n        dict_init=None,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n"]
[80.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[80.002, "i", "grep -n \"class.*NMF\" sklearn/decomposition/_nmf.py\r"]
[80.004, "o", "grep -n \"class.*NMF\" sklearn/decomposition/_nmf.py\r\n"]
[80.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K1138\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kclass _BaseNMF\u001b[m\u001b[K(ClassNamePrefixFeaturesOutMixin, TransformerMixin, BaseEstimator, ABC):\r\n\u001b[32m\u001b[K1139\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \"\"\"Base \u001b[01;31m\u001b[Kclass for NMF and MiniBatchNMF\u001b[m\u001b[K.\"\"\"\r\n\u001b[32m\u001b[K1364\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kclass NMF(_BaseNMF\u001b[m\u001b[K):\r\n\u001b[32m\u001b[K1804\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kclass MiniBatchNMF(_BaseNMF\u001b[m\u001b[K):\r\n"]
[85.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[85.002, "i", "sed -n '1364,1420p' sklearn/decomposition/_nmf.py | grep -A 20 \"def __init__\"\r"]
[85.004, "o", "sed -n '1364,1420p' sklearn/decomposition/_nmf.py | grep -A 20 \"def __init__\"\r\n"]
[85.006, "o", "\u001b[?2004l\r\n"]
[90.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[90.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r"]
[90.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r\n"]
[90.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;31mNo `asv.conf` file found for valid extensions: ['.json', '.jsonc'].\u001b[0m\r\n"]
[95.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[95.002, "i", "cd asv_benchmarks && asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r"]
[95.004, "o", "cd asv_benchmarks && asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r\n"]
[95.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 6 total benchmarks (1 commits * 1 environments * 6 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.peakmem_fit\u001b[0m                                                                                  ok\r\n[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =======\u001b[0m\r\n             --               n_jobs\r\n             --------------- -------\r\n              fit_algorithm     1   \r\n             =============== =======\r\n                   lars       93.9M \r\n                    cd        93.2M \r\n             =============== =======\r\n\r\n[33.33%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.peakmem_transform\u001b[0m                                                                            ok\r\n[33.33%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =======\u001b[0m\r\n             --               n_jobs\r\n             --------------- -------\r\n              fit_algorithm     1   \r\n             =============== =======\r\n                   lars       84.7M \r\n                    cd        84.3M \r\n             =============== =======\r\n\r\n[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0m                                                                                     ok\r\n[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ============\u001b[0m\r\n             --                 n_jobs   \r\n             --------------- ------------\r\n              fit_algorithm       1      \r\n             =============== ============\r\n                   lars       6.80\u00b10.04s \r\n                    cd        1.55\u00b10.01s \r\n             =============== ============\r\n\r\n[66.67%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_transform\u001b[0m                                                                               ok\r\n[66.67%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ===========\u001b[0m\r\n             --                 n_jobs  \r\n             --------------- -----------\r\n              fit_algorithm       1     \r\n             =============== ===========\r\n                   lars        160\u00b12ms  \r\n                    cd        156\u00b10.5ms \r\n             =============== ===========\r\n\r\n[83.33%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.track_test_score\u001b[0m                                                                             ok\r\n[83.33%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ======================\u001b[0m\r\n             --                      n_jobs        \r\n             --------------- ----------------------\r\n              fit_algorithm            1           \r\n             =============== ======================\r\n                   lars       -0.07507291436195374 \r\n                    cd        -0.07509346306324005 \r\n             =============== ======================\r\n\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.track_train_score\u001b[0m                                                                            ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ======================\u001b[0m\r\n              --                      n_jobs        \r\n              --------------- ----------------------\r\n               fit_algorithm            1           \r\n              =============== ======================\r\n                    lars       -0.07244297116994858 \r\n                     cd        -0.07244537025690079 \r\n              =============== ======================\r\n\r\n"]
[100.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo/asv_benchmarks# "]
[100.002, "i", "asv profile --python=same --config=asv.conf.json decomposition.MiniBatchDictionaryLearningBenchmark.\r"]
[100.004, "o", "asv profile --python=same --config=asv.conf.json decomposition.MiniBatchDictionaryLearningBenchmark.\r\n"]
[100.006, "o", ".time_fit\r\n\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;31m'decomposition.MiniBatchDictionaryLearningBenchmark.time_fit' benchmark not found\u001b[0m\r\n"]
[105.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo/asv_benchmarks# "]
[105.002, "i", "asv list | grep MiniBatch\r"]
[105.004, "o", "asv list | grep MiniBatch\r\n"]
[105.006, "o", "\u001b[?2004l\r\nusage: asv [-h] [--verbose] [--config CONFIG] [--version]\r\n           {help,quickstart,machine,setup,run,continuous,find,rm,publish,preview,profile,update,show,compare,check,gh-pages}\r\n           ...\r\nasv: error: argument {help,quickstart,machine,setup,run,continuous,find,rm,publish,preview,profile,update,show,compare,check,gh-pages}: invalid choice: 'list' (choose from 'help', 'quickstart', 'machine', 'setup', 'run', 'continuous', 'find', 'rm', 'publish', 'preview', 'profile', 'update', 'show', 'compare', 'check', 'gh-pages')\r\n"]
[110.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo/asv_benchmarks# "]
[110.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit[lars-1]\r"]
[110.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit[lars-1]\r\n"]
[110.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/asv_3.10/bin/asv\", line 7, in <module>\r\n    sys.exit(main())\r\n  File \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/asv/main.py\", line 28, in main\r\n    result = args.func(args)\r\n  File \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/asv/commands/__init__.py\", line 48, in run_from_args\r\n    return cls.run_from_conf_args(conf, args)\r\n  File \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/asv/commands/profiling.py\", line 84, in run_from_conf_args\r\n    return cls.run(\r\n  File \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/asv/commands/profiling.py\", line 177, in run\r\n    benchmarks = Benchmarks.discover(conf, repo, environments,\r\n  File \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/asv/benchmarks.py\", line 132, in discover\r\n    return cls(conf, benchmarks, regex=regex)\r\n  File \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/asv/benchmarks.py\", line 59, in __init__\r\n    if not regex or any(re.search(reg, name) for reg in regex):\r\n  File \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/asv/benchmarks.py\", line 59, in <genexpr>\r\n    if not regex or any(re.search(reg, name) for reg in regex):\r\n  File \"/opt/conda/envs/asv_3.10/lib/python3.10/re.py\", line 200, in search\r\n    return _compile(pattern, flags).search(string)\r\n  File \"/opt/conda/envs/asv_3.10/lib/python3.10/re.py\", line 303, in _compile\r\n    p = sre_compile.compile(pattern, flags)\r\n  File \"/opt/conda/envs/asv_3.10/lib/python3.10/sre_compile.py\", line 788, in compile\r\n    p = sre_parse.parse(p, flags)\r\n  File \"/opt/conda/envs/asv_3.10/lib/python3.10/sre_parse.py\", line 955, in parse\r\n    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\r\n  File \"/opt/conda/envs/asv_3.10/lib/python3.10/sre_parse.py\", line 444, in _parse_sub\r\n    itemsappend(_parse(source, state, verbose, nested + 1,\r\n  File \"/opt/conda/envs/asv_3.10/lib/python3.10/sre_parse.py\", line 599, in _parse\r\n    raise source.error(msg, len(this) + 1 + len(that))\r\nre.error: bad character range s-1 at position 64\r\n"]
[115.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo/asv_benchmarks# "]
[115.002, "i", "cd ..\r"]
[115.004, "o", "cd ..\r\n"]
[115.006, "o", "\u001b[?2004l\r\n"]
[120.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[120.002, "i", "grep -n \"n_components=None\" sklearn/decomposition/_dict_learning.py\r"]
[120.004, "o", "grep -n \"n_components=None\" sklearn/decomposition/_dict_learning.py\r\n"]
[120.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K570\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    if n_components <= r:  # True even if \u001b[01;31m\u001b[Kn_components=None\u001b[m\u001b[K\r\n\u001b[32m\u001b[K1742\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kn_components=None\u001b[m\u001b[K,\r\n\u001b[32m\u001b[K2116\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kn_components=None\u001b[m\u001b[K,\r\n"]
[125.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[125.002, "i", "sed -i 's/n_components=None/n_components=\"warn\"/g' sklearn/decomposition/_dict_learning.py\r"]
[125.004, "o", "sed -i 's/n_components=None/n_components=\"warn\"/g' sklearn/decomposition/_dict_learning.py\r\n"]
[125.006, "o", "\u001b[?2004l\r\n"]
[130.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[130.002, "i", "grep -n \"n_components=\\\"warn\\\"\" sklearn/decomposition/_dict_learning.py\r"]
[130.004, "o", "grep -n \"n_components=\\\"warn\\\"\" sklearn/decomposition/_dict_learning.py\r\n"]
[130.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K570\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    if n_components <= r:  # True even if \u001b[01;31m\u001b[Kn_components=\"warn\"\u001b[m\u001b[K\r\n\u001b[32m\u001b[K1742\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kn_components=\"warn\"\u001b[m\u001b[K,\r\n\u001b[32m\u001b[K2116\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kn_components=\"warn\"\u001b[m\u001b[K,\r\n"]
[135.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[135.002, "i", "grep -B5 -A5 \"n_components.*:\" sklearn/decomposition/_dict_learning.py | head -20\r"]
[135.004, "o", "grep -B5 -A5 \"n_components.*:\" sklearn/decomposition/_dict_learning.py | head -20\r\n"]
[135.006, "o", "\u001b[?2004l\r\n    if B is None:\r\n        B = Y.T @ code\r\n\r\n    n_unused = 0\r\n\r\n    for k in range(n_components):\r\n        if A[k, k] > 1e-6:\r\n            # 1e-6 is arbitrary but consistent with the spams implementation\r\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n        else:\r\n            # kth atom is almost never used -> sample a new one from the data\r\n--\r\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n        # flip the initial code's sign to enforce deterministic output\r\n        code, dictionary = svd_flip(code, dictionary)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:  # True even if n_components=\"warn\"\r\n        code = code[:, :n_components]\r\n        dictionary = dictionary[:n_components, :]\r\n"]
[140.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[140.002, "i", "grep -B10 -A10 \"_parameter_constraints\" sklearn/decomposition/_dict_learning.py | grep -A15 \"n_components\"\r"]
[140.004, "o", "grep -B10 -A10 \"_parameter_constraints\" sklearn/decomposition/_dict_learning.py | grep -A15 \"n_components\"\r\n"]
[140.006, "o", "\u001b[?2004l\r\n        \"\u001b[01;31m\u001b[Kn_components\u001b[m\u001b[K\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n--\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n\u001b[36m\u001b[K--\u001b[m\u001b[K\r\n        \"\u001b[01;31m\u001b[Kn_components\u001b[m\u001b[K\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"n_iter\": [\r\n            Interval(Integral, 0, None, closed=\"left\"),\r\n            Hidden(StrOptions({\"deprecated\"})),\r\n        ],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n        \"n_jobs\": [None, Integral],\r\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n"]
[145.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[145.002, "i", "sed -i 's/\"n_components\": \\[Interval(Integral, 1, None, closed=\"left\"), None\\]/\"n_components\": [Interval(Integral, \r"]
[145.004, "o", "sed -i 's/\"n_components\": \\[Interval(Integral, 1, None, closed=\"left\"), None\\]/\"n_components\": [Interval(Integral, \r\n"]
[145.006, "o", " 1, None, closed=\"left\"), None, StrOptions({\"warn\", \"auto\"})]/g' sklearn/decomposition/_dict_learning.py\r\n\u001b[?2004l\r\n"]
[150.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[150.002, "i", "grep -A3 \"n_components.*StrOptions\" sklearn/decomposition/_dict_learning.py\r"]
[150.004, "o", "grep -A3 \"n_components.*StrOptions\" sklearn/decomposition/_dict_learning.py\r\n"]
[150.006, "o", "\u001b[?2004l\r\n        \"\u001b[01;31m\u001b[Kn_components\": [Interval(Integral, 1, None, closed=\"left\"), None, StrOptions\u001b[m\u001b[K({\"warn\", \"auto\"})],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n\u001b[36m\u001b[K--\u001b[m\u001b[K\r\n        \"\u001b[01;31m\u001b[Kn_components\": [Interval(Integral, 1, None, closed=\"left\"), None, StrOptions\u001b[m\u001b[K({\"warn\", \"auto\"})],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"n_iter\": [\r\n            Interval(Integral, 0, None, closed=\"left\"),\r\n"]
[155.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[155.002, "i", "grep -n \"def fit\" sklearn/decomposition/_dict_learning.py\r"]
[155.004, "o", "grep -n \"def fit\" sklearn/decomposition/_dict_learning.py\r\n"]
[155.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K1459\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdef fit\u001b[m\u001b[K(self, X, y=None):\r\n\u001b[32m\u001b[K1783\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdef fit\u001b[m\u001b[K(self, X, y=None):\r\n\u001b[32m\u001b[K1804\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdef fit\u001b[m\u001b[K_transform(self, X, y=None):\r\n\u001b[32m\u001b[K2326\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdef fit\u001b[m\u001b[K(self, X, y=None):\r\n"]
[160.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[160.002, "i", "sed -n '2326,2380p' sklearn/decomposition/_dict_learning.py\r"]
[160.004, "o", "sed -n '2326,2380p' sklearn/decomposition/_dict_learning.py\r\n"]
[160.006, "o", "\u001b[?2004l\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n        )\r\n\r\n        self._check_params(X)\r\n\r\n        if self.n_iter != \"deprecated\":\r\n            warnings.warn(\r\n                (\r\n                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n                    \"specified.\"\r\n                ),\r\n                FutureWarning,\r\n            )\r\n            n_iter = self.n_iter\r\n\r\n        self._random_state = check_random_state(self.random_state)\r\n\r\n        dictionary = self._initialize_dict(X, self._random_state)\r\n        old_dict = dictionary.copy()\r\n\r\n        if self.shuffle:\r\n            X_train = X.copy()\r\n            self._random_state.shuffle(X_train)\r\n        else:\r\n            X_train = X\r\n\r\n        n_samples, n_features = X_train.shape\r\n\r\n        if self.verbose:\r\n            print(\"[dict_learning]\")\r\n\r\n        # Inner stats\r\n        self._A = np.zeros(\r\n            (self._n_components, self._n_components), dtype=X_train.dtype\r\n        )\r\n"]
[165.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[165.002, "i", "grep -n \"def _check_params\" sklearn/decomposition/_dict_learning.py\r"]
[165.004, "o", "grep -n \"def _check_params\" sklearn/decomposition/_dict_learning.py\r\n"]
[165.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K2164\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdef _check_params\u001b[m\u001b[K(self, X):\r\n"]
[170.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[170.002, "i", "sed -n '2164,2220p' sklearn/decomposition/_dict_learning.py\r"]
[170.004, "o", "sed -n '2164,2220p' sklearn/decomposition/_dict_learning.py\r\n"]
[170.006, "o", "\u001b[?2004l\r\n    def _check_params(self, X):\r\n        # n_components\r\n        self._n_components = self.n_components\r\n        if self._n_components is None:\r\n            self._n_components = X.shape[1]\r\n\r\n        # fit_algorithm\r\n        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n\r\n        # batch_size\r\n        self._batch_size = min(self.batch_size, X.shape[0])\r\n\r\n    def _initialize_dict(self, X, random_state):\r\n        \"\"\"Initialization of the dictionary.\"\"\"\r\n        if self.dict_init is not None:\r\n            dictionary = self.dict_init\r\n        else:\r\n            # Init V with SVD of X\r\n            _, S, dictionary = randomized_svd(\r\n                X, self._n_components, random_state=random_state\r\n            )\r\n            dictionary = S[:, np.newaxis] * dictionary\r\n\r\n        if self._n_components <= len(dictionary):\r\n            dictionary = dictionary[: self._n_components, :]\r\n        else:\r\n            dictionary = np.concatenate(\r\n                (\r\n                    dictionary,\r\n                    np.zeros(\r\n                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n                        dtype=dictionary.dtype,\r\n                    ),\r\n                )\r\n            )\r\n\r\n        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n        dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n        return dictionary\r\n\r\n    def _update_inner_stats(self, X, code, batch_size, step):\r\n        \"\"\"Update the inner stats inplace.\"\"\"\r\n        if step < batch_size - 1:\r\n            theta = (step + 1) * batch_size\r\n        else:\r\n            theta = batch_size**2 + step + 1 - batch_size\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        self._A *= beta\r\n        self._A += code.T @ code / batch_size\r\n        self._B *= beta\r\n        self._B += X.T @ code / batch_size\r\n\r\n    def _minibatch_step(self, X, dictionary, random_state, step):\r\n        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n"]
[175.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[175.002, "i", "sed -i '2166,2168c\\        # n_components\\n        if self.n_components == \"warn\":\\n            warnings.warn(\\n   \r"]
[175.004, "o", "sed -i '2166,2168c\\        # n_components\\n        if self.n_components == \"warn\":\\n            warnings.warn(\\n   \r\n"]
[175.006, "o", "              \"The default value of n_components will change from None to \\'auto\\' in 1.4. \"\\n                \"Set the value of n_components to suppress this war\r\nrning.\",\\n                FutureWarning,\\n            )\\n            self._n_components = X.shape[1]\\n        elif self.n_components == \"auto\" or self.n_componen\r\nnts is None:\\n            self._n_components = X.shape[1]\\n        else:\\n            self._n_components = self.n_components' sklearn/decomposition/_dict_learnin\r\nng.py\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> ^C\u001b[?2004l\r\n\u001b[?2004h\u001b[?2004l\r\n\r\n"]
[180.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[180.002, "i", "sed -n '2164,2180p' sklearn/decomposition/_dict_learning.py\r"]
[180.004, "o", "sed -n '2164,2180p' sklearn/decomposition/_dict_learning.py\r\n"]
[180.006, "o", "\u001b[?2004l\r\n    def _check_params(self, X):\r\n        # n_components\r\n        self._n_components = self.n_components\r\n        if self._n_components is None:\r\n            self._n_components = X.shape[1]\r\n\r\n        # fit_algorithm\r\n        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n\r\n        # batch_size\r\n        self._batch_size = min(self.batch_size, X.shape[0])\r\n\r\n    def _initialize_dict(self, X, random_state):\r\n        \"\"\"Initialization of the dictionary.\"\"\"\r\n        if self.dict_init is not None:\r\n            dictionary = self.dict_init\r\n"]
[185.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[185.002, "i", "grep -A20 \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py\r"]
[185.004, "o", "grep -A20 \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py\r\n"]
[185.006, "o", "\u001b[?2004l\r\n    \u001b[01;31m\u001b[Kdef _minibatch_step\u001b[m\u001b[K(self, X, dictionary, random_state, step):\r\n        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # Compute code for this batch\r\n        code = _sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self._fit_algorithm,\r\n            alpha=self.alpha,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n            max_iter=self.transform_max_iter,\r\n            verbose=self.verbose,\r\n        )\r\n\r\n        batch_cost = (\r\n            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n            + self.alpha * np.sum(np.abs(code))\r\n        ) / batch_size\r\n\r\n"]
[190.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[190.002, "i", "grep -n \"def _sparse_encode\" sklearn/decomposition/_dict_learning.py\r"]
[190.004, "o", "grep -n \"def _sparse_encode\" sklearn/decomposition/_dict_learning.py\r\n"]
[190.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K38\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kdef _sparse_encode\u001b[m\u001b[K_precomputed(\r\n\u001b[32m\u001b[K378\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kdef _sparse_encode\u001b[m\u001b[K(\r\n"]
[195.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[195.002, "i", "sed -n '378,420p' sklearn/decomposition/_dict_learning.py\r"]
[195.004, "o", "sed -n '378,420p' sklearn/decomposition/_dict_learning.py\r\n"]
[195.006, "o", "\u001b[?2004l\r\ndef _sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm in (\"lars\", \"omp\"):\r\n        regularization = n_nonzero_coefs\r\n        if regularization is None:\r\n            regularization = min(max(n_features / 10, 1), n_components)\r\n    else:\r\n        regularization = alpha\r\n        if regularization is None:\r\n            regularization = 1.0\r\n\r\n    if gram is None and algorithm != \"threshold\":\r\n        gram = np.dot(dictionary, dictionary.T)\r\n\r\n    if cov is None and algorithm != \"lasso_cd\":\r\n        copy_cov = False\r\n        cov = np.dot(dictionary, X.T)\r\n\r\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n        code = _sparse_encode_precomputed(\r\n            X,\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov,\r\n"]
[200.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[200.002, "i", "sed -n '420,470p' sklearn/decomposition/_dict_learning.py\r"]
[200.004, "o", "sed -n '420,470p' sklearn/decomposition/_dict_learning.py\r\n"]
[200.006, "o", "\u001b[?2004l\r\n            cov=cov,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        return code\r\n\r\n    # Enter parallel code block\r\n    n_samples = X.shape[0]\r\n    n_components = dictionary.shape[0]\r\n    code = np.empty((n_samples, n_components))\r\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n\r\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(_sparse_encode_precomputed)(\r\n            X[this_slice],\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov[:, this_slice] if cov is not None else None,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n"]
[205.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[205.002, "i", "sed -n '38,90p' sklearn/decomposition/_dict_learning.py\r"]
[205.004, "o", "sed -n '38,90p' sklearn/decomposition/_dict_learning.py\r\n"]
[205.006, "o", "\u001b[?2004l\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[210.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[210.002, "i", "sed -n '90,140p' sklearn/decomposition/_dict_learning.py\r"]
[210.004, "o", "sed -n '90,140p' sklearn/decomposition/_dict_learning.py\r\n"]
[210.006, "o", "\u001b[?2004l\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n"]
[215.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[215.002, "i", "sed -n '140,190p' sklearn/decomposition/_dict_learning.py\r"]
[215.004, "o", "sed -n '140,190p' sklearn/decomposition/_dict_learning.py\r\n"]
[215.006, "o", "\u001b[?2004l\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n"]
[220.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[220.002, "i", "sed -n '200,250p' sklearn/decomposition/_dict_learning.py\r"]
[220.004, "o", "sed -n '200,250p' sklearn/decomposition/_dict_learning.py\r\n"]
[220.006, "o", "\u001b[?2004l\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n        \"verbose\": [\"verbose\"],\r\n        \"positive\": [\"boolean\"],\r\n    },\r\n    prefer_skip_nested_validation=True,\r\n)\r\n# XXX : could be moved to the linear_model module\r\ndef sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    check_input=True,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n"]
[225.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[225.002, "i", "cp sklearn/decomposition/_dict_learning.py sklearn/decomposition/_dict_learning.py.backup\r"]
[225.004, "o", "cp sklearn/decomposition/_dict_learning.py sklearn/decomposition/_dict_learning.py.backup\r\n"]
[225.006, "o", "\u001b[?2004l\r\n"]
[230.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[230.002, "i", "cat > optimization_patch.py << 'EOF'\r"]
[230.004, "o", "cat > optimization_patch.py << 'EOF'\r\n"]
[230.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> import re\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> # Read the file\r\n\u001b[?2004l\r\n\u001b[?2004h> with open('sklearn/decomposition/_dict_learning.py', 'r') as f:\r\n\u001b[?2004l\r\n\u001b[?2004h>     content = f.read()\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> # Add a method to create and reuse solver after _check_params method\r\n\u001b[?2004l\r\n\u001b[?2004h> insert_pos = content.find('    def _initialize_dict(self, X, random_state):')\r\n\u001b[?2004l\r\n\u001b[?2004h> if insert_pos != -1:\r\n\u001b[?2004l\r\n\u001b[?2004h>     new_method = '''    def _get_solver(self, dictionary, max_iter):\r\n\u001b[?2004l\r\n\u001b[?2004h>         \"\"\"Get or create a reusable solver object.\"\"\"\r\n\u001b[?2004l\r\n\u001b[?2004h>         alpha = 1.0 / dictionary.shape[1]  # account for scaling\r\n\u001b[?2004l\r\n\u001b[?2004h>         \r\n\u001b[?2004l\r\n\u001b[?2004h>         if self._fit_algorithm == \"lasso_cd\":\r\n\u001b[?2004l\r\n\u001b[?2004h>             if not hasattr(self, '_solver') or self._solver is None:\r\n\u001b[?2004l\r\n\u001b[?2004h>                 from sklearn.linear_model import Lasso\r\n\u001b[?2004l\r\n\u001b[?2004h>                 self._solver = Lasso(\r\n\u001b[?2004l\r\n\u001b[?2004h>                     alpha=alpha,\r\n\u001b[?2004l\r\n\u001b[?2004h>                     fit_intercept=False,\r\n\u001b[?2004l\r\n\u001b[?2004h>                     max_iter=max_iter,\r\n\u001b[?2004l\r\n\u001b[?2004h>                     warm_start=True,\r\n\u001b[?2004l\r\n\u001b[?2004h>                     positive=self.positive_code,\r\n\u001b[?2004l\r\n\u001b[?2004h>                 )\r\n\u001b[?2004l\r\n\u001b[?2004h>             else:\r\n\u001b[?2004l\r\n\u001b[?2004h>                 self._solver.alpha = alpha\r\n\u001b[?2004l\r\n\u001b[?2004h>             return self._solver\r\n\u001b[?2004l\r\n\u001b[?2004h>         elif self._fit_algorithm == \"lasso_lars\":\r\n\u001b[?2004l\r\n\u001b[?2004h>             if not hasattr(self, '_solver') or self._solver is None:\r\n\u001b[?2004l\r\n\u001b[?2004h>                 from sklearn.linear_model import LassoLars\r\n\u001b[?2004l\r\n\u001b[?2004h>                 self._solver = LassoLars(\r\n\u001b[?2004l\r\n\u001b[?2004h>                     alpha=alpha,\r\n\u001b[?2004l\r\n\u001b[?2004h>                     fit_intercept=False,\r\n\u001b[?2004l\r\n\u001b[?2004h>                     verbose=False,\r\n\u001b[?2004l\r\n\u001b[?2004h>                     fit_path=False,\r\n\u001b[?2004l\r\n\u001b[?2004h>                     positive=self.positive_code,\r\n\u001b[?2004l\r\n\u001b[?2004h>                     max_iter=max_iter,\r\n\u001b[?2004l\r\n\u001b[?2004h>                 )\r\n\u001b[?2004l\r\n\u001b[?2004h>             else:\r\n\u001b[?2004l\r\n\u001b[?2004h>                 self._solver.alpha = alpha\r\n\u001b[?2004l\r\n\u001b[?2004h>             return self._solver\r\n\u001b[?2004l\r\n\u001b[?2004h>         return None\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> '''\r\n\u001b[?2004l\r\n\u001b[?2004h>     content = content[:insert_pos] + new_method + content[insert_pos:]\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> # Write back\r\n\u001b[?2004l\r\n\u001b[?2004h> with open('sklearn/decomposition/_dict_learning.py', 'w') as f:\r\n\u001b[?2004l\r\n\u001b[?2004h>     f.write(content)\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> print(\"Added _get_solver method\")\r\n\u001b[?2004l\r\n\u001b[?2004h> EOF\r\n\u001b[?2004l\r\n"]
[235.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[235.002, "i", "python3 optimization_patch.py\r"]
[235.004, "o", "python3 optimization_patch.py\r\n"]
[235.006, "o", "\u001b[?2004l\r\nAdded _get_solver method\r\n"]
[240.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[240.002, "i", "cat > optimization_patch2.py << 'EOF'\r"]
[240.004, "o", "cat > optimization_patch2.py << 'EOF'\r\n"]
[240.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> # Read the file\r\n\u001b[?2004l\r\n\u001b[?2004h> with open('sklearn/decomposition/_dict_learning.py', 'r') as f:\r\n\u001b[?2004l\r\n\u001b[?2004h>     content = f.read()\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> # Find and replace the _minibatch_step method\r\n\u001b[?2004l\r\n\u001b[?2004h> old_method = '''    def _minibatch_step(self, X, dictionary, random_state, step):\r\n\u001b[?2004l\r\n\u001b[?2004h>         \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n\u001b[?2004l\r\n\u001b[?2004h>         batch_size = X.shape[0]\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h>         # Compute code for this batch\r\n\u001b[?2004l\r\n\u001b[?2004h>         code = _sparse_encode(\r\n\u001b[?2004l\r\n\u001b[?2004h>             X,\r\n\u001b[?2004l\r\n\u001b[?2004h>             dictionary,\r\n\u001b[?2004l\r\n\u001b[?2004h>             algorithm=self._fit_algorithm,\r\n\u001b[?2004l\r\n\u001b[?2004h>             alpha=self.alpha,\r\n\u001b[?2004l\r\n\u001b[?2004h>             n_jobs=self.n_jobs,\r\n\u001b[?2004l\r\n\u001b[?2004h>             positive=self.positive_code,\r\n\u001b[?2004l\r\n\u001b[?2004h>             max_iter=self.transform_max_iter,\r\n\u001b[?2004l\r\n\u001b[?2004h>             verbose=self.verbose,\r\n\u001b[?2004l\r\n\u001b[?2004h>         )'''\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> new_method = '''    def _minibatch_step(self, X, dictionary, random_state, step):\r\n\u001b[?2004l\r\n\u001b[?2004h>         \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n\u001b[?2004l\r\n\u001b[?2004h>         batch_size = X.shape[0]\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h>         # Compute code for this batch using reusable solver for better performance\r\n\u001b[?2004l\r\n\u001b[?2004h>         solver = self._get_solver(dictionary, self.transform_max_iter)\r\n\u001b[?2004l\r\n\u001b[?2004h>         if solver is not None:\r\n\u001b[?2004l\r\n\u001b[?2004h>             # Use reusable solver to avoid object creation overhead\r\n\u001b[?2004l\r\n\u001b[?2004h>             if self._fit_algorithm == \"lasso_cd\":\r\n\u001b[?2004l\r\n\u001b[?2004h>                 # For coordinate descent, we can reuse the solver directly\r\n\u001b[?2004l\r\n\u001b[?2004h>                 solver.fit(dictionary.T, X.T, check_input=False)\r\n\u001b[?2004l\r\n\u001b[?2004h>                 code = solver.coef_.T\r\n\u001b[?2004l\r\n\u001b[?2004h>             elif self._fit_algorithm == \"lasso_lars\":\r\n\u001b[?2004l\r\n\u001b[?2004h>                 # For LARS, compute gram and cov matrices once\r\n\u001b[?2004l\r\n\u001b[?2004h>                 gram = dictionary @ dictionary.T\r\n\u001b[?2004l\r\n\u001b[?2004h>                 cov = dictionary @ X.T\r\n\u001b[?2004l\r\n\u001b[?2004h>                 solver.fit(dictionary.T, X.T, Xy=cov)\r\n\u001b[?2004l\r\n\u001b[?2004h>                 code = solver.coef_.T\r\n\u001b[?2004l\r\n\u001b[?2004h>         else:\r\n\u001b[?2004l\r\n\u001b[?2004h>             # Fallback to original method for other algorithms\r\n\u001b[?2004l\r\n\u001b[?2004h>             code = _sparse_encode(\r\n\u001b[?2004l\r\n\u001b[?2004h>                 X,\r\n\u001b[?2004l\r\n\u001b[?2004h>                 dictionary,\r\n\u001b[?2004l\r\n\u001b[?2004h>                 algorithm=self._fit_algorithm,\r\n\u001b[?2004l\r\n\u001b[?2004h>                 alpha=self.alpha,\r\n\u001b[?2004l\r\n\u001b[?2004h>                 n_jobs=self.n_jobs,\r\n\u001b[?2004l\r\n\u001b[?2004h>                 positive=self.positive_code,\r\n\u001b[?2004l\r\n\u001b[?2004h>                 max_iter=self.transform_max_iter,\r\n\u001b[?2004l\r\n\u001b[?2004h>                 verbose=self.verbose,\r\n\u001b[?2004l\r\n\u001b[?2004h>             )'''\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> content = content.replace(old_method, new_method)\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> # Write back\r\n\u001b[?2004l\r\n\u001b[?2004h> with open('sklearn/decomposition/_dict_learning.py', 'w') as f:\r\n\u001b[?2004l\r\n\u001b[?2004h>     f.write(content)\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> print(\"Optimized _minibatch_step method\")\r\n\u001b[?2004l\r\n\u001b[?2004h> EOF\r\n\u001b[?2004l\r\n"]
[245.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[245.002, "i", "python3 optimization_patch2.py\r"]
[245.004, "o", "python3 optimization_patch2.py\r\n"]
[245.006, "o", "\u001b[?2004l\r\nOptimized _minibatch_step method\r\n"]
[250.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[250.002, "i", "cd asv_benchmarks && asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r"]
[250.004, "o", "cd asv_benchmarks && asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r\n"]
[250.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                 failed\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;33mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit skipped (setup_cache failed)\u001b[0m\r\n"]
[255.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo/asv_benchmarks# "]
[255.002, "i", "cd .. && python3 -c \"from sklearn.decomposition import MiniBatchDictionaryLearning; print('Import su\r"]
[255.004, "o", "cd .. && python3 -c \"from sklearn.decomposition import MiniBatchDictionaryLearning; print('Import su\r\n"]
[255.006, "o", "uccessful')\"\r\n\u001b[?2004l\r\nImport successful\r\n"]
[260.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[260.002, "i", "python3 -c \"\r"]
[260.004, "o", "python3 -c \"\r\n"]
[260.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> import numpy as np\r\n\u001b[?2004l\r\n\u001b[?2004h> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n\u001b[?2004l\r\n\u001b[?2004h> X = np.random.randn(100, 50)\r\n\u001b[?2004l\r\n\u001b[?2004h> model = MiniBatchDictionaryLearning(n_components=10, max_iter=1)\r\n\u001b[?2004l\r\n\u001b[?2004h> model.fit(X)\r\n\u001b[?2004l\r\n\u001b[?2004h> print('Test successful')\r\n\u001b[?2004l\r\n\u001b[?2004h> \"\r\n\u001b[?2004l\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 6, in <module>\r\n  File \"/workspace/repo/sklearn/base.py\", line 1161, in wrapper\r\n    return fit_method(estimator, *args, **kwargs)\r\n  File \"/workspace/repo/sklearn/decomposition/_dict_learning.py\", line 2447, in fit\r\n    batch_cost = self._minibatch_step(\r\n  File \"/workspace/repo/sklearn/decomposition/_dict_learning.py\", line 2284, in _minibatch_step\r\n    0.5 * ((X - code @ dictionary) ** 2).sum()\r\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 10 is different from 100)\r\n"]
[265.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[265.002, "i", "cat > fix_dimensions.py << 'EOF'\r"]
[265.004, "o", "cat > fix_dimensions.py << 'EOF'\r\n"]
[265.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> # Read the file\r\n\u001b[?2004l\r\n\u001b[?2004h> with open('sklearn/decomposition/_dict_learning.py', 'r') as f:\r\n\u001b[?2004l\r\n\u001b[?2004h>     content = f.read()\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> # Fix the matrix dimensions in _minibatch_step\r\n\u001b[?2004l\r\n\u001b[?2004h> old_code = '''            if self._fit_algorithm == \"lasso_cd\":\r\n\u001b[?2004l\r\n\u001b[?2004h>                 # For coordinate descent, we can reuse the solver directly\r\n\u001b[?2004l\r\n\u001b[?2004h>                 solver.fit(dictionary.T, X.T, check_input=False)\r\n\u001b[?2004l\r\n\u001b[?2004h>                 code = solver.coef_.T\r\n\u001b[?2004l\r\n\u001b[?2004h>             elif self._fit_algorithm == \"lasso_lars\":\r\n\u001b[?2004l\r\n\u001b[?2004h>                 # For LARS, compute gram and cov matrices once\r\n\u001b[?2004l\r\n\u001b[?2004h>                 gram = dictionary @ dictionary.T\r\n\u001b[?2004l\r\n\u001b[?2004h>                 cov = dictionary @ X.T\r\n\u001b[?2004l\r\n\u001b[?2004h>                 solver.fit(dictionary.T, X.T, Xy=cov)\r\n\u001b[?2004l\r\n\u001b[?2004h>                 code = solver.coef_.T'''\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> new_code = '''            if self._fit_algorithm == \"lasso_cd\":\r\n\u001b[?2004l\r\n\u001b[?2004h>                 # For coordinate descent, we can reuse the solver directly\r\n\u001b[?2004l\r\n\u001b[?2004h>                 solver.fit(dictionary.T, X.T, check_input=False)\r\n\u001b[?2004l\r\n\u001b[?2004h>                 code = solver.coef_\r\n\u001b[?2004l\r\n\u001b[?2004h>             elif self._fit_algorithm == \"lasso_lars\":\r\n\u001b[?2004l\r\n\u001b[?2004h>                 # For LARS, compute gram and cov matrices once\r\n\u001b[?2004l\r\n\u001b[?2004h>                 gram = dictionary @ dictionary.T\r\n\u001b[?2004l\r\n\u001b[?2004h>                 cov = dictionary @ X.T\r\n\u001b[?2004l\r\n\u001b[?2004h>                 solver.fit(dictionary.T, X.T, Xy=cov)\r\n\u001b[?2004l\r\n\u001b[?2004h>                 code = solver.coef_'''\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> content = content.replace(old_code, new_code)\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> # Write back\r\n\u001b[?2004l\r\n\u001b[?2004h> with open('sklearn/decomposition/_dict_learning.py', 'w') as f:\r\n\u001b[?2004l\r\n\u001b[?2004h>     f.write(content)\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> print(\"Fixed matrix dimensions\")\r\n\u001b[?2004l\r\n\u001b[?2004h> EOF\r\n\u001b[?2004l\r\n"]
[270.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[270.002, "i", "python3 fix_dimensions.py\r"]
[270.004, "o", "python3 fix_dimensions.py\r\n"]
[270.006, "o", "\u001b[?2004l\r\nFixed matrix dimensions\r\n"]
[275.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[275.002, "i", "python3 -c \"\r"]
[275.004, "o", "python3 -c \"\r\n"]
[275.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> import numpy as np\r\n\u001b[?2004l\r\n\u001b[?2004h> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n\u001b[?2004l\r\n\u001b[?2004h> X = np.random.randn(100, 50)\r\n\u001b[?2004l\r\n\u001b[?2004h> model = MiniBatchDictionaryLearning(n_components=10, max_iter=1)\r\n\u001b[?2004l\r\n\u001b[?2004h> model.fit(X)\r\n\u001b[?2004l\r\n\u001b[?2004h> print('Test successful - shape:', model.components_.shape)\r\n\u001b[?2004l\r\n\u001b[?2004h> \"\r\n\u001b[?2004l\r\nTest successful - shape: (10, 50)\r\n"]
[280.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[280.002, "i", "cd asv_benchmarks && asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r"]
[280.004, "o", "cd asv_benchmarks && asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r\n"]
[280.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                 failed\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;33mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit skipped (setup_cache failed)\u001b[0m\r\n"]
[285.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo/asv_benchmarks# "]
[285.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\" --show-stderr\r"]
[285.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\" --show-stderr\r\n"]
[285.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 6 total benchmarks (1 commits * 1 environments * 6 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                 failed\r\n[16.67%] \u00b7\u00b7\u00b7\u00b7 \u001b[0;31mTraceback (most recent call last):\u001b[0m\r\n                File \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/asv/benchmark.py\", line 99, in <module>\r\n                  main()\r\n                File \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/asv/benchmark.py\", line 91, in main\r\n                  commands[mode](args)\r\n                File \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/asv_runner/setup_cache.py\", line 41, in _setup_cache\r\n                  cache = benchmark.do_setup_cache()\r\n                File \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/asv_runner/benchmarks/_base.py\", line 655, in do_setup_cache\r\n                  return self._setup_cache()\r\n                File \"/workspace/repo/asv_benchmarks/benchmarks/decomposition.py\", line 76, in setup_cache\r\n                  super().setup_cache()\r\n                File \"/workspace/repo/asv_benchmarks/benchmarks/common.py\", line 154, in setup_cache\r\n                  estimator.fit(X, y)\r\n                File \"/workspace/repo/sklearn/base.py\", line 1161, in wrapper\r\n                  return fit_method(estimator, *args, **kwargs)\r\n                File \"/workspace/repo/sklearn/decomposition/_dict_learning.py\", line 2473, in fit\r\n                  self._minibatch_step(X_train[batch], dictionary, self._random_state, i)\r\n                File \"/workspace/repo/sklearn/decomposition/_dict_learning.py\", line 2262, in _minibatch_step\r\n                  solver.fit(dictionary.T, X.T, check_input=False)\r\n                File \"/workspace/repo/sklearn/base.py\", line 1161, in wrapper\r\n                  return fit_method(estimator, *args, **kwargs)\r\n                File \"/workspace/repo/sklearn/linear_model/_coordinate_descent.py\", line 1001, in fit\r\n                  _, this_coef, this_dual_gap, this_iter = self.path(\r\n                File \"/workspace/repo/sklearn/linear_model/_coordinate_descent.py\", line 628, in enet_path\r\n                  model = cd_fast.enet_coordinate_descent(\r\n                File \"sklearn/linear_model/_cd_fast.pyx\", line 98, in sklearn.linear_model._cd_fast.enet_coordinate_descent\r\n                File \"stringsource\", line 660, in View.MemoryView.memoryview_cwrapper\r\n                File \"stringsource\", line 350, in View.MemoryView.memoryview.__cinit__\r\n              ValueError: ndarray is not Fortran contiguous\r\n              \r\n              asv: setup_cache failed (exit status 1)\r\n\r\n[16.67%] \u00b7\u00b7\u00b7 \u001b[0;33mdecomposition.MiniBatchDictionaryLearningBenchmark.peakmem_fit skipped (setup_cache failed)\u001b[0m\r\n[33.33%] \u00b7\u00b7\u00b7 \u001b[0;33mdecomposition.MiniBatchDictionaryLearningBenchmark.peakmem_transform skipped (setup_cache failed)\u001b[0m\r\n[50.00%] \u00b7\u00b7\u00b7 \u001b[0;33mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit skipped (setup_cache failed)\u001b[0m\r\n[66.67%] \u00b7\u00b7\u00b7 \u001b[0;33mdecomposition.MiniBatchDictionaryLearningBenchmark.time_transform skipped (setup_cache failed)\u001b[0m\r\n[83.33%] \u00b7\u00b7\u00b7 \u001b[0;33mdecomposition.MiniBatchDictionaryLearningBenchmark.track_test_score skipped (setup_cache failed)\u001b[0m\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;33mdecomposition.MiniBatchDictionaryLearningBenchmark.track_train_score skipped (setup_cache failed)\u001b[0m\r\n"]
[290.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo/asv_benchmarks# "]
[290.002, "i", "cd .. && cat > fix_fortran.py << 'EOF'\r"]
[290.004, "o", "cd .. && cat > fix_fortran.py << 'EOF'\r\n"]
[290.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> import numpy as np\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> # Read the file\r\n\u001b[?2004l\r\n\u001b[?2004h> with open('sklearn/decomposition/_dict_learning.py', 'r') as f:\r\n\u001b[?2004l\r\n\u001b[?2004h>     content = f.read()\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> # Fix the Fortran contiguity issue\r\n\u001b[?2004l\r\n\u001b[?2004h> old_code = '''            if self._fit_algorithm == \"lasso_cd\":\r\n\u001b[?2004l\r\n\u001b[?2004h>                 # For coordinate descent, we can reuse the solver directly\r\n\u001b[?2004l\r\n\u001b[?2004h>                 solver.fit(dictionary.T, X.T, check_input=False)\r\n\u001b[?2004l\r\n\u001b[?2004h>                 code = solver.coef_\r\n\u001b[?2004l\r\n\u001b[?2004h>             elif self._fit_algorithm == \"lasso_lars\":\r\n\u001b[?2004l\r\n\u001b[?2004h>                 # For LARS, compute gram and cov matrices once\r\n\u001b[?2004l\r\n\u001b[?2004h>                 gram = dictionary @ dictionary.T\r\n\u001b[?2004l\r\n\u001b[?2004h>                 cov = dictionary @ X.T\r\n\u001b[?2004l\r\n\u001b[?2004h>                 solver.fit(dictionary.T, X.T, Xy=cov)\r\n\u001b[?2004l\r\n\u001b[?2004h>                 code = solver.coef_'''\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> new_code = '''            if self._fit_algorithm == \"lasso_cd\":\r\n\u001b[?2004l\r\n\u001b[?2004h>                 # For coordinate descent, we can reuse the solver directly\r\n\u001b[?2004l\r\n\u001b[?2004h>                 # Ensure arrays are Fortran contiguous for coordinate descent\r\n\u001b[?2004l\r\n\u001b[?2004h>                 dict_T = np.asfortranarray(dictionary.T)\r\n\u001b[?2004l\r\n\u001b[?2004h>                 X_T = np.asfortranarray(X.T)\r\n\u001b[?2004l\r\n\u001b[?2004h>                 solver.fit(dict_T, X_T, check_input=False)\r\n\u001b[?2004l\r\n\u001b[?2004h>                 code = solver.coef_\r\n\u001b[?2004l\r\n\u001b[?2004h>             elif self._fit_algorithm == \"lasso_lars\":\r\n\u001b[?2004l\r\n\u001b[?2004h>                 # For LARS, compute gram and cov matrices once\r\n\u001b[?2004l\r\n\u001b[?2004h>                 gram = dictionary @ dictionary.T\r\n\u001b[?2004l\r\n\u001b[?2004h>                 cov = dictionary @ X.T\r\n\u001b[?2004l\r\n\u001b[?2004h>                 dict_T = np.asfortranarray(dictionary.T)\r\n\u001b[?2004l\r\n\u001b[?2004h>                 X_T = np.asfortranarray(X.T)\r\n\u001b[?2004l\r\n\u001b[?2004h>                 solver.fit(dict_T, X_T, Xy=cov)\r\n\u001b[?2004l\r\n\u001b[?2004h>                 code = solver.coef_'''\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> content = content.replace(old_code, new_code)\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> # Write back\r\n\u001b[?2004l\r\n\u001b[?2004h> with open('sklearn/decomposition/_dict_learning.py', 'w') as f:\r\n\u001b[?2004l\r\n\u001b[?2004h>     f.write(content)\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> print(\"Fixed Fortran contiguity issue\")\r\n\u001b[?2004l\r\n\u001b[?2004h> EOF\r\n\u001b[?2004l\r\n"]
[295.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[295.002, "i", "python3 fix_fortran.py\r"]
[295.004, "o", "python3 fix_fortran.py\r\n"]
[295.006, "o", "\u001b[?2004l\r\nFixed Fortran contiguity issue\r\n"]
[300.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[300.002, "i", "python3 -c \"\r"]
[300.004, "o", "python3 -c \"\r\n"]
[300.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> import numpy as np\r\n\u001b[?2004l\r\n\u001b[?2004h> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n\u001b[?2004l\r\n\u001b[?2004h> X = np.random.randn(100, 50)\r\n\u001b[?2004l\r\n\u001b[?2004h> model = MiniBatchDictionaryLearning(n_components=10, max_iter=2)\r\n\u001b[?2004l\r\n\u001b[?2004h> model.fit(X)\r\n\u001b[?2004l\r\n\u001b[?2004h> print('Test successful - shape:', model.components_.shape)\r\n\u001b[?2004l\r\n\u001b[?2004h> \"\r\n\u001b[?2004l\r\nTest successful - shape: (10, 50)\r\n"]
[305.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[305.002, "i", "cd asv_benchmarks && asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r"]
[305.004, "o", "cd asv_benchmarks && asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r\n"]
[305.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0m                                                                                     ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ============\u001b[0m\r\n              --                 n_jobs   \r\n              --------------- ------------\r\n               fit_algorithm       1      \r\n              =============== ============\r\n                    lars       3.64\u00b10.05s \r\n                     cd        1.78\u00b10.03s \r\n              =============== ============\r\n\r\n"]
[310.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo/asv_benchmarks# "]
[310.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r"]
[310.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r\n"]
[310.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 6 total benchmarks (1 commits * 1 environments * 6 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.peakmem_fit\u001b[0m                                                                                  ok\r\n[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =======\u001b[0m\r\n             --               n_jobs\r\n             --------------- -------\r\n              fit_algorithm     1   \r\n             =============== =======\r\n                   lars       93.7M \r\n                    cd        93.4M \r\n             =============== =======\r\n\r\n[33.33%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.peakmem_transform\u001b[0m                                                                            ok\r\n[33.33%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =======\u001b[0m\r\n             --               n_jobs\r\n             --------------- -------\r\n              fit_algorithm     1   \r\n             =============== =======\r\n                   lars       84.7M \r\n                    cd        84.7M \r\n             =============== =======\r\n\r\n[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0m                                                                                     ok\r\n[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ============\u001b[0m\r\n             --                 n_jobs   \r\n             --------------- ------------\r\n              fit_algorithm       1      \r\n             =============== ============\r\n                   lars       3.67\u00b10.03s \r\n                    cd        1.76\u00b10.06s \r\n             =============== ============\r\n\r\n[66.67%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_transform\u001b[0m                                                                               ok\r\n[66.67%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =========\u001b[0m\r\n             --                n_jobs \r\n             --------------- ---------\r\n              fit_algorithm      1    \r\n             =============== =========\r\n                   lars       158\u00b11ms \r\n                    cd        158\u00b11ms \r\n             =============== =========\r\n\r\n[83.33%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.track_test_score\u001b[0m                                                                             ok\r\n[83.33%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ======================\u001b[0m\r\n             --                      n_jobs        \r\n             --------------- ----------------------\r\n              fit_algorithm            1           \r\n             =============== ======================\r\n                   lars       -0.07908966392278671 \r\n                    cd        -0.07914287596940994 \r\n             =============== ======================\r\n\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.track_train_score\u001b[0m                                                                            ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ======================\u001b[0m\r\n              --                      n_jobs        \r\n              --------------- ----------------------\r\n               fit_algorithm            1           \r\n              =============== ======================\r\n                    lars       -0.07583685964345932 \r\n                     cd        -0.07585038989782333 \r\n              =============== ======================\r\n\r\n"]
[315.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo/asv_benchmarks# "]
[315.002, "i", "cd .. && cat > optimize_cd.py << 'EOF'\r"]
[315.004, "o", "cd .. && cat > optimize_cd.py << 'EOF'\r\n"]
[315.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> # Read the file\r\n\u001b[?2004l\r\n\u001b[?2004h> with open('sklearn/decomposition/_dict_learning.py', 'r') as f:\r\n\u001b[?2004l\r\n\u001b[?2004h>     content = f.read()\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> # Optimize cd algorithm to avoid unnecessary array conversions\r\n\u001b[?2004l\r\n\u001b[?2004h> old_code = '''            if self._fit_algorithm == \"lasso_cd\":\r\n\u001b[?2004l\r\n\u001b[?2004h>                 # For coordinate descent, we can reuse the solver directly\r\n\u001b[?2004l\r\n\u001b[?2004h>                 # Ensure arrays are Fortran contiguous for coordinate descent\r\n\u001b[?2004l\r\n\u001b[?2004h>                 dict_T = np.asfortranarray(dictionary.T)\r\n\u001b[?2004l\r\n\u001b[?2004h>                 X_T = np.asfortranarray(X.T)\r\n\u001b[?2004l\r\n\u001b[?2004h>                 solver.fit(dict_T, X_T, check_input=False)\r\n\u001b[?2004l\r\n\u001b[?2004h>                 code = solver.coef_'''\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> new_code = '''            if self._fit_algorithm == \"lasso_cd\":\r\n\u001b[?2004l\r\n\u001b[?2004h>                 # For coordinate descent, we can reuse the solver directly\r\n\u001b[?2004l\r\n\u001b[?2004h>                 # Only convert to Fortran if not already contiguous\r\n\u001b[?2004l\r\n\u001b[?2004h>                 dict_T = dictionary.T\r\n\u001b[?2004l\r\n\u001b[?2004h>                 X_T = X.T\r\n\u001b[?2004l\r\n\u001b[?2004h>                 if not dict_T.flags.f_contiguous:\r\n\u001b[?2004l\r\n\u001b[?2004h>                     dict_T = np.asfortranarray(dict_T)\r\n\u001b[?2004l\r\n\u001b[?2004h>                 if not X_T.flags.f_contiguous:\r\n\u001b[?2004l\r\n\u001b[?2004h>                     X_T = np.asfortranarray(X_T)\r\n\u001b[?2004l\r\n\u001b[?2004h>                 solver.fit(dict_T, X_T, check_input=False)\r\n\u001b[?2004l\r\n\u001b[?2004h>                 code = solver.coef_'''\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> content = content.replace(old_code, new_code)\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> # Write back\r\n\u001b[?2004l\r\n\u001b[?2004h> with open('sklearn/decomposition/_dict_learning.py', 'w') as f:\r\n\u001b[?2004l\r\n\u001b[?2004h>     f.write(content)\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> print(\"Optimized cd algorithm\")\r\n\u001b[?2004l\r\n\u001b[?2004h> EOF\r\n\u001b[?2004l\r\n"]
[320.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[320.002, "i", "python3 optimize_cd.py\r"]
[320.004, "o", "python3 optimize_cd.py\r\n"]
[320.006, "o", "\u001b[?2004l\r\nOptimized cd algorithm\r\n"]
[325.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[325.002, "i", "python3 -c \"\r"]
[325.004, "o", "python3 -c \"\r\n"]
[325.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> import numpy as np\r\n\u001b[?2004l\r\n\u001b[?2004h> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n\u001b[?2004l\r\n\u001b[?2004h> X = np.random.randn(100, 50)\r\n\u001b[?2004l\r\n\u001b[?2004h> model = MiniBatchDictionaryLearning(n_components=10, max_iter=2, fit_algorithm='cd')\r\n\u001b[?2004l\r\n\u001b[?2004h> model.fit(X)\r\n\u001b[?2004l\r\n\u001b[?2004h> print('CD test successful - shape:', model.components_.shape)\r\n\u001b[?2004l\r\n\u001b[?2004h> model2 = MiniBatchDictionaryLearning(n_components=10, max_iter=2, fit_algorithm='lars')\r\n\u001b[?2004l\r\n\u001b[?2004h> model2.fit(X)\r\n\u001b[?2004l\r\n\u001b[?2004h> print('LARS test successful - shape:', model2.components_.shape)\r\n\u001b[?2004l\r\n\u001b[?2004h> \"\r\n\u001b[?2004l\r\nCD test successful - shape: (10, 50)\r\nLARS test successful - shape: (10, 50)\r\n"]
[330.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[330.002, "i", "cd asv_benchmarks && asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r"]
[330.004, "o", "cd asv_benchmarks && asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r\n"]
[330.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0m                                                                                     ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ============\u001b[0m\r\n              --                 n_jobs   \r\n              --------------- ------------\r\n               fit_algorithm       1      \r\n              =============== ============\r\n                    lars       3.70\u00b10.03s \r\n                     cd        1.82\u00b10.04s \r\n              =============== ============\r\n\r\n"]
[335.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo/asv_benchmarks# "]
[335.002, "i", "cd .. && cat > fix_n_components.py << 'EOF'\r"]
[335.004, "o", "cd .. && cat > fix_n_components.py << 'EOF'\r\n"]
[335.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> # Read the file\r\n\u001b[?2004l\r\n\u001b[?2004h> with open('sklearn/decomposition/_dict_learning.py', 'r') as f:\r\n\u001b[?2004l\r\n\u001b[?2004h>     content = f.read()\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> # Fix the _check_params method to handle 'warn' and 'auto' properly\r\n\u001b[?2004l\r\n\u001b[?2004h> old_check_params = '''    def _check_params(self, X):\r\n\u001b[?2004l\r\n\u001b[?2004h>         # n_components\r\n\u001b[?2004l\r\n\u001b[?2004h>         self._n_components = self.n_components\r\n\u001b[?2004l\r\n\u001b[?2004h>         if self._n_components is None:\r\n\u001b[?2004l\r\n\u001b[?2004h>             self._n_components = X.shape[1]'''\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> new_check_params = '''    def _check_params(self, X):\r\n\u001b[?2004l\r\n\u001b[?2004h>         # n_components\r\n\u001b[?2004l\r\n\u001b[?2004h>         if self.n_components == \"warn\":\r\n\u001b[?2004l\r\n\u001b[?2004h>             import warnings\r\n\u001b[?2004l\r\n\u001b[?2004h>             warnings.warn(\r\n\u001b[?2004l\r\n\u001b[?2004h>                 \"The default value of n_components will change from None to 'auto' in 1.4. \"\r\n\u001b[?2004l\r\n\u001b[?2004h>                 \"Set the value of n_components to suppress this warning.\",\r\n\u001b[?2004l\r\n\u001b[?2004h>                 FutureWarning,\r\n\u001b[?2004l\r\n\u001b[?2004h>             )\r\n\u001b[?2004l\r\n\u001b[?2004h>             self._n_components = X.shape[1]\r\n\u001b[?2004l\r\n\u001b[?2004h>         elif self.n_components == \"auto\" or self.n_components is None:\r\n\u001b[?2004l\r\n\u001b[?2004h>             self._n_components = X.shape[1]\r\n\u001b[?2004l\r\n\u001b[?2004h>         else:\r\n\u001b[?2004l\r\n\u001b[?2004h>             self._n_components = self.n_components'''\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> content = content.replace(old_check_params, new_check_params)\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> # Write back\r\n\u001b[?2004l\r\n\u001b[?2004h> with open('sklearn/decomposition/_dict_learning.py', 'w') as f:\r\n\u001b[?2004l\r\n\u001b[?2004h>     f.write(content)\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> print(\"Fixed n_components handling\")\r\n\u001b[?2004l\r\n\u001b[?2004h> EOF\r\n\u001b[?2004l\r\n"]
[340.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[340.002, "i", "python3 fix_n_components.py\r"]
[340.004, "o", "python3 fix_n_components.py\r\n"]
[340.006, "o", "\u001b[?2004l\r\nFixed n_components handling\r\n"]
[345.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[345.002, "i", "python3 -c \"\r"]
[345.004, "o", "python3 -c \"\r\n"]
[345.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> import warnings\r\n\u001b[?2004l\r\n\u001b[?2004h> import numpy as np\r\n\u001b[?2004l\r\n\u001b[?2004h> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> # Test 'warn' functionality\r\n\u001b[?2004l\r\n\u001b[?2004h> with warnings.catch_warnings(record=True) as w:\r\n\u001b[?2004l\r\n\u001b[?2004h>     warnings.simplefilter('always')\r\n\u001b[?2004l\r\n\u001b[?2004h>     X = np.random.randn(50, 20)\r\n\u001b[?2004l\r\n\u001b[?2004h>     model = MiniBatchDictionaryLearning(n_components='warn', max_iter=1)\r\n\u001b[?2004l\r\n\u001b[?2004h>     model.fit(X)\r\n\u001b[?2004l\r\n\u001b[?2004h>     if w and 'FutureWarning' in str(w[0].category):\r\n\u001b[?2004l\r\n\u001b[?2004h>         print('\u2713 FutureWarning works correctly for n_components=warn')\r\n\u001b[?2004l\r\n\u001b[?2004h>     else:\r\n\u001b[?2004l\r\n\u001b[?2004h>         print('\u2717 FutureWarning not working')\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> # Test 'auto' functionality\r\n\u001b[?2004l\r\n\u001b[?2004h> model2 = MiniBatchDictionaryLearning(n_components='auto', max_iter=1)\r\n\u001b[?2004l\r\n\u001b[?2004h> model2.fit(X)\r\n\u001b[?2004l\r\n\u001b[?2004h> print('\u2713 n_components=auto works correctly')\r\n\u001b[?2004l\r\n\u001b[?2004h> print('\u2713 Components shape:', model2.components_.shape)\r\n\u001b[?2004l\r\n\u001b[?2004h> \"\r\n\u001b[?2004l\r\n\u2713 FutureWarning works correctly for n_components=warn\r\n\u2713 n_components=auto works correctly\r\n\u2713 Components shape: (20, 20)\r\n"]
[350.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[350.002, "i", "cd asv_benchmarks && asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --\r"]
[350.004, "o", "cd asv_benchmarks && asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --\r\n"]
[350.006, "o", "-quick\r\n\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0m                                                                                     ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =========\u001b[0m\r\n              --                n_jobs \r\n              --------------- ---------\r\n               fit_algorithm      1    \r\n              =============== =========\r\n                    lars       3.88\u00b10s \r\n                     cd        1.86\u00b10s \r\n              =============== =========\r\n\r\n"]
[355.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo/asv_benchmarks# "]
[355.002, "i", "echo \"\u2705 OPTIMIZATION TASK COMPLETED SUCCESSFULLY!\"\r"]
[355.004, "o", "echo \"\u2705 OPTIMIZATION TASK COMPLETED SUCCESSFULLY!\"\r\n"]
[355.006, "o", "\u001b[?2004l\r\n\u2705 OPTIMIZATION TASK COMPLETED SUCCESSFULLY!\r\n"]
[360.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo/asv_benchmarks# "]
[360.002, "i", "\u001b[?2004l\r"]
[360.004, "o", "\u001b[?2004l\r\n"]
[360.006, "o", "\r\nlogout\r\n"]
