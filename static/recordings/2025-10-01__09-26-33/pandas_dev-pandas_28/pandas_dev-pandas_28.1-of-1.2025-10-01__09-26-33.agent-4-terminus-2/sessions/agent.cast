{"version": 2, "width": 160, "height": 40, "timestamp": 1762942265, "env": {"SHELL": "/bin/bash", "TERM": "screen"}}
[0.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[0.002, "i", "asciinema rec --stdin /logs/agent.cast\r"]
[0.004, "o", "asciinema rec --stdin /logs/agent.cast\r\n"]
[0.006, "o", "\u001b[?2004l\r\n\u001b[0;31masciinema: /logs/agent.cast already exists, aborting\u001b[0m\r\n\u001b[0;31masciinema: use --overwrite option if you want to overwrite existing recording\u001b[0m\r\n\u001b[0;31masciinema: use --append option if you want to append to existing recording\u001b[0m\r\n"]
[5.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[5.002, "i", "clear\r"]
[5.004, "o", "clear\r\n"]
[5.006, "o", "\u001b[?2004l\r\n\u001b[H\u001b[J\r\n"]
[10.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[10.002, "i", "ls -la\r"]
[10.004, "o", "ls -la\r\n"]
[10.006, "o", "\u001b[?2004l\r\ntotal 212\r\ndrwxr-xr-x 1 root root  4096 Oct  1 17:58 \u001b[0m\u001b[01;34m.\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 15 17:16 \u001b[01;34m..\u001b[0m\r\ndrwxr-xr-x 2 root root  4096 Sep 12 22:25 \u001b[01;34m.circleci\u001b[0m\r\n-rw-r--r-- 1 root root  1021 Sep 12 22:25 .devcontainer.json\r\ndrwxr-xr-x 1 root root  4096 Oct  1 17:58 \u001b[01;34m.git\u001b[0m\r\n-rw-r--r-- 1 root root   229 Sep 12 22:25 .gitattributes\r\ndrwxr-xr-x 1 root root  4096 Sep 12 22:25 \u001b[01;34m.github\u001b[0m\r\n-rw-r--r-- 1 root root  1765 Sep 12 22:25 .gitignore\r\n-rw-r--r-- 1 root root  2322 Sep 12 22:25 .gitpod.yml\r\n-rw-r--r-- 1 root root   761 Sep 12 22:25 .libcst.codemod.yaml\r\n-rw-r--r-- 1 root root 15546 Sep 12 22:25 .pre-commit-config.yaml\r\n-rw-r--r-- 1 root root  2284 Sep 12 22:25 AUTHORS.md\r\n-rw-r--r-- 1 root root   372 Sep 12 22:25 CITATION.cff\r\n-rw-r--r-- 1 root root   369 Sep 12 22:25 Dockerfile\r\n-rw-r--r-- 1 root root  1634 Sep 12 22:25 LICENSE\r\ndrwxr-xr-x 1 root root  4096 Sep 12 22:25 \u001b[01;34mLICENSES\u001b[0m\r\n-rw-r--r-- 1 root root  1294 Sep 12 22:25 MANIFEST.in\r\n-rw-r--r-- 1 root root 10387 Sep 12 22:25 README.md\r\n-rw-r--r-- 1 root root   246 Sep 12 22:25 RELEASE.md\r\ndrwxr-xr-x 1 root root  4096 Sep 12 22:25 \u001b[01;34masv_bench\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 12 22:25 \u001b[01;34mci\u001b[0m\r\n-rw-r--r-- 1 root root   256 Sep 12 22:25 codecov.yml\r\ndrwxr-xr-x 1 root root  4096 Sep 12 22:25 \u001b[01;34mdoc\u001b[0m\r\n-rw-r--r-- 1 root root  2405 Sep 12 22:25 environment.yml\r\ndrwxr-xr-x 1 root root  4096 Sep 12 22:25 \u001b[01;34mgitpod\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 13 21:58 \u001b[01;34mpandas\u001b[0m\r\ndrwxr-xr-x 2 root root  4096 Sep 13 21:43 \u001b[01;34mpandas.egg-info\u001b[0m\r\n-rw-r--r-- 1 root root 17415 Sep 12 22:25 pyproject.toml\r\n-rw-r--r-- 1 root root  4429 Sep 12 22:25 pyright_reportGeneralTypeIssues.json\r\n-rw-r--r-- 1 root root  1355 Sep 12 22:25 requirements-dev.txt\r\ndrwxr-xr-x 1 root root  4096 Sep 12 22:25 \u001b[01;34mscripts\u001b[0m\r\n-rw-r--r-- 1 root root   645 Sep 12 22:25 setup.cfg\r\n-rwxr-xr-x 1 root root 22455 Sep 12 22:25 \u001b[01;32msetup.py\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 12 22:25 \u001b[01;34mtypings\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 12 22:25 \u001b[01;34mweb\u001b[0m\r\n"]
[15.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[15.002, "i", "git status -sb\r"]
[15.004, "o", "git status -sb\r\n"]
[15.006, "o", "\u001b[?2004l\r\n## \u001b[32mmain\u001b[m\r\n"]
[20.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[20.002, "i", "grep -RIl \"asv.conf.json\" . || true\r"]
[20.004, "o", "grep -RIl \"asv.conf.json\" . || true\r\n"]
[20.006, "o", "\u001b[?2004l\r\n\u001b[35m\u001b[K./scripts/tests/data/deps_minimum.toml\u001b[m\u001b[K\r\n\u001b[35m\u001b[K./pyproject.toml\u001b[m\u001b[K\r\n\u001b[35m\u001b[K./setup.py\u001b[m\u001b[K\r\n\u001b[35m\u001b[K./doc/source/development/contributing_codebase.rst\u001b[m\u001b[K\r\n"]
[25.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[25.002, "i", "ls -la | sed -n '1,200p'\r"]
[25.004, "o", "ls -la | sed -n '1,200p'\r\n"]
[25.006, "o", "\u001b[?2004l\r\ntotal 212\r\ndrwxr-xr-x 1 root root  4096 Oct  1 17:58 .\r\ndrwxr-xr-x 1 root root  4096 Sep 15 17:16 ..\r\ndrwxr-xr-x 2 root root  4096 Sep 12 22:25 .circleci\r\n-rw-r--r-- 1 root root  1021 Sep 12 22:25 .devcontainer.json\r\ndrwxr-xr-x 1 root root  4096 Oct  1 18:00 .git\r\n-rw-r--r-- 1 root root   229 Sep 12 22:25 .gitattributes\r\ndrwxr-xr-x 1 root root  4096 Sep 12 22:25 .github\r\n-rw-r--r-- 1 root root  1765 Sep 12 22:25 .gitignore\r\n-rw-r--r-- 1 root root  2322 Sep 12 22:25 .gitpod.yml\r\n-rw-r--r-- 1 root root   761 Sep 12 22:25 .libcst.codemod.yaml\r\n-rw-r--r-- 1 root root 15546 Sep 12 22:25 .pre-commit-config.yaml\r\n-rw-r--r-- 1 root root  2284 Sep 12 22:25 AUTHORS.md\r\n-rw-r--r-- 1 root root   372 Sep 12 22:25 CITATION.cff\r\n-rw-r--r-- 1 root root   369 Sep 12 22:25 Dockerfile\r\n-rw-r--r-- 1 root root  1634 Sep 12 22:25 LICENSE\r\ndrwxr-xr-x 1 root root  4096 Sep 12 22:25 LICENSES\r\n-rw-r--r-- 1 root root  1294 Sep 12 22:25 MANIFEST.in\r\n-rw-r--r-- 1 root root 10387 Sep 12 22:25 README.md\r\n-rw-r--r-- 1 root root   246 Sep 12 22:25 RELEASE.md\r\ndrwxr-xr-x 1 root root  4096 Sep 12 22:25 asv_bench\r\ndrwxr-xr-x 1 root root  4096 Sep 12 22:25 ci\r\n-rw-r--r-- 1 root root   256 Sep 12 22:25 codecov.yml\r\ndrwxr-xr-x 1 root root  4096 Sep 12 22:25 doc\r\n-rw-r--r-- 1 root root  2405 Sep 12 22:25 environment.yml\r\ndrwxr-xr-x 1 root root  4096 Sep 12 22:25 gitpod\r\ndrwxr-xr-x 1 root root  4096 Sep 13 21:58 pandas\r\ndrwxr-xr-x 2 root root  4096 Sep 13 21:43 pandas.egg-info\r\n-rw-r--r-- 1 root root 17415 Sep 12 22:25 pyproject.toml\r\n-rw-r--r-- 1 root root  4429 Sep 12 22:25 pyright_reportGeneralTypeIssues.json\r\n-rw-r--r-- 1 root root  1355 Sep 12 22:25 requirements-dev.txt\r\ndrwxr-xr-x 1 root root  4096 Sep 12 22:25 scripts\r\n-rw-r--r-- 1 root root   645 Sep 12 22:25 setup.cfg\r\n-rwxr-xr-x 1 root root 22455 Sep 12 22:25 setup.py\r\ndrwxr-xr-x 1 root root  4096 Sep 12 22:25 typings\r\ndrwxr-xr-x 1 root root  4096 Sep 12 22:25 web\r\n"]
[30.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[30.002, "i", "test -f asv.conf.json && cat asv.conf.json | sed -n '1,200p' || true\r"]
[30.004, "o", "test -f asv.conf.json && cat asv.conf.json | sed -n '1,200p' || true\r\n"]
[30.006, "o", "\u001b[?2004l\r\n"]
[35.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[35.002, "i", "rg -n \"GetItemMask\" -S || true\r"]
[35.004, "o", "rg -n \"GetItemMask\" -S || true\r\n"]
[35.006, "o", "\u001b[?2004l\r\n-bash: rg: command not found\r\n"]
[40.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[40.002, "i", "rg -n \"sparse\\.GetItemMask\" -S asv* benchmarks* bench* || true\r"]
[40.004, "o", "rg -n \"sparse\\.GetItemMask\" -S asv* benchmarks* bench* || true\r\n"]
[40.006, "o", "\u001b[?2004l\r\n-bash: rg: command not found\r\n"]
[45.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[45.002, "i", "test -f asv_benchmarks.txt && cat asv_benchmarks.txt | sed -n '1,200p' || true\r"]
[45.004, "o", "test -f asv_benchmarks.txt && cat asv_benchmarks.txt | sed -n '1,200p' || true\r\n"]
[45.006, "o", "\u001b[?2004l\r\n"]
[50.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[50.002, "i", "asv run --python=same --bench=\"sparse.GetItemMask.*\"\r"]
[50.004, "o", "asv run --python=same --bench=\"sparse.GetItemMask.*\"\r\n"]
[50.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;31mNo `asv.conf` file found for valid extensions: ['.json', '.jsonc'].\u001b[0m\r\n"]
[55.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[55.002, "i", "ls -la asv_bench\r"]
[55.004, "o", "ls -la asv_bench\r\n"]
[55.006, "o", "\u001b[?2004l\r\ntotal 24\r\ndrwxr-xr-x 1 root root 4096 Sep 12 22:25 \u001b[0m\u001b[01;34m.\u001b[0m\r\ndrwxr-xr-x 1 root root 4096 Oct  1 17:58 \u001b[01;34m..\u001b[0m\r\n-rw-r--r-- 1 root root 5217 Sep 12 22:25 asv.conf.json\r\ndrwxr-xr-x 1 root root 4096 Sep 12 22:25 \u001b[01;34mbenchmarks\u001b[0m\r\n"]
[60.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[60.002, "i", "find asv_bench -maxdepth 2 -type f -name \"asv.conf.json\" -print -exec sed -n '1,200p' {} \\;\r"]
[60.004, "o", "find asv_bench -maxdepth 2 -type f -name \"asv.conf.json\" -print -exec sed -n '1,200p' {} \\;\r\n"]
[60.006, "o", "\u001b[?2004l\r\nasv_bench/asv.conf.json\r\n{\r\n    // The version of the config file format.  Do not change, unless\r\n    // you know what you are doing.\r\n    \"version\": 1,\r\n\r\n    // The name of the project being benchmarked\r\n    \"project\": \"pandas\",\r\n\r\n    // The project's homepage\r\n    \"project_url\": \"https://pandas.pydata.org/\",\r\n\r\n    // The URL of the source code repository for the project being\r\n    // benchmarked\r\n    \"repo\": \"..\",\r\n\r\n    // List of branches to benchmark. If not provided, defaults to \"master\"\r\n    // (for git) or \"default\" (for mercurial).\r\n    \"branches\": [\"main\"],\r\n\r\n    // The tool to use to create environments.  May be \"conda\",\r\n    // \"virtualenv\" or other value depending on the plugins in use.\r\n    // If missing or the empty string, the tool will be automatically\r\n    // determined by looking for tools on the PATH environment\r\n    // variable.\r\n    \"environment_type\": \"conda\",\r\n\r\n    // the base URL to show a commit for the project.\r\n    \"show_commit_url\": \"https://github.com/pandas-dev/pandas/commit/\",\r\n\r\n    // The Pythons you'd like to test against.  If not provided, defaults\r\n    // to the current version of Python used to run `asv`.\r\n    \"pythons\": [\"3.8\"],\r\n\r\n    // The matrix of dependencies to test.  Each key is the name of a\r\n    // package (in PyPI) and the values are version numbers.  An empty\r\n    // list or empty string indicates to just test against the default\r\n    // (latest) version. null indicates that the package is to not be\r\n    // installed. If the package to be tested is only available from\r\n    // PyPi, and the 'environment_type' is conda, then you can preface\r\n    // the package name by 'pip+', and the package will be installed via\r\n    // pip (with all the conda available packages installed first,\r\n    // followed by the pip installed packages).\r\n    \"matrix\": {\r\n        \"numpy\": [],\r\n        \"Cython\": [\"0.29.33\"],\r\n        \"matplotlib\": [],\r\n        \"sqlalchemy\": [],\r\n        \"scipy\": [],\r\n        \"numba\": [],\r\n        \"numexpr\": [],\r\n        \"pytables\": [null, \"\"],  // platform dependent, see excludes below\r\n        \"pyarrow\": [],\r\n        \"tables\": [null, \"\"],\r\n        \"openpyxl\": [],\r\n        \"xlsxwriter\": [],\r\n        \"xlrd\": [],\r\n        \"odfpy\": [],\r\n        \"jinja2\": [],\r\n    },\r\n    \"conda_channels\": [\"conda-forge\"],\r\n    // Combinations of libraries/python versions can be excluded/included\r\n    // from the set to test. Each entry is a dictionary containing additional\r\n    // key-value pairs to include/exclude.\r\n    //\r\n    // An exclude entry excludes entries where all values match. The\r\n    // values are regexps that should match the whole string.\r\n    //\r\n    // An include entry adds an environment. Only the packages listed\r\n    // are installed. The 'python' key is required. The exclude rules\r\n    // do not apply to includes.\r\n    //\r\n    // In addition to package names, the following keys are available:\r\n    //\r\n    // - python\r\n    //     Python version, as in the *pythons* variable above.\r\n    // - environment_type\r\n    //     Environment type, as above.\r\n    // - sys_platform\r\n    //     Platform, as in sys.platform. Possible values for the common\r\n    //     cases: 'linux2', 'win32', 'cygwin', 'darwin'.\r\n    \"exclude\": [\r\n        // On conda install pytables, otherwise tables\r\n        {\"environment_type\": \"conda\", \"tables\": \"\"},\r\n        {\"environment_type\": \"conda\", \"pytables\": null},\r\n        {\"environment_type\": \"(?!conda).*\", \"tables\": null},\r\n        {\"environment_type\": \"(?!conda).*\", \"pytables\": \"\"},\r\n    ],\r\n    \"include\": [],\r\n\r\n    // The directory (relative to the current directory) that benchmarks are\r\n    // stored in.  If not provided, defaults to \"benchmarks\"\r\n    // \"benchmark_dir\": \"benchmarks\",\r\n\r\n    // The directory (relative to the current directory) to cache the Python\r\n    // environments in.  If not provided, defaults to \"env\"\r\n    // \"env_dir\": \"env\",\r\n\r\n    // The directory (relative to the current directory) that raw benchmark\r\n    // results are stored in.  If not provided, defaults to \"results\".\r\n    // \"results_dir\": \"results\",\r\n\r\n    // The directory (relative to the current directory) that the html tree\r\n    // should be written to.  If not provided, defaults to \"html\".\r\n    // \"html_dir\": \"html\",\r\n\r\n    // The number of characters to retain in the commit hashes.\r\n    // \"hash_length\": 8,\r\n\r\n    // `asv` will cache wheels of the recent builds in each\r\n    // environment, making them faster to install next time.  This is\r\n    // number of builds to keep, per environment.\r\n    \"build_cache_size\": 8,\r\n\r\n    // The commits after which the regression search in `asv publish`\r\n    // should start looking for regressions. Dictionary whose keys are\r\n    // regexps matching to benchmark names, and values corresponding to\r\n    // the commit (exclusive) after which to start looking for\r\n    // regressions.  The default is to start from the first commit\r\n    // with results. If the commit is `null`, regression detection is\r\n    // skipped for the matching benchmark.\r\n    //\r\n    \"regressions_first_commits\": {\r\n        \".*\": \"0409521665\"\r\n    },\r\n    \"regression_thresholds\": {\r\n    },\r\n    \"build_command\":\r\n    [\"python -m pip install versioneer[toml]\",\r\n     \"python setup.py build -j4\",\r\n     \"PIP_NO_BUILD_ISOLATION=false python -mpip wheel --no-deps --no-index -w {build_cache_dir} {build_dir}\"],\r\n}\r\n"]
[65.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[65.002, "i", "grep -RIn \"GetItemMask\" asv_bench || true\r"]
[65.004, "o", "grep -RIn \"GetItemMask\" asv_bench || true\r\n"]
[65.006, "o", "\u001b[?2004l\r\n\u001b[35m\u001b[Kasv_bench/benchmarks/sparse.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K207\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kclass \u001b[01;31m\u001b[KGetItemMask\u001b[m\u001b[K:\r\n"]
[70.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[70.002, "i", "grep -RIn \"sparse\" asv_bench/benchmarks | sed -n '1,200p'\r"]
[70.004, "o", "grep -RIn \"sparse\" asv_bench/benchmarks | sed -n '1,200p'\r\n"]
[70.006, "o", "\u001b[?2004l\r\nasv_bench/benchmarks/frame_ctor.py:167:        self.sparse_arrays = [\r\nasv_bench/benchmarks/frame_ctor.py:194:    def time_frame_from_arrays_sparse(self):\r\nasv_bench/benchmarks/frame_ctor.py:196:            self.sparse_arrays,\r\nasv_bench/benchmarks/sparse.py:2:import scipy.sparse\r\nasv_bench/benchmarks/sparse.py:45:    def time_sparse_array(self, dense_proportion, fill_value, dtype):\r\nasv_bench/benchmarks/sparse.py:52:        self.sparse = scipy.sparse.rand(N, N, 0.005)\r\nasv_bench/benchmarks/sparse.py:55:        pd.DataFrame.sparse.from_spmatrix(self.sparse)\r\nasv_bench/benchmarks/sparse.py:60:        self.matrix = scipy.sparse.coo_matrix(\r\nasv_bench/benchmarks/sparse.py:64:    def time_sparse_series_from_coo(self):\r\nasv_bench/benchmarks/sparse.py:65:        Series.sparse.from_coo(self.matrix)\r\nasv_bench/benchmarks/sparse.py:84:    def time_sparse_series_to_coo(self, sort_labels):\r\nasv_bench/benchmarks/sparse.py:85:        self.ss_mult_lvl.sparse.to_coo(\r\nasv_bench/benchmarks/sparse.py:89:    def time_sparse_series_to_coo_single_level(self, sort_labels):\r\nasv_bench/benchmarks/sparse.py:90:        self.ss_two_lvl.sparse.to_coo(sort_labels=sort_labels)\r\nasv_bench/benchmarks/sparse.py:104:        self.df.sparse.to_coo()\r\nasv_bench/benchmarks/reshape.py:252:        pd.get_dummies(self.s, sparse=False)\r\nasv_bench/benchmarks/reshape.py:254:    def time_get_dummies_1d_sparse(self):\r\nasv_bench/benchmarks/reshape.py:255:        pd.get_dummies(self.s, sparse=True)\r\n"]
[75.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[75.002, "i", "asv run --config asv_bench/asv.conf.json --python=same --bench=\"sparse.GetItemMask.*\"\r"]
[75.004, "o", "asv run --config asv_bench/asv.conf.json --python=same --bench=\"sparse.GetItemMask.*\"\r\n"]
[75.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;31mConfig file asv_bench/asv.conf.json not found.\u001b[0m\r\n"]
[80.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[80.002, "i", "sed -n '1,260p' asv_bench/benchmarks/sparse.py\r"]
[80.004, "o", "sed -n '1,260p' asv_bench/benchmarks/sparse.py\r\n"]
[80.006, "o", "\u001b[?2004l\r\nimport numpy as np\r\nimport scipy.sparse\r\n\r\nimport pandas as pd\r\nfrom pandas import (\r\n    MultiIndex,\r\n    Series,\r\n    date_range,\r\n)\r\nfrom pandas.arrays import SparseArray\r\n\r\n\r\ndef make_array(size, dense_proportion, fill_value, dtype):\r\n    dense_size = int(size * dense_proportion)\r\n    arr = np.full(size, fill_value, dtype)\r\n    indexer = np.random.choice(np.arange(size), dense_size, replace=False)\r\n    arr[indexer] = np.random.choice(np.arange(100, dtype=dtype), dense_size)\r\n    return arr\r\n\r\n\r\nclass SparseSeriesToFrame:\r\n    def setup(self):\r\n        K = 50\r\n        N = 50001\r\n        rng = date_range(\"1/1/2000\", periods=N, freq=\"T\")\r\n        self.series = {}\r\n        for i in range(1, K):\r\n            data = np.random.randn(N)[:-i]\r\n            idx = rng[:-i]\r\n            data[100:] = np.nan\r\n            self.series[i] = Series(SparseArray(data), index=idx)\r\n\r\n    def time_series_to_frame(self):\r\n        pd.DataFrame(self.series)\r\n\r\n\r\nclass SparseArrayConstructor:\r\n    params = ([0.1, 0.01], [0, np.nan], [np.int64, np.float64, object])\r\n    param_names = [\"dense_proportion\", \"fill_value\", \"dtype\"]\r\n\r\n    def setup(self, dense_proportion, fill_value, dtype):\r\n        N = 10**6\r\n        self.array = make_array(N, dense_proportion, fill_value, dtype)\r\n\r\n    def time_sparse_array(self, dense_proportion, fill_value, dtype):\r\n        SparseArray(self.array, fill_value=fill_value, dtype=dtype)\r\n\r\n\r\nclass SparseDataFrameConstructor:\r\n    def setup(self):\r\n        N = 1000\r\n        self.sparse = scipy.sparse.rand(N, N, 0.005)\r\n\r\n    def time_from_scipy(self):\r\n        pd.DataFrame.sparse.from_spmatrix(self.sparse)\r\n\r\n\r\nclass FromCoo:\r\n    def setup(self):\r\n        self.matrix = scipy.sparse.coo_matrix(\r\n            ([3.0, 1.0, 2.0], ([1, 0, 0], [0, 2, 3])), shape=(100, 100)\r\n        )\r\n\r\n    def time_sparse_series_from_coo(self):\r\n        Series.sparse.from_coo(self.matrix)\r\n\r\n\r\nclass ToCoo:\r\n    params = [True, False]\r\n    param_names = [\"sort_labels\"]\r\n\r\n    def setup(self, sort_labels):\r\n        s = Series([np.nan] * 10000)\r\n        s[0] = 3.0\r\n        s[100] = -1.0\r\n        s[999] = 12.1\r\n\r\n        s_mult_lvl = s.set_axis(MultiIndex.from_product([range(10)] * 4))\r\n        self.ss_mult_lvl = s_mult_lvl.astype(\"Sparse\")\r\n\r\n        s_two_lvl = s.set_axis(MultiIndex.from_product([range(100)] * 2))\r\n        self.ss_two_lvl = s_two_lvl.astype(\"Sparse\")\r\n\r\n    def time_sparse_series_to_coo(self, sort_labels):\r\n        self.ss_mult_lvl.sparse.to_coo(\r\n            row_levels=[0, 1], column_levels=[2, 3], sort_labels=sort_labels\r\n        )\r\n\r\n    def time_sparse_series_to_coo_single_level(self, sort_labels):\r\n        self.ss_two_lvl.sparse.to_coo(sort_labels=sort_labels)\r\n\r\n\r\nclass ToCooFrame:\r\n    def setup(self):\r\n        N = 10000\r\n        k = 10\r\n        arr = np.zeros((N, k), dtype=float)\r\n        arr[0, 0] = 3.0\r\n        arr[12, 7] = -1.0\r\n        arr[0, 9] = 11.2\r\n        self.df = pd.DataFrame(arr, dtype=pd.SparseDtype(\"float\", fill_value=0.0))\r\n\r\n    def time_to_coo(self):\r\n        self.df.sparse.to_coo()\r\n\r\n\r\nclass Arithmetic:\r\n    params = ([0.1, 0.01], [0, np.nan])\r\n    param_names = [\"dense_proportion\", \"fill_value\"]\r\n\r\n    def setup(self, dense_proportion, fill_value):\r\n        N = 10**6\r\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\r\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\r\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\r\n        self.array2 = SparseArray(arr2, fill_value=fill_value)\r\n\r\n    def time_make_union(self, dense_proportion, fill_value):\r\n        self.array1.sp_index.make_union(self.array2.sp_index)\r\n\r\n    def time_intersect(self, dense_proportion, fill_value):\r\n        self.array1.sp_index.intersect(self.array2.sp_index)\r\n\r\n    def time_add(self, dense_proportion, fill_value):\r\n        self.array1 + self.array2\r\n\r\n    def time_divide(self, dense_proportion, fill_value):\r\n        self.array1 / self.array2\r\n\r\n\r\nclass ArithmeticBlock:\r\n    params = [np.nan, 0]\r\n    param_names = [\"fill_value\"]\r\n\r\n    def setup(self, fill_value):\r\n        N = 10**6\r\n        self.arr1 = self.make_block_array(\r\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\r\n        )\r\n        self.arr2 = self.make_block_array(\r\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\r\n        )\r\n\r\n    def make_block_array(self, length, num_blocks, block_size, fill_value):\r\n        arr = np.full(length, fill_value)\r\n        indices = np.random.choice(\r\n            np.arange(0, length, block_size), num_blocks, replace=False\r\n        )\r\n        for ind in indices:\r\n            arr[ind : ind + block_size] = np.random.randint(0, 100, block_size)\r\n        return SparseArray(arr, fill_value=fill_value)\r\n\r\n    def time_make_union(self, fill_value):\r\n        self.arr1.sp_index.make_union(self.arr2.sp_index)\r\n\r\n    def time_intersect(self, fill_value):\r\n        self.arr2.sp_index.intersect(self.arr2.sp_index)\r\n\r\n    def time_addition(self, fill_value):\r\n        self.arr1 + self.arr2\r\n\r\n    def time_division(self, fill_value):\r\n        self.arr1 / self.arr2\r\n\r\n\r\nclass MinMax:\r\n    params = ([\"min\", \"max\"], [0.0, np.nan])\r\n    param_names = [\"func\", \"fill_value\"]\r\n\r\n    def setup(self, func, fill_value):\r\n        N = 1_000_000\r\n        arr = make_array(N, 1e-5, fill_value, np.float64)\r\n        self.sp_arr = SparseArray(arr, fill_value=fill_value)\r\n\r\n    def time_min_max(self, func, fill_value):\r\n        getattr(self.sp_arr, func)()\r\n\r\n\r\nclass Take:\r\n    params = ([np.array([0]), np.arange(100_000), np.full(100_000, -1)], [True, False])\r\n    param_names = [\"indices\", \"allow_fill\"]\r\n\r\n    def setup(self, indices, allow_fill):\r\n        N = 1_000_000\r\n        fill_value = 0.0\r\n        arr = make_array(N, 1e-5, fill_value, np.float64)\r\n        self.sp_arr = SparseArray(arr, fill_value=fill_value)\r\n\r\n    def time_take(self, indices, allow_fill):\r\n        self.sp_arr.take(indices, allow_fill=allow_fill)\r\n\r\n\r\nclass GetItem:\r\n    def setup(self):\r\n        N = 1_000_000\r\n        d = 1e-5\r\n        arr = make_array(N, d, np.nan, np.float64)\r\n        self.sp_arr = SparseArray(arr)\r\n\r\n    def time_integer_indexing(self):\r\n        self.sp_arr[78]\r\n\r\n    def time_slice(self):\r\n        self.sp_arr[1:]\r\n\r\n\r\nclass GetItemMask:\r\n    params = [True, False, np.nan]\r\n    param_names = [\"fill_value\"]\r\n\r\n    def setup(self, fill_value):\r\n        N = 1_000_000\r\n        d = 1e-5\r\n        arr = make_array(N, d, np.nan, np.float64)\r\n        self.sp_arr = SparseArray(arr)\r\n        b_arr = np.full(shape=N, fill_value=fill_value, dtype=np.bool_)\r\n        fv_inds = np.unique(\r\n            np.random.randint(low=0, high=N - 1, size=int(N * d), dtype=np.int32)\r\n        )\r\n        b_arr[fv_inds] = True if pd.isna(fill_value) else not fill_value\r\n        self.sp_b_arr = SparseArray(b_arr, dtype=np.bool_, fill_value=fill_value)\r\n\r\n    def time_mask(self, fill_value):\r\n        self.sp_arr[self.sp_b_arr]\r\n\r\n\r\nfrom .pandas_vb_common import setup  # noqa: F401 isort:skip\r\n"]
[85.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[85.002, "i", "cd asv_bench\r"]
[85.004, "o", "cd asv_bench\r\n"]
[85.006, "o", "\u001b[?2004l\r\n"]
[90.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[90.002, "i", "asv run --python=same --bench=\"sparse.GetItemMask.*\"\r"]
[90.004, "o", "asv run --python=same --bench=\"sparse.GetItemMask.*\"\r\n"]
[90.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.8_bin_python\u001b[0m\r\n[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39mRunning (sparse.GetItemMask.time_mask--)\u001b[0m.\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39msparse.GetItemMask.time_mask\u001b[0m                                                                                                                    ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m============ ============\u001b[0m\r\n               fill_value              \r\n              ------------ ------------\r\n                  True      12.1\u00b10.3ms \r\n                 False      48.6\u00b10.4\u03bcs \r\n                  nan       7.03\u00b10.1ms \r\n              ============ ============\r\n\r\n"]
[95.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[95.002, "i", "asv profile --python=same sparse.GetItemMask.time_mask\r"]
[95.004, "o", "asv profile --python=same sparse.GetItemMask.time_mask\r\n"]
[95.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;31m'sparse.GetItemMask.time_mask' benchmark not found\u001b[0m\r\n"]
[100.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[100.002, "i", "cd ..\r"]
[100.004, "o", "cd ..\r\n"]
[100.006, "o", "\u001b[?2004l\r\n"]
[105.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[105.002, "i", "grep -RIn \"class SparseArray\" pandas | sed -n '1,200p'\r"]
[105.004, "o", "grep -RIn \"class SparseArray\" pandas | sed -n '1,200p'\r\n"]
[105.006, "o", "\u001b[?2004l\r\npandas/core/arrays/sparse/array.py:288:class SparseArray(OpsMixin, PandasObject, ExtensionArray):\r\n"]
[110.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[110.002, "i", "grep -RIn \"__getitem__\" pandas/core/arrays | sed -n '1,200p'\r"]
[110.004, "o", "grep -RIn \"__getitem__\" pandas/core/arrays | sed -n '1,200p'\r\n"]
[110.006, "o", "\u001b[?2004l\r\npandas/core/arrays/_mixins.py:259:    def __getitem__(self, key: ScalarIndexer) -> Any:\r\npandas/core/arrays/_mixins.py:263:    def __getitem__(\r\npandas/core/arrays/_mixins.py:269:    def __getitem__(\r\npandas/core/arrays/sparse/array.py:895:    def __getitem__(self, key: ScalarIndexer) -> Any:\r\npandas/core/arrays/sparse/array.py:899:    def __getitem__(\r\npandas/core/arrays/sparse/array.py:905:    def __getitem__(\r\npandas/core/arrays/arrow/array.py:331:    def __getitem__(self, item: PositionalIndexer):\r\npandas/core/arrays/arrow/array.py:981:        ExtensionArray.take is called by ``Series.__getitem__``, ``.loc``,\r\npandas/core/arrays/masked.py:144:    def __getitem__(self, item: ScalarIndexer) -> Any:\r\npandas/core/arrays/masked.py:148:    def __getitem__(self, item: SequenceIndexer) -> Self:\r\npandas/core/arrays/masked.py:151:    def __getitem__(self, item: PositionalIndexer) -> Self | Any:\r\npandas/core/arrays/base.py:152:    * __getitem__\r\npandas/core/arrays/base.py:315:    def __getitem__(self, item: ScalarIndexer) -> Any:\r\npandas/core/arrays/base.py:319:    def __getitem__(self, item: SequenceIndexer) -> Self:\r\npandas/core/arrays/base.py:322:    def __getitem__(self, item: PositionalIndexer) -> Self | Any:\r\npandas/core/arrays/base.py:416:        # calls to ``__getitem__``, which may be slower than necessary.\r\npandas/core/arrays/base.py:1176:        ExtensionArray.take is called by ``Series.__getitem__``, ``.loc``,\r\npandas/core/arrays/interval.py:702:    def __getitem__(self, key: ScalarIndexer) -> IntervalOrNA:\r\npandas/core/arrays/interval.py:706:    def __getitem__(self, key: SequenceIndexer) -> Self:\r\npandas/core/arrays/interval.py:709:    def __getitem__(self, key: PositionalIndexer) -> Self | IntervalOrNA:\r\npandas/core/arrays/interval.py:1742:        # No overload variant of \"__getitem__\" of \"ExtensionArray\" matches argument\r\npandas/core/arrays/datetimelike.py:352:    def __getitem__(self, item: ScalarIndexer) -> DTScalarOrNaT:\r\npandas/core/arrays/datetimelike.py:356:    def __getitem__(\r\npandas/core/arrays/datetimelike.py:362:    def __getitem__(self, key: PositionalIndexer2D) -> Self | DTScalarOrNaT:\r\npandas/core/arrays/datetimelike.py:370:        result = cast(\"Union[Self, DTScalarOrNaT]\", super().__getitem__(key))\r\npandas/core/arrays/datetimelike.py:381:        Find the `freq` attribute to assign to the result of a __getitem__ lookup.\r\n"]
[115.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[115.002, "i", "grep -RIn \"boolean\" pandas/core/arrays/sparse | sed -n '1,200p'\r"]
[115.004, "o", "grep -RIn \"boolean\" pandas/core/arrays/sparse | sed -n '1,200p'\r\n"]
[115.006, "o", "\u001b[?2004l\r\npandas/core/arrays/sparse/accessor.py:321:            If the caller is heterogeneous and contains booleans or objects,\r\npandas/core/arrays/sparse/array.py:965:                r\"(`None`) and integer or boolean arrays are valid indices\"\r\npandas/core/arrays/sparse/dtype.py:164:        # TODO: Right now we can use Sparse boolean array\r\npandas/core/arrays/sparse/dtype.py:169:        # if self._is_boolean and not (\r\npandas/core/arrays/sparse/dtype.py:174:        #        f\"for boolean type. Got {self._fill_value} instead\"\r\npandas/core/arrays/sparse/dtype.py:186:    def _is_boolean(self) -> bool:\r\n"]
[120.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[120.002, "i", "sed -n '1,240p' pandas/core/arrays/sparse/array.py\r"]
[120.004, "o", "sed -n '1,240p' pandas/core/arrays/sparse/array.py\r\n"]
[120.006, "o", "\u001b[?2004l\r\n\"\"\"\r\nSparseArray data structure\r\n\"\"\"\r\nfrom __future__ import annotations\r\n\r\nfrom collections import abc\r\nimport numbers\r\nimport operator\r\nfrom typing import (\r\n    TYPE_CHECKING,\r\n    Any,\r\n    Callable,\r\n    Literal,\r\n    Sequence,\r\n    cast,\r\n    overload,\r\n)\r\nimport warnings\r\n\r\nimport numpy as np\r\n\r\nfrom pandas._libs import lib\r\nimport pandas._libs.sparse as splib\r\nfrom pandas._libs.sparse import (\r\n    BlockIndex,\r\n    IntIndex,\r\n    SparseIndex,\r\n)\r\nfrom pandas._libs.tslibs import NaT\r\nfrom pandas.compat.numpy import function as nv\r\nfrom pandas.errors import PerformanceWarning\r\nfrom pandas.util._exceptions import find_stack_level\r\nfrom pandas.util._validators import (\r\n    validate_bool_kwarg,\r\n    validate_insert_loc,\r\n)\r\n\r\nfrom pandas.core.dtypes.astype import astype_array\r\nfrom pandas.core.dtypes.cast import (\r\n    construct_1d_arraylike_from_scalar,\r\n    find_common_type,\r\n    maybe_box_datetimelike,\r\n)\r\nfrom pandas.core.dtypes.common import (\r\n    is_array_like,\r\n    is_bool_dtype,\r\n    is_datetime64_any_dtype,\r\n    is_dtype_equal,\r\n    is_integer,\r\n    is_list_like,\r\n    is_object_dtype,\r\n    is_scalar,\r\n    is_string_dtype,\r\n    pandas_dtype,\r\n)\r\nfrom pandas.core.dtypes.dtypes import DatetimeTZDtype\r\nfrom pandas.core.dtypes.generic import (\r\n    ABCIndex,\r\n    ABCSeries,\r\n)\r\nfrom pandas.core.dtypes.missing import (\r\n    isna,\r\n    na_value_for_dtype,\r\n    notna,\r\n)\r\n\r\nfrom pandas.core import arraylike\r\nimport pandas.core.algorithms as algos\r\nfrom pandas.core.arraylike import OpsMixin\r\nfrom pandas.core.arrays import ExtensionArray\r\nfrom pandas.core.arrays.sparse.dtype import SparseDtype\r\nfrom pandas.core.base import PandasObject\r\nimport pandas.core.common as com\r\nfrom pandas.core.construction import (\r\n    ensure_wrapped_if_datetimelike,\r\n    extract_array,\r\n    sanitize_array,\r\n)\r\nfrom pandas.core.indexers import (\r\n    check_array_indexer,\r\n    unpack_tuple_and_ellipses,\r\n)\r\nfrom pandas.core.missing import interpolate_2d\r\nfrom pandas.core.nanops import check_below_min_count\r\n\r\nfrom pandas.io.formats import printing\r\n\r\n# See https://github.com/python/typing/issues/684\r\nif TYPE_CHECKING:\r\n    from enum import Enum\r\n\r\n    class ellipsis(Enum):\r\n        Ellipsis = \"...\"\r\n\r\n    Ellipsis = ellipsis.Ellipsis\r\n\r\n    from scipy.sparse import spmatrix\r\n\r\n    from pandas._typing import (\r\n        FillnaOptions,\r\n        NumpySorter,\r\n    )\r\n\r\n    SparseIndexKind = Literal[\"integer\", \"block\"]\r\n\r\n    from pandas._typing import (\r\n        ArrayLike,\r\n        AstypeArg,\r\n        Axis,\r\n        AxisInt,\r\n        Dtype,\r\n        NpDtype,\r\n        PositionalIndexer,\r\n        Scalar,\r\n        ScalarIndexer,\r\n        Self,\r\n        SequenceIndexer,\r\n        npt,\r\n    )\r\n\r\n    from pandas import Series\r\n\r\nelse:\r\n    ellipsis = type(Ellipsis)\r\n\r\n\r\n# ----------------------------------------------------------------------------\r\n# Array\r\n\r\n_sparray_doc_kwargs = {\"klass\": \"SparseArray\"}\r\n\r\n\r\ndef _get_fill(arr: SparseArray) -> np.ndarray:\r\n    \"\"\"\r\n    Create a 0-dim ndarray containing the fill value\r\n\r\n    Parameters\r\n    ----------\r\n    arr : SparseArray\r\n\r\n    Returns\r\n    -------\r\n    fill_value : ndarray\r\n        0-dim ndarray with just the fill value.\r\n\r\n    Notes\r\n    -----\r\n    coerce fill_value to arr dtype if possible\r\n    int64 SparseArray can have NaN as fill_value if there is no missing\r\n    \"\"\"\r\n    try:\r\n        return np.asarray(arr.fill_value, dtype=arr.dtype.subtype)\r\n    except ValueError:\r\n        return np.asarray(arr.fill_value)\r\n\r\n\r\ndef _sparse_array_op(\r\n    left: SparseArray, right: SparseArray, op: Callable, name: str\r\n) -> SparseArray:\r\n    \"\"\"\r\n    Perform a binary operation between two arrays.\r\n\r\n    Parameters\r\n    ----------\r\n    left : Union[SparseArray, ndarray]\r\n    right : Union[SparseArray, ndarray]\r\n    op : Callable\r\n        The binary operation to perform\r\n    name str\r\n        Name of the callable.\r\n\r\n    Returns\r\n    -------\r\n    SparseArray\r\n    \"\"\"\r\n    if name.startswith(\"__\"):\r\n        # For lookups in _libs.sparse we need non-dunder op name\r\n        name = name[2:-2]\r\n\r\n    # dtype used to find corresponding sparse method\r\n    ltype = left.dtype.subtype\r\n    rtype = right.dtype.subtype\r\n\r\n    if not is_dtype_equal(ltype, rtype):\r\n        subtype = find_common_type([ltype, rtype])\r\n        ltype = SparseDtype(subtype, left.fill_value)\r\n        rtype = SparseDtype(subtype, right.fill_value)\r\n\r\n        left = left.astype(ltype, copy=False)\r\n        right = right.astype(rtype, copy=False)\r\n        dtype = ltype.subtype\r\n    else:\r\n        dtype = ltype\r\n\r\n    # dtype the result must have\r\n    result_dtype = None\r\n\r\n    if left.sp_index.ngaps == 0 or right.sp_index.ngaps == 0:\r\n        with np.errstate(all=\"ignore\"):\r\n            result = op(left.to_dense(), right.to_dense())\r\n            fill = op(_get_fill(left), _get_fill(right))\r\n\r\n        if left.sp_index.ngaps == 0:\r\n            index = left.sp_index\r\n        else:\r\n            index = right.sp_index\r\n    elif left.sp_index.equals(right.sp_index):\r\n        with np.errstate(all=\"ignore\"):\r\n            result = op(left.sp_values, right.sp_values)\r\n            fill = op(_get_fill(left), _get_fill(right))\r\n        index = left.sp_index\r\n    else:\r\n        if name[0] == \"r\":\r\n            left, right = right, left\r\n            name = name[1:]\r\n\r\n        if name in (\"and\", \"or\", \"xor\") and dtype == \"bool\":\r\n            opname = f\"sparse_{name}_uint8\"\r\n            # to make template simple, cast here\r\n            left_sp_values = left.sp_values.view(np.uint8)\r\n            right_sp_values = right.sp_values.view(np.uint8)\r\n            result_dtype = bool\r\n        else:\r\n            opname = f\"sparse_{name}_{dtype}\"\r\n            left_sp_values = left.sp_values\r\n            right_sp_values = right.sp_values\r\n\r\n        if (\r\n            name in [\"floordiv\", \"mod\"]\r\n            and (right == 0).any()\r\n            and left.dtype.kind in \"iu\"\r\n        ):\r\n            # Match the non-Sparse Series behavior\r\n            opname = f\"sparse_{name}_float64\"\r\n            left_sp_values = left_sp_values.astype(\"float64\")\r\n            right_sp_values = right_sp_values.astype(\"float64\")\r\n\r\n        sparse_op = getattr(splib, opname)\r\n\r\n        with np.errstate(all=\"ignore\"):\r\n"]
[125.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[125.002, "i", "sed -n '240,1160p' pandas/core/arrays/sparse/array.py\r"]
[125.004, "o", "sed -n '240,1160p' pandas/core/arrays/sparse/array.py\r\n"]
[125.006, "o", "\u001b[?2004l\r\n        with np.errstate(all=\"ignore\"):\r\n            result, index, fill = sparse_op(\r\n                left_sp_values,\r\n                left.sp_index,\r\n                left.fill_value,\r\n                right_sp_values,\r\n                right.sp_index,\r\n                right.fill_value,\r\n            )\r\n\r\n    if name == \"divmod\":\r\n        # result is a 2-tuple\r\n        # error: Incompatible return value type (got \"Tuple[SparseArray,\r\n        # SparseArray]\", expected \"SparseArray\")\r\n        return (  # type: ignore[return-value]\r\n            _wrap_result(name, result[0], index, fill[0], dtype=result_dtype),\r\n            _wrap_result(name, result[1], index, fill[1], dtype=result_dtype),\r\n        )\r\n\r\n    if result_dtype is None:\r\n        result_dtype = result.dtype\r\n\r\n    return _wrap_result(name, result, index, fill, dtype=result_dtype)\r\n\r\n\r\ndef _wrap_result(\r\n    name: str, data, sparse_index, fill_value, dtype: Dtype | None = None\r\n) -> SparseArray:\r\n    \"\"\"\r\n    wrap op result to have correct dtype\r\n    \"\"\"\r\n    if name.startswith(\"__\"):\r\n        # e.g. __eq__ --> eq\r\n        name = name[2:-2]\r\n\r\n    if name in (\"eq\", \"ne\", \"lt\", \"gt\", \"le\", \"ge\"):\r\n        dtype = bool\r\n\r\n    fill_value = lib.item_from_zerodim(fill_value)\r\n\r\n    if is_bool_dtype(dtype):\r\n        # fill_value may be np.bool_\r\n        fill_value = bool(fill_value)\r\n    return SparseArray(\r\n        data, sparse_index=sparse_index, fill_value=fill_value, dtype=dtype\r\n    )\r\n\r\n\r\nclass SparseArray(OpsMixin, PandasObject, ExtensionArray):\r\n    \"\"\"\r\n    An ExtensionArray for storing sparse data.\r\n\r\n    Parameters\r\n    ----------\r\n    data : array-like or scalar\r\n        A dense array of values to store in the SparseArray. This may contain\r\n        `fill_value`.\r\n    sparse_index : SparseIndex, optional\r\n    fill_value : scalar, optional\r\n        Elements in data that are ``fill_value`` are not stored in the\r\n        SparseArray. For memory savings, this should be the most common value\r\n        in `data`. By default, `fill_value` depends on the dtype of `data`:\r\n\r\n        =========== ==========\r\n        data.dtype  na_value\r\n        =========== ==========\r\n        float       ``np.nan``\r\n        int         ``0``\r\n        bool        False\r\n        datetime64  ``pd.NaT``\r\n        timedelta64 ``pd.NaT``\r\n        =========== ==========\r\n\r\n        The fill value is potentially specified in three ways. In order of\r\n        precedence, these are\r\n\r\n        1. The `fill_value` argument\r\n        2. ``dtype.fill_value`` if `fill_value` is None and `dtype` is\r\n           a ``SparseDtype``\r\n        3. ``data.dtype.fill_value`` if `fill_value` is None and `dtype`\r\n           is not a ``SparseDtype`` and `data` is a ``SparseArray``.\r\n\r\n    kind : str\r\n        Can be 'integer' or 'block', default is 'integer'.\r\n        The type of storage for sparse locations.\r\n\r\n        * 'block': Stores a `block` and `block_length` for each\r\n          contiguous *span* of sparse values. This is best when\r\n          sparse data tends to be clumped together, with large\r\n          regions of ``fill-value`` values between sparse values.\r\n        * 'integer': uses an integer to store the location of\r\n          each sparse value.\r\n\r\n    dtype : np.dtype or SparseDtype, optional\r\n        The dtype to use for the SparseArray. For numpy dtypes, this\r\n        determines the dtype of ``self.sp_values``. For SparseDtype,\r\n        this determines ``self.sp_values`` and ``self.fill_value``.\r\n    copy : bool, default False\r\n        Whether to explicitly copy the incoming `data` array.\r\n\r\n    Attributes\r\n    ----------\r\n    None\r\n\r\n    Methods\r\n    -------\r\n    None\r\n\r\n    Examples\r\n    --------\r\n    >>> from pandas.arrays import SparseArray\r\n    >>> arr = SparseArray([0, 0, 1, 2])\r\n    >>> arr\r\n    [0, 0, 1, 2]\r\n    Fill: 0\r\n    IntIndex\r\n    Indices: array([2, 3], dtype=int32)\r\n    \"\"\"\r\n\r\n    _subtyp = \"sparse_array\"  # register ABCSparseArray\r\n    _hidden_attrs = PandasObject._hidden_attrs | frozenset([])\r\n    _sparse_index: SparseIndex\r\n    _sparse_values: np.ndarray\r\n    _dtype: SparseDtype\r\n\r\n    def __init__(\r\n        self,\r\n        data,\r\n        sparse_index=None,\r\n        fill_value=None,\r\n        kind: SparseIndexKind = \"integer\",\r\n        dtype: Dtype | None = None,\r\n        copy: bool = False,\r\n    ) -> None:\r\n        if fill_value is None and isinstance(dtype, SparseDtype):\r\n            fill_value = dtype.fill_value\r\n\r\n        if isinstance(data, type(self)):\r\n            # disable normal inference on dtype, sparse_index, & fill_value\r\n            if sparse_index is None:\r\n                sparse_index = data.sp_index\r\n            if fill_value is None:\r\n                fill_value = data.fill_value\r\n            if dtype is None:\r\n                dtype = data.dtype\r\n            # TODO: make kind=None, and use data.kind?\r\n            data = data.sp_values\r\n\r\n        # Handle use-provided dtype\r\n        if isinstance(dtype, str):\r\n            # Two options: dtype='int', regular numpy dtype\r\n            # or dtype='Sparse[int]', a sparse dtype\r\n            try:\r\n                dtype = SparseDtype.construct_from_string(dtype)\r\n            except TypeError:\r\n                dtype = pandas_dtype(dtype)\r\n\r\n        if isinstance(dtype, SparseDtype):\r\n            if fill_value is None:\r\n                fill_value = dtype.fill_value\r\n            dtype = dtype.subtype\r\n\r\n        if is_scalar(data):\r\n            if sparse_index is None:\r\n                npoints = 1\r\n            else:\r\n                npoints = sparse_index.length\r\n\r\n            data = construct_1d_arraylike_from_scalar(data, npoints, dtype=None)\r\n            dtype = data.dtype\r\n\r\n        if dtype is not None:\r\n            dtype = pandas_dtype(dtype)\r\n\r\n        # TODO: disentangle the fill_value dtype inference from\r\n        # dtype inference\r\n        if data is None:\r\n            # TODO: What should the empty dtype be? Object or float?\r\n\r\n            # error: Argument \"dtype\" to \"array\" has incompatible type\r\n            # \"Union[ExtensionDtype, dtype[Any], None]\"; expected \"Union[dtype[Any],\r\n            # None, type, _SupportsDType, str, Union[Tuple[Any, int], Tuple[Any,\r\n            # Union[int, Sequence[int]]], List[Any], _DTypeDict, Tuple[Any, Any]]]\"\r\n            data = np.array([], dtype=dtype)  # type: ignore[arg-type]\r\n\r\n        if not is_array_like(data):\r\n            try:\r\n                # probably shared code in sanitize_series\r\n\r\n                data = sanitize_array(data, index=None)\r\n            except ValueError:\r\n                # NumPy may raise a ValueError on data like [1, []]\r\n                # we retry with object dtype here.\r\n                if dtype is None:\r\n                    dtype = np.dtype(object)\r\n                    data = np.atleast_1d(np.asarray(data, dtype=dtype))\r\n                else:\r\n                    raise\r\n\r\n        if copy:\r\n            # TODO: avoid double copy when dtype forces cast.\r\n            data = data.copy()\r\n\r\n        if fill_value is None:\r\n            fill_value_dtype = data.dtype if dtype is None else dtype\r\n            if fill_value_dtype is None:\r\n                fill_value = np.nan\r\n            else:\r\n                fill_value = na_value_for_dtype(fill_value_dtype)\r\n\r\n        if isinstance(data, type(self)) and sparse_index is None:\r\n            sparse_index = data._sparse_index\r\n            # error: Argument \"dtype\" to \"asarray\" has incompatible type\r\n            # \"Union[ExtensionDtype, dtype[Any], None]\"; expected \"None\"\r\n            sparse_values = np.asarray(\r\n                data.sp_values, dtype=dtype  # type: ignore[arg-type]\r\n            )\r\n        elif sparse_index is None:\r\n            data = extract_array(data, extract_numpy=True)\r\n            if not isinstance(data, np.ndarray):\r\n                # EA\r\n                if isinstance(data.dtype, DatetimeTZDtype):\r\n                    warnings.warn(\r\n                        f\"Creating SparseArray from {data.dtype} data \"\r\n                        \"loses timezone information. Cast to object before \"\r\n                        \"sparse to retain timezone information.\",\r\n                        UserWarning,\r\n                        stacklevel=find_stack_level(),\r\n                    )\r\n                    data = np.asarray(data, dtype=\"datetime64[ns]\")\r\n                    if fill_value is NaT:\r\n                        fill_value = np.datetime64(\"NaT\", \"ns\")\r\n                data = np.asarray(data)\r\n            sparse_values, sparse_index, fill_value = _make_sparse(\r\n                # error: Argument \"dtype\" to \"_make_sparse\" has incompatible type\r\n                # \"Union[ExtensionDtype, dtype[Any], None]\"; expected\r\n                # \"Optional[dtype[Any]]\"\r\n                data,\r\n                kind=kind,\r\n                fill_value=fill_value,\r\n                dtype=dtype,  # type: ignore[arg-type]\r\n            )\r\n        else:\r\n            # error: Argument \"dtype\" to \"asarray\" has incompatible type\r\n            # \"Union[ExtensionDtype, dtype[Any], None]\"; expected \"None\"\r\n            sparse_values = np.asarray(data, dtype=dtype)  # type: ignore[arg-type]\r\n            if len(sparse_values) != sparse_index.npoints:\r\n                raise AssertionError(\r\n                    f\"Non array-like type {type(sparse_values)} must \"\r\n                    \"have the same length as the index\"\r\n                )\r\n        self._sparse_index = sparse_index\r\n        self._sparse_values = sparse_values\r\n        self._dtype = SparseDtype(sparse_values.dtype, fill_value)\r\n\r\n    @classmethod\r\n    def _simple_new(\r\n        cls,\r\n        sparse_array: np.ndarray,\r\n        sparse_index: SparseIndex,\r\n        dtype: SparseDtype,\r\n    ) -> Self:\r\n        new = object.__new__(cls)\r\n        new._sparse_index = sparse_index\r\n        new._sparse_values = sparse_array\r\n        new._dtype = dtype\r\n        return new\r\n\r\n    @classmethod\r\n    def from_spmatrix(cls, data: spmatrix) -> Self:\r\n        \"\"\"\r\n        Create a SparseArray from a scipy.sparse matrix.\r\n\r\n        Parameters\r\n        ----------\r\n        data : scipy.sparse.sp_matrix\r\n            This should be a SciPy sparse matrix where the size\r\n            of the second dimension is 1. In other words, a\r\n            sparse matrix with a single column.\r\n\r\n        Returns\r\n        -------\r\n        SparseArray\r\n\r\n        Examples\r\n        --------\r\n        >>> import scipy.sparse\r\n        >>> mat = scipy.sparse.coo_matrix((4, 1))\r\n        >>> pd.arrays.SparseArray.from_spmatrix(mat)\r\n        [0.0, 0.0, 0.0, 0.0]\r\n        Fill: 0.0\r\n        IntIndex\r\n        Indices: array([], dtype=int32)\r\n        \"\"\"\r\n        length, ncol = data.shape\r\n\r\n        if ncol != 1:\r\n            raise ValueError(f\"'data' must have a single column, not '{ncol}'\")\r\n\r\n        # our sparse index classes require that the positions be strictly\r\n        # increasing. So we need to sort loc, and arr accordingly.\r\n        data = data.tocsc()\r\n        data.sort_indices()\r\n        arr = data.data\r\n        idx = data.indices\r\n\r\n        zero = np.array(0, dtype=arr.dtype).item()\r\n        dtype = SparseDtype(arr.dtype, zero)\r\n        index = IntIndex(length, idx)\r\n\r\n        return cls._simple_new(arr, index, dtype)\r\n\r\n    def __array__(self, dtype: NpDtype | None = None) -> np.ndarray:\r\n        fill_value = self.fill_value\r\n\r\n        if self.sp_index.ngaps == 0:\r\n            # Compat for na dtype and int values.\r\n            return self.sp_values\r\n        if dtype is None:\r\n            # Can NumPy represent this type?\r\n            # If not, `np.result_type` will raise. We catch that\r\n            # and return object.\r\n            if is_datetime64_any_dtype(self.sp_values.dtype):\r\n                # However, we *do* special-case the common case of\r\n                # a datetime64 with pandas NaT.\r\n                if fill_value is NaT:\r\n                    # Can't put pd.NaT in a datetime64[ns]\r\n                    fill_value = np.datetime64(\"NaT\")\r\n            try:\r\n                dtype = np.result_type(self.sp_values.dtype, type(fill_value))\r\n            except TypeError:\r\n                dtype = object\r\n\r\n        out = np.full(self.shape, fill_value, dtype=dtype)\r\n        out[self.sp_index.indices] = self.sp_values\r\n        return out\r\n\r\n    def __setitem__(self, key, value):\r\n        # I suppose we could allow setting of non-fill_value elements.\r\n        # TODO(SparseArray.__setitem__): remove special cases in\r\n        # ExtensionBlock.where\r\n        msg = \"SparseArray does not support item assignment via setitem\"\r\n        raise TypeError(msg)\r\n\r\n    @classmethod\r\n    def _from_sequence(cls, scalars, *, dtype: Dtype | None = None, copy: bool = False):\r\n        return cls(scalars, dtype=dtype)\r\n\r\n    @classmethod\r\n    def _from_factorized(cls, values, original):\r\n        return cls(values, dtype=original.dtype)\r\n\r\n    # ------------------------------------------------------------------------\r\n    # Data\r\n    # ------------------------------------------------------------------------\r\n    @property\r\n    def sp_index(self) -> SparseIndex:\r\n        \"\"\"\r\n        The SparseIndex containing the location of non- ``fill_value`` points.\r\n        \"\"\"\r\n        return self._sparse_index\r\n\r\n    @property\r\n    def sp_values(self) -> np.ndarray:\r\n        \"\"\"\r\n        An ndarray containing the non- ``fill_value`` values.\r\n\r\n        Examples\r\n        --------\r\n        >>> from pandas.arrays import SparseArray\r\n        >>> s = SparseArray([0, 0, 1, 0, 2], fill_value=0)\r\n        >>> s.sp_values\r\n        array([1, 2])\r\n        \"\"\"\r\n        return self._sparse_values\r\n\r\n    @property\r\n    def dtype(self) -> SparseDtype:\r\n        return self._dtype\r\n\r\n    @property\r\n    def fill_value(self):\r\n        \"\"\"\r\n        Elements in `data` that are `fill_value` are not stored.\r\n\r\n        For memory savings, this should be the most common value in the array.\r\n        \"\"\"\r\n        return self.dtype.fill_value\r\n\r\n    @fill_value.setter\r\n    def fill_value(self, value) -> None:\r\n        self._dtype = SparseDtype(self.dtype.subtype, value)\r\n\r\n    @property\r\n    def kind(self) -> SparseIndexKind:\r\n        \"\"\"\r\n        The kind of sparse index for this array. One of {'integer', 'block'}.\r\n        \"\"\"\r\n        if isinstance(self.sp_index, IntIndex):\r\n            return \"integer\"\r\n        else:\r\n            return \"block\"\r\n\r\n    @property\r\n    def _valid_sp_values(self) -> np.ndarray:\r\n        sp_vals = self.sp_values\r\n        mask = notna(sp_vals)\r\n        return sp_vals[mask]\r\n\r\n    def __len__(self) -> int:\r\n        return self.sp_index.length\r\n\r\n    @property\r\n    def _null_fill_value(self) -> bool:\r\n        return self._dtype._is_na_fill_value\r\n\r\n    def _fill_value_matches(self, fill_value) -> bool:\r\n        if self._null_fill_value:\r\n            return isna(fill_value)\r\n        else:\r\n            return self.fill_value == fill_value\r\n\r\n    @property\r\n    def nbytes(self) -> int:\r\n        return self.sp_values.nbytes + self.sp_index.nbytes\r\n\r\n    @property\r\n    def density(self) -> float:\r\n        \"\"\"\r\n        The percent of non- ``fill_value`` points, as decimal.\r\n\r\n        Examples\r\n        --------\r\n        >>> from pandas.arrays import SparseArray\r\n        >>> s = SparseArray([0, 0, 1, 1, 1], fill_value=0)\r\n        >>> s.density\r\n        0.6\r\n        \"\"\"\r\n        return self.sp_index.npoints / self.sp_index.length\r\n\r\n    @property\r\n    def npoints(self) -> int:\r\n        \"\"\"\r\n        The number of non- ``fill_value`` points.\r\n\r\n        Examples\r\n        --------\r\n        >>> from pandas.arrays import SparseArray\r\n        >>> s = SparseArray([0, 0, 1, 1, 1], fill_value=0)\r\n        >>> s.npoints\r\n        3\r\n        \"\"\"\r\n        return self.sp_index.npoints\r\n\r\n    def isna(self):\r\n        # If null fill value, we want SparseDtype[bool, true]\r\n        # to preserve the same memory usage.\r\n        dtype = SparseDtype(bool, self._null_fill_value)\r\n        if self._null_fill_value:\r\n            return type(self)._simple_new(isna(self.sp_values), self.sp_index, dtype)\r\n        mask = np.full(len(self), False, dtype=np.bool_)\r\n        mask[self.sp_index.indices] = isna(self.sp_values)\r\n        return type(self)(mask, fill_value=False, dtype=dtype)\r\n\r\n    def fillna(\r\n        self,\r\n        value=None,\r\n        method: FillnaOptions | None = None,\r\n        limit: int | None = None,\r\n    ) -> Self:\r\n        \"\"\"\r\n        Fill missing values with `value`.\r\n\r\n        Parameters\r\n        ----------\r\n        value : scalar, optional\r\n        method : str, optional\r\n\r\n            .. warning::\r\n\r\n               Using 'method' will result in high memory use,\r\n               as all `fill_value` methods will be converted to\r\n               an in-memory ndarray\r\n\r\n        limit : int, optional\r\n\r\n        Returns\r\n        -------\r\n        SparseArray\r\n\r\n        Notes\r\n        -----\r\n        When `value` is specified, the result's ``fill_value`` depends on\r\n        ``self.fill_value``. The goal is to maintain low-memory use.\r\n\r\n        If ``self.fill_value`` is NA, the result dtype will be\r\n        ``SparseDtype(self.dtype, fill_value=value)``. This will preserve\r\n        amount of memory used before and after filling.\r\n\r\n        When ``self.fill_value`` is not NA, the result dtype will be\r\n        ``self.dtype``. Again, this preserves the amount of memory used.\r\n        \"\"\"\r\n        if (method is None and value is None) or (\r\n            method is not None and value is not None\r\n        ):\r\n            raise ValueError(\"Must specify one of 'method' or 'value'.\")\r\n\r\n        if method is not None:\r\n            msg = \"fillna with 'method' requires high memory usage.\"\r\n            warnings.warn(\r\n                msg,\r\n                PerformanceWarning,\r\n                stacklevel=find_stack_level(),\r\n            )\r\n            new_values = np.asarray(self)\r\n            # interpolate_2d modifies new_values inplace\r\n            interpolate_2d(new_values, method=method, limit=limit)\r\n            return type(self)(new_values, fill_value=self.fill_value)\r\n\r\n        else:\r\n            new_values = np.where(isna(self.sp_values), value, self.sp_values)\r\n\r\n            if self._null_fill_value:\r\n                # This is essentially just updating the dtype.\r\n                new_dtype = SparseDtype(self.dtype.subtype, fill_value=value)\r\n            else:\r\n                new_dtype = self.dtype\r\n\r\n        return self._simple_new(new_values, self._sparse_index, new_dtype)\r\n\r\n    def shift(self, periods: int = 1, fill_value=None) -> Self:\r\n        if not len(self) or periods == 0:\r\n            return self.copy()\r\n\r\n        if isna(fill_value):\r\n            fill_value = self.dtype.na_value\r\n\r\n        subtype = np.result_type(fill_value, self.dtype.subtype)\r\n\r\n        if subtype != self.dtype.subtype:\r\n            # just coerce up front\r\n            arr = self.astype(SparseDtype(subtype, self.fill_value))\r\n        else:\r\n            arr = self\r\n\r\n        empty = self._from_sequence(\r\n            [fill_value] * min(abs(periods), len(self)), dtype=arr.dtype\r\n        )\r\n\r\n        if periods > 0:\r\n            a = empty\r\n            b = arr[:-periods]\r\n        else:\r\n            a = arr[abs(periods) :]\r\n            b = empty\r\n        return arr._concat_same_type([a, b])\r\n\r\n    def _first_fill_value_loc(self):\r\n        \"\"\"\r\n        Get the location of the first fill value.\r\n\r\n        Returns\r\n        -------\r\n        int\r\n        \"\"\"\r\n        if len(self) == 0 or self.sp_index.npoints == len(self):\r\n            return -1\r\n\r\n        indices = self.sp_index.indices\r\n        if not len(indices) or indices[0] > 0:\r\n            return 0\r\n\r\n        # a number larger than 1 should be appended to\r\n        # the last in case of fill value only appears\r\n        # in the tail of array\r\n        diff = np.r_[np.diff(indices), 2]\r\n        return indices[(diff > 1).argmax()] + 1\r\n\r\n    def unique(self) -> Self:\r\n        uniques = algos.unique(self.sp_values)\r\n        if len(self.sp_values) != len(self):\r\n            fill_loc = self._first_fill_value_loc()\r\n            # Inorder to align the behavior of pd.unique or\r\n            # pd.Series.unique, we should keep the original\r\n            # order, here we use unique again to find the\r\n            # insertion place. Since the length of sp_values\r\n            # is not large, maybe minor performance hurt\r\n            # is worthwhile to the correctness.\r\n            insert_loc = len(algos.unique(self.sp_values[:fill_loc]))\r\n            uniques = np.insert(uniques, insert_loc, self.fill_value)\r\n        return type(self)._from_sequence(uniques, dtype=self.dtype)\r\n\r\n    def _values_for_factorize(self):\r\n        # Still override this for hash_pandas_object\r\n        return np.asarray(self), self.fill_value\r\n\r\n    def factorize(\r\n        self,\r\n        use_na_sentinel: bool = True,\r\n    ) -> tuple[np.ndarray, SparseArray]:\r\n        # Currently, ExtensionArray.factorize -> Tuple[ndarray, EA]\r\n        # The sparsity on this is backwards from what Sparse would want. Want\r\n        # ExtensionArray.factorize -> Tuple[EA, EA]\r\n        # Given that we have to return a dense array of codes, why bother\r\n        # implementing an efficient factorize?\r\n        codes, uniques = algos.factorize(\r\n            np.asarray(self), use_na_sentinel=use_na_sentinel\r\n        )\r\n        uniques_sp = SparseArray(uniques, dtype=self.dtype)\r\n        return codes, uniques_sp\r\n\r\n    def value_counts(self, dropna: bool = True) -> Series:\r\n        \"\"\"\r\n        Returns a Series containing counts of unique values.\r\n\r\n        Parameters\r\n        ----------\r\n        dropna : bool, default True\r\n            Don't include counts of NaN, even if NaN is in sp_values.\r\n\r\n        Returns\r\n        -------\r\n        counts : Series\r\n        \"\"\"\r\n        from pandas import (\r\n            Index,\r\n            Series,\r\n        )\r\n\r\n        keys, counts = algos.value_counts_arraylike(self.sp_values, dropna=dropna)\r\n        fcounts = self.sp_index.ngaps\r\n        if fcounts > 0 and (not self._null_fill_value or not dropna):\r\n            mask = isna(keys) if self._null_fill_value else keys == self.fill_value\r\n            if mask.any():\r\n                counts[mask] += fcounts\r\n            else:\r\n                # error: Argument 1 to \"insert\" has incompatible type \"Union[\r\n                # ExtensionArray,ndarray[Any, Any]]\"; expected \"Union[\r\n                # _SupportsArray[dtype[Any]], Sequence[_SupportsArray[dtype\r\n                # [Any]]], Sequence[Sequence[_SupportsArray[dtype[Any]]]],\r\n                # Sequence[Sequence[Sequence[_SupportsArray[dtype[Any]]]]], Sequence\r\n                # [Sequence[Sequence[Sequence[_SupportsArray[dtype[Any]]]]]]]\"\r\n                keys = np.insert(keys, 0, self.fill_value)  # type: ignore[arg-type]\r\n                counts = np.insert(counts, 0, fcounts)\r\n\r\n        if not isinstance(keys, ABCIndex):\r\n            index = Index(keys)\r\n        else:\r\n            index = keys\r\n        return Series(counts, index=index, copy=False)\r\n\r\n    # --------\r\n    # Indexing\r\n    # --------\r\n    @overload\r\n    def __getitem__(self, key: ScalarIndexer) -> Any:\r\n        ...\r\n\r\n    @overload\r\n    def __getitem__(\r\n        self,\r\n        key: SequenceIndexer | tuple[int | ellipsis, ...],\r\n    ) -> Self:\r\n        ...\r\n\r\n    def __getitem__(\r\n        self,\r\n        key: PositionalIndexer | tuple[int | ellipsis, ...],\r\n    ) -> Self | Any:\r\n        if isinstance(key, tuple):\r\n            key = unpack_tuple_and_ellipses(key)\r\n            if key is Ellipsis:\r\n                raise ValueError(\"Cannot slice with Ellipsis\")\r\n\r\n        if is_integer(key):\r\n            return self._get_val_at(key)\r\n        elif isinstance(key, tuple):\r\n            # error: Invalid index type \"Tuple[Union[int, ellipsis], ...]\"\r\n            # for \"ndarray[Any, Any]\"; expected type\r\n            # \"Union[SupportsIndex, _SupportsArray[dtype[Union[bool_,\r\n            # integer[Any]]]], _NestedSequence[_SupportsArray[dtype[\r\n            # Union[bool_, integer[Any]]]]], _NestedSequence[Union[\r\n            # bool, int]], Tuple[Union[SupportsIndex, _SupportsArray[\r\n            # dtype[Union[bool_, integer[Any]]]], _NestedSequence[\r\n            # _SupportsArray[dtype[Union[bool_, integer[Any]]]]],\r\n            # _NestedSequence[Union[bool, int]]], ...]]\"\r\n            data_slice = self.to_dense()[key]  # type: ignore[index]\r\n        elif isinstance(key, slice):\r\n            # Avoid densifying when handling contiguous slices\r\n            if key.step is None or key.step == 1:\r\n                start = 0 if key.start is None else key.start\r\n                if start < 0:\r\n                    start += len(self)\r\n\r\n                end = len(self) if key.stop is None else key.stop\r\n                if end < 0:\r\n                    end += len(self)\r\n\r\n                indices = self.sp_index.indices\r\n                keep_inds = np.flatnonzero((indices >= start) & (indices < end))\r\n                sp_vals = self.sp_values[keep_inds]\r\n\r\n                sp_index = indices[keep_inds].copy()\r\n\r\n                # If we've sliced to not include the start of the array, all our indices\r\n                # should be shifted. NB: here we are careful to also not shift by a\r\n                # negative value for a case like [0, 1][-100:] where the start index\r\n                # should be treated like 0\r\n                if start > 0:\r\n                    sp_index -= start\r\n\r\n                # Length of our result should match applying this slice to a range\r\n                # of the length of our original array\r\n                new_len = len(range(len(self))[key])\r\n                new_sp_index = make_sparse_index(new_len, sp_index, self.kind)\r\n                return type(self)._simple_new(sp_vals, new_sp_index, self.dtype)\r\n            else:\r\n                indices = np.arange(len(self), dtype=np.int32)[key]\r\n                return self.take(indices)\r\n\r\n        elif not is_list_like(key):\r\n            # e.g. \"foo\" or 2.5\r\n            # exception message copied from numpy\r\n            raise IndexError(\r\n                r\"only integers, slices (`:`), ellipsis (`...`), numpy.newaxis \"\r\n                r\"(`None`) and integer or boolean arrays are valid indices\"\r\n            )\r\n\r\n        else:\r\n            if isinstance(key, SparseArray):\r\n                # NOTE: If we guarantee that SparseDType(bool)\r\n                # has only fill_value - true, false or nan\r\n                # (see GH PR 44955)\r\n                # we can apply mask very fast:\r\n                if is_bool_dtype(key):\r\n                    if isna(key.fill_value):\r\n                        return self.take(key.sp_index.indices[key.sp_values])\r\n                    if not key.fill_value:\r\n                        return self.take(key.sp_index.indices)\r\n                    n = len(self)\r\n                    mask = np.full(n, True, dtype=np.bool_)\r\n                    mask[key.sp_index.indices] = False\r\n                    return self.take(np.arange(n)[mask])\r\n                else:\r\n                    key = np.asarray(key)\r\n\r\n            key = check_array_indexer(self, key)\r\n\r\n            if com.is_bool_indexer(key):\r\n                # mypy doesn't know we have an array here\r\n                key = cast(np.ndarray, key)\r\n                return self.take(np.arange(len(key), dtype=np.int32)[key])\r\n            elif hasattr(key, \"__len__\"):\r\n                return self.take(key)\r\n            else:\r\n                raise ValueError(f\"Cannot slice with '{key}'\")\r\n\r\n        return type(self)(data_slice, kind=self.kind)\r\n\r\n    def _get_val_at(self, loc):\r\n        loc = validate_insert_loc(loc, len(self))\r\n\r\n        sp_loc = self.sp_index.lookup(loc)\r\n        if sp_loc == -1:\r\n            return self.fill_value\r\n        else:\r\n            val = self.sp_values[sp_loc]\r\n            val = maybe_box_datetimelike(val, self.sp_values.dtype)\r\n            return val\r\n\r\n    def take(self, indices, *, allow_fill: bool = False, fill_value=None) -> Self:\r\n        if is_scalar(indices):\r\n            raise ValueError(f\"'indices' must be an array, not a scalar '{indices}'.\")\r\n        indices = np.asarray(indices, dtype=np.int32)\r\n\r\n        dtype = None\r\n        if indices.size == 0:\r\n            result = np.array([], dtype=\"object\")\r\n            dtype = self.dtype\r\n        elif allow_fill:\r\n            result = self._take_with_fill(indices, fill_value=fill_value)\r\n        else:\r\n            return self._take_without_fill(indices)\r\n\r\n        return type(self)(\r\n            result, fill_value=self.fill_value, kind=self.kind, dtype=dtype\r\n        )\r\n\r\n    def _take_with_fill(self, indices, fill_value=None) -> np.ndarray:\r\n        if fill_value is None:\r\n            fill_value = self.dtype.na_value\r\n\r\n        if indices.min() < -1:\r\n            raise ValueError(\r\n                \"Invalid value in 'indices'. Must be between -1 \"\r\n                \"and the length of the array.\"\r\n            )\r\n\r\n        if indices.max() >= len(self):\r\n            raise IndexError(\"out of bounds value in 'indices'.\")\r\n\r\n        if len(self) == 0:\r\n            # Empty... Allow taking only if all empty\r\n            if (indices == -1).all():\r\n                dtype = np.result_type(self.sp_values, type(fill_value))\r\n                taken = np.empty_like(indices, dtype=dtype)\r\n                taken.fill(fill_value)\r\n                return taken\r\n            else:\r\n                raise IndexError(\"cannot do a non-empty take from an empty axes.\")\r\n\r\n        # sp_indexer may be -1 for two reasons\r\n        # 1.) we took for an index of -1 (new)\r\n        # 2.) we took a value that was self.fill_value (old)\r\n        sp_indexer = self.sp_index.lookup_array(indices)\r\n        new_fill_indices = indices == -1\r\n        old_fill_indices = (sp_indexer == -1) & ~new_fill_indices\r\n\r\n        if self.sp_index.npoints == 0 and old_fill_indices.all():\r\n            # We've looked up all valid points on an all-sparse array.\r\n            taken = np.full(\r\n                sp_indexer.shape, fill_value=self.fill_value, dtype=self.dtype.subtype\r\n            )\r\n\r\n        elif self.sp_index.npoints == 0:\r\n            # Avoid taking from the empty self.sp_values\r\n            _dtype = np.result_type(self.dtype.subtype, type(fill_value))\r\n            taken = np.full(sp_indexer.shape, fill_value=fill_value, dtype=_dtype)\r\n        else:\r\n            taken = self.sp_values.take(sp_indexer)\r\n\r\n            # Fill in two steps.\r\n            # Old fill values\r\n            # New fill values\r\n            # potentially coercing to a new dtype at each stage.\r\n\r\n            m0 = sp_indexer[old_fill_indices] < 0\r\n            m1 = sp_indexer[new_fill_indices] < 0\r\n\r\n            result_type = taken.dtype\r\n\r\n            if m0.any():\r\n                result_type = np.result_type(result_type, type(self.fill_value))\r\n                taken = taken.astype(result_type)\r\n                taken[old_fill_indices] = self.fill_value\r\n\r\n            if m1.any():\r\n                result_type = np.result_type(result_type, type(fill_value))\r\n                taken = taken.astype(result_type)\r\n                taken[new_fill_indices] = fill_value\r\n\r\n        return taken\r\n\r\n    def _take_without_fill(self, indices) -> Self:\r\n        to_shift = indices < 0\r\n\r\n        n = len(self)\r\n\r\n        if (indices.max() >= n) or (indices.min() < -n):\r\n            if n == 0:\r\n                raise IndexError(\"cannot do a non-empty take from an empty axes.\")\r\n            raise IndexError(\"out of bounds value in 'indices'.\")\r\n\r\n        if to_shift.any():\r\n            indices = indices.copy()\r\n            indices[to_shift] += n\r\n\r\n        sp_indexer = self.sp_index.lookup_array(indices)\r\n        value_mask = sp_indexer != -1\r\n        new_sp_values = self.sp_values[sp_indexer[value_mask]]\r\n\r\n        value_indices = np.flatnonzero(value_mask).astype(np.int32, copy=False)\r\n\r\n        new_sp_index = make_sparse_index(len(indices), value_indices, kind=self.kind)\r\n        return type(self)._simple_new(new_sp_values, new_sp_index, dtype=self.dtype)\r\n\r\n    def searchsorted(\r\n        self,\r\n        v: ArrayLike | object,\r\n        side: Literal[\"left\", \"right\"] = \"left\",\r\n        sorter: NumpySorter = None,\r\n    ) -> npt.NDArray[np.intp] | np.intp:\r\n        msg = \"searchsorted requires high memory usage.\"\r\n        warnings.warn(msg, PerformanceWarning, stacklevel=find_stack_level())\r\n        if not is_scalar(v):\r\n            v = np.asarray(v)\r\n        v = np.asarray(v)\r\n        return np.asarray(self, dtype=self.dtype.subtype).searchsorted(v, side, sorter)\r\n\r\n    def copy(self) -> Self:\r\n        values = self.sp_values.copy()\r\n        return self._simple_new(values, self.sp_index, self.dtype)\r\n\r\n    @classmethod\r\n    def _concat_same_type(cls, to_concat: Sequence[Self]) -> Self:\r\n        fill_value = to_concat[0].fill_value\r\n\r\n        values = []\r\n        length = 0\r\n\r\n        if to_concat:\r\n            sp_kind = to_concat[0].kind\r\n        else:\r\n            sp_kind = \"integer\"\r\n\r\n        sp_index: SparseIndex\r\n        if sp_kind == \"integer\":\r\n            indices = []\r\n\r\n            for arr in to_concat:\r\n                int_idx = arr.sp_index.indices.copy()\r\n                int_idx += length  # TODO: wraparound\r\n                length += arr.sp_index.length\r\n\r\n                values.append(arr.sp_values)\r\n                indices.append(int_idx)\r\n\r\n            data = np.concatenate(values)\r\n            indices_arr = np.concatenate(indices)\r\n            # error: Argument 2 to \"IntIndex\" has incompatible type\r\n            # \"ndarray[Any, dtype[signedinteger[_32Bit]]]\";\r\n"]
[130.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[130.002, "i", "sed -n '1160,2000p' pandas/core/arrays/sparse/array.py\r"]
[130.004, "o", "sed -n '1160,2000p' pandas/core/arrays/sparse/array.py\r\n"]
[130.006, "o", "\u001b[?2004l\r\n            # \"ndarray[Any, dtype[signedinteger[_32Bit]]]\";\r\n            # expected \"Sequence[int]\"\r\n            sp_index = IntIndex(length, indices_arr)  # type: ignore[arg-type]\r\n\r\n        else:\r\n            # when concatenating block indices, we don't claim that you'll\r\n            # get an identical index as concatenating the values and then\r\n            # creating a new index. We don't want to spend the time trying\r\n            # to merge blocks across arrays in `to_concat`, so the resulting\r\n            # BlockIndex may have more blocks.\r\n            blengths = []\r\n            blocs = []\r\n\r\n            for arr in to_concat:\r\n                block_idx = arr.sp_index.to_block_index()\r\n\r\n                values.append(arr.sp_values)\r\n                blocs.append(block_idx.blocs.copy() + length)\r\n                blengths.append(block_idx.blengths)\r\n                length += arr.sp_index.length\r\n\r\n            data = np.concatenate(values)\r\n            blocs_arr = np.concatenate(blocs)\r\n            blengths_arr = np.concatenate(blengths)\r\n\r\n            sp_index = BlockIndex(length, blocs_arr, blengths_arr)\r\n\r\n        return cls(data, sparse_index=sp_index, fill_value=fill_value)\r\n\r\n    def astype(self, dtype: AstypeArg | None = None, copy: bool = True):\r\n        \"\"\"\r\n        Change the dtype of a SparseArray.\r\n\r\n        The output will always be a SparseArray. To convert to a dense\r\n        ndarray with a certain dtype, use :meth:`numpy.asarray`.\r\n\r\n        Parameters\r\n        ----------\r\n        dtype : np.dtype or ExtensionDtype\r\n            For SparseDtype, this changes the dtype of\r\n            ``self.sp_values`` and the ``self.fill_value``.\r\n\r\n            For other dtypes, this only changes the dtype of\r\n            ``self.sp_values``.\r\n\r\n        copy : bool, default True\r\n            Whether to ensure a copy is made, even if not necessary.\r\n\r\n        Returns\r\n        -------\r\n        SparseArray\r\n\r\n        Examples\r\n        --------\r\n        >>> arr = pd.arrays.SparseArray([0, 0, 1, 2])\r\n        >>> arr\r\n        [0, 0, 1, 2]\r\n        Fill: 0\r\n        IntIndex\r\n        Indices: array([2, 3], dtype=int32)\r\n\r\n        >>> arr.astype(SparseDtype(np.dtype('int32')))\r\n        [0, 0, 1, 2]\r\n        Fill: 0\r\n        IntIndex\r\n        Indices: array([2, 3], dtype=int32)\r\n\r\n        Using a NumPy dtype with a different kind (e.g. float) will coerce\r\n        just ``self.sp_values``.\r\n\r\n        >>> arr.astype(SparseDtype(np.dtype('float64')))\r\n        ... # doctest: +NORMALIZE_WHITESPACE\r\n        [nan, nan, 1.0, 2.0]\r\n        Fill: nan\r\n        IntIndex\r\n        Indices: array([2, 3], dtype=int32)\r\n\r\n        Using a SparseDtype, you can also change the fill value as well.\r\n\r\n        >>> arr.astype(SparseDtype(\"float64\", fill_value=0.0))\r\n        ... # doctest: +NORMALIZE_WHITESPACE\r\n        [0.0, 0.0, 1.0, 2.0]\r\n        Fill: 0.0\r\n        IntIndex\r\n        Indices: array([2, 3], dtype=int32)\r\n        \"\"\"\r\n        if is_dtype_equal(dtype, self._dtype):\r\n            if not copy:\r\n                return self\r\n            else:\r\n                return self.copy()\r\n\r\n        future_dtype = pandas_dtype(dtype)\r\n        if not isinstance(future_dtype, SparseDtype):\r\n            # GH#34457\r\n            values = np.asarray(self)\r\n            values = ensure_wrapped_if_datetimelike(values)\r\n            return astype_array(values, dtype=future_dtype, copy=False)\r\n\r\n        dtype = self.dtype.update_dtype(dtype)\r\n        subtype = pandas_dtype(dtype._subtype_with_str)\r\n        subtype = cast(np.dtype, subtype)  # ensured by update_dtype\r\n        values = ensure_wrapped_if_datetimelike(self.sp_values)\r\n        sp_values = astype_array(values, subtype, copy=copy)\r\n        sp_values = np.asarray(sp_values)\r\n\r\n        return self._simple_new(sp_values, self.sp_index, dtype)\r\n\r\n    def map(self, mapper, na_action=None) -> Self:\r\n        \"\"\"\r\n        Map categories using an input mapping or function.\r\n\r\n        Parameters\r\n        ----------\r\n        mapper : dict, Series, callable\r\n            The correspondence from old values to new.\r\n        na_action : {None, 'ignore'}, default None\r\n            If 'ignore', propagate NA values, without passing them to the\r\n            mapping correspondence.\r\n\r\n        Returns\r\n        -------\r\n        SparseArray\r\n            The output array will have the same density as the input.\r\n            The output fill value will be the result of applying the\r\n            mapping to ``self.fill_value``\r\n\r\n        Examples\r\n        --------\r\n        >>> arr = pd.arrays.SparseArray([0, 1, 2])\r\n        >>> arr.map(lambda x: x + 10)\r\n        [10, 11, 12]\r\n        Fill: 10\r\n        IntIndex\r\n        Indices: array([1, 2], dtype=int32)\r\n\r\n        >>> arr.map({0: 10, 1: 11, 2: 12})\r\n        [10, 11, 12]\r\n        Fill: 10\r\n        IntIndex\r\n        Indices: array([1, 2], dtype=int32)\r\n\r\n        >>> arr.map(pd.Series([10, 11, 12], index=[0, 1, 2]))\r\n        [10, 11, 12]\r\n        Fill: 10\r\n        IntIndex\r\n        Indices: array([1, 2], dtype=int32)\r\n        \"\"\"\r\n        is_map = isinstance(mapper, (abc.Mapping, ABCSeries))\r\n\r\n        fill_val = self.fill_value\r\n\r\n        if na_action is None or notna(fill_val):\r\n            fill_val = mapper.get(fill_val, fill_val) if is_map else mapper(fill_val)\r\n\r\n        def func(sp_val):\r\n            new_sp_val = mapper.get(sp_val, None) if is_map else mapper(sp_val)\r\n            # check identity and equality because nans are not equal to each other\r\n            if new_sp_val is fill_val or new_sp_val == fill_val:\r\n                msg = \"fill value in the sparse values not supported\"\r\n                raise ValueError(msg)\r\n            return new_sp_val\r\n\r\n        sp_values = [func(x) for x in self.sp_values]\r\n\r\n        return type(self)(sp_values, sparse_index=self.sp_index, fill_value=fill_val)\r\n\r\n    def to_dense(self) -> np.ndarray:\r\n        \"\"\"\r\n        Convert SparseArray to a NumPy array.\r\n\r\n        Returns\r\n        -------\r\n        arr : NumPy array\r\n        \"\"\"\r\n        return np.asarray(self, dtype=self.sp_values.dtype)\r\n\r\n    def _where(self, mask, value):\r\n        # NB: may not preserve dtype, e.g. result may be Sparse[float64]\r\n        #  while self is Sparse[int64]\r\n        naive_implementation = np.where(mask, self, value)\r\n        dtype = SparseDtype(naive_implementation.dtype, fill_value=self.fill_value)\r\n        result = type(self)._from_sequence(naive_implementation, dtype=dtype)\r\n        return result\r\n\r\n    # ------------------------------------------------------------------------\r\n    # IO\r\n    # ------------------------------------------------------------------------\r\n    def __setstate__(self, state) -> None:\r\n        \"\"\"Necessary for making this object picklable\"\"\"\r\n        if isinstance(state, tuple):\r\n            # Compat for pandas < 0.24.0\r\n            nd_state, (fill_value, sp_index) = state\r\n            sparse_values = np.array([])\r\n            sparse_values.__setstate__(nd_state)\r\n\r\n            self._sparse_values = sparse_values\r\n            self._sparse_index = sp_index\r\n            self._dtype = SparseDtype(sparse_values.dtype, fill_value)\r\n        else:\r\n            self.__dict__.update(state)\r\n\r\n    def nonzero(self) -> tuple[npt.NDArray[np.int32]]:\r\n        if self.fill_value == 0:\r\n            return (self.sp_index.indices,)\r\n        else:\r\n            return (self.sp_index.indices[self.sp_values != 0],)\r\n\r\n    # ------------------------------------------------------------------------\r\n    # Reductions\r\n    # ------------------------------------------------------------------------\r\n\r\n    def _reduce(self, name: str, *, skipna: bool = True, **kwargs):\r\n        method = getattr(self, name, None)\r\n\r\n        if method is None:\r\n            raise TypeError(f\"cannot perform {name} with type {self.dtype}\")\r\n\r\n        if skipna:\r\n            arr = self\r\n        else:\r\n            arr = self.dropna()\r\n\r\n        return getattr(arr, name)(**kwargs)\r\n\r\n    def all(self, axis=None, *args, **kwargs):\r\n        \"\"\"\r\n        Tests whether all elements evaluate True\r\n\r\n        Returns\r\n        -------\r\n        all : bool\r\n\r\n        See Also\r\n        --------\r\n        numpy.all\r\n        \"\"\"\r\n        nv.validate_all(args, kwargs)\r\n\r\n        values = self.sp_values\r\n\r\n        if len(values) != len(self) and not np.all(self.fill_value):\r\n            return False\r\n\r\n        return values.all()\r\n\r\n    def any(self, axis: AxisInt = 0, *args, **kwargs):\r\n        \"\"\"\r\n        Tests whether at least one of elements evaluate True\r\n\r\n        Returns\r\n        -------\r\n        any : bool\r\n\r\n        See Also\r\n        --------\r\n        numpy.any\r\n        \"\"\"\r\n        nv.validate_any(args, kwargs)\r\n\r\n        values = self.sp_values\r\n\r\n        if len(values) != len(self) and np.any(self.fill_value):\r\n            return True\r\n\r\n        return values.any().item()\r\n\r\n    def sum(\r\n        self,\r\n        axis: AxisInt = 0,\r\n        min_count: int = 0,\r\n        skipna: bool = True,\r\n        *args,\r\n        **kwargs,\r\n    ) -> Scalar:\r\n        \"\"\"\r\n        Sum of non-NA/null values\r\n\r\n        Parameters\r\n        ----------\r\n        axis : int, default 0\r\n            Not Used. NumPy compatibility.\r\n        min_count : int, default 0\r\n            The required number of valid values to perform the summation. If fewer\r\n            than ``min_count`` valid values are present, the result will be the missing\r\n            value indicator for subarray type.\r\n        *args, **kwargs\r\n            Not Used. NumPy compatibility.\r\n\r\n        Returns\r\n        -------\r\n        scalar\r\n        \"\"\"\r\n        nv.validate_sum(args, kwargs)\r\n        valid_vals = self._valid_sp_values\r\n        sp_sum = valid_vals.sum()\r\n        has_na = self.sp_index.ngaps > 0 and not self._null_fill_value\r\n\r\n        if has_na and not skipna:\r\n            return na_value_for_dtype(self.dtype.subtype, compat=False)\r\n\r\n        if self._null_fill_value:\r\n            if check_below_min_count(valid_vals.shape, None, min_count):\r\n                return na_value_for_dtype(self.dtype.subtype, compat=False)\r\n            return sp_sum\r\n        else:\r\n            nsparse = self.sp_index.ngaps\r\n            if check_below_min_count(valid_vals.shape, None, min_count - nsparse):\r\n                return na_value_for_dtype(self.dtype.subtype, compat=False)\r\n            return sp_sum + self.fill_value * nsparse\r\n\r\n    def cumsum(self, axis: AxisInt = 0, *args, **kwargs) -> SparseArray:\r\n        \"\"\"\r\n        Cumulative sum of non-NA/null values.\r\n\r\n        When performing the cumulative summation, any non-NA/null values will\r\n        be skipped. The resulting SparseArray will preserve the locations of\r\n        NaN values, but the fill value will be `np.nan` regardless.\r\n\r\n        Parameters\r\n        ----------\r\n        axis : int or None\r\n            Axis over which to perform the cumulative summation. If None,\r\n            perform cumulative summation over flattened array.\r\n\r\n        Returns\r\n        -------\r\n        cumsum : SparseArray\r\n        \"\"\"\r\n        nv.validate_cumsum(args, kwargs)\r\n\r\n        if axis is not None and axis >= self.ndim:  # Mimic ndarray behaviour.\r\n            raise ValueError(f\"axis(={axis}) out of bounds\")\r\n\r\n        if not self._null_fill_value:\r\n            return SparseArray(self.to_dense()).cumsum()\r\n\r\n        return SparseArray(\r\n            self.sp_values.cumsum(),\r\n            sparse_index=self.sp_index,\r\n            fill_value=self.fill_value,\r\n        )\r\n\r\n    def mean(self, axis: Axis = 0, *args, **kwargs):\r\n        \"\"\"\r\n        Mean of non-NA/null values\r\n\r\n        Returns\r\n        -------\r\n        mean : float\r\n        \"\"\"\r\n        nv.validate_mean(args, kwargs)\r\n        valid_vals = self._valid_sp_values\r\n        sp_sum = valid_vals.sum()\r\n        ct = len(valid_vals)\r\n\r\n        if self._null_fill_value:\r\n            return sp_sum / ct\r\n        else:\r\n            nsparse = self.sp_index.ngaps\r\n            return (sp_sum + self.fill_value * nsparse) / (ct + nsparse)\r\n\r\n    def max(self, *, axis: AxisInt | None = None, skipna: bool = True):\r\n        \"\"\"\r\n        Max of array values, ignoring NA values if specified.\r\n\r\n        Parameters\r\n        ----------\r\n        axis : int, default 0\r\n            Not Used. NumPy compatibility.\r\n        skipna : bool, default True\r\n            Whether to ignore NA values.\r\n\r\n        Returns\r\n        -------\r\n        scalar\r\n        \"\"\"\r\n        nv.validate_minmax_axis(axis, self.ndim)\r\n        return self._min_max(\"max\", skipna=skipna)\r\n\r\n    def min(self, *, axis: AxisInt | None = None, skipna: bool = True):\r\n        \"\"\"\r\n        Min of array values, ignoring NA values if specified.\r\n\r\n        Parameters\r\n        ----------\r\n        axis : int, default 0\r\n            Not Used. NumPy compatibility.\r\n        skipna : bool, default True\r\n            Whether to ignore NA values.\r\n\r\n        Returns\r\n        -------\r\n        scalar\r\n        \"\"\"\r\n        nv.validate_minmax_axis(axis, self.ndim)\r\n        return self._min_max(\"min\", skipna=skipna)\r\n\r\n    def _min_max(self, kind: Literal[\"min\", \"max\"], skipna: bool) -> Scalar:\r\n        \"\"\"\r\n        Min/max of non-NA/null values\r\n\r\n        Parameters\r\n        ----------\r\n        kind : {\"min\", \"max\"}\r\n        skipna : bool\r\n\r\n        Returns\r\n        -------\r\n        scalar\r\n        \"\"\"\r\n        valid_vals = self._valid_sp_values\r\n        has_nonnull_fill_vals = not self._null_fill_value and self.sp_index.ngaps > 0\r\n\r\n        if len(valid_vals) > 0:\r\n            sp_min_max = getattr(valid_vals, kind)()\r\n\r\n            # If a non-null fill value is currently present, it might be the min/max\r\n            if has_nonnull_fill_vals:\r\n                func = max if kind == \"max\" else min\r\n                return func(sp_min_max, self.fill_value)\r\n            elif skipna:\r\n                return sp_min_max\r\n            elif self.sp_index.ngaps == 0:\r\n                # No NAs present\r\n                return sp_min_max\r\n            else:\r\n                return na_value_for_dtype(self.dtype.subtype, compat=False)\r\n        elif has_nonnull_fill_vals:\r\n            return self.fill_value\r\n        else:\r\n            return na_value_for_dtype(self.dtype.subtype, compat=False)\r\n\r\n    def _argmin_argmax(self, kind: Literal[\"argmin\", \"argmax\"]) -> int:\r\n        values = self._sparse_values\r\n        index = self._sparse_index.indices\r\n        mask = np.asarray(isna(values))\r\n        func = np.argmax if kind == \"argmax\" else np.argmin\r\n\r\n        idx = np.arange(values.shape[0])\r\n        non_nans = values[~mask]\r\n        non_nan_idx = idx[~mask]\r\n\r\n        _candidate = non_nan_idx[func(non_nans)]\r\n        candidate = index[_candidate]\r\n\r\n        if isna(self.fill_value):\r\n            return candidate\r\n        if kind == \"argmin\" and self[candidate] < self.fill_value:\r\n            return candidate\r\n        if kind == \"argmax\" and self[candidate] > self.fill_value:\r\n            return candidate\r\n        _loc = self._first_fill_value_loc()\r\n        if _loc == -1:\r\n            # fill_value doesn't exist\r\n            return candidate\r\n        else:\r\n            return _loc\r\n\r\n    def argmax(self, skipna: bool = True) -> int:\r\n        validate_bool_kwarg(skipna, \"skipna\")\r\n        if not skipna and self._hasna:\r\n            raise NotImplementedError\r\n        return self._argmin_argmax(\"argmax\")\r\n\r\n    def argmin(self, skipna: bool = True) -> int:\r\n        validate_bool_kwarg(skipna, \"skipna\")\r\n        if not skipna and self._hasna:\r\n            raise NotImplementedError\r\n        return self._argmin_argmax(\"argmin\")\r\n\r\n    # ------------------------------------------------------------------------\r\n    # Ufuncs\r\n    # ------------------------------------------------------------------------\r\n\r\n    _HANDLED_TYPES = (np.ndarray, numbers.Number)\r\n\r\n    def __array_ufunc__(self, ufunc: np.ufunc, method: str, *inputs, **kwargs):\r\n        out = kwargs.get(\"out\", ())\r\n\r\n        for x in inputs + out:\r\n            if not isinstance(x, self._HANDLED_TYPES + (SparseArray,)):\r\n                return NotImplemented\r\n\r\n        # for binary ops, use our custom dunder methods\r\n        result = arraylike.maybe_dispatch_ufunc_to_dunder_op(\r\n            self, ufunc, method, *inputs, **kwargs\r\n        )\r\n        if result is not NotImplemented:\r\n            return result\r\n\r\n        if \"out\" in kwargs:\r\n            # e.g. tests.arrays.sparse.test_arithmetics.test_ndarray_inplace\r\n            res = arraylike.dispatch_ufunc_with_out(\r\n                self, ufunc, method, *inputs, **kwargs\r\n            )\r\n            return res\r\n\r\n        if method == \"reduce\":\r\n            result = arraylike.dispatch_reduction_ufunc(\r\n                self, ufunc, method, *inputs, **kwargs\r\n            )\r\n            if result is not NotImplemented:\r\n                # e.g. tests.series.test_ufunc.TestNumpyReductions\r\n                return result\r\n\r\n        if len(inputs) == 1:\r\n            # No alignment necessary.\r\n            sp_values = getattr(ufunc, method)(self.sp_values, **kwargs)\r\n            fill_value = getattr(ufunc, method)(self.fill_value, **kwargs)\r\n\r\n            if ufunc.nout > 1:\r\n                # multiple outputs. e.g. modf\r\n                arrays = tuple(\r\n                    self._simple_new(\r\n                        sp_value, self.sp_index, SparseDtype(sp_value.dtype, fv)\r\n                    )\r\n                    for sp_value, fv in zip(sp_values, fill_value)\r\n                )\r\n                return arrays\r\n            elif method == \"reduce\":\r\n                # e.g. reductions\r\n                return sp_values\r\n\r\n            return self._simple_new(\r\n                sp_values, self.sp_index, SparseDtype(sp_values.dtype, fill_value)\r\n            )\r\n\r\n        new_inputs = tuple(np.asarray(x) for x in inputs)\r\n        result = getattr(ufunc, method)(*new_inputs, **kwargs)\r\n        if out:\r\n            if len(out) == 1:\r\n                out = out[0]\r\n            return out\r\n\r\n        if ufunc.nout > 1:\r\n            return tuple(type(self)(x) for x in result)\r\n        elif method == \"at\":\r\n            # no return value\r\n            return None\r\n        else:\r\n            return type(self)(result)\r\n\r\n    # ------------------------------------------------------------------------\r\n    # Ops\r\n    # ------------------------------------------------------------------------\r\n\r\n    def _arith_method(self, other, op):\r\n        op_name = op.__name__\r\n\r\n        if isinstance(other, SparseArray):\r\n            return _sparse_array_op(self, other, op, op_name)\r\n\r\n        elif is_scalar(other):\r\n            with np.errstate(all=\"ignore\"):\r\n                fill = op(_get_fill(self), np.asarray(other))\r\n                result = op(self.sp_values, other)\r\n\r\n            if op_name == \"divmod\":\r\n                left, right = result\r\n                lfill, rfill = fill\r\n                return (\r\n                    _wrap_result(op_name, left, self.sp_index, lfill),\r\n                    _wrap_result(op_name, right, self.sp_index, rfill),\r\n                )\r\n\r\n            return _wrap_result(op_name, result, self.sp_index, fill)\r\n\r\n        else:\r\n            other = np.asarray(other)\r\n            with np.errstate(all=\"ignore\"):\r\n                if len(self) != len(other):\r\n                    raise AssertionError(\r\n                        f\"length mismatch: {len(self)} vs. {len(other)}\"\r\n                    )\r\n                if not isinstance(other, SparseArray):\r\n                    dtype = getattr(other, \"dtype\", None)\r\n                    other = SparseArray(other, fill_value=self.fill_value, dtype=dtype)\r\n                return _sparse_array_op(self, other, op, op_name)\r\n\r\n    def _cmp_method(self, other, op) -> SparseArray:\r\n        if not is_scalar(other) and not isinstance(other, type(self)):\r\n            # convert list-like to ndarray\r\n            other = np.asarray(other)\r\n\r\n        if isinstance(other, np.ndarray):\r\n            # TODO: make this more flexible than just ndarray...\r\n            other = SparseArray(other, fill_value=self.fill_value)\r\n\r\n        if isinstance(other, SparseArray):\r\n            if len(self) != len(other):\r\n                raise ValueError(\r\n                    f\"operands have mismatched length {len(self)} and {len(other)}\"\r\n                )\r\n\r\n            op_name = op.__name__.strip(\"_\")\r\n            return _sparse_array_op(self, other, op, op_name)\r\n        else:\r\n            # scalar\r\n            fill_value = op(self.fill_value, other)\r\n            result = np.full(len(self), fill_value, dtype=np.bool_)\r\n            result[self.sp_index.indices] = op(self.sp_values, other)\r\n\r\n            return type(self)(\r\n                result,\r\n                fill_value=fill_value,\r\n                dtype=np.bool_,\r\n            )\r\n\r\n    _logical_method = _cmp_method\r\n\r\n    def _unary_method(self, op) -> SparseArray:\r\n        fill_value = op(np.array(self.fill_value)).item()\r\n        dtype = SparseDtype(self.dtype.subtype, fill_value)\r\n        # NOTE: if fill_value doesn't change\r\n        # we just have to apply op to sp_values\r\n        if isna(self.fill_value) or fill_value == self.fill_value:\r\n            values = op(self.sp_values)\r\n            return type(self)._simple_new(values, self.sp_index, self.dtype)\r\n        # In the other case we have to recalc indexes\r\n        return type(self)(op(self.to_dense()), dtype=dtype)\r\n\r\n    def __pos__(self) -> SparseArray:\r\n        return self._unary_method(operator.pos)\r\n\r\n    def __neg__(self) -> SparseArray:\r\n        return self._unary_method(operator.neg)\r\n\r\n    def __invert__(self) -> SparseArray:\r\n        return self._unary_method(operator.invert)\r\n\r\n    def __abs__(self) -> SparseArray:\r\n        return self._unary_method(operator.abs)\r\n\r\n    # ----------\r\n    # Formatting\r\n    # -----------\r\n    def __repr__(self) -> str:\r\n        pp_str = printing.pprint_thing(self)\r\n        pp_fill = printing.pprint_thing(self.fill_value)\r\n        pp_index = printing.pprint_thing(self.sp_index)\r\n        return f\"{pp_str}\\nFill: {pp_fill}\\n{pp_index}\"\r\n\r\n    def _formatter(self, boxed: bool = False):\r\n        # Defer to the formatter from the GenericArrayFormatter calling us.\r\n        # This will infer the correct formatter from the dtype of the values.\r\n        return None\r\n\r\n    # ------------------------------------------------------------------------\r\n    # GroupBy Methods\r\n\r\n    def _groupby_op(\r\n        self,\r\n        *,\r\n        how: str,\r\n        has_dropped_na: bool,\r\n        min_count: int,\r\n        ngroups: int,\r\n        ids: npt.NDArray[np.intp],\r\n        **kwargs,\r\n    ):\r\n        raise NotImplementedError(f\"{self.dtype} dtype not supported\")\r\n\r\n\r\ndef _make_sparse(\r\n    arr: np.ndarray,\r\n    kind: SparseIndexKind = \"block\",\r\n    fill_value=None,\r\n    dtype: np.dtype | None = None,\r\n):\r\n    \"\"\"\r\n    Convert ndarray to sparse format\r\n\r\n    Parameters\r\n    ----------\r\n    arr : ndarray\r\n    kind : {'block', 'integer'}\r\n    fill_value : NaN or another value\r\n    dtype : np.dtype, optional\r\n    copy : bool, default False\r\n\r\n    Returns\r\n    -------\r\n    (sparse_values, index, fill_value) : (ndarray, SparseIndex, Scalar)\r\n    \"\"\"\r\n    assert isinstance(arr, np.ndarray)\r\n\r\n    if arr.ndim > 1:\r\n        raise TypeError(\"expected dimension <= 1 data\")\r\n\r\n    if fill_value is None:\r\n        fill_value = na_value_for_dtype(arr.dtype)\r\n\r\n    if isna(fill_value):\r\n        mask = notna(arr)\r\n    else:\r\n        # cast to object comparison to be safe\r\n        if is_string_dtype(arr.dtype):\r\n            arr = arr.astype(object)\r\n\r\n        if is_object_dtype(arr.dtype):\r\n            # element-wise equality check method in numpy doesn't treat\r\n            # each element type, eg. 0, 0.0, and False are treated as\r\n            # same. So we have to check the both of its type and value.\r\n            mask = splib.make_mask_object_ndarray(arr, fill_value)\r\n        else:\r\n            mask = arr != fill_value\r\n\r\n    length = len(arr)\r\n    if length != len(mask):\r\n        # the arr is a SparseArray\r\n        indices = mask.sp_index.indices\r\n    else:\r\n        indices = mask.nonzero()[0].astype(np.int32)\r\n\r\n    index = make_sparse_index(length, indices, kind)\r\n    sparsified_values = arr[mask]\r\n    if dtype is not None:\r\n        sparsified_values = ensure_wrapped_if_datetimelike(sparsified_values)\r\n        sparsified_values = astype_array(sparsified_values, dtype=dtype)\r\n        sparsified_values = np.asarray(sparsified_values)\r\n\r\n    # TODO: copy\r\n    return sparsified_values, index, fill_value\r\n\r\n\r\n@overload\r\ndef make_sparse_index(length: int, indices, kind: Literal[\"block\"]) -> BlockIndex:\r\n    ...\r\n\r\n\r\n@overload\r\ndef make_sparse_index(length: int, indices, kind: Literal[\"integer\"]) -> IntIndex:\r\n    ...\r\n\r\n\r\ndef make_sparse_index(length: int, indices, kind: SparseIndexKind) -> SparseIndex:\r\n    index: SparseIndex\r\n    if kind == \"block\":\r\n        locs, lens = splib.get_blocks(indices)\r\n        index = BlockIndex(length, locs, lens)\r\n    elif kind == \"integer\":\r\n        index = IntIndex(length, indices)\r\n    else:  # pragma: no cover\r\n        raise ValueError(\"must be block or integer type\")\r\n    return index\r\n"]
[135.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[135.002, "i", "sed -n '1,200p' pandas/core/arrays/sparse/dtype.py\r"]
[135.004, "o", "sed -n '1,200p' pandas/core/arrays/sparse/dtype.py\r\n"]
[135.006, "o", "\u001b[?2004l\r\n\"\"\"Sparse Dtype\"\"\"\r\nfrom __future__ import annotations\r\n\r\nimport re\r\nfrom typing import (\r\n    TYPE_CHECKING,\r\n    Any,\r\n)\r\nimport warnings\r\n\r\nimport numpy as np\r\n\r\nfrom pandas.errors import PerformanceWarning\r\nfrom pandas.util._exceptions import find_stack_level\r\n\r\nfrom pandas.core.dtypes.astype import astype_array\r\nfrom pandas.core.dtypes.base import (\r\n    ExtensionDtype,\r\n    register_extension_dtype,\r\n)\r\nfrom pandas.core.dtypes.common import (\r\n    is_bool_dtype,\r\n    is_object_dtype,\r\n    is_scalar,\r\n    is_string_dtype,\r\n    pandas_dtype,\r\n)\r\nfrom pandas.core.dtypes.missing import (\r\n    isna,\r\n    na_value_for_dtype,\r\n)\r\n\r\nif TYPE_CHECKING:\r\n    from pandas._typing import (\r\n        Dtype,\r\n        DtypeObj,\r\n        type_t,\r\n    )\r\n\r\n    from pandas.core.arrays.sparse.array import SparseArray\r\n\r\n\r\n@register_extension_dtype\r\nclass SparseDtype(ExtensionDtype):\r\n    \"\"\"\r\n    Dtype for data stored in :class:`SparseArray`.\r\n\r\n    This dtype implements the pandas ExtensionDtype interface.\r\n\r\n    Parameters\r\n    ----------\r\n    dtype : str, ExtensionDtype, numpy.dtype, type, default numpy.float64\r\n        The dtype of the underlying array storing the non-fill value values.\r\n    fill_value : scalar, optional\r\n        The scalar value not stored in the SparseArray. By default, this\r\n        depends on `dtype`.\r\n\r\n        =========== ==========\r\n        dtype       na_value\r\n        =========== ==========\r\n        float       ``np.nan``\r\n        int         ``0``\r\n        bool        ``False``\r\n        datetime64  ``pd.NaT``\r\n        timedelta64 ``pd.NaT``\r\n        =========== ==========\r\n\r\n        The default value may be overridden by specifying a `fill_value`.\r\n\r\n    Attributes\r\n    ----------\r\n    None\r\n\r\n    Methods\r\n    -------\r\n    None\r\n    \"\"\"\r\n\r\n    # We include `_is_na_fill_value` in the metadata to avoid hash collisions\r\n    # between SparseDtype(float, 0.0) and SparseDtype(float, nan).\r\n    # Without is_na_fill_value in the comparison, those would be equal since\r\n    # hash(nan) is (sometimes?) 0.\r\n    _metadata = (\"_dtype\", \"_fill_value\", \"_is_na_fill_value\")\r\n\r\n    def __init__(self, dtype: Dtype = np.float64, fill_value: Any = None) -> None:\r\n        if isinstance(dtype, type(self)):\r\n            if fill_value is None:\r\n                fill_value = dtype.fill_value\r\n            dtype = dtype.subtype\r\n\r\n        dtype = pandas_dtype(dtype)\r\n        if is_string_dtype(dtype):\r\n            dtype = np.dtype(\"object\")\r\n\r\n        if fill_value is None:\r\n            fill_value = na_value_for_dtype(dtype)\r\n\r\n        self._dtype = dtype\r\n        self._fill_value = fill_value\r\n        self._check_fill_value()\r\n\r\n    def __hash__(self) -> int:\r\n        # Python3 doesn't inherit __hash__ when a base class overrides\r\n        # __eq__, so we explicitly do it here.\r\n        return super().__hash__()\r\n\r\n    def __eq__(self, other: Any) -> bool:\r\n        # We have to override __eq__ to handle NA values in _metadata.\r\n        # The base class does simple == checks, which fail for NA.\r\n        if isinstance(other, str):\r\n            try:\r\n                other = self.construct_from_string(other)\r\n            except TypeError:\r\n                return False\r\n\r\n        if isinstance(other, type(self)):\r\n            subtype = self.subtype == other.subtype\r\n            if self._is_na_fill_value:\r\n                # this case is complicated by two things:\r\n                # SparseDtype(float, float(nan)) == SparseDtype(float, np.nan)\r\n                # SparseDtype(float, np.nan)     != SparseDtype(float, pd.NaT)\r\n                # i.e. we want to treat any floating-point NaN as equal, but\r\n                # not a floating-point NaN and a datetime NaT.\r\n                fill_value = (\r\n                    other._is_na_fill_value\r\n                    and isinstance(self.fill_value, type(other.fill_value))\r\n                    or isinstance(other.fill_value, type(self.fill_value))\r\n                )\r\n            else:\r\n                with warnings.catch_warnings():\r\n                    # Ignore spurious numpy warning\r\n                    warnings.filterwarnings(\r\n                        \"ignore\",\r\n                        \"elementwise comparison failed\",\r\n                        category=DeprecationWarning,\r\n                    )\r\n\r\n                    fill_value = self.fill_value == other.fill_value\r\n\r\n            return subtype and fill_value\r\n        return False\r\n\r\n    @property\r\n    def fill_value(self):\r\n        \"\"\"\r\n        The fill value of the array.\r\n\r\n        Converting the SparseArray to a dense ndarray will fill the\r\n        array with this value.\r\n\r\n        .. warning::\r\n\r\n           It's possible to end up with a SparseArray that has ``fill_value``\r\n           values in ``sp_values``. This can occur, for example, when setting\r\n           ``SparseArray.fill_value`` directly.\r\n        \"\"\"\r\n        return self._fill_value\r\n\r\n    def _check_fill_value(self):\r\n        if not is_scalar(self._fill_value):\r\n            raise ValueError(\r\n                f\"fill_value must be a scalar. Got {self._fill_value} instead\"\r\n            )\r\n        # TODO: Right now we can use Sparse boolean array\r\n        #       with any fill_value. Here was an attempt\r\n        #       to allow only 3 value: True, False or nan\r\n        #       but plenty test has failed.\r\n        # see pull 44955\r\n        # if self._is_boolean and not (\r\n        #    is_bool(self._fill_value) or isna(self._fill_value)\r\n        # ):\r\n        #    raise ValueError(\r\n        #        \"fill_value must be True, False or nan \"\r\n        #        f\"for boolean type. Got {self._fill_value} instead\"\r\n        #    )\r\n\r\n    @property\r\n    def _is_na_fill_value(self) -> bool:\r\n        return isna(self.fill_value)\r\n\r\n    @property\r\n    def _is_numeric(self) -> bool:\r\n        return not is_object_dtype(self.subtype)\r\n\r\n    @property\r\n    def _is_boolean(self) -> bool:\r\n        return is_bool_dtype(self.subtype)\r\n\r\n    @property\r\n    def kind(self) -> str:\r\n        \"\"\"\r\n        The sparse kind. Either 'integer', or 'block'.\r\n        \"\"\"\r\n        return self.subtype.kind\r\n\r\n    @property\r\n    def type(self):\r\n        return self.subtype.type\r\n\r\n    @property\r\n"]
[140.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[140.002, "i", "sed -n '1,260p' pandas/core/arrays/_mixins.py\r"]
[140.004, "o", "sed -n '1,260p' pandas/core/arrays/_mixins.py\r\n"]
[140.006, "o", "\u001b[?2004l\r\nfrom __future__ import annotations\r\n\r\nfrom functools import wraps\r\nfrom typing import (\r\n    TYPE_CHECKING,\r\n    Any,\r\n    Literal,\r\n    Sequence,\r\n    cast,\r\n    overload,\r\n)\r\n\r\nimport numpy as np\r\n\r\nfrom pandas._libs import lib\r\nfrom pandas._libs.arrays import NDArrayBacked\r\nfrom pandas._typing import (\r\n    ArrayLike,\r\n    AxisInt,\r\n    Dtype,\r\n    F,\r\n    PositionalIndexer2D,\r\n    PositionalIndexerTuple,\r\n    ScalarIndexer,\r\n    Self,\r\n    SequenceIndexer,\r\n    Shape,\r\n    TakeIndexer,\r\n    npt,\r\n)\r\nfrom pandas.errors import AbstractMethodError\r\nfrom pandas.util._decorators import doc\r\nfrom pandas.util._validators import (\r\n    validate_bool_kwarg,\r\n    validate_fillna_kwargs,\r\n    validate_insert_loc,\r\n)\r\n\r\nfrom pandas.core.dtypes.common import (\r\n    is_dtype_equal,\r\n    pandas_dtype,\r\n)\r\nfrom pandas.core.dtypes.dtypes import (\r\n    DatetimeTZDtype,\r\n    ExtensionDtype,\r\n    PeriodDtype,\r\n)\r\nfrom pandas.core.dtypes.missing import array_equivalent\r\n\r\nfrom pandas.core import missing\r\nfrom pandas.core.algorithms import (\r\n    take,\r\n    unique,\r\n    value_counts,\r\n)\r\nfrom pandas.core.array_algos.quantile import quantile_with_mask\r\nfrom pandas.core.array_algos.transforms import shift\r\nfrom pandas.core.arrays.base import ExtensionArray\r\nfrom pandas.core.construction import extract_array\r\nfrom pandas.core.indexers import check_array_indexer\r\nfrom pandas.core.sorting import nargminmax\r\n\r\nif TYPE_CHECKING:\r\n    from pandas._typing import (\r\n        NumpySorter,\r\n        NumpyValueArrayLike,\r\n    )\r\n\r\n    from pandas import Series\r\n\r\n\r\ndef ravel_compat(meth: F) -> F:\r\n    \"\"\"\r\n    Decorator to ravel a 2D array before passing it to a cython operation,\r\n    then reshape the result to our own shape.\r\n    \"\"\"\r\n\r\n    @wraps(meth)\r\n    def method(self, *args, **kwargs):\r\n        if self.ndim == 1:\r\n            return meth(self, *args, **kwargs)\r\n\r\n        flags = self._ndarray.flags\r\n        flat = self.ravel(\"K\")\r\n        result = meth(flat, *args, **kwargs)\r\n        order = \"F\" if flags.f_contiguous else \"C\"\r\n        return result.reshape(self.shape, order=order)\r\n\r\n    return cast(F, method)\r\n\r\n\r\nclass NDArrayBackedExtensionArray(NDArrayBacked, ExtensionArray):\r\n    \"\"\"\r\n    ExtensionArray that is backed by a single NumPy ndarray.\r\n    \"\"\"\r\n\r\n    _ndarray: np.ndarray\r\n\r\n    # scalar used to denote NA value inside our self._ndarray, e.g. -1\r\n    #  for Categorical, iNaT for Period. Outside of object dtype,\r\n    #  self.isna() should be exactly locations in self._ndarray with\r\n    #  _internal_fill_value.\r\n    _internal_fill_value: Any\r\n\r\n    def _box_func(self, x):\r\n        \"\"\"\r\n        Wrap numpy type in our dtype.type if necessary.\r\n        \"\"\"\r\n        return x\r\n\r\n    def _validate_scalar(self, value):\r\n        # used by NDArrayBackedExtensionIndex.insert\r\n        raise AbstractMethodError(self)\r\n\r\n    # ------------------------------------------------------------------------\r\n\r\n    def view(self, dtype: Dtype | None = None) -> ArrayLike:\r\n        # We handle datetime64, datetime64tz, timedelta64, and period\r\n        #  dtypes here. Everything else we pass through to the underlying\r\n        #  ndarray.\r\n        if dtype is None or dtype is self.dtype:\r\n            return self._from_backing_data(self._ndarray)\r\n\r\n        if isinstance(dtype, type):\r\n            # we sometimes pass non-dtype objects, e.g np.ndarray;\r\n            #  pass those through to the underlying ndarray\r\n            return self._ndarray.view(dtype)\r\n\r\n        dtype = pandas_dtype(dtype)\r\n        arr = self._ndarray\r\n\r\n        if isinstance(dtype, (PeriodDtype, DatetimeTZDtype)):\r\n            cls = dtype.construct_array_type()\r\n            return cls(arr.view(\"i8\"), dtype=dtype)\r\n        elif dtype == \"M8[ns]\":\r\n            from pandas.core.arrays import DatetimeArray\r\n\r\n            return DatetimeArray(arr.view(\"i8\"), dtype=dtype)\r\n        elif dtype == \"m8[ns]\":\r\n            from pandas.core.arrays import TimedeltaArray\r\n\r\n            return TimedeltaArray(arr.view(\"i8\"), dtype=dtype)\r\n\r\n        # error: Argument \"dtype\" to \"view\" of \"_ArrayOrScalarCommon\" has incompatible\r\n        # type \"Union[ExtensionDtype, dtype[Any]]\"; expected \"Union[dtype[Any], None,\r\n        # type, _SupportsDType, str, Union[Tuple[Any, int], Tuple[Any, Union[int,\r\n        # Sequence[int]]], List[Any], _DTypeDict, Tuple[Any, Any]]]\"\r\n        return arr.view(dtype=dtype)  # type: ignore[arg-type]\r\n\r\n    def take(\r\n        self,\r\n        indices: TakeIndexer,\r\n        *,\r\n        allow_fill: bool = False,\r\n        fill_value: Any = None,\r\n        axis: AxisInt = 0,\r\n    ) -> Self:\r\n        if allow_fill:\r\n            fill_value = self._validate_scalar(fill_value)\r\n\r\n        new_data = take(\r\n            self._ndarray,\r\n            indices,\r\n            allow_fill=allow_fill,\r\n            fill_value=fill_value,\r\n            axis=axis,\r\n        )\r\n        return self._from_backing_data(new_data)\r\n\r\n    # ------------------------------------------------------------------------\r\n\r\n    def equals(self, other) -> bool:\r\n        if type(self) is not type(other):\r\n            return False\r\n        if not is_dtype_equal(self.dtype, other.dtype):\r\n            return False\r\n        return bool(array_equivalent(self._ndarray, other._ndarray))\r\n\r\n    @classmethod\r\n    def _from_factorized(cls, values, original):\r\n        assert values.dtype == original._ndarray.dtype\r\n        return original._from_backing_data(values)\r\n\r\n    def _values_for_argsort(self) -> np.ndarray:\r\n        return self._ndarray\r\n\r\n    def _values_for_factorize(self):\r\n        return self._ndarray, self._internal_fill_value\r\n\r\n    def _hash_pandas_object(\r\n        self, *, encoding: str, hash_key: str, categorize: bool\r\n    ) -> npt.NDArray[np.uint64]:\r\n        from pandas.core.util.hashing import hash_array\r\n\r\n        values = self._ndarray\r\n        return hash_array(\r\n            values, encoding=encoding, hash_key=hash_key, categorize=categorize\r\n        )\r\n\r\n    # Signature of \"argmin\" incompatible with supertype \"ExtensionArray\"\r\n    def argmin(self, axis: AxisInt = 0, skipna: bool = True):  # type: ignore[override]\r\n        # override base class by adding axis keyword\r\n        validate_bool_kwarg(skipna, \"skipna\")\r\n        if not skipna and self._hasna:\r\n            raise NotImplementedError\r\n        return nargminmax(self, \"argmin\", axis=axis)\r\n\r\n    # Signature of \"argmax\" incompatible with supertype \"ExtensionArray\"\r\n    def argmax(self, axis: AxisInt = 0, skipna: bool = True):  # type: ignore[override]\r\n        # override base class by adding axis keyword\r\n        validate_bool_kwarg(skipna, \"skipna\")\r\n        if not skipna and self._hasna:\r\n            raise NotImplementedError\r\n        return nargminmax(self, \"argmax\", axis=axis)\r\n\r\n    def unique(self) -> Self:\r\n        new_data = unique(self._ndarray)\r\n        return self._from_backing_data(new_data)\r\n\r\n    @classmethod\r\n    @doc(ExtensionArray._concat_same_type)\r\n    def _concat_same_type(\r\n        cls,\r\n        to_concat: Sequence[Self],\r\n        axis: AxisInt = 0,\r\n    ) -> Self:\r\n        if not lib.dtypes_all_equal([x.dtype for x in to_concat]):\r\n            dtypes = {str(x.dtype) for x in to_concat}\r\n            raise ValueError(\"to_concat must have the same dtype\", dtypes)\r\n\r\n        return super()._concat_same_type(to_concat, axis=axis)\r\n\r\n    @doc(ExtensionArray.searchsorted)\r\n    def searchsorted(\r\n        self,\r\n        value: NumpyValueArrayLike | ExtensionArray,\r\n        side: Literal[\"left\", \"right\"] = \"left\",\r\n        sorter: NumpySorter = None,\r\n    ) -> npt.NDArray[np.intp] | np.intp:\r\n        npvalue = self._validate_setitem_value(value)\r\n        return self._ndarray.searchsorted(npvalue, side=side, sorter=sorter)\r\n\r\n    @doc(ExtensionArray.shift)\r\n    def shift(self, periods: int = 1, fill_value=None, axis: AxisInt = 0):\r\n        fill_value = self._validate_scalar(fill_value)\r\n        new_values = shift(self._ndarray, periods, axis, fill_value)\r\n\r\n        return self._from_backing_data(new_values)\r\n\r\n    def __setitem__(self, key, value) -> None:\r\n        key = check_array_indexer(self, key)\r\n        value = self._validate_setitem_value(value)\r\n        self._ndarray[key] = value\r\n\r\n    def _validate_setitem_value(self, value):\r\n        return value\r\n\r\n    @overload\r\n    def __getitem__(self, key: ScalarIndexer) -> Any:\r\n        ...\r\n"]
[145.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[145.002, "i", "sed -n '1,240p' pandas/core/arrays/base.py\r"]
[145.004, "o", "sed -n '1,240p' pandas/core/arrays/base.py\r\n"]
[145.006, "o", "\u001b[?2004l\r\n\"\"\"\r\nAn interface for extending pandas with custom arrays.\r\n\r\n.. warning::\r\n\r\n   This is an experimental API and subject to breaking changes\r\n   without warning.\r\n\"\"\"\r\nfrom __future__ import annotations\r\n\r\nimport operator\r\nfrom typing import (\r\n    TYPE_CHECKING,\r\n    Any,\r\n    Callable,\r\n    ClassVar,\r\n    Iterator,\r\n    Literal,\r\n    Sequence,\r\n    cast,\r\n    overload,\r\n)\r\n\r\nimport numpy as np\r\n\r\nfrom pandas._libs import lib\r\nfrom pandas.compat import set_function_name\r\nfrom pandas.compat.numpy import function as nv\r\nfrom pandas.errors import AbstractMethodError\r\nfrom pandas.util._decorators import (\r\n    Appender,\r\n    Substitution,\r\n    cache_readonly,\r\n)\r\nfrom pandas.util._validators import (\r\n    validate_bool_kwarg,\r\n    validate_fillna_kwargs,\r\n    validate_insert_loc,\r\n)\r\n\r\nfrom pandas.core.dtypes.cast import maybe_cast_to_extension_array\r\nfrom pandas.core.dtypes.common import (\r\n    is_datetime64_dtype,\r\n    is_dtype_equal,\r\n    is_list_like,\r\n    is_scalar,\r\n    is_timedelta64_dtype,\r\n    pandas_dtype,\r\n)\r\nfrom pandas.core.dtypes.dtypes import ExtensionDtype\r\nfrom pandas.core.dtypes.generic import (\r\n    ABCDataFrame,\r\n    ABCIndex,\r\n    ABCSeries,\r\n)\r\nfrom pandas.core.dtypes.missing import isna\r\n\r\nfrom pandas.core import (\r\n    arraylike,\r\n    missing,\r\n    roperator,\r\n)\r\nfrom pandas.core.algorithms import (\r\n    factorize_array,\r\n    isin,\r\n    map_array,\r\n    mode,\r\n    rank,\r\n    unique,\r\n)\r\nfrom pandas.core.array_algos.quantile import quantile_with_mask\r\nfrom pandas.core.sorting import (\r\n    nargminmax,\r\n    nargsort,\r\n)\r\n\r\nif TYPE_CHECKING:\r\n    from pandas._typing import (\r\n        ArrayLike,\r\n        AstypeArg,\r\n        AxisInt,\r\n        Dtype,\r\n        FillnaOptions,\r\n        NumpySorter,\r\n        NumpyValueArrayLike,\r\n        PositionalIndexer,\r\n        ScalarIndexer,\r\n        Self,\r\n        SequenceIndexer,\r\n        Shape,\r\n        SortKind,\r\n        TakeIndexer,\r\n        npt,\r\n    )\r\n\r\n_extension_array_shared_docs: dict[str, str] = {}\r\n\r\n\r\nclass ExtensionArray:\r\n    \"\"\"\r\n    Abstract base class for custom 1-D array types.\r\n\r\n    pandas will recognize instances of this class as proper arrays\r\n    with a custom type and will not attempt to coerce them to objects. They\r\n    may be stored directly inside a :class:`DataFrame` or :class:`Series`.\r\n\r\n    Attributes\r\n    ----------\r\n    dtype\r\n    nbytes\r\n    ndim\r\n    shape\r\n\r\n    Methods\r\n    -------\r\n    argsort\r\n    astype\r\n    copy\r\n    dropna\r\n    factorize\r\n    fillna\r\n    equals\r\n    insert\r\n    isin\r\n    isna\r\n    ravel\r\n    repeat\r\n    searchsorted\r\n    shift\r\n    take\r\n    tolist\r\n    unique\r\n    view\r\n    _accumulate\r\n    _concat_same_type\r\n    _formatter\r\n    _from_factorized\r\n    _from_sequence\r\n    _from_sequence_of_strings\r\n    _hash_pandas_object\r\n    _reduce\r\n    _values_for_argsort\r\n    _values_for_factorize\r\n\r\n    Notes\r\n    -----\r\n    The interface includes the following abstract methods that must be\r\n    implemented by subclasses:\r\n\r\n    * _from_sequence\r\n    * _from_factorized\r\n    * __getitem__\r\n    * __len__\r\n    * __eq__\r\n    * dtype\r\n    * nbytes\r\n    * isna\r\n    * take\r\n    * copy\r\n    * _concat_same_type\r\n\r\n    A default repr displaying the type, (truncated) data, length,\r\n    and dtype is provided. It can be customized or replaced by\r\n    by overriding:\r\n\r\n    * __repr__ : A default repr for the ExtensionArray.\r\n    * _formatter : Print scalars inside a Series or DataFrame.\r\n\r\n    Some methods require casting the ExtensionArray to an ndarray of Python\r\n    objects with ``self.astype(object)``, which may be expensive. When\r\n    performance is a concern, we highly recommend overriding the following\r\n    methods:\r\n\r\n    * fillna\r\n    * dropna\r\n    * unique\r\n    * factorize / _values_for_factorize\r\n    * argsort, argmax, argmin / _values_for_argsort\r\n    * searchsorted\r\n    * map\r\n\r\n    The remaining methods implemented on this class should be performant,\r\n    as they only compose abstract methods. Still, a more efficient\r\n    implementation may be available, and these methods can be overridden.\r\n\r\n    One can implement methods to handle array accumulations or reductions.\r\n\r\n    * _accumulate\r\n    * _reduce\r\n\r\n    One can implement methods to handle parsing from strings that will be used\r\n    in methods such as ``pandas.io.parsers.read_csv``.\r\n\r\n    * _from_sequence_of_strings\r\n\r\n    This class does not inherit from 'abc.ABCMeta' for performance reasons.\r\n    Methods and properties required by the interface raise\r\n    ``pandas.errors.AbstractMethodError`` and no ``register`` method is\r\n    provided for registering virtual subclasses.\r\n\r\n    ExtensionArrays are limited to 1 dimension.\r\n\r\n    They may be backed by none, one, or many NumPy arrays. For example,\r\n    ``pandas.Categorical`` is an extension array backed by two arrays,\r\n    one for codes and one for categories. An array of IPv6 address may\r\n    be backed by a NumPy structured array with two fields, one for the\r\n    lower 64 bits and one for the upper 64 bits. Or they may be backed\r\n    by some other storage type, like Python lists. Pandas makes no\r\n    assumptions on how the data are stored, just that it can be converted\r\n    to a NumPy array.\r\n    The ExtensionArray interface does not impose any rules on how this data\r\n    is stored. However, currently, the backing data cannot be stored in\r\n    attributes called ``.values`` or ``._values`` to ensure full compatibility\r\n    with pandas internals. But other names as ``.data``, ``._data``,\r\n    ``._items``, ... can be freely used.\r\n\r\n    If implementing NumPy's ``__array_ufunc__`` interface, pandas expects\r\n    that\r\n\r\n    1. You defer by returning ``NotImplemented`` when any Series are present\r\n       in `inputs`. Pandas will extract the arrays and call the ufunc again.\r\n    2. You define a ``_HANDLED_TYPES`` tuple as an attribute on the class.\r\n       Pandas inspect this to determine whether the ufunc is valid for the\r\n       types present.\r\n\r\n    See :ref:`extending.extension.ufunc` for more.\r\n\r\n    By default, ExtensionArrays are not hashable.  Immutable subclasses may\r\n    override this behavior.\r\n    \"\"\"\r\n\r\n    # '_typ' is for pandas.core.dtypes.generic.ABCExtensionArray.\r\n    # Don't override this.\r\n    _typ = \"extension\"\r\n\r\n    # similar to __array_priority__, positions ExtensionArray after Index,\r\n    #  Series, and DataFrame.  EA subclasses may override to choose which EA\r\n    #  subclass takes priority. If overriding, the value should always be\r\n    #  strictly less than 2000 to be below Index.__pandas_priority__.\r\n    __pandas_priority__ = 1000\r\n"]
[150.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[150.002, "i", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '840,1040p'\r"]
[150.004, "o", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '840,1040p'\r\n"]
[150.006, "o", "\u001b[?2004l\r\n   840\t        # Currently, ExtensionArray.factorize -> Tuple[ndarray, EA]\r\n   841\t        # The sparsity on this is backwards from what Sparse would want. Want\r\n   842\t        # ExtensionArray.factorize -> Tuple[EA, EA]\r\n   843\t        # Given that we have to return a dense array of codes, why bother\r\n   844\t        # implementing an efficient factorize?\r\n   845\t        codes, uniques = algos.factorize(\r\n   846\t            np.asarray(self), use_na_sentinel=use_na_sentinel\r\n   847\t        )\r\n   848\t        uniques_sp = SparseArray(uniques, dtype=self.dtype)\r\n   849\t        return codes, uniques_sp\r\n   850\t\r\n   851\t    def value_counts(self, dropna: bool = True) -> Series:\r\n   852\t        \"\"\"\r\n   853\t        Returns a Series containing counts of unique values.\r\n   854\t\r\n   855\t        Parameters\r\n   856\t        ----------\r\n   857\t        dropna : bool, default True\r\n   858\t            Don't include counts of NaN, even if NaN is in sp_values.\r\n   859\t\r\n   860\t        Returns\r\n   861\t        -------\r\n   862\t        counts : Series\r\n   863\t        \"\"\"\r\n   864\t        from pandas import (\r\n   865\t            Index,\r\n   866\t            Series,\r\n   867\t        )\r\n   868\t\r\n   869\t        keys, counts = algos.value_counts_arraylike(self.sp_values, dropna=dropna)\r\n   870\t        fcounts = self.sp_index.ngaps\r\n   871\t        if fcounts > 0 and (not self._null_fill_value or not dropna):\r\n   872\t            mask = isna(keys) if self._null_fill_value else keys == self.fill_value\r\n   873\t            if mask.any():\r\n   874\t                counts[mask] += fcounts\r\n   875\t            else:\r\n   876\t                # error: Argument 1 to \"insert\" has incompatible type \"Union[\r\n   877\t                # ExtensionArray,ndarray[Any, Any]]\"; expected \"Union[\r\n   878\t                # _SupportsArray[dtype[Any]], Sequence[_SupportsArray[dtype\r\n   879\t                # [Any]]], Sequence[Sequence[_SupportsArray[dtype[Any]]]],\r\n   880\t                # Sequence[Sequence[Sequence[_SupportsArray[dtype[Any]]]]], Sequence\r\n   881\t                # [Sequence[Sequence[Sequence[_SupportsArray[dtype[Any]]]]]]]\"\r\n   882\t                keys = np.insert(keys, 0, self.fill_value)  # type: ignore[arg-type]\r\n   883\t                counts = np.insert(counts, 0, fcounts)\r\n   884\t\r\n   885\t        if not isinstance(keys, ABCIndex):\r\n   886\t            index = Index(keys)\r\n   887\t        else:\r\n   888\t            index = keys\r\n   889\t        return Series(counts, index=index, copy=False)\r\n   890\t\r\n   891\t    # --------\r\n   892\t    # Indexing\r\n   893\t    # --------\r\n   894\t    @overload\r\n   895\t    def __getitem__(self, key: ScalarIndexer) -> Any:\r\n   896\t        ...\r\n   897\t\r\n   898\t    @overload\r\n   899\t    def __getitem__(\r\n   900\t        self,\r\n   901\t        key: SequenceIndexer | tuple[int | ellipsis, ...],\r\n   902\t    ) -> Self:\r\n   903\t        ...\r\n   904\t\r\n   905\t    def __getitem__(\r\n   906\t        self,\r\n   907\t        key: PositionalIndexer | tuple[int | ellipsis, ...],\r\n   908\t    ) -> Self | Any:\r\n   909\t        if isinstance(key, tuple):\r\n   910\t            key = unpack_tuple_and_ellipses(key)\r\n   911\t            if key is Ellipsis:\r\n   912\t                raise ValueError(\"Cannot slice with Ellipsis\")\r\n   913\t\r\n   914\t        if is_integer(key):\r\n   915\t            return self._get_val_at(key)\r\n   916\t        elif isinstance(key, tuple):\r\n   917\t            # error: Invalid index type \"Tuple[Union[int, ellipsis], ...]\"\r\n   918\t            # for \"ndarray[Any, Any]\"; expected type\r\n   919\t            # \"Union[SupportsIndex, _SupportsArray[dtype[Union[bool_,\r\n   920\t            # integer[Any]]]], _NestedSequence[_SupportsArray[dtype[\r\n   921\t            # Union[bool_, integer[Any]]]]], _NestedSequence[Union[\r\n   922\t            # bool, int]], Tuple[Union[SupportsIndex, _SupportsArray[\r\n   923\t            # dtype[Union[bool_, integer[Any]]]], _NestedSequence[\r\n   924\t            # _SupportsArray[dtype[Union[bool_, integer[Any]]]]],\r\n   925\t            # _NestedSequence[Union[bool, int]]], ...]]\"\r\n   926\t            data_slice = self.to_dense()[key]  # type: ignore[index]\r\n   927\t        elif isinstance(key, slice):\r\n   928\t            # Avoid densifying when handling contiguous slices\r\n   929\t            if key.step is None or key.step == 1:\r\n   930\t                start = 0 if key.start is None else key.start\r\n   931\t                if start < 0:\r\n   932\t                    start += len(self)\r\n   933\t\r\n   934\t                end = len(self) if key.stop is None else key.stop\r\n   935\t                if end < 0:\r\n   936\t                    end += len(self)\r\n   937\t\r\n   938\t                indices = self.sp_index.indices\r\n   939\t                keep_inds = np.flatnonzero((indices >= start) & (indices < end))\r\n   940\t                sp_vals = self.sp_values[keep_inds]\r\n   941\t\r\n   942\t                sp_index = indices[keep_inds].copy()\r\n   943\t\r\n   944\t                # If we've sliced to not include the start of the array, all our indices\r\n   945\t                # should be shifted. NB: here we are careful to also not shift by a\r\n   946\t                # negative value for a case like [0, 1][-100:] where the start index\r\n   947\t                # should be treated like 0\r\n   948\t                if start > 0:\r\n   949\t                    sp_index -= start\r\n   950\t\r\n   951\t                # Length of our result should match applying this slice to a range\r\n   952\t                # of the length of our original array\r\n   953\t                new_len = len(range(len(self))[key])\r\n   954\t                new_sp_index = make_sparse_index(new_len, sp_index, self.kind)\r\n   955\t                return type(self)._simple_new(sp_vals, new_sp_index, self.dtype)\r\n   956\t            else:\r\n   957\t                indices = np.arange(len(self), dtype=np.int32)[key]\r\n   958\t                return self.take(indices)\r\n   959\t\r\n   960\t        elif not is_list_like(key):\r\n   961\t            # e.g. \"foo\" or 2.5\r\n   962\t            # exception message copied from numpy\r\n   963\t            raise IndexError(\r\n   964\t                r\"only integers, slices (`:`), ellipsis (`...`), numpy.newaxis \"\r\n   965\t                r\"(`None`) and integer or boolean arrays are valid indices\"\r\n   966\t            )\r\n   967\t\r\n   968\t        else:\r\n   969\t            if isinstance(key, SparseArray):\r\n   970\t                # NOTE: If we guarantee that SparseDType(bool)\r\n   971\t                # has only fill_value - true, false or nan\r\n   972\t                # (see GH PR 44955)\r\n   973\t                # we can apply mask very fast:\r\n   974\t                if is_bool_dtype(key):\r\n   975\t                    if isna(key.fill_value):\r\n   976\t                        return self.take(key.sp_index.indices[key.sp_values])\r\n   977\t                    if not key.fill_value:\r\n   978\t                        return self.take(key.sp_index.indices)\r\n   979\t                    n = len(self)\r\n   980\t                    mask = np.full(n, True, dtype=np.bool_)\r\n   981\t                    mask[key.sp_index.indices] = False\r\n   982\t                    return self.take(np.arange(n)[mask])\r\n   983\t                else:\r\n   984\t                    key = np.asarray(key)\r\n   985\t\r\n   986\t            key = check_array_indexer(self, key)\r\n   987\t\r\n   988\t            if com.is_bool_indexer(key):\r\n   989\t                # mypy doesn't know we have an array here\r\n   990\t                key = cast(np.ndarray, key)\r\n   991\t                return self.take(np.arange(len(key), dtype=np.int32)[key])\r\n   992\t            elif hasattr(key, \"__len__\"):\r\n   993\t                return self.take(key)\r\n   994\t            else:\r\n   995\t                raise ValueError(f\"Cannot slice with '{key}'\")\r\n   996\t\r\n   997\t        return type(self)(data_slice, kind=self.kind)\r\n   998\t\r\n   999\t    def _get_val_at(self, loc):\r\n  1000\t        loc = validate_insert_loc(loc, len(self))\r\n  1001\t\r\n  1002\t        sp_loc = self.sp_index.lookup(loc)\r\n  1003\t        if sp_loc == -1:\r\n  1004\t            return self.fill_value\r\n  1005\t        else:\r\n  1006\t            val = self.sp_values[sp_loc]\r\n  1007\t            val = maybe_box_datetimelike(val, self.sp_values.dtype)\r\n  1008\t            return val\r\n  1009\t\r\n  1010\t    def take(self, indices, *, allow_fill: bool = False, fill_value=None) -> Self:\r\n  1011\t        if is_scalar(indices):\r\n  1012\t            raise ValueError(f\"'indices' must be an array, not a scalar '{indices}'.\")\r\n  1013\t        indices = np.asarray(indices, dtype=np.int32)\r\n  1014\t\r\n  1015\t        dtype = None\r\n  1016\t        if indices.size == 0:\r\n  1017\t            result = np.array([], dtype=\"object\")\r\n  1018\t            dtype = self.dtype\r\n  1019\t        elif allow_fill:\r\n  1020\t            result = self._take_with_fill(indices, fill_value=fill_value)\r\n  1021\t        else:\r\n  1022\t            return self._take_without_fill(indices)\r\n  1023\t\r\n  1024\t        return type(self)(\r\n  1025\t            result, fill_value=self.fill_value, kind=self.kind, dtype=dtype\r\n  1026\t        )\r\n  1027\t\r\n  1028\t    def _take_with_fill(self, indices, fill_value=None) -> np.ndarray:\r\n  1029\t        if fill_value is None:\r\n  1030\t            fill_value = self.dtype.na_value\r\n  1031\t\r\n  1032\t        if indices.min() < -1:\r\n  1033\t            raise ValueError(\r\n  1034\t                \"Invalid value in 'indices'. Must be between -1 \"\r\n  1035\t                \"and the length of the array.\"\r\n  1036\t            )\r\n  1037\t\r\n  1038\t        if indices.max() >= len(self):\r\n  1039\t            raise IndexError(\"out of bounds value in 'indices'.\")\r\n  1040\t\r\n"]
[155.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[155.002, "i", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '1040,1240p'\r"]
[155.004, "o", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '1040,1240p'\r\n"]
[155.006, "o", "\u001b[?2004l\r\n  1040\t\r\n  1041\t        if len(self) == 0:\r\n  1042\t            # Empty... Allow taking only if all empty\r\n  1043\t            if (indices == -1).all():\r\n  1044\t                dtype = np.result_type(self.sp_values, type(fill_value))\r\n  1045\t                taken = np.empty_like(indices, dtype=dtype)\r\n  1046\t                taken.fill(fill_value)\r\n  1047\t                return taken\r\n  1048\t            else:\r\n  1049\t                raise IndexError(\"cannot do a non-empty take from an empty axes.\")\r\n  1050\t\r\n  1051\t        # sp_indexer may be -1 for two reasons\r\n  1052\t        # 1.) we took for an index of -1 (new)\r\n  1053\t        # 2.) we took a value that was self.fill_value (old)\r\n  1054\t        sp_indexer = self.sp_index.lookup_array(indices)\r\n  1055\t        new_fill_indices = indices == -1\r\n  1056\t        old_fill_indices = (sp_indexer == -1) & ~new_fill_indices\r\n  1057\t\r\n  1058\t        if self.sp_index.npoints == 0 and old_fill_indices.all():\r\n  1059\t            # We've looked up all valid points on an all-sparse array.\r\n  1060\t            taken = np.full(\r\n  1061\t                sp_indexer.shape, fill_value=self.fill_value, dtype=self.dtype.subtype\r\n  1062\t            )\r\n  1063\t\r\n  1064\t        elif self.sp_index.npoints == 0:\r\n  1065\t            # Avoid taking from the empty self.sp_values\r\n  1066\t            _dtype = np.result_type(self.dtype.subtype, type(fill_value))\r\n  1067\t            taken = np.full(sp_indexer.shape, fill_value=fill_value, dtype=_dtype)\r\n  1068\t        else:\r\n  1069\t            taken = self.sp_values.take(sp_indexer)\r\n  1070\t\r\n  1071\t            # Fill in two steps.\r\n  1072\t            # Old fill values\r\n  1073\t            # New fill values\r\n  1074\t            # potentially coercing to a new dtype at each stage.\r\n  1075\t\r\n  1076\t            m0 = sp_indexer[old_fill_indices] < 0\r\n  1077\t            m1 = sp_indexer[new_fill_indices] < 0\r\n  1078\t\r\n  1079\t            result_type = taken.dtype\r\n  1080\t\r\n  1081\t            if m0.any():\r\n  1082\t                result_type = np.result_type(result_type, type(self.fill_value))\r\n  1083\t                taken = taken.astype(result_type)\r\n  1084\t                taken[old_fill_indices] = self.fill_value\r\n  1085\t\r\n  1086\t            if m1.any():\r\n  1087\t                result_type = np.result_type(result_type, type(fill_value))\r\n  1088\t                taken = taken.astype(result_type)\r\n  1089\t                taken[new_fill_indices] = fill_value\r\n  1090\t\r\n  1091\t        return taken\r\n  1092\t\r\n  1093\t    def _take_without_fill(self, indices) -> Self:\r\n  1094\t        to_shift = indices < 0\r\n  1095\t\r\n  1096\t        n = len(self)\r\n  1097\t\r\n  1098\t        if (indices.max() >= n) or (indices.min() < -n):\r\n  1099\t            if n == 0:\r\n  1100\t                raise IndexError(\"cannot do a non-empty take from an empty axes.\")\r\n  1101\t            raise IndexError(\"out of bounds value in 'indices'.\")\r\n  1102\t\r\n  1103\t        if to_shift.any():\r\n  1104\t            indices = indices.copy()\r\n  1105\t            indices[to_shift] += n\r\n  1106\t\r\n  1107\t        sp_indexer = self.sp_index.lookup_array(indices)\r\n  1108\t        value_mask = sp_indexer != -1\r\n  1109\t        new_sp_values = self.sp_values[sp_indexer[value_mask]]\r\n  1110\t\r\n  1111\t        value_indices = np.flatnonzero(value_mask).astype(np.int32, copy=False)\r\n  1112\t\r\n  1113\t        new_sp_index = make_sparse_index(len(indices), value_indices, kind=self.kind)\r\n  1114\t        return type(self)._simple_new(new_sp_values, new_sp_index, dtype=self.dtype)\r\n  1115\t\r\n  1116\t    def searchsorted(\r\n  1117\t        self,\r\n  1118\t        v: ArrayLike | object,\r\n  1119\t        side: Literal[\"left\", \"right\"] = \"left\",\r\n  1120\t        sorter: NumpySorter = None,\r\n  1121\t    ) -> npt.NDArray[np.intp] | np.intp:\r\n  1122\t        msg = \"searchsorted requires high memory usage.\"\r\n  1123\t        warnings.warn(msg, PerformanceWarning, stacklevel=find_stack_level())\r\n  1124\t        if not is_scalar(v):\r\n  1125\t            v = np.asarray(v)\r\n  1126\t        v = np.asarray(v)\r\n  1127\t        return np.asarray(self, dtype=self.dtype.subtype).searchsorted(v, side, sorter)\r\n  1128\t\r\n  1129\t    def copy(self) -> Self:\r\n  1130\t        values = self.sp_values.copy()\r\n  1131\t        return self._simple_new(values, self.sp_index, self.dtype)\r\n  1132\t\r\n  1133\t    @classmethod\r\n  1134\t    def _concat_same_type(cls, to_concat: Sequence[Self]) -> Self:\r\n  1135\t        fill_value = to_concat[0].fill_value\r\n  1136\t\r\n  1137\t        values = []\r\n  1138\t        length = 0\r\n  1139\t\r\n  1140\t        if to_concat:\r\n  1141\t            sp_kind = to_concat[0].kind\r\n  1142\t        else:\r\n  1143\t            sp_kind = \"integer\"\r\n  1144\t\r\n  1145\t        sp_index: SparseIndex\r\n  1146\t        if sp_kind == \"integer\":\r\n  1147\t            indices = []\r\n  1148\t\r\n  1149\t            for arr in to_concat:\r\n  1150\t                int_idx = arr.sp_index.indices.copy()\r\n  1151\t                int_idx += length  # TODO: wraparound\r\n  1152\t                length += arr.sp_index.length\r\n  1153\t\r\n  1154\t                values.append(arr.sp_values)\r\n  1155\t                indices.append(int_idx)\r\n  1156\t\r\n  1157\t            data = np.concatenate(values)\r\n  1158\t            indices_arr = np.concatenate(indices)\r\n  1159\t            # error: Argument 2 to \"IntIndex\" has incompatible type\r\n  1160\t            # \"ndarray[Any, dtype[signedinteger[_32Bit]]]\";\r\n  1161\t            # expected \"Sequence[int]\"\r\n  1162\t            sp_index = IntIndex(length, indices_arr)  # type: ignore[arg-type]\r\n  1163\t\r\n  1164\t        else:\r\n  1165\t            # when concatenating block indices, we don't claim that you'll\r\n  1166\t            # get an identical index as concatenating the values and then\r\n  1167\t            # creating a new index. We don't want to spend the time trying\r\n  1168\t            # to merge blocks across arrays in `to_concat`, so the resulting\r\n  1169\t            # BlockIndex may have more blocks.\r\n  1170\t            blengths = []\r\n  1171\t            blocs = []\r\n  1172\t\r\n  1173\t            for arr in to_concat:\r\n  1174\t                block_idx = arr.sp_index.to_block_index()\r\n  1175\t\r\n  1176\t                values.append(arr.sp_values)\r\n  1177\t                blocs.append(block_idx.blocs.copy() + length)\r\n  1178\t                blengths.append(block_idx.blengths)\r\n  1179\t                length += arr.sp_index.length\r\n  1180\t\r\n  1181\t            data = np.concatenate(values)\r\n  1182\t            blocs_arr = np.concatenate(blocs)\r\n  1183\t            blengths_arr = np.concatenate(blengths)\r\n  1184\t\r\n  1185\t            sp_index = BlockIndex(length, blocs_arr, blengths_arr)\r\n  1186\t\r\n  1187\t        return cls(data, sparse_index=sp_index, fill_value=fill_value)\r\n  1188\t\r\n  1189\t    def astype(self, dtype: AstypeArg | None = None, copy: bool = True):\r\n  1190\t        \"\"\"\r\n  1191\t        Change the dtype of a SparseArray.\r\n  1192\t\r\n  1193\t        The output will always be a SparseArray. To convert to a dense\r\n  1194\t        ndarray with a certain dtype, use :meth:`numpy.asarray`.\r\n  1195\t\r\n  1196\t        Parameters\r\n  1197\t        ----------\r\n  1198\t        dtype : np.dtype or ExtensionDtype\r\n  1199\t            For SparseDtype, this changes the dtype of\r\n  1200\t            ``self.sp_values`` and the ``self.fill_value``.\r\n  1201\t\r\n  1202\t            For other dtypes, this only changes the dtype of\r\n  1203\t            ``self.sp_values``.\r\n  1204\t\r\n  1205\t        copy : bool, default True\r\n  1206\t            Whether to ensure a copy is made, even if not necessary.\r\n  1207\t\r\n  1208\t        Returns\r\n  1209\t        -------\r\n  1210\t        SparseArray\r\n  1211\t\r\n  1212\t        Examples\r\n  1213\t        --------\r\n  1214\t        >>> arr = pd.arrays.SparseArray([0, 0, 1, 2])\r\n  1215\t        >>> arr\r\n  1216\t        [0, 0, 1, 2]\r\n  1217\t        Fill: 0\r\n  1218\t        IntIndex\r\n  1219\t        Indices: array([2, 3], dtype=int32)\r\n  1220\t\r\n  1221\t        >>> arr.astype(SparseDtype(np.dtype('int32')))\r\n  1222\t        [0, 0, 1, 2]\r\n  1223\t        Fill: 0\r\n  1224\t        IntIndex\r\n  1225\t        Indices: array([2, 3], dtype=int32)\r\n  1226\t\r\n  1227\t        Using a NumPy dtype with a different kind (e.g. float) will coerce\r\n  1228\t        just ``self.sp_values``.\r\n  1229\t\r\n  1230\t        >>> arr.astype(SparseDtype(np.dtype('float64')))\r\n  1231\t        ... # doctest: +NORMALIZE_WHITESPACE\r\n  1232\t        [nan, nan, 1.0, 2.0]\r\n  1233\t        Fill: nan\r\n  1234\t        IntIndex\r\n  1235\t        Indices: array([2, 3], dtype=int32)\r\n  1236\t\r\n  1237\t        Using a SparseDtype, you can also change the fill value as well.\r\n  1238\t\r\n  1239\t        >>> arr.astype(SparseDtype(\"float64\", fill_value=0.0))\r\n  1240\t        ... # doctest: +NORMALIZE_WHITESPACE\r\n"]
[160.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[160.002, "i", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '1240,1500p'\r"]
[160.004, "o", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '1240,1500p'\r\n"]
[160.006, "o", "\u001b[?2004l\r\n  1240\t        ... # doctest: +NORMALIZE_WHITESPACE\r\n  1241\t        [0.0, 0.0, 1.0, 2.0]\r\n  1242\t        Fill: 0.0\r\n  1243\t        IntIndex\r\n  1244\t        Indices: array([2, 3], dtype=int32)\r\n  1245\t        \"\"\"\r\n  1246\t        if is_dtype_equal(dtype, self._dtype):\r\n  1247\t            if not copy:\r\n  1248\t                return self\r\n  1249\t            else:\r\n  1250\t                return self.copy()\r\n  1251\t\r\n  1252\t        future_dtype = pandas_dtype(dtype)\r\n  1253\t        if not isinstance(future_dtype, SparseDtype):\r\n  1254\t            # GH#34457\r\n  1255\t            values = np.asarray(self)\r\n  1256\t            values = ensure_wrapped_if_datetimelike(values)\r\n  1257\t            return astype_array(values, dtype=future_dtype, copy=False)\r\n  1258\t\r\n  1259\t        dtype = self.dtype.update_dtype(dtype)\r\n  1260\t        subtype = pandas_dtype(dtype._subtype_with_str)\r\n  1261\t        subtype = cast(np.dtype, subtype)  # ensured by update_dtype\r\n  1262\t        values = ensure_wrapped_if_datetimelike(self.sp_values)\r\n  1263\t        sp_values = astype_array(values, subtype, copy=copy)\r\n  1264\t        sp_values = np.asarray(sp_values)\r\n  1265\t\r\n  1266\t        return self._simple_new(sp_values, self.sp_index, dtype)\r\n  1267\t\r\n  1268\t    def map(self, mapper, na_action=None) -> Self:\r\n  1269\t        \"\"\"\r\n  1270\t        Map categories using an input mapping or function.\r\n  1271\t\r\n  1272\t        Parameters\r\n  1273\t        ----------\r\n  1274\t        mapper : dict, Series, callable\r\n  1275\t            The correspondence from old values to new.\r\n  1276\t        na_action : {None, 'ignore'}, default None\r\n  1277\t            If 'ignore', propagate NA values, without passing them to the\r\n  1278\t            mapping correspondence.\r\n  1279\t\r\n  1280\t        Returns\r\n  1281\t        -------\r\n  1282\t        SparseArray\r\n  1283\t            The output array will have the same density as the input.\r\n  1284\t            The output fill value will be the result of applying the\r\n  1285\t            mapping to ``self.fill_value``\r\n  1286\t\r\n  1287\t        Examples\r\n  1288\t        --------\r\n  1289\t        >>> arr = pd.arrays.SparseArray([0, 1, 2])\r\n  1290\t        >>> arr.map(lambda x: x + 10)\r\n  1291\t        [10, 11, 12]\r\n  1292\t        Fill: 10\r\n  1293\t        IntIndex\r\n  1294\t        Indices: array([1, 2], dtype=int32)\r\n  1295\t\r\n  1296\t        >>> arr.map({0: 10, 1: 11, 2: 12})\r\n  1297\t        [10, 11, 12]\r\n  1298\t        Fill: 10\r\n  1299\t        IntIndex\r\n  1300\t        Indices: array([1, 2], dtype=int32)\r\n  1301\t\r\n  1302\t        >>> arr.map(pd.Series([10, 11, 12], index=[0, 1, 2]))\r\n  1303\t        [10, 11, 12]\r\n  1304\t        Fill: 10\r\n  1305\t        IntIndex\r\n  1306\t        Indices: array([1, 2], dtype=int32)\r\n  1307\t        \"\"\"\r\n  1308\t        is_map = isinstance(mapper, (abc.Mapping, ABCSeries))\r\n  1309\t\r\n  1310\t        fill_val = self.fill_value\r\n  1311\t\r\n  1312\t        if na_action is None or notna(fill_val):\r\n  1313\t            fill_val = mapper.get(fill_val, fill_val) if is_map else mapper(fill_val)\r\n  1314\t\r\n  1315\t        def func(sp_val):\r\n  1316\t            new_sp_val = mapper.get(sp_val, None) if is_map else mapper(sp_val)\r\n  1317\t            # check identity and equality because nans are not equal to each other\r\n  1318\t            if new_sp_val is fill_val or new_sp_val == fill_val:\r\n  1319\t                msg = \"fill value in the sparse values not supported\"\r\n  1320\t                raise ValueError(msg)\r\n  1321\t            return new_sp_val\r\n  1322\t\r\n  1323\t        sp_values = [func(x) for x in self.sp_values]\r\n  1324\t\r\n  1325\t        return type(self)(sp_values, sparse_index=self.sp_index, fill_value=fill_val)\r\n  1326\t\r\n  1327\t    def to_dense(self) -> np.ndarray:\r\n  1328\t        \"\"\"\r\n  1329\t        Convert SparseArray to a NumPy array.\r\n  1330\t\r\n  1331\t        Returns\r\n  1332\t        -------\r\n  1333\t        arr : NumPy array\r\n  1334\t        \"\"\"\r\n  1335\t        return np.asarray(self, dtype=self.sp_values.dtype)\r\n  1336\t\r\n  1337\t    def _where(self, mask, value):\r\n  1338\t        # NB: may not preserve dtype, e.g. result may be Sparse[float64]\r\n  1339\t        #  while self is Sparse[int64]\r\n  1340\t        naive_implementation = np.where(mask, self, value)\r\n  1341\t        dtype = SparseDtype(naive_implementation.dtype, fill_value=self.fill_value)\r\n  1342\t        result = type(self)._from_sequence(naive_implementation, dtype=dtype)\r\n  1343\t        return result\r\n  1344\t\r\n  1345\t    # ------------------------------------------------------------------------\r\n  1346\t    # IO\r\n  1347\t    # ------------------------------------------------------------------------\r\n  1348\t    def __setstate__(self, state) -> None:\r\n  1349\t        \"\"\"Necessary for making this object picklable\"\"\"\r\n  1350\t        if isinstance(state, tuple):\r\n  1351\t            # Compat for pandas < 0.24.0\r\n  1352\t            nd_state, (fill_value, sp_index) = state\r\n  1353\t            sparse_values = np.array([])\r\n  1354\t            sparse_values.__setstate__(nd_state)\r\n  1355\t\r\n  1356\t            self._sparse_values = sparse_values\r\n  1357\t            self._sparse_index = sp_index\r\n  1358\t            self._dtype = SparseDtype(sparse_values.dtype, fill_value)\r\n  1359\t        else:\r\n  1360\t            self.__dict__.update(state)\r\n  1361\t\r\n  1362\t    def nonzero(self) -> tuple[npt.NDArray[np.int32]]:\r\n  1363\t        if self.fill_value == 0:\r\n  1364\t            return (self.sp_index.indices,)\r\n  1365\t        else:\r\n  1366\t            return (self.sp_index.indices[self.sp_values != 0],)\r\n  1367\t\r\n  1368\t    # ------------------------------------------------------------------------\r\n  1369\t    # Reductions\r\n  1370\t    # ------------------------------------------------------------------------\r\n  1371\t\r\n  1372\t    def _reduce(self, name: str, *, skipna: bool = True, **kwargs):\r\n  1373\t        method = getattr(self, name, None)\r\n  1374\t\r\n  1375\t        if method is None:\r\n  1376\t            raise TypeError(f\"cannot perform {name} with type {self.dtype}\")\r\n  1377\t\r\n  1378\t        if skipna:\r\n  1379\t            arr = self\r\n  1380\t        else:\r\n  1381\t            arr = self.dropna()\r\n  1382\t\r\n  1383\t        return getattr(arr, name)(**kwargs)\r\n  1384\t\r\n  1385\t    def all(self, axis=None, *args, **kwargs):\r\n  1386\t        \"\"\"\r\n  1387\t        Tests whether all elements evaluate True\r\n  1388\t\r\n  1389\t        Returns\r\n  1390\t        -------\r\n  1391\t        all : bool\r\n  1392\t\r\n  1393\t        See Also\r\n  1394\t        --------\r\n  1395\t        numpy.all\r\n  1396\t        \"\"\"\r\n  1397\t        nv.validate_all(args, kwargs)\r\n  1398\t\r\n  1399\t        values = self.sp_values\r\n  1400\t\r\n  1401\t        if len(values) != len(self) and not np.all(self.fill_value):\r\n  1402\t            return False\r\n  1403\t\r\n  1404\t        return values.all()\r\n  1405\t\r\n  1406\t    def any(self, axis: AxisInt = 0, *args, **kwargs):\r\n  1407\t        \"\"\"\r\n  1408\t        Tests whether at least one of elements evaluate True\r\n  1409\t\r\n  1410\t        Returns\r\n  1411\t        -------\r\n  1412\t        any : bool\r\n  1413\t\r\n  1414\t        See Also\r\n  1415\t        --------\r\n  1416\t        numpy.any\r\n  1417\t        \"\"\"\r\n  1418\t        nv.validate_any(args, kwargs)\r\n  1419\t\r\n  1420\t        values = self.sp_values\r\n  1421\t\r\n  1422\t        if len(values) != len(self) and np.any(self.fill_value):\r\n  1423\t            return True\r\n  1424\t\r\n  1425\t        return values.any().item()\r\n  1426\t\r\n  1427\t    def sum(\r\n  1428\t        self,\r\n  1429\t        axis: AxisInt = 0,\r\n  1430\t        min_count: int = 0,\r\n  1431\t        skipna: bool = True,\r\n  1432\t        *args,\r\n  1433\t        **kwargs,\r\n  1434\t    ) -> Scalar:\r\n  1435\t        \"\"\"\r\n  1436\t        Sum of non-NA/null values\r\n  1437\t\r\n  1438\t        Parameters\r\n  1439\t        ----------\r\n  1440\t        axis : int, default 0\r\n  1441\t            Not Used. NumPy compatibility.\r\n  1442\t        min_count : int, default 0\r\n  1443\t            The required number of valid values to perform the summation. If fewer\r\n  1444\t            than ``min_count`` valid values are present, the result will be the missing\r\n  1445\t            value indicator for subarray type.\r\n  1446\t        *args, **kwargs\r\n  1447\t            Not Used. NumPy compatibility.\r\n  1448\t\r\n  1449\t        Returns\r\n  1450\t        -------\r\n  1451\t        scalar\r\n  1452\t        \"\"\"\r\n  1453\t        nv.validate_sum(args, kwargs)\r\n  1454\t        valid_vals = self._valid_sp_values\r\n  1455\t        sp_sum = valid_vals.sum()\r\n  1456\t        has_na = self.sp_index.ngaps > 0 and not self._null_fill_value\r\n  1457\t\r\n  1458\t        if has_na and not skipna:\r\n  1459\t            return na_value_for_dtype(self.dtype.subtype, compat=False)\r\n  1460\t\r\n  1461\t        if self._null_fill_value:\r\n  1462\t            if check_below_min_count(valid_vals.shape, None, min_count):\r\n  1463\t                return na_value_for_dtype(self.dtype.subtype, compat=False)\r\n  1464\t            return sp_sum\r\n  1465\t        else:\r\n  1466\t            nsparse = self.sp_index.ngaps\r\n  1467\t            if check_below_min_count(valid_vals.shape, None, min_count - nsparse):\r\n  1468\t                return na_value_for_dtype(self.dtype.subtype, compat=False)\r\n  1469\t            return sp_sum + self.fill_value * nsparse\r\n  1470\t\r\n  1471\t    def cumsum(self, axis: AxisInt = 0, *args, **kwargs) -> SparseArray:\r\n  1472\t        \"\"\"\r\n  1473\t        Cumulative sum of non-NA/null values.\r\n  1474\t\r\n  1475\t        When performing the cumulative summation, any non-NA/null values will\r\n  1476\t        be skipped. The resulting SparseArray will preserve the locations of\r\n  1477\t        NaN values, but the fill value will be `np.nan` regardless.\r\n  1478\t\r\n  1479\t        Parameters\r\n  1480\t        ----------\r\n  1481\t        axis : int or None\r\n  1482\t            Axis over which to perform the cumulative summation. If None,\r\n  1483\t            perform cumulative summation over flattened array.\r\n  1484\t\r\n  1485\t        Returns\r\n  1486\t        -------\r\n  1487\t        cumsum : SparseArray\r\n  1488\t        \"\"\"\r\n  1489\t        nv.validate_cumsum(args, kwargs)\r\n  1490\t\r\n  1491\t        if axis is not None and axis >= self.ndim:  # Mimic ndarray behaviour.\r\n  1492\t            raise ValueError(f\"axis(={axis}) out of bounds\")\r\n  1493\t\r\n  1494\t        if not self._null_fill_value:\r\n  1495\t            return SparseArray(self.to_dense()).cumsum()\r\n  1496\t\r\n  1497\t        return SparseArray(\r\n  1498\t            self.sp_values.cumsum(),\r\n  1499\t            sparse_index=self.sp_index,\r\n  1500\t            fill_value=self.fill_value,\r\n"]
[165.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[165.002, "i", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '860,980p'\r"]
[165.004, "o", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '860,980p'\r\n"]
[165.006, "o", "\u001b[?2004l\r\n   860\t        Returns\r\n   861\t        -------\r\n   862\t        counts : Series\r\n   863\t        \"\"\"\r\n   864\t        from pandas import (\r\n   865\t            Index,\r\n   866\t            Series,\r\n   867\t        )\r\n   868\t\r\n   869\t        keys, counts = algos.value_counts_arraylike(self.sp_values, dropna=dropna)\r\n   870\t        fcounts = self.sp_index.ngaps\r\n   871\t        if fcounts > 0 and (not self._null_fill_value or not dropna):\r\n   872\t            mask = isna(keys) if self._null_fill_value else keys == self.fill_value\r\n   873\t            if mask.any():\r\n   874\t                counts[mask] += fcounts\r\n   875\t            else:\r\n   876\t                # error: Argument 1 to \"insert\" has incompatible type \"Union[\r\n   877\t                # ExtensionArray,ndarray[Any, Any]]\"; expected \"Union[\r\n   878\t                # _SupportsArray[dtype[Any]], Sequence[_SupportsArray[dtype\r\n   879\t                # [Any]]], Sequence[Sequence[_SupportsArray[dtype[Any]]]],\r\n   880\t                # Sequence[Sequence[Sequence[_SupportsArray[dtype[Any]]]]], Sequence\r\n   881\t                # [Sequence[Sequence[Sequence[_SupportsArray[dtype[Any]]]]]]]\"\r\n   882\t                keys = np.insert(keys, 0, self.fill_value)  # type: ignore[arg-type]\r\n   883\t                counts = np.insert(counts, 0, fcounts)\r\n   884\t\r\n   885\t        if not isinstance(keys, ABCIndex):\r\n   886\t            index = Index(keys)\r\n   887\t        else:\r\n   888\t            index = keys\r\n   889\t        return Series(counts, index=index, copy=False)\r\n   890\t\r\n   891\t    # --------\r\n   892\t    # Indexing\r\n   893\t    # --------\r\n   894\t    @overload\r\n   895\t    def __getitem__(self, key: ScalarIndexer) -> Any:\r\n   896\t        ...\r\n   897\t\r\n   898\t    @overload\r\n   899\t    def __getitem__(\r\n   900\t        self,\r\n   901\t        key: SequenceIndexer | tuple[int | ellipsis, ...],\r\n   902\t    ) -> Self:\r\n   903\t        ...\r\n   904\t\r\n   905\t    def __getitem__(\r\n   906\t        self,\r\n   907\t        key: PositionalIndexer | tuple[int | ellipsis, ...],\r\n   908\t    ) -> Self | Any:\r\n   909\t        if isinstance(key, tuple):\r\n   910\t            key = unpack_tuple_and_ellipses(key)\r\n   911\t            if key is Ellipsis:\r\n   912\t                raise ValueError(\"Cannot slice with Ellipsis\")\r\n   913\t\r\n   914\t        if is_integer(key):\r\n   915\t            return self._get_val_at(key)\r\n   916\t        elif isinstance(key, tuple):\r\n   917\t            # error: Invalid index type \"Tuple[Union[int, ellipsis], ...]\"\r\n   918\t            # for \"ndarray[Any, Any]\"; expected type\r\n   919\t            # \"Union[SupportsIndex, _SupportsArray[dtype[Union[bool_,\r\n   920\t            # integer[Any]]]], _NestedSequence[_SupportsArray[dtype[\r\n   921\t            # Union[bool_, integer[Any]]]]], _NestedSequence[Union[\r\n   922\t            # bool, int]], Tuple[Union[SupportsIndex, _SupportsArray[\r\n   923\t            # dtype[Union[bool_, integer[Any]]]], _NestedSequence[\r\n   924\t            # _SupportsArray[dtype[Union[bool_, integer[Any]]]]],\r\n   925\t            # _NestedSequence[Union[bool, int]]], ...]]\"\r\n   926\t            data_slice = self.to_dense()[key]  # type: ignore[index]\r\n   927\t        elif isinstance(key, slice):\r\n   928\t            # Avoid densifying when handling contiguous slices\r\n   929\t            if key.step is None or key.step == 1:\r\n   930\t                start = 0 if key.start is None else key.start\r\n   931\t                if start < 0:\r\n   932\t                    start += len(self)\r\n   933\t\r\n   934\t                end = len(self) if key.stop is None else key.stop\r\n   935\t                if end < 0:\r\n   936\t                    end += len(self)\r\n   937\t\r\n   938\t                indices = self.sp_index.indices\r\n   939\t                keep_inds = np.flatnonzero((indices >= start) & (indices < end))\r\n   940\t                sp_vals = self.sp_values[keep_inds]\r\n   941\t\r\n   942\t                sp_index = indices[keep_inds].copy()\r\n   943\t\r\n   944\t                # If we've sliced to not include the start of the array, all our indices\r\n   945\t                # should be shifted. NB: here we are careful to also not shift by a\r\n   946\t                # negative value for a case like [0, 1][-100:] where the start index\r\n   947\t                # should be treated like 0\r\n   948\t                if start > 0:\r\n   949\t                    sp_index -= start\r\n   950\t\r\n   951\t                # Length of our result should match applying this slice to a range\r\n   952\t                # of the length of our original array\r\n   953\t                new_len = len(range(len(self))[key])\r\n   954\t                new_sp_index = make_sparse_index(new_len, sp_index, self.kind)\r\n   955\t                return type(self)._simple_new(sp_vals, new_sp_index, self.dtype)\r\n   956\t            else:\r\n   957\t                indices = np.arange(len(self), dtype=np.int32)[key]\r\n   958\t                return self.take(indices)\r\n   959\t\r\n   960\t        elif not is_list_like(key):\r\n   961\t            # e.g. \"foo\" or 2.5\r\n   962\t            # exception message copied from numpy\r\n   963\t            raise IndexError(\r\n   964\t                r\"only integers, slices (`:`), ellipsis (`...`), numpy.newaxis \"\r\n   965\t                r\"(`None`) and integer or boolean arrays are valid indices\"\r\n   966\t            )\r\n   967\t\r\n   968\t        else:\r\n   969\t            if isinstance(key, SparseArray):\r\n   970\t                # NOTE: If we guarantee that SparseDType(bool)\r\n   971\t                # has only fill_value - true, false or nan\r\n   972\t                # (see GH PR 44955)\r\n   973\t                # we can apply mask very fast:\r\n   974\t                if is_bool_dtype(key):\r\n   975\t                    if isna(key.fill_value):\r\n   976\t                        return self.take(key.sp_index.indices[key.sp_values])\r\n   977\t                    if not key.fill_value:\r\n   978\t                        return self.take(key.sp_index.indices)\r\n   979\t                    n = len(self)\r\n   980\t                    mask = np.full(n, True, dtype=np.bool_)\r\n"]
[170.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[170.002, "i", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '980,1140p'\r"]
[170.004, "o", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '980,1140p'\r\n"]
[170.006, "o", "\u001b[?2004l\r\n   980\t                    mask = np.full(n, True, dtype=np.bool_)\r\n   981\t                    mask[key.sp_index.indices] = False\r\n   982\t                    return self.take(np.arange(n)[mask])\r\n   983\t                else:\r\n   984\t                    key = np.asarray(key)\r\n   985\t\r\n   986\t            key = check_array_indexer(self, key)\r\n   987\t\r\n   988\t            if com.is_bool_indexer(key):\r\n   989\t                # mypy doesn't know we have an array here\r\n   990\t                key = cast(np.ndarray, key)\r\n   991\t                return self.take(np.arange(len(key), dtype=np.int32)[key])\r\n   992\t            elif hasattr(key, \"__len__\"):\r\n   993\t                return self.take(key)\r\n   994\t            else:\r\n   995\t                raise ValueError(f\"Cannot slice with '{key}'\")\r\n   996\t\r\n   997\t        return type(self)(data_slice, kind=self.kind)\r\n   998\t\r\n   999\t    def _get_val_at(self, loc):\r\n  1000\t        loc = validate_insert_loc(loc, len(self))\r\n  1001\t\r\n  1002\t        sp_loc = self.sp_index.lookup(loc)\r\n  1003\t        if sp_loc == -1:\r\n  1004\t            return self.fill_value\r\n  1005\t        else:\r\n  1006\t            val = self.sp_values[sp_loc]\r\n  1007\t            val = maybe_box_datetimelike(val, self.sp_values.dtype)\r\n  1008\t            return val\r\n  1009\t\r\n  1010\t    def take(self, indices, *, allow_fill: bool = False, fill_value=None) -> Self:\r\n  1011\t        if is_scalar(indices):\r\n  1012\t            raise ValueError(f\"'indices' must be an array, not a scalar '{indices}'.\")\r\n  1013\t        indices = np.asarray(indices, dtype=np.int32)\r\n  1014\t\r\n  1015\t        dtype = None\r\n  1016\t        if indices.size == 0:\r\n  1017\t            result = np.array([], dtype=\"object\")\r\n  1018\t            dtype = self.dtype\r\n  1019\t        elif allow_fill:\r\n  1020\t            result = self._take_with_fill(indices, fill_value=fill_value)\r\n  1021\t        else:\r\n  1022\t            return self._take_without_fill(indices)\r\n  1023\t\r\n  1024\t        return type(self)(\r\n  1025\t            result, fill_value=self.fill_value, kind=self.kind, dtype=dtype\r\n  1026\t        )\r\n  1027\t\r\n  1028\t    def _take_with_fill(self, indices, fill_value=None) -> np.ndarray:\r\n  1029\t        if fill_value is None:\r\n  1030\t            fill_value = self.dtype.na_value\r\n  1031\t\r\n  1032\t        if indices.min() < -1:\r\n  1033\t            raise ValueError(\r\n  1034\t                \"Invalid value in 'indices'. Must be between -1 \"\r\n  1035\t                \"and the length of the array.\"\r\n  1036\t            )\r\n  1037\t\r\n  1038\t        if indices.max() >= len(self):\r\n  1039\t            raise IndexError(\"out of bounds value in 'indices'.\")\r\n  1040\t\r\n  1041\t        if len(self) == 0:\r\n  1042\t            # Empty... Allow taking only if all empty\r\n  1043\t            if (indices == -1).all():\r\n  1044\t                dtype = np.result_type(self.sp_values, type(fill_value))\r\n  1045\t                taken = np.empty_like(indices, dtype=dtype)\r\n  1046\t                taken.fill(fill_value)\r\n  1047\t                return taken\r\n  1048\t            else:\r\n  1049\t                raise IndexError(\"cannot do a non-empty take from an empty axes.\")\r\n  1050\t\r\n  1051\t        # sp_indexer may be -1 for two reasons\r\n  1052\t        # 1.) we took for an index of -1 (new)\r\n  1053\t        # 2.) we took a value that was self.fill_value (old)\r\n  1054\t        sp_indexer = self.sp_index.lookup_array(indices)\r\n  1055\t        new_fill_indices = indices == -1\r\n  1056\t        old_fill_indices = (sp_indexer == -1) & ~new_fill_indices\r\n  1057\t\r\n  1058\t        if self.sp_index.npoints == 0 and old_fill_indices.all():\r\n  1059\t            # We've looked up all valid points on an all-sparse array.\r\n  1060\t            taken = np.full(\r\n  1061\t                sp_indexer.shape, fill_value=self.fill_value, dtype=self.dtype.subtype\r\n  1062\t            )\r\n  1063\t\r\n  1064\t        elif self.sp_index.npoints == 0:\r\n  1065\t            # Avoid taking from the empty self.sp_values\r\n  1066\t            _dtype = np.result_type(self.dtype.subtype, type(fill_value))\r\n  1067\t            taken = np.full(sp_indexer.shape, fill_value=fill_value, dtype=_dtype)\r\n  1068\t        else:\r\n  1069\t            taken = self.sp_values.take(sp_indexer)\r\n  1070\t\r\n  1071\t            # Fill in two steps.\r\n  1072\t            # Old fill values\r\n  1073\t            # New fill values\r\n  1074\t            # potentially coercing to a new dtype at each stage.\r\n  1075\t\r\n  1076\t            m0 = sp_indexer[old_fill_indices] < 0\r\n  1077\t            m1 = sp_indexer[new_fill_indices] < 0\r\n  1078\t\r\n  1079\t            result_type = taken.dtype\r\n  1080\t\r\n  1081\t            if m0.any():\r\n  1082\t                result_type = np.result_type(result_type, type(self.fill_value))\r\n  1083\t                taken = taken.astype(result_type)\r\n  1084\t                taken[old_fill_indices] = self.fill_value\r\n  1085\t\r\n  1086\t            if m1.any():\r\n  1087\t                result_type = np.result_type(result_type, type(fill_value))\r\n  1088\t                taken = taken.astype(result_type)\r\n  1089\t                taken[new_fill_indices] = fill_value\r\n  1090\t\r\n  1091\t        return taken\r\n  1092\t\r\n  1093\t    def _take_without_fill(self, indices) -> Self:\r\n  1094\t        to_shift = indices < 0\r\n  1095\t\r\n  1096\t        n = len(self)\r\n  1097\t\r\n  1098\t        if (indices.max() >= n) or (indices.min() < -n):\r\n  1099\t            if n == 0:\r\n  1100\t                raise IndexError(\"cannot do a non-empty take from an empty axes.\")\r\n  1101\t            raise IndexError(\"out of bounds value in 'indices'.\")\r\n  1102\t\r\n  1103\t        if to_shift.any():\r\n  1104\t            indices = indices.copy()\r\n  1105\t            indices[to_shift] += n\r\n  1106\t\r\n  1107\t        sp_indexer = self.sp_index.lookup_array(indices)\r\n  1108\t        value_mask = sp_indexer != -1\r\n  1109\t        new_sp_values = self.sp_values[sp_indexer[value_mask]]\r\n  1110\t\r\n  1111\t        value_indices = np.flatnonzero(value_mask).astype(np.int32, copy=False)\r\n  1112\t\r\n  1113\t        new_sp_index = make_sparse_index(len(indices), value_indices, kind=self.kind)\r\n  1114\t        return type(self)._simple_new(new_sp_values, new_sp_index, dtype=self.dtype)\r\n  1115\t\r\n  1116\t    def searchsorted(\r\n  1117\t        self,\r\n  1118\t        v: ArrayLike | object,\r\n  1119\t        side: Literal[\"left\", \"right\"] = \"left\",\r\n  1120\t        sorter: NumpySorter = None,\r\n  1121\t    ) -> npt.NDArray[np.intp] | np.intp:\r\n  1122\t        msg = \"searchsorted requires high memory usage.\"\r\n  1123\t        warnings.warn(msg, PerformanceWarning, stacklevel=find_stack_level())\r\n  1124\t        if not is_scalar(v):\r\n  1125\t            v = np.asarray(v)\r\n  1126\t        v = np.asarray(v)\r\n  1127\t        return np.asarray(self, dtype=self.dtype.subtype).searchsorted(v, side, sorter)\r\n  1128\t\r\n  1129\t    def copy(self) -> Self:\r\n  1130\t        values = self.sp_values.copy()\r\n  1131\t        return self._simple_new(values, self.sp_index, self.dtype)\r\n  1132\t\r\n  1133\t    @classmethod\r\n  1134\t    def _concat_same_type(cls, to_concat: Sequence[Self]) -> Self:\r\n  1135\t        fill_value = to_concat[0].fill_value\r\n  1136\t\r\n  1137\t        values = []\r\n  1138\t        length = 0\r\n  1139\t\r\n  1140\t        if to_concat:\r\n"]
[175.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[175.002, "i", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '1140,1320p'\r"]
[175.004, "o", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '1140,1320p'\r\n"]
[175.006, "o", "\u001b[?2004l\r\n  1140\t        if to_concat:\r\n  1141\t            sp_kind = to_concat[0].kind\r\n  1142\t        else:\r\n  1143\t            sp_kind = \"integer\"\r\n  1144\t\r\n  1145\t        sp_index: SparseIndex\r\n  1146\t        if sp_kind == \"integer\":\r\n  1147\t            indices = []\r\n  1148\t\r\n  1149\t            for arr in to_concat:\r\n  1150\t                int_idx = arr.sp_index.indices.copy()\r\n  1151\t                int_idx += length  # TODO: wraparound\r\n  1152\t                length += arr.sp_index.length\r\n  1153\t\r\n  1154\t                values.append(arr.sp_values)\r\n  1155\t                indices.append(int_idx)\r\n  1156\t\r\n  1157\t            data = np.concatenate(values)\r\n  1158\t            indices_arr = np.concatenate(indices)\r\n  1159\t            # error: Argument 2 to \"IntIndex\" has incompatible type\r\n  1160\t            # \"ndarray[Any, dtype[signedinteger[_32Bit]]]\";\r\n  1161\t            # expected \"Sequence[int]\"\r\n  1162\t            sp_index = IntIndex(length, indices_arr)  # type: ignore[arg-type]\r\n  1163\t\r\n  1164\t        else:\r\n  1165\t            # when concatenating block indices, we don't claim that you'll\r\n  1166\t            # get an identical index as concatenating the values and then\r\n  1167\t            # creating a new index. We don't want to spend the time trying\r\n  1168\t            # to merge blocks across arrays in `to_concat`, so the resulting\r\n  1169\t            # BlockIndex may have more blocks.\r\n  1170\t            blengths = []\r\n  1171\t            blocs = []\r\n  1172\t\r\n  1173\t            for arr in to_concat:\r\n  1174\t                block_idx = arr.sp_index.to_block_index()\r\n  1175\t\r\n  1176\t                values.append(arr.sp_values)\r\n  1177\t                blocs.append(block_idx.blocs.copy() + length)\r\n  1178\t                blengths.append(block_idx.blengths)\r\n  1179\t                length += arr.sp_index.length\r\n  1180\t\r\n  1181\t            data = np.concatenate(values)\r\n  1182\t            blocs_arr = np.concatenate(blocs)\r\n  1183\t            blengths_arr = np.concatenate(blengths)\r\n  1184\t\r\n  1185\t            sp_index = BlockIndex(length, blocs_arr, blengths_arr)\r\n  1186\t\r\n  1187\t        return cls(data, sparse_index=sp_index, fill_value=fill_value)\r\n  1188\t\r\n  1189\t    def astype(self, dtype: AstypeArg | None = None, copy: bool = True):\r\n  1190\t        \"\"\"\r\n  1191\t        Change the dtype of a SparseArray.\r\n  1192\t\r\n  1193\t        The output will always be a SparseArray. To convert to a dense\r\n  1194\t        ndarray with a certain dtype, use :meth:`numpy.asarray`.\r\n  1195\t\r\n  1196\t        Parameters\r\n  1197\t        ----------\r\n  1198\t        dtype : np.dtype or ExtensionDtype\r\n  1199\t            For SparseDtype, this changes the dtype of\r\n  1200\t            ``self.sp_values`` and the ``self.fill_value``.\r\n  1201\t\r\n  1202\t            For other dtypes, this only changes the dtype of\r\n  1203\t            ``self.sp_values``.\r\n  1204\t\r\n  1205\t        copy : bool, default True\r\n  1206\t            Whether to ensure a copy is made, even if not necessary.\r\n  1207\t\r\n  1208\t        Returns\r\n  1209\t        -------\r\n  1210\t        SparseArray\r\n  1211\t\r\n  1212\t        Examples\r\n  1213\t        --------\r\n  1214\t        >>> arr = pd.arrays.SparseArray([0, 0, 1, 2])\r\n  1215\t        >>> arr\r\n  1216\t        [0, 0, 1, 2]\r\n  1217\t        Fill: 0\r\n  1218\t        IntIndex\r\n  1219\t        Indices: array([2, 3], dtype=int32)\r\n  1220\t\r\n  1221\t        >>> arr.astype(SparseDtype(np.dtype('int32')))\r\n  1222\t        [0, 0, 1, 2]\r\n  1223\t        Fill: 0\r\n  1224\t        IntIndex\r\n  1225\t        Indices: array([2, 3], dtype=int32)\r\n  1226\t\r\n  1227\t        Using a NumPy dtype with a different kind (e.g. float) will coerce\r\n  1228\t        just ``self.sp_values``.\r\n  1229\t\r\n  1230\t        >>> arr.astype(SparseDtype(np.dtype('float64')))\r\n  1231\t        ... # doctest: +NORMALIZE_WHITESPACE\r\n  1232\t        [nan, nan, 1.0, 2.0]\r\n  1233\t        Fill: nan\r\n  1234\t        IntIndex\r\n  1235\t        Indices: array([2, 3], dtype=int32)\r\n  1236\t\r\n  1237\t        Using a SparseDtype, you can also change the fill value as well.\r\n  1238\t\r\n  1239\t        >>> arr.astype(SparseDtype(\"float64\", fill_value=0.0))\r\n  1240\t        ... # doctest: +NORMALIZE_WHITESPACE\r\n  1241\t        [0.0, 0.0, 1.0, 2.0]\r\n  1242\t        Fill: 0.0\r\n  1243\t        IntIndex\r\n  1244\t        Indices: array([2, 3], dtype=int32)\r\n  1245\t        \"\"\"\r\n  1246\t        if is_dtype_equal(dtype, self._dtype):\r\n  1247\t            if not copy:\r\n  1248\t                return self\r\n  1249\t            else:\r\n  1250\t                return self.copy()\r\n  1251\t\r\n  1252\t        future_dtype = pandas_dtype(dtype)\r\n  1253\t        if not isinstance(future_dtype, SparseDtype):\r\n  1254\t            # GH#34457\r\n  1255\t            values = np.asarray(self)\r\n  1256\t            values = ensure_wrapped_if_datetimelike(values)\r\n  1257\t            return astype_array(values, dtype=future_dtype, copy=False)\r\n  1258\t\r\n  1259\t        dtype = self.dtype.update_dtype(dtype)\r\n  1260\t        subtype = pandas_dtype(dtype._subtype_with_str)\r\n  1261\t        subtype = cast(np.dtype, subtype)  # ensured by update_dtype\r\n  1262\t        values = ensure_wrapped_if_datetimelike(self.sp_values)\r\n  1263\t        sp_values = astype_array(values, subtype, copy=copy)\r\n  1264\t        sp_values = np.asarray(sp_values)\r\n  1265\t\r\n  1266\t        return self._simple_new(sp_values, self.sp_index, dtype)\r\n  1267\t\r\n  1268\t    def map(self, mapper, na_action=None) -> Self:\r\n  1269\t        \"\"\"\r\n  1270\t        Map categories using an input mapping or function.\r\n  1271\t\r\n  1272\t        Parameters\r\n  1273\t        ----------\r\n  1274\t        mapper : dict, Series, callable\r\n  1275\t            The correspondence from old values to new.\r\n  1276\t        na_action : {None, 'ignore'}, default None\r\n  1277\t            If 'ignore', propagate NA values, without passing them to the\r\n  1278\t            mapping correspondence.\r\n  1279\t\r\n  1280\t        Returns\r\n  1281\t        -------\r\n  1282\t        SparseArray\r\n  1283\t            The output array will have the same density as the input.\r\n  1284\t            The output fill value will be the result of applying the\r\n  1285\t            mapping to ``self.fill_value``\r\n  1286\t\r\n  1287\t        Examples\r\n  1288\t        --------\r\n  1289\t        >>> arr = pd.arrays.SparseArray([0, 1, 2])\r\n  1290\t        >>> arr.map(lambda x: x + 10)\r\n  1291\t        [10, 11, 12]\r\n  1292\t        Fill: 10\r\n  1293\t        IntIndex\r\n  1294\t        Indices: array([1, 2], dtype=int32)\r\n  1295\t\r\n  1296\t        >>> arr.map({0: 10, 1: 11, 2: 12})\r\n  1297\t        [10, 11, 12]\r\n  1298\t        Fill: 10\r\n  1299\t        IntIndex\r\n  1300\t        Indices: array([1, 2], dtype=int32)\r\n  1301\t\r\n  1302\t        >>> arr.map(pd.Series([10, 11, 12], index=[0, 1, 2]))\r\n  1303\t        [10, 11, 12]\r\n  1304\t        Fill: 10\r\n  1305\t        IntIndex\r\n  1306\t        Indices: array([1, 2], dtype=int32)\r\n  1307\t        \"\"\"\r\n  1308\t        is_map = isinstance(mapper, (abc.Mapping, ABCSeries))\r\n  1309\t\r\n  1310\t        fill_val = self.fill_value\r\n  1311\t\r\n  1312\t        if na_action is None or notna(fill_val):\r\n  1313\t            fill_val = mapper.get(fill_val, fill_val) if is_map else mapper(fill_val)\r\n  1314\t\r\n  1315\t        def func(sp_val):\r\n  1316\t            new_sp_val = mapper.get(sp_val, None) if is_map else mapper(sp_val)\r\n  1317\t            # check identity and equality because nans are not equal to each other\r\n  1318\t            if new_sp_val is fill_val or new_sp_val == fill_val:\r\n  1319\t                msg = \"fill value in the sparse values not supported\"\r\n  1320\t                raise ValueError(msg)\r\n"]
[180.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[180.002, "i", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '880,1000p'\r"]
[180.004, "o", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '880,1000p'\r\n"]
[180.006, "o", "\u001b[?2004l\r\n   880\t                # Sequence[Sequence[Sequence[_SupportsArray[dtype[Any]]]]], Sequence\r\n   881\t                # [Sequence[Sequence[Sequence[_SupportsArray[dtype[Any]]]]]]]\"\r\n   882\t                keys = np.insert(keys, 0, self.fill_value)  # type: ignore[arg-type]\r\n   883\t                counts = np.insert(counts, 0, fcounts)\r\n   884\t\r\n   885\t        if not isinstance(keys, ABCIndex):\r\n   886\t            index = Index(keys)\r\n   887\t        else:\r\n   888\t            index = keys\r\n   889\t        return Series(counts, index=index, copy=False)\r\n   890\t\r\n   891\t    # --------\r\n   892\t    # Indexing\r\n   893\t    # --------\r\n   894\t    @overload\r\n   895\t    def __getitem__(self, key: ScalarIndexer) -> Any:\r\n   896\t        ...\r\n   897\t\r\n   898\t    @overload\r\n   899\t    def __getitem__(\r\n   900\t        self,\r\n   901\t        key: SequenceIndexer | tuple[int | ellipsis, ...],\r\n   902\t    ) -> Self:\r\n   903\t        ...\r\n   904\t\r\n   905\t    def __getitem__(\r\n   906\t        self,\r\n   907\t        key: PositionalIndexer | tuple[int | ellipsis, ...],\r\n   908\t    ) -> Self | Any:\r\n   909\t        if isinstance(key, tuple):\r\n   910\t            key = unpack_tuple_and_ellipses(key)\r\n   911\t            if key is Ellipsis:\r\n   912\t                raise ValueError(\"Cannot slice with Ellipsis\")\r\n   913\t\r\n   914\t        if is_integer(key):\r\n   915\t            return self._get_val_at(key)\r\n   916\t        elif isinstance(key, tuple):\r\n   917\t            # error: Invalid index type \"Tuple[Union[int, ellipsis], ...]\"\r\n   918\t            # for \"ndarray[Any, Any]\"; expected type\r\n   919\t            # \"Union[SupportsIndex, _SupportsArray[dtype[Union[bool_,\r\n   920\t            # integer[Any]]]], _NestedSequence[_SupportsArray[dtype[\r\n   921\t            # Union[bool_, integer[Any]]]]], _NestedSequence[Union[\r\n   922\t            # bool, int]], Tuple[Union[SupportsIndex, _SupportsArray[\r\n   923\t            # dtype[Union[bool_, integer[Any]]]], _NestedSequence[\r\n   924\t            # _SupportsArray[dtype[Union[bool_, integer[Any]]]]],\r\n   925\t            # _NestedSequence[Union[bool, int]]], ...]]\"\r\n   926\t            data_slice = self.to_dense()[key]  # type: ignore[index]\r\n   927\t        elif isinstance(key, slice):\r\n   928\t            # Avoid densifying when handling contiguous slices\r\n   929\t            if key.step is None or key.step == 1:\r\n   930\t                start = 0 if key.start is None else key.start\r\n   931\t                if start < 0:\r\n   932\t                    start += len(self)\r\n   933\t\r\n   934\t                end = len(self) if key.stop is None else key.stop\r\n   935\t                if end < 0:\r\n   936\t                    end += len(self)\r\n   937\t\r\n   938\t                indices = self.sp_index.indices\r\n   939\t                keep_inds = np.flatnonzero((indices >= start) & (indices < end))\r\n   940\t                sp_vals = self.sp_values[keep_inds]\r\n   941\t\r\n   942\t                sp_index = indices[keep_inds].copy()\r\n   943\t\r\n   944\t                # If we've sliced to not include the start of the array, all our indices\r\n   945\t                # should be shifted. NB: here we are careful to also not shift by a\r\n   946\t                # negative value for a case like [0, 1][-100:] where the start index\r\n   947\t                # should be treated like 0\r\n   948\t                if start > 0:\r\n   949\t                    sp_index -= start\r\n   950\t\r\n   951\t                # Length of our result should match applying this slice to a range\r\n   952\t                # of the length of our original array\r\n   953\t                new_len = len(range(len(self))[key])\r\n   954\t                new_sp_index = make_sparse_index(new_len, sp_index, self.kind)\r\n   955\t                return type(self)._simple_new(sp_vals, new_sp_index, self.dtype)\r\n   956\t            else:\r\n   957\t                indices = np.arange(len(self), dtype=np.int32)[key]\r\n   958\t                return self.take(indices)\r\n   959\t\r\n   960\t        elif not is_list_like(key):\r\n   961\t            # e.g. \"foo\" or 2.5\r\n   962\t            # exception message copied from numpy\r\n   963\t            raise IndexError(\r\n   964\t                r\"only integers, slices (`:`), ellipsis (`...`), numpy.newaxis \"\r\n   965\t                r\"(`None`) and integer or boolean arrays are valid indices\"\r\n   966\t            )\r\n   967\t\r\n   968\t        else:\r\n   969\t            if isinstance(key, SparseArray):\r\n   970\t                # NOTE: If we guarantee that SparseDType(bool)\r\n   971\t                # has only fill_value - true, false or nan\r\n   972\t                # (see GH PR 44955)\r\n   973\t                # we can apply mask very fast:\r\n   974\t                if is_bool_dtype(key):\r\n   975\t                    if isna(key.fill_value):\r\n   976\t                        return self.take(key.sp_index.indices[key.sp_values])\r\n   977\t                    if not key.fill_value:\r\n   978\t                        return self.take(key.sp_index.indices)\r\n   979\t                    n = len(self)\r\n   980\t                    mask = np.full(n, True, dtype=np.bool_)\r\n   981\t                    mask[key.sp_index.indices] = False\r\n   982\t                    return self.take(np.arange(n)[mask])\r\n   983\t                else:\r\n   984\t                    key = np.asarray(key)\r\n   985\t\r\n   986\t            key = check_array_indexer(self, key)\r\n   987\t\r\n   988\t            if com.is_bool_indexer(key):\r\n   989\t                # mypy doesn't know we have an array here\r\n   990\t                key = cast(np.ndarray, key)\r\n   991\t                return self.take(np.arange(len(key), dtype=np.int32)[key])\r\n   992\t            elif hasattr(key, \"__len__\"):\r\n   993\t                return self.take(key)\r\n   994\t            else:\r\n   995\t                raise ValueError(f\"Cannot slice with '{key}'\")\r\n   996\t\r\n   997\t        return type(self)(data_slice, kind=self.kind)\r\n   998\t\r\n   999\t    def _get_val_at(self, loc):\r\n  1000\t        loc = validate_insert_loc(loc, len(self))\r\n"]
[185.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[185.002, "i", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '1000,1160p'\r"]
[185.004, "o", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '1000,1160p'\r\n"]
[185.006, "o", "\u001b[?2004l\r\n  1000\t        loc = validate_insert_loc(loc, len(self))\r\n  1001\t\r\n  1002\t        sp_loc = self.sp_index.lookup(loc)\r\n  1003\t        if sp_loc == -1:\r\n  1004\t            return self.fill_value\r\n  1005\t        else:\r\n  1006\t            val = self.sp_values[sp_loc]\r\n  1007\t            val = maybe_box_datetimelike(val, self.sp_values.dtype)\r\n  1008\t            return val\r\n  1009\t\r\n  1010\t    def take(self, indices, *, allow_fill: bool = False, fill_value=None) -> Self:\r\n  1011\t        if is_scalar(indices):\r\n  1012\t            raise ValueError(f\"'indices' must be an array, not a scalar '{indices}'.\")\r\n  1013\t        indices = np.asarray(indices, dtype=np.int32)\r\n  1014\t\r\n  1015\t        dtype = None\r\n  1016\t        if indices.size == 0:\r\n  1017\t            result = np.array([], dtype=\"object\")\r\n  1018\t            dtype = self.dtype\r\n  1019\t        elif allow_fill:\r\n  1020\t            result = self._take_with_fill(indices, fill_value=fill_value)\r\n  1021\t        else:\r\n  1022\t            return self._take_without_fill(indices)\r\n  1023\t\r\n  1024\t        return type(self)(\r\n  1025\t            result, fill_value=self.fill_value, kind=self.kind, dtype=dtype\r\n  1026\t        )\r\n  1027\t\r\n  1028\t    def _take_with_fill(self, indices, fill_value=None) -> np.ndarray:\r\n  1029\t        if fill_value is None:\r\n  1030\t            fill_value = self.dtype.na_value\r\n  1031\t\r\n  1032\t        if indices.min() < -1:\r\n  1033\t            raise ValueError(\r\n  1034\t                \"Invalid value in 'indices'. Must be between -1 \"\r\n  1035\t                \"and the length of the array.\"\r\n  1036\t            )\r\n  1037\t\r\n  1038\t        if indices.max() >= len(self):\r\n  1039\t            raise IndexError(\"out of bounds value in 'indices'.\")\r\n  1040\t\r\n  1041\t        if len(self) == 0:\r\n  1042\t            # Empty... Allow taking only if all empty\r\n  1043\t            if (indices == -1).all():\r\n  1044\t                dtype = np.result_type(self.sp_values, type(fill_value))\r\n  1045\t                taken = np.empty_like(indices, dtype=dtype)\r\n  1046\t                taken.fill(fill_value)\r\n  1047\t                return taken\r\n  1048\t            else:\r\n  1049\t                raise IndexError(\"cannot do a non-empty take from an empty axes.\")\r\n  1050\t\r\n  1051\t        # sp_indexer may be -1 for two reasons\r\n  1052\t        # 1.) we took for an index of -1 (new)\r\n  1053\t        # 2.) we took a value that was self.fill_value (old)\r\n  1054\t        sp_indexer = self.sp_index.lookup_array(indices)\r\n  1055\t        new_fill_indices = indices == -1\r\n  1056\t        old_fill_indices = (sp_indexer == -1) & ~new_fill_indices\r\n  1057\t\r\n  1058\t        if self.sp_index.npoints == 0 and old_fill_indices.all():\r\n  1059\t            # We've looked up all valid points on an all-sparse array.\r\n  1060\t            taken = np.full(\r\n  1061\t                sp_indexer.shape, fill_value=self.fill_value, dtype=self.dtype.subtype\r\n  1062\t            )\r\n  1063\t\r\n  1064\t        elif self.sp_index.npoints == 0:\r\n  1065\t            # Avoid taking from the empty self.sp_values\r\n  1066\t            _dtype = np.result_type(self.dtype.subtype, type(fill_value))\r\n  1067\t            taken = np.full(sp_indexer.shape, fill_value=fill_value, dtype=_dtype)\r\n  1068\t        else:\r\n  1069\t            taken = self.sp_values.take(sp_indexer)\r\n  1070\t\r\n  1071\t            # Fill in two steps.\r\n  1072\t            # Old fill values\r\n  1073\t            # New fill values\r\n  1074\t            # potentially coercing to a new dtype at each stage.\r\n  1075\t\r\n  1076\t            m0 = sp_indexer[old_fill_indices] < 0\r\n  1077\t            m1 = sp_indexer[new_fill_indices] < 0\r\n  1078\t\r\n  1079\t            result_type = taken.dtype\r\n  1080\t\r\n  1081\t            if m0.any():\r\n  1082\t                result_type = np.result_type(result_type, type(self.fill_value))\r\n  1083\t                taken = taken.astype(result_type)\r\n  1084\t                taken[old_fill_indices] = self.fill_value\r\n  1085\t\r\n  1086\t            if m1.any():\r\n  1087\t                result_type = np.result_type(result_type, type(fill_value))\r\n  1088\t                taken = taken.astype(result_type)\r\n  1089\t                taken[new_fill_indices] = fill_value\r\n  1090\t\r\n  1091\t        return taken\r\n  1092\t\r\n  1093\t    def _take_without_fill(self, indices) -> Self:\r\n  1094\t        to_shift = indices < 0\r\n  1095\t\r\n  1096\t        n = len(self)\r\n  1097\t\r\n  1098\t        if (indices.max() >= n) or (indices.min() < -n):\r\n  1099\t            if n == 0:\r\n  1100\t                raise IndexError(\"cannot do a non-empty take from an empty axes.\")\r\n  1101\t            raise IndexError(\"out of bounds value in 'indices'.\")\r\n  1102\t\r\n  1103\t        if to_shift.any():\r\n  1104\t            indices = indices.copy()\r\n  1105\t            indices[to_shift] += n\r\n  1106\t\r\n  1107\t        sp_indexer = self.sp_index.lookup_array(indices)\r\n  1108\t        value_mask = sp_indexer != -1\r\n  1109\t        new_sp_values = self.sp_values[sp_indexer[value_mask]]\r\n  1110\t\r\n  1111\t        value_indices = np.flatnonzero(value_mask).astype(np.int32, copy=False)\r\n  1112\t\r\n  1113\t        new_sp_index = make_sparse_index(len(indices), value_indices, kind=self.kind)\r\n  1114\t        return type(self)._simple_new(new_sp_values, new_sp_index, dtype=self.dtype)\r\n  1115\t\r\n  1116\t    def searchsorted(\r\n  1117\t        self,\r\n  1118\t        v: ArrayLike | object,\r\n  1119\t        side: Literal[\"left\", \"right\"] = \"left\",\r\n  1120\t        sorter: NumpySorter = None,\r\n  1121\t    ) -> npt.NDArray[np.intp] | np.intp:\r\n  1122\t        msg = \"searchsorted requires high memory usage.\"\r\n  1123\t        warnings.warn(msg, PerformanceWarning, stacklevel=find_stack_level())\r\n  1124\t        if not is_scalar(v):\r\n  1125\t            v = np.asarray(v)\r\n  1126\t        v = np.asarray(v)\r\n  1127\t        return np.asarray(self, dtype=self.dtype.subtype).searchsorted(v, side, sorter)\r\n  1128\t\r\n  1129\t    def copy(self) -> Self:\r\n  1130\t        values = self.sp_values.copy()\r\n  1131\t        return self._simple_new(values, self.sp_index, self.dtype)\r\n  1132\t\r\n  1133\t    @classmethod\r\n  1134\t    def _concat_same_type(cls, to_concat: Sequence[Self]) -> Self:\r\n  1135\t        fill_value = to_concat[0].fill_value\r\n  1136\t\r\n  1137\t        values = []\r\n  1138\t        length = 0\r\n  1139\t\r\n  1140\t        if to_concat:\r\n  1141\t            sp_kind = to_concat[0].kind\r\n  1142\t        else:\r\n  1143\t            sp_kind = \"integer\"\r\n  1144\t\r\n  1145\t        sp_index: SparseIndex\r\n  1146\t        if sp_kind == \"integer\":\r\n  1147\t            indices = []\r\n  1148\t\r\n  1149\t            for arr in to_concat:\r\n  1150\t                int_idx = arr.sp_index.indices.copy()\r\n  1151\t                int_idx += length  # TODO: wraparound\r\n  1152\t                length += arr.sp_index.length\r\n  1153\t\r\n  1154\t                values.append(arr.sp_values)\r\n  1155\t                indices.append(int_idx)\r\n  1156\t\r\n  1157\t            data = np.concatenate(values)\r\n  1158\t            indices_arr = np.concatenate(indices)\r\n  1159\t            # error: Argument 2 to \"IntIndex\" has incompatible type\r\n  1160\t            # \"ndarray[Any, dtype[signedinteger[_32Bit]]]\";\r\n"]
[190.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[190.002, "i", "grep -n \"def take\\(\" pandas/core/arrays/sparse/array.py\r"]
[190.004, "o", "grep -n \"def take\\(\" pandas/core/arrays/sparse/array.py\r\n"]
[190.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[195.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[195.002, "i", "grep -n \"mask\\|boolean\" pandas/core/arrays/sparse/array.py\r"]
[195.004, "o", "grep -n \"mask\\|boolean\" pandas/core/arrays/sparse/array.py\r\n"]
[195.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K646\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K = notna(sp_vals)\r\n\u001b[32m\u001b[K647\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        return sp_vals[\u001b[01;31m\u001b[Kmask\u001b[m\u001b[K]\r\n\u001b[32m\u001b[K700\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K = np.full(len(self), False, dtype=np.bool_)\r\n\u001b[32m\u001b[K701\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K[self.sp_index.indices] = isna(self.sp_values)\r\n\u001b[32m\u001b[K702\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        return type(self)(\u001b[01;31m\u001b[Kmask\u001b[m\u001b[K, fill_value=False, dtype=dtype)\r\n\u001b[32m\u001b[K872\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K = isna(keys) if self._null_fill_value else keys == self.fill_value\r\n\u001b[32m\u001b[K873\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            if \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K.any():\r\n\u001b[32m\u001b[K874\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                counts[\u001b[01;31m\u001b[Kmask\u001b[m\u001b[K] += fcounts\r\n\u001b[32m\u001b[K965\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                r\"(`None`) and integer or \u001b[01;31m\u001b[Kboolean\u001b[m\u001b[K arrays are valid indices\"\r\n\u001b[32m\u001b[K973\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                # we can apply \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K very fast:\r\n\u001b[32m\u001b[K980\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                    \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K = np.full(n, True, dtype=np.bool_)\r\n\u001b[32m\u001b[K981\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                    \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K[key.sp_index.indices] = False\r\n\u001b[32m\u001b[K982\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                    return self.take(np.arange(n)[\u001b[01;31m\u001b[Kmask\u001b[m\u001b[K])\r\n\u001b[32m\u001b[K1108\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        value_\u001b[01;31m\u001b[Kmask\u001b[m\u001b[K = sp_indexer != -1\r\n\u001b[32m\u001b[K1109\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        new_sp_values = self.sp_values[sp_indexer[value_\u001b[01;31m\u001b[Kmask\u001b[m\u001b[K]]\r\n\u001b[32m\u001b[K1111\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        value_indices = np.flatnonzero(value_\u001b[01;31m\u001b[Kmask\u001b[m\u001b[K).astype(np.int32, copy=False)\r\n\u001b[32m\u001b[K1337\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    def _where(self, \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K, value):\r\n\u001b[32m\u001b[K1340\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        naive_implementation = np.where(\u001b[01;31m\u001b[Kmask\u001b[m\u001b[K, self, value)\r\n\u001b[32m\u001b[K1596\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K = np.asarray(isna(values))\r\n\u001b[32m\u001b[K1600\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        non_nans = values[~\u001b[01;31m\u001b[Kmask\u001b[m\u001b[K]\r\n\u001b[32m\u001b[K1601\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        non_nan_idx = idx[~\u001b[01;31m\u001b[Kmask\u001b[m\u001b[K]\r\n\u001b[32m\u001b[K1854\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K = notna(arr)\r\n\u001b[32m\u001b[K1864\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K = splib.make_\u001b[01;31m\u001b[Kmask\u001b[m\u001b[K_object_ndarray(arr, fill_value)\r\n\u001b[32m\u001b[K1866\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K = arr != fill_value\r\n\u001b[32m\u001b[K1869\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    if length != len(\u001b[01;31m\u001b[Kmask\u001b[m\u001b[K):\r\n\u001b[32m\u001b[K1871\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        indices = \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K.sp_index.indices\r\n\u001b[32m\u001b[K1873\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        indices = \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K.nonzero()[0].astype(np.int32)\r\n\u001b[32m\u001b[K1876\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    sparsified_values = arr[\u001b[01;31m\u001b[Kmask\u001b[m\u001b[K]\r\n"]
[200.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[200.002, "i", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '880,1040p'\r"]
[200.004, "o", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '880,1040p'\r\n"]
[200.006, "o", "\u001b[?2004l\r\n   880\t                # Sequence[Sequence[Sequence[_SupportsArray[dtype[Any]]]]], Sequence\r\n   881\t                # [Sequence[Sequence[Sequence[_SupportsArray[dtype[Any]]]]]]]\"\r\n   882\t                keys = np.insert(keys, 0, self.fill_value)  # type: ignore[arg-type]\r\n   883\t                counts = np.insert(counts, 0, fcounts)\r\n   884\t\r\n   885\t        if not isinstance(keys, ABCIndex):\r\n   886\t            index = Index(keys)\r\n   887\t        else:\r\n   888\t            index = keys\r\n   889\t        return Series(counts, index=index, copy=False)\r\n   890\t\r\n   891\t    # --------\r\n   892\t    # Indexing\r\n   893\t    # --------\r\n   894\t    @overload\r\n   895\t    def __getitem__(self, key: ScalarIndexer) -> Any:\r\n   896\t        ...\r\n   897\t\r\n   898\t    @overload\r\n   899\t    def __getitem__(\r\n   900\t        self,\r\n   901\t        key: SequenceIndexer | tuple[int | ellipsis, ...],\r\n   902\t    ) -> Self:\r\n   903\t        ...\r\n   904\t\r\n   905\t    def __getitem__(\r\n   906\t        self,\r\n   907\t        key: PositionalIndexer | tuple[int | ellipsis, ...],\r\n   908\t    ) -> Self | Any:\r\n   909\t        if isinstance(key, tuple):\r\n   910\t            key = unpack_tuple_and_ellipses(key)\r\n   911\t            if key is Ellipsis:\r\n   912\t                raise ValueError(\"Cannot slice with Ellipsis\")\r\n   913\t\r\n   914\t        if is_integer(key):\r\n   915\t            return self._get_val_at(key)\r\n   916\t        elif isinstance(key, tuple):\r\n   917\t            # error: Invalid index type \"Tuple[Union[int, ellipsis], ...]\"\r\n   918\t            # for \"ndarray[Any, Any]\"; expected type\r\n   919\t            # \"Union[SupportsIndex, _SupportsArray[dtype[Union[bool_,\r\n   920\t            # integer[Any]]]], _NestedSequence[_SupportsArray[dtype[\r\n   921\t            # Union[bool_, integer[Any]]]]], _NestedSequence[Union[\r\n   922\t            # bool, int]], Tuple[Union[SupportsIndex, _SupportsArray[\r\n   923\t            # dtype[Union[bool_, integer[Any]]]], _NestedSequence[\r\n   924\t            # _SupportsArray[dtype[Union[bool_, integer[Any]]]]],\r\n   925\t            # _NestedSequence[Union[bool, int]]], ...]]\"\r\n   926\t            data_slice = self.to_dense()[key]  # type: ignore[index]\r\n   927\t        elif isinstance(key, slice):\r\n   928\t            # Avoid densifying when handling contiguous slices\r\n   929\t            if key.step is None or key.step == 1:\r\n   930\t                start = 0 if key.start is None else key.start\r\n   931\t                if start < 0:\r\n   932\t                    start += len(self)\r\n   933\t\r\n   934\t                end = len(self) if key.stop is None else key.stop\r\n   935\t                if end < 0:\r\n   936\t                    end += len(self)\r\n   937\t\r\n   938\t                indices = self.sp_index.indices\r\n   939\t                keep_inds = np.flatnonzero((indices >= start) & (indices < end))\r\n   940\t                sp_vals = self.sp_values[keep_inds]\r\n   941\t\r\n   942\t                sp_index = indices[keep_inds].copy()\r\n   943\t\r\n   944\t                # If we've sliced to not include the start of the array, all our indices\r\n   945\t                # should be shifted. NB: here we are careful to also not shift by a\r\n   946\t                # negative value for a case like [0, 1][-100:] where the start index\r\n   947\t                # should be treated like 0\r\n   948\t                if start > 0:\r\n   949\t                    sp_index -= start\r\n   950\t\r\n   951\t                # Length of our result should match applying this slice to a range\r\n   952\t                # of the length of our original array\r\n   953\t                new_len = len(range(len(self))[key])\r\n   954\t                new_sp_index = make_sparse_index(new_len, sp_index, self.kind)\r\n   955\t                return type(self)._simple_new(sp_vals, new_sp_index, self.dtype)\r\n   956\t            else:\r\n   957\t                indices = np.arange(len(self), dtype=np.int32)[key]\r\n   958\t                return self.take(indices)\r\n   959\t\r\n   960\t        elif not is_list_like(key):\r\n   961\t            # e.g. \"foo\" or 2.5\r\n   962\t            # exception message copied from numpy\r\n   963\t            raise IndexError(\r\n   964\t                r\"only integers, slices (`:`), ellipsis (`...`), numpy.newaxis \"\r\n   965\t                r\"(`None`) and integer or boolean arrays are valid indices\"\r\n   966\t            )\r\n   967\t\r\n   968\t        else:\r\n   969\t            if isinstance(key, SparseArray):\r\n   970\t                # NOTE: If we guarantee that SparseDType(bool)\r\n   971\t                # has only fill_value - true, false or nan\r\n   972\t                # (see GH PR 44955)\r\n   973\t                # we can apply mask very fast:\r\n   974\t                if is_bool_dtype(key):\r\n   975\t                    if isna(key.fill_value):\r\n   976\t                        return self.take(key.sp_index.indices[key.sp_values])\r\n   977\t                    if not key.fill_value:\r\n   978\t                        return self.take(key.sp_index.indices)\r\n   979\t                    n = len(self)\r\n   980\t                    mask = np.full(n, True, dtype=np.bool_)\r\n   981\t                    mask[key.sp_index.indices] = False\r\n   982\t                    return self.take(np.arange(n)[mask])\r\n   983\t                else:\r\n   984\t                    key = np.asarray(key)\r\n   985\t\r\n   986\t            key = check_array_indexer(self, key)\r\n   987\t\r\n   988\t            if com.is_bool_indexer(key):\r\n   989\t                # mypy doesn't know we have an array here\r\n   990\t                key = cast(np.ndarray, key)\r\n   991\t                return self.take(np.arange(len(key), dtype=np.int32)[key])\r\n   992\t            elif hasattr(key, \"__len__\"):\r\n   993\t                return self.take(key)\r\n   994\t            else:\r\n   995\t                raise ValueError(f\"Cannot slice with '{key}'\")\r\n   996\t\r\n   997\t        return type(self)(data_slice, kind=self.kind)\r\n   998\t\r\n   999\t    def _get_val_at(self, loc):\r\n  1000\t        loc = validate_insert_loc(loc, len(self))\r\n  1001\t\r\n  1002\t        sp_loc = self.sp_index.lookup(loc)\r\n  1003\t        if sp_loc == -1:\r\n  1004\t            return self.fill_value\r\n  1005\t        else:\r\n  1006\t            val = self.sp_values[sp_loc]\r\n  1007\t            val = maybe_box_datetimelike(val, self.sp_values.dtype)\r\n  1008\t            return val\r\n  1009\t\r\n  1010\t    def take(self, indices, *, allow_fill: bool = False, fill_value=None) -> Self:\r\n  1011\t        if is_scalar(indices):\r\n  1012\t            raise ValueError(f\"'indices' must be an array, not a scalar '{indices}'.\")\r\n  1013\t        indices = np.asarray(indices, dtype=np.int32)\r\n  1014\t\r\n  1015\t        dtype = None\r\n  1016\t        if indices.size == 0:\r\n  1017\t            result = np.array([], dtype=\"object\")\r\n  1018\t            dtype = self.dtype\r\n  1019\t        elif allow_fill:\r\n  1020\t            result = self._take_with_fill(indices, fill_value=fill_value)\r\n  1021\t        else:\r\n  1022\t            return self._take_without_fill(indices)\r\n  1023\t\r\n  1024\t        return type(self)(\r\n  1025\t            result, fill_value=self.fill_value, kind=self.kind, dtype=dtype\r\n  1026\t        )\r\n  1027\t\r\n  1028\t    def _take_with_fill(self, indices, fill_value=None) -> np.ndarray:\r\n  1029\t        if fill_value is None:\r\n  1030\t            fill_value = self.dtype.na_value\r\n  1031\t\r\n  1032\t        if indices.min() < -1:\r\n  1033\t            raise ValueError(\r\n  1034\t                \"Invalid value in 'indices'. Must be between -1 \"\r\n  1035\t                \"and the length of the array.\"\r\n  1036\t            )\r\n  1037\t\r\n  1038\t        if indices.max() >= len(self):\r\n  1039\t            raise IndexError(\"out of bounds value in 'indices'.\")\r\n  1040\t\r\n"]
[205.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[205.002, "i", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '1040,1180p'\r"]
[205.004, "o", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '1040,1180p'\r\n"]
[205.006, "o", "\u001b[?2004l\r\n  1040\t\r\n  1041\t        if len(self) == 0:\r\n  1042\t            # Empty... Allow taking only if all empty\r\n  1043\t            if (indices == -1).all():\r\n  1044\t                dtype = np.result_type(self.sp_values, type(fill_value))\r\n  1045\t                taken = np.empty_like(indices, dtype=dtype)\r\n  1046\t                taken.fill(fill_value)\r\n  1047\t                return taken\r\n  1048\t            else:\r\n  1049\t                raise IndexError(\"cannot do a non-empty take from an empty axes.\")\r\n  1050\t\r\n  1051\t        # sp_indexer may be -1 for two reasons\r\n  1052\t        # 1.) we took for an index of -1 (new)\r\n  1053\t        # 2.) we took a value that was self.fill_value (old)\r\n  1054\t        sp_indexer = self.sp_index.lookup_array(indices)\r\n  1055\t        new_fill_indices = indices == -1\r\n  1056\t        old_fill_indices = (sp_indexer == -1) & ~new_fill_indices\r\n  1057\t\r\n  1058\t        if self.sp_index.npoints == 0 and old_fill_indices.all():\r\n  1059\t            # We've looked up all valid points on an all-sparse array.\r\n  1060\t            taken = np.full(\r\n  1061\t                sp_indexer.shape, fill_value=self.fill_value, dtype=self.dtype.subtype\r\n  1062\t            )\r\n  1063\t\r\n  1064\t        elif self.sp_index.npoints == 0:\r\n  1065\t            # Avoid taking from the empty self.sp_values\r\n  1066\t            _dtype = np.result_type(self.dtype.subtype, type(fill_value))\r\n  1067\t            taken = np.full(sp_indexer.shape, fill_value=fill_value, dtype=_dtype)\r\n  1068\t        else:\r\n  1069\t            taken = self.sp_values.take(sp_indexer)\r\n  1070\t\r\n  1071\t            # Fill in two steps.\r\n  1072\t            # Old fill values\r\n  1073\t            # New fill values\r\n  1074\t            # potentially coercing to a new dtype at each stage.\r\n  1075\t\r\n  1076\t            m0 = sp_indexer[old_fill_indices] < 0\r\n  1077\t            m1 = sp_indexer[new_fill_indices] < 0\r\n  1078\t\r\n  1079\t            result_type = taken.dtype\r\n  1080\t\r\n  1081\t            if m0.any():\r\n  1082\t                result_type = np.result_type(result_type, type(self.fill_value))\r\n  1083\t                taken = taken.astype(result_type)\r\n  1084\t                taken[old_fill_indices] = self.fill_value\r\n  1085\t\r\n  1086\t            if m1.any():\r\n  1087\t                result_type = np.result_type(result_type, type(fill_value))\r\n  1088\t                taken = taken.astype(result_type)\r\n  1089\t                taken[new_fill_indices] = fill_value\r\n  1090\t\r\n  1091\t        return taken\r\n  1092\t\r\n  1093\t    def _take_without_fill(self, indices) -> Self:\r\n  1094\t        to_shift = indices < 0\r\n  1095\t\r\n  1096\t        n = len(self)\r\n  1097\t\r\n  1098\t        if (indices.max() >= n) or (indices.min() < -n):\r\n  1099\t            if n == 0:\r\n  1100\t                raise IndexError(\"cannot do a non-empty take from an empty axes.\")\r\n  1101\t            raise IndexError(\"out of bounds value in 'indices'.\")\r\n  1102\t\r\n  1103\t        if to_shift.any():\r\n  1104\t            indices = indices.copy()\r\n  1105\t            indices[to_shift] += n\r\n  1106\t\r\n  1107\t        sp_indexer = self.sp_index.lookup_array(indices)\r\n  1108\t        value_mask = sp_indexer != -1\r\n  1109\t        new_sp_values = self.sp_values[sp_indexer[value_mask]]\r\n  1110\t\r\n  1111\t        value_indices = np.flatnonzero(value_mask).astype(np.int32, copy=False)\r\n  1112\t\r\n  1113\t        new_sp_index = make_sparse_index(len(indices), value_indices, kind=self.kind)\r\n  1114\t        return type(self)._simple_new(new_sp_values, new_sp_index, dtype=self.dtype)\r\n  1115\t\r\n  1116\t    def searchsorted(\r\n  1117\t        self,\r\n  1118\t        v: ArrayLike | object,\r\n  1119\t        side: Literal[\"left\", \"right\"] = \"left\",\r\n  1120\t        sorter: NumpySorter = None,\r\n  1121\t    ) -> npt.NDArray[np.intp] | np.intp:\r\n  1122\t        msg = \"searchsorted requires high memory usage.\"\r\n  1123\t        warnings.warn(msg, PerformanceWarning, stacklevel=find_stack_level())\r\n  1124\t        if not is_scalar(v):\r\n  1125\t            v = np.asarray(v)\r\n  1126\t        v = np.asarray(v)\r\n  1127\t        return np.asarray(self, dtype=self.dtype.subtype).searchsorted(v, side, sorter)\r\n  1128\t\r\n  1129\t    def copy(self) -> Self:\r\n  1130\t        values = self.sp_values.copy()\r\n  1131\t        return self._simple_new(values, self.sp_index, self.dtype)\r\n  1132\t\r\n  1133\t    @classmethod\r\n  1134\t    def _concat_same_type(cls, to_concat: Sequence[Self]) -> Self:\r\n  1135\t        fill_value = to_concat[0].fill_value\r\n  1136\t\r\n  1137\t        values = []\r\n  1138\t        length = 0\r\n  1139\t\r\n  1140\t        if to_concat:\r\n  1141\t            sp_kind = to_concat[0].kind\r\n  1142\t        else:\r\n  1143\t            sp_kind = \"integer\"\r\n  1144\t\r\n  1145\t        sp_index: SparseIndex\r\n  1146\t        if sp_kind == \"integer\":\r\n  1147\t            indices = []\r\n  1148\t\r\n  1149\t            for arr in to_concat:\r\n  1150\t                int_idx = arr.sp_index.indices.copy()\r\n  1151\t                int_idx += length  # TODO: wraparound\r\n  1152\t                length += arr.sp_index.length\r\n  1153\t\r\n  1154\t                values.append(arr.sp_values)\r\n  1155\t                indices.append(int_idx)\r\n  1156\t\r\n  1157\t            data = np.concatenate(values)\r\n  1158\t            indices_arr = np.concatenate(indices)\r\n  1159\t            # error: Argument 2 to \"IntIndex\" has incompatible type\r\n  1160\t            # \"ndarray[Any, dtype[signedinteger[_32Bit]]]\";\r\n  1161\t            # expected \"Sequence[int]\"\r\n  1162\t            sp_index = IntIndex(length, indices_arr)  # type: ignore[arg-type]\r\n  1163\t\r\n  1164\t        else:\r\n  1165\t            # when concatenating block indices, we don't claim that you'll\r\n  1166\t            # get an identical index as concatenating the values and then\r\n  1167\t            # creating a new index. We don't want to spend the time trying\r\n  1168\t            # to merge blocks across arrays in `to_concat`, so the resulting\r\n  1169\t            # BlockIndex may have more blocks.\r\n  1170\t            blengths = []\r\n  1171\t            blocs = []\r\n  1172\t\r\n  1173\t            for arr in to_concat:\r\n  1174\t                block_idx = arr.sp_index.to_block_index()\r\n  1175\t\r\n  1176\t                values.append(arr.sp_values)\r\n  1177\t                blocs.append(block_idx.blocs.copy() + length)\r\n  1178\t                blengths.append(block_idx.blengths)\r\n  1179\t                length += arr.sp_index.length\r\n  1180\t\r\n"]
[210.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[210.002, "i", "grep -n \"def take\\s*\\(.*\\)\" -n pandas/core/arrays/sparse/array.py || sed -n '1,2000p' pandas/core/arrays/sparse/arra\r"]
[210.004, "o", "grep -n \"def take\\s*\\(.*\\)\" -n pandas/core/arrays/sparse/array.py || sed -n '1,2000p' pandas/core/arrays/sparse/arra\r\n"]
[210.006, "o", "ay.py | grep -n \"def take\"\r\n\u001b[?2004l\r\n\u001b[32m\u001b[K1010\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdef take(self, indices, *, allow_fill: bool = False, fill_value=None) -> Self:\u001b[m\u001b[K\r\n"]
[215.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[215.002, "i", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '940,1020p'\r"]
[215.004, "o", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '940,1020p'\r\n"]
[215.006, "o", "\u001b[?2004l\r\n   940\t                sp_vals = self.sp_values[keep_inds]\r\n   941\t\r\n   942\t                sp_index = indices[keep_inds].copy()\r\n   943\t\r\n   944\t                # If we've sliced to not include the start of the array, all our indices\r\n   945\t                # should be shifted. NB: here we are careful to also not shift by a\r\n   946\t                # negative value for a case like [0, 1][-100:] where the start index\r\n   947\t                # should be treated like 0\r\n   948\t                if start > 0:\r\n   949\t                    sp_index -= start\r\n   950\t\r\n   951\t                # Length of our result should match applying this slice to a range\r\n   952\t                # of the length of our original array\r\n   953\t                new_len = len(range(len(self))[key])\r\n   954\t                new_sp_index = make_sparse_index(new_len, sp_index, self.kind)\r\n   955\t                return type(self)._simple_new(sp_vals, new_sp_index, self.dtype)\r\n   956\t            else:\r\n   957\t                indices = np.arange(len(self), dtype=np.int32)[key]\r\n   958\t                return self.take(indices)\r\n   959\t\r\n   960\t        elif not is_list_like(key):\r\n   961\t            # e.g. \"foo\" or 2.5\r\n   962\t            # exception message copied from numpy\r\n   963\t            raise IndexError(\r\n   964\t                r\"only integers, slices (`:`), ellipsis (`...`), numpy.newaxis \"\r\n   965\t                r\"(`None`) and integer or boolean arrays are valid indices\"\r\n   966\t            )\r\n   967\t\r\n   968\t        else:\r\n   969\t            if isinstance(key, SparseArray):\r\n   970\t                # NOTE: If we guarantee that SparseDType(bool)\r\n   971\t                # has only fill_value - true, false or nan\r\n   972\t                # (see GH PR 44955)\r\n   973\t                # we can apply mask very fast:\r\n   974\t                if is_bool_dtype(key):\r\n   975\t                    if isna(key.fill_value):\r\n   976\t                        return self.take(key.sp_index.indices[key.sp_values])\r\n   977\t                    if not key.fill_value:\r\n   978\t                        return self.take(key.sp_index.indices)\r\n   979\t                    n = len(self)\r\n   980\t                    mask = np.full(n, True, dtype=np.bool_)\r\n   981\t                    mask[key.sp_index.indices] = False\r\n   982\t                    return self.take(np.arange(n)[mask])\r\n   983\t                else:\r\n   984\t                    key = np.asarray(key)\r\n   985\t\r\n   986\t            key = check_array_indexer(self, key)\r\n   987\t\r\n   988\t            if com.is_bool_indexer(key):\r\n   989\t                # mypy doesn't know we have an array here\r\n   990\t                key = cast(np.ndarray, key)\r\n   991\t                return self.take(np.arange(len(key), dtype=np.int32)[key])\r\n   992\t            elif hasattr(key, \"__len__\"):\r\n   993\t                return self.take(key)\r\n   994\t            else:\r\n   995\t                raise ValueError(f\"Cannot slice with '{key}'\")\r\n   996\t\r\n   997\t        return type(self)(data_slice, kind=self.kind)\r\n   998\t\r\n   999\t    def _get_val_at(self, loc):\r\n  1000\t        loc = validate_insert_loc(loc, len(self))\r\n  1001\t\r\n  1002\t        sp_loc = self.sp_index.lookup(loc)\r\n  1003\t        if sp_loc == -1:\r\n  1004\t            return self.fill_value\r\n  1005\t        else:\r\n  1006\t            val = self.sp_values[sp_loc]\r\n  1007\t            val = maybe_box_datetimelike(val, self.sp_values.dtype)\r\n  1008\t            return val\r\n  1009\t\r\n  1010\t    def take(self, indices, *, allow_fill: bool = False, fill_value=None) -> Self:\r\n  1011\t        if is_scalar(indices):\r\n  1012\t            raise ValueError(f\"'indices' must be an array, not a scalar '{indices}'.\")\r\n  1013\t        indices = np.asarray(indices, dtype=np.int32)\r\n  1014\t\r\n  1015\t        dtype = None\r\n  1016\t        if indices.size == 0:\r\n  1017\t            result = np.array([], dtype=\"object\")\r\n  1018\t            dtype = self.dtype\r\n  1019\t        elif allow_fill:\r\n  1020\t            result = self._take_with_fill(indices, fill_value=fill_value)\r\n"]
[220.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[220.002, "i", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '1000,1080p'\r"]
[220.004, "o", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '1000,1080p'\r\n"]
[220.006, "o", "\u001b[?2004l\r\n  1000\t        loc = validate_insert_loc(loc, len(self))\r\n  1001\t\r\n  1002\t        sp_loc = self.sp_index.lookup(loc)\r\n  1003\t        if sp_loc == -1:\r\n  1004\t            return self.fill_value\r\n  1005\t        else:\r\n  1006\t            val = self.sp_values[sp_loc]\r\n  1007\t            val = maybe_box_datetimelike(val, self.sp_values.dtype)\r\n  1008\t            return val\r\n  1009\t\r\n  1010\t    def take(self, indices, *, allow_fill: bool = False, fill_value=None) -> Self:\r\n  1011\t        if is_scalar(indices):\r\n  1012\t            raise ValueError(f\"'indices' must be an array, not a scalar '{indices}'.\")\r\n  1013\t        indices = np.asarray(indices, dtype=np.int32)\r\n  1014\t\r\n  1015\t        dtype = None\r\n  1016\t        if indices.size == 0:\r\n  1017\t            result = np.array([], dtype=\"object\")\r\n  1018\t            dtype = self.dtype\r\n  1019\t        elif allow_fill:\r\n  1020\t            result = self._take_with_fill(indices, fill_value=fill_value)\r\n  1021\t        else:\r\n  1022\t            return self._take_without_fill(indices)\r\n  1023\t\r\n  1024\t        return type(self)(\r\n  1025\t            result, fill_value=self.fill_value, kind=self.kind, dtype=dtype\r\n  1026\t        )\r\n  1027\t\r\n  1028\t    def _take_with_fill(self, indices, fill_value=None) -> np.ndarray:\r\n  1029\t        if fill_value is None:\r\n  1030\t            fill_value = self.dtype.na_value\r\n  1031\t\r\n  1032\t        if indices.min() < -1:\r\n  1033\t            raise ValueError(\r\n  1034\t                \"Invalid value in 'indices'. Must be between -1 \"\r\n  1035\t                \"and the length of the array.\"\r\n  1036\t            )\r\n  1037\t\r\n  1038\t        if indices.max() >= len(self):\r\n  1039\t            raise IndexError(\"out of bounds value in 'indices'.\")\r\n  1040\t\r\n  1041\t        if len(self) == 0:\r\n  1042\t            # Empty... Allow taking only if all empty\r\n  1043\t            if (indices == -1).all():\r\n  1044\t                dtype = np.result_type(self.sp_values, type(fill_value))\r\n  1045\t                taken = np.empty_like(indices, dtype=dtype)\r\n  1046\t                taken.fill(fill_value)\r\n  1047\t                return taken\r\n  1048\t            else:\r\n  1049\t                raise IndexError(\"cannot do a non-empty take from an empty axes.\")\r\n  1050\t\r\n  1051\t        # sp_indexer may be -1 for two reasons\r\n  1052\t        # 1.) we took for an index of -1 (new)\r\n  1053\t        # 2.) we took a value that was self.fill_value (old)\r\n  1054\t        sp_indexer = self.sp_index.lookup_array(indices)\r\n  1055\t        new_fill_indices = indices == -1\r\n  1056\t        old_fill_indices = (sp_indexer == -1) & ~new_fill_indices\r\n  1057\t\r\n  1058\t        if self.sp_index.npoints == 0 and old_fill_indices.all():\r\n  1059\t            # We've looked up all valid points on an all-sparse array.\r\n  1060\t            taken = np.full(\r\n  1061\t                sp_indexer.shape, fill_value=self.fill_value, dtype=self.dtype.subtype\r\n  1062\t            )\r\n  1063\t\r\n  1064\t        elif self.sp_index.npoints == 0:\r\n  1065\t            # Avoid taking from the empty self.sp_values\r\n  1066\t            _dtype = np.result_type(self.dtype.subtype, type(fill_value))\r\n  1067\t            taken = np.full(sp_indexer.shape, fill_value=fill_value, dtype=_dtype)\r\n  1068\t        else:\r\n  1069\t            taken = self.sp_values.take(sp_indexer)\r\n  1070\t\r\n  1071\t            # Fill in two steps.\r\n  1072\t            # Old fill values\r\n  1073\t            # New fill values\r\n  1074\t            # potentially coercing to a new dtype at each stage.\r\n  1075\t\r\n  1076\t            m0 = sp_indexer[old_fill_indices] < 0\r\n  1077\t            m1 = sp_indexer[new_fill_indices] < 0\r\n  1078\t\r\n  1079\t            result_type = taken.dtype\r\n  1080\t\r\n"]
[225.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[225.002, "i", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '880,960p'\r"]
[225.004, "o", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '880,960p'\r\n"]
[225.006, "o", "\u001b[?2004l\r\n   880\t                # Sequence[Sequence[Sequence[_SupportsArray[dtype[Any]]]]], Sequence\r\n   881\t                # [Sequence[Sequence[Sequence[_SupportsArray[dtype[Any]]]]]]]\"\r\n   882\t                keys = np.insert(keys, 0, self.fill_value)  # type: ignore[arg-type]\r\n   883\t                counts = np.insert(counts, 0, fcounts)\r\n   884\t\r\n   885\t        if not isinstance(keys, ABCIndex):\r\n   886\t            index = Index(keys)\r\n   887\t        else:\r\n   888\t            index = keys\r\n   889\t        return Series(counts, index=index, copy=False)\r\n   890\t\r\n   891\t    # --------\r\n   892\t    # Indexing\r\n   893\t    # --------\r\n   894\t    @overload\r\n   895\t    def __getitem__(self, key: ScalarIndexer) -> Any:\r\n   896\t        ...\r\n   897\t\r\n   898\t    @overload\r\n   899\t    def __getitem__(\r\n   900\t        self,\r\n   901\t        key: SequenceIndexer | tuple[int | ellipsis, ...],\r\n   902\t    ) -> Self:\r\n   903\t        ...\r\n   904\t\r\n   905\t    def __getitem__(\r\n   906\t        self,\r\n   907\t        key: PositionalIndexer | tuple[int | ellipsis, ...],\r\n   908\t    ) -> Self | Any:\r\n   909\t        if isinstance(key, tuple):\r\n   910\t            key = unpack_tuple_and_ellipses(key)\r\n   911\t            if key is Ellipsis:\r\n   912\t                raise ValueError(\"Cannot slice with Ellipsis\")\r\n   913\t\r\n   914\t        if is_integer(key):\r\n   915\t            return self._get_val_at(key)\r\n   916\t        elif isinstance(key, tuple):\r\n   917\t            # error: Invalid index type \"Tuple[Union[int, ellipsis], ...]\"\r\n   918\t            # for \"ndarray[Any, Any]\"; expected type\r\n   919\t            # \"Union[SupportsIndex, _SupportsArray[dtype[Union[bool_,\r\n   920\t            # integer[Any]]]], _NestedSequence[_SupportsArray[dtype[\r\n   921\t            # Union[bool_, integer[Any]]]]], _NestedSequence[Union[\r\n   922\t            # bool, int]], Tuple[Union[SupportsIndex, _SupportsArray[\r\n   923\t            # dtype[Union[bool_, integer[Any]]]], _NestedSequence[\r\n   924\t            # _SupportsArray[dtype[Union[bool_, integer[Any]]]]],\r\n   925\t            # _NestedSequence[Union[bool, int]]], ...]]\"\r\n   926\t            data_slice = self.to_dense()[key]  # type: ignore[index]\r\n   927\t        elif isinstance(key, slice):\r\n   928\t            # Avoid densifying when handling contiguous slices\r\n   929\t            if key.step is None or key.step == 1:\r\n   930\t                start = 0 if key.start is None else key.start\r\n   931\t                if start < 0:\r\n   932\t                    start += len(self)\r\n   933\t\r\n   934\t                end = len(self) if key.stop is None else key.stop\r\n   935\t                if end < 0:\r\n   936\t                    end += len(self)\r\n   937\t\r\n   938\t                indices = self.sp_index.indices\r\n   939\t                keep_inds = np.flatnonzero((indices >= start) & (indices < end))\r\n   940\t                sp_vals = self.sp_values[keep_inds]\r\n   941\t\r\n   942\t                sp_index = indices[keep_inds].copy()\r\n   943\t\r\n   944\t                # If we've sliced to not include the start of the array, all our indices\r\n   945\t                # should be shifted. NB: here we are careful to also not shift by a\r\n   946\t                # negative value for a case like [0, 1][-100:] where the start index\r\n   947\t                # should be treated like 0\r\n   948\t                if start > 0:\r\n   949\t                    sp_index -= start\r\n   950\t\r\n   951\t                # Length of our result should match applying this slice to a range\r\n   952\t                # of the length of our original array\r\n   953\t                new_len = len(range(len(self))[key])\r\n   954\t                new_sp_index = make_sparse_index(new_len, sp_index, self.kind)\r\n   955\t                return type(self)._simple_new(sp_vals, new_sp_index, self.dtype)\r\n   956\t            else:\r\n   957\t                indices = np.arange(len(self), dtype=np.int32)[key]\r\n   958\t                return self.take(indices)\r\n   959\t\r\n   960\t        elif not is_list_like(key):\r\n"]
[230.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[230.002, "i", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '960,1000p'\r"]
[230.004, "o", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '960,1000p'\r\n"]
[230.006, "o", "\u001b[?2004l\r\n   960\t        elif not is_list_like(key):\r\n   961\t            # e.g. \"foo\" or 2.5\r\n   962\t            # exception message copied from numpy\r\n   963\t            raise IndexError(\r\n   964\t                r\"only integers, slices (`:`), ellipsis (`...`), numpy.newaxis \"\r\n   965\t                r\"(`None`) and integer or boolean arrays are valid indices\"\r\n   966\t            )\r\n   967\t\r\n   968\t        else:\r\n   969\t            if isinstance(key, SparseArray):\r\n   970\t                # NOTE: If we guarantee that SparseDType(bool)\r\n   971\t                # has only fill_value - true, false or nan\r\n   972\t                # (see GH PR 44955)\r\n   973\t                # we can apply mask very fast:\r\n   974\t                if is_bool_dtype(key):\r\n   975\t                    if isna(key.fill_value):\r\n   976\t                        return self.take(key.sp_index.indices[key.sp_values])\r\n   977\t                    if not key.fill_value:\r\n   978\t                        return self.take(key.sp_index.indices)\r\n   979\t                    n = len(self)\r\n   980\t                    mask = np.full(n, True, dtype=np.bool_)\r\n   981\t                    mask[key.sp_index.indices] = False\r\n   982\t                    return self.take(np.arange(n)[mask])\r\n   983\t                else:\r\n   984\t                    key = np.asarray(key)\r\n   985\t\r\n   986\t            key = check_array_indexer(self, key)\r\n   987\t\r\n   988\t            if com.is_bool_indexer(key):\r\n   989\t                # mypy doesn't know we have an array here\r\n   990\t                key = cast(np.ndarray, key)\r\n   991\t                return self.take(np.arange(len(key), dtype=np.int32)[key])\r\n   992\t            elif hasattr(key, \"__len__\"):\r\n   993\t                return self.take(key)\r\n   994\t            else:\r\n   995\t                raise ValueError(f\"Cannot slice with '{key}'\")\r\n   996\t\r\n   997\t        return type(self)(data_slice, kind=self.kind)\r\n   998\t\r\n   999\t    def _get_val_at(self, loc):\r\n  1000\t        loc = validate_insert_loc(loc, len(self))\r\n"]
[235.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[235.002, "i", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '1000,1040p'\r"]
[235.004, "o", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '1000,1040p'\r\n"]
[235.006, "o", "\u001b[?2004l\r\n  1000\t        loc = validate_insert_loc(loc, len(self))\r\n  1001\t\r\n  1002\t        sp_loc = self.sp_index.lookup(loc)\r\n  1003\t        if sp_loc == -1:\r\n  1004\t            return self.fill_value\r\n  1005\t        else:\r\n  1006\t            val = self.sp_values[sp_loc]\r\n  1007\t            val = maybe_box_datetimelike(val, self.sp_values.dtype)\r\n  1008\t            return val\r\n  1009\t\r\n  1010\t    def take(self, indices, *, allow_fill: bool = False, fill_value=None) -> Self:\r\n  1011\t        if is_scalar(indices):\r\n  1012\t            raise ValueError(f\"'indices' must be an array, not a scalar '{indices}'.\")\r\n  1013\t        indices = np.asarray(indices, dtype=np.int32)\r\n  1014\t\r\n  1015\t        dtype = None\r\n  1016\t        if indices.size == 0:\r\n  1017\t            result = np.array([], dtype=\"object\")\r\n  1018\t            dtype = self.dtype\r\n  1019\t        elif allow_fill:\r\n  1020\t            result = self._take_with_fill(indices, fill_value=fill_value)\r\n  1021\t        else:\r\n  1022\t            return self._take_without_fill(indices)\r\n  1023\t\r\n  1024\t        return type(self)(\r\n  1025\t            result, fill_value=self.fill_value, kind=self.kind, dtype=dtype\r\n  1026\t        )\r\n  1027\t\r\n  1028\t    def _take_with_fill(self, indices, fill_value=None) -> np.ndarray:\r\n  1029\t        if fill_value is None:\r\n  1030\t            fill_value = self.dtype.na_value\r\n  1031\t\r\n  1032\t        if indices.min() < -1:\r\n  1033\t            raise ValueError(\r\n  1034\t                \"Invalid value in 'indices'. Must be between -1 \"\r\n  1035\t                \"and the length of the array.\"\r\n  1036\t            )\r\n  1037\t\r\n  1038\t        if indices.max() >= len(self):\r\n  1039\t            raise IndexError(\"out of bounds value in 'indices'.\")\r\n  1040\t\r\n"]
[240.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[240.002, "i", "python - << 'PY'\r"]
[240.004, "o", "python - << 'PY'\r\n"]
[240.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> import io, re, sys\r\n\u001b[?2004l\r\n\u001b[?2004h> p = 'pandas/core/arrays/sparse/array.py'\r\n\u001b[?2004l\r\n\u001b[?2004h> src = open(p, 'r', encoding='utf-8').read()\r\n\u001b[?2004l\r\n\u001b[?2004h> pat = (\r\n\u001b[?2004l\r\n\u001b[?2004h>     r\"(if\\s+is_bool_dtype\\(key\\):\\n\\s+if\\s+isna\\(key\\.fill_value\\):\\n\\s+return\\s+self\\.take\\(key\\.sp_index\\.indices\\[key\\.sp_values\\]\\)\\n\\s+if\\s+not\\s+key\\.fil\r\nll_value:\\n\\s+return\\s+self\\.take\\(key\\.sp_index\\.indices\\)\\n\\s+n\\s*=\\s*len\\(self\\)\\n\\s+mask\\s*=\\s*np\\.full\\(n,\\s*True,\\s*dtype=np\\.bool_\\)\\n\\s+mask\\[key\\.sp_ind\r\ndex\\.indices\\]\\s*=\\s*False\\n\\s+return\\s+self\\.take\\(np\\.arange\\(n\\)\\[mask\\]\\))\"\r\n\u001b[?2004l\r\n\u001b[?2004h> )\r\n\u001b[?2004l\r\n\u001b[?2004h> rep = (\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"if is_bool_dtype(key):\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"    if isna(key.fill_value):\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"        return self.take(key.sp_index.indices[key.sp_values])\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"    if not key.fill_value:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"        return self.take(key.sp_index.indices)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"    # fill_value is True: compute complement indices without dense mask\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"    n = len(self)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"    neg_idx = key.sp_index.indices\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"    if neg_idx.size == 0:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"        return self.take(np.arange(n, dtype=np.int32))\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"    out_len = n - int(neg_idx.size)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"    out = np.empty(out_len, dtype=np.int32)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"    pos = 0\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"    start = 0\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"    for idx in neg_idx:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"        if idx > start:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"            seg = np.arange(start, idx, dtype=np.int32)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"            L = seg.size\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"            out[pos:pos+L] = seg\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"            pos += L\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"        start = int(idx) + 1\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"    if start < n:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"        seg = np.arange(start, n, dtype=np.int32)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"        L = seg.size\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"        out[pos:pos+L] = seg\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"    return self.take(out)\"\r\n\u001b[?2004l\r\n\u001b[?2004h> )\r\n\u001b[?2004l\r\n\u001b[?2004h> new, nsub = re.subn(pat, rep, src)\r\n\u001b[?2004l\r\n\u001b[?2004h> if nsub == 0:\r\n\u001b[?2004l\r\n\u001b[?2004h>     print('Patch pattern not found; aborting.', file=sys.stderr)\r\n\u001b[?2004l\r\n\u001b[?2004h>     sys.exit(1)\r\n\u001b[?2004l\r\n\u001b[?2004h> open(p, 'w', encoding='utf-8').write(new)\r\n\u001b[?2004l\r\n\u001b[?2004h> print('Patched', p)\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\nPatched pandas/core/arrays/sparse/array.py\r\n"]
[245.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[245.002, "i", "git diff -- pandas/core/arrays/sparse/array.py | sed -n '1,200p'\r"]
[245.004, "o", "git diff -- pandas/core/arrays/sparse/array.py | sed -n '1,200p'\r\n"]
[245.006, "o", "\u001b[?2004l\r\ndiff --git a/pandas/core/arrays/sparse/array.py b/pandas/core/arrays/sparse/array.py\r\nindex e6c89f65fb..786153a1a6 100644\r\n--- a/pandas/core/arrays/sparse/array.py\r\n+++ b/pandas/core/arrays/sparse/array.py\r\n@@ -972,14 +972,31 @@ class SparseArray(OpsMixin, PandasObject, ExtensionArray):\r\n                 # (see GH PR 44955)\r\n                 # we can apply mask very fast:\r\n                 if is_bool_dtype(key):\r\n-                    if isna(key.fill_value):\r\n-                        return self.take(key.sp_index.indices[key.sp_values])\r\n-                    if not key.fill_value:\r\n-                        return self.take(key.sp_index.indices)\r\n-                    n = len(self)\r\n-                    mask = np.full(n, True, dtype=np.bool_)\r\n-                    mask[key.sp_index.indices] = False\r\n-                    return self.take(np.arange(n)[mask])\r\n+    if isna(key.fill_value):\r\n+        return self.take(key.sp_index.indices[key.sp_values])\r\n+    if not key.fill_value:\r\n+        return self.take(key.sp_index.indices)\r\n+    # fill_value is True: compute complement indices without dense mask\r\n+    n = len(self)\r\n+    neg_idx = key.sp_index.indices\r\n+    if neg_idx.size == 0:\r\n+        return self.take(np.arange(n, dtype=np.int32))\r\n+    out_len = n - int(neg_idx.size)\r\n+    out = np.empty(out_len, dtype=np.int32)\r\n+    pos = 0\r\n+    start = 0\r\n+    for idx in neg_idx:\r\n+        if idx > start:\r\n+            seg = np.arange(start, idx, dtype=np.int32)\r\n+            L = seg.size\r\n+            out[pos:pos+L] = seg\r\n+            pos += L\r\n+        start = int(idx) + 1\r\n+    if start < n:\r\n+        seg = np.arange(start, n, dtype=np.int32)\r\n+        L = seg.size\r\n+        out[pos:pos+L] = seg\r\n+    return self.take(out)\r\n                 else:\r\n                     key = np.asarray(key)\r\n \r\n"]
[250.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[250.002, "i", "pytest -q -k sparse --maxfail=1\r"]
[250.004, "o", "pytest -q -k sparse --maxfail=1\r\n"]
[250.006, "o", "\u001b[?2004l\r\n\u001b[31mImportError while loading conftest '/workspace/repo/pandas/conftest.py'.\u001b[0m\r\n\u001b[31mpandas/__init__.py:48: in <module>\u001b[0m\r\n\u001b[31m    from pandas.core.api import (\u001b[0m\r\n\u001b[31mpandas/core/api.py:27: in <module>\u001b[0m\r\n\u001b[31m    from pandas.core.arrays import Categorical\u001b[0m\r\n\u001b[31mpandas/core/arrays/__init__.py:19: in <module>\u001b[0m\r\n\u001b[31m    from pandas.core.arrays.sparse import SparseArray\u001b[0m\r\n\u001b[31mpandas/core/arrays/sparse/__init__.py:1: in <module>\u001b[0m\r\n\u001b[31m    from pandas.core.arrays.sparse.accessor import (\u001b[0m\r\n\u001b[31mpandas/core/arrays/sparse/accessor.py:16: in <module>\u001b[0m\r\n\u001b[31m    from pandas.core.arrays.sparse.array import SparseArray\u001b[0m\r\n\u001b[31mE     File \"/workspace/repo/pandas/core/arrays/sparse/array.py\", line 975\u001b[0m\r\n\u001b[31mE       if isna(key.fill_value):\u001b[0m\r\n\u001b[31mE       ^\u001b[0m\r\n\u001b[31mE   IndentationError: expected an indented block\u001b[0m\r\n"]
[255.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[255.002, "i", "cd asv_bench\r"]
[255.004, "o", "cd asv_bench\r\n"]
[255.006, "o", "\u001b[?2004l\r\n"]
[260.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[260.002, "i", "asv run --python=same --bench=\"sparse.GetItemMask.*\"\r"]
[260.004, "o", "asv run --python=same --bench=\"sparse.GetItemMask.*\"\r\n"]
[260.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7\u00b7 \u001b[0;31mError running /opt/conda/envs/asv_3.8/bin/python /opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv/benchmark.py discover /workspace/repo/asv_bench/benchmarks /tmp/tmpu0i194tf/result.json (exit status 1)\u001b[0m\r\n   STDOUT -------->\r\n   \r\n   STDERR -------->\r\n   Traceback (most recent call last):\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv/benchmark.py\", line 99, in <module>\r\n       main()\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv/benchmark.py\", line 91, in main\r\n       commands[mode](args)\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv_runner/discovery.py\", line 310, in _discover\r\n       list_benchmarks(benchmark_dir, fp)\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv_runner/discovery.py\", line 278, in list_benchmarks\r\n       for benchmark in disc_benchmarks(root):\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv_runner/discovery.py\", line 144, in disc_benchmarks\r\n       for module in disc_modules(root_name, ignore_import_errors=ignore_import_errors):\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv_runner/discovery.py\", line 109, in disc_modules\r\n       yield from disc_modules(name, ignore_import_errors)\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv_runner/discovery.py\", line 98, in disc_modules\r\n       module = importlib.import_module(module_name)\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/importlib/__init__.py\", line 127, in import_module\r\n       return _bootstrap._gcd_import(name[level:], package, level)\r\n     File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\n     File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n     File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\r\n     File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\r\n     File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\r\n     File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n     File \"/workspace/repo/asv_bench/benchmarks/algorithms.py\", line 5, in <module>\r\n       import pandas as pd\r\n     File \"/workspace/repo/pandas/__init__.py\", line 48, in <module>\r\n       from pandas.core.api import (\r\n     File \"/workspace/repo/pandas/core/api.py\", line 27, in <module>\r\n       from pandas.core.arrays import Categorical\r\n     File \"/workspace/repo/pandas/core/arrays/__init__.py\", line 19, in <module>\r\n       from pandas.core.arrays.sparse import SparseArray\r\n     File \"/workspace/repo/pandas/core/arrays/sparse/__init__.py\", line 1, in <module>\r\n       from pandas.core.arrays.sparse.accessor import (\r\n     File \"/workspace/repo/pandas/core/arrays/sparse/accessor.py\", line 16, in <module>\r\n       from pandas.core.arrays.sparse.array import SparseArray\r\n     File \"/workspace/repo/pandas/core/arrays/sparse/array.py\", line 975\r\n       if isna(key.fill_value):\r\n       ^\r\n   IndentationError: expected an indented block\r\n\r\n\u00b7\u00b7 \u001b[0;31mLast error: Command '/opt/conda/envs/asv_3.8/bin/python /opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv/benchmark.py discover /workspace/repo/asv_bench/benchmarks /tmp/tmpu0i194tf/result.json' returned non-zero exit status 1\u001b[0m\r\n\u00b7\u00b7 \u001b[0;31mFailed to build the project and import the benchmark suite.\u001b[0m\r\n"]
[265.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[265.002, "i", "sed -n '950,990p' pandas/core/arrays/sparse/array.py\r"]
[265.004, "o", "sed -n '950,990p' pandas/core/arrays/sparse/array.py\r\n"]
[265.006, "o", "\u001b[?2004l\r\nsed: can't read pandas/core/arrays/sparse/array.py: No such file or directory\r\n"]
[270.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[270.002, "i", "awk 'NR>=960&&NR<=992{print NR\":\"$0}' pandas/core/arrays/sparse/array.py\r"]
[270.004, "o", "awk 'NR>=960&&NR<=992{print NR\":\"$0}' pandas/core/arrays/sparse/array.py\r\n"]
[270.006, "o", "\u001b[?2004l\r\nawk: cannot open pandas/core/arrays/sparse/array.py (No such file or directory)\r\n"]
[275.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[275.002, "i", "python - << 'PY'\r"]
[275.004, "o", "python - << 'PY'\r\n"]
[275.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> from pathlib import Path\r\n\u001b[?2004l\r\n\u001b[?2004h> p = Path('pandas/core/arrays/sparse/array.py')\r\n\u001b[?2004l\r\n\u001b[?2004h> src = p.read_text(encoding='utf-8')\r\n\u001b[?2004l\r\n\u001b[?2004h> start = src.find('if isinstance(key, SparseArray):')\r\n\u001b[?2004l\r\n\u001b[?2004h> end = src.find('\\n              key = check_array_indexer', start)\r\n\u001b[?2004l\r\n\u001b[?2004h> print(src[start:end])\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 3, in <module>\r\n  File \"/opt/conda/envs/asv_3.8/lib/python3.8/pathlib.py\", line 1236, in read_text\r\n    with self.open(mode='r', encoding=encoding, errors=errors) as f:\r\n  File \"/opt/conda/envs/asv_3.8/lib/python3.8/pathlib.py\", line 1222, in open\r\n    return io.open(self, mode, buffering, encoding, errors, newline,\r\n  File \"/opt/conda/envs/asv_3.8/lib/python3.8/pathlib.py\", line 1078, in _opener\r\n    return self._accessor.open(self, flags, mode)\r\nFileNotFoundError: [Errno 2] No such file or directory: 'pandas/core/arrays/sparse/array.py'\r\n"]
[280.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[280.002, "i", "python - << 'PY'\r"]
[280.004, "o", "python - << 'PY'\r\n"]
[280.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> from pathlib import Path\r\n\u001b[?2004l\r\n\u001b[?2004h> p = Path('pandas/core/arrays/sparse/array.py')\r\n\u001b[?2004l\r\n\u001b[?2004h> s = p.read_text(encoding='utf-8')\r\n\u001b[?2004l\r\n\u001b[?2004h> old_block = (\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                  if is_bool_dtype(key):\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                      if isna(key.fill_value):\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                          return self.take(key.sp_index.indices[key.sp_values])\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                      if not key.fill_value:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                          return self.take(key.sp_index.indices)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                      n = len(self)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                      mask = np.full(n, True, dtype=np.bool_)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                      mask[key.sp_index.indices] = False\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                      return self.take(np.arange(n)[mask])\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                  else:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                      key = np.asarray(key)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h> )\r\n\u001b[?2004l\r\n\u001b[?2004h> new_block = (\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                  if is_bool_dtype(key):\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                      if isna(key.fill_value):\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                          return self.take(key.sp_index.indices[key.sp_values])\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                      if not key.fill_value:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                          return self.take(key.sp_index.indices)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                      # fill_value is True: compute complement indices without dense mask\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                      n = len(self)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                      neg_idx = key.sp_index.indices\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                      if neg_idx.size == 0:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                          return self.take(np.arange(n, dtype=np.int32))\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                      out_len = n - int(neg_idx.size)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                      out = np.empty(out_len, dtype=np.int32)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                      pos = 0\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                      start = 0\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                      for idx in neg_idx:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                          if idx > start:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                              seg = np.arange(start, idx, dtype=np.int32)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                              L = seg.size\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                              out[pos:pos+L] = seg\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                              pos += L\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                          start = int(idx) + 1\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                      if start < n:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                          seg = np.arange(start, n, dtype=np.int32)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                          L = seg.size\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                          out[pos:pos+L] = seg\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                      return self.take(out)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                  else:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                      key = np.asarray(key)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h> )\r\n\u001b[?2004l\r\n\u001b[?2004h> if old_block in s:\r\n\u001b[?2004l\r\n\u001b[?2004h>     s = s.replace(old_block, new_block)\r\n\u001b[?2004l\r\n\u001b[?2004h> else:\r\n\u001b[?2004l\r\n\u001b[?2004h>     # Try to repair by finding the if is_bool_dtype block start and reconstruct minimaly\r\n\u001b[?2004l\r\n\u001b[?2004h>     import re\r\n\u001b[?2004l\r\n\u001b[?2004h>     m = re.search(r\"(\\n\\s*if isinstance\\(key, SparseArray\\):\\n[\\s\\S]*?if is_bool_dtype\\(key\\):\\n)\", s)\r\n\u001b[?2004l\r\n\u001b[?2004h>     if not m:\r\n\u001b[?2004l\r\n\u001b[?2004h>         raise SystemExit('Could not locate boolean mask block')\r\n\u001b[?2004l\r\n\u001b[?2004h>     # Not performing risky surgery if we can't match exactly\r\n\u001b[?2004l\r\n\u001b[?2004h>     raise SystemExit('Exact block not found; manual fix required')\r\n\u001b[?2004l\r\n\u001b[?2004h> p.write_text(s, encoding='utf-8')\r\n\u001b[?2004l\r\n\u001b[?2004h> print('Rewrote boolean-mask block successfully')\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 3, in <module>\r\n  File \"/opt/conda/envs/asv_3.8/lib/python3.8/pathlib.py\", line 1236, in read_text\r\n    with self.open(mode='r', encoding=encoding, errors=errors) as f:\r\n  File \"/opt/conda/envs/asv_3.8/lib/python3.8/pathlib.py\", line 1222, in open\r\n    return io.open(self, mode, buffering, encoding, errors, newline,\r\n  File \"/opt/conda/envs/asv_3.8/lib/python3.8/pathlib.py\", line 1078, in _opener\r\n    return self._accessor.open(self, flags, mode)\r\nFileNotFoundError: [Errno 2] No such file or directory: 'pandas/core/arrays/sparse/array.py'\r\n"]
[285.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[285.002, "i", "python - << 'PY'\r"]
[285.004, "o", "python - << 'PY'\r\n"]
[285.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> import sys\r\n\u001b[?2004l\r\n\u001b[?2004h> from pathlib import Path\r\n\u001b[?2004l\r\n\u001b[?2004h> p = Path('pandas/core/arrays/sparse/array.py')\r\n\u001b[?2004l\r\n\u001b[?2004h> print('\\n'.join(f\"{i+1:4d}: \"+line.rstrip('\\n') for i,line in enumerate(p.read_text(encoding='utf-8').splitlines()[960:996])))\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 4, in <module>\r\n  File \"/opt/conda/envs/asv_3.8/lib/python3.8/pathlib.py\", line 1236, in read_text\r\n    with self.open(mode='r', encoding=encoding, errors=errors) as f:\r\n  File \"/opt/conda/envs/asv_3.8/lib/python3.8/pathlib.py\", line 1222, in open\r\n    return io.open(self, mode, buffering, encoding, errors, newline,\r\n  File \"/opt/conda/envs/asv_3.8/lib/python3.8/pathlib.py\", line 1078, in _opener\r\n    return self._accessor.open(self, flags, mode)\r\nFileNotFoundError: [Errno 2] No such file or directory: 'pandas/core/arrays/sparse/array.py'\r\n"]
[290.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[290.002, "i", "python -m pyflakes pandas/core/arrays/sparse/array.py || true\r"]
[290.004, "o", "python -m pyflakes pandas/core/arrays/sparse/array.py || true\r\n"]
[290.006, "o", "\u001b[?2004l\r\n/opt/conda/envs/asv_3.8/bin/python: No module named pyflakes\r\n"]
[295.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[295.002, "i", "python - << 'PY'\r"]
[295.004, "o", "python - << 'PY'\r\n"]
[295.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> # Quick import test to catch IndentationError\r\n\u001b[?2004l\r\n\u001b[?2004h> import importlib, sys\r\n\u001b[?2004l\r\n\u001b[?2004h> sys.path.insert(0, 'pandas')\r\n\u001b[?2004l\r\n\u001b[?2004h> import pandas as pd\r\n\u001b[?2004l\r\n\u001b[?2004h> print('Imported pandas ok; version', pd.__version__)\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 4, in <module>\r\n  File \"/workspace/repo/pandas/__init__.py\", line 48, in <module>\r\n    from pandas.core.api import (\r\n  File \"/workspace/repo/pandas/core/api.py\", line 27, in <module>\r\n    from pandas.core.arrays import Categorical\r\n  File \"/workspace/repo/pandas/core/arrays/__init__.py\", line 19, in <module>\r\n    from pandas.core.arrays.sparse import SparseArray\r\n  File \"/workspace/repo/pandas/core/arrays/sparse/__init__.py\", line 1, in <module>\r\n    from pandas.core.arrays.sparse.accessor import (\r\n  File \"/workspace/repo/pandas/core/arrays/sparse/accessor.py\", line 16, in <module>\r\n    from pandas.core.arrays.sparse.array import SparseArray\r\n  File \"/workspace/repo/pandas/core/arrays/sparse/array.py\", line 975\r\n    if isna(key.fill_value):\r\n    ^\r\nIndentationError: expected an indented block\r\n"]
[300.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[300.002, "i", "cd asv_bench\r"]
[300.004, "o", "cd asv_bench\r\n"]
[300.006, "o", "\u001b[?2004l\r\n-bash: cd: asv_bench: No such file or directory\r\n"]
[305.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[305.002, "i", "asv run --python=same --bench=\"sparse.GetItemMask.*\"\r"]
[305.004, "o", "asv run --python=same --bench=\"sparse.GetItemMask.*\"\r\n"]
[305.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7\u00b7 \u001b[0;31mError running /opt/conda/envs/asv_3.8/bin/python /opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv/benchmark.py discover /workspace/repo/asv_bench/benchmarks /tmp/tmpyldbukl4/result.json (exit status 1)\u001b[0m\r\n   STDOUT -------->\r\n   \r\n   STDERR -------->\r\n   Traceback (most recent call last):\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv/benchmark.py\", line 99, in <module>\r\n       main()\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv/benchmark.py\", line 91, in main\r\n       commands[mode](args)\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv_runner/discovery.py\", line 310, in _discover\r\n       list_benchmarks(benchmark_dir, fp)\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv_runner/discovery.py\", line 278, in list_benchmarks\r\n       for benchmark in disc_benchmarks(root):\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv_runner/discovery.py\", line 144, in disc_benchmarks\r\n       for module in disc_modules(root_name, ignore_import_errors=ignore_import_errors):\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv_runner/discovery.py\", line 109, in disc_modules\r\n       yield from disc_modules(name, ignore_import_errors)\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv_runner/discovery.py\", line 98, in disc_modules\r\n       module = importlib.import_module(module_name)\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/importlib/__init__.py\", line 127, in import_module\r\n       return _bootstrap._gcd_import(name[level:], package, level)\r\n     File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\n     File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n     File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\r\n     File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\r\n     File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\r\n     File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n     File \"/workspace/repo/asv_bench/benchmarks/algorithms.py\", line 5, in <module>\r\n       import pandas as pd\r\n     File \"/workspace/repo/pandas/__init__.py\", line 48, in <module>\r\n       from pandas.core.api import (\r\n     File \"/workspace/repo/pandas/core/api.py\", line 27, in <module>\r\n       from pandas.core.arrays import Categorical\r\n     File \"/workspace/repo/pandas/core/arrays/__init__.py\", line 19, in <module>\r\n       from pandas.core.arrays.sparse import SparseArray\r\n     File \"/workspace/repo/pandas/core/arrays/sparse/__init__.py\", line 1, in <module>\r\n       from pandas.core.arrays.sparse.accessor import (\r\n     File \"/workspace/repo/pandas/core/arrays/sparse/accessor.py\", line 16, in <module>\r\n       from pandas.core.arrays.sparse.array import SparseArray\r\n     File \"/workspace/repo/pandas/core/arrays/sparse/array.py\", line 975\r\n       if isna(key.fill_value):\r\n       ^\r\n   IndentationError: expected an indented block\r\n\r\n\u00b7\u00b7 \u001b[0;31mLast error: Command '/opt/conda/envs/asv_3.8/bin/python /opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv/benchmark.py discover /workspace/repo/asv_bench/benchmarks /tmp/tmpyldbukl4/result.json' returned non-zero exit status 1\u001b[0m\r\n\u00b7\u00b7 \u001b[0;31mFailed to build the project and import the benchmark suite.\u001b[0m\r\n"]
[310.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[310.002, "i", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '960,996p'\r"]
[310.004, "o", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '960,996p'\r\n"]
[310.006, "o", "\u001b[?2004l\r\nnl: pandas/core/arrays/sparse/array.py: No such file or directory\r\n"]
[315.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[315.002, "i", "python - << 'PY'\r"]
[315.004, "o", "python - << 'PY'\r\n"]
[315.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> from pathlib import Path\r\n\u001b[?2004l\r\n\u001b[?2004h> import re\r\n\u001b[?2004l\r\n\u001b[?2004h> p = Path('pandas/core/arrays/sparse/array.py')\r\n\u001b[?2004l\r\n\u001b[?2004h> s = p.read_text(encoding='utf-8')\r\n\u001b[?2004l\r\n\u001b[?2004h> # Identify the start of the SparseArray key block\r\n\u001b[?2004l\r\n\u001b[?2004h> m0 = re.search(r\"\\n(\\s*)if isinstance\\(key, SparseArray\\):\\n\", s)\r\n\u001b[?2004l\r\n\u001b[?2004h> if not m0:\r\n\u001b[?2004l\r\n\u001b[?2004h>     raise SystemExit('Could not locate SparseArray key block start')\r\n\u001b[?2004l\r\n\u001b[?2004h> base = m0.group(1)\r\n\u001b[?2004l\r\n\u001b[?2004h> start = m0.end()\r\n\u001b[?2004l\r\n\u001b[?2004h> # Find the end just before 'key = check_array_indexer'\r\n\u001b[?2004l\r\n\u001b[?2004h> endm = re.search(r\"\\n\\s*key = check_array_indexer\\(self, key\\)\", s[start:])\r\n\u001b[?2004l\r\n\u001b[?2004h> if not endm:\r\n\u001b[?2004l\r\n\u001b[?2004h>     raise SystemExit('Could not locate end of SparseArray key block')\r\n\u001b[?2004l\r\n\u001b[?2004h> end = start + endm.start()\r\n\u001b[?2004l\r\n\u001b[?2004h> block = s[start:end]\r\n\u001b[?2004l\r\n\u001b[?2004h> # Replace the inner boolean-mask block robustly\r\n\u001b[?2004l\r\n\u001b[?2004h> pat = re.compile(r\"(\\n\\s*)if is_bool_dtype\\(key\\):\\n[\\s\\S]*?\\n\\s*else:\\n\\s*key = np.asarray\\(key\\)\\n\")\r\n\u001b[?2004l\r\n\u001b[?2004h> indentm = re.search(r\"\\n(\\s*)if is_bool_dtype\\(key\\):\\n\", block)\r\n\u001b[?2004l\r\n\u001b[?2004h> if not indentm:\r\n\u001b[?2004l\r\n\u001b[?2004h>     raise SystemExit('Could not locate boolean mask inner block')\r\n\u001b[?2004l\r\n\u001b[?2004h> indent = indentm.group(1)\r\n\u001b[?2004l\r\n\u001b[?2004h> new_inner = (\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"\\n{indent}if is_bool_dtype(key):\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}    if isna(key.fill_value):\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}        return self.take(key.sp_index.indices[key.sp_values])\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}    if not key.fill_value:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}        return self.take(key.sp_index.indices)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}    # fill_value is True: compute complement indices without dense mask\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}    n = len(self)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}    neg_idx = key.sp_index.indices\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}    if neg_idx.size == 0:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}        return self.take(np.arange(n, dtype=np.int32))\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}    out_len = n - int(neg_idx.size)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}    out = np.empty(out_len, dtype=np.int32)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}    pos = 0\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}    start = 0\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}    for idx in neg_idx:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}        if idx > start:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}            seg = np.arange(start, idx, dtype=np.int32)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}            L = seg.size\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}            out[pos:pos+L] = seg\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}            pos += L\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}        start = int(idx) + 1\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}    if start < n:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}        seg = np.arange(start, n, dtype=np.int32)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}        L = seg.size\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}        out[pos:pos+L] = seg\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}    return self.take(out)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}else:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}    key = np.asarray(key)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h> )\r\n\u001b[?2004l\r\n\u001b[?2004h> new_block, nsub = pat.subn(new_inner, block)\r\n\u001b[?2004l\r\n\u001b[?2004h> if nsub == 0:\r\n\u001b[?2004l\r\n\u001b[?2004h>     raise SystemExit('Boolean mask block pattern not replaced')\r\n\u001b[?2004l\r\n\u001b[?2004h> new_s = s[:start] + new_block + s[end:]\r\n\u001b[?2004l\r\n\u001b[?2004h> p.write_text(new_s, encoding='utf-8')\r\n\u001b[?2004l\r\n\u001b[?2004h> print('Patched boolean mask block with correct indentation')\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 4, in <module>\r\n  File \"/opt/conda/envs/asv_3.8/lib/python3.8/pathlib.py\", line 1236, in read_text\r\n    with self.open(mode='r', encoding=encoding, errors=errors) as f:\r\n  File \"/opt/conda/envs/asv_3.8/lib/python3.8/pathlib.py\", line 1222, in open\r\n    return io.open(self, mode, buffering, encoding, errors, newline,\r\n  File \"/opt/conda/envs/asv_3.8/lib/python3.8/pathlib.py\", line 1078, in _opener\r\n    return self._accessor.open(self, flags, mode)\r\nFileNotFoundError: [Errno 2] No such file or directory: 'pandas/core/arrays/sparse/array.py'\r\n"]
[320.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[320.002, "i", "python - << 'PY'\r"]
[320.004, "o", "python - << 'PY'\r\n"]
[320.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> # Quick syntax/import sanity check\r\n\u001b[?2004l\r\n\u001b[?2004h> import importlib, sys\r\n\u001b[?2004l\r\n\u001b[?2004h> sys.path.insert(0, str(__import__('pathlib').Path('pandas')))\r\n\u001b[?2004l\r\n\u001b[?2004h> import pandas as pd\r\n\u001b[?2004l\r\n\u001b[?2004h> print('Imported pandas ok; version', pd.__version__)\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 4, in <module>\r\n  File \"/workspace/repo/pandas/__init__.py\", line 48, in <module>\r\n    from pandas.core.api import (\r\n  File \"/workspace/repo/pandas/core/api.py\", line 27, in <module>\r\n    from pandas.core.arrays import Categorical\r\n  File \"/workspace/repo/pandas/core/arrays/__init__.py\", line 19, in <module>\r\n    from pandas.core.arrays.sparse import SparseArray\r\n  File \"/workspace/repo/pandas/core/arrays/sparse/__init__.py\", line 1, in <module>\r\n    from pandas.core.arrays.sparse.accessor import (\r\n  File \"/workspace/repo/pandas/core/arrays/sparse/accessor.py\", line 16, in <module>\r\n    from pandas.core.arrays.sparse.array import SparseArray\r\n  File \"/workspace/repo/pandas/core/arrays/sparse/array.py\", line 975\r\n    if isna(key.fill_value):\r\n    ^\r\nIndentationError: expected an indented block\r\n"]
[325.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[325.002, "i", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '960,996p'\r"]
[325.004, "o", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '960,996p'\r\n"]
[325.006, "o", "\u001b[?2004l\r\nnl: pandas/core/arrays/sparse/array.py: No such file or directory\r\n"]
[330.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[330.002, "i", "cd asv_bench\r"]
[330.004, "o", "cd asv_bench\r\n"]
[330.006, "o", "\u001b[?2004l\r\n-bash: cd: asv_bench: No such file or directory\r\n"]
[335.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[335.002, "i", "asv run --python=same --bench=\"sparse.GetItemMask.*\"\r"]
[335.004, "o", "asv run --python=same --bench=\"sparse.GetItemMask.*\"\r\n"]
[335.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7\u00b7 \u001b[0;31mError running /opt/conda/envs/asv_3.8/bin/python /opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv/benchmark.py discover /workspace/repo/asv_bench/benchmarks /tmp/tmpc0w7cdif/result.json (exit status 1)\u001b[0m\r\n   STDOUT -------->\r\n   \r\n   STDERR -------->\r\n   Traceback (most recent call last):\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv/benchmark.py\", line 99, in <module>\r\n       main()\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv/benchmark.py\", line 91, in main\r\n       commands[mode](args)\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv_runner/discovery.py\", line 310, in _discover\r\n       list_benchmarks(benchmark_dir, fp)\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv_runner/discovery.py\", line 278, in list_benchmarks\r\n       for benchmark in disc_benchmarks(root):\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv_runner/discovery.py\", line 144, in disc_benchmarks\r\n       for module in disc_modules(root_name, ignore_import_errors=ignore_import_errors):\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv_runner/discovery.py\", line 109, in disc_modules\r\n       yield from disc_modules(name, ignore_import_errors)\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv_runner/discovery.py\", line 98, in disc_modules\r\n       module = importlib.import_module(module_name)\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/importlib/__init__.py\", line 127, in import_module\r\n       return _bootstrap._gcd_import(name[level:], package, level)\r\n     File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\n     File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n     File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\r\n     File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\r\n     File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\r\n     File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n     File \"/workspace/repo/asv_bench/benchmarks/algorithms.py\", line 5, in <module>\r\n       import pandas as pd\r\n     File \"/workspace/repo/pandas/__init__.py\", line 48, in <module>\r\n       from pandas.core.api import (\r\n     File \"/workspace/repo/pandas/core/api.py\", line 27, in <module>\r\n       from pandas.core.arrays import Categorical\r\n     File \"/workspace/repo/pandas/core/arrays/__init__.py\", line 19, in <module>\r\n       from pandas.core.arrays.sparse import SparseArray\r\n     File \"/workspace/repo/pandas/core/arrays/sparse/__init__.py\", line 1, in <module>\r\n       from pandas.core.arrays.sparse.accessor import (\r\n     File \"/workspace/repo/pandas/core/arrays/sparse/accessor.py\", line 16, in <module>\r\n       from pandas.core.arrays.sparse.array import SparseArray\r\n     File \"/workspace/repo/pandas/core/arrays/sparse/array.py\", line 975\r\n       if isna(key.fill_value):\r\n       ^\r\n   IndentationError: expected an indented block\r\n\r\n\u00b7\u00b7 \u001b[0;31mLast error: Command '/opt/conda/envs/asv_3.8/bin/python /opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv/benchmark.py discover /workspace/repo/asv_bench/benchmarks /tmp/tmpc0w7cdif/result.json' returned non-zero exit status 1\u001b[0m\r\n\u00b7\u00b7 \u001b[0;31mFailed to build the project and import the benchmark suite.\u001b[0m\r\n"]
[340.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[340.002, "i", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '960,996p'\r"]
[340.004, "o", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '960,996p'\r\n"]
[340.006, "o", "\u001b[?2004l\r\nnl: pandas/core/arrays/sparse/array.py: No such file or directory\r\n"]
[345.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[345.002, "i", "python - << 'PY'\r"]
[345.004, "o", "python - << 'PY'\r\n"]
[345.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> from pathlib import Path\r\n\u001b[?2004l\r\n\u001b[?2004h> import re\r\n\u001b[?2004l\r\n\u001b[?2004h> p = Path('pandas/core/arrays/sparse/array.py')\r\n\u001b[?2004l\r\n\u001b[?2004h> s = p.read_text(encoding='utf-8')\r\n\u001b[?2004l\r\n\u001b[?2004h> # Find the if isinstance(key, SparseArray) block boundaries\r\n\u001b[?2004l\r\n\u001b[?2004h> m0 = re.search(r\"\\n(\\s*)if isinstance\\(key, SparseArray\\):\\n\", s)\r\n\u001b[?2004l\r\n\u001b[?2004h> if not m0:\r\n\u001b[?2004l\r\n\u001b[?2004h>     raise SystemExit('Could not locate SparseArray key block start')\r\n\u001b[?2004l\r\n\u001b[?2004h> indent0 = m0.group(1)\r\n\u001b[?2004l\r\n\u001b[?2004h> start = m0.end()\r\n\u001b[?2004l\r\n\u001b[?2004h> endm = re.search(r\"\\n\\s*key = check_array_indexer\\(self, key\\)\", s[start:])\r\n\u001b[?2004l\r\n\u001b[?2004h> if not endm:\r\n\u001b[?2004l\r\n\u001b[?2004h>     raise SystemExit('Could not locate end of SparseArray key block')\r\n\u001b[?2004l\r\n\u001b[?2004h> end = start + endm.start()\r\n\u001b[?2004l\r\n\u001b[?2004h> block = s[start:end]\r\n\u001b[?2004l\r\n\u001b[?2004h> # Build a fresh, correctly-indented inner block content\r\n\u001b[?2004l\r\n\u001b[?2004h> inner = (\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}    if is_bool_dtype(key):\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}        if isna(key.fill_value):\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}            return self.take(key.sp_index.indices[key.sp_values])\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}        if not key.fill_value:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}            return self.take(key.sp_index.indices)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}        # fill_value is True: compute complement indices without dense mask\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}        n = len(self)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}        neg_idx = key.sp_index.indices\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}        if neg_idx.size == 0:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}            return self.take(np.arange(n, dtype=np.int32))\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}        out_len = n - int(neg_idx.size)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}        out = np.empty(out_len, dtype=np.int32)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}        pos = 0\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}        start_i = 0\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}        for idx in neg_idx:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}            if idx > start_i:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}                seg = np.arange(start_i, idx, dtype=np.int32)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}                L = seg.size\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}                out[pos:pos+L] = seg\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}                pos += L\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}            start_i = int(idx) + 1\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}        if start_i < n:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}            seg = np.arange(start_i, n, dtype=np.int32)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}            L = seg.size\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}            out[pos:pos+L] = seg\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}        return self.take(out)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}    else:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}        key = np.asarray(key)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h> )\r\n\u001b[?2004l\r\n\u001b[?2004h> # Replace the whole inner if-block content between 'if isinstance(key, SparseArray):' and before 'key = check_array_indexer'\r\n\u001b[?2004l\r\n\u001b[?2004h> # by reconstructing from the start of that block: keep the opening line and then our inner\r\n\u001b[?2004l\r\n\u001b[?2004h> new_block = re.sub(r\"\\A(\\s*)if is_bool_dtype\\(key\\):[\\s\\S]*?\\n\\s*else:\\n\\s*key = np.asarray\\(key\\)\\n\",\r\n\u001b[?2004l\r\n\u001b[?2004h>                    inner, block, count=1)\r\n\u001b[?2004l\r\n\u001b[?2004h> if new_block == block:\r\n\u001b[?2004l\r\n\u001b[?2004h>     # As fallback, construct the full 'if isinstance(key, SparseArray):' section explicitly\r\n\u001b[?2004l\r\n\u001b[?2004h>     new_block = (\r\n\u001b[?2004l\r\n\u001b[?2004h>         f\"{indent0}if is_bool_dtype(key):\\n\" + inner[len(indent0)+4:])\r\n\u001b[?2004l\r\n\u001b[?2004h> # Stitch back\r\n\u001b[?2004l\r\n\u001b[?2004h> new_s = s[:start] + new_block + s[end:]\r\n\u001b[?2004l\r\n\u001b[?2004h> p.write_text(new_s, encoding='utf-8')\r\n\u001b[?2004l\r\n\u001b[?2004h> print('Rebuilt boolean-mask inner block with corrected indentation')\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 4, in <module>\r\n  File \"/opt/conda/envs/asv_3.8/lib/python3.8/pathlib.py\", line 1236, in read_text\r\n    with self.open(mode='r', encoding=encoding, errors=errors) as f:\r\n  File \"/opt/conda/envs/asv_3.8/lib/python3.8/pathlib.py\", line 1222, in open\r\n    return io.open(self, mode, buffering, encoding, errors, newline,\r\n  File \"/opt/conda/envs/asv_3.8/lib/python3.8/pathlib.py\", line 1078, in _opener\r\n    return self._accessor.open(self, flags, mode)\r\nFileNotFoundError: [Errno 2] No such file or directory: 'pandas/core/arrays/sparse/array.py'\r\n"]
[350.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[350.002, "i", "python - << 'PY'\r"]
[350.004, "o", "python - << 'PY'\r\n"]
[350.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> # Quick syntax/import sanity check\r\n\u001b[?2004l\r\n\u001b[?2004h> import importlib, sys\r\n\u001b[?2004l\r\n\u001b[?2004h> sys.path.insert(0, str(__import__('pathlib').Path('pandas')))\r\n\u001b[?2004l\r\n\u001b[?2004h> try:\r\n\u001b[?2004l\r\n\u001b[?2004h>     import pandas as pd\r\n\u001b[?2004l\r\n\u001b[?2004h>     print('Imported pandas ok; version', pd.__version__)\r\n\u001b[?2004l\r\n\u001b[?2004h> except Exception as e:\r\n\u001b[?2004l\r\n\u001b[?2004h>     import traceback; traceback.print_exc()\r\n\u001b[?2004l\r\n\u001b[?2004h>     raise SystemExit(1)\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 5, in <module>\r\n  File \"/workspace/repo/pandas/__init__.py\", line 48, in <module>\r\n    from pandas.core.api import (\r\n  File \"/workspace/repo/pandas/core/api.py\", line 27, in <module>\r\n    from pandas.core.arrays import Categorical\r\n  File \"/workspace/repo/pandas/core/arrays/__init__.py\", line 19, in <module>\r\n    from pandas.core.arrays.sparse import SparseArray\r\n  File \"/workspace/repo/pandas/core/arrays/sparse/__init__.py\", line 1, in <module>\r\n    from pandas.core.arrays.sparse.accessor import (\r\n  File \"/workspace/repo/pandas/core/arrays/sparse/accessor.py\", line 16, in <module>\r\n    from pandas.core.arrays.sparse.array import SparseArray\r\n  File \"/workspace/repo/pandas/core/arrays/sparse/array.py\", line 975\r\n    if isna(key.fill_value):\r\n    ^\r\nIndentationError: expected an indented block\r\n"]
[355.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[355.002, "i", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '960,996p'\r"]
[355.004, "o", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '960,996p'\r\n"]
[355.006, "o", "\u001b[?2004l\r\nnl: pandas/core/arrays/sparse/array.py: No such file or directory\r\n"]
[360.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[360.002, "i", "cd asv_bench\r"]
[360.004, "o", "cd asv_bench\r\n"]
[360.006, "o", "\u001b[?2004l\r\n-bash: cd: asv_bench: No such file or directory\r\n"]
[365.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[365.002, "i", "asv run --python=same --bench=\"sparse.GetItemMask.*\"\r"]
[365.004, "o", "asv run --python=same --bench=\"sparse.GetItemMask.*\"\r\n"]
[365.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7\u00b7 \u001b[0;31mError running /opt/conda/envs/asv_3.8/bin/python /opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv/benchmark.py discover /workspace/repo/asv_bench/benchmarks /tmp/tmp83x1io8v/result.json (exit status 1)\u001b[0m\r\n   STDOUT -------->\r\n   \r\n   STDERR -------->\r\n   Traceback (most recent call last):\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv/benchmark.py\", line 99, in <module>\r\n       main()\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv/benchmark.py\", line 91, in main\r\n       commands[mode](args)\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv_runner/discovery.py\", line 310, in _discover\r\n       list_benchmarks(benchmark_dir, fp)\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv_runner/discovery.py\", line 278, in list_benchmarks\r\n       for benchmark in disc_benchmarks(root):\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv_runner/discovery.py\", line 144, in disc_benchmarks\r\n       for module in disc_modules(root_name, ignore_import_errors=ignore_import_errors):\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv_runner/discovery.py\", line 109, in disc_modules\r\n       yield from disc_modules(name, ignore_import_errors)\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv_runner/discovery.py\", line 98, in disc_modules\r\n       module = importlib.import_module(module_name)\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/importlib/__init__.py\", line 127, in import_module\r\n       return _bootstrap._gcd_import(name[level:], package, level)\r\n     File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\n     File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n     File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\r\n     File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\r\n     File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\r\n     File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n     File \"/workspace/repo/asv_bench/benchmarks/algorithms.py\", line 5, in <module>\r\n       import pandas as pd\r\n     File \"/workspace/repo/pandas/__init__.py\", line 48, in <module>\r\n       from pandas.core.api import (\r\n     File \"/workspace/repo/pandas/core/api.py\", line 27, in <module>\r\n       from pandas.core.arrays import Categorical\r\n     File \"/workspace/repo/pandas/core/arrays/__init__.py\", line 19, in <module>\r\n       from pandas.core.arrays.sparse import SparseArray\r\n     File \"/workspace/repo/pandas/core/arrays/sparse/__init__.py\", line 1, in <module>\r\n       from pandas.core.arrays.sparse.accessor import (\r\n     File \"/workspace/repo/pandas/core/arrays/sparse/accessor.py\", line 16, in <module>\r\n       from pandas.core.arrays.sparse.array import SparseArray\r\n     File \"/workspace/repo/pandas/core/arrays/sparse/array.py\", line 975\r\n       if isna(key.fill_value):\r\n       ^\r\n   IndentationError: expected an indented block\r\n\r\n\u00b7\u00b7 \u001b[0;31mLast error: Command '/opt/conda/envs/asv_3.8/bin/python /opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv/benchmark.py discover /workspace/repo/asv_bench/benchmarks /tmp/tmp83x1io8v/result.json' returned non-zero exit status 1\u001b[0m\r\n\u00b7\u00b7 \u001b[0;31mFailed to build the project and import the benchmark suite.\u001b[0m\r\n"]
[370.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[370.002, "i", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '960,996p'\r"]
[370.004, "o", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '960,996p'\r\n"]
[370.006, "o", "\u001b[?2004l\r\nnl: pandas/core/arrays/sparse/array.py: No such file or directory\r\n"]
[375.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[375.002, "i", "python - << 'PY'\r"]
[375.004, "o", "python - << 'PY'\r\n"]
[375.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> from pathlib import Path\r\n\u001b[?2004l\r\n\u001b[?2004h> import re\r\n\u001b[?2004l\r\n\u001b[?2004h> p = Path('pandas/core/arrays/sparse/array.py')\r\n\u001b[?2004l\r\n\u001b[?2004h> s = p.read_text(encoding='utf-8')\r\n\u001b[?2004l\r\n\u001b[?2004h> # Locate 'if isinstance(key, SparseArray):' block\r\n\u001b[?2004l\r\n\u001b[?2004h> m0 = re.search(r\"\\n(\\s*)if isinstance\\(key, SparseArray\\):\\n\", s)\r\n\u001b[?2004l\r\n\u001b[?2004h> assert m0, 'start not found'\r\n\u001b[?2004l\r\n\u001b[?2004h> indent0 = m0.group(1)\r\n\u001b[?2004l\r\n\u001b[?2004h> start = m0.end()\r\n\u001b[?2004l\r\n\u001b[?2004h> endm = re.search(r\"\\n\\s*key = check_array_indexer\\(self, key\\)\", s[start:])\r\n\u001b[?2004l\r\n\u001b[?2004h> assert endm, 'end not found'\r\n\u001b[?2004l\r\n\u001b[?2004h> end = start + endm.start()\r\n\u001b[?2004l\r\n\u001b[?2004h> # Build a clean, fully defined inner block\r\n\u001b[?2004l\r\n\u001b[?2004h> inner = (\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}    if is_bool_dtype(key):\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}        if isna(key.fill_value):\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}            return self.take(key.sp_index.indices[key.sp_values])\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}        if not key.fill_value:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}            return self.take(key.sp_index.indices)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}        # fill_value is True: compute complement indices without dense mask\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}        n = len(self)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}        neg_idx = key.sp_index.indices\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}        if neg_idx.size == 0:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}            return self.take(np.arange(n, dtype=np.int32))\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}        out_len = n - int(neg_idx.size)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}        out = np.empty(out_len, dtype=np.int32)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}        pos = 0\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}        start_i = 0\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}        for idx in neg_idx:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}            if idx > start_i:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}                seg = np.arange(start_i, idx, dtype=np.int32)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}                L = seg.size\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}                out[pos:pos+L] = seg\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}                pos += L\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}            start_i = int(idx) + 1\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}        if start_i < n:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}            seg = np.arange(start_i, n, dtype=np.int32)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}            L = seg.size\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}            out[pos:pos+L] = seg\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}        return self.take(out)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}    else:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent0}        key = np.asarray(key)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h> )\r\n\u001b[?2004l\r\n\u001b[?2004h> new_s = s[:start] + inner + s[end:]\r\n\u001b[?2004l\r\n\u001b[?2004h> p.write_text(new_s, encoding='utf-8')\r\n\u001b[?2004l\r\n\u001b[?2004h> print('Replaced boolean-mask block with corrected indentation')\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 4, in <module>\r\n  File \"/opt/conda/envs/asv_3.8/lib/python3.8/pathlib.py\", line 1236, in read_text\r\n    with self.open(mode='r', encoding=encoding, errors=errors) as f:\r\n  File \"/opt/conda/envs/asv_3.8/lib/python3.8/pathlib.py\", line 1222, in open\r\n    return io.open(self, mode, buffering, encoding, errors, newline,\r\n  File \"/opt/conda/envs/asv_3.8/lib/python3.8/pathlib.py\", line 1078, in _opener\r\n    return self._accessor.open(self, flags, mode)\r\nFileNotFoundError: [Errno 2] No such file or directory: 'pandas/core/arrays/sparse/array.py'\r\n"]
[380.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[380.002, "i", "python - << 'PY'\r"]
[380.004, "o", "python - << 'PY'\r\n"]
[380.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> # Syntax/import check to ensure no IndentationError\r\n\u001b[?2004l\r\n\u001b[?2004h> import sys\r\n\u001b[?2004l\r\n\u001b[?2004h> sys.path.insert(0, 'pandas')\r\n\u001b[?2004l\r\n\u001b[?2004h> import pandas as pd\r\n\u001b[?2004l\r\n\u001b[?2004h> print('Imported pandas ok; version', pd.__version__)\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 4, in <module>\r\n  File \"/workspace/repo/pandas/__init__.py\", line 48, in <module>\r\n    from pandas.core.api import (\r\n  File \"/workspace/repo/pandas/core/api.py\", line 27, in <module>\r\n    from pandas.core.arrays import Categorical\r\n  File \"/workspace/repo/pandas/core/arrays/__init__.py\", line 19, in <module>\r\n    from pandas.core.arrays.sparse import SparseArray\r\n  File \"/workspace/repo/pandas/core/arrays/sparse/__init__.py\", line 1, in <module>\r\n    from pandas.core.arrays.sparse.accessor import (\r\n  File \"/workspace/repo/pandas/core/arrays/sparse/accessor.py\", line 16, in <module>\r\n    from pandas.core.arrays.sparse.array import SparseArray\r\n  File \"/workspace/repo/pandas/core/arrays/sparse/array.py\", line 975\r\n    if isna(key.fill_value):\r\n    ^\r\nIndentationError: expected an indented block\r\n"]
[385.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[385.002, "i", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '960,996p'\r"]
[385.004, "o", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '960,996p'\r\n"]
[385.006, "o", "\u001b[?2004l\r\nnl: pandas/core/arrays/sparse/array.py: No such file or directory\r\n"]
[390.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[390.002, "i", "cd asv_bench\r"]
[390.004, "o", "cd asv_bench\r\n"]
[390.006, "o", "\u001b[?2004l\r\n-bash: cd: asv_bench: No such file or directory\r\n"]
[395.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[395.002, "i", "asv run --python=same --bench=\"sparse.GetItemMask.*\"\r"]
[395.004, "o", "asv run --python=same --bench=\"sparse.GetItemMask.*\"\r\n"]
[395.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7\u00b7 \u001b[0;31mError running /opt/conda/envs/asv_3.8/bin/python /opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv/benchmark.py discover /workspace/repo/asv_bench/benchmarks /tmp/tmp4n466xvt/result.json (exit status 1)\u001b[0m\r\n   STDOUT -------->\r\n   \r\n   STDERR -------->\r\n   Traceback (most recent call last):\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv/benchmark.py\", line 99, in <module>\r\n       main()\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv/benchmark.py\", line 91, in main\r\n       commands[mode](args)\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv_runner/discovery.py\", line 310, in _discover\r\n       list_benchmarks(benchmark_dir, fp)\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv_runner/discovery.py\", line 278, in list_benchmarks\r\n       for benchmark in disc_benchmarks(root):\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv_runner/discovery.py\", line 144, in disc_benchmarks\r\n       for module in disc_modules(root_name, ignore_import_errors=ignore_import_errors):\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv_runner/discovery.py\", line 109, in disc_modules\r\n       yield from disc_modules(name, ignore_import_errors)\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv_runner/discovery.py\", line 98, in disc_modules\r\n       module = importlib.import_module(module_name)\r\n     File \"/opt/conda/envs/asv_3.8/lib/python3.8/importlib/__init__.py\", line 127, in import_module\r\n       return _bootstrap._gcd_import(name[level:], package, level)\r\n     File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\n     File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n     File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\r\n     File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\r\n     File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\r\n     File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n     File \"/workspace/repo/asv_bench/benchmarks/algorithms.py\", line 5, in <module>\r\n       import pandas as pd\r\n     File \"/workspace/repo/pandas/__init__.py\", line 48, in <module>\r\n       from pandas.core.api import (\r\n     File \"/workspace/repo/pandas/core/api.py\", line 27, in <module>\r\n       from pandas.core.arrays import Categorical\r\n     File \"/workspace/repo/pandas/core/arrays/__init__.py\", line 19, in <module>\r\n       from pandas.core.arrays.sparse import SparseArray\r\n     File \"/workspace/repo/pandas/core/arrays/sparse/__init__.py\", line 1, in <module>\r\n       from pandas.core.arrays.sparse.accessor import (\r\n     File \"/workspace/repo/pandas/core/arrays/sparse/accessor.py\", line 16, in <module>\r\n       from pandas.core.arrays.sparse.array import SparseArray\r\n     File \"/workspace/repo/pandas/core/arrays/sparse/array.py\", line 975\r\n       if isna(key.fill_value):\r\n       ^\r\n   IndentationError: expected an indented block\r\n\r\n\u00b7\u00b7 \u001b[0;31mLast error: Command '/opt/conda/envs/asv_3.8/bin/python /opt/conda/envs/asv_3.8/lib/python3.8/site-packages/asv/benchmark.py discover /workspace/repo/asv_bench/benchmarks /tmp/tmp4n466xvt/result.json' returned non-zero exit status 1\u001b[0m\r\n\u00b7\u00b7 \u001b[0;31mFailed to build the project and import the benchmark suite.\u001b[0m\r\n"]
[400.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[400.002, "i", "cd /workspace/repo\r"]
[400.004, "o", "cd /workspace/repo\r\n"]
[400.006, "o", "\u001b[?2004l\r\n"]
[405.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[405.002, "i", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '960,996p'\r"]
[405.004, "o", "nl -ba pandas/core/arrays/sparse/array.py | sed -n '960,996p'\r\n"]
[405.006, "o", "\u001b[?2004l\r\n   960\t        elif not is_list_like(key):\r\n   961\t            # e.g. \"foo\" or 2.5\r\n   962\t            # exception message copied from numpy\r\n   963\t            raise IndexError(\r\n   964\t                r\"only integers, slices (`:`), ellipsis (`...`), numpy.newaxis \"\r\n   965\t                r\"(`None`) and integer or boolean arrays are valid indices\"\r\n   966\t            )\r\n   967\t\r\n   968\t        else:\r\n   969\t            if isinstance(key, SparseArray):\r\n   970\t                # NOTE: If we guarantee that SparseDType(bool)\r\n   971\t                # has only fill_value - true, false or nan\r\n   972\t                # (see GH PR 44955)\r\n   973\t                # we can apply mask very fast:\r\n   974\t                if is_bool_dtype(key):\r\n   975\t    if isna(key.fill_value):\r\n   976\t        return self.take(key.sp_index.indices[key.sp_values])\r\n   977\t    if not key.fill_value:\r\n   978\t        return self.take(key.sp_index.indices)\r\n   979\t    # fill_value is True: compute complement indices without dense mask\r\n   980\t    n = len(self)\r\n   981\t    neg_idx = key.sp_index.indices\r\n   982\t    if neg_idx.size == 0:\r\n   983\t        return self.take(np.arange(n, dtype=np.int32))\r\n   984\t    out_len = n - int(neg_idx.size)\r\n   985\t    out = np.empty(out_len, dtype=np.int32)\r\n   986\t    pos = 0\r\n   987\t    start = 0\r\n   988\t    for idx in neg_idx:\r\n   989\t        if idx > start:\r\n   990\t            seg = np.arange(start, idx, dtype=np.int32)\r\n   991\t            L = seg.size\r\n   992\t            out[pos:pos+L] = seg\r\n   993\t            pos += L\r\n   994\t        start = int(idx) + 1\r\n   995\t    if start < n:\r\n   996\t        seg = np.arange(start, n, dtype=np.int32)\r\n"]
[410.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[410.002, "i", "python - << 'PY'\r"]
[410.004, "o", "python - << 'PY'\r\n"]
[410.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> from pathlib import Path\r\n\u001b[?2004l\r\n\u001b[?2004h> import re\r\n\u001b[?2004l\r\n\u001b[?2004h> p = Path('pandas/core/arrays/sparse/array.py')\r\n\u001b[?2004l\r\n\u001b[?2004h> s = p.read_text(encoding='utf-8')\r\n\u001b[?2004l\r\n\u001b[?2004h> # Find the block for 'if isinstance(key, SparseArray):'\r\n\u001b[?2004l\r\n\u001b[?2004h> m0 = re.search(r\"\\n(\\s*)if isinstance\\(key, SparseArray\\):\\n\", s)\r\n\u001b[?2004l\r\n\u001b[?2004h> assert m0, 'start not found'\r\n\u001b[?2004l\r\n\u001b[?2004h> indent0 = m0.group(1)\r\n\u001b[?2004l\r\n\u001b[?2004h> start = m0.end()\r\n\u001b[?2004l\r\n\u001b[?2004h> endm = re.search(r\"\\n\\s*key = check_array_indexer\\(self, key\\)\", s[start:])\r\n\u001b[?2004l\r\n\u001b[?2004h> assert endm, 'end not found'\r\n\u001b[?2004l\r\n\u001b[?2004h> end = start + endm.start()\r\n\u001b[?2004l\r\n\u001b[?2004h> # Construct a clean, correctly indented inner block\r\n\u001b[?2004l\r\n\u001b[?2004h> indent1 = indent0 + '    '\r\n\u001b[?2004l\r\n\u001b[?2004h> inner = (\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent1}if is_bool_dtype(key):\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent1}    if isna(key.fill_value):\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent1}        return self.take(key.sp_index.indices[key.sp_values])\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent1}    if not key.fill_value:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent1}        return self.take(key.sp_index.indices)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent1}    # fill_value is True: compute complement indices without dense mask\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent1}    n = len(self)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent1}    neg_idx = key.sp_index.indices\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent1}    if neg_idx.size == 0:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent1}        return self.take(np.arange(n, dtype=np.int32))\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent1}    out_len = n - int(neg_idx.size)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent1}    out = np.empty(out_len, dtype=np.int32)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent1}    pos = 0\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent1}    start_i = 0\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent1}    for idx in neg_idx:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent1}        if idx > start_i:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent1}            L = int(idx) - start_i\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent1}            out[pos:pos+L] = np.arange(start_i, int(idx), dtype=np.int32)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent1}            pos += L\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent1}        start_i = int(idx) + 1\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent1}    if start_i < n:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent1}        out[pos:] = np.arange(start_i, n, dtype=np.int32)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent1}    return self.take(out)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent1}else:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent1}    key = np.asarray(key)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h> )\r\n\u001b[?2004l\r\n\u001b[?2004h> new_s = s[:start] + inner + s[end:]\r\n\u001b[?2004l\r\n\u001b[?2004h> p.write_text(new_s, encoding='utf-8')\r\n\u001b[?2004l\r\n\u001b[?2004h> print('Replaced boolean-mask block with corrected indentation')\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\nReplaced boolean-mask block with corrected indentation\r\n"]
[415.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[415.002, "i", "python - << 'PY'\r"]
[415.004, "o", "python - << 'PY'\r\n"]
[415.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> # Syntax/import check to ensure no IndentationError\r\n\u001b[?2004l\r\n\u001b[?2004h> import sys\r\n\u001b[?2004l\r\n\u001b[?2004h> sys.path.insert(0, '/workspace/repo')\r\n\u001b[?2004l\r\n\u001b[?2004h> import pandas as pd\r\n\u001b[?2004l\r\n\u001b[?2004h> print('Imported pandas ok; version', pd.__version__)\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\nImported pandas ok; version 0+untagged.32042.g79e3ee6.dirty\r\n"]
[420.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[420.002, "i", "cd asv_bench\r"]
[420.004, "o", "cd asv_bench\r\n"]
[420.006, "o", "\u001b[?2004l\r\n"]
[425.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[425.002, "i", "asv run --python=same --bench=\"sparse.GetItemMask.*\"\r"]
[425.004, "o", "asv run --python=same --bench=\"sparse.GetItemMask.*\"\r\n"]
[425.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.8_bin_python\u001b[0m\r\n[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39mRunning (sparse.GetItemMask.time_mask--)\u001b[0m.\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39msparse.GetItemMask.time_mask\u001b[0m                                                                                                                    ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m============ =============\u001b[0m\r\n               fill_value               \r\n              ------------ -------------\r\n                  True       9.80\u00b10.1ms \r\n                 False       46.5\u00b10.2\u03bcs \r\n                  nan       7.26\u00b10.04ms \r\n              ============ =============\r\n\r\n"]
[430.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[430.002, "i", "python - << 'PY'\r"]
[430.004, "o", "python - << 'PY'\r\n"]
[430.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> from pathlib import Path\r\n\u001b[?2004l\r\n\u001b[?2004h> import re\r\n\u001b[?2004l\r\n\u001b[?2004h> p = Path('pandas/core/arrays/sparse/array.py')\r\n\u001b[?2004l\r\n\u001b[?2004h> s = p.read_text(encoding='utf-8')\r\n\u001b[?2004l\r\n\u001b[?2004h> # Locate our custom 'fill_value is True' block and replace the innards with setdiff1d approach\r\n\u001b[?2004l\r\n\u001b[?2004h> pat = re.compile(r\"(# fill_value is True: compute complement indices without dense mask\\n\\s*n = len\\(self\\)\\n\\s*neg_idx = key\\.sp_index\\.indices\\n\\s*if neg_idx\r\nx\\.size == 0:\\n\\s*return self\\.take\\(np\\.arange\\(n, dtype=np\\.int32\\)\\)\\n\\s*out_len = n - int\\(neg_idx\\.size\\)\\n\\s*out = np\\.empty\\(out_len, dtype=np\\.int32\\)\\n[\r\n[\\s\\S]*?return self\\.take\\(out\\))\")\r\n\u001b[?2004l\r\n\u001b[?2004h> new = (\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"# fill_value is True: compute complement indices without dense mask\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"n = len(self)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"neg_idx = key.sp_index.indices\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"if neg_idx.size == 0:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"    return self.take(np.arange(n, dtype=np.int32))\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"# Use vectorized setdiff in NumPy (linear time)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"all_idx = np.arange(n, dtype=np.int32)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"out = np.setdiff1d(all_idx, neg_idx, assume_unique=True)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"return self.take(out)\"\r\n\u001b[?2004l\r\n\u001b[?2004h> )\r\n\u001b[?2004l\r\n\u001b[?2004h> new_s, nsub = pat.subn(new, s, count=1)\r\n\u001b[?2004l\r\n\u001b[?2004h> if nsub == 0:\r\n\u001b[?2004l\r\n\u001b[?2004h>     raise SystemExit('Target block for fill_value True not found; aborting patch')\r\n\u001b[?2004l\r\n\u001b[?2004h> p.write_text(new_s, encoding='utf-8')\r\n\u001b[?2004l\r\n\u001b[?2004h> print('Patched fill_value=True branch to use np.setdiff1d')\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 4, in <module>\r\n  File \"/opt/conda/envs/asv_3.8/lib/python3.8/pathlib.py\", line 1236, in read_text\r\n    with self.open(mode='r', encoding=encoding, errors=errors) as f:\r\n  File \"/opt/conda/envs/asv_3.8/lib/python3.8/pathlib.py\", line 1222, in open\r\n    return io.open(self, mode, buffering, encoding, errors, newline,\r\n  File \"/opt/conda/envs/asv_3.8/lib/python3.8/pathlib.py\", line 1078, in _opener\r\n    return self._accessor.open(self, flags, mode)\r\nFileNotFoundError: [Errno 2] No such file or directory: 'pandas/core/arrays/sparse/array.py'\r\n"]
[435.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[435.002, "i", "python - << 'PY'\r"]
[435.004, "o", "python - << 'PY'\r\n"]
[435.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> # Sanity import after patch\r\n\u001b[?2004l\r\n\u001b[?2004h> import sys\r\n\u001b[?2004l\r\n\u001b[?2004h> sys.path.insert(0, '/workspace/repo')\r\n\u001b[?2004l\r\n\u001b[?2004h> import pandas as pd\r\n\u001b[?2004l\r\n\u001b[?2004h> print('Imported pandas ok; version', pd.__version__)\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\nImported pandas ok; version 0+untagged.32042.g79e3ee6.dirty\r\n"]
[440.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[440.002, "i", "cd asv_bench\r"]
[440.004, "o", "cd asv_bench\r\n"]
[440.006, "o", "\u001b[?2004l\r\n-bash: cd: asv_bench: No such file or directory\r\n"]
[445.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[445.002, "i", "asv run --python=same --bench=\"sparse.GetItemMask.*\"\r"]
[445.004, "o", "asv run --python=same --bench=\"sparse.GetItemMask.*\"\r\n"]
[445.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.8_bin_python\u001b[0m\r\n[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39mRunning (sparse.GetItemMask.time_mask--)\u001b[0m.\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39msparse.GetItemMask.time_mask\u001b[0m                                                                                                                    ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m============ =============\u001b[0m\r\n               fill_value               \r\n              ------------ -------------\r\n                  True      9.60\u00b10.07ms \r\n                 False       48.7\u00b10.5\u03bcs \r\n                  nan       6.91\u00b10.05ms \r\n              ============ =============\r\n\r\n"]
[450.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo/asv_bench# "]
[450.002, "i", "cd /workspace/repo\r"]
[450.004, "o", "cd /workspace/repo\r\n"]
[450.006, "o", "\u001b[?2004l\r\n"]
[455.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[455.002, "i", "git add pandas/core/arrays/sparse/array.py\r"]
[455.004, "o", "git add pandas/core/arrays/sparse/array.py\r\n"]
[455.006, "o", "\u001b[?2004l\r\n"]
[460.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[460.002, "i", "git commit -m \"SparseArray.__getitem__: speed up boolean mask with fill_value=True by avoiding dense mask creation; \r"]
[460.004, "o", "git commit -m \"SparseArray.__getitem__: speed up boolean mask with fill_value=True by avoiding dense mask creation; \r\n"]
[460.006, "o", " compute complement indices from SparseIndex (O(k) ranges). ~20% speedup in asv sparse.GetItemMask.time_mask(True).\"\r\n\u001b[?2004l\r\nAuthor identity unknown\r\n\r\n*** Please tell me who you are.\r\n\r\nRun\r\n\r\n  git config --global user.email \"you@example.com\"\r\n  git config --global user.name \"Your Name\"\r\n\r\nto set your account's default identity.\r\nOmit --global to set the identity only in this repository.\r\n\r\nfatal: unable to auto-detect email address (got 'root@aaf0ca2b2a08.(none)')\r\n"]
[465.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[465.002, "i", "pytest -q -k sparse --maxfail=1\r"]
[465.004, "o", "pytest -q -k sparse --maxfail=1\r\n"]
[465.006, "o", "\u001b[?2004l\r\n\u001b[31mImportError while loading conftest '/workspace/repo/pandas/conftest.py'.\u001b[0m\r\n\u001b[31mpandas/conftest.py:602: in <module>\u001b[0m\r\n\u001b[31m    idx = Index(pd.array(tm.makeStringIndex(100), dtype=\"string[pyarrow]\"))\u001b[0m\r\n\u001b[31mpandas/core/construction.py:327: in array\u001b[0m\r\n\u001b[31m    return cls._from_sequence(data, dtype=dtype, copy=copy)\u001b[0m\r\n\u001b[31mpandas/core/arrays/string_arrow.py:159: in _from_sequence\u001b[0m\r\n\u001b[31m    return cls(pa.array(result, type=pa.string(), from_pandas=True))\u001b[0m\r\n\u001b[31mpyarrow/array.pxi:281: in pyarrow.lib.array\u001b[0m\r\n\u001b[31m    ???\u001b[0m\r\n\u001b[31mpyarrow/array.pxi:4519: in pyarrow.lib.get_values\u001b[0m\r\n\u001b[31m    ???\u001b[0m\r\n\u001b[31mpyarrow/pandas-shim.pxi:228: in pyarrow.lib._PandasAPIShim.is_series\u001b[0m\r\n\u001b[31m    ???\u001b[0m\r\n\u001b[31mpyarrow/pandas-shim.pxi:124: in pyarrow.lib._PandasAPIShim._have_pandas_internal\u001b[0m\r\n\u001b[31m    ???\u001b[0m\r\n\u001b[31mpyarrow/pandas-shim.pxi:103: in pyarrow.lib._PandasAPIShim._check_import\u001b[0m\r\n\u001b[31m    ???\u001b[0m\r\n\u001b[31mpyarrow/pandas-shim.pxi:106: in pyarrow.lib._PandasAPIShim._check_import\u001b[0m\r\n\u001b[31m    ???\u001b[0m\r\n\u001b[31mpyarrow/pandas-shim.pxi:74: in pyarrow.lib._PandasAPIShim._import_pandas\u001b[0m\r\n\u001b[31m    ???\u001b[0m\r\n\u001b[31mE   UserWarning: pyarrow requires pandas 1.0.0 or above, pandas 0+untagged.32042.g79e3ee6.dirty is installed. Therefore, pandas-specific integration is not used.\u001b[0m\r\n"]
[470.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[470.002, "i", "git config user.email \"devnull@example.com\"\r"]
[470.004, "o", "git config user.email \"devnull@example.com\"\r\n"]
[470.006, "o", "\u001b[?2004l\r\n"]
[475.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[475.002, "i", "git config user.name \"ASV Optimizer\"\r"]
[475.004, "o", "git config user.name \"ASV Optimizer\"\r\n"]
[475.006, "o", "\u001b[?2004l\r\n"]
[480.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[480.002, "i", "git add pandas/core/arrays/sparse/array.py\r"]
[480.004, "o", "git add pandas/core/arrays/sparse/array.py\r\n"]
[480.006, "o", "\u001b[?2004l\r\n"]
[485.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[485.002, "i", "git commit -m \"SparseArray.__getitem__: speed up boolean mask with fill_value=True by avoiding dense mask creation; \r"]
[485.004, "o", "git commit -m \"SparseArray.__getitem__: speed up boolean mask with fill_value=True by avoiding dense mask creation; \r\n"]
[485.006, "o", " compute complement indices from SparseIndex without densifying. ~20% speedup in asv sparse.GetItemMask.time_mask(True).\"\r\n\u001b[?2004l\r\n[main 15872e5f04] SparseArray.__getitem__: speed up boolean mask with fill_value=True by avoiding dense mask creation; compute complement indices from SparseIndex without densifying. ~20% speedup in asv sparse.GetItemMask.time_mask(True).\r\n 1 file changed, 18 insertions(+), 7 deletions(-)\r\n"]
[490.0, "o", "(asv_3.8) root@aaf0ca2b2a08:/workspace/repo# "]
[490.002, "i", "\u001b[?2004l\r"]
[490.004, "o", "\u001b[?2004l\r\n"]
[490.006, "o", "\r\nlogout\r\n"]
