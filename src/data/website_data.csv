id,level,agent_id,agent/nop,oracle/nop,task_id,agent_recording,repo_name,benchmark_name,benchmark_decoposed,benchmark_codes,benchmark_without_params,benchmark_type,advantage
42,param-level,"terminus-2,gpt-5",0.9690183933220252,1.0166635229568928,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,modeling.model.time_init_gaussian_with_units,modeling.model,"{""modeling.model.time_init_gaussian_with_units"": ""def time_init_gaussian_with_units():\n    models.Gaussian1D(amplitude=10 * u.Hz, mean=5 * u.m, stddev=1.2 * u.cm)""}",modeling.model.time_init_gaussian_with_units,time,-0.03369528262720478
56,param-level,"terminus-2,oracle",0.9502411460411848,0.9502411460411848,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_time,imports,"{""imports.timeraw_import_astropy_time"": ""def timeraw_import_astropy_time():\n    return \""\""\""\n    from astropy import time\n    \""\""\""""}",imports.timeraw_import_astropy_time,time,0.0
46,param-level,"terminus-2,gpt-5",0.977650517371278,0.9528888363018873,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,wcs.WCSTransformations.time_pix2world_x_y_0,wcs,"{""wcs.WCSTransformations.time_pix2world_x_y_0"": ""class WCSTransformations:\n    def time_pix2world_x_y_0(self, size):\n        wcs.wcs_pix2world(self.px, self.py, 0)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)""}",wcs.WCSTransformations.time_pix2world_x_y_0,time,0.017511797078777012
32,param-level,"terminus-2,gpt-5",1.0372915399081946,1.0537416530971275,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.core.CoreSuite.time_default_splitter_call,io_ascii.core,"{""io_ascii.core.CoreSuite.time_default_splitter_call"": ""class CoreSuite:\n    def time_default_splitter_call(self):\n        core.DefaultSplitter()(self.csv_line)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]""}",io_ascii.core.CoreSuite.time_default_splitter_call,time,-0.011633743415086965
75,param-level,"terminus-2,oracle",1.092478111708183,1.092478111708183,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.CommentedHeaderInt.time_write,io_ascii.main,"{""io_ascii.main.CommentedHeaderInt.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.CommentedHeaderInt.time_write,time,0.0
62,param-level,"terminus-2,oracle",0.9433432111662834,0.9433432111662834,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_utils,imports,"{""imports.timeraw_import_astropy_utils"": ""def timeraw_import_astropy_utils():\n    return \""\""\""\n    from astropy import utils\n    \""\""\""""}",imports.timeraw_import_astropy_utils,time,0.0
26,param-level,"terminus-2,gpt-5",1.1090733624811406,1.0611753029756317,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_io_misc,imports,"{""imports.timeraw_import_astropy_io_misc"": ""def timeraw_import_astropy_io_misc():\n    return \""\""\""\n    from astropy.io import misc\n    \""\""\""""}",imports.timeraw_import_astropy_io_misc,time,0.03387415806613081
30,param-level,"terminus-2,gpt-5",1.0560109880479434,0.9874496199734252,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_timeseries_io,imports,"{""imports.timeraw_import_astropy_timeseries_io"": ""def timeraw_import_astropy_timeseries_io():\n    return \""\""\""\n    from astropy.timeseries import io\n    \""\""\""""}",imports.timeraw_import_astropy_timeseries_io,time,0.04848753046288415
45,param-level,"terminus-2,gpt-5",1.025835527044144,1.0312724196031475,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,wcs.WCSTransformations.time_ape14_world_to_pixel,wcs,"{""wcs.WCSTransformations.time_ape14_world_to_pixel"": ""class WCSTransformations:\n    def time_ape14_world_to_pixel(self, size):\n        wcs.world_to_pixel(self.coord)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)""}",wcs.WCSTransformations.time_ape14_world_to_pixel,time,-0.003845044242576798
84,param-level,"terminus-2,oracle",1.0168241644485905,1.0168241644485905,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_fits.FITSHeader.time_get_int,io_fits,"{""io_fits.FITSHeader.time_get_int"": ""class FITSHeader:\n    def time_get_int(self):\n        self.hdr.get(\""INT999\"")\n\n    def setup(self):\n        self.hdr = make_header()\n        self.hdr_string = self.hdr.tostring()""}",io_fits.FITSHeader.time_get_int,time,0.0
12,param-level,"terminus-2,claude",1.0532855922428737,1.0187313338888078,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,stats.sigma_clipping.SigmaClipBenchmarks.time_3d_array_axis2,stats.sigma_clipping,"{""stats.sigma_clipping.SigmaClipBenchmarks.time_3d_array_axis2"": ""class SigmaClipBenchmarks:\n    def time_3d_array_axis2(self):\n        self.sigclip(self.data, axis=(0, 1))\n\n    def setup(self):\n        # Avoid top-level module import to make sure that the benchmarks are\n        # compatible with versions of astropy that did not have this functionality.\n        from astropy.stats import SigmaClip\n    \n        size = (4, 2048, 2048)\n    \n        with NumpyRNGContext(12345):\n            self.data = np.random.normal(size=size)\n    \n            # add outliers\n            nbad = 100000\n            zbad = np.random.randint(low=0, high=size[0] - 1, size=nbad)\n            ybad = np.random.randint(low=0, high=size[1] - 1, size=nbad)\n            xbad = np.random.randint(low=0, high=size[2] - 1, size=nbad)\n            self.data[zbad, ybad, xbad] = np.random.choice([-1, 1], size=nbad) * (\n                10 + np.random.rand(nbad)\n            )\n    \n            # The defaults use median as the cenfunc and standard\n            # deviation as the stdfunc.  The default iters is 5.\n            self.sigclip = SigmaClip(sigma=3)""}",stats.sigma_clipping.SigmaClipBenchmarks.time_3d_array_axis2,time,0.024437240703016897
53,param-level,"terminus-2,oracle",1.0611753029756317,1.0611753029756317,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_io_misc,imports,"{""imports.timeraw_import_astropy_io_misc"": ""def timeraw_import_astropy_io_misc():\n    return \""\""\""\n    from astropy.io import misc\n    \""\""\""""}",imports.timeraw_import_astropy_io_misc,time,0.0
39,param-level,"terminus-2,gpt-5",1.065910977414392,1.055795161606203,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.TabInt.time_write,io_ascii.main,"{""io_ascii.main.TabInt.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.TabInt.time_write,time,0.007154042297163437
79,param-level,"terminus-2,oracle",1.0677605090251543,1.0677605090251543,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.SextractorString.time_read,io_ascii.main,"{""io_ascii.main.SextractorString.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.SextractorString.time_read,time,0.0
6,param-level,"terminus-2,claude",0.955402557487482,0.93716742554461,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_wcs,imports,"{""imports.timeraw_import_astropy_wcs"": ""def timeraw_import_astropy_wcs():\n    return \""\""\""\n    from astropy import wcs\n    \""\""\""""}",imports.timeraw_import_astropy_wcs,time,0.012896132915751113
51,param-level,"terminus-2,oracle",1.0791845115486305,1.0791845115486305,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_io,imports,"{""imports.timeraw_import_astropy_io"": ""def timeraw_import_astropy_io():\n    return \""\""\""\n    from astropy import io\n    \""\""\""""}",imports.timeraw_import_astropy_io,time,0.0
43,param-level,"terminus-2,gpt-5",1.0761991296122446,1.070279171332443,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,table.TimeTableInitWithLists.time_init_lists,table,"{""table.TimeTableInitWithLists.time_init_lists"": ""class TimeTableInitWithLists:\n    def time_init_lists(self):\n        Table([self.dat, self.dat, self.dat], names=[\""time\"", \""rate\"", \""error\""])\n\n    def setup(self):\n        self.dat = list(range(100_000))""}",table.TimeTableInitWithLists.time_init_lists,time,0.004186674879633323
94,param-level,"terminus-2,oracle",0.9383809606086992,0.9383809606086992,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.TimeTable.time_init_from_np_array_copy,table,"{""table.TimeTable.time_init_from_np_array_copy"": ""class TimeTable:\n    def time_init_from_np_array_copy(self):\n        Table(self.np_table, copy=True)\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6""}",table.TimeTable.time_init_from_np_array_copy,time,0.0
102,param-level,"terminus-2,oracle",1.016057738104207,1.016057738104207,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,units.TimeQuantityOpSmallArrayDiffUnit.time_quantity_np_add,units,"{""units.TimeQuantityOpSmallArrayDiffUnit.time_quantity_np_add"": ""class TimeQuantityOpSmallArrayDiffUnit:\n    def time_quantity_np_add(self):\n        np.add(self.data, self.data2)\n\n    def setup(self):\n        data = np.array([1.0, 2.0, 3.0])\n        self.data = data * u.g\n    \n        # A different but dimensionally compatible unit\n        self.data2 = 0.001 * data * u.kg""}",units.TimeQuantityOpSmallArrayDiffUnit.time_quantity_np_add,time,0.0
50,param-level,"terminus-2,oracle",1.093442182763926,1.093442182763926,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,coordinates.SkyCoordBenchmarks.time_iter_array,coordinates,"{""coordinates.SkyCoordBenchmarks.time_iter_array"": ""class SkyCoordBenchmarks:\n    def time_iter_array(self):\n        for c in self.coord_array_1e3:\n            pass\n\n    def setup(self):\n        self.coord_scalar = SkyCoord(1, 2, unit=\""deg\"", frame=\""icrs\"")\n    \n        lon, lat = np.ones((2, 1000))\n        self.coord_array_1e3 = SkyCoord(lon, lat, unit=\""deg\"", frame=\""icrs\"")\n    \n        self.lon_1e6, self.lat_1e6 = np.ones((2, int(1e6)))\n        self.coord_array_1e6 = SkyCoord(\n            self.lon_1e6, self.lat_1e6, unit=\""deg\"", frame=\""icrs\""\n        )\n    \n        self.scalar_q_ra = 1 * u.deg\n        self.scalar_q_dec = 2 * u.deg\n    \n        np.random.seed(12345)\n        self.array_q_ra = np.random.rand(int(1e6)) * 360 * u.deg\n        self.array_q_dec = (np.random.rand(int(1e6)) * 180 - 90) * u.deg\n    \n        self.scalar_repr = UnitSphericalRepresentation(\n            lat=self.scalar_q_dec, lon=self.scalar_q_ra\n        )\n        self.array_repr = UnitSphericalRepresentation(\n            lat=self.array_q_dec, lon=self.array_q_ra\n        )""}",coordinates.SkyCoordBenchmarks.time_iter_array,time,0.0
41,param-level,"terminus-2,gpt-5",1.013860207073542,1.052227833379354,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,modeling.fitting.time_init_LevMarLSQFitter,modeling.fitting,"{""modeling.fitting.time_init_LevMarLSQFitter"": ""def time_init_LevMarLSQFitter():\n    fitting.LevMarLSQFitter()""}",modeling.fitting.time_init_LevMarLSQFitter,time,-0.027134106298311117
4,param-level,"terminus-2,claude",0.9698083188264875,0.9375326437838574,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_utils_iers,imports,"{""imports.timeraw_import_astropy_utils_iers"": ""def timeraw_import_astropy_utils_iers():\n    return \""\""\""\n    from astropy.utils import iers\n    \""\""\""""}",imports.timeraw_import_astropy_utils_iers,time,0.022825795645424456
35,param-level,"terminus-2,gpt-5",1.048329577037415,1.092478111708183,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.CommentedHeaderInt.time_write,io_ascii.main,"{""io_ascii.main.CommentedHeaderInt.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.CommentedHeaderInt.time_write,time,-0.03122244319007635
108,param-level,"terminus-2,oracle",0.949979563682008,0.949979563682008,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,wcs.WCSTransformations.time_pix2world_x_y_1,wcs,"{""wcs.WCSTransformations.time_pix2world_x_y_1"": ""class WCSTransformations:\n    def time_pix2world_x_y_1(self, size):\n        wcs.wcs_pix2world(self.px, self.py, 1)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)""}",wcs.WCSTransformations.time_pix2world_x_y_1,time,0.0
16,param-level,"terminus-2,claude",1.0135023024404557,1.019241621879223,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.TimeTableInitWithMultiDimLists.time_init_int_masked_3d,table,"{""table.TimeTableInitWithMultiDimLists.time_init_int_masked_3d"": ""class TimeTableInitWithMultiDimLists:\n    def time_init_int_masked_3d(self):\n        Table([self.data_int_masked_3d])\n\n    def setup(self):\n        np_data_int = np.arange(1_000_000, dtype=np.int64)\n        np_data_float = np_data_int.astype(np.float64)\n        np_data_str = np_data_int.astype(\""U\"")\n    \n        self.data_int_1d = np_data_int.tolist()\n    \n        self.data_int_3d = np_data_int.reshape(1000, 100, 10).tolist()\n    \n        self.data_int_masked_1d = self.data_int_1d.copy()\n        self.data_int_masked_1d[-1] = np.ma.masked\n    \n        self.data_int_masked_3d = self.data_int_3d.copy()\n        self.data_int_masked_3d[-1][-1][-1] = np.ma.masked\n    \n        self.data_float_1d = np_data_float.tolist()\n    \n        self.data_str_1d = np_data_str.tolist()\n    \n        self.data_str_masked_1d = self.data_str_1d.copy()\n        self.data_str_masked_1d[-1] = np.ma.masked""}",table.TimeTableInitWithMultiDimLists.time_init_int_masked_3d,time,-0.0040589246384493466
10,param-level,"terminus-2,claude",1.0764251935311682,1.0677605090251543,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.SextractorString.time_read,io_ascii.main,"{""io_ascii.main.SextractorString.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.SextractorString.time_read,time,0.006127782536077706
105,param-level,"terminus-2,oracle",1.0136094389155583,1.0136094389155583,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,votable.TimeVOTableSmallOverhead.time_small_binary2,votable,"{""votable.TimeVOTableSmallOverhead.time_small_binary2"": ""class TimeVOTableSmallOverhead:\n    def time_small_binary2(self):\n        parse(io.BytesIO(self.binary2_data))\n\n    def setup(self):\n        table = Table(\n            [\n                ra_data[:SMALL_SIZE],\n                dec_data[:SMALL_SIZE],\n                mag_data[:SMALL_SIZE]\n            ],\n            names=['ra', 'dec', 'mag']\n        )\n    \n        self.binary_data = create_votable_bytes(table, 'binary')\n        self.binary2_data = create_votable_bytes(table, 'binary2')""}",votable.TimeVOTableSmallOverhead.time_small_binary2,time,0.0
24,param-level,"terminus-2,gpt-5",1.1218778451080516,1.0791845115486305,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_io,imports,"{""imports.timeraw_import_astropy_io"": ""def timeraw_import_astropy_io():\n    return \""\""\""\n    from astropy import io\n    \""\""\""""}",imports.timeraw_import_astropy_io,time,0.030193305204682507
107,param-level,"terminus-2,oracle",0.9528888363018873,0.9528888363018873,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,wcs.WCSTransformations.time_pix2world_x_y_0,wcs,"{""wcs.WCSTransformations.time_pix2world_x_y_0"": ""class WCSTransformations:\n    def time_pix2world_x_y_0(self, size):\n        wcs.wcs_pix2world(self.px, self.py, 0)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)""}",wcs.WCSTransformations.time_pix2world_x_y_0,time,0.0
106,param-level,"terminus-2,oracle",1.0312724196031475,1.0312724196031475,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,wcs.WCSTransformations.time_ape14_world_to_pixel,wcs,"{""wcs.WCSTransformations.time_ape14_world_to_pixel"": ""class WCSTransformations:\n    def time_ape14_world_to_pixel(self, size):\n        wcs.world_to_pixel(self.coord)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)""}",wcs.WCSTransformations.time_ape14_world_to_pixel,time,0.0
48,param-level,"terminus-2,oracle",0.997006747390139,0.997006747390139,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,convolve.ConvolveFFT.time_convolve_fft,convolve,"{""convolve.ConvolveFFT.time_convolve_fft"": ""class ConvolveFFT:\n    def time_convolve_fft(self, ndim, size, boundary, nan_treatment):\n        convolve_fft(\n            self.array, self.kernel, boundary=boundary, nan_treatment=nan_treatment\n        )\n\n    def setup(self, ndim, size, boundary, nan_treatment):\n        np.random.seed(12345)\n    \n        self.kernel = np.random.random(kernel_shapes[ndim][size])\n        self.array = np.random.random(array_shapes[ndim][size])""}",convolve.ConvolveFFT.time_convolve_fft,time,0.0
37,param-level,"terminus-2,gpt-5",1.0179588718978554,1.0248349391596268,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.RdbInt.time_read,io_ascii.main,"{""io_ascii.main.RdbInt.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.RdbInt.time_read,time,-0.004862848134208888
61,param-level,"terminus-2,oracle",0.9587349685274658,0.9587349685274658,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_units_quantity,imports,"{""imports.timeraw_import_astropy_units_quantity"": ""def timeraw_import_astropy_units_quantity():\n    return \""\""\""\n    from astropy.units import quantity\n    \""\""\""""}",imports.timeraw_import_astropy_units_quantity,time,0.0
0,param-level,"terminus-2,claude",0.9717202118900872,0.9589327323279948,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,convolve.ConvolveFFT.time_convolve_fft,convolve,"{""convolve.ConvolveFFT.time_convolve_fft"": ""class ConvolveFFT:\n    def time_convolve_fft(self, ndim, size, boundary, nan_treatment):\n        convolve_fft(\n            self.array, self.kernel, boundary=boundary, nan_treatment=nan_treatment\n        )\n\n    def setup(self, ndim, size, boundary, nan_treatment):\n        np.random.seed(12345)\n    \n        self.kernel = np.random.random(kernel_shapes[ndim][size])\n        self.array = np.random.random(array_shapes[ndim][size])""}",convolve.ConvolveFFT.time_convolve_fft,time,0.009043479181111997
11,param-level,"terminus-2,claude",1.0251839802771694,1.0075690507073685,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,modeling.fitting.time_init_LinearLSQFitter,modeling.fitting,"{""modeling.fitting.time_init_LinearLSQFitter"": ""def time_init_LinearLSQFitter():\n    fitting.LinearLSQFitter()""}",modeling.fitting.time_init_LinearLSQFitter,time,0.012457517376096839
28,param-level,"terminus-2,gpt-5",1.058565116636493,0.9502411460411848,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_time,imports,"{""imports.timeraw_import_astropy_time"": ""def timeraw_import_astropy_time():\n    return \""\""\""\n    from astropy import time\n    \""\""\""""}",imports.timeraw_import_astropy_time,time,0.07660818288211337
64,param-level,"terminus-2,oracle",0.9471224197161896,0.9471224197161896,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_visualization,imports,"{""imports.timeraw_import_astropy_visualization"": ""def timeraw_import_astropy_visualization():\n    return \""\""\""\n    from astropy import visualization\n    \""\""\""""}",imports.timeraw_import_astropy_visualization,time,0.0
1,param-level,"terminus-2,claude",0.9667647284879493,0.9690919685025468,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_table,imports,"{""imports.timeraw_import_astropy_table"": ""def timeraw_import_astropy_table():\n    return \""\""\""\n    from astropy import table\n    \""\""\""""}",imports.timeraw_import_astropy_table,time,-0.0016458557387535348
49,param-level,"terminus-2,oracle",0.9436728904153254,0.9436728904153254,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,coordinates.FrameBenchmarks.time_concatenate_array,coordinates,"{""coordinates.FrameBenchmarks.time_concatenate_array"": ""class FrameBenchmarks:\n    def time_concatenate_array(self):\n        concatenate((self.icrs_array, self.icrs_array))\n\n    def setup(self):\n        self.scalar_ra = 3.2 * u.deg\n        self.scalar_dec = 2.2 * u.deg\n    \n        self.scalar_pmra = 3.2 * u.mas / u.yr\n        self.scalar_pmdec = 2.2 * u.mas / u.yr\n    \n        self.array_ra = np.linspace(0.0, 360.0, 1000) * u.deg\n        self.array_dec = np.linspace(-90.0, 90.0, 1000) * u.deg\n    \n        np.random.seed(12345)\n        self.icrs_scalar = ICRS(ra=1 * u.deg, dec=2 * u.deg)\n        self.icrs_array = ICRS(\n            ra=np.random.random(10000) * u.deg, dec=np.random.random(10000) * u.deg\n        )\n    \n        # Some points to use for benchmarking coordinate matching.\n        # These were motivated by some tests done in astropy/astropy#7324:\n        # https://github.com/astropy/astropy/pull/7324#issuecomment-392382719\n        xyz_uniform1 = rnd.uniform(size=(3, 10000)) * u.kpc\n        xyz_uniform2 = rnd.uniform(size=(3, 10000)) * u.kpc\n        self.icrs_uniform1 = ICRS(\n            xyz_uniform1, representation_type=CartesianRepresentation\n        )\n        self.icrs_uniform2 = ICRS(\n            xyz_uniform2, representation_type=CartesianRepresentation\n        )\n    \n        phi = rnd.uniform(0, 2 * np.pi, size=10000)\n        theta = np.arccos(2 * rnd.uniform(size=10000) - 1)\n        xyz_uniform_sph1 = (\n            np.vstack(\n                (\n                    np.cos(phi) * np.sin(theta),\n                    np.sin(phi) * np.sin(theta),\n                    np.cos(theta),\n                )\n            )\n            * u.kpc\n        )\n    \n        phi = rnd.uniform(0, 2 * np.pi, size=10000)\n        theta = np.arccos(2 * rnd.uniform(size=10000) - 1)\n        xyz_uniform_sph2 = (\n            np.vstack(\n                (\n                    np.cos(phi) * np.sin(theta),\n                    np.sin(phi) * np.sin(theta),\n                    np.cos(theta),\n                )\n            )\n            * u.kpc\n        )\n        self.icrs_uniform_sph1 = ICRS(\n            xyz_uniform_sph1, representation_type=CartesianRepresentation\n        )\n        self.icrs_uniform_sph2 = ICRS(\n            xyz_uniform_sph2, representation_type=CartesianRepresentation\n        )\n    \n        xyz0 = rnd.uniform(-100, 100, size=(8, 3))\n        xyz_clustered1 = np.vstack(rnd.normal(xyz0, size=(10000, 8, 3))).T * u.kpc\n        xyz_clustered2 = np.vstack(rnd.normal(xyz0, size=(10000, 8, 3))).T * u.kpc\n        self.icrs_clustered1 = ICRS(\n            xyz_clustered1, representation_type=CartesianRepresentation\n        )\n        self.icrs_clustered2 = ICRS(\n            xyz_clustered2, representation_type=CartesianRepresentation\n        )""}",coordinates.FrameBenchmarks.time_concatenate_array,time,0.0
29,param-level,"terminus-2,gpt-5",1.0735850081135054,0.969432561350119,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_timeseries,imports,"{""imports.timeraw_import_astropy_timeseries"": ""def timeraw_import_astropy_timeseries():\n    return \""\""\""\n    from astropy import timeseries\n    \""\""\""""}",imports.timeraw_import_astropy_timeseries,time,0.07365802458513891
20,param-level,"terminus-2,claude",0.9564521149576416,1.0312724196031475,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,wcs.WCSTransformations.time_ape14_world_to_pixel,wcs,"{""wcs.WCSTransformations.time_ape14_world_to_pixel"": ""class WCSTransformations:\n    def time_ape14_world_to_pixel(self, size):\n        wcs.world_to_pixel(self.coord)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)""}",wcs.WCSTransformations.time_ape14_world_to_pixel,time,-0.05291393539286134
77,param-level,"terminus-2,oracle",1.0645890030356506,1.0645890030356506,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.NoHeaderInt.time_write,io_ascii.main,"{""io_ascii.main.NoHeaderInt.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.NoHeaderInt.time_write,time,0.0
65,param-level,"terminus-2,oracle",0.9257499597310214,0.9257499597310214,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_visualization_wcsaxes,imports,"{""imports.timeraw_import_astropy_visualization_wcsaxes"": ""def timeraw_import_astropy_visualization_wcsaxes():\n    return \""\""\""\n    from astropy.visualization import wcsaxes\n    \""\""\""""}",imports.timeraw_import_astropy_visualization_wcsaxes,time,0.0
110,param-level,"terminus-2,oracle",0.916563197620162,0.916563197620162,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,wcs.WCSTransformations.time_world2pix_x_y_1,wcs,"{""wcs.WCSTransformations.time_world2pix_x_y_1"": ""class WCSTransformations:\n    def time_world2pix_x_y_1(self, size):\n        wcs.wcs_world2pix(self.wx, self.wy, 1)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)""}",wcs.WCSTransformations.time_world2pix_x_y_1,time,0.0
7,param-level,"terminus-2,claude",1.020818645377212,1.0278074337985588,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.core.CoreSuite.time_continuation_inputter,io_ascii.core,"{""io_ascii.core.CoreSuite.time_continuation_inputter"": ""class CoreSuite:\n    def time_continuation_inputter(self):\n        core.ContinuationLinesInputter().process_lines(self.lines)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]""}",io_ascii.core.CoreSuite.time_continuation_inputter,time,-0.0049425660688450284
47,param-level,"terminus-2,oracle",1.0071536445507023,1.0071536445507023,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,convolve.Convolve.time_convolve,convolve,"{""convolve.Convolve.time_convolve"": ""class Convolve:\n    def time_convolve(self, ndim, size, boundary, nan_treatment):\n        convolve(\n            self.array, self.kernel, boundary=boundary, nan_treatment=nan_treatment\n        )\n\n    def setup(self, ndim, size, boundary, nan_treatment):\n        np.random.seed(12345)\n    \n        self.kernel = np.random.random(kernel_shapes[ndim][size])\n        self.array = np.random.random(array_shapes[ndim][size])""}",convolve.Convolve.time_convolve,time,0.0
73,param-level,"terminus-2,oracle",1.030803669803022,1.030803669803022,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.CommentedHeaderFloat.time_read,io_ascii.main,"{""io_ascii.main.CommentedHeaderFloat.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.CommentedHeaderFloat.time_read,time,0.0
98,param-level,"terminus-2,oracle",1.0684349181619717,1.0684349181619717,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.TimeTableInitWithMultiDimLists.time_init_int_1d,table,"{""table.TimeTableInitWithMultiDimLists.time_init_int_1d"": ""class TimeTableInitWithMultiDimLists:\n    def time_init_int_1d(self):\n        Table([self.data_int_1d])\n\n    def setup(self):\n        np_data_int = np.arange(1_000_000, dtype=np.int64)\n        np_data_float = np_data_int.astype(np.float64)\n        np_data_str = np_data_int.astype(\""U\"")\n    \n        self.data_int_1d = np_data_int.tolist()\n    \n        self.data_int_3d = np_data_int.reshape(1000, 100, 10).tolist()\n    \n        self.data_int_masked_1d = self.data_int_1d.copy()\n        self.data_int_masked_1d[-1] = np.ma.masked\n    \n        self.data_int_masked_3d = self.data_int_3d.copy()\n        self.data_int_masked_3d[-1][-1][-1] = np.ma.masked\n    \n        self.data_float_1d = np_data_float.tolist()\n    \n        self.data_str_1d = np_data_str.tolist()\n    \n        self.data_str_masked_1d = self.data_str_1d.copy()\n        self.data_str_masked_1d[-1] = np.ma.masked""}",table.TimeTableInitWithMultiDimLists.time_init_int_1d,time,0.0
52,param-level,"terminus-2,oracle",1.0391439947064631,1.0391439947064631,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_io_fits,imports,"{""imports.timeraw_import_astropy_io_fits"": ""def timeraw_import_astropy_io_fits():\n    return \""\""\""\n    from astropy.io import fits\n    \""\""\""""}",imports.timeraw_import_astropy_io_fits,time,0.0
109,param-level,"terminus-2,oracle",0.925068101922638,0.925068101922638,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,wcs.WCSTransformations.time_world2pix_x_y_0,wcs,"{""wcs.WCSTransformations.time_world2pix_x_y_0"": ""class WCSTransformations:\n    def time_world2pix_x_y_0(self, size):\n        wcs.wcs_world2pix(self.wx, self.wy, 0)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)""}",wcs.WCSTransformations.time_world2pix_x_y_0,time,0.0
101,param-level,"terminus-2,oracle",1.0436571240682462,1.0436571240682462,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,units.TimeQuantityOpSmallArrayDiffUnit.time_quantity_add,units,"{""units.TimeQuantityOpSmallArrayDiffUnit.time_quantity_add"": ""class TimeQuantityOpSmallArrayDiffUnit:\n    def time_quantity_add(self):\n        # Same as operator.add\n        self.data + self.data2\n\n    def setup(self):\n        data = np.array([1.0, 2.0, 3.0])\n        self.data = data * u.g\n    \n        # A different but dimensionally compatible unit\n        self.data2 = 0.001 * data * u.kg""}",units.TimeQuantityOpSmallArrayDiffUnit.time_quantity_add,time,0.0
85,param-level,"terminus-2,oracle",0.968112742119425,0.968112742119425,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,modeling.fitting.time_Chebyshev1D_LevMarLSQFitter,modeling.fitting,"{""modeling.fitting.time_Chebyshev1D_LevMarLSQFitter"": ""def time_Chebyshev1D_LevMarLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        y = y_base + np.random.normal(0.0, 0.2, y_base.shape)\n        fit_LevMarLSQFitter(Chebyshev1D, x, y)\n    except Warning:\n        pass""}",modeling.fitting.time_Chebyshev1D_LevMarLSQFitter,time,0.0
92,param-level,"terminus-2,oracle",0.9596615099407187,0.9596615099407187,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.TimeTable.time_column_slice_bool,table,"{""table.TimeTable.time_column_slice_bool"": ""class TimeTable:\n    def time_column_slice_bool(self):\n        self.table[\""a\""][self.bool_mask]\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6""}",table.TimeTable.time_column_slice_bool,time,0.0
78,param-level,"terminus-2,oracle",1.0248349391596268,1.0248349391596268,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.RdbInt.time_read,io_ascii.main,"{""io_ascii.main.RdbInt.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.RdbInt.time_read,time,0.0
63,param-level,"terminus-2,oracle",0.9375326437838574,0.9375326437838574,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_utils_iers,imports,"{""imports.timeraw_import_astropy_utils_iers"": ""def timeraw_import_astropy_utils_iers():\n    return \""\""\""\n    from astropy.utils import iers\n    \""\""\""""}",imports.timeraw_import_astropy_utils_iers,time,0.0
22,param-level,"terminus-2,claude",0.9809142254016988,0.949979563682008,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,wcs.WCSTransformations.time_pix2world_x_y_1,wcs,"{""wcs.WCSTransformations.time_pix2world_x_y_1"": ""class WCSTransformations:\n    def time_pix2world_x_y_1(self, size):\n        wcs.wcs_pix2world(self.px, self.py, 1)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)""}",wcs.WCSTransformations.time_pix2world_x_y_1,time,0.0218774128144914
67,param-level,"terminus-2,oracle",0.9440264105865166,0.9440264105865166,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_wcs_wcsapi,imports,"{""imports.timeraw_import_astropy_wcs_wcsapi"": ""def timeraw_import_astropy_wcs_wcsapi():\n    return \""\""\""\n    from astropy.wcs import wcsapi\n    \""\""\""""}",imports.timeraw_import_astropy_wcs_wcsapi,time,0.0
5,param-level,"terminus-2,claude",0.9832837638809778,0.9471224197161896,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_visualization,imports,"{""imports.timeraw_import_astropy_visualization"": ""def timeraw_import_astropy_visualization():\n    return \""\""\""\n    from astropy import visualization\n    \""\""\""""}",imports.timeraw_import_astropy_visualization,time,0.025573793610175546
14,param-level,"terminus-2,claude",1.0812914844911257,1.070279171332443,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.TimeTableInitWithLists.time_init_lists,table,"{""table.TimeTableInitWithLists.time_init_lists"": ""class TimeTableInitWithLists:\n    def time_init_lists(self):\n        Table([self.dat, self.dat, self.dat], names=[\""time\"", \""rate\"", \""error\""])\n\n    def setup(self):\n        self.dat = list(range(100_000))""}",table.TimeTableInitWithLists.time_init_lists,time,0.007788057396522322
66,param-level,"terminus-2,oracle",0.93716742554461,0.93716742554461,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_wcs,imports,"{""imports.timeraw_import_astropy_wcs"": ""def timeraw_import_astropy_wcs():\n    return \""\""\""\n    from astropy import wcs\n    \""\""\""""}",imports.timeraw_import_astropy_wcs,time,0.0
25,param-level,"terminus-2,gpt-5",1.085173471563104,1.0391439947064631,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_io_fits,imports,"{""imports.timeraw_import_astropy_io_fits"": ""def timeraw_import_astropy_io_fits():\n    return \""\""\""\n    from astropy.io import fits\n    \""\""\""""}",imports.timeraw_import_astropy_io_fits,time,0.03255267104430053
103,param-level,"terminus-2,oracle",1.043050322455329,1.043050322455329,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,units.time_quantity_view,units,"{""units.time_quantity_view"": ""def time_quantity_view():\n    q1.view(u.Quantity)""}",units.time_quantity_view,time,0.0
95,param-level,"terminus-2,oracle",1.006789553503843,1.006789553503843,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.TimeTable.time_item_get_rowfirst,table,"{""table.TimeTable.time_item_get_rowfirst"": ""class TimeTable:\n    def time_item_get_rowfirst(self):\n        self.table[300][\""b\""]\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6""}",table.TimeTable.time_item_get_rowfirst,time,0.0
54,param-level,"terminus-2,oracle",1.0528219741954823,1.0528219741954823,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_io_misc_hdf5,imports,"{""imports.timeraw_import_astropy_io_misc_hdf5"": ""def timeraw_import_astropy_io_misc_hdf5():\n    return \""\""\""\n    from astropy.io.misc import hdf5\n    \""\""\""""}",imports.timeraw_import_astropy_io_misc_hdf5,time,0.0
57,param-level,"terminus-2,oracle",0.969432561350119,0.969432561350119,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_timeseries,imports,"{""imports.timeraw_import_astropy_timeseries"": ""def timeraw_import_astropy_timeseries():\n    return \""\""\""\n    from astropy import timeseries\n    \""\""\""""}",imports.timeraw_import_astropy_timeseries,time,0.0
15,param-level,"terminus-2,claude",1.0644133111058784,1.0684349181619717,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.TimeTableInitWithMultiDimLists.time_init_int_1d,table,"{""table.TimeTableInitWithMultiDimLists.time_init_int_1d"": ""class TimeTableInitWithMultiDimLists:\n    def time_init_int_1d(self):\n        Table([self.data_int_1d])\n\n    def setup(self):\n        np_data_int = np.arange(1_000_000, dtype=np.int64)\n        np_data_float = np_data_int.astype(np.float64)\n        np_data_str = np_data_int.astype(\""U\"")\n    \n        self.data_int_1d = np_data_int.tolist()\n    \n        self.data_int_3d = np_data_int.reshape(1000, 100, 10).tolist()\n    \n        self.data_int_masked_1d = self.data_int_1d.copy()\n        self.data_int_masked_1d[-1] = np.ma.masked\n    \n        self.data_int_masked_3d = self.data_int_3d.copy()\n        self.data_int_masked_3d[-1][-1][-1] = np.ma.masked\n    \n        self.data_float_1d = np_data_float.tolist()\n    \n        self.data_str_1d = np_data_str.tolist()\n    \n        self.data_str_masked_1d = self.data_str_1d.copy()\n        self.data_str_masked_1d[-1] = np.ma.masked""}",table.TimeTableInitWithMultiDimLists.time_init_int_1d,time,-0.0028441351174633834
76,param-level,"terminus-2,oracle",1.0141282493958388,1.0141282493958388,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.NoHeaderInt.time_read,io_ascii.main,"{""io_ascii.main.NoHeaderInt.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.NoHeaderInt.time_read,time,0.0
104,param-level,"terminus-2,oracle",1.015357168017734,1.015357168017734,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,units.time_unit_to,units,"{""units.time_unit_to"": ""def time_unit_to():\n    u.m.to(u.pc)""}",units.time_unit_to,time,0.0
31,param-level,"terminus-2,gpt-5",1.0165220062328009,1.0128514245735518,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.core.CoreSuite.time_convert_vals,io_ascii.core,"{""io_ascii.core.CoreSuite.time_convert_vals"": ""class CoreSuite:\n    def time_convert_vals(self):\n        core.TableOutputter()._convert_vals(self.cols)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]""}",io_ascii.core.CoreSuite.time_convert_vals,time,0.0025958851904165844
68,param-level,"terminus-2,oracle",1.0278074337985588,1.0278074337985588,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.core.CoreSuite.time_continuation_inputter,io_ascii.core,"{""io_ascii.core.CoreSuite.time_continuation_inputter"": ""class CoreSuite:\n    def time_continuation_inputter(self):\n        core.ContinuationLinesInputter().process_lines(self.lines)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]""}",io_ascii.core.CoreSuite.time_continuation_inputter,time,0.0
74,param-level,"terminus-2,oracle",1.0229372004400374,1.0229372004400374,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.CommentedHeaderFloat.time_write,io_ascii.main,"{""io_ascii.main.CommentedHeaderFloat.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.CommentedHeaderFloat.time_write,time,0.0
91,param-level,"terminus-2,oracle",0.9806920326250612,0.9806920326250612,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.TimeTable.time_add_column,table,"{""table.TimeTable.time_add_column"": ""class TimeTable:\n    def time_add_column(self):\n        self.table[\""e\""] = self.extra_column\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6""}",table.TimeTable.time_add_column,time,0.0
59,param-level,"terminus-2,oracle",0.9665951387805676,0.9665951387805676,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_timeseries_periodograms,imports,"{""imports.timeraw_import_astropy_timeseries_periodograms"": ""def timeraw_import_astropy_timeseries_periodograms():\n    return \""\""\""\n    from astropy.timeseries import periodograms\n    \""\""\""""}",imports.timeraw_import_astropy_timeseries_periodograms,time,0.0
13,param-level,"terminus-2,claude",1.0242751521344056,1.0210058620492353,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.TimeTable.time_read_rows,table,"{""table.TimeTable.time_read_rows"": ""class TimeTable:\n    def time_read_rows(self):\n        for row in self.table:\n            tuple(row)\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6""}",table.TimeTable.time_read_rows,time,0.002312086340290167
21,param-level,"terminus-2,claude",0.9573371633381628,0.9528888363018873,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,wcs.WCSTransformations.time_pix2world_x_y_0,wcs,"{""wcs.WCSTransformations.time_pix2world_x_y_0"": ""class WCSTransformations:\n    def time_pix2world_x_y_0(self, size):\n        wcs.wcs_pix2world(self.px, self.py, 0)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)""}",wcs.WCSTransformations.time_pix2world_x_y_0,time,0.0031459172816658506
33,param-level,"terminus-2,gpt-5",1.0170891957626351,1.015350347678538,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.core.CoreSuite.time_whitespace_splitter,io_ascii.core,"{""io_ascii.core.CoreSuite.time_whitespace_splitter"": ""class CoreSuite:\n    def time_whitespace_splitter(self):\n        core.WhitespaceSplitter().process_line(self.line)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]""}",io_ascii.core.CoreSuite.time_whitespace_splitter,time,0.0012297369760234716
83,param-level,"terminus-2,oracle",1.059748629030831,1.059748629030831,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_fits.FITSHeader.time_get_hierarch,io_fits,"{""io_fits.FITSHeader.time_get_hierarch"": ""class FITSHeader:\n    def time_get_hierarch(self):\n        self.hdr.get(\""HIERARCH FOO BAR 999\"")\n\n    def setup(self):\n        self.hdr = make_header()\n        self.hdr_string = self.hdr.tostring()""}",io_fits.FITSHeader.time_get_hierarch,time,0.0
69,param-level,"terminus-2,oracle",1.0128514245735518,1.0128514245735518,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.core.CoreSuite.time_convert_vals,io_ascii.core,"{""io_ascii.core.CoreSuite.time_convert_vals"": ""class CoreSuite:\n    def time_convert_vals(self):\n        core.TableOutputter()._convert_vals(self.cols)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]""}",io_ascii.core.CoreSuite.time_convert_vals,time,0.0
3,param-level,"terminus-2,claude",0.959910040211352,0.9433432111662834,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_utils,imports,"{""imports.timeraw_import_astropy_utils"": ""def timeraw_import_astropy_utils():\n    return \""\""\""\n    from astropy import utils\n    \""\""\""""}",imports.timeraw_import_astropy_utils,time,0.011716286453372395
18,param-level,"terminus-2,claude",1.0475555107561574,1.0436571240682462,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,units.TimeQuantityOpSmallArrayDiffUnit.time_quantity_add,units,"{""units.TimeQuantityOpSmallArrayDiffUnit.time_quantity_add"": ""class TimeQuantityOpSmallArrayDiffUnit:\n    def time_quantity_add(self):\n        # Same as operator.add\n        self.data + self.data2\n\n    def setup(self):\n        data = np.array([1.0, 2.0, 3.0])\n        self.data = data * u.g\n    \n        # A different but dimensionally compatible unit\n        self.data2 = 0.001 * data * u.kg""}",units.TimeQuantityOpSmallArrayDiffUnit.time_quantity_add,time,0.0027569919999372024
97,param-level,"terminus-2,oracle",1.070279171332443,1.070279171332443,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.TimeTableInitWithLists.time_init_lists,table,"{""table.TimeTableInitWithLists.time_init_lists"": ""class TimeTableInitWithLists:\n    def time_init_lists(self):\n        Table([self.dat, self.dat, self.dat], names=[\""time\"", \""rate\"", \""error\""])\n\n    def setup(self):\n        self.dat = list(range(100_000))""}",table.TimeTableInitWithLists.time_init_lists,time,0.0
60,param-level,"terminus-2,oracle",0.9447062140645422,0.9447062140645422,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_units,imports,"{""imports.timeraw_import_astropy_units"": ""def timeraw_import_astropy_units():\n    return \""\""\""\n    from astropy import units\n    \""\""\""""}",imports.timeraw_import_astropy_units,time,0.0
86,param-level,"terminus-2,oracle",1.052227833379354,1.052227833379354,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,modeling.fitting.time_init_LevMarLSQFitter,modeling.fitting,"{""modeling.fitting.time_init_LevMarLSQFitter"": ""def time_init_LevMarLSQFitter():\n    fitting.LevMarLSQFitter()""}",modeling.fitting.time_init_LevMarLSQFitter,time,0.0
58,param-level,"terminus-2,oracle",0.9874496199734252,0.9874496199734252,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_timeseries_io,imports,"{""imports.timeraw_import_astropy_timeseries_io"": ""def timeraw_import_astropy_timeseries_io():\n    return \""\""\""\n    from astropy.timeseries import io\n    \""\""\""""}",imports.timeraw_import_astropy_timeseries_io,time,0.0
80,param-level,"terminus-2,oracle",1.055795161606203,1.055795161606203,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.TabInt.time_write,io_ascii.main,"{""io_ascii.main.TabInt.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.TabInt.time_write,time,0.0
100,param-level,"terminus-2,oracle",1.0080852034220826,1.0080852034220826,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,units.TimeQuantityOpLargeArray.time_quantity_np_square,units,"{""units.TimeQuantityOpLargeArray.time_quantity_np_square"": ""class TimeQuantityOpSmallArray:\n    def time_quantity_np_square(self):\n        np.power(self.data, 2)\n\nclass TimeQuantityOpLargeArray:\n    def setup(self):\n        data = np.arange(1e6) + 1\n        self.data = data * u.g\n        self.out_sq = data * u.g**2\n        self.out_sqrt = data * u.g**0.5""}",units.TimeQuantityOpLargeArray.time_quantity_np_square,time,0.0
96,param-level,"terminus-2,oracle",1.0210058620492353,1.0210058620492353,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.TimeTable.time_read_rows,table,"{""table.TimeTable.time_read_rows"": ""class TimeTable:\n    def time_read_rows(self):\n        for row in self.table:\n            tuple(row)\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6""}",table.TimeTable.time_read_rows,time,0.0
23,param-level,"terminus-2,gpt-5",0.9929863139631648,0.9591490792274736,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,convolve.ConvolveFFT.time_convolve_fft,convolve,"{""convolve.ConvolveFFT.time_convolve_fft"": ""class ConvolveFFT:\n    def time_convolve_fft(self, ndim, size, boundary, nan_treatment):\n        convolve_fft(\n            self.array, self.kernel, boundary=boundary, nan_treatment=nan_treatment\n        )\n\n    def setup(self, ndim, size, boundary, nan_treatment):\n        np.random.seed(12345)\n    \n        self.kernel = np.random.random(kernel_shapes[ndim][size])\n        self.array = np.random.random(array_shapes[ndim][size])""}",convolve.ConvolveFFT.time_convolve_fft,time,0.023930151863996624
9,param-level,"terminus-2,claude",0.9397125921072624,0.931172319629449,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.AastexFloat.time_read,io_ascii.main,"{""io_ascii.main.AastexFloat.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.AastexFloat.time_read,time,0.006039796660405527
93,param-level,"terminus-2,oracle",0.955421083240806,0.955421083240806,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.TimeTable.time_hstack,table,"{""table.TimeTable.time_hstack"": ""class TimeTable:\n    def time_hstack(self):\n        hstack([self.table, self.other_table_2])\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6""}",table.TimeTable.time_hstack,time,0.0
40,param-level,"terminus-2,gpt-5",0.9814727981246726,0.968112742119425,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,modeling.fitting.time_Chebyshev1D_LevMarLSQFitter,modeling.fitting,"{""modeling.fitting.time_Chebyshev1D_LevMarLSQFitter"": ""def time_Chebyshev1D_LevMarLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        y = y_base + np.random.normal(0.0, 0.2, y_base.shape)\n        fit_LevMarLSQFitter(Chebyshev1D, x, y)\n    except Warning:\n        pass""}",modeling.fitting.time_Chebyshev1D_LevMarLSQFitter,time,0.009448413016441011
99,param-level,"terminus-2,oracle",1.019241621879223,1.019241621879223,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.TimeTableInitWithMultiDimLists.time_init_int_masked_3d,table,"{""table.TimeTableInitWithMultiDimLists.time_init_int_masked_3d"": ""class TimeTableInitWithMultiDimLists:\n    def time_init_int_masked_3d(self):\n        Table([self.data_int_masked_3d])\n\n    def setup(self):\n        np_data_int = np.arange(1_000_000, dtype=np.int64)\n        np_data_float = np_data_int.astype(np.float64)\n        np_data_str = np_data_int.astype(\""U\"")\n    \n        self.data_int_1d = np_data_int.tolist()\n    \n        self.data_int_3d = np_data_int.reshape(1000, 100, 10).tolist()\n    \n        self.data_int_masked_1d = self.data_int_1d.copy()\n        self.data_int_masked_1d[-1] = np.ma.masked\n    \n        self.data_int_masked_3d = self.data_int_3d.copy()\n        self.data_int_masked_3d[-1][-1][-1] = np.ma.masked\n    \n        self.data_float_1d = np_data_float.tolist()\n    \n        self.data_str_1d = np_data_str.tolist()\n    \n        self.data_str_masked_1d = self.data_str_1d.copy()\n        self.data_str_masked_1d[-1] = np.ma.masked""}",table.TimeTableInitWithMultiDimLists.time_init_int_masked_3d,time,0.0
27,param-level,"terminus-2,gpt-5",1.0810134963298987,1.0528219741954823,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_io_misc_hdf5,imports,"{""imports.timeraw_import_astropy_io_misc_hdf5"": ""def timeraw_import_astropy_io_misc_hdf5():\n    return \""\""\""\n    from astropy.io.misc import hdf5\n    \""\""\""""}",imports.timeraw_import_astropy_io_misc_hdf5,time,0.019937427252062552
81,param-level,"terminus-2,oracle",1.0145693146798205,1.0145693146798205,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.table.TableSuite.time_str_vals_str,io_ascii.table,"{""io_ascii.table.TableSuite.time_str_vals_str"": ""class TableSuite:\n    def time_str_vals_str(self):\n        self.table_cols[2].iter_str_vals()\n\n    def setup(self):\n        self.lst = []\n        self.lst.append([random.randint(-500, 500) for i in range(1000)])\n        self.lst.append([random.random() * 500 - 500 for i in range(1000)])\n        self.lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, self.lst):\n            col.data = x\n        self.table_cols = [table.Column(x) for x in self.lst]\n        self.outputter = core.TableOutputter()\n        self.table = table.Table()""}",io_ascii.table.TableSuite.time_str_vals_str,time,0.0
82,param-level,"terminus-2,oracle",1.0207959134133997,1.0207959134133997,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_fits.FITSHeader.time_get_float,io_fits,"{""io_fits.FITSHeader.time_get_float"": ""class FITSHeader:\n    def time_get_float(self):\n        self.hdr.get(\""FLT999\"")\n\n    def setup(self):\n        self.hdr = make_header()\n        self.hdr_string = self.hdr.tostring()""}",io_fits.FITSHeader.time_get_float,time,0.0
71,param-level,"terminus-2,oracle",1.015350347678538,1.015350347678538,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.core.CoreSuite.time_whitespace_splitter,io_ascii.core,"{""io_ascii.core.CoreSuite.time_whitespace_splitter"": ""class CoreSuite:\n    def time_whitespace_splitter(self):\n        core.WhitespaceSplitter().process_line(self.line)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]""}",io_ascii.core.CoreSuite.time_whitespace_splitter,time,0.0
72,param-level,"terminus-2,oracle",0.931172319629449,0.931172319629449,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.AastexFloat.time_read,io_ascii.main,"{""io_ascii.main.AastexFloat.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.AastexFloat.time_read,time,0.0
17,param-level,"terminus-2,claude",25.02060600614115,1.0080852034220826,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,units.TimeQuantityOpLargeArray.time_quantity_np_square,units,"{""units.TimeQuantityOpLargeArray.time_quantity_np_square"": ""class TimeQuantityOpSmallArray:\n    def time_quantity_np_square(self):\n        np.power(self.data, 2)\n\nclass TimeQuantityOpLargeArray:\n    def setup(self):\n        data = np.arange(1e6) + 1\n        self.data = data * u.g\n        self.out_sq = data * u.g**2\n        self.out_sqrt = data * u.g**0.5""}",units.TimeQuantityOpLargeArray.time_quantity_np_square,time,16.981980765713626
34,param-level,"terminus-2,gpt-5",1.0205798553151009,1.030803669803022,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.CommentedHeaderFloat.time_read,io_ascii.main,"{""io_ascii.main.CommentedHeaderFloat.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.CommentedHeaderFloat.time_read,time,-0.007230420429930128
70,param-level,"terminus-2,oracle",1.0537416530971275,1.0537416530971275,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.core.CoreSuite.time_default_splitter_call,io_ascii.core,"{""io_ascii.core.CoreSuite.time_default_splitter_call"": ""class CoreSuite:\n    def time_default_splitter_call(self):\n        core.DefaultSplitter()(self.csv_line)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]""}",io_ascii.core.CoreSuite.time_default_splitter_call,time,0.0
55,param-level,"terminus-2,oracle",0.9690919685025468,0.9690919685025468,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_table,imports,"{""imports.timeraw_import_astropy_table"": ""def timeraw_import_astropy_table():\n    return \""\""\""\n    from astropy import table\n    \""\""\""""}",imports.timeraw_import_astropy_table,time,0.0
36,param-level,"terminus-2,gpt-5",1.0387164237717323,1.0141282493958388,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.NoHeaderInt.time_read,io_ascii.main,"{""io_ascii.main.NoHeaderInt.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.NoHeaderInt.time_read,time,0.017389090789175075
89,param-level,"terminus-2,oracle",1.0187313338888078,1.0187313338888078,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,stats.sigma_clipping.SigmaClipBenchmarks.time_3d_array_axis2,stats.sigma_clipping,"{""stats.sigma_clipping.SigmaClipBenchmarks.time_3d_array_axis2"": ""class SigmaClipBenchmarks:\n    def time_3d_array_axis2(self):\n        self.sigclip(self.data, axis=(0, 1))\n\n    def setup(self):\n        # Avoid top-level module import to make sure that the benchmarks are\n        # compatible with versions of astropy that did not have this functionality.\n        from astropy.stats import SigmaClip\n    \n        size = (4, 2048, 2048)\n    \n        with NumpyRNGContext(12345):\n            self.data = np.random.normal(size=size)\n    \n            # add outliers\n            nbad = 100000\n            zbad = np.random.randint(low=0, high=size[0] - 1, size=nbad)\n            ybad = np.random.randint(low=0, high=size[1] - 1, size=nbad)\n            xbad = np.random.randint(low=0, high=size[2] - 1, size=nbad)\n            self.data[zbad, ybad, xbad] = np.random.choice([-1, 1], size=nbad) * (\n                10 + np.random.rand(nbad)\n            )\n    \n            # The defaults use median as the cenfunc and standard\n            # deviation as the stdfunc.  The default iters is 5.\n            self.sigclip = SigmaClip(sigma=3)""}",stats.sigma_clipping.SigmaClipBenchmarks.time_3d_array_axis2,time,0.0
19,param-level,"terminus-2,claude",1.0520263716203884,1.043050322455329,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,units.time_quantity_view,units,"{""units.time_quantity_view"": ""def time_quantity_view():\n    q1.view(u.Quantity)""}",units.time_quantity_view,time,0.00634798385081998
2,param-level,"terminus-2,claude",0.969560605876141,0.9447062140645422,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_units,imports,"{""imports.timeraw_import_astropy_units"": ""def timeraw_import_astropy_units():\n    return \""\""\""\n    from astropy import units\n    \""\""\""""}",imports.timeraw_import_astropy_units,time,0.017577363374539436
87,param-level,"terminus-2,oracle",1.0075690507073685,1.0075690507073685,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,modeling.fitting.time_init_LinearLSQFitter,modeling.fitting,"{""modeling.fitting.time_init_LinearLSQFitter"": ""def time_init_LinearLSQFitter():\n    fitting.LinearLSQFitter()""}",modeling.fitting.time_init_LinearLSQFitter,time,0.0
8,param-level,"terminus-2,claude",1.0264646683950471,1.0537416530971275,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.core.CoreSuite.time_default_splitter_call,io_ascii.core,"{""io_ascii.core.CoreSuite.time_default_splitter_call"": ""class CoreSuite:\n    def time_default_splitter_call(self):\n        core.DefaultSplitter()(self.csv_line)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]""}",io_ascii.core.CoreSuite.time_default_splitter_call,time,-0.019290653961867327
38,param-level,"terminus-2,gpt-5",1.084152241692071,1.0677605090251543,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.SextractorString.time_read,io_ascii.main,"{""io_ascii.main.SextractorString.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.SextractorString.time_read,time,0.011592455917197127
44,param-level,"terminus-2,gpt-5",1.0591447239874952,1.0684349181619717,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,table.TimeTableInitWithMultiDimLists.time_init_int_1d,table,"{""table.TimeTableInitWithMultiDimLists.time_init_int_1d"": ""class TimeTableInitWithMultiDimLists:\n    def time_init_int_1d(self):\n        Table([self.data_int_1d])\n\n    def setup(self):\n        np_data_int = np.arange(1_000_000, dtype=np.int64)\n        np_data_float = np_data_int.astype(np.float64)\n        np_data_str = np_data_int.astype(\""U\"")\n    \n        self.data_int_1d = np_data_int.tolist()\n    \n        self.data_int_3d = np_data_int.reshape(1000, 100, 10).tolist()\n    \n        self.data_int_masked_1d = self.data_int_1d.copy()\n        self.data_int_masked_1d[-1] = np.ma.masked\n    \n        self.data_int_masked_3d = self.data_int_3d.copy()\n        self.data_int_masked_3d[-1][-1][-1] = np.ma.masked\n    \n        self.data_float_1d = np_data_float.tolist()\n    \n        self.data_str_1d = np_data_str.tolist()\n    \n        self.data_str_masked_1d = self.data_str_1d.copy()\n        self.data_str_masked_1d[-1] = np.ma.masked""}",table.TimeTableInitWithMultiDimLists.time_init_int_1d,time,-0.0065701514670979625
88,param-level,"terminus-2,oracle",1.0166635229568928,1.0166635229568928,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,modeling.model.time_init_gaussian_with_units,modeling.model,"{""modeling.model.time_init_gaussian_with_units"": ""def time_init_gaussian_with_units():\n    models.Gaussian1D(amplitude=10 * u.Hz, mean=5 * u.m, stddev=1.2 * u.cm)""}",modeling.model.time_init_gaussian_with_units,time,0.0
90,param-level,"terminus-2,oracle",0.9314596873228048,0.9314596873228048,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.TimeMaskedTable.time_join_inner,table,"{""table.TimeMaskedTable.time_join_inner"": ""class TimeTable:\n    def time_join_inner(self):\n        join(self.table, self.other_table, keys=\""i\"", join_type=\""inner\"")\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6""}",table.TimeMaskedTable.time_join_inner,time,0.0
3692,param-level,"terminus-2,claude",1.133311625383533,1.1089147165785465,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algorithms.Quantile.time_quantile,algorithms,"{""algorithms.Quantile.time_quantile"": ""class Quantile:\n    def time_quantile(self, quantile, interpolation, dtype):\n        self.idx.quantile(quantile, interpolation=interpolation)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, quantile, interpolation, dtype):\n        N = 10**5\n        data = {\n            \""int\"": np.arange(N),\n            \""uint\"": np.arange(N).astype(np.uint64),\n            \""float\"": np.random.randn(N),\n        }\n        self.idx = pd.Series(data[dtype].repeat(5))""}",algorithms.Quantile.time_quantile,time,0.01725382518032989
3694,param-level,"terminus-2,claude",1.0925336030979893,1.1108688007180527,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.IsIn.time_isin_mismatched_dtype,algos.isin,"{""algos.isin.IsIn.time_isin_mismatched_dtype"": ""class IsIn:\n    def time_isin_mismatched_dtype(self, dtype):\n        self.series.isin(self.mismatched)\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)""}",algos.isin.IsIn.time_isin_mismatched_dtype,time,-0.012966900721402698
3691,param-level,"terminus-2,claude",1.1648842382224,1.121587361662498,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algorithms.Factorize.time_factorize,algorithms,"{""algorithms.Factorize.time_factorize"": ""class Factorize:\n    def time_factorize(self, unique, sort, dtype):\n        pd.factorize(self.data, sort=sort)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Factorize:\n    def setup(self, unique, sort, dtype):\n        N = 10**5\n        string_index = tm.makeStringIndex(N)\n        string_arrow = None\n        if dtype == \""string[pyarrow]\"":\n            try:\n                string_arrow = pd.array(string_index, dtype=\""string[pyarrow]\"")\n            except ImportError:\n                raise NotImplementedError\n    \n        data = {\n            \""int\"": pd.Index(np.arange(N), dtype=\""int64\""),\n            \""uint\"": pd.Index(np.arange(N), dtype=\""uint64\""),\n            \""float\"": pd.Index(np.random.randn(N), dtype=\""float64\""),\n            \""object\"": string_index,\n            \""datetime64[ns]\"": pd.date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n            \""datetime64[ns, tz]\"": pd.date_range(\n                \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n            ),\n            \""Int64\"": pd.array(np.arange(N), dtype=\""Int64\""),\n            \""boolean\"": pd.array(np.random.randint(0, 2, N), dtype=\""boolean\""),\n            \""string[pyarrow]\"": string_arrow,\n        }[dtype]\n        if not unique:\n            data = data.repeat(5)\n        self.data = data""}",algorithms.Factorize.time_factorize,time,0.03062013900983168
3695,param-level,"terminus-2,claude",1.1934580578541278,1.173784775878192,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.IsInLongSeriesLookUpDominates.time_isin,algos.isin,"{""algos.isin.IsInLongSeriesLookUpDominates.time_isin"": ""class IsInLongSeriesLookUpDominates:\n    def time_isin(self, dtypes, MaxNumber, series_type):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, MaxNumber, series_type):\n        N = 10**7\n    \n        if series_type == \""random_hits\"":\n            array = np.random.randint(0, MaxNumber, N)\n        if series_type == \""random_misses\"":\n            array = np.random.randint(0, MaxNumber, N) + MaxNumber\n        if series_type == \""monotone_hits\"":\n            array = np.repeat(np.arange(MaxNumber), N // MaxNumber)\n        if series_type == \""monotone_misses\"":\n            array = np.arange(N) + MaxNumber\n    \n        self.series = Series(array).astype(dtype)\n    \n        self.values = np.arange(MaxNumber).astype(dtype.lower())""}",algos.isin.IsInLongSeriesLookUpDominates.time_isin,time,0.013913212147054996
3699,param-level,"terminus-2,claude",0.9824268958157192,0.9813637113697236,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,array.ArrowExtensionArray.time_to_numpy,array,"{""array.ArrowExtensionArray.time_to_numpy"": ""class ArrowExtensionArray:\n    def time_to_numpy(self, dtype, hasna):\n        self.arr.to_numpy()\n\n    def setup(self, dtype, hasna):\n        N = 100_000\n        if dtype == \""boolean[pyarrow]\"":\n            data = np.random.choice([True, False], N, replace=True)\n        elif dtype == \""float64[pyarrow]\"":\n            data = np.random.randn(N)\n        elif dtype == \""int64[pyarrow]\"":\n            data = np.arange(N)\n        elif dtype == \""string[pyarrow]\"":\n            data = tm.rands_array(10, N)\n        elif dtype == \""timestamp[ns][pyarrow]\"":\n            data = pd.date_range(\""2000-01-01\"", freq=\""s\"", periods=N)\n        else:\n            raise NotImplementedError\n    \n        arr = pd.array(data, dtype=dtype)\n        if hasna:\n            arr[::2] = pd.NA\n        self.arr = arr""}",array.ArrowExtensionArray.time_to_numpy,time,0.0007518984766588572
3693,param-level,"terminus-2,claude",1.0528985669144937,1.0396141997096964,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.IsIn.time_isin_empty,algos.isin,"{""algos.isin.IsIn.time_isin_empty"": ""class IsIn:\n    def time_isin_empty(self, dtype):\n        self.series.isin([])\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)""}",algos.isin.IsIn.time_isin_empty,time,0.009394884869022143
3696,param-level,"terminus-2,claude",1.0990278911993103,1.0296213811038797,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.IsinAlmostFullWithRandomInt.time_isin,algos.isin,"{""algos.isin.IsinAlmostFullWithRandomInt.time_isin"": ""class IsinAlmostFullWithRandomInt:\n    def time_isin(self, dtype, exponent, title):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, exponent, title):\n        M = 3 * 2 ** (exponent - 2)\n        # 0.77-the maximal share of occupied buckets\n        self.series = Series(np.random.randint(0, M, M)).astype(dtype)\n    \n        values = np.random.randint(0, M, M).astype(dtype)\n        if title == \""inside\"":\n            self.values = values\n        elif title == \""outside\"":\n            self.values = values + M\n        else:\n            raise ValueError(title)""}",algos.isin.IsinAlmostFullWithRandomInt.time_isin,time,0.04908522637583497
3698,param-level,"terminus-2,claude",0.9964518521955192,0.9848606535373996,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.Ops2.time_frame_float_mod,arithmetic,"{""arithmetic.Ops2.time_frame_float_mod"": ""class Ops2:\n    def time_frame_float_mod(self):\n        self.df % self.df2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Ops2:\n    def setup(self):\n        N = 10**3\n        self.df = DataFrame(np.random.randn(N, N))\n        self.df2 = DataFrame(np.random.randn(N, N))\n    \n        self.df_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n        self.df2_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n    \n        self.s = Series(np.random.randn(N))""}",arithmetic.Ops2.time_frame_float_mod,time,0.008197453082121386
3707,param-level,"terminus-2,claude",1.084285370782898,1.0679607860056457,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.SelectDtypes.time_select_dtype_int_exclude,dtypes,"{""dtypes.SelectDtypes.time_select_dtype_int_exclude"": ""class SelectDtypes:\n    def time_select_dtype_int_exclude(self, dtype):\n        self.df_int.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )""}",dtypes.SelectDtypes.time_select_dtype_int_exclude,time,0.011544968017858844
3702,param-level,"terminus-2,claude",1.0186548518363805,0.9767446348584446,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,ctors.DatetimeIndexConstructor.time_from_list_of_dates,ctors,"{""ctors.DatetimeIndexConstructor.time_from_list_of_dates"": ""class DatetimeIndexConstructor:\n    def time_from_list_of_dates(self):\n        DatetimeIndex(self.list_of_dates)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndexConstructor:\n    def setup(self):\n        N = 20_000\n        dti = date_range(\""1900-01-01\"", periods=N)\n    \n        self.list_of_timestamps = dti.tolist()\n        self.list_of_dates = dti.date.tolist()\n        self.list_of_datetimes = dti.to_pydatetime().tolist()\n        self.list_of_str = dti.strftime(\""%Y-%m-%d\"").tolist()""}",ctors.DatetimeIndexConstructor.time_from_list_of_dates,time,0.029639474524707103
3697,param-level,"terminus-2,claude",1.0161320571669217,0.8638397455809634,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.OffsetArrayArithmetic.time_add_series_offset,arithmetic,"{""arithmetic.OffsetArrayArithmetic.time_add_series_offset"": ""class OffsetArrayArithmetic:\n    def time_add_series_offset(self, offset):\n        with warnings.catch_warnings(record=True):\n            self.ser + offset\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass OffsetArrayArithmetic:\n    def setup(self, offset):\n        N = 10000\n        rng = date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"")\n        self.rng = rng\n        self.ser = Series(rng)""}",arithmetic.OffsetArrayArithmetic.time_add_series_offset,time,0.10770319065485025
3706,param-level,"terminus-2,claude",1.0425175197787877,0.9747697591590926,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.SelectDtypes.time_select_dtype_float_include,dtypes,"{""dtypes.SelectDtypes.time_select_dtype_float_include"": ""class SelectDtypes:\n    def time_select_dtype_float_include(self, dtype):\n        self.df_float.select_dtypes(include=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )""}",dtypes.SelectDtypes.time_select_dtype_float_include,time,0.04791213622326388
3703,param-level,"terminus-2,claude",1.0548761297850124,0.9279782369189892,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,ctors.SeriesConstructors.time_series_constructor,ctors,"{""ctors.SeriesConstructors.time_series_constructor"": ""class SeriesConstructors:\n    def time_series_constructor(self, data_fmt, with_index, dtype):\n        Series(self.data, index=self.index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesConstructors:\n    def setup(self, data_fmt, with_index, dtype):\n        if data_fmt in (gen_of_str, gen_of_tuples) and with_index:\n            raise NotImplementedError(\n                \""Series constructors do not support using generators with indexes\""\n            )\n        N = 10**4\n        if dtype == \""float\"":\n            arr = np.random.randn(N)\n        else:\n            arr = np.arange(N)\n        self.data = data_fmt(arr)\n        self.index = np.arange(N) if with_index else None""}",ctors.SeriesConstructors.time_series_constructor,time,0.08974391291797969
3705,param-level,"terminus-2,claude",1.0285055100576934,0.9843871123238844,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.DtypesInvalid.time_pandas_dtype_invalid,dtypes,"{""dtypes.DtypesInvalid.time_pandas_dtype_invalid"": ""class DtypesInvalid:\n    def time_pandas_dtype_invalid(self, dtype):\n        try:\n            pandas_dtype(self.data_dict[dtype])\n        except TypeError:\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)""}",dtypes.DtypesInvalid.time_pandas_dtype_invalid,time,0.031201129939044548
3700,param-level,"terminus-2,claude",0.9898216294453116,0.9878651791606782,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,attrs_caching.SeriesArrayAttribute.time_extract_array,attrs_caching,"{""attrs_caching.SeriesArrayAttribute.time_extract_array"": ""class SeriesArrayAttribute:\n    def time_extract_array(self, dtype):\n        extract_array(self.series)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesArrayAttribute:\n    def setup(self, dtype):\n        if dtype == \""numeric\"":\n            self.series = pd.Series([1, 2, 3])\n        elif dtype == \""object\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=object)\n        elif dtype == \""category\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=\""category\"")\n        elif dtype == \""datetime64\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3))\n        elif dtype == \""datetime64tz\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3, tz=\""UTC\""))""}",attrs_caching.SeriesArrayAttribute.time_extract_array,time,0.0013836282069543328
3701,param-level,"terminus-2,claude",0.973824571908645,0.9825157394271008,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.CategoricalSlicing.time_getitem_scalar,categoricals,"{""categoricals.CategoricalSlicing.time_getitem_scalar"": ""class CategoricalSlicing:\n    def time_getitem_scalar(self, index):\n        self.data[self.scalar]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalSlicing:\n    def setup(self, index):\n        N = 10**6\n        categories = [\""a\"", \""b\"", \""c\""]\n        values = [0] * N + [1] * N + [2] * N\n        if index == \""monotonic_incr\"":\n            self.data = pd.Categorical.from_codes(values, categories=categories)\n        elif index == \""monotonic_decr\"":\n            self.data = pd.Categorical.from_codes(\n                list(reversed(values)), categories=categories\n            )\n        elif index == \""non_monotonic\"":\n            self.data = pd.Categorical.from_codes([0, 1, 2] * N, categories=categories)\n        else:\n            raise ValueError(f\""Invalid index param: {index}\"")\n    \n        self.scalar = 10000\n        self.list = list(range(10000))\n        self.cat_scalar = \""b\""""}",categoricals.CategoricalSlicing.time_getitem_scalar,time,-0.006146511682076222
3704,param-level,"terminus-2,claude",1.0270767187934915,1.0630451419651972,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.Dtypes.time_pandas_dtype,dtypes,"{""dtypes.Dtypes.time_pandas_dtype"": ""class Dtypes:\n    def time_pandas_dtype(self, dtype):\n        pandas_dtype(dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)""}",dtypes.Dtypes.time_pandas_dtype,time,-0.02543735726428973
3712,param-level,"terminus-2,claude",1.3328781077964085,1.3893436394248913,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.Eval.time_mult,eval,"{""eval.Eval.time_mult"": ""class Eval:\n    def time_mult(self, engine, threads):\n        pd.eval(\""self.df * self.df2 * self.df3 * self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)""}",eval.Eval.time_mult,time,-0.03993319068492417
3713,param-level,"terminus-2,claude",1.2037987398290126,1.1317455443362747,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.Query.time_query_datetime_column,eval,"{""eval.Query.time_query_datetime_column"": ""class Query:\n    def time_query_datetime_column(self):\n        self.df.query(\""dates < @self.ts\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Query:\n    def setup(self):\n        N = 10**6\n        halfway = (N // 2) - 1\n        index = pd.date_range(\""20010101\"", periods=N, freq=\""T\"")\n        s = pd.Series(index)\n        self.ts = s.iloc[halfway]\n        self.df = pd.DataFrame({\""a\"": np.random.randn(N), \""dates\"": index}, index=index)\n        data = np.random.randn(N)\n        self.min_val = data.min()\n        self.max_val = data.max()""}",eval.Query.time_query_datetime_column,time,0.05095699822683022
3709,param-level,"terminus-2,claude",1.3953496003618713,1.2709339453302828,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.Eval.time_add,eval,"{""eval.Eval.time_add"": ""class Eval:\n    def time_add(self, engine, threads):\n        pd.eval(\""self.df + self.df2 + self.df3 + self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)""}",eval.Eval.time_add,time,0.08798844061639921
3708,param-level,"terminus-2,claude",1.1128212073620574,1.3767683410933085,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.SelectDtypes.time_select_dtype_string_exclude,dtypes,"{""dtypes.SelectDtypes.time_select_dtype_string_exclude"": ""class SelectDtypes:\n    def time_select_dtype_string_exclude(self, dtype):\n        self.df_string.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )""}",dtypes.SelectDtypes.time_select_dtype_string_exclude,time,-0.18666699698108283
3711,param-level,"terminus-2,claude",1.1503155650897716,1.159924219526538,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.Eval.time_chained_cmp,eval,"{""eval.Eval.time_chained_cmp"": ""class Eval:\n    def time_chained_cmp(self, engine, threads):\n        pd.eval(\""self.df < self.df2 < self.df3 < self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)""}",eval.Eval.time_chained_cmp,time,-0.00679537088880226
3718,param-level,"terminus-2,claude",1.086827676208385,1.0588739842694828,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromDicts.time_nested_dict_int64,frame_ctor,"{""frame_ctor.FromDicts.time_nested_dict_int64"": ""class FromDicts:\n    def time_nested_dict_int64(self):\n        # nested dict, integer indexes, regression described in #621\n        DataFrame(self.data2)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDicts:\n    def setup(self):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n        frame = DataFrame(np.random.randn(N, K), index=self.index, columns=self.columns)\n        self.data = frame.to_dict()\n        self.dict_list = frame.to_dict(orient=\""records\"")\n        self.data2 = {i: {j: float(j) for j in range(100)} for i in range(2000)}\n    \n        # arrays which we won't consolidate\n        self.dict_of_categoricals = {i: Categorical(np.arange(N)) for i in range(K)}""}",frame_ctor.FromDicts.time_nested_dict_int64,time,0.019769230508417463
3725,param-level,"terminus-2,claude",1.102871712480657,1.0482445796343702,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Describe.time_series_describe,frame_methods,"{""frame_methods.Describe.time_series_describe"": ""class Describe:\n    def time_series_describe(self):\n        self.df[\""a\""].describe()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Describe:\n    def setup(self):\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(0, 100, 10**6),\n                \""b\"": np.random.randint(0, 100, 10**6),\n                \""c\"": np.random.randint(0, 100, 10**6),\n            }\n        )""}",frame_methods.Describe.time_series_describe,time,0.038633050103456044
3722,param-level,"terminus-2,claude",1.0516602928577283,1.0553971358160663,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Apply.time_apply_lambda_mean,frame_methods,"{""frame_methods.Apply.time_apply_lambda_mean"": ""class Apply:\n    def time_apply_lambda_mean(self):\n        self.df.apply(lambda x: x.mean())\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self):\n        self.df = DataFrame(np.random.randn(1000, 100))\n    \n        self.s = Series(np.arange(1028.0))\n        self.df2 = DataFrame({i: self.s for i in range(1028)})\n        self.df3 = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))""}",frame_methods.Apply.time_apply_lambda_mean,time,-0.002642746080861374
3720,param-level,"terminus-2,claude",1.0673483349349584,1.048155438963408,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromRecords.time_frame_from_records_generator,frame_ctor,"{""frame_ctor.FromRecords.time_frame_from_records_generator"": ""class FromRecords:\n    def time_frame_from_records_generator(self, nrows):\n        # issue-6700\n        self.df = DataFrame.from_records(self.gen, nrows=nrows)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromRecords:\n    def setup(self, nrows):\n        N = 100000\n        self.gen = ((x, (x * 20), (x * 100)) for x in range(N))""}",frame_ctor.FromRecords.time_frame_from_records_generator,time,0.013573476641831955
3714,param-level,"terminus-2,claude",1.2329444195455332,1.1546050789147206,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.Query.time_query_with_boolean_selection,eval,"{""eval.Query.time_query_with_boolean_selection"": ""class Query:\n    def time_query_with_boolean_selection(self):\n        self.df.query(\""(a >= @self.min_val) & (a <= @self.max_val)\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Query:\n    def setup(self):\n        N = 10**6\n        halfway = (N // 2) - 1\n        index = pd.date_range(\""20010101\"", periods=N, freq=\""T\"")\n        s = pd.Series(index)\n        self.ts = s.iloc[halfway]\n        self.df = pd.DataFrame({\""a\"": np.random.randn(N), \""dates\"": index}, index=index)\n        data = np.random.randn(N)\n        self.min_val = data.min()\n        self.max_val = data.max()""}",eval.Query.time_query_with_boolean_selection,time,0.055402645424902836
3717,param-level,"terminus-2,claude",1.2917013945587692,1.0644373832418372,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromDicts.time_list_of_dict,frame_ctor,"{""frame_ctor.FromDicts.time_list_of_dict"": ""class FromDicts:\n    def time_list_of_dict(self):\n        DataFrame(self.dict_list)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDicts:\n    def setup(self):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n        frame = DataFrame(np.random.randn(N, K), index=self.index, columns=self.columns)\n        self.data = frame.to_dict()\n        self.dict_list = frame.to_dict(orient=\""records\"")\n        self.data2 = {i: {j: float(j) for j in range(100)} for i in range(2000)}\n    \n        # arrays which we won't consolidate\n        self.dict_of_categoricals = {i: Categorical(np.arange(N)) for i in range(K)}""}",frame_ctor.FromDicts.time_list_of_dict,time,0.1607241947078727
3719,param-level,"terminus-2,claude",1.0370344496148716,1.0154414722650773,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromLists.time_frame_from_lists,frame_ctor,"{""frame_ctor.FromLists.time_frame_from_lists"": ""class FromLists:\n    def time_frame_from_lists(self):\n        self.df = DataFrame(self.data)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromLists:\n    def setup(self):\n        N = 1000\n        M = 100\n        self.data = [list(range(M)) for i in range(N)]""}",frame_ctor.FromLists.time_frame_from_lists,time,0.015270846782032717
3721,param-level,"terminus-2,claude",1.0370228351786577,1.028257179873277,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Apply.time_apply_axis_1,frame_methods,"{""frame_methods.Apply.time_apply_axis_1"": ""class Apply:\n    def time_apply_axis_1(self):\n        self.df.apply(lambda x: x + 1, axis=1)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self):\n        self.df = DataFrame(np.random.randn(1000, 100))\n    \n        self.s = Series(np.arange(1028.0))\n        self.df2 = DataFrame({i: self.s for i in range(1028)})\n        self.df3 = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))""}",frame_methods.Apply.time_apply_axis_1,time,0.006199190456422037
3726,param-level,"terminus-2,claude",1.2321341011815543,1.1480368354203323,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Dropna.time_dropna,frame_methods,"{""frame_methods.Dropna.time_dropna"": ""class Dropna:\n    def time_dropna(self, how, axis):\n        self.df.dropna(how=how, axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Dropna:\n    def setup(self, how, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""""}",frame_methods.Dropna.time_dropna,time,0.05947472826111882
3710,param-level,"terminus-2,claude",1.0934879570027516,1.1345405893436904,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.Eval.time_and,eval,"{""eval.Eval.time_and"": ""class Eval:\n    def time_and(self, engine, threads):\n        pd.eval(\n            \""(self.df > 0) & (self.df2 > 0) & (self.df3 > 0) & (self.df4 > 0)\"",\n            engine=engine,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)""}",eval.Eval.time_and,time,-0.029032979024709204
3716,param-level,"terminus-2,claude",1.0801749345590252,1.0681690669423896,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromDicts.time_dict_of_categoricals,frame_ctor,"{""frame_ctor.FromDicts.time_dict_of_categoricals"": ""class FromDicts:\n    def time_dict_of_categoricals(self):\n        # dict of arrays that we won't consolidate\n        DataFrame(self.dict_of_categoricals)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDicts:\n    def setup(self):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n        frame = DataFrame(np.random.randn(N, K), index=self.index, columns=self.columns)\n        self.data = frame.to_dict()\n        self.dict_list = frame.to_dict(orient=\""records\"")\n        self.data2 = {i: {j: float(j) for j in range(100)} for i in range(2000)}\n    \n        # arrays which we won't consolidate\n        self.dict_of_categoricals = {i: Categorical(np.arange(N)) for i in range(K)}""}",frame_ctor.FromDicts.time_dict_of_categoricals,time,0.008490712600166685
3727,param-level,"terminus-2,claude",1.2352836128246911,1.1474070058342605,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Dropna.time_dropna_axis_mixed_dtypes,frame_methods,"{""frame_methods.Dropna.time_dropna_axis_mixed_dtypes"": ""class Dropna:\n    def time_dropna_axis_mixed_dtypes(self, how, axis):\n        self.df_mixed.dropna(how=how, axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Dropna:\n    def setup(self, how, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""""}",frame_methods.Dropna.time_dropna_axis_mixed_dtypes,time,0.06214752969620273
3715,param-level,"terminus-2,claude",1.190751097121914,1.109764724574163,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromArrays.time_frame_from_arrays_sparse,frame_ctor,"{""frame_ctor.FromArrays.time_frame_from_arrays_sparse"": ""class FromArrays:\n    def time_frame_from_arrays_sparse(self):\n        self.df = DataFrame._from_arrays(\n            self.sparse_arrays,\n            index=self.index,\n            columns=self.columns,\n            verify_integrity=False,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromArrays:\n    def setup(self):\n        N_rows = 1000\n        N_cols = 1000\n        self.float_arrays = [np.random.randn(N_rows) for _ in range(N_cols)]\n        self.sparse_arrays = [\n            pd.arrays.SparseArray(np.random.randint(0, 2, N_rows), dtype=\""float64\"")\n            for _ in range(N_cols)\n        ]\n        self.int_arrays = [\n            pd.array(np.random.randint(1000, size=N_rows), dtype=\""Int64\"")\n            for _ in range(N_cols)\n        ]\n        self.index = pd.Index(range(N_rows))\n        self.columns = pd.Index(range(N_cols))""}",frame_ctor.FromArrays.time_frame_from_arrays_sparse,time,0.05727466233928633
3733,param-level,"terminus-2,claude",1.158283804858569,1.1137320610096797,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.NSort.time_nsmallest_two_columns,frame_methods,"{""frame_methods.NSort.time_nsmallest_two_columns"": ""class NSort:\n    def time_nsmallest_two_columns(self, keep):\n        self.df.nsmallest(100, [\""A\"", \""B\""], keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NSort:\n    def setup(self, keep):\n        self.df = DataFrame(np.random.randn(100000, 3), columns=list(\""ABC\""))""}",frame_methods.NSort.time_nsmallest_two_columns,time,0.031507598195819934
3724,param-level,"terminus-2,claude",1.0948167539051892,1.0485527998893167,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Describe.time_dataframe_describe,frame_methods,"{""frame_methods.Describe.time_dataframe_describe"": ""class Describe:\n    def time_dataframe_describe(self):\n        self.df.describe()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Describe:\n    def setup(self):\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(0, 100, 10**6),\n                \""b\"": np.random.randint(0, 100, 10**6),\n                \""c\"": np.random.randint(0, 100, 10**6),\n            }\n        )""}",frame_methods.Describe.time_dataframe_describe,time,0.03271849647515733
3728,param-level,"terminus-2,claude",1.0458341879438715,1.0415164232546907,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Duplicated.time_frame_duplicated_wide,frame_methods,"{""frame_methods.Duplicated.time_frame_duplicated_wide"": ""class Duplicated:\n    def time_frame_duplicated_wide(self):\n        self.df2.duplicated()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Duplicated:\n    def setup(self):\n        n = 1 << 20\n        t = date_range(\""2015-01-01\"", freq=\""S\"", periods=(n // 64))\n        xs = np.random.randn(n // 64).round(2)\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(-1 << 8, 1 << 8, n),\n                \""b\"": np.random.choice(t, n),\n                \""c\"": np.random.choice(xs, n),\n            }\n        )\n        self.df2 = DataFrame(np.random.randn(1000, 100).astype(str)).T""}",frame_methods.Duplicated.time_frame_duplicated_wide,time,0.003053581816959526
3738,param-level,"terminus-2,claude",1.0790536747171422,1.070948001002442,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.Cumulative.time_frame_transform,groupby,"{""groupby.Cumulative.time_frame_transform"": ""class Cumulative:\n    def time_frame_transform(self, dtype, method, with_nans):\n        self.df.groupby(\""key\"").transform(method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cumulative:\n    def setup(self, dtype, method, with_nans):\n        if with_nans and dtype == \""int64\"":\n            raise NotImplementedError(\""Construction of df would raise\"")\n    \n        N = 500_000\n        keys = np.random.randint(0, 100, size=N)\n        vals = np.random.randint(-10, 10, (N, 5))\n    \n        if with_nans:\n            null_vals = vals.astype(float, copy=True)\n            null_vals[::2, :] = np.nan\n            null_vals[::3, :] = np.nan\n            df = DataFrame(null_vals, columns=list(\""abcde\""), dtype=dtype)\n            df[\""key\""] = keys\n            self.df = df\n        else:\n            df = DataFrame(vals, columns=list(\""abcde\"")).astype(dtype, copy=False)\n            df[\""key\""] = keys\n            self.df = df""}",groupby.Cumulative.time_frame_transform,time,0.005732442513932305
3735,param-level,"terminus-2,claude",1.0575302948020138,1.1079542764811172,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,gil.ParallelGroupbyMethods.time_parallel,gil,"{""gil.ParallelGroupbyMethods.time_parallel"": ""class ParallelGroupbyMethods:\n    def time_parallel(self, threads, method):\n        self.parallel()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ParallelGroupbyMethods:\n    def setup(self, threads, method):\n        N = 10**6\n        ngroups = 10**3\n        df = DataFrame(\n            {\""key\"": np.random.randint(0, ngroups, size=N), \""data\"": np.random.randn(N)}\n        )\n    \n        @test_parallel(num_threads=threads)\n        def parallel():\n            getattr(df.groupby(\""key\"")[\""data\""], method)()\n    \n        self.parallel = parallel\n    \n        def loop():\n            getattr(df.groupby(\""key\"")[\""data\""], method)()\n    \n        self.loop = loop""}",gil.ParallelGroupbyMethods.time_parallel,time,-0.03566052452553285
3730,param-level,"terminus-2,claude",1.0527241543064776,1.07645130760125,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Iteration.time_itertuples_raw_start,frame_methods,"{""frame_methods.Iteration.time_itertuples_raw_start"": ""class Iteration:\n    def time_itertuples_raw_start(self):\n        self.df4.itertuples(index=False, name=None)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))""}",frame_methods.Iteration.time_itertuples_raw_start,time,-0.016780164989230895
3732,param-level,"terminus-2,claude",1.153540011059381,1.116944134539179,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.NSort.time_nsmallest_one_column,frame_methods,"{""frame_methods.NSort.time_nsmallest_one_column"": ""class NSort:\n    def time_nsmallest_one_column(self, keep):\n        self.df.nsmallest(100, \""A\"", keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NSort:\n    def setup(self, keep):\n        self.df = DataFrame(np.random.randn(100000, 3), columns=list(\""ABC\""))""}",frame_methods.NSort.time_nsmallest_one_column,time,0.025881100792222
3723,param-level,"terminus-2,claude",1.2679011711536292,1.1014807750057138,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Count.time_count_level_mixed_dtypes_multi,frame_methods,"{""frame_methods.Count.time_count_level_mixed_dtypes_multi"": ""class Count:\n    def time_count_level_mixed_dtypes_multi(self, axis):\n        self.df_mixed.count(axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Count:\n    def setup(self, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""\n    \n        self.df.index = MultiIndex.from_arrays([self.df.index, self.df.index])\n        self.df.columns = MultiIndex.from_arrays([self.df.columns, self.df.columns])\n        self.df_mixed.index = MultiIndex.from_arrays(\n            [self.df_mixed.index, self.df_mixed.index]\n        )\n        self.df_mixed.columns = MultiIndex.from_arrays(\n            [self.df_mixed.columns, self.df_mixed.columns]\n        )""}",frame_methods.Count.time_count_level_mixed_dtypes_multi,time,0.11769476389527254
3729,param-level,"terminus-2,claude",1.2349416916784892,1.097394403466586,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Fillna.time_frame_fillna,frame_methods,"{""frame_methods.Fillna.time_frame_fillna"": ""class Fillna:\n    def time_frame_fillna(self, inplace, method, dtype):\n        self.df.fillna(inplace=inplace, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Fillna:\n    def setup(self, inplace, method, dtype):\n        N, M = 10000, 100\n        if dtype in (\""datetime64[ns]\"", \""datetime64[ns, tz]\"", \""timedelta64[ns]\""):\n            data = {\n                \""datetime64[ns]\"": date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n                \""datetime64[ns, tz]\"": date_range(\n                    \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n                ),\n                \""timedelta64[ns]\"": timedelta_range(start=\""1 day\"", periods=N, freq=\""1D\""),\n            }\n            self.df = DataFrame({f\""col_{i}\"": data[dtype] for i in range(M)})\n            self.df[::2] = None\n        else:\n            values = np.random.randn(N, M)\n            values[::2] = np.nan\n            if dtype == \""Int64\"":\n                values = values.round()\n            self.df = DataFrame(values, dtype=dtype)""}",frame_methods.Fillna.time_frame_fillna,time,0.09727530990940829
3736,param-level,"terminus-2,claude",1.0664601671525424,1.0230055870235066,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.Apply.time_scalar_function_multi_col,groupby,"{""groupby.Apply.time_scalar_function_multi_col"": ""class Apply:\n    def time_scalar_function_multi_col(self, factor):\n        self.df.groupby([\""key\"", \""key2\""]).apply(lambda x: 1)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, factor):\n        N = 10**factor\n        # two cases:\n        # - small groups: small data (N**4) + many labels (2000) -> average group\n        #   size of 5 (-> larger overhead of slicing method)\n        # - larger groups: larger data (N**5) + fewer labels (20) -> average group\n        #   size of 5000\n        labels = np.random.randint(0, 2000 if factor == 4 else 20, size=N)\n        labels2 = np.random.randint(0, 3, size=N)\n        df = DataFrame(\n            {\n                \""key\"": labels,\n                \""key2\"": labels2,\n                \""value1\"": np.random.randn(N),\n                \""value2\"": [\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (N // 4),\n            }\n        )\n        self.df = df""}",groupby.Apply.time_scalar_function_multi_col,time,0.030731669115301136
3737,param-level,"terminus-2,claude",1.1539903005891918,1.0188571417553025,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.ApplyNonUniqueUnsortedIndex.time_groupby_apply_non_unique_unsorted_index,groupby,"{""groupby.ApplyNonUniqueUnsortedIndex.time_groupby_apply_non_unique_unsorted_index"": ""class ApplyNonUniqueUnsortedIndex:\n    def time_groupby_apply_non_unique_unsorted_index(self):\n        self.df.groupby(\""key\"", group_keys=False).apply(lambda x: x)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ApplyNonUniqueUnsortedIndex:\n    def setup(self):\n        # GH 46527\n        # unsorted and non-unique index\n        idx = np.arange(100)[::-1]\n        idx = Index(np.repeat(idx, 200), name=\""key\"")\n        self.df = DataFrame(np.random.randn(len(idx), 10), index=idx)""}",groupby.ApplyNonUniqueUnsortedIndex.time_groupby_apply_non_unique_unsorted_index,time,0.09556800483301933
3742,param-level,"terminus-2,claude",1.0661276571731069,0.7453157545798069,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.RankWithTies.time_rank_ties,groupby,"{""groupby.RankWithTies.time_rank_ties"": ""class RankWithTies:\n    def time_rank_ties(self, dtype, tie_method):\n        self.df.groupby(\""key\"").rank(method=tie_method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass RankWithTies:\n    def setup(self, dtype, tie_method):\n        N = 10**4\n        if dtype == \""datetime64\"":\n            data = np.array([Timestamp(\""2011/01/01\"")] * N, dtype=dtype)\n        else:\n            data = np.array([1] * N, dtype=dtype)\n        self.df = DataFrame({\""values\"": data, \""key\"": [\""foo\""] * N})""}",groupby.RankWithTies.time_rank_ties,time,0.22688253365862798
3740,param-level,"terminus-2,claude",1.0829825703007137,1.071055066756287,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.GroupByMethods.time_dtype_as_group,groupby,"{""groupby.GroupByMethods.time_dtype_as_group"": ""class GroupByMethods:\n    def time_dtype_as_group(self, dtype, method, application, ncols):\n        self.as_group_method()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupByMethods:\n    def setup(self, dtype, method, application, ncols):\n        if method in method_blocklist.get(dtype, {}):\n            raise NotImplementedError  # skip benchmark\n    \n        if ncols != 1 and method in [\""value_counts\"", \""unique\""]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if application == \""transformation\"" and method in [\n            \""describe\"",\n            \""head\"",\n            \""tail\"",\n            \""unique\"",\n            \""value_counts\"",\n            \""size\"",\n        ]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if method == \""describe\"":\n            ngroups = 20\n        elif method == \""skew\"":\n            ngroups = 100\n        else:\n            ngroups = 1000\n        size = ngroups * 2\n        rng = np.arange(ngroups).reshape(-1, 1)\n        rng = np.broadcast_to(rng, (len(rng), ncols))\n        taker = np.random.randint(0, ngroups, size=size)\n        values = rng.take(taker, axis=0)\n        if dtype == \""int\"":\n            key = np.random.randint(0, size, size=size)\n        elif dtype in (\""int16\"", \""uint\""):\n            key = np.random.randint(0, size, size=size, dtype=dtype)\n        elif dtype == \""float\"":\n            key = np.concatenate(\n                [np.random.random(ngroups) * 0.1, np.random.random(ngroups) * 10.0]\n            )\n        elif dtype == \""object\"":\n            key = [\""foo\""] * size\n        elif dtype == \""datetime\"":\n            key = date_range(\""1/1/2011\"", periods=size, freq=\""s\"")\n    \n        cols = [f\""values{n}\"" for n in range(ncols)]\n        df = DataFrame(values, columns=cols)\n        df[\""key\""] = key\n    \n        if len(cols) == 1:\n            cols = cols[0]\n    \n        if application == \""transformation\"":\n            self.as_group_method = lambda: df.groupby(\""key\"")[cols].transform(method)\n            self.as_field_method = lambda: df.groupby(cols)[\""key\""].transform(method)\n        else:\n            self.as_group_method = getattr(df.groupby(\""key\"")[cols], method)\n            self.as_field_method = getattr(df.groupby(cols)[\""key\""], method)""}",groupby.GroupByMethods.time_dtype_as_group,time,0.00843529246423382
3731,param-level,"terminus-2,claude",1.095662227171521,1.1236743877828543,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.NSort.time_nlargest_two_columns,frame_methods,"{""frame_methods.NSort.time_nlargest_two_columns"": ""class NSort:\n    def time_nlargest_two_columns(self, keep):\n        self.df.nlargest(100, [\""A\"", \""B\""], keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NSort:\n    def setup(self, keep):\n        self.df = DataFrame(np.random.randn(100000, 3), columns=list(\""ABC\""))""}",frame_methods.NSort.time_nlargest_two_columns,time,-0.019810580347477562
3739,param-level,"terminus-2,claude",1.0692863976783202,1.074014587749526,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.GroupByMethods.time_dtype_as_field,groupby,"{""groupby.GroupByMethods.time_dtype_as_field"": ""class GroupByMethods:\n    def time_dtype_as_field(self, dtype, method, application, ncols):\n        self.as_field_method()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupByMethods:\n    def setup(self, dtype, method, application, ncols):\n        if method in method_blocklist.get(dtype, {}):\n            raise NotImplementedError  # skip benchmark\n    \n        if ncols != 1 and method in [\""value_counts\"", \""unique\""]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if application == \""transformation\"" and method in [\n            \""describe\"",\n            \""head\"",\n            \""tail\"",\n            \""unique\"",\n            \""value_counts\"",\n            \""size\"",\n        ]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if method == \""describe\"":\n            ngroups = 20\n        elif method == \""skew\"":\n            ngroups = 100\n        else:\n            ngroups = 1000\n        size = ngroups * 2\n        rng = np.arange(ngroups).reshape(-1, 1)\n        rng = np.broadcast_to(rng, (len(rng), ncols))\n        taker = np.random.randint(0, ngroups, size=size)\n        values = rng.take(taker, axis=0)\n        if dtype == \""int\"":\n            key = np.random.randint(0, size, size=size)\n        elif dtype in (\""int16\"", \""uint\""):\n            key = np.random.randint(0, size, size=size, dtype=dtype)\n        elif dtype == \""float\"":\n            key = np.concatenate(\n                [np.random.random(ngroups) * 0.1, np.random.random(ngroups) * 10.0]\n            )\n        elif dtype == \""object\"":\n            key = [\""foo\""] * size\n        elif dtype == \""datetime\"":\n            key = date_range(\""1/1/2011\"", periods=size, freq=\""s\"")\n    \n        cols = [f\""values{n}\"" for n in range(ncols)]\n        df = DataFrame(values, columns=cols)\n        df[\""key\""] = key\n    \n        if len(cols) == 1:\n            cols = cols[0]\n    \n        if application == \""transformation\"":\n            self.as_group_method = lambda: df.groupby(\""key\"")[cols].transform(method)\n            self.as_field_method = lambda: df.groupby(cols)[\""key\""].transform(method)\n        else:\n            self.as_group_method = getattr(df.groupby(\""key\"")[cols], method)\n            self.as_field_method = getattr(df.groupby(cols)[\""key\""], method)""}",groupby.GroupByMethods.time_dtype_as_field,time,-0.0033438402200889174
3741,param-level,"terminus-2,claude",1.0525076631231682,1.0355765116122002,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.GroupManyLabels.time_sum,groupby,"{""groupby.GroupManyLabels.time_sum"": ""class GroupManyLabels:\n    def time_sum(self, ncols):\n        self.df.groupby(self.labels).sum()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupManyLabels:\n    def setup(self, ncols):\n        N = 1000\n        data = np.random.randn(N, ncols)\n        self.labels = np.random.randint(0, 100, size=N)\n        self.df = DataFrame(data)""}",groupby.GroupManyLabels.time_sum,time,0.011973940248209353
3734,param-level,"terminus-2,claude",1.0352487998803463,1.0248017468505206,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Quantile.time_frame_quantile,frame_methods,"{""frame_methods.Quantile.time_frame_quantile"": ""class Quantile:\n    def time_frame_quantile(self, axis):\n        self.df.quantile([0.1, 0.5], axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, axis):\n        self.df = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))""}",frame_methods.Quantile.time_frame_quantile,time,0.007388297758009715
3747,param-level,"terminus-2,claude",1.129020629433639,1.1141947051174346,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,hash_functions.UniqueAndFactorizeArange.time_factorize,hash_functions,"{""hash_functions.UniqueAndFactorizeArange.time_factorize"": ""class UniqueAndFactorizeArange:\n    def time_factorize(self, exponent):\n        pd.factorize(self.a2)\n\n    def setup(self, exponent):\n        a = np.arange(10**4, dtype=\""float64\"")\n        self.a2 = (a + 10**exponent).repeat(100)""}",hash_functions.UniqueAndFactorizeArange.time_factorize,time,0.010485094990243516
3746,param-level,"terminus-2,claude",1.0971801607389584,1.0227767447743357,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,hash_functions.NumericSeriesIndexing.time_loc_slice,hash_functions,"{""hash_functions.NumericSeriesIndexing.time_loc_slice"": ""class NumericSeriesIndexing:\n    def time_loc_slice(self, index, N):\n        # trigger building of mapping\n        self.data.loc[:800]\n\n    def setup(self, dtype, N):\n        vals = np.array(list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype)\n        indices = pd.Index(vals)\n        self.data = pd.Series(np.arange(N), index=indices)""}",hash_functions.NumericSeriesIndexing.time_loc_slice,time,0.0526191060570175
3756,param-level,"terminus-2,claude",0.9753593178122134,0.9826809808730528,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.MethodLookup.time_lookup_loc,indexing,"{""indexing.MethodLookup.time_lookup_loc"": ""class MethodLookup:\n    def time_lookup_loc(self, s):\n        s.loc\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MethodLookup:\n    def setup_cache(self):\n        s = Series()\n        return s""}",indexing.MethodLookup.time_lookup_loc,time,-0.00517797953383265
3750,param-level,"terminus-2,claude",1.0785916293887383,1.0220004458551766,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_cached_properties.IndexCache.time_is_monotonic_increasing,index_cached_properties,"{""index_cached_properties.IndexCache.time_is_monotonic_increasing"": ""class IndexCache:\n    def time_is_monotonic_increasing(self, index_type):\n        self.idx.is_monotonic_increasing\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}""}",index_cached_properties.IndexCache.time_is_monotonic_increasing,time,0.040022053418360494
3744,param-level,"terminus-2,claude",1.0688795064751455,0.8448937156170028,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.String.time_str_func,groupby,"{""groupby.String.time_str_func"": ""class String:\n    def time_str_func(self, dtype, method):\n        self.df.groupby(\""a\"")[self.df.columns[1:]].agg(method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass String:\n    def setup(self, dtype, method):\n        cols = list(\""abcdefghjkl\"")\n        self.df = DataFrame(\n            np.random.randint(0, 100, size=(10_000, len(cols))),\n            columns=cols,\n            dtype=dtype,\n        )""}",groupby.String.time_str_func,time,0.15840579268609814
3748,param-level,"terminus-2,claude",1.04918511585981,1.0398100956535965,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_cached_properties.IndexCache.time_engine,index_cached_properties,"{""index_cached_properties.IndexCache.time_engine"": ""class IndexCache:\n    def time_engine(self, index_type):\n        self.idx._engine\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}""}",index_cached_properties.IndexCache.time_engine,time,0.006630141588552689
3749,param-level,"terminus-2,claude",1.1050428638006742,1.0838227693246487,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_cached_properties.IndexCache.time_is_monotonic_decreasing,index_cached_properties,"{""index_cached_properties.IndexCache.time_is_monotonic_decreasing"": ""class IndexCache:\n    def time_is_monotonic_decreasing(self, index_type):\n        self.idx.is_monotonic_decreasing\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}""}",index_cached_properties.IndexCache.time_is_monotonic_decreasing,time,0.015007138950513059
3751,param-level,"terminus-2,claude",0.9781066578649829,0.9864520167007408,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.CategoricalIndexIndexing.time_getitem_scalar,indexing,"{""indexing.CategoricalIndexIndexing.time_getitem_scalar"": ""class CategoricalIndexIndexing:\n    def time_getitem_scalar(self, index):\n        self.data[self.int_scalar]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalIndexIndexing:\n    def setup(self, index):\n        N = 10**5\n        values = list(\""a\"" * N + \""b\"" * N + \""c\"" * N)\n        indices = {\n            \""monotonic_incr\"": CategoricalIndex(values),\n            \""monotonic_decr\"": CategoricalIndex(reversed(values)),\n            \""non_monotonic\"": CategoricalIndex(list(\""abc\"" * N)),\n        }\n        self.data = indices[index]\n        self.data_unique = CategoricalIndex([str(i) for i in range(N * 3)])\n    \n        self.int_scalar = 10000\n        self.int_list = list(range(10000))\n    \n        self.cat_scalar = \""b\""\n        self.cat_list = [\""1\"", \""3\""]""}",indexing.CategoricalIndexIndexing.time_getitem_scalar,time,-0.005901951086108863
3745,param-level,"terminus-2,claude",1.0393049894783648,1.042586536069814,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.Transform.time_transform_lambda_max_wide,groupby,"{""groupby.Transform.time_transform_lambda_max_wide"": ""class Transform:\n    def time_transform_lambda_max_wide(self):\n        self.df_wide.groupby(level=0).transform(lambda x: np.max(x, axis=0))\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]""}",groupby.Transform.time_transform_lambda_max_wide,time,-0.0023207543079556596
3755,param-level,"terminus-2,claude",0.9667672142142164,0.9670654987224886,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.MethodLookup.time_lookup_iloc,indexing,"{""indexing.MethodLookup.time_lookup_iloc"": ""class MethodLookup:\n    def time_lookup_iloc(self, s):\n        s.iloc\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MethodLookup:\n    def setup_cache(self):\n        s = Series()\n        return s""}",indexing.MethodLookup.time_lookup_iloc,time,-0.00021095085450649514
3757,param-level,"terminus-2,claude",1.1974388419001345,1.052174150069658,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.MultiIndexing.time_loc_all_scalars,indexing,"{""indexing.MultiIndexing.time_loc_all_scalars"": ""class MultiIndexing:\n    def time_loc_all_scalars(self, unique_levels):\n        target = tuple([self.tgt_scalar] * self.nlevels)\n        self.df.loc[target, :]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MultiIndexing:\n    def setup(self, unique_levels):\n        self.nlevels = 2\n        if unique_levels:\n            mi = MultiIndex.from_arrays([range(1000000)] * self.nlevels)\n        else:\n            mi = MultiIndex.from_product([range(1000)] * self.nlevels)\n        self.df = DataFrame(np.random.randn(len(mi)), index=mi)\n    \n        self.tgt_slice = slice(200, 800)\n        self.tgt_null_slice = slice(None)\n        self.tgt_list = list(range(0, 1000, 10))\n        self.tgt_scalar = 500\n    \n        bool_indexer = np.zeros(len(mi), dtype=np.bool_)\n        bool_indexer[slice(0, len(mi), 100)] = True\n        self.tgt_bool_indexer = bool_indexer""}",indexing.MultiIndexing.time_loc_all_scalars,time,0.10273316253923376
3743,param-level,"terminus-2,claude",1.0284597004830336,0.9242780314831636,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.Size.time_category_size,groupby,"{""groupby.Size.time_category_size"": ""class Size:\n    def time_category_size(self):\n        self.draws.groupby(self.cats).size()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Size:\n    def setup(self):\n        n = 10**5\n        offsets = np.random.randint(n, size=n).astype(\""timedelta64[ns]\"")\n        dates = np.datetime64(\""now\"") + offsets\n        self.df = DataFrame(\n            {\n                \""key1\"": np.random.randint(0, 500, size=n),\n                \""key2\"": np.random.randint(0, 100, size=n),\n                \""value1\"": np.random.randn(n),\n                \""value2\"": np.random.randn(n),\n                \""value3\"": np.random.randn(n),\n                \""dates\"": dates,\n            }\n        )\n        self.draws = Series(np.random.randn(n))\n        labels = Series([\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (n // 4))\n        self.cats = labels.astype(\""category\"")""}",groupby.Size.time_category_size,time,0.07367869094757427
3753,param-level,"terminus-2,claude",0.983835109845662,0.9839495425121074,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.GetItemSingleColumn.time_frame_getitem_single_column_int,indexing,"{""indexing.GetItemSingleColumn.time_frame_getitem_single_column_int"": ""class GetItemSingleColumn:\n    def time_frame_getitem_single_column_int(self):\n        self.df_int_col[0]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GetItemSingleColumn:\n    def setup(self):\n        self.df_string_col = DataFrame(np.random.randn(3000, 1), columns=[\""A\""])\n        self.df_int_col = DataFrame(np.random.randn(3000, 1))""}",indexing.GetItemSingleColumn.time_frame_getitem_single_column_int,time,-8.092833553422667e-05
3760,param-level,"terminus-2,claude",1.2656880762130045,1.2617980705467426,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericMaskedIndexing.time_get_indexer,indexing,"{""indexing.NumericMaskedIndexing.time_get_indexer"": ""class NumericMaskedIndexing:\n    def time_get_indexer(self, dtype, monotonic):\n        self.data.get_indexer(self.indexer)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericMaskedIndexing:\n    def setup(self, dtype, monotonic):\n        indices = {\n            True: Index(self.monotonic_list, dtype=dtype),\n            False: Index(self.non_monotonic_list, dtype=dtype).append(\n                Index([NA], dtype=dtype)\n            ),\n        }\n        self.data = indices[monotonic]\n        self.indexer = np.arange(300, 1_000)\n        self.data_dups = self.data.append(self.data)""}",indexing.NumericMaskedIndexing.time_get_indexer,time,0.002751064827625073
3754,param-level,"terminus-2,claude",1.0230200839080663,1.038504163552764,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.IndexSingleRow.time_loc_row,indexing,"{""indexing.IndexSingleRow.time_loc_row"": ""class IndexSingleRow:\n    def time_loc_row(self, unique_cols):\n        self.df.loc[10000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IndexSingleRow:\n    def setup(self, unique_cols):\n        arr = np.arange(10**7).reshape(-1, 10)\n        df = DataFrame(arr)\n        dtypes = [\""u1\"", \""u2\"", \""u4\"", \""u8\"", \""i1\"", \""i2\"", \""i4\"", \""i8\"", \""f8\"", \""f4\""]\n        for i, d in enumerate(dtypes):\n            df[i] = df[i].astype(d)\n    \n        if not unique_cols:\n            # GH#33032 single-row lookups with non-unique columns were\n            #  15x slower than with unique columns\n            df.columns = [\""A\"", \""A\""] + list(df.columns[2:])\n    \n        self.df = df""}",indexing.IndexSingleRow.time_loc_row,time,-0.010950551375316676
3763,param-level,"terminus-2,claude",1.268344347663962,1.3297764991995344,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericSeriesIndexing.time_getitem_list_like,indexing,"{""indexing.NumericSeriesIndexing.time_getitem_list_like"": ""class NumericSeriesIndexing:\n    def time_getitem_list_like(self, index, index_structure):\n        self.data[[800000]]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.NumericSeriesIndexing.time_getitem_list_like,time,-0.04344565172246992
3758,param-level,"terminus-2,claude",1.091964196574294,1.0473046935586134,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NonNumericSeriesIndexing.time_getitem_label_slice,indexing,"{""indexing.NonNumericSeriesIndexing.time_getitem_label_slice"": ""class NonNumericSeriesIndexing:\n    def time_getitem_label_slice(self, index, index_structure):\n        self.s[: self.lbl]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]""}",indexing.NonNumericSeriesIndexing.time_getitem_label_slice,time,0.03158380694178265
3759,param-level,"terminus-2,claude",0.9651853581138266,0.9622641767121336,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NonNumericSeriesIndexing.time_getitem_scalar,indexing,"{""indexing.NonNumericSeriesIndexing.time_getitem_scalar"": ""class NonNumericSeriesIndexing:\n    def time_getitem_scalar(self, index, index_structure):\n        self.s[self.lbl]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]""}",indexing.NonNumericSeriesIndexing.time_getitem_scalar,time,0.002065899152540984
3752,param-level,"terminus-2,claude",1.1700522916098166,1.123397506290995,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.DataFrameStringIndexing.time_at_setitem,indexing,"{""indexing.DataFrameStringIndexing.time_at_setitem"": ""class DataFrameStringIndexing:\n    def time_at_setitem(self):\n        self.df.at[self.idx_scalar, self.col_scalar] = 0.0\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DataFrameStringIndexing:\n    def setup(self):\n        index = tm.makeStringIndex(1000)\n        columns = tm.makeStringIndex(30)\n        with warnings.catch_warnings(record=True):\n            self.df = DataFrame(np.random.randn(1000, 30), index=index, columns=columns)\n        self.idx_scalar = index[100]\n        self.col_scalar = columns[10]\n        self.bool_indexer = self.df[self.col_scalar] > 0\n        self.bool_obj_indexer = self.bool_indexer.astype(object)\n        self.boolean_indexer = (self.df[self.col_scalar] > 0).astype(\""boolean\"")""}",indexing.DataFrameStringIndexing.time_at_setitem,time,0.032994897679506016
3768,param-level,"terminus-2,claude",1.042131212065787,1.0720742353261292,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing_engines.MaskedNumericEngineIndexing.time_get_loc,indexing_engines,"{""indexing_engines.MaskedNumericEngineIndexing.time_get_loc"": ""class MaskedNumericEngineIndexing:\n    def time_get_loc(self, engine_and_dtype, index_type, unique, N):\n        self.data.get_loc(self.key_early)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())[::-1]\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.zeros(N * 3, dtype=dtype.lower())\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype.lower())\n                arr[N:] = np.arange(N * 2, dtype=dtype.lower())\n    \n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n            mask[-1] = True\n    \n        self.data = engine(BaseMaskedArray(arr, mask))\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]""}",indexing_engines.MaskedNumericEngineIndexing.time_get_loc,time,-0.021176112631076487
3762,param-level,"terminus-2,claude",1.152386552649178,1.1377686919683618,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericSeriesIndexing.time_getitem_array,indexing,"{""indexing.NumericSeriesIndexing.time_getitem_array"": ""class NumericSeriesIndexing:\n    def time_getitem_array(self, index, index_structure):\n        self.data[self.array]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.NumericSeriesIndexing.time_getitem_array,time,0.010337949562104862
3765,param-level,"terminus-2,claude",1.2052607171767469,1.1173194857077458,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericSeriesIndexing.time_getitem_scalar,indexing,"{""indexing.NumericSeriesIndexing.time_getitem_scalar"": ""class NumericSeriesIndexing:\n    def time_getitem_scalar(self, index, index_structure):\n        self.data[800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.NumericSeriesIndexing.time_getitem_scalar,time,0.0621932330049513
3774,param-level,"terminus-2,claude",1.0255708562611685,1.0503761610002444,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.csv.ReadCSVEngine.time_read_bytescsv,io.csv,"{""io.csv.ReadCSVEngine.time_read_bytescsv"": ""class ReadCSVEngine:\n    def time_read_bytescsv(self, engine):\n        read_csv(self.data(self.BytesIO_input), engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVEngine:\n    def setup(self, engine):\n        data = [\""A,B,C,D,E\""] + ([\""1,2,3,4,5\""] * 100000)\n        self.StringIO_input = StringIO(\""\\n\"".join(data))\n        # simulate reading from file\n        self.BytesIO_input = BytesIO(self.StringIO_input.read().encode(\""utf-8\""))""}",io.csv.ReadCSVEngine.time_read_bytescsv,time,-0.017542648330322447
3766,param-level,"terminus-2,claude",1.2122411735831442,1.2623581032741231,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericSeriesIndexing.time_getitem_slice,indexing,"{""indexing.NumericSeriesIndexing.time_getitem_slice"": ""class NumericSeriesIndexing:\n    def time_getitem_slice(self, index, index_structure):\n        self.data[:800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.NumericSeriesIndexing.time_getitem_slice,time,-0.035443373190225544
3772,param-level,"terminus-2,claude",1.0232202472077538,1.0287380341736552,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,inference.ToNumericDowncast.time_downcast,inference,"{""inference.ToNumericDowncast.time_downcast"": ""class ToNumericDowncast:\n    def time_downcast(self, dtype, downcast):\n        to_numeric(self.data, downcast=downcast)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToNumericDowncast:\n    def setup(self, dtype, downcast):\n        self.data = self.data_dict[dtype]""}",inference.ToNumericDowncast.time_downcast,time,-0.003902253865559638
3764,param-level,"terminus-2,claude",1.2380816076528536,1.208053664760974,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericSeriesIndexing.time_getitem_lists,indexing,"{""indexing.NumericSeriesIndexing.time_getitem_lists"": ""class NumericSeriesIndexing:\n    def time_getitem_lists(self, index, index_structure):\n        self.data[self.array_list]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.NumericSeriesIndexing.time_getitem_lists,time,0.021236168947580968
3767,param-level,"terminus-2,claude",1.1321112935328206,1.092596161613923,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericSeriesIndexing.time_loc_array,indexing,"{""indexing.NumericSeriesIndexing.time_loc_array"": ""class NumericSeriesIndexing:\n    def time_loc_array(self, index, index_structure):\n        self.data.loc[self.array]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.NumericSeriesIndexing.time_loc_array,time,0.027945637849291056
3777,param-level,"terminus-2,claude",1.023823510221133,1.029046138693322,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,join_merge.MergeAsof.time_by_object,join_merge,"{""join_merge.MergeAsof.time_by_object"": ""class MergeAsof:\n    def time_by_object(self, direction, tolerance):\n        merge_asof(\n            self.df1b,\n            self.df2b,\n            on=\""time\"",\n            by=\""key\"",\n            direction=direction,\n            tolerance=tolerance,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MergeAsof:\n    def setup(self, direction, tolerance):\n        one_count = 200000\n        two_count = 1000000\n    \n        df1 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, one_count / 20, one_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), one_count),\n                \""key2\"": np.random.randint(0, 25, one_count),\n                \""value1\"": np.random.randn(one_count),\n            }\n        )\n        df2 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, two_count / 20, two_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), two_count),\n                \""key2\"": np.random.randint(0, 25, two_count),\n                \""value2\"": np.random.randn(two_count),\n            }\n        )\n    \n        df1 = df1.sort_values(\""time\"")\n        df2 = df2.sort_values(\""time\"")\n    \n        df1[\""time32\""] = np.int32(df1.time)\n        df2[\""time32\""] = np.int32(df2.time)\n    \n        df1[\""timeu64\""] = np.uint64(df1.time)\n        df2[\""timeu64\""] = np.uint64(df2.time)\n    \n        self.df1a = df1[[\""time\"", \""value1\""]]\n        self.df2a = df2[[\""time\"", \""value2\""]]\n        self.df1b = df1[[\""time\"", \""key\"", \""value1\""]]\n        self.df2b = df2[[\""time\"", \""key\"", \""value2\""]]\n        self.df1c = df1[[\""time\"", \""key2\"", \""value1\""]]\n        self.df2c = df2[[\""time\"", \""key2\"", \""value2\""]]\n        self.df1d = df1[[\""time32\"", \""value1\""]]\n        self.df2d = df2[[\""time32\"", \""value2\""]]\n        self.df1e = df1[[\""time\"", \""key\"", \""key2\"", \""value1\""]]\n        self.df2e = df2[[\""time\"", \""key\"", \""key2\"", \""value2\""]]\n        self.df1f = df1[[\""timeu64\"", \""value1\""]]\n        self.df2f = df2[[\""timeu64\"", \""value2\""]]""}",join_merge.MergeAsof.time_by_object,time,-0.003693513770996476
3761,param-level,"terminus-2,claude",1.2826399838873093,1.2018416031412138,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericMaskedIndexing.time_get_indexer_dups,indexing,"{""indexing.NumericMaskedIndexing.time_get_indexer_dups"": ""class NumericMaskedIndexing:\n    def time_get_indexer_dups(self, dtype, monotonic):\n        self.data.get_indexer_for(self.indexer)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericMaskedIndexing:\n    def setup(self, dtype, monotonic):\n        indices = {\n            True: Index(self.monotonic_list, dtype=dtype),\n            False: Index(self.non_monotonic_list, dtype=dtype).append(\n                Index([NA], dtype=dtype)\n            ),\n        }\n        self.data = indices[monotonic]\n        self.indexer = np.arange(300, 1_000)\n        self.data_dups = self.data.append(self.data)""}",indexing.NumericMaskedIndexing.time_get_indexer_dups,time,0.05714171198450882
3770,param-level,"terminus-2,claude",0.97589858782559,0.9817947556689464,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing_engines.NumericEngineIndexing.time_get_loc,indexing_engines,"{""indexing_engines.NumericEngineIndexing.time_get_loc"": ""class NumericEngineIndexing:\n    def time_get_loc(self, engine_and_dtype, index_type, unique, N):\n        self.data.get_loc(self.key_early)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)[::-1]\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.empty(N * 3, dtype=dtype)\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype)\n                arr[N:] = np.arange(N * 2, dtype=dtype)\n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype)\n    \n        self.data = engine(arr)\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]""}",indexing_engines.NumericEngineIndexing.time_get_loc,time,-0.004169849959940832
3785,param-level,"terminus-2,claude",1.0201766673229071,1.0345564193262995,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.WideToLong.time_wide_to_long_big,reshape,"{""reshape.WideToLong.time_wide_to_long_big"": ""class WideToLong:\n    def time_wide_to_long_big(self):\n        wide_to_long(self.df, self.letters, i=\""id\"", j=\""year\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass WideToLong:\n    def setup(self):\n        nyrs = 20\n        nidvars = 20\n        N = 5000\n        self.letters = list(\""ABCD\"")\n        yrvars = [\n            letter + str(num)\n            for letter, num in product(self.letters, range(1, nyrs + 1))\n        ]\n        columns = [str(i) for i in range(nidvars)] + yrvars\n        self.df = DataFrame(np.random.randn(N, nidvars + len(yrvars)), columns=columns)\n        self.df[\""id\""] = self.df.index""}",reshape.WideToLong.time_wide_to_long_big,time,-0.01016955587227183
3769,param-level,"terminus-2,claude",0.9697459709984686,0.9912539862742844,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing_engines.MaskedNumericEngineIndexing.time_get_loc_near_middle,indexing_engines,"{""indexing_engines.MaskedNumericEngineIndexing.time_get_loc_near_middle"": ""class MaskedNumericEngineIndexing:\n    def time_get_loc_near_middle(self, engine_and_dtype, index_type, unique, N):\n        # searchsorted performance may be different near the middle of a range\n        #  vs near an endpoint\n        self.data.get_loc(self.key_middle)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())[::-1]\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.zeros(N * 3, dtype=dtype.lower())\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype.lower())\n                arr[N:] = np.arange(N * 2, dtype=dtype.lower())\n    \n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n            mask[-1] = True\n    \n        self.data = engine(BaseMaskedArray(arr, mask))\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]""}",indexing_engines.MaskedNumericEngineIndexing.time_get_loc_near_middle,time,-0.015210760449657572
3771,param-level,"terminus-2,claude",0.9781403239100914,0.979555696240948,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing_engines.NumericEngineIndexing.time_get_loc_near_middle,indexing_engines,"{""indexing_engines.NumericEngineIndexing.time_get_loc_near_middle"": ""class NumericEngineIndexing:\n    def time_get_loc_near_middle(self, engine_and_dtype, index_type, unique, N):\n        # searchsorted performance may be different near the middle of a range\n        #  vs near an endpoint\n        self.data.get_loc(self.key_middle)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)[::-1]\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.empty(N * 3, dtype=dtype)\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype)\n                arr[N:] = np.arange(N * 2, dtype=dtype)\n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype)\n    \n        self.data = engine(arr)\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]""}",indexing_engines.NumericEngineIndexing.time_get_loc_near_middle,time,-0.0010009705310159368
3776,param-level,"terminus-2,claude",0.9618587767167588,0.97232287374828,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.style.Render.time_format_render,io.style,"{""io.style.Render.time_format_render"": ""class Render:\n    def time_format_render(self, cols, rows):\n        self._style_format()\n        self.st._render_html(True, True)\n\n    def setup(self, cols, rows):\n        self.df = DataFrame(\n            np.random.randn(rows, cols),\n            columns=[f\""float_{i+1}\"" for i in range(cols)],\n            index=[f\""row_{i+1}\"" for i in range(rows)],\n        )""}",io.style.Render.time_format_render,time,-0.007400351507440722
3773,param-level,"terminus-2,claude",1.0874673740712428,1.121376298596207,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.csv.ReadCSVCategorical.time_convert_post,io.csv,"{""io.csv.ReadCSVCategorical.time_convert_post"": ""class ReadCSVCategorical:\n    def time_convert_post(self, engine):\n        read_csv(self.fname, engine=engine).apply(Categorical)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVCategorical:\n    def setup(self, engine):\n        N = 100000\n        group1 = [\""aaaaaaaa\"", \""bbbbbbb\"", \""cccccccc\"", \""dddddddd\"", \""eeeeeeee\""]\n        df = DataFrame(np.random.choice(group1, (N, 3)), columns=list(\""abc\""))\n        df.to_csv(self.fname, index=False)""}",io.csv.ReadCSVCategorical.time_convert_post,time,-0.02398085185641032
3779,param-level,"terminus-2,claude",1.0094226647694298,1.0420834684650648,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,libs.ScalarListLike.time_is_list_like,libs,"{""libs.ScalarListLike.time_is_list_like"": ""class ScalarListLike:\n    def time_is_list_like(self, param):\n        is_list_like(param)""}",libs.ScalarListLike.time_is_list_like,time,-0.0230981638582992
3775,param-level,"terminus-2,claude",1.035572857603586,1.065536048258366,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.json.NormalizeJSON.time_normalize_json,io.json,"{""io.json.NormalizeJSON.time_normalize_json"": ""class NormalizeJSON:\n    def time_normalize_json(self, orient, frame):\n        json_normalize(self.data)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NormalizeJSON:\n    def setup(self, orient, frame):\n        data = {\n            \""hello\"": [\""thisisatest\"", 999898, \""mixed types\""],\n            \""nest1\"": {\""nest2\"": {\""nest3\"": \""nest3_value\"", \""nest3_int\"": 3445}},\n            \""nest1_list\"": {\""nest2\"": [\""blah\"", 32423, 546456.876, 92030234]},\n            \""hello2\"": \""string\"",\n        }\n        self.data = [data for i in range(10000)]""}",io.json.NormalizeJSON.time_normalize_json,time,-0.02119037528626586
3783,param-level,"terminus-2,claude",1.1209021120866982,1.1395950020018186,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.Cut.time_qcut_datetime,reshape,"{""reshape.Cut.time_qcut_datetime"": ""class Cut:\n    def time_qcut_datetime(self, bins):\n        pd.qcut(self.datetime_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))""}",reshape.Cut.time_qcut_datetime,time,-0.01321986556939209
3788,param-level,"terminus-2,claude",1.0343581803854074,1.0459585804573994,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.ForwardWindowMethods.time_rolling,rolling,"{""rolling.ForwardWindowMethods.time_rolling"": ""class ForwardWindowMethods:\n    def time_rolling(self, constructor, window_size, dtype, method):\n        getattr(self.roll, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ForwardWindowMethods:\n    def setup(self, constructor, window_size, dtype, method):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=window_size)\n        self.roll = getattr(pd, constructor)(arr).rolling(window=indexer)""}",rolling.ForwardWindowMethods.time_rolling,time,-0.008203960446953349
3780,param-level,"terminus-2,claude",0.9558450517631384,0.9662950819021658,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,multiindex_object.Integer.time_is_monotonic,multiindex_object,"{""multiindex_object.Integer.time_is_monotonic"": ""class Integer:\n    def time_is_monotonic(self):\n        self.mi_int.is_monotonic_increasing\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Integer:\n    def setup(self):\n        self.mi_int = MultiIndex.from_product(\n            [np.arange(1000), np.arange(1000)], names=[\""one\"", \""two\""]\n        )\n        self.obj_index = np.array(\n            [\n                (0, 10),\n                (0, 11),\n                (0, 12),\n                (0, 13),\n                (0, 14),\n                (0, 15),\n                (0, 16),\n                (0, 17),\n                (0, 18),\n                (0, 19),\n            ],\n            dtype=object,\n        )\n        self.other_mi_many_mismatches = MultiIndex.from_tuples(\n            [\n                (-7, 41),\n                (-2, 3),\n                (-0.7, 5),\n                (0, 0),\n                (0, 1.5),\n                (0, 340),\n                (0, 1001),\n                (1, -4),\n                (1, 20),\n                (1, 1040),\n                (432, -5),\n                (432, 17),\n                (439, 165.5),\n                (998, -4),\n                (998, 24065),\n                (999, 865.2),\n                (999, 1000),\n                (1045, -843),\n            ]\n        )""}",multiindex_object.Integer.time_is_monotonic,time,-0.00739040321006185
3781,param-level,"terminus-2,claude",0.9865002009491602,0.9821791176867636,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.Cut.time_cut_interval,reshape,"{""reshape.Cut.time_cut_interval"": ""class Cut:\n    def time_cut_interval(self, bins):\n        # GH 27668\n        pd.cut(self.int_series, self.interval_bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))""}",reshape.Cut.time_cut_interval,time,0.003055928756999055
3782,param-level,"terminus-2,claude",1.1272804856936771,1.1568802304914567,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.Cut.time_cut_timedelta,reshape,"{""reshape.Cut.time_cut_timedelta"": ""class Cut:\n    def time_cut_timedelta(self, bins):\n        pd.cut(self.timedelta_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))""}",reshape.Cut.time_cut_timedelta,time,-0.020933341441145406
3786,param-level,"terminus-2,claude",1.023523215951254,1.0680562094800006,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Apply.time_rolling,rolling,"{""rolling.Apply.time_rolling"": ""class Apply:\n    def time_rolling(self, constructor, window, dtype, function, raw):\n        self.roll.apply(function, raw=raw)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, constructor, window, dtype, function, raw):\n        N = 10**3\n        arr = (100 * np.random.random(N)).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)""}",rolling.Apply.time_rolling,time,-0.03149433771481374
3784,param-level,"terminus-2,claude",0.9920469880372312,1.0221675206886116,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.Explode.time_explode,reshape,"{""reshape.Explode.time_explode"": ""class Explode:\n    def time_explode(self, n_rows, max_list_length):\n        self.series.explode()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Explode:\n    def setup(self, n_rows, max_list_length):\n        data = [np.arange(np.random.randint(max_list_length)) for _ in range(n_rows)]\n        self.series = pd.Series(data)""}",reshape.Explode.time_explode,time,-0.021301649682730107
3787,param-level,"terminus-2,claude",1.0720491588329937,1.0919853885459143,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.EWMMethods.time_ewm,rolling,"{""rolling.EWMMethods.time_ewm"": ""class EWMMethods:\n    def time_ewm(self, constructor, kwargs_method, dtype):\n        getattr(self.ewm, self.method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass EWMMethods:\n    def setup(self, constructor, kwargs_method, dtype):\n        N = 10**5\n        kwargs, method = kwargs_method\n        arr = (100 * np.random.random(N)).astype(dtype)\n        self.method = method\n        self.ewm = getattr(pd, constructor)(arr).ewm(**kwargs)""}",rolling.EWMMethods.time_ewm,time,-0.014099172357086734
3792,param-level,"terminus-2,claude",1.0336586160589174,1.0647197439195957,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Quantile.time_quantile,rolling,"{""rolling.Quantile.time_quantile"": ""class Quantile:\n    def time_quantile(self, constructor, window, dtype, percentile, interpolation):\n        self.roll.quantile(percentile, interpolation=interpolation)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, constructor, window, dtype, percentile, interpolation):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)""}",rolling.Quantile.time_quantile,time,-0.0219668513866183
3790,param-level,"terminus-2,claude",0.9298952524513872,1.0348813857642023,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Methods.time_method,rolling,"{""rolling.Methods.time_method"": ""class Methods:\n    def time_method(self, constructor, window_kwargs, dtype, method):\n        getattr(self.window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Methods:\n    def setup(self, constructor, window_kwargs, dtype, method):\n        N = 10**5\n        window, kwargs = window_kwargs\n        arr = (100 * np.random.random(N)).astype(dtype)\n        obj = getattr(pd, constructor)(arr)\n        self.window = getattr(obj, window)(**kwargs)""}",rolling.Methods.time_method,time,-0.07424761903310831
3789,param-level,"terminus-2,claude",1.0722471169548256,1.089453826648073,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Groupby.time_method,rolling,"{""rolling.Groupby.time_method"": ""class Groupby:\n    def time_method(self, method, window_kwargs):\n        getattr(self.groupby_window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Groupby:\n    def setup(self, method, window_kwargs):\n        N = 1000\n        window, kwargs = window_kwargs\n        df = pd.DataFrame(\n            {\n                \""A\"": [str(i) for i in range(N)] * 10,\n                \""B\"": list(range(N)) * 10,\n            }\n        )\n        if isinstance(kwargs.get(\""window\"", None), str):\n            df.index = pd.date_range(start=\""1900-01-01\"", freq=\""1min\"", periods=N * 10)\n        self.groupby_window = getattr(df.groupby(\""A\""), window)(**kwargs)""}",rolling.Groupby.time_method,time,-0.012168818736384411
3778,param-level,"terminus-2,claude",1.0410915933272462,1.0651493160553005,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,join_merge.MergeAsof.time_on_int32,join_merge,"{""join_merge.MergeAsof.time_on_int32"": ""class MergeAsof:\n    def time_on_int32(self, direction, tolerance):\n        merge_asof(\n            self.df1d, self.df2d, on=\""time32\"", direction=direction, tolerance=tolerance\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MergeAsof:\n    def setup(self, direction, tolerance):\n        one_count = 200000\n        two_count = 1000000\n    \n        df1 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, one_count / 20, one_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), one_count),\n                \""key2\"": np.random.randint(0, 25, one_count),\n                \""value1\"": np.random.randn(one_count),\n            }\n        )\n        df2 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, two_count / 20, two_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), two_count),\n                \""key2\"": np.random.randint(0, 25, two_count),\n                \""value2\"": np.random.randn(two_count),\n            }\n        )\n    \n        df1 = df1.sort_values(\""time\"")\n        df2 = df2.sort_values(\""time\"")\n    \n        df1[\""time32\""] = np.int32(df1.time)\n        df2[\""time32\""] = np.int32(df2.time)\n    \n        df1[\""timeu64\""] = np.uint64(df1.time)\n        df2[\""timeu64\""] = np.uint64(df2.time)\n    \n        self.df1a = df1[[\""time\"", \""value1\""]]\n        self.df2a = df2[[\""time\"", \""value2\""]]\n        self.df1b = df1[[\""time\"", \""key\"", \""value1\""]]\n        self.df2b = df2[[\""time\"", \""key\"", \""value2\""]]\n        self.df1c = df1[[\""time\"", \""key2\"", \""value1\""]]\n        self.df2c = df2[[\""time\"", \""key2\"", \""value2\""]]\n        self.df1d = df1[[\""time32\"", \""value1\""]]\n        self.df2d = df2[[\""time32\"", \""value2\""]]\n        self.df1e = df1[[\""time\"", \""key\"", \""key2\"", \""value1\""]]\n        self.df2e = df2[[\""time\"", \""key\"", \""key2\"", \""value2\""]]\n        self.df1f = df1[[\""timeu64\"", \""value1\""]]\n        self.df2f = df2[[\""timeu64\"", \""value2\""]]""}",join_merge.MergeAsof.time_on_int32,time,-0.017013948181085068
3794,param-level,"terminus-2,claude",1.0367516467908529,1.0685224038975427,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.VariableWindowMethods.time_method,rolling,"{""rolling.VariableWindowMethods.time_method"": ""class Methods:\n    def time_method(self, constructor, window_kwargs, dtype, method):\n        getattr(self.window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass VariableWindowMethods:\n    def setup(self, constructor, window, dtype, method):\n        N = 10**5\n        arr = (100 * np.random.random(N)).astype(dtype)\n        index = pd.date_range(\""2017-01-01\"", periods=N, freq=\""5s\"")\n        self.window = getattr(pd, constructor)(arr, index=index).rolling(window)""}",rolling.VariableWindowMethods.time_method,time,-0.022468710825098912
3805,param-level,"terminus-2,claude",1.0363817620577986,1.037615166606883,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Methods.time_endswith,strings,"{""strings.Methods.time_endswith"": ""class Methods:\n    def time_endswith(self, dtype):\n        self.s.str.endswith(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.Methods.time_endswith,time,-0.0008722804448969389
3795,param-level,"terminus-2,claude",1.032162152737261,1.069441903454608,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.NanOps.time_func,series_methods,"{""series_methods.NanOps.time_func"": ""class NanOps:\n    def time_func(self, func, N, dtype):\n        self.func()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NanOps:\n    def setup(self, func, N, dtype):\n        if func == \""argmax\"" and dtype in {\""Int64\"", \""boolean\""}:\n            # Skip argmax for nullable int since this doesn't work yet (GH-24382)\n            raise NotImplementedError\n        self.s = Series([1] * N, dtype=dtype)\n        self.func = getattr(self.s, func)""}",series_methods.NanOps.time_func,time,-0.02636474591042929
3791,param-level,"terminus-2,claude",1.038201593307916,1.056450567090326,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Pairwise.time_pairwise,rolling,"{""rolling.Pairwise.time_pairwise"": ""class Pairwise:\n    def time_pairwise(self, kwargs_window, method, pairwise):\n        getattr(self.window, method)(self.df, pairwise=pairwise)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Pairwise:\n    def setup(self, kwargs_window, method, pairwise):\n        N = 10**4\n        n_groups = 20\n        kwargs, window = kwargs_window\n        groups = [i for _ in range(N // n_groups) for i in range(n_groups)]\n        arr = np.random.random(N)\n        self.df = pd.DataFrame(arr)\n        self.window = getattr(self.df, window)(**kwargs)\n        self.window_group = getattr(\n            pd.DataFrame({\""A\"": groups, \""B\"": arr}).groupby(\""A\""), window\n        )(**kwargs)""}",rolling.Pairwise.time_pairwise,time,-0.012905922052623664
3800,param-level,"terminus-2,claude",0.9029970317607616,0.912691752252956,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.ArithmeticBlock.time_addition,sparse,"{""sparse.ArithmeticBlock.time_addition"": ""class ArithmeticBlock:\n    def time_addition(self, fill_value):\n        self.arr1 + self.arr2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )""}",sparse.ArithmeticBlock.time_addition,time,-0.006856237971848891
3798,param-level,"terminus-2,claude",0.901870976407824,0.91583897802989,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.Arithmetic.time_divide,sparse,"{""sparse.Arithmetic.time_divide"": ""class Arithmetic:\n    def time_divide(self, dense_proportion, fill_value):\n        self.array1 / self.array2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)""}",sparse.Arithmetic.time_divide,time,-0.009878360411644992
3797,param-level,"terminus-2,claude",0.907003308295164,0.9048225669589512,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.Arithmetic.time_add,sparse,"{""sparse.Arithmetic.time_add"": ""class Arithmetic:\n    def time_add(self, dense_proportion, fill_value):\n        self.array1 + self.array2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)""}",sparse.Arithmetic.time_add,time,0.001542249884167405
3793,param-level,"terminus-2,claude",1.0135964294916813,1.024648637049011,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Rank.time_rank,rolling,"{""rolling.Rank.time_rank"": ""class Rank:\n    def time_rank(self, constructor, window, dtype, percentile, ascending, method):\n        self.roll.rank(pct=percentile, ascending=ascending, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Rank:\n    def setup(self, constructor, window, dtype, percentile, ascending, method):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)""}",rolling.Rank.time_rank,time,-0.00781627125695179
3806,param-level,"terminus-2,claude",1.2990149339732082,1.3086013614510008,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Methods.time_lower,strings,"{""strings.Methods.time_lower"": ""class Methods:\n    def time_lower(self, dtype):\n        self.s.str.lower()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.Methods.time_lower,time,-0.006779651681607208
3801,param-level,"terminus-2,claude",0.9082765254154672,0.9086828083158498,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.ArithmeticBlock.time_division,sparse,"{""sparse.ArithmeticBlock.time_division"": ""class ArithmeticBlock:\n    def time_division(self, fill_value):\n        self.arr1 / self.arr2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )""}",sparse.ArithmeticBlock.time_division,time,-0.0002873287838632121
3796,param-level,"terminus-2,claude",1.065313482244743,1.1004792458334434,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.SearchSorted.time_searchsorted,series_methods,"{""series_methods.SearchSorted.time_searchsorted"": ""class SearchSorted:\n    def time_searchsorted(self, dtype):\n        key = \""2\"" if dtype == \""str\"" else 2\n        self.s.searchsorted(key)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SearchSorted:\n    def setup(self, dtype):\n        N = 10**5\n        data = np.array([1] * N + [2] * N + [3] * N).astype(dtype)\n        self.s = Series(data)""}",series_methods.SearchSorted.time_searchsorted,time,-0.02486970550827467
3799,param-level,"terminus-2,claude",0.906351903253844,0.9198406736292096,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.Arithmetic.time_make_union,sparse,"{""sparse.Arithmetic.time_make_union"": ""class Arithmetic:\n    def time_make_union(self, dense_proportion, fill_value):\n        self.array1.sp_index.make_union(self.array2.sp_index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)""}",sparse.Arithmetic.time_make_union,time,-0.009539441566736681
3802,param-level,"terminus-2,claude",0.8834481150541339,0.8920809027583091,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.ArithmeticBlock.time_make_union,sparse,"{""sparse.ArithmeticBlock.time_make_union"": ""class ArithmeticBlock:\n    def time_make_union(self, fill_value):\n        self.arr1.sp_index.make_union(self.arr2.sp_index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )""}",sparse.ArithmeticBlock.time_make_union,time,-0.006105224684706643
3804,param-level,"terminus-2,claude",1.008567272346436,1.0198460204179998,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Dummies.time_get_dummies,strings,"{""strings.Dummies.time_get_dummies"": ""class Dummies:\n    def time_get_dummies(self, dtype):\n        self.s.str.get_dummies(\""|\"")\n\n    def setup(self, dtype):\n        super().setup(dtype)\n        self.s = self.s.str.join(\""|\"")""}",strings.Dummies.time_get_dummies,time,-0.00797648378469852
3803,param-level,"terminus-2,claude",0.9810111873552728,1.0204791059132996,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strftime.BusinessHourStrftime.time_frame_offset_repr,strftime,"{""strftime.BusinessHourStrftime.time_frame_offset_repr"": ""class BusinessHourStrftime:\n    def time_frame_offset_repr(self, obs):\n        self.data[\""off\""].apply(repr)\n\n    def setup(self, obs):\n        self.data = pd.DataFrame(\n            {\n                \""off\"": [offsets.BusinessHour()] * obs,\n            }\n        )""}",strftime.BusinessHourStrftime.time_frame_offset_repr,time,-0.027912247919396595
3808,param-level,"terminus-2,claude",1.0340796936974983,1.0312078855701563,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Methods.time_wrap,strings,"{""strings.Methods.time_wrap"": ""class Methods:\n    def time_wrap(self, dtype):\n        self.s.str.wrap(10)\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.Methods.time_wrap,time,0.0020309817025049516
3813,param-level,"terminus-2,claude",0.9939602532093392,0.9749632137875638,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.fields.TimeGetDateField.time_get_date_field,tslibs.fields,"{""tslibs.fields.TimeGetDateField.time_get_date_field"": ""class TimeGetDateField:\n    def time_get_date_field(self, size, field):\n        get_date_field(self.i8data, field)\n\n    def setup(self, size, field):\n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.fields.TimeGetDateField.time_get_date_field,time,0.013434964230392817
3819,param-level,"terminus-2,claude",1.0343418984428585,1.0701985069308426,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.OffestDatetimeArithmetic.time_subtract,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_subtract"": ""class OffestDatetimeArithmetic:\n    def time_subtract(self, offset):\n        self.date - offset\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.OffestDatetimeArithmetic.time_subtract,time,-0.025358280401686077
3807,param-level,"terminus-2,claude",1.3072067435402686,1.3596393751609956,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Methods.time_upper,strings,"{""strings.Methods.time_upper"": ""class Methods:\n    def time_upper(self, dtype):\n        self.s.str.upper()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.Methods.time_upper,time,-0.03708106903870374
3816,param-level,"terminus-2,claude",1.0258982717045702,1.0497694699987314,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.OffestDatetimeArithmetic.time_add,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_add"": ""class OffestDatetimeArithmetic:\n    def time_add(self, offset):\n        self.date + offset\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.OffestDatetimeArithmetic.time_add,time,-0.016882035568713784
3812,param-level,"terminus-2,claude",1.0789464905606996,1.0714083894064304,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.TzLocalize.time_infer_dst,timeseries,"{""timeseries.TzLocalize.time_infer_dst"": ""class TzLocalize:\n    def time_infer_dst(self, tz):\n        self.index.tz_localize(tz, ambiguous=\""infer\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TzLocalize:\n    def setup(self, tz):\n        dst_rng = date_range(\n            start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n        )\n        self.index = date_range(start=\""10/29/2000\"", end=\""10/29/2000 00:59:59\"", freq=\""S\"")\n        self.index = self.index.append(dst_rng)\n        self.index = self.index.append(dst_rng)\n        self.index = self.index.append(\n            date_range(start=\""10/29/2000 2:00:00\"", end=\""10/29/2000 3:00:00\"", freq=\""S\"")\n        )""}",timeseries.TzLocalize.time_infer_dst,time,0.005331047492411052
3810,param-level,"terminus-2,claude",0.978481232734824,0.9911992258081832,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.DatetimeIndex.time_timeseries_is_month_start,timeseries,"{""timeseries.DatetimeIndex.time_timeseries_is_month_start"": ""class DatetimeIndex:\n    def time_timeseries_is_month_start(self, index_type):\n        self.index.is_month_start\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]""}",timeseries.DatetimeIndex.time_timeseries_is_month_start,time,-0.00899433739275756
3811,param-level,"terminus-2,claude",0.9863214042910328,0.9852909091765004,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.DatetimeIndex.time_to_date,timeseries,"{""timeseries.DatetimeIndex.time_to_date"": ""class DatetimeIndex:\n    def time_to_date(self, index_type):\n        self.index.date\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]""}",timeseries.DatetimeIndex.time_to_date,time,0.0007287801375759182
3809,param-level,"terminus-2,claude",0.9578109586223608,0.9594607562437548,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.DatetimeAccessor.time_dt_accessor_month_name,timeseries,"{""timeseries.DatetimeAccessor.time_dt_accessor_month_name"": ""class DatetimeAccessor:\n    def time_dt_accessor_month_name(self, tz):\n        self.series.dt.month_name()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeAccessor:\n    def setup(self, tz):\n        N = 100000\n        self.series = Series(date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"", tz=tz))""}",timeseries.DatetimeAccessor.time_dt_accessor_month_name,time,-0.0011667592796280452
3814,param-level,"terminus-2,claude",1.0175278213569792,1.0255275966642323,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.fields.TimeGetStartEndField.time_get_start_end_field,tslibs.fields,"{""tslibs.fields.TimeGetStartEndField.time_get_start_end_field"": ""class TimeGetStartEndField:\n    def time_get_start_end_field(self, size, side, period, freqstr, month_kw):\n        get_start_end_field(self.i8data, self.attrname, freqstr, month_kw=month_kw)\n\n    def setup(self, size, side, period, freqstr, month_kw):\n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr\n    \n        self.attrname = f\""is_{period}_{side}\""""}",tslibs.fields.TimeGetStartEndField.time_get_start_end_field,time,-0.005657549722244098
3817,param-level,"terminus-2,claude",1.037556228698402,1.0460564224178994,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.OffestDatetimeArithmetic.time_add_10,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_add_10"": ""class OffestDatetimeArithmetic:\n    def time_add_10(self, offset):\n        self.date + (10 * offset)\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.OffestDatetimeArithmetic.time_add_10,time,-0.006011452418315069
3824,param-level,"terminus-2,claude",0.9681999031708676,0.9754416429948614,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr,tslibs.period,"{""tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr"": ""class TimePeriodArrToDT64Arr:\n    def time_periodarray_to_dt64arr(self, size, freq):\n        periodarr_to_dt64arr(self.i8values, freq)\n\n    def setup(self, size, freq):\n        arr = np.arange(10, dtype=\""i8\"").repeat(size // 10)\n        self.i8values = arr""}",tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr,time,-0.005121456735497728
3825,param-level,"terminus-2,claude",0.9740867278162968,0.9958619199845872,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.resolution.TimeResolution.time_get_resolution,tslibs.resolution,"{""tslibs.resolution.TimeResolution.time_get_resolution"": ""class TimeResolution:\n    def time_get_resolution(self, unit, size, tz):\n        get_resolution(self.i8data, tz)\n\n    def setup(self, unit, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        arr = arr.view(f\""M8[{unit}]\"").astype(\""M8[ns]\"").view(\""i8\"")\n        self.i8data = arr""}",tslibs.resolution.TimeResolution.time_get_resolution,time,-0.015399711575877214
3815,param-level,"terminus-2,claude",0.965368946532643,0.964714718516657,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.normalize.Normalize.time_is_date_array_normalized,tslibs.normalize,"{""tslibs.normalize.Normalize.time_is_date_array_normalized"": ""class Normalize:\n    def time_is_date_array_normalized(self, size, tz):\n        # TODO: cases with different levels of short-circuiting\n        # 10 i.e. NPY_FR_ns\n        is_date_array_normalized(self.i8data, tz, 10)\n\n    def setup(self, size, tz):\n        # use an array that will have is_date_array_normalized give True,\n        #  so we do not short-circuit early.\n        dti = pd.date_range(\""2016-01-01\"", periods=10, tz=tz).repeat(size // 10)\n        self.i8data = dti.asi8\n    \n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError""}",tslibs.normalize.Normalize.time_is_date_array_normalized,time,0.00046267893634089447
3822,param-level,"terminus-2,claude",0.976999345527098,1.0187289916107118,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.PeriodUnaryMethods.time_asfreq,tslibs.period,"{""tslibs.period.PeriodUnaryMethods.time_asfreq"": ""class PeriodUnaryMethods:\n    def time_asfreq(self, freq):\n        self.per.asfreq(\""A\"")\n\n    def setup(self, freq):\n        self.per = Period(\""2012-06-01\"", freq=freq)""}",tslibs.period.PeriodUnaryMethods.time_asfreq,time,-0.029511772336360573
3830,param-level,"terminus-2,claude",0.976740856511139,1.0250200716088496,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampOps.time_floor,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_floor"": ""class TimestampOps:\n    def time_floor(self, tz):\n        self.ts.floor(\""5T\"")\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.TimestampOps.time_floor,time,-0.03414371647645729
3829,param-level,"terminus-2,claude",0.9991201711043596,1.0436114744293197,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampOps.time_ceil,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_ceil"": ""class TimestampOps:\n    def time_ceil(self, tz):\n        self.ts.ceil(\""5T\"")\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.TimestampOps.time_ceil,time,-0.031464853836605446
3818,param-level,"terminus-2,claude",1.0657905849412628,1.0665039317249991,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.OffestDatetimeArithmetic.time_add_np_dt64,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_add_np_dt64"": ""class OffestDatetimeArithmetic:\n    def time_add_np_dt64(self, offset):\n        offset + self.dt64\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.OffestDatetimeArithmetic.time_add_np_dt64,time,-0.0005044885316381593
3820,param-level,"terminus-2,claude",1.0476579684743583,1.0596632947327829,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.OffestDatetimeArithmetic.time_subtract_10,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_subtract_10"": ""class OffestDatetimeArithmetic:\n    def time_subtract_10(self, offset):\n        self.date - (10 * offset)\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.OffestDatetimeArithmetic.time_subtract_10,time,-0.008490329744288927
3821,param-level,"terminus-2,claude",0.9846621274957932,0.9637683780969988,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.OnOffset.time_on_offset,tslibs.offsets,"{""tslibs.offsets.OnOffset.time_on_offset"": ""class OnOffset:\n    def time_on_offset(self, offset):\n        for date in self.dates:\n            offset.is_on_offset(date)\n\n    def setup(self, offset):\n        self.dates = [\n            datetime(2016, m, d)\n            for m in [10, 11, 12]\n            for d in [1, 2, 3, 28, 29, 30, 31]\n            if not (m == 11 and d == 31)\n        ]""}",tslibs.offsets.OnOffset.time_on_offset,time,0.01477634328061839
3823,param-level,"terminus-2,claude",0.9770122721104428,0.9966201154783566,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr,tslibs.period,"{""tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr"": ""class TimeDT64ArrToPeriodArr:\n    def time_dt64arr_to_periodarr(self, size, freq, tz):\n        dt64arr_to_periodarr(self.i8values, freq, tz)\n\n    def setup(self, size, freq, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.arange(10, dtype=\""i8\"").repeat(size // 10)\n        self.i8values = arr""}",tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr,time,-0.013866933074903656
3836,param-level,"terminus-2,claude",0.99044397149246,1.0073551653987558,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampProperties.time_is_quarter_start,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_is_quarter_start"": ""class TimestampProperties:\n    def time_is_quarter_start(self, tz):\n        self.ts.is_quarter_start\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.TimestampProperties.time_is_quarter_start,time,-0.011959825959190812
3839,param-level,"terminus-2,claude",0.9874857971476948,0.9815589348407184,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampProperties.time_weekday_name,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_weekday_name"": ""class TimestampProperties:\n    def time_weekday_name(self, tz):\n        self.ts.day_name()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.TimestampProperties.time_weekday_name,time,0.004191557501397713
3833,param-level,"terminus-2,claude",1.007877364357194,1.0141674302365769,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampOps.time_to_julian_date,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_to_julian_date"": ""class TimestampOps:\n    def time_to_julian_date(self, tz):\n        self.ts.to_julian_date()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.TimestampOps.time_to_julian_date,time,-0.004448419999563469
3831,param-level,"terminus-2,claude",1.000635883812254,1.040393835731974,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampOps.time_normalize,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_normalize"": ""class TimestampOps:\n    def time_normalize(self, tz):\n        self.ts.normalize()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.TimestampOps.time_normalize,time,-0.02811736345100423
3827,param-level,"terminus-2,claude",1.0333647046128065,1.01738764873653,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampConstruction.time_from_npdatetime64,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_from_npdatetime64"": ""class TimestampConstruction:\n    def time_from_npdatetime64(self):\n        Timestamp(self.npdatetime64)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.TimestampConstruction.time_from_npdatetime64,time,0.011299190860167188
3834,param-level,"terminus-2,claude",0.993524248271918,0.97861892483466,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampOps.time_tz_localize,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_tz_localize"": ""class TimestampOps:\n    def time_tz_localize(self, tz):\n        if self.ts.tz is None:\n            self.ts.tz_localize(tz)\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.TimestampOps.time_tz_localize,time,0.010541247126773717
3826,param-level,"terminus-2,claude",1.036665999451161,1.0156490510826353,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timedelta.TimedeltaConstructor.time_from_datetime_timedelta,tslibs.timedelta,"{""tslibs.timedelta.TimedeltaConstructor.time_from_datetime_timedelta"": ""class TimedeltaConstructor:\n    def time_from_datetime_timedelta(self):\n        Timedelta(self.dttimedelta)\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")""}",tslibs.timedelta.TimedeltaConstructor.time_from_datetime_timedelta,time,0.014863471264869615
3837,param-level,"terminus-2,claude",0.9916259670054186,1.0090970559195138,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampProperties.time_microsecond,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_microsecond"": ""class TimestampProperties:\n    def time_microsecond(self, tz):\n        self.ts.microsecond\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.TimestampProperties.time_microsecond,time,-0.01235579131124131
3835,param-level,"terminus-2,claude",0.9859417141392418,1.0088496122333308,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampProperties.time_days_in_month,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_days_in_month"": ""class TimestampProperties:\n    def time_days_in_month(self, tz):\n        self.ts.days_in_month\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.TimestampProperties.time_days_in_month,time,-0.01620077658705021
3840,param-level,"terminus-2,claude",0.98887136318111,1.018084972226594,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime,tslibs.tslib,"{""tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime"": ""class TimeIntsToPydatetime:\n    def time_ints_to_pydatetime(self, box, size, tz):\n        ints_to_pydatetime(self.i8data, tz, box=box)\n\n    def setup(self, box, size, tz):\n        if box == \""date\"" and tz is not None:\n            # tz is ignored, so avoid running redundant benchmarks\n            raise NotImplementedError  # skip benchmark\n        if size == 10**6 and tz is _tzs[-1]:\n            # This is cumbersomely-slow, so skip to trim runtime\n            raise NotImplementedError  # skip benchmark\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime,time,-0.020660260993977435
3828,param-level,"terminus-2,claude",1.0485715149287966,1.0390043640338635,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampConstruction.time_parse_iso8601_no_tz,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_parse_iso8601_no_tz"": ""class TimestampConstruction:\n    def time_parse_iso8601_no_tz(self):\n        Timestamp(\""2017-08-25 08:16:14\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.TimestampConstruction.time_parse_iso8601_no_tz,time,0.006766019020461879
3838,param-level,"terminus-2,claude",0.982610059394668,0.9791569957625458,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampProperties.time_month_name,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_month_name"": ""class TimestampProperties:\n    def time_month_name(self, tz):\n        self.ts.month_name()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.TimestampProperties.time_month_name,time,0.002442053488063767
3832,param-level,"terminus-2,claude",0.9803632786892996,0.9834207220656008,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampOps.time_replace_tz,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_replace_tz"": ""class TimestampOps:\n    def time_replace_tz(self, tz):\n        self.ts.replace(tzinfo=pytz.timezone(\""US/Eastern\""))\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.TimestampOps.time_replace_tz,time,-0.00216226547121726
3841,param-level,"terminus-2,claude",0.9819732726671584,0.992241436005673,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.tz_convert.TimeTZConvert.time_tz_convert_from_utc,tslibs.tz_convert,"{""tslibs.tz_convert.TimeTZConvert.time_tz_convert_from_utc"": ""class TimeTZConvert:\n    def time_tz_convert_from_utc(self, size, tz):\n        # effectively:\n        #  dti = DatetimeIndex(self.i8data, tz=tz)\n        #  dti.tz_localize(None)\n        if old_sig:\n            tz_convert_from_utc(self.i8data, UTC, tz)\n        else:\n            tz_convert_from_utc(self.i8data, tz)\n\n    def setup(self, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.tz_convert.TimeTZConvert.time_tz_convert_from_utc,time,-0.00726178453926072
3842,param-level,"terminus-2,gpt-5",1.320380371101082,1.0265993124974522,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,algorithms.Duplicated.time_duplicated,algorithms,"{""algorithms.Duplicated.time_duplicated"": ""class Duplicated:\n    def time_duplicated(self, unique, keep, dtype):\n        self.idx.duplicated(keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Duplicated:\n    def setup(self, unique, keep, dtype):\n        N = 10**5\n        data = {\n            \""int\"": pd.Index(np.arange(N), dtype=\""int64\""),\n            \""uint\"": pd.Index(np.arange(N), dtype=\""uint64\""),\n            \""float\"": pd.Index(np.random.randn(N), dtype=\""float64\""),\n            \""string\"": tm.makeStringIndex(N),\n            \""datetime64[ns]\"": pd.date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n            \""datetime64[ns, tz]\"": pd.date_range(\n                \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n            ),\n        }[dtype]\n        if not unique:\n            data = data.repeat(5)\n        self.idx = data\n        # cache is_unique\n        self.idx.is_unique""}",algorithms.Duplicated.time_duplicated,time,0.207765953750799
3845,param-level,"terminus-2,gpt-5",1.04541781423117,1.0618129829695413,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.IsIn.time_isin_categorical,algos.isin,"{""algos.isin.IsIn.time_isin_categorical"": ""class IsIn:\n    def time_isin_categorical(self, dtype):\n        self.series.isin(self.cat_values)\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)""}",algos.isin.IsIn.time_isin_categorical,time,-0.011594885953586431
3843,param-level,"terminus-2,gpt-5",1.2250480322656268,1.1702881536723302,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,algorithms.Factorize.time_factorize,algorithms,"{""algorithms.Factorize.time_factorize"": ""class Factorize:\n    def time_factorize(self, unique, sort, dtype):\n        pd.factorize(self.data, sort=sort)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Factorize:\n    def setup(self, unique, sort, dtype):\n        N = 10**5\n        string_index = tm.makeStringIndex(N)\n        string_arrow = None\n        if dtype == \""string[pyarrow]\"":\n            try:\n                string_arrow = pd.array(string_index, dtype=\""string[pyarrow]\"")\n            except ImportError:\n                raise NotImplementedError\n    \n        data = {\n            \""int\"": pd.Index(np.arange(N), dtype=\""int64\""),\n            \""uint\"": pd.Index(np.arange(N), dtype=\""uint64\""),\n            \""float\"": pd.Index(np.random.randn(N), dtype=\""float64\""),\n            \""object\"": string_index,\n            \""datetime64[ns]\"": pd.date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n            \""datetime64[ns, tz]\"": pd.date_range(\n                \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n            ),\n            \""Int64\"": pd.array(np.arange(N), dtype=\""Int64\""),\n            \""boolean\"": pd.array(np.random.randint(0, 2, N), dtype=\""boolean\""),\n            \""string[pyarrow]\"": string_arrow,\n        }[dtype]\n        if not unique:\n            data = data.repeat(5)\n        self.data = data""}",algorithms.Factorize.time_factorize,time,0.03872692969822954
3848,param-level,"terminus-2,gpt-5",1.122841416979358,1.173784775878192,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.IsInLongSeriesLookUpDominates.time_isin,algos.isin,"{""algos.isin.IsInLongSeriesLookUpDominates.time_isin"": ""class IsInLongSeriesLookUpDominates:\n    def time_isin(self, dtypes, MaxNumber, series_type):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, MaxNumber, series_type):\n        N = 10**7\n    \n        if series_type == \""random_hits\"":\n            array = np.random.randint(0, MaxNumber, N)\n        if series_type == \""random_misses\"":\n            array = np.random.randint(0, MaxNumber, N) + MaxNumber\n        if series_type == \""monotone_hits\"":\n            array = np.repeat(np.arange(MaxNumber), N // MaxNumber)\n        if series_type == \""monotone_misses\"":\n            array = np.arange(N) + MaxNumber\n    \n        self.series = Series(array).astype(dtype)\n    \n        self.values = np.arange(MaxNumber).astype(dtype.lower())""}",algos.isin.IsInLongSeriesLookUpDominates.time_isin,time,-0.03602783514769036
3853,param-level,"terminus-2,gpt-5",0.9777050864705156,0.981116408258753,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,array.BooleanArray.time_from_bool_array,array,"{""array.BooleanArray.time_from_bool_array"": ""class BooleanArray:\n    def time_from_bool_array(self):\n        pd.array(self.values_bool, dtype=\""boolean\"")\n\n    def setup(self):\n        self.values_bool = np.array([True, False, True, False])\n        self.values_float = np.array([1.0, 0.0, 1.0, 0.0])\n        self.values_integer = np.array([1, 0, 1, 0])\n        self.values_integer_like = [1, 0, 1, 0]\n        self.data = np.array([True, False, True, False])\n        self.mask = np.array([False, False, True, False])""}",array.BooleanArray.time_from_bool_array,time,-0.0024125330892768334
3850,param-level,"terminus-2,gpt-5",1.122802610650385,0.7970819164535795,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.NumericInferOps.time_divide,arithmetic,"{""arithmetic.NumericInferOps.time_divide"": ""class NumericInferOps:\n    def time_divide(self, dtype):\n        self.df[\""A\""] / self.df[\""B\""]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericInferOps:\n    def setup(self, dtype):\n        N = 5 * 10**5\n        self.df = DataFrame(\n            {\""A\"": np.arange(N).astype(dtype), \""B\"": np.arange(N).astype(dtype)}\n        )""}",arithmetic.NumericInferOps.time_divide,time,0.23035409773465737
3846,param-level,"terminus-2,gpt-5",1.0677645872985508,1.0572102104876364,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.IsIn.time_isin_empty,algos.isin,"{""algos.isin.IsIn.time_isin_empty"": ""class IsIn:\n    def time_isin_empty(self, dtype):\n        self.series.isin([])\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)""}",algos.isin.IsIn.time_isin_empty,time,0.007464198593291672
3847,param-level,"terminus-2,gpt-5",1.1104912903573572,1.1108688007180527,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.IsIn.time_isin_mismatched_dtype,algos.isin,"{""algos.isin.IsIn.time_isin_mismatched_dtype"": ""class IsIn:\n    def time_isin_mismatched_dtype(self, dtype):\n        self.series.isin(self.mismatched)\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)""}",algos.isin.IsIn.time_isin_mismatched_dtype,time,-0.0002669804531085957
3851,param-level,"terminus-2,gpt-5",0.99927294112674,0.969296484273031,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.OffsetArrayArithmetic.time_add_dti_offset,arithmetic,"{""arithmetic.OffsetArrayArithmetic.time_add_dti_offset"": ""class OffsetArrayArithmetic:\n    def time_add_dti_offset(self, offset):\n        with warnings.catch_warnings(record=True):\n            self.rng + offset\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass OffsetArrayArithmetic:\n    def setup(self, offset):\n        N = 10000\n        rng = date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"")\n        self.rng = rng\n        self.ser = Series(rng)""}",arithmetic.OffsetArrayArithmetic.time_add_dti_offset,time,0.021199757322283658
3852,param-level,"terminus-2,gpt-5",0.980583029376074,0.948302164804101,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,array.ArrowExtensionArray.time_to_numpy,array,"{""array.ArrowExtensionArray.time_to_numpy"": ""class ArrowExtensionArray:\n    def time_to_numpy(self, dtype, hasna):\n        self.arr.to_numpy()\n\n    def setup(self, dtype, hasna):\n        N = 100_000\n        if dtype == \""boolean[pyarrow]\"":\n            data = np.random.choice([True, False], N, replace=True)\n        elif dtype == \""float64[pyarrow]\"":\n            data = np.random.randn(N)\n        elif dtype == \""int64[pyarrow]\"":\n            data = np.arange(N)\n        elif dtype == \""string[pyarrow]\"":\n            data = tm.rands_array(10, N)\n        elif dtype == \""timestamp[ns][pyarrow]\"":\n            data = pd.date_range(\""2000-01-01\"", freq=\""s\"", periods=N)\n        else:\n            raise NotImplementedError\n    \n        arr = pd.array(data, dtype=dtype)\n        if hasna:\n            arr[::2] = pd.NA\n        self.arr = arr""}",array.ArrowExtensionArray.time_to_numpy,time,0.02282946575104172
3854,param-level,"terminus-2,gpt-5",0.9856389943622818,0.9690018695661616,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,boolean.TimeLogicalOps.time_and_scalar,boolean,"{""boolean.TimeLogicalOps.time_and_scalar"": ""class TimeLogicalOps:\n    def time_and_scalar(self):\n        self.left & True\n        self.left & False\n\n    def setup(self):\n        N = 10_000\n        left, right, lmask, rmask = np.random.randint(0, 2, size=(4, N)).astype(\""bool\"")\n        self.left = pd.arrays.BooleanArray(left, lmask)\n        self.right = pd.arrays.BooleanArray(right, rmask)""}",boolean.TimeLogicalOps.time_and_scalar,time,0.011766000563026957
3857,param-level,"terminus-2,gpt-5",1.0155700319684131,0.9843871123238844,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.DtypesInvalid.time_pandas_dtype_invalid,dtypes,"{""dtypes.DtypesInvalid.time_pandas_dtype_invalid"": ""class DtypesInvalid:\n    def time_pandas_dtype_invalid(self, dtype):\n        try:\n            pandas_dtype(self.data_dict[dtype])\n        except TypeError:\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)""}",dtypes.DtypesInvalid.time_pandas_dtype_invalid,time,0.022052984189907133
3844,param-level,"terminus-2,gpt-5",1.1345145036953457,1.123615183139807,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,algorithms.Quantile.time_quantile,algorithms,"{""algorithms.Quantile.time_quantile"": ""class Quantile:\n    def time_quantile(self, quantile, interpolation, dtype):\n        self.idx.quantile(quantile, interpolation=interpolation)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, quantile, interpolation, dtype):\n        N = 10**5\n        data = {\n            \""int\"": np.arange(N),\n            \""uint\"": np.arange(N).astype(np.uint64),\n            \""float\"": np.random.randn(N),\n        }\n        self.idx = pd.Series(data[dtype].repeat(5))""}",algorithms.Quantile.time_quantile,time,0.007708147493308891
3849,param-level,"terminus-2,gpt-5",1.0634911940872729,0.9570005702528782,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.IntFrameWithScalar.time_frame_op_with_scalar,arithmetic,"{""arithmetic.IntFrameWithScalar.time_frame_op_with_scalar"": ""class IntFrameWithScalar:\n    def time_frame_op_with_scalar(self, dtype, scalar, op):\n        op(self.df, scalar)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IntFrameWithScalar:\n    def setup(self, dtype, scalar, op):\n        arr = np.random.randn(20000, 100)\n        self.df = DataFrame(arr.astype(dtype))""}",arithmetic.IntFrameWithScalar.time_frame_op_with_scalar,time,0.07531161515869496
3855,param-level,"terminus-2,gpt-5",0.9751943239358802,0.9825157394271008,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.CategoricalSlicing.time_getitem_scalar,categoricals,"{""categoricals.CategoricalSlicing.time_getitem_scalar"": ""class CategoricalSlicing:\n    def time_getitem_scalar(self, index):\n        self.data[self.scalar]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalSlicing:\n    def setup(self, index):\n        N = 10**6\n        categories = [\""a\"", \""b\"", \""c\""]\n        values = [0] * N + [1] * N + [2] * N\n        if index == \""monotonic_incr\"":\n            self.data = pd.Categorical.from_codes(values, categories=categories)\n        elif index == \""monotonic_decr\"":\n            self.data = pd.Categorical.from_codes(\n                list(reversed(values)), categories=categories\n            )\n        elif index == \""non_monotonic\"":\n            self.data = pd.Categorical.from_codes([0, 1, 2] * N, categories=categories)\n        else:\n            raise ValueError(f\""Invalid index param: {index}\"")\n    \n        self.scalar = 10000\n        self.list = list(range(10000))\n        self.cat_scalar = \""b\""""}",categoricals.CategoricalSlicing.time_getitem_scalar,time,-0.005177804449236607
3862,param-level,"terminus-2,gpt-5",1.1556759264583172,1.159924219526538,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.Eval.time_chained_cmp,eval,"{""eval.Eval.time_chained_cmp"": ""class Eval:\n    def time_chained_cmp(self, engine, threads):\n        pd.eval(\""self.df < self.df2 < self.df3 < self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)""}",eval.Eval.time_chained_cmp,time,-0.0030044505432962037
3863,param-level,"terminus-2,gpt-5",1.4722037190624588,1.3893436394248913,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.Eval.time_mult,eval,"{""eval.Eval.time_mult"": ""class Eval:\n    def time_mult(self, engine, threads):\n        pd.eval(\""self.df * self.df2 * self.df3 * self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)""}",eval.Eval.time_mult,time,0.05859977343533776
3856,param-level,"terminus-2,gpt-5",1.0997558588050889,0.901924432099614,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,ctors.SeriesConstructors.time_series_constructor,ctors,"{""ctors.SeriesConstructors.time_series_constructor"": ""class SeriesConstructors:\n    def time_series_constructor(self, data_fmt, with_index, dtype):\n        Series(self.data, index=self.index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesConstructors:\n    def setup(self, data_fmt, with_index, dtype):\n        if data_fmt in (gen_of_str, gen_of_tuples) and with_index:\n            raise NotImplementedError(\n                \""Series constructors do not support using generators with indexes\""\n            )\n        N = 10**4\n        if dtype == \""float\"":\n            arr = np.random.randn(N)\n        else:\n            arr = np.arange(N)\n        self.data = data_fmt(arr)\n        self.index = np.arange(N) if with_index else None""}",ctors.SeriesConstructors.time_series_constructor,time,0.13990907122027924
3860,param-level,"terminus-2,gpt-5",1.3115797104729858,1.2709339453302828,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.Eval.time_add,eval,"{""eval.Eval.time_add"": ""class Eval:\n    def time_add(self, engine, threads):\n        pd.eval(\""self.df + self.df2 + self.df3 + self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)""}",eval.Eval.time_add,time,0.02874523701747031
3861,param-level,"terminus-2,gpt-5",1.2009407550352635,1.1345405893436904,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.Eval.time_and,eval,"{""eval.Eval.time_and"": ""class Eval:\n    def time_and(self, engine, threads):\n        pd.eval(\n            \""(self.df > 0) & (self.df2 > 0) & (self.df3 > 0) & (self.df4 > 0)\"",\n            engine=engine,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)""}",eval.Eval.time_and,time,0.04695909879177733
3858,param-level,"terminus-2,gpt-5",1.0719707549674986,1.0679607860056457,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.SelectDtypes.time_select_dtype_int_exclude,dtypes,"{""dtypes.SelectDtypes.time_select_dtype_int_exclude"": ""class SelectDtypes:\n    def time_select_dtype_int_exclude(self, dtype):\n        self.df_int.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )""}",dtypes.SelectDtypes.time_select_dtype_int_exclude,time,0.0028359044991887595
3867,param-level,"terminus-2,gpt-5",1.1568948063018856,1.109764724574163,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromArrays.time_frame_from_arrays_sparse,frame_ctor,"{""frame_ctor.FromArrays.time_frame_from_arrays_sparse"": ""class FromArrays:\n    def time_frame_from_arrays_sparse(self):\n        self.df = DataFrame._from_arrays(\n            self.sparse_arrays,\n            index=self.index,\n            columns=self.columns,\n            verify_integrity=False,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromArrays:\n    def setup(self):\n        N_rows = 1000\n        N_cols = 1000\n        self.float_arrays = [np.random.randn(N_rows) for _ in range(N_cols)]\n        self.sparse_arrays = [\n            pd.arrays.SparseArray(np.random.randint(0, 2, N_rows), dtype=\""float64\"")\n            for _ in range(N_cols)\n        ]\n        self.int_arrays = [\n            pd.array(np.random.randint(1000, size=N_rows), dtype=\""Int64\"")\n            for _ in range(N_cols)\n        ]\n        self.index = pd.Index(range(N_rows))\n        self.columns = pd.Index(range(N_cols))""}",frame_ctor.FromArrays.time_frame_from_arrays_sparse,time,0.033331033753693416
3865,param-level,"terminus-2,gpt-5",1.17618474881113,1.1546050789147206,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.Query.time_query_with_boolean_selection,eval,"{""eval.Query.time_query_with_boolean_selection"": ""class Query:\n    def time_query_with_boolean_selection(self):\n        self.df.query(\""(a >= @self.min_val) & (a <= @self.max_val)\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Query:\n    def setup(self):\n        N = 10**6\n        halfway = (N // 2) - 1\n        index = pd.date_range(\""20010101\"", periods=N, freq=\""T\"")\n        s = pd.Series(index)\n        self.ts = s.iloc[halfway]\n        self.df = pd.DataFrame({\""a\"": np.random.randn(N), \""dates\"": index}, index=index)\n        data = np.random.randn(N)\n        self.min_val = data.min()\n        self.max_val = data.max()""}",eval.Query.time_query_with_boolean_selection,time,0.015261435570303728
3868,param-level,"terminus-2,gpt-5",1.2988358280316128,1.0644373832418372,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromDicts.time_list_of_dict,frame_ctor,"{""frame_ctor.FromDicts.time_list_of_dict"": ""class FromDicts:\n    def time_list_of_dict(self):\n        DataFrame(self.dict_list)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDicts:\n    def setup(self):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n        frame = DataFrame(np.random.randn(N, K), index=self.index, columns=self.columns)\n        self.data = frame.to_dict()\n        self.dict_list = frame.to_dict(orient=\""records\"")\n        self.data2 = {i: {j: float(j) for j in range(100)} for i in range(2000)}\n    \n        # arrays which we won't consolidate\n        self.dict_of_categoricals = {i: Categorical(np.arange(N)) for i in range(K)}""}",frame_ctor.FromDicts.time_list_of_dict,time,0.16576976293477763
3864,param-level,"terminus-2,gpt-5",1.1780682023749691,1.1317455443362747,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.Query.time_query_datetime_column,eval,"{""eval.Query.time_query_datetime_column"": ""class Query:\n    def time_query_datetime_column(self):\n        self.df.query(\""dates < @self.ts\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Query:\n    def setup(self):\n        N = 10**6\n        halfway = (N // 2) - 1\n        index = pd.date_range(\""20010101\"", periods=N, freq=\""T\"")\n        s = pd.Series(index)\n        self.ts = s.iloc[halfway]\n        self.df = pd.DataFrame({\""a\"": np.random.randn(N), \""dates\"": index}, index=index)\n        data = np.random.randn(N)\n        self.min_val = data.min()\n        self.max_val = data.max()""}",eval.Query.time_query_datetime_column,time,0.03276001275720962
3871,param-level,"terminus-2,gpt-5",1.1142605285625165,1.0485527998893167,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Describe.time_dataframe_describe,frame_methods,"{""frame_methods.Describe.time_dataframe_describe"": ""class Describe:\n    def time_dataframe_describe(self):\n        self.df.describe()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Describe:\n    def setup(self):\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(0, 100, 10**6),\n                \""b\"": np.random.randint(0, 100, 10**6),\n                \""c\"": np.random.randint(0, 100, 10**6),\n            }\n        )""}",frame_methods.Describe.time_dataframe_describe,time,0.04646939793012712
3869,param-level,"terminus-2,gpt-5",1.0535484176124674,1.0407155721351138,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromScalar.time_frame_from_scalar_ea_float64_na,frame_ctor,"{""frame_ctor.FromScalar.time_frame_from_scalar_ea_float64_na"": ""class FromScalar:\n    def time_frame_from_scalar_ea_float64_na(self):\n        DataFrame(\n            NA,\n            index=range(self.nrows),\n            columns=list(\""abc\""),\n            dtype=Float64Dtype(),\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromScalar:\n    def setup(self):\n        self.nrows = 100_000""}",frame_ctor.FromScalar.time_frame_from_scalar_ea_float64_na,time,0.009075562572385884
3866,param-level,"terminus-2,gpt-5",0.9836834185219744,0.994961338034834,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,finalize.Finalize.time_finalize_micro,finalize,"{""finalize.Finalize.time_finalize_micro"": ""class Finalize:\n    def time_finalize_micro(self, param):\n        self.obj.__finalize__(self.obj, method=\""__finalize__\"")\n\n    def setup(self, param):\n        N = 1000\n        obj = param(dtype=float)\n        for i in range(N):\n            obj.attrs[i] = i\n        self.obj = obj""}",finalize.Finalize.time_finalize_micro,time,-0.0079758978167324
3870,param-level,"terminus-2,gpt-5",1.2331877612548838,1.1014807750057138,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Count.time_count_level_mixed_dtypes_multi,frame_methods,"{""frame_methods.Count.time_count_level_mixed_dtypes_multi"": ""class Count:\n    def time_count_level_mixed_dtypes_multi(self, axis):\n        self.df_mixed.count(axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Count:\n    def setup(self, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""\n    \n        self.df.index = MultiIndex.from_arrays([self.df.index, self.df.index])\n        self.df.columns = MultiIndex.from_arrays([self.df.columns, self.df.columns])\n        self.df_mixed.index = MultiIndex.from_arrays(\n            [self.df_mixed.index, self.df_mixed.index]\n        )\n        self.df_mixed.columns = MultiIndex.from_arrays(\n            [self.df_mixed.columns, self.df_mixed.columns]\n        )""}",frame_methods.Count.time_count_level_mixed_dtypes_multi,time,0.09314496905881894
3873,param-level,"terminus-2,gpt-5",1.22214795531945,1.1480368354203323,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Dropna.time_dropna,frame_methods,"{""frame_methods.Dropna.time_dropna"": ""class Dropna:\n    def time_dropna(self, how, axis):\n        self.df.dropna(how=how, axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Dropna:\n    def setup(self, how, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""""}",frame_methods.Dropna.time_dropna,time,0.05241239031055006
3859,param-level,"terminus-2,gpt-5",1.1083670438653708,1.3767683410933085,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.SelectDtypes.time_select_dtype_string_exclude,dtypes,"{""dtypes.SelectDtypes.time_select_dtype_string_exclude"": ""class SelectDtypes:\n    def time_select_dtype_string_exclude(self, dtype):\n        self.df_string.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )""}",dtypes.SelectDtypes.time_select_dtype_string_exclude,time,-0.1898170418868018
3872,param-level,"terminus-2,gpt-5",1.1032608803409794,1.0482445796343702,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Describe.time_series_describe,frame_methods,"{""frame_methods.Describe.time_series_describe"": ""class Describe:\n    def time_series_describe(self):\n        self.df[\""a\""].describe()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Describe:\n    def setup(self):\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(0, 100, 10**6),\n                \""b\"": np.random.randint(0, 100, 10**6),\n                \""c\"": np.random.randint(0, 100, 10**6),\n            }\n        )""}",frame_methods.Describe.time_series_describe,time,0.03890827489859203
3881,param-level,"terminus-2,gpt-5",1.0748106205367165,1.139579860075716,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,gil.ParallelGroupbyMethods.time_parallel,gil,"{""gil.ParallelGroupbyMethods.time_parallel"": ""class ParallelGroupbyMethods:\n    def time_parallel(self, threads, method):\n        self.parallel()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ParallelGroupbyMethods:\n    def setup(self, threads, method):\n        N = 10**6\n        ngroups = 10**3\n        df = DataFrame(\n            {\""key\"": np.random.randint(0, ngroups, size=N), \""data\"": np.random.randn(N)}\n        )\n    \n        @test_parallel(num_threads=threads)\n        def parallel():\n            getattr(df.groupby(\""key\"")[\""data\""], method)()\n    \n        self.parallel = parallel\n    \n        def loop():\n            getattr(df.groupby(\""key\"")[\""data\""], method)()\n    \n        self.loop = loop""}",gil.ParallelGroupbyMethods.time_parallel,time,-0.04580568567114541
3882,param-level,"terminus-2,gpt-5",1.119538568783104,1.0230055870235066,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.Apply.time_scalar_function_multi_col,groupby,"{""groupby.Apply.time_scalar_function_multi_col"": ""class Apply:\n    def time_scalar_function_multi_col(self, factor):\n        self.df.groupby([\""key\"", \""key2\""]).apply(lambda x: 1)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, factor):\n        N = 10**factor\n        # two cases:\n        # - small groups: small data (N**4) + many labels (2000) -> average group\n        #   size of 5 (-> larger overhead of slicing method)\n        # - larger groups: larger data (N**5) + fewer labels (20) -> average group\n        #   size of 5000\n        labels = np.random.randint(0, 2000 if factor == 4 else 20, size=N)\n        labels2 = np.random.randint(0, 3, size=N)\n        df = DataFrame(\n            {\n                \""key\"": labels,\n                \""key2\"": labels2,\n                \""value1\"": np.random.randn(N),\n                \""value2\"": [\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (N // 4),\n            }\n        )\n        self.df = df""}",groupby.Apply.time_scalar_function_multi_col,time,0.06826943547354841
3888,param-level,"terminus-2,gpt-5",1.1111333736972115,0.7678419434164367,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.String.time_str_func,groupby,"{""groupby.String.time_str_func"": ""class String:\n    def time_str_func(self, dtype, method):\n        self.df.groupby(\""a\"")[self.df.columns[1:]].agg(method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass String:\n    def setup(self, dtype, method):\n        cols = list(\""abcdefghjkl\"")\n        self.df = DataFrame(\n            np.random.randint(0, 100, size=(10_000, len(cols))),\n            columns=cols,\n            dtype=dtype,\n        )""}",groupby.String.time_str_func,time,0.2427803608774928
3877,param-level,"terminus-2,gpt-5",1.1691773958960416,1.0972896468407467,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Fillna.time_frame_fillna,frame_methods,"{""frame_methods.Fillna.time_frame_fillna"": ""class Fillna:\n    def time_frame_fillna(self, inplace, method, dtype):\n        self.df.fillna(inplace=inplace, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Fillna:\n    def setup(self, inplace, method, dtype):\n        N, M = 10000, 100\n        if dtype in (\""datetime64[ns]\"", \""datetime64[ns, tz]\"", \""timedelta64[ns]\""):\n            data = {\n                \""datetime64[ns]\"": date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n                \""datetime64[ns, tz]\"": date_range(\n                    \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n                ),\n                \""timedelta64[ns]\"": timedelta_range(start=\""1 day\"", periods=N, freq=\""1D\""),\n            }\n            self.df = DataFrame({f\""col_{i}\"": data[dtype] for i in range(M)})\n            self.df[::2] = None\n        else:\n            values = np.random.randn(N, M)\n            values[::2] = np.nan\n            if dtype == \""Int64\"":\n                values = values.round()\n            self.df = DataFrame(values, dtype=dtype)""}",frame_methods.Fillna.time_frame_fillna,time,0.05083999225975597
3875,param-level,"terminus-2,gpt-5",1.0356180912954065,1.0415164232546907,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Duplicated.time_frame_duplicated_wide,frame_methods,"{""frame_methods.Duplicated.time_frame_duplicated_wide"": ""class Duplicated:\n    def time_frame_duplicated_wide(self):\n        self.df2.duplicated()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Duplicated:\n    def setup(self):\n        n = 1 << 20\n        t = date_range(\""2015-01-01\"", freq=\""S\"", periods=(n // 64))\n        xs = np.random.randn(n // 64).round(2)\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(-1 << 8, 1 << 8, n),\n                \""b\"": np.random.choice(t, n),\n                \""c\"": np.random.choice(xs, n),\n            }\n        )\n        self.df2 = DataFrame(np.random.randn(1000, 100).astype(str)).T""}",frame_methods.Duplicated.time_frame_duplicated_wide,time,-0.004171380452110496
3874,param-level,"terminus-2,gpt-5",1.1908681192596289,1.170278022390865,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Dropna.time_dropna_axis_mixed_dtypes,frame_methods,"{""frame_methods.Dropna.time_dropna_axis_mixed_dtypes"": ""class Dropna:\n    def time_dropna_axis_mixed_dtypes(self, how, axis):\n        self.df_mixed.dropna(how=how, axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Dropna:\n    def setup(self, how, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""""}",frame_methods.Dropna.time_dropna_axis_mixed_dtypes,time,0.014561596088234643
3883,param-level,"terminus-2,gpt-5",1.177381398721525,1.0188571417553025,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.ApplyNonUniqueUnsortedIndex.time_groupby_apply_non_unique_unsorted_index,groupby,"{""groupby.ApplyNonUniqueUnsortedIndex.time_groupby_apply_non_unique_unsorted_index"": ""class ApplyNonUniqueUnsortedIndex:\n    def time_groupby_apply_non_unique_unsorted_index(self):\n        self.df.groupby(\""key\"", group_keys=False).apply(lambda x: x)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ApplyNonUniqueUnsortedIndex:\n    def setup(self):\n        # GH 46527\n        # unsorted and non-unique index\n        idx = np.arange(100)[::-1]\n        idx = Index(np.repeat(idx, 200), name=\""key\"")\n        self.df = DataFrame(np.random.randn(len(idx), 10), index=idx)""}",groupby.ApplyNonUniqueUnsortedIndex.time_groupby_apply_non_unique_unsorted_index,time,0.11211050704824782
3879,param-level,"terminus-2,gpt-5",1.0116562576417258,1.1062177869730845,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Iteration.time_iterrows,frame_methods,"{""frame_methods.Iteration.time_iterrows"": ""class Iteration:\n    def time_iterrows(self):\n        for row in self.df.iterrows():\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))""}",frame_methods.Iteration.time_iterrows,time,-0.06687519754692973
3878,param-level,"terminus-2,gpt-5",1.0375599233103503,1.0356350267655572,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Iteration.time_items_cached,frame_methods,"{""frame_methods.Iteration.time_items_cached"": ""class Iteration:\n    def time_items_cached(self):\n        for name, col in self.df.items():\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))""}",frame_methods.Iteration.time_items_cached,time,0.0013613129736867813
3880,param-level,"terminus-2,gpt-5",1.1189741944212834,1.274450093142372,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.ToDict.time_to_dict_ints,frame_methods,"{""frame_methods.ToDict.time_to_dict_ints"": ""class ToDict:\n    def time_to_dict_ints(self, orient):\n        self.int_df.to_dict(orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToDict:\n    def setup(self, orient):\n        data = np.random.randint(0, 1000, size=(10000, 4))\n        self.int_df = DataFrame(data)\n        self.datetimelike_df = self.int_df.astype(\""timedelta64[ns]\"")""}",frame_methods.ToDict.time_to_dict_ints,time,-0.10995466670515451
3885,param-level,"terminus-2,gpt-5",1.0619004043364555,1.0764056266782285,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.GroupByMethods.time_dtype_as_field,groupby,"{""groupby.GroupByMethods.time_dtype_as_field"": ""class GroupByMethods:\n    def time_dtype_as_field(self, dtype, method, application, ncols):\n        self.as_field_method()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupByMethods:\n    def setup(self, dtype, method, application, ncols):\n        if method in method_blocklist.get(dtype, {}):\n            raise NotImplementedError  # skip benchmark\n    \n        if ncols != 1 and method in [\""value_counts\"", \""unique\""]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if application == \""transformation\"" and method in [\n            \""describe\"",\n            \""head\"",\n            \""tail\"",\n            \""unique\"",\n            \""value_counts\"",\n            \""size\"",\n        ]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if method == \""describe\"":\n            ngroups = 20\n        elif method == \""skew\"":\n            ngroups = 100\n        else:\n            ngroups = 1000\n        size = ngroups * 2\n        rng = np.arange(ngroups).reshape(-1, 1)\n        rng = np.broadcast_to(rng, (len(rng), ncols))\n        taker = np.random.randint(0, ngroups, size=size)\n        values = rng.take(taker, axis=0)\n        if dtype == \""int\"":\n            key = np.random.randint(0, size, size=size)\n        elif dtype in (\""int16\"", \""uint\""):\n            key = np.random.randint(0, size, size=size, dtype=dtype)\n        elif dtype == \""float\"":\n            key = np.concatenate(\n                [np.random.random(ngroups) * 0.1, np.random.random(ngroups) * 10.0]\n            )\n        elif dtype == \""object\"":\n            key = [\""foo\""] * size\n        elif dtype == \""datetime\"":\n            key = date_range(\""1/1/2011\"", periods=size, freq=\""s\"")\n    \n        cols = [f\""values{n}\"" for n in range(ncols)]\n        df = DataFrame(values, columns=cols)\n        df[\""key\""] = key\n    \n        if len(cols) == 1:\n            cols = cols[0]\n    \n        if application == \""transformation\"":\n            self.as_group_method = lambda: df.groupby(\""key\"")[cols].transform(method)\n            self.as_field_method = lambda: df.groupby(cols)[\""key\""].transform(method)\n        else:\n            self.as_group_method = getattr(df.groupby(\""key\"")[cols], method)\n            self.as_field_method = getattr(df.groupby(cols)[\""key\""], method)""}",groupby.GroupByMethods.time_dtype_as_field,time,-0.01025829019927368
3876,param-level,"terminus-2,gpt-5",1.7318344749928167,1.5831426169002387,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Equals.time_frame_float_equal,frame_methods,"{""frame_methods.Equals.time_frame_float_equal"": ""class Equals:\n    def time_frame_float_equal(self):\n        self.float_df.equals(self.float_df)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Equals:\n    def setup(self):\n        N = 10**3\n        self.float_df = DataFrame(np.random.randn(N, N))\n        self.float_df_nan = self.float_df.copy()\n        self.float_df_nan.iloc[-1, -1] = np.nan\n    \n        self.object_df = DataFrame(\""foo\"", index=range(N), columns=range(N))\n        self.object_df_nan = self.object_df.copy()\n        self.object_df_nan.iloc[-1, -1] = np.nan\n    \n        self.nonunique_cols = self.object_df.copy()\n        self.nonunique_cols.columns = [\""A\""] * len(self.nonunique_cols.columns)\n        self.nonunique_cols_nan = self.nonunique_cols.copy()\n        self.nonunique_cols_nan.iloc[-1, -1] = np.nan""}",frame_methods.Equals.time_frame_float_equal,time,0.10515690105557145
3887,param-level,"terminus-2,gpt-5",1.0521344820479284,0.7453157545798069,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.RankWithTies.time_rank_ties,groupby,"{""groupby.RankWithTies.time_rank_ties"": ""class RankWithTies:\n    def time_rank_ties(self, dtype, tie_method):\n        self.df.groupby(\""key\"").rank(method=tie_method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass RankWithTies:\n    def setup(self, dtype, tie_method):\n        N = 10**4\n        if dtype == \""datetime64\"":\n            data = np.array([Timestamp(\""2011/01/01\"")] * N, dtype=dtype)\n        else:\n            data = np.array([1] * N, dtype=dtype)\n        self.df = DataFrame({\""values\"": data, \""key\"": [\""foo\""] * N})""}",groupby.RankWithTies.time_rank_ties,time,0.2169863702037634
3895,param-level,"terminus-2,gpt-5",1.0444604293552595,1.0263895534781546,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_object.Indexing.time_get_loc_sorted,index_object,"{""index_object.Indexing.time_get_loc_sorted"": ""class Indexing:\n    def time_get_loc_sorted(self, dtype):\n        self.sorted.get_loc(self.key)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self, dtype):\n        N = 10**6\n        self.idx = getattr(tm, f\""make{dtype}Index\"")(N)\n        self.array_mask = (np.arange(N) % 3) == 0\n        self.series_mask = Series(self.array_mask)\n        self.sorted = self.idx.sort_values()\n        half = N // 2\n        self.non_unique = self.idx[:half].append(self.idx[:half])\n        self.non_unique_sorted = (\n            self.sorted[:half].append(self.sorted[:half]).sort_values()\n        )\n        self.key = self.sorted[N // 4]""}",index_object.Indexing.time_get_loc_sorted,time,0.012779968795689446
3890,param-level,"terminus-2,gpt-5",1.0902066194515143,1.0353260541593894,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,hash_functions.NumericSeriesIndexing.time_loc_slice,hash_functions,"{""hash_functions.NumericSeriesIndexing.time_loc_slice"": ""class NumericSeriesIndexing:\n    def time_loc_slice(self, index, N):\n        # trigger building of mapping\n        self.data.loc[:800]\n\n    def setup(self, dtype, N):\n        vals = np.array(list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype)\n        indices = pd.Index(vals)\n        self.data = pd.Series(np.arange(N), index=indices)""}",hash_functions.NumericSeriesIndexing.time_loc_slice,time,0.03881228097038538
3886,param-level,"terminus-2,gpt-5",1.07488800404684,1.0647825340230623,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.GroupByMethods.time_dtype_as_group,groupby,"{""groupby.GroupByMethods.time_dtype_as_group"": ""class GroupByMethods:\n    def time_dtype_as_group(self, dtype, method, application, ncols):\n        self.as_group_method()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupByMethods:\n    def setup(self, dtype, method, application, ncols):\n        if method in method_blocklist.get(dtype, {}):\n            raise NotImplementedError  # skip benchmark\n    \n        if ncols != 1 and method in [\""value_counts\"", \""unique\""]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if application == \""transformation\"" and method in [\n            \""describe\"",\n            \""head\"",\n            \""tail\"",\n            \""unique\"",\n            \""value_counts\"",\n            \""size\"",\n        ]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if method == \""describe\"":\n            ngroups = 20\n        elif method == \""skew\"":\n            ngroups = 100\n        else:\n            ngroups = 1000\n        size = ngroups * 2\n        rng = np.arange(ngroups).reshape(-1, 1)\n        rng = np.broadcast_to(rng, (len(rng), ncols))\n        taker = np.random.randint(0, ngroups, size=size)\n        values = rng.take(taker, axis=0)\n        if dtype == \""int\"":\n            key = np.random.randint(0, size, size=size)\n        elif dtype in (\""int16\"", \""uint\""):\n            key = np.random.randint(0, size, size=size, dtype=dtype)\n        elif dtype == \""float\"":\n            key = np.concatenate(\n                [np.random.random(ngroups) * 0.1, np.random.random(ngroups) * 10.0]\n            )\n        elif dtype == \""object\"":\n            key = [\""foo\""] * size\n        elif dtype == \""datetime\"":\n            key = date_range(\""1/1/2011\"", periods=size, freq=\""s\"")\n    \n        cols = [f\""values{n}\"" for n in range(ncols)]\n        df = DataFrame(values, columns=cols)\n        df[\""key\""] = key\n    \n        if len(cols) == 1:\n            cols = cols[0]\n    \n        if application == \""transformation\"":\n            self.as_group_method = lambda: df.groupby(\""key\"")[cols].transform(method)\n            self.as_field_method = lambda: df.groupby(cols)[\""key\""].transform(method)\n        else:\n            self.as_group_method = getattr(df.groupby(\""key\"")[cols], method)\n            self.as_field_method = getattr(df.groupby(cols)[\""key\""], method)""}",groupby.GroupByMethods.time_dtype_as_group,time,0.007146725617947459
3889,param-level,"terminus-2,gpt-5",1.047004930851596,1.042586536069814,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.Transform.time_transform_lambda_max_wide,groupby,"{""groupby.Transform.time_transform_lambda_max_wide"": ""class Transform:\n    def time_transform_lambda_max_wide(self):\n        self.df_wide.groupby(level=0).transform(lambda x: np.max(x, axis=0))\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]""}",groupby.Transform.time_transform_lambda_max_wide,time,0.0031247487848528746
3901,param-level,"terminus-2,gpt-5",1.1234814963034754,1.052174150069658,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.MultiIndexing.time_loc_all_scalars,indexing,"{""indexing.MultiIndexing.time_loc_all_scalars"": ""class MultiIndexing:\n    def time_loc_all_scalars(self, unique_levels):\n        target = tuple([self.tgt_scalar] * self.nlevels)\n        self.df.loc[target, :]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MultiIndexing:\n    def setup(self, unique_levels):\n        self.nlevels = 2\n        if unique_levels:\n            mi = MultiIndex.from_arrays([range(1000000)] * self.nlevels)\n        else:\n            mi = MultiIndex.from_product([range(1000)] * self.nlevels)\n        self.df = DataFrame(np.random.randn(len(mi)), index=mi)\n    \n        self.tgt_slice = slice(200, 800)\n        self.tgt_null_slice = slice(None)\n        self.tgt_list = list(range(0, 1000, 10))\n        self.tgt_scalar = 500\n    \n        bool_indexer = np.zeros(len(mi), dtype=np.bool_)\n        bool_indexer[slice(0, len(mi), 100)] = True\n        self.tgt_bool_indexer = bool_indexer""}",indexing.MultiIndexing.time_loc_all_scalars,time,0.05042952350340694
3897,param-level,"terminus-2,gpt-5",0.9893410544612726,0.9875688486727868,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.CategoricalIndexIndexing.time_getitem_scalar,indexing,"{""indexing.CategoricalIndexIndexing.time_getitem_scalar"": ""class CategoricalIndexIndexing:\n    def time_getitem_scalar(self, index):\n        self.data[self.int_scalar]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalIndexIndexing:\n    def setup(self, index):\n        N = 10**5\n        values = list(\""a\"" * N + \""b\"" * N + \""c\"" * N)\n        indices = {\n            \""monotonic_incr\"": CategoricalIndex(values),\n            \""monotonic_decr\"": CategoricalIndex(reversed(values)),\n            \""non_monotonic\"": CategoricalIndex(list(\""abc\"" * N)),\n        }\n        self.data = indices[index]\n        self.data_unique = CategoricalIndex([str(i) for i in range(N * 3)])\n    \n        self.int_scalar = 10000\n        self.int_list = list(range(10000))\n    \n        self.cat_scalar = \""b\""\n        self.cat_list = [\""1\"", \""3\""]""}",indexing.CategoricalIndexIndexing.time_getitem_scalar,time,0.0012533279975147126
3894,param-level,"terminus-2,gpt-5",1.1488001500199212,1.0220004458551766,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_cached_properties.IndexCache.time_is_monotonic_increasing,index_cached_properties,"{""index_cached_properties.IndexCache.time_is_monotonic_increasing"": ""class IndexCache:\n    def time_is_monotonic_increasing(self, index_type):\n        self.idx.is_monotonic_increasing\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}""}",index_cached_properties.IndexCache.time_is_monotonic_increasing,time,0.08967447253518007
3884,param-level,"terminus-2,gpt-5",1.0701570395163813,1.070948001002442,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.Cumulative.time_frame_transform,groupby,"{""groupby.Cumulative.time_frame_transform"": ""class Cumulative:\n    def time_frame_transform(self, dtype, method, with_nans):\n        self.df.groupby(\""key\"").transform(method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cumulative:\n    def setup(self, dtype, method, with_nans):\n        if with_nans and dtype == \""int64\"":\n            raise NotImplementedError(\""Construction of df would raise\"")\n    \n        N = 500_000\n        keys = np.random.randint(0, 100, size=N)\n        vals = np.random.randint(-10, 10, (N, 5))\n    \n        if with_nans:\n            null_vals = vals.astype(float, copy=True)\n            null_vals[::2, :] = np.nan\n            null_vals[::3, :] = np.nan\n            df = DataFrame(null_vals, columns=list(\""abcde\""), dtype=dtype)\n            df[\""key\""] = keys\n            self.df = df\n        else:\n            df = DataFrame(vals, columns=list(\""abcde\"")).astype(dtype, copy=False)\n            df[\""key\""] = keys\n            self.df = df""}",groupby.Cumulative.time_frame_transform,time,-0.0005593787030131648
3899,param-level,"terminus-2,gpt-5",1.0189707786432889,1.038504163552764,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.IndexSingleRow.time_loc_row,indexing,"{""indexing.IndexSingleRow.time_loc_row"": ""class IndexSingleRow:\n    def time_loc_row(self, unique_cols):\n        self.df.loc[10000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IndexSingleRow:\n    def setup(self, unique_cols):\n        arr = np.arange(10**7).reshape(-1, 10)\n        df = DataFrame(arr)\n        dtypes = [\""u1\"", \""u2\"", \""u4\"", \""u8\"", \""i1\"", \""i2\"", \""i4\"", \""i8\"", \""f8\"", \""f4\""]\n        for i, d in enumerate(dtypes):\n            df[i] = df[i].astype(d)\n    \n        if not unique_cols:\n            # GH#33032 single-row lookups with non-unique columns were\n            #  15x slower than with unique columns\n            df.columns = [\""A\"", \""A\""] + list(df.columns[2:])\n    \n        self.df = df""}",indexing.IndexSingleRow.time_loc_row,time,-0.013814275042061673
3892,param-level,"terminus-2,gpt-5",1.276018616164088,1.0398100956535965,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_cached_properties.IndexCache.time_engine,index_cached_properties,"{""index_cached_properties.IndexCache.time_engine"": ""class IndexCache:\n    def time_engine(self, index_type):\n        self.idx._engine\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}""}",index_cached_properties.IndexCache.time_engine,time,0.16704987306258245
3898,param-level,"terminus-2,gpt-5",1.0684170052273432,1.0333263660754053,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.DatetimeIndexIndexing.time_get_indexer_mismatched_tz,indexing,"{""indexing.DatetimeIndexIndexing.time_get_indexer_mismatched_tz"": ""class DatetimeIndexIndexing:\n    def time_get_indexer_mismatched_tz(self):\n        # reached via e.g.\n        #  ser = Series(range(len(dti)), index=dti)\n        #  ser[dti2]\n        self.dti.get_indexer(self.dti2)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndexIndexing:\n    def setup(self):\n        dti = date_range(\""2016-01-01\"", periods=10000, tz=\""US/Pacific\"")\n        dti2 = dti.tz_convert(\""UTC\"")\n        self.dti = dti\n        self.dti2 = dti2""}",indexing.DatetimeIndexIndexing.time_get_indexer_mismatched_tz,time,0.024816576486519016
3893,param-level,"terminus-2,gpt-5",1.108824880447787,1.0838227693246487,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_cached_properties.IndexCache.time_is_monotonic_decreasing,index_cached_properties,"{""index_cached_properties.IndexCache.time_is_monotonic_decreasing"": ""class IndexCache:\n    def time_is_monotonic_decreasing(self, index_type):\n        self.idx.is_monotonic_decreasing\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}""}",index_cached_properties.IndexCache.time_is_monotonic_decreasing,time,0.017681832477466915
3891,param-level,"terminus-2,gpt-5",1.1057859716008107,1.1141947051174346,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,hash_functions.UniqueAndFactorizeArange.time_factorize,hash_functions,"{""hash_functions.UniqueAndFactorizeArange.time_factorize"": ""class UniqueAndFactorizeArange:\n    def time_factorize(self, exponent):\n        pd.factorize(self.a2)\n\n    def setup(self, exponent):\n        a = np.arange(10**4, dtype=\""float64\"")\n        self.a2 = (a + 10**exponent).repeat(100)""}",hash_functions.UniqueAndFactorizeArange.time_factorize,time,-0.005946770520950394
3902,param-level,"terminus-2,gpt-5",1.0484895577529203,1.0473046935586134,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NonNumericSeriesIndexing.time_getitem_label_slice,indexing,"{""indexing.NonNumericSeriesIndexing.time_getitem_label_slice"": ""class NonNumericSeriesIndexing:\n    def time_getitem_label_slice(self, index, index_structure):\n        self.s[: self.lbl]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]""}",indexing.NonNumericSeriesIndexing.time_getitem_label_slice,time,0.000837952046893109
3896,param-level,"terminus-2,gpt-5",1.273250890439918,1.16908494765572,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_object.SetOperations.time_operation,index_object,"{""index_object.SetOperations.time_operation"": ""class SetOperations:\n    def time_operation(self, index_structure, dtype, method):\n        getattr(self.left, method)(self.right)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SetOperations:\n    def setup(self, index_structure, dtype, method):\n        N = 10**5\n        dates_left = date_range(\""1/1/2000\"", periods=N, freq=\""T\"")\n        fmt = \""%Y-%m-%d %H:%M:%S\""\n        date_str_left = Index(dates_left.strftime(fmt))\n        int_left = Index(np.arange(N))\n        ea_int_left = Index(np.arange(N), dtype=\""Int64\"")\n        str_left = tm.makeStringIndex(N)\n    \n        data = {\n            \""datetime\"": dates_left,\n            \""date_string\"": date_str_left,\n            \""int\"": int_left,\n            \""strings\"": str_left,\n            \""ea_int\"": ea_int_left,\n        }\n    \n        if index_structure == \""non_monotonic\"":\n            data = {k: mi[::-1] for k, mi in data.items()}\n    \n        data = {k: {\""left\"": idx, \""right\"": idx[:-1]} for k, idx in data.items()}\n    \n        self.left = data[dtype][\""left\""]\n        self.right = data[dtype][\""right\""]""}",index_object.SetOperations.time_operation,time,0.07366756915431266
3904,param-level,"terminus-2,gpt-5",0.9624885172942552,0.9622641767121336,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NonNumericSeriesIndexing.time_getitem_scalar,indexing,"{""indexing.NonNumericSeriesIndexing.time_getitem_scalar"": ""class NonNumericSeriesIndexing:\n    def time_getitem_scalar(self, index, index_structure):\n        self.s[self.lbl]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]""}",indexing.NonNumericSeriesIndexing.time_getitem_scalar,time,0.0001586567058851614
3908,param-level,"terminus-2,gpt-5",1.305740203732204,1.3297764991995344,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericSeriesIndexing.time_getitem_list_like,indexing,"{""indexing.NumericSeriesIndexing.time_getitem_list_like"": ""class NumericSeriesIndexing:\n    def time_getitem_list_like(self, index, index_structure):\n        self.data[[800000]]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.NumericSeriesIndexing.time_getitem_list_like,time,-0.016998794531351166
3900,param-level,"terminus-2,gpt-5",1.2339709997730812,1.061558771647169,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.MultiIndexing.time_loc_all_null_slices,indexing,"{""indexing.MultiIndexing.time_loc_all_null_slices"": ""class MultiIndexing:\n    def time_loc_all_null_slices(self, unique_levels):\n        target = tuple([self.tgt_null_slice] * self.nlevels)\n        self.df.loc[target, :]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MultiIndexing:\n    def setup(self, unique_levels):\n        self.nlevels = 2\n        if unique_levels:\n            mi = MultiIndex.from_arrays([range(1000000)] * self.nlevels)\n        else:\n            mi = MultiIndex.from_product([range(1000)] * self.nlevels)\n        self.df = DataFrame(np.random.randn(len(mi)), index=mi)\n    \n        self.tgt_slice = slice(200, 800)\n        self.tgt_null_slice = slice(None)\n        self.tgt_list = list(range(0, 1000, 10))\n        self.tgt_scalar = 500\n    \n        bool_indexer = np.zeros(len(mi), dtype=np.bool_)\n        bool_indexer[slice(0, len(mi), 100)] = True\n        self.tgt_bool_indexer = bool_indexer""}",indexing.MultiIndexing.time_loc_all_null_slices,time,0.12193226883020664
3907,param-level,"terminus-2,gpt-5",1.0952383674801311,1.0708721027634038,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericSeriesIndexing.time_getitem_array,indexing,"{""indexing.NumericSeriesIndexing.time_getitem_array"": ""class NumericSeriesIndexing:\n    def time_getitem_array(self, index, index_structure):\n        self.data[self.array]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.NumericSeriesIndexing.time_getitem_array,time,0.017232153265012245
3903,param-level,"terminus-2,gpt-5",1.0608290433766288,1.064515417167363,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NonNumericSeriesIndexing.time_getitem_list_like,indexing,"{""indexing.NonNumericSeriesIndexing.time_getitem_list_like"": ""class NonNumericSeriesIndexing:\n    def time_getitem_list_like(self, index, index_structure):\n        self.s[[self.lbl]]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]""}",indexing.NonNumericSeriesIndexing.time_getitem_list_like,time,-0.0026070536002363656
3912,param-level,"terminus-2,gpt-5",1.118500458692621,1.092596161613923,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericSeriesIndexing.time_loc_array,indexing,"{""indexing.NumericSeriesIndexing.time_loc_array"": ""class NumericSeriesIndexing:\n    def time_loc_array(self, index, index_structure):\n        self.data.loc[self.array]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.NumericSeriesIndexing.time_loc_array,time,0.018319870635571438
3909,param-level,"terminus-2,gpt-5",1.2361869970793182,1.208053664760974,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericSeriesIndexing.time_getitem_lists,indexing,"{""indexing.NumericSeriesIndexing.time_getitem_lists"": ""class NumericSeriesIndexing:\n    def time_getitem_lists(self, index, index_structure):\n        self.data[self.array_list]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.NumericSeriesIndexing.time_getitem_lists,time,0.01989627462400576
3910,param-level,"terminus-2,gpt-5",1.176844611240668,1.1699685577784924,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericSeriesIndexing.time_getitem_scalar,indexing,"{""indexing.NumericSeriesIndexing.time_getitem_scalar"": ""class NumericSeriesIndexing:\n    def time_getitem_scalar(self, index, index_structure):\n        self.data[800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.NumericSeriesIndexing.time_getitem_scalar,time,0.00486283837494744
3905,param-level,"terminus-2,gpt-5",1.2642587347976593,1.2617980705467426,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericMaskedIndexing.time_get_indexer,indexing,"{""indexing.NumericMaskedIndexing.time_get_indexer"": ""class NumericMaskedIndexing:\n    def time_get_indexer(self, dtype, monotonic):\n        self.data.get_indexer(self.indexer)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericMaskedIndexing:\n    def setup(self, dtype, monotonic):\n        indices = {\n            True: Index(self.monotonic_list, dtype=dtype),\n            False: Index(self.non_monotonic_list, dtype=dtype).append(\n                Index([NA], dtype=dtype)\n            ),\n        }\n        self.data = indices[monotonic]\n        self.indexer = np.arange(300, 1_000)\n        self.data_dups = self.data.append(self.data)""}",indexing.NumericMaskedIndexing.time_get_indexer,time,0.0017402151703795065
3911,param-level,"terminus-2,gpt-5",1.2689658857262036,1.2623581032741231,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericSeriesIndexing.time_getitem_slice,indexing,"{""indexing.NumericSeriesIndexing.time_getitem_slice"": ""class NumericSeriesIndexing:\n    def time_getitem_slice(self, index, index_structure):\n        self.data[:800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.NumericSeriesIndexing.time_getitem_slice,time,0.004673113473890022
3920,param-level,"terminus-2,gpt-5",1.2186384323505546,1.1630325669190456,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.hdf.HDFStoreDataFrame.time_query_store_table,io.hdf,"{""io.hdf.HDFStoreDataFrame.time_query_store_table"": ""class HDFStoreDataFrame:\n    def time_query_store_table(self):\n        self.store.select(\""table\"", where=\""index > self.start and index < self.stop\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass HDFStoreDataFrame:\n    def setup(self):\n        N = 25000\n        index = tm.makeStringIndex(N)\n        self.df = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)}, index=index\n        )\n        self.df_mixed = DataFrame(\n            {\n                \""float1\"": np.random.randn(N),\n                \""float2\"": np.random.randn(N),\n                \""string1\"": [\""foo\""] * N,\n                \""bool1\"": [True] * N,\n                \""int1\"": np.random.randint(0, N, size=N),\n            },\n            index=index,\n        )\n        self.df_wide = DataFrame(np.random.randn(N, 100))\n        self.start_wide = self.df_wide.index[10000]\n        self.stop_wide = self.df_wide.index[15000]\n        self.df2 = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)},\n            index=date_range(\""1/1/2000\"", periods=N),\n        )\n        self.start = self.df2.index[10000]\n        self.stop = self.df2.index[15000]\n        self.df_wide2 = DataFrame(\n            np.random.randn(N, 100), index=date_range(\""1/1/2000\"", periods=N)\n        )\n        self.df_dc = DataFrame(\n            np.random.randn(N, 10), columns=[f\""C{i:03d}\"" for i in range(10)]\n        )\n    \n        self.fname = \""__test__.h5\""\n    \n        self.store = HDFStore(self.fname)\n        self.store.put(\""fixed\"", self.df)\n        self.store.put(\""fixed_mixed\"", self.df_mixed)\n        self.store.append(\""table\"", self.df2)\n        self.store.append(\""table_mixed\"", self.df_mixed)\n        self.store.append(\""table_wide\"", self.df_wide)\n        self.store.append(\""table_wide2\"", self.df_wide2)""}",io.hdf.HDFStoreDataFrame.time_query_store_table,time,0.03932522307744627
3906,param-level,"terminus-2,gpt-5",1.2294133914530463,1.2018416031412138,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericMaskedIndexing.time_get_indexer_dups,indexing,"{""indexing.NumericMaskedIndexing.time_get_indexer_dups"": ""class NumericMaskedIndexing:\n    def time_get_indexer_dups(self, dtype, monotonic):\n        self.data.get_indexer_for(self.indexer)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericMaskedIndexing:\n    def setup(self, dtype, monotonic):\n        indices = {\n            True: Index(self.monotonic_list, dtype=dtype),\n            False: Index(self.non_monotonic_list, dtype=dtype).append(\n                Index([NA], dtype=dtype)\n            ),\n        }\n        self.data = indices[monotonic]\n        self.indexer = np.arange(300, 1_000)\n        self.data_dups = self.data.append(self.data)""}",indexing.NumericMaskedIndexing.time_get_indexer_dups,time,0.01949914307767503
3918,param-level,"terminus-2,gpt-5",1.0656415947558855,1.0503761610002444,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.csv.ReadCSVEngine.time_read_bytescsv,io.csv,"{""io.csv.ReadCSVEngine.time_read_bytescsv"": ""class ReadCSVEngine:\n    def time_read_bytescsv(self, engine):\n        read_csv(self.data(self.BytesIO_input), engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVEngine:\n    def setup(self, engine):\n        data = [\""A,B,C,D,E\""] + ([\""1,2,3,4,5\""] * 100000)\n        self.StringIO_input = StringIO(\""\\n\"".join(data))\n        # simulate reading from file\n        self.BytesIO_input = BytesIO(self.StringIO_input.read().encode(\""utf-8\""))""}",io.csv.ReadCSVEngine.time_read_bytescsv,time,0.010795922033692394
3914,param-level,"terminus-2,gpt-5",1.1001082482015132,1.090993811669739,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing_engines.MaskedNumericEngineIndexing.time_get_loc,indexing_engines,"{""indexing_engines.MaskedNumericEngineIndexing.time_get_loc"": ""class MaskedNumericEngineIndexing:\n    def time_get_loc(self, engine_and_dtype, index_type, unique, N):\n        self.data.get_loc(self.key_early)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())[::-1]\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.zeros(N * 3, dtype=dtype.lower())\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype.lower())\n                arr[N:] = np.arange(N * 2, dtype=dtype.lower())\n    \n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n            mask[-1] = True\n    \n        self.data = engine(BaseMaskedArray(arr, mask))\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]""}",indexing_engines.MaskedNumericEngineIndexing.time_get_loc,time,0.006445853275653556
3916,param-level,"terminus-2,gpt-5",1.0496370833544018,1.0426701947055392,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing_engines.NumericEngineIndexing.time_get_loc,indexing_engines,"{""indexing_engines.NumericEngineIndexing.time_get_loc"": ""class NumericEngineIndexing:\n    def time_get_loc(self, engine_and_dtype, index_type, unique, N):\n        self.data.get_loc(self.key_early)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)[::-1]\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.empty(N * 3, dtype=dtype)\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype)\n                arr[N:] = np.arange(N * 2, dtype=dtype)\n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype)\n    \n        self.data = engine(arr)\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]""}",indexing_engines.NumericEngineIndexing.time_get_loc,time,0.004927078252378023
3913,param-level,"terminus-2,gpt-5",1.0557091446616276,1.0881409032083509,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericSeriesIndexing.time_loc_slice,indexing,"{""indexing.NumericSeriesIndexing.time_loc_slice"": ""class NumericSeriesIndexing:\n    def time_loc_slice(self, index, index_structure):\n        self.data.loc[:800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.NumericSeriesIndexing.time_loc_slice,time,-0.022936180018899088
3917,param-level,"terminus-2,gpt-5",1.1446884577534786,1.121376298596207,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.csv.ReadCSVCategorical.time_convert_post,io.csv,"{""io.csv.ReadCSVCategorical.time_convert_post"": ""class ReadCSVCategorical:\n    def time_convert_post(self, engine):\n        read_csv(self.fname, engine=engine).apply(Categorical)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVCategorical:\n    def setup(self, engine):\n        N = 100000\n        group1 = [\""aaaaaaaa\"", \""bbbbbbb\"", \""cccccccc\"", \""dddddddd\"", \""eeeeeeee\""]\n        df = DataFrame(np.random.choice(group1, (N, 3)), columns=list(\""abc\""))\n        df.to_csv(self.fname, index=False)""}",io.csv.ReadCSVCategorical.time_convert_post,time,0.016486675500192068
3919,param-level,"terminus-2,gpt-5",1.0571727132763542,1.03767982340548,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.csv.ReadCSVEngine.time_read_stringcsv,io.csv,"{""io.csv.ReadCSVEngine.time_read_stringcsv"": ""class ReadCSVEngine:\n    def time_read_stringcsv(self, engine):\n        read_csv(self.data(self.StringIO_input), engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVEngine:\n    def setup(self, engine):\n        data = [\""A,B,C,D,E\""] + ([\""1,2,3,4,5\""] * 100000)\n        self.StringIO_input = StringIO(\""\\n\"".join(data))\n        # simulate reading from file\n        self.BytesIO_input = BytesIO(self.StringIO_input.read().encode(\""utf-8\""))""}",io.csv.ReadCSVEngine.time_read_stringcsv,time,0.013785636400901106
3915,param-level,"terminus-2,gpt-5",1.0735450775542064,1.0153873906209916,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing_engines.MaskedNumericEngineIndexing.time_get_loc_near_middle,indexing_engines,"{""indexing_engines.MaskedNumericEngineIndexing.time_get_loc_near_middle"": ""class MaskedNumericEngineIndexing:\n    def time_get_loc_near_middle(self, engine_and_dtype, index_type, unique, N):\n        # searchsorted performance may be different near the middle of a range\n        #  vs near an endpoint\n        self.data.get_loc(self.key_middle)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())[::-1]\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.zeros(N * 3, dtype=dtype.lower())\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype.lower())\n                arr[N:] = np.arange(N * 2, dtype=dtype.lower())\n    \n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n            mask[-1] = True\n    \n        self.data = engine(BaseMaskedArray(arr, mask))\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]""}",indexing_engines.MaskedNumericEngineIndexing.time_get_loc_near_middle,time,0.04112990589336264
3921,param-level,"terminus-2,gpt-5",1.0318322167913276,1.0532229372599846,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.json.NormalizeJSON.time_normalize_json,io.json,"{""io.json.NormalizeJSON.time_normalize_json"": ""class NormalizeJSON:\n    def time_normalize_json(self, orient, frame):\n        json_normalize(self.data)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NormalizeJSON:\n    def setup(self, orient, frame):\n        data = {\n            \""hello\"": [\""thisisatest\"", 999898, \""mixed types\""],\n            \""nest1\"": {\""nest2\"": {\""nest3\"": \""nest3_value\"", \""nest3_int\"": 3445}},\n            \""nest1_list\"": {\""nest2\"": [\""blah\"", 32423, 546456.876, 92030234]},\n            \""hello2\"": \""string\"",\n        }\n        self.data = [data for i in range(10000)]""}",io.json.NormalizeJSON.time_normalize_json,time,-0.015127807969347226
3926,param-level,"terminus-2,gpt-5",1.0545492694538523,1.0472118667839048,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.sql.WriteSQLDtypes.time_read_sql_query_select_column,io.sql,"{""io.sql.WriteSQLDtypes.time_read_sql_query_select_column"": ""class WriteSQLDtypes:\n    def time_read_sql_query_select_column(self, connection, dtype):\n        read_sql_query(self.query_col, self.con)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass WriteSQLDtypes:\n    def setup(self, connection, dtype):\n        N = 10000\n        con = {\n            \""sqlalchemy\"": create_engine(\""sqlite:///:memory:\""),\n            \""sqlite\"": sqlite3.connect(\"":memory:\""),\n        }\n        self.table_name = \""test_type\""\n        self.query_col = f\""SELECT {dtype} FROM {self.table_name}\""\n        self.con = con[connection]\n        self.df = DataFrame(\n            {\n                \""float\"": np.random.randn(N),\n                \""float_with_nan\"": np.random.randn(N),\n                \""string\"": [\""foo\""] * N,\n                \""bool\"": [True] * N,\n                \""int\"": np.random.randint(0, N, size=N),\n                \""datetime\"": date_range(\""2000-01-01\"", periods=N, freq=\""s\""),\n            },\n            index=tm.makeStringIndex(N),\n        )\n        self.df.iloc[1000:3000, 1] = np.nan\n        self.df[\""date\""] = self.df[\""datetime\""].dt.date\n        self.df[\""time\""] = self.df[\""datetime\""].dt.time\n        self.df[\""datetime_string\""] = self.df[\""datetime\""].astype(str)\n        self.df.to_sql(self.table_name, self.con, if_exists=\""replace\"")""}",io.sql.WriteSQLDtypes.time_read_sql_query_select_column,time,0.0051891107991142394
3923,param-level,"terminus-2,gpt-5",1.034878133189342,1.0151594714925496,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.json.ToJSONLines.time_delta_int_tstamp_lines,io.json,"{""io.json.ToJSONLines.time_delta_int_tstamp_lines"": ""class ToJSONLines:\n    def time_delta_int_tstamp_lines(self):\n        self.df_td_int_ts.to_json(self.fname, orient=\""records\"", lines=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSONLines:\n    def setup(self):\n        N = 10**5\n        ncols = 5\n        index = date_range(\""20000101\"", periods=N, freq=\""H\"")\n        timedeltas = timedelta_range(start=1, periods=N, freq=\""s\"")\n        datetimes = date_range(start=1, periods=N, freq=\""s\"")\n        ints = np.random.randint(100000000, size=N)\n        longints = sys.maxsize * np.random.randint(100000000, size=N)\n        floats = np.random.randn(N)\n        strings = tm.makeStringIndex(N)\n        self.df = DataFrame(np.random.randn(N, ncols), index=np.arange(N))\n        self.df_date_idx = DataFrame(np.random.randn(N, ncols), index=index)\n        self.df_td_int_ts = DataFrame(\n            {\n                \""td_1\"": timedeltas,\n                \""td_2\"": timedeltas,\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""ts_1\"": datetimes,\n                \""ts_2\"": datetimes,\n            },\n            index=index,\n        )\n        self.df_int_floats = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""int_3\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""float_3\"": floats,\n            },\n            index=index,\n        )\n        self.df_int_float_str = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )\n        self.df_longint_float_str = DataFrame(\n            {\n                \""longint_1\"": longints,\n                \""longint_2\"": longints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )""}",io.json.ToJSONLines.time_delta_int_tstamp_lines,time,0.013945305301833362
3927,param-level,"terminus-2,gpt-5",1.0612458037563528,1.0651493160553005,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,join_merge.MergeAsof.time_on_int32,join_merge,"{""join_merge.MergeAsof.time_on_int32"": ""class MergeAsof:\n    def time_on_int32(self, direction, tolerance):\n        merge_asof(\n            self.df1d, self.df2d, on=\""time32\"", direction=direction, tolerance=tolerance\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MergeAsof:\n    def setup(self, direction, tolerance):\n        one_count = 200000\n        two_count = 1000000\n    \n        df1 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, one_count / 20, one_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), one_count),\n                \""key2\"": np.random.randint(0, 25, one_count),\n                \""value1\"": np.random.randn(one_count),\n            }\n        )\n        df2 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, two_count / 20, two_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), two_count),\n                \""key2\"": np.random.randint(0, 25, two_count),\n                \""value2\"": np.random.randn(two_count),\n            }\n        )\n    \n        df1 = df1.sort_values(\""time\"")\n        df2 = df2.sort_values(\""time\"")\n    \n        df1[\""time32\""] = np.int32(df1.time)\n        df2[\""time32\""] = np.int32(df2.time)\n    \n        df1[\""timeu64\""] = np.uint64(df1.time)\n        df2[\""timeu64\""] = np.uint64(df2.time)\n    \n        self.df1a = df1[[\""time\"", \""value1\""]]\n        self.df2a = df2[[\""time\"", \""value2\""]]\n        self.df1b = df1[[\""time\"", \""key\"", \""value1\""]]\n        self.df2b = df2[[\""time\"", \""key\"", \""value2\""]]\n        self.df1c = df1[[\""time\"", \""key2\"", \""value1\""]]\n        self.df2c = df2[[\""time\"", \""key2\"", \""value2\""]]\n        self.df1d = df1[[\""time32\"", \""value1\""]]\n        self.df2d = df2[[\""time32\"", \""value2\""]]\n        self.df1e = df1[[\""time\"", \""key\"", \""key2\"", \""value1\""]]\n        self.df2e = df2[[\""time\"", \""key\"", \""key2\"", \""value2\""]]\n        self.df1f = df1[[\""timeu64\"", \""value1\""]]\n        self.df2f = df2[[\""timeu64\"", \""value2\""]]""}",join_merge.MergeAsof.time_on_int32,time,-0.002760616901660324
3935,param-level,"terminus-2,gpt-5",1.1295411658667804,1.1391285843457797,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.PivotTable.time_pivot_table_categorical,reshape,"{""reshape.PivotTable.time_pivot_table_categorical"": ""class PivotTable:\n    def time_pivot_table_categorical(self):\n        self.df2.pivot_table(\n            index=\""col1\"", values=\""col3\"", columns=\""col2\"", aggfunc=np.sum, fill_value=0\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")""}",reshape.PivotTable.time_pivot_table_categorical,time,-0.006780352531116863
3925,param-level,"terminus-2,gpt-5",0.9564595661905344,0.9525199004060194,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.parsers.DoesStringLookLikeDatetime.time_check_datetimes,io.parsers,"{""io.parsers.DoesStringLookLikeDatetime.time_check_datetimes"": ""class DoesStringLookLikeDatetime:\n    def time_check_datetimes(self, value):\n        for obj in self.objects:\n            _does_string_look_like_datetime(obj)\n\n    def setup(self, value):\n        self.objects = [value] * 1000000""}",io.parsers.DoesStringLookLikeDatetime.time_check_datetimes,time,0.002786185137563642
3928,param-level,"terminus-2,gpt-5",1.0027976053603749,1.0420834684650648,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,libs.ScalarListLike.time_is_list_like,libs,"{""libs.ScalarListLike.time_is_list_like"": ""class ScalarListLike:\n    def time_is_list_like(self, param):\n        is_list_like(param)""}",libs.ScalarListLike.time_is_list_like,time,-0.027783495830756687
3930,param-level,"terminus-2,gpt-5",1.134055805843431,1.123214383263225,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.Cut.time_cut_timedelta,reshape,"{""reshape.Cut.time_cut_timedelta"": ""class Cut:\n    def time_cut_timedelta(self, bins):\n        pd.cut(self.timedelta_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))""}",reshape.Cut.time_cut_timedelta,time,0.007667201258985727
3929,param-level,"terminus-2,gpt-5",0.922564721858806,0.9662950819021658,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,multiindex_object.Integer.time_is_monotonic,multiindex_object,"{""multiindex_object.Integer.time_is_monotonic"": ""class Integer:\n    def time_is_monotonic(self):\n        self.mi_int.is_monotonic_increasing\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Integer:\n    def setup(self):\n        self.mi_int = MultiIndex.from_product(\n            [np.arange(1000), np.arange(1000)], names=[\""one\"", \""two\""]\n        )\n        self.obj_index = np.array(\n            [\n                (0, 10),\n                (0, 11),\n                (0, 12),\n                (0, 13),\n                (0, 14),\n                (0, 15),\n                (0, 16),\n                (0, 17),\n                (0, 18),\n                (0, 19),\n            ],\n            dtype=object,\n        )\n        self.other_mi_many_mismatches = MultiIndex.from_tuples(\n            [\n                (-7, 41),\n                (-2, 3),\n                (-0.7, 5),\n                (0, 0),\n                (0, 1.5),\n                (0, 340),\n                (0, 1001),\n                (1, -4),\n                (1, 20),\n                (1, 1040),\n                (432, -5),\n                (432, 17),\n                (439, 165.5),\n                (998, -4),\n                (998, 24065),\n                (999, 865.2),\n                (999, 1000),\n                (1045, -843),\n            ]\n        )""}",multiindex_object.Integer.time_is_monotonic,time,-0.030926704415388855
3922,param-level,"terminus-2,gpt-5",1.0319564503661036,1.019150027730217,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.json.ToJSON.time_to_json,io.json,"{""io.json.ToJSON.time_to_json"": ""class ToJSON:\n    def time_to_json(self, orient, frame):\n        getattr(self, frame).to_json(self.fname, orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSON:\n    def setup(self, orient, frame):\n        N = 10**5\n        ncols = 5\n        index = date_range(\""20000101\"", periods=N, freq=\""H\"")\n        timedeltas = timedelta_range(start=1, periods=N, freq=\""s\"")\n        datetimes = date_range(start=1, periods=N, freq=\""s\"")\n        ints = np.random.randint(100000000, size=N)\n        longints = sys.maxsize * np.random.randint(100000000, size=N)\n        floats = np.random.randn(N)\n        strings = tm.makeStringIndex(N)\n        self.df = DataFrame(np.random.randn(N, ncols), index=np.arange(N))\n        self.df_date_idx = DataFrame(np.random.randn(N, ncols), index=index)\n        self.df_td_int_ts = DataFrame(\n            {\n                \""td_1\"": timedeltas,\n                \""td_2\"": timedeltas,\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""ts_1\"": datetimes,\n                \""ts_2\"": datetimes,\n            },\n            index=index,\n        )\n        self.df_int_floats = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""int_3\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""float_3\"": floats,\n            },\n            index=index,\n        )\n        self.df_int_float_str = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )\n    \n        self.df_longint_float_str = DataFrame(\n            {\n                \""longint_1\"": longints,\n                \""longint_2\"": longints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )""}",io.json.ToJSON.time_to_json,time,0.009056875980117826
3933,param-level,"terminus-2,gpt-5",1.0433120478808708,1.0285263308926698,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.Explode.time_explode,reshape,"{""reshape.Explode.time_explode"": ""class Explode:\n    def time_explode(self, n_rows, max_list_length):\n        self.series.explode()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Explode:\n    def setup(self, n_rows, max_list_length):\n        data = [np.arange(np.random.randint(max_list_length)) for _ in range(n_rows)]\n        self.series = pd.Series(data)""}",reshape.Explode.time_explode,time,0.010456659821924386
3934,param-level,"terminus-2,gpt-5",1.1141466394257598,1.1113814191673432,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.Melt.time_melt_dataframe,reshape,"{""reshape.Melt.time_melt_dataframe"": ""class Melt:\n    def time_melt_dataframe(self, dtype):\n        melt(self.df, id_vars=[\""id1\"", \""id2\""])\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Melt:\n    def setup(self, dtype):\n        self.df = DataFrame(\n            np.random.randn(100_000, 3), columns=[\""A\"", \""B\"", \""C\""], dtype=dtype\n        )\n        self.df[\""id1\""] = pd.Series(np.random.randint(0, 10, 10000))\n        self.df[\""id2\""] = pd.Series(np.random.randint(100, 1000, 10000))""}",reshape.Melt.time_melt_dataframe,time,0.0019556013142973997
3932,param-level,"terminus-2,gpt-5",1.0324992733073688,1.024862591515639,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.Cut.time_qcut_float,reshape,"{""reshape.Cut.time_qcut_float"": ""class Cut:\n    def time_qcut_float(self, bins):\n        pd.qcut(self.float_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))""}",reshape.Cut.time_qcut_float,time,0.005400765057800365
3931,param-level,"terminus-2,gpt-5",1.1210531193239583,1.1130333586673145,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.Cut.time_qcut_datetime,reshape,"{""reshape.Cut.time_qcut_datetime"": ""class Cut:\n    def time_qcut_datetime(self, bins):\n        pd.qcut(self.datetime_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))""}",reshape.Cut.time_qcut_datetime,time,0.005671683632704309
3924,param-level,"terminus-2,gpt-5",1.0377535457574276,1.0277600230717796,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.json.ToJSONWide.time_to_json,io.json,"{""io.json.ToJSONWide.time_to_json"": ""class ToJSON:\n    def time_to_json(self, orient, frame):\n        getattr(self, frame).to_json(self.fname, orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSONWide:\n    def setup(self, orient, frame):\n        super().setup(orient, frame)\n        base_df = getattr(self, frame).copy()\n        df_wide = concat([base_df.iloc[:100]] * 1000, ignore_index=True, axis=1)\n        self.df_wide = df_wide""}",io.json.ToJSONWide.time_to_json,time,0.007067554940345132
3936,param-level,"terminus-2,gpt-5",1.2298699254682994,1.2283349290061478,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.PivotTable.time_pivot_table_categorical_observed,reshape,"{""reshape.PivotTable.time_pivot_table_categorical_observed"": ""class PivotTable:\n    def time_pivot_table_categorical_observed(self):\n        self.df2.pivot_table(\n            index=\""col1\"",\n            values=\""col3\"",\n            columns=\""col2\"",\n            aggfunc=np.sum,\n            fill_value=0,\n            observed=True,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")""}",reshape.PivotTable.time_pivot_table_categorical_observed,time,0.0010855703409841622
3937,param-level,"terminus-2,gpt-5",1.105180383236151,1.085041769248503,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.PivotTable.time_pivot_table_margins,reshape,"{""reshape.PivotTable.time_pivot_table_margins"": ""class PivotTable:\n    def time_pivot_table_margins(self):\n        self.df.pivot_table(index=\""key1\"", columns=[\""key2\"", \""key3\""], margins=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")""}",reshape.PivotTable.time_pivot_table_margins,time,0.014242301264248972
3941,param-level,"terminus-2,gpt-5",1.0391200920219597,1.0605630731393874,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Apply.time_rolling,rolling,"{""rolling.Apply.time_rolling"": ""class Apply:\n    def time_rolling(self, constructor, window, dtype, function, raw):\n        self.roll.apply(function, raw=raw)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, constructor, window, dtype, function, raw):\n        N = 10**3\n        arr = (100 * np.random.random(N)).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)""}",rolling.Apply.time_rolling,time,-0.015164767409779142
3939,param-level,"terminus-2,gpt-5",1.4107327412531467,1.4175765820288715,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.ReshapeExtensionDtype.time_stack,reshape,"{""reshape.ReshapeExtensionDtype.time_stack"": ""class ReshapeExtensionDtype:\n    def time_stack(self, dtype):\n        self.df.stack()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReshapeExtensionDtype:\n    def setup(self, dtype):\n        lev = pd.Index(list(\""ABCDEFGHIJ\""))\n        ri = pd.Index(range(1000))\n        mi = MultiIndex.from_product([lev, ri], names=[\""foo\"", \""bar\""])\n    \n        index = date_range(\""2016-01-01\"", periods=10000, freq=\""s\"", tz=\""US/Pacific\"")\n        if dtype == \""Period[s]\"":\n            index = index.tz_localize(None).to_period(\""s\"")\n    \n        ser = pd.Series(index, index=mi)\n        df = ser.unstack(\""bar\"")\n        # roundtrips -> df.stack().equals(ser)\n    \n        self.ser = ser\n        self.df = df""}",reshape.ReshapeExtensionDtype.time_stack,time,-0.004840057125689358
3942,param-level,"terminus-2,gpt-5",1.057727120900387,1.0570098215679355,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.EWMMethods.time_ewm,rolling,"{""rolling.EWMMethods.time_ewm"": ""class EWMMethods:\n    def time_ewm(self, constructor, kwargs_method, dtype):\n        getattr(self.ewm, self.method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass EWMMethods:\n    def setup(self, constructor, kwargs_method, dtype):\n        N = 10**5\n        kwargs, method = kwargs_method\n        arr = (100 * np.random.random(N)).astype(dtype)\n        self.method = method\n        self.ewm = getattr(pd, constructor)(arr).ewm(**kwargs)""}",rolling.EWMMethods.time_ewm,time,0.0005072838277590868
3940,param-level,"terminus-2,gpt-5",1.064465515071442,1.0345564193262995,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.WideToLong.time_wide_to_long_big,reshape,"{""reshape.WideToLong.time_wide_to_long_big"": ""class WideToLong:\n    def time_wide_to_long_big(self):\n        wide_to_long(self.df, self.letters, i=\""id\"", j=\""year\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass WideToLong:\n    def setup(self):\n        nyrs = 20\n        nidvars = 20\n        N = 5000\n        self.letters = list(\""ABCD\"")\n        yrvars = [\n            letter + str(num)\n            for letter, num in product(self.letters, range(1, nyrs + 1))\n        ]\n        columns = [str(i) for i in range(nidvars)] + yrvars\n        self.df = DataFrame(np.random.randn(N, nidvars + len(yrvars)), columns=columns)\n        self.df[\""id\""] = self.df.index""}",reshape.WideToLong.time_wide_to_long_big,time,0.021152118631642465
3943,param-level,"terminus-2,gpt-5",1.0399870245389802,1.0462885251255507,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.ForwardWindowMethods.time_rolling,rolling,"{""rolling.ForwardWindowMethods.time_rolling"": ""class ForwardWindowMethods:\n    def time_rolling(self, constructor, window_size, dtype, method):\n        getattr(self.roll, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ForwardWindowMethods:\n    def setup(self, constructor, window_size, dtype, method):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=window_size)\n        self.roll = getattr(pd, constructor)(arr).rolling(window=indexer)""}",rolling.ForwardWindowMethods.time_rolling,time,-0.00445650677975285
3946,param-level,"terminus-2,gpt-5",1.0528150478903315,1.0461862350524451,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.GroupbyEWMEngine.time_groupby_mean,rolling,"{""rolling.GroupbyEWMEngine.time_groupby_mean"": ""class GroupbyEWMEngine:\n    def time_groupby_mean(self, engine):\n        self.gb_ewm.mean(engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupbyEWMEngine:\n    def setup(self, engine):\n        df = pd.DataFrame({\""A\"": range(50), \""B\"": range(50)})\n        self.gb_ewm = df.groupby(\""A\"").ewm(com=1.0)""}",rolling.GroupbyEWMEngine.time_groupby_mean,time,0.004687986448293077
3944,param-level,"terminus-2,gpt-5",1.0872302734586787,1.0914052068359117,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Groupby.time_method,rolling,"{""rolling.Groupby.time_method"": ""class Groupby:\n    def time_method(self, method, window_kwargs):\n        getattr(self.groupby_window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Groupby:\n    def setup(self, method, window_kwargs):\n        N = 1000\n        window, kwargs = window_kwargs\n        df = pd.DataFrame(\n            {\n                \""A\"": [str(i) for i in range(N)] * 10,\n                \""B\"": list(range(N)) * 10,\n            }\n        )\n        if isinstance(kwargs.get(\""window\"", None), str):\n            df.index = pd.date_range(start=\""1900-01-01\"", freq=\""1min\"", periods=N * 10)\n        self.groupby_window = getattr(df.groupby(\""A\""), window)(**kwargs)""}",rolling.Groupby.time_method,time,-0.002952569573714964
3947,param-level,"terminus-2,gpt-5",1.100478784705483,1.0835962245212107,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.GroupbyLargeGroups.time_rolling_multiindex_creation,rolling,"{""rolling.GroupbyLargeGroups.time_rolling_multiindex_creation"": ""class GroupbyLargeGroups:\n    def time_rolling_multiindex_creation(self):\n        self.df.groupby(\""A\"").rolling(3).mean()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupbyLargeGroups:\n    def setup(self):\n        N = 100000\n        self.df = pd.DataFrame({\""A\"": [1, 2] * (N // 2), \""B\"": np.random.randn(N)})""}",rolling.GroupbyLargeGroups.time_rolling_multiindex_creation,time,0.01193957580217282
3938,param-level,"terminus-2,gpt-5",1.0568596542514763,1.0645771443999537,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.PivotTable.time_pivot_table_margins_only_column,reshape,"{""reshape.PivotTable.time_pivot_table_margins_only_column"": ""class PivotTable:\n    def time_pivot_table_margins_only_column(self):\n        self.df.pivot_table(columns=[\""key1\"", \""key2\"", \""key3\""], margins=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")""}",reshape.PivotTable.time_pivot_table_margins_only_column,time,-0.00545791382494861
3949,param-level,"terminus-2,gpt-5",1.0564919755569504,1.0529958245106887,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Pairwise.time_pairwise,rolling,"{""rolling.Pairwise.time_pairwise"": ""class Pairwise:\n    def time_pairwise(self, kwargs_window, method, pairwise):\n        getattr(self.window, method)(self.df, pairwise=pairwise)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Pairwise:\n    def setup(self, kwargs_window, method, pairwise):\n        N = 10**4\n        n_groups = 20\n        kwargs, window = kwargs_window\n        groups = [i for _ in range(N // n_groups) for i in range(n_groups)]\n        arr = np.random.random(N)\n        self.df = pd.DataFrame(arr)\n        self.window = getattr(self.df, window)(**kwargs)\n        self.window_group = getattr(\n            pd.DataFrame({\""A\"": groups, \""B\"": arr}).groupby(\""A\""), window\n        )(**kwargs)""}",rolling.Pairwise.time_pairwise,time,0.002472525492405703
3945,param-level,"terminus-2,gpt-5",1.0827575908690534,1.1082312294361656,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.GroupbyEWM.time_groupby_method,rolling,"{""rolling.GroupbyEWM.time_groupby_method"": ""class GroupbyEWM:\n    def time_groupby_method(self, method):\n        getattr(self.gb_ewm, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupbyEWM:\n    def setup(self, method):\n        df = pd.DataFrame({\""A\"": range(50), \""B\"": range(50)})\n        self.gb_ewm = df.groupby(\""A\"").ewm(com=1.0)""}",rolling.GroupbyEWM.time_groupby_method,time,-0.018015303088481018
3948,param-level,"terminus-2,gpt-5",1.0569603847743732,1.0509165780775958,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Methods.time_method,rolling,"{""rolling.Methods.time_method"": ""class Methods:\n    def time_method(self, constructor, window_kwargs, dtype, method):\n        getattr(self.window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Methods:\n    def setup(self, constructor, window_kwargs, dtype, method):\n        N = 10**5\n        window, kwargs = window_kwargs\n        arr = (100 * np.random.random(N)).astype(dtype)\n        obj = getattr(pd, constructor)(arr)\n        self.window = getattr(obj, window)(**kwargs)""}",rolling.Methods.time_method,time,0.004274262161794483
3954,param-level,"terminus-2,gpt-5",1.1084518137171673,1.1976886276828178,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.Fillna.time_fillna,series_methods,"{""series_methods.Fillna.time_fillna"": ""class Fillna:\n    def time_fillna(self, dtype, method):\n        value = self.fill_value if method is None else None\n        self.ser.fillna(value=value, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Fillna:\n    def setup(self, dtype, method):\n        N = 10**6\n        if dtype == \""datetime64[ns]\"":\n            data = date_range(\""2000-01-01\"", freq=\""S\"", periods=N)\n            na_value = NaT\n        elif dtype in (\""float64\"", \""Float64\""):\n            data = np.random.randn(N)\n            na_value = np.nan\n        elif dtype in (\""Int64\"", \""int64[pyarrow]\""):\n            data = np.arange(N)\n            na_value = NA\n        elif dtype in (\""string\"", \""string[pyarrow]\""):\n            data = tm.rands_array(5, N)\n            na_value = NA\n        else:\n            raise NotImplementedError\n        fill_value = data[0]\n        ser = Series(data, dtype=dtype)\n        ser[::2] = na_value\n        self.ser = ser\n        self.fill_value = fill_value""}",series_methods.Fillna.time_fillna,time,-0.0631094865386496
3956,param-level,"terminus-2,gpt-5",1.0410190222734892,1.0623573846427314,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.SearchSorted.time_searchsorted,series_methods,"{""series_methods.SearchSorted.time_searchsorted"": ""class SearchSorted:\n    def time_searchsorted(self, dtype):\n        key = \""2\"" if dtype == \""str\"" else 2\n        self.s.searchsorted(key)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SearchSorted:\n    def setup(self, dtype):\n        N = 10**5\n        data = np.array([1] * N + [2] * N + [3] * N).astype(dtype)\n        self.s = Series(data)""}",series_methods.SearchSorted.time_searchsorted,time,-0.01509077961049664
3961,param-level,"terminus-2,gpt-5",0.9122514023207678,0.912691752252956,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.ArithmeticBlock.time_addition,sparse,"{""sparse.ArithmeticBlock.time_addition"": ""class ArithmeticBlock:\n    def time_addition(self, fill_value):\n        self.arr1 + self.arr2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )""}",sparse.ArithmeticBlock.time_addition,time,-0.0003114214513352978
3950,param-level,"terminus-2,gpt-5",1.035932610315794,1.0647197439195957,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Quantile.time_quantile,rolling,"{""rolling.Quantile.time_quantile"": ""class Quantile:\n    def time_quantile(self, constructor, window, dtype, percentile, interpolation):\n        self.roll.quantile(percentile, interpolation=interpolation)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, constructor, window, dtype, percentile, interpolation):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)""}",rolling.Quantile.time_quantile,time,-0.020358651770722557
3952,param-level,"terminus-2,gpt-5",1.059443156912705,1.060642811266571,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.TableMethod.time_apply,rolling,"{""rolling.TableMethod.time_apply"": ""class TableMethod:\n    def time_apply(self, method):\n        self.df.rolling(2, method=method).apply(\n            table_method_func, raw=True, engine=\""numba\""\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TableMethod:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(10, 1000))""}",rolling.TableMethod.time_apply,time,-0.0008484118485614789
3962,param-level,"terminus-2,gpt-5",0.909030324994456,0.9086828083158498,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.ArithmeticBlock.time_division,sparse,"{""sparse.ArithmeticBlock.time_division"": ""class ArithmeticBlock:\n    def time_division(self, fill_value):\n        self.arr1 / self.arr2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )""}",sparse.ArithmeticBlock.time_division,time,0.0002457685138657843
3957,param-level,"terminus-2,gpt-5",1.0252508663975362,1.0118678655169218,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.SeriesConstructor.time_constructor_dict,series_methods,"{""series_methods.SeriesConstructor.time_constructor_dict"": ""class SeriesConstructor:\n    def time_constructor_dict(self):\n        Series(data=self.data, index=self.idx)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesConstructor:\n    def setup(self):\n        self.idx = date_range(\n            start=datetime(2015, 10, 26), end=datetime(2016, 1, 1), freq=\""50s\""\n        )\n        self.data = dict(zip(self.idx, range(len(self.idx))))\n        self.array = np.array([1, 2, 3])\n        self.idx2 = Index([\""a\"", \""b\"", \""c\""])""}",series_methods.SeriesConstructor.time_constructor_dict,time,0.009464639943857421
3951,param-level,"terminus-2,gpt-5",1.0158117231477402,1.0235342956125444,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Rank.time_rank,rolling,"{""rolling.Rank.time_rank"": ""class Rank:\n    def time_rank(self, constructor, window, dtype, percentile, ascending, method):\n        self.roll.rank(pct=percentile, ascending=ascending, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Rank:\n    def setup(self, constructor, window, dtype, percentile, ascending, method):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)""}",rolling.Rank.time_rank,time,-0.005461508108065229
3964,param-level,"terminus-2,gpt-5",1.0250425473131854,1.0425458957734757,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,stat_ops.Correlation.time_corr_series,stat_ops,"{""stat_ops.Correlation.time_corr_series"": ""class Correlation:\n    def time_corr_series(self, method):\n        self.s.corr(self.s2, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Correlation:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(500, 15))\n        self.df2 = pd.DataFrame(np.random.randn(500, 15))\n        self.df_wide = pd.DataFrame(np.random.randn(500, 100))\n        self.df_wide_nans = self.df_wide.where(np.random.random((500, 100)) < 0.9)\n        self.s = pd.Series(np.random.randn(500))\n        self.s2 = pd.Series(np.random.randn(500))""}",stat_ops.Correlation.time_corr_series,time,-0.012378605700346695
3958,param-level,"terminus-2,gpt-5",0.9126102627268862,0.9048225669589512,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.Arithmetic.time_add,sparse,"{""sparse.Arithmetic.time_add"": ""class Arithmetic:\n    def time_add(self, dense_proportion, fill_value):\n        self.array1 + self.array2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)""}",sparse.Arithmetic.time_add,time,0.005507564192316069
3953,param-level,"terminus-2,gpt-5",1.0423492258524891,1.044855253893347,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.VariableWindowMethods.time_method,rolling,"{""rolling.VariableWindowMethods.time_method"": ""class Methods:\n    def time_method(self, constructor, window_kwargs, dtype, method):\n        getattr(self.window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass VariableWindowMethods:\n    def setup(self, constructor, window, dtype, method):\n        N = 10**5\n        arr = (100 * np.random.random(N)).astype(dtype)\n        index = pd.date_range(\""2017-01-01\"", periods=N, freq=\""5s\"")\n        self.window = getattr(pd, constructor)(arr, index=index).rolling(window)""}",rolling.VariableWindowMethods.time_method,time,-0.0017722970585982865
3955,param-level,"terminus-2,gpt-5",1.0584044800057109,1.0597253376016218,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.NanOps.time_func,series_methods,"{""series_methods.NanOps.time_func"": ""class NanOps:\n    def time_func(self, func, N, dtype):\n        self.func()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NanOps:\n    def setup(self, func, N, dtype):\n        if func == \""argmax\"" and dtype in {\""Int64\"", \""boolean\""}:\n            # Skip argmax for nullable int since this doesn't work yet (GH-24382)\n            raise NotImplementedError\n        self.s = Series([1] * N, dtype=dtype)\n        self.func = getattr(self.s, func)""}",series_methods.NanOps.time_func,time,-0.0009341284270940173
3966,param-level,"terminus-2,gpt-5",1.0592058407757647,1.0413555377244976,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,stat_ops.FrameOps.time_op,stat_ops,"{""stat_ops.FrameOps.time_op"": ""class FrameOps:\n    def time_op(self, op, dtype, axis):\n        self.df_func(axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FrameOps:\n    def setup(self, op, dtype, axis):\n        values = np.random.randn(100000, 4)\n        if dtype == \""Int64\"":\n            values = values.astype(int)\n        df = pd.DataFrame(values).astype(dtype)\n        self.df_func = getattr(df, op)""}",stat_ops.FrameOps.time_op,time,0.012623976698208708
3965,param-level,"terminus-2,gpt-5",1.0710537985610438,1.0475539470866344,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,stat_ops.FrameMultiIndexOps.time_op,stat_ops,"{""stat_ops.FrameMultiIndexOps.time_op"": ""class FrameMultiIndexOps:\n    def time_op(self, op):\n        self.df_func()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FrameMultiIndexOps:\n    def setup(self, op):\n        levels = [np.arange(10), np.arange(100), np.arange(100)]\n        codes = [\n            np.arange(10).repeat(10000),\n            np.tile(np.arange(100).repeat(100), 10),\n            np.tile(np.tile(np.arange(100), 100), 10),\n        ]\n        index = pd.MultiIndex(levels=levels, codes=codes)\n        df = pd.DataFrame(np.random.randn(len(index), 4), index=index)\n        self.df_func = getattr(df, op)""}",stat_ops.FrameMultiIndexOps.time_op,time,0.016619414055452166
3963,param-level,"terminus-2,gpt-5",0.8907659851475722,0.8920809027583091,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.ArithmeticBlock.time_make_union,sparse,"{""sparse.ArithmeticBlock.time_make_union"": ""class ArithmeticBlock:\n    def time_make_union(self, fill_value):\n        self.arr1.sp_index.make_union(self.arr2.sp_index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )""}",sparse.ArithmeticBlock.time_make_union,time,-0.0009299275889228264
3960,param-level,"terminus-2,gpt-5",0.921972721343999,0.9198406736292096,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.Arithmetic.time_make_union,sparse,"{""sparse.Arithmetic.time_make_union"": ""class Arithmetic:\n    def time_make_union(self, dense_proportion, fill_value):\n        self.array1.sp_index.make_union(self.array2.sp_index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)""}",sparse.Arithmetic.time_make_union,time,0.001507813093910442
3959,param-level,"terminus-2,gpt-5",0.9091904913610988,0.91583897802989,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.Arithmetic.time_divide,sparse,"{""sparse.Arithmetic.time_divide"": ""class Arithmetic:\n    def time_divide(self, dense_proportion, fill_value):\n        self.array1 / self.array2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)""}",sparse.Arithmetic.time_divide,time,-0.004701900048650109
3968,param-level,"terminus-2,gpt-5",1.0278443416482657,1.033114634096658,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,strftime.BusinessHourStrftime.time_frame_offset_repr,strftime,"{""strftime.BusinessHourStrftime.time_frame_offset_repr"": ""class BusinessHourStrftime:\n    def time_frame_offset_repr(self, obs):\n        self.data[\""off\""].apply(repr)\n\n    def setup(self, obs):\n        self.data = pd.DataFrame(\n            {\n                \""off\"": [offsets.BusinessHour()] * obs,\n            }\n        )""}",strftime.BusinessHourStrftime.time_frame_offset_repr,time,-0.0037272223821727566
3978,param-level,"terminus-2,gpt-5",1.0147936886724054,1.010748999041344,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.DatetimeIndex.time_to_time,timeseries,"{""timeseries.DatetimeIndex.time_to_time"": ""class DatetimeIndex:\n    def time_to_time(self, index_type):\n        self.index.time\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]""}",timeseries.DatetimeIndex.time_to_time,time,0.0028604594279075706
3972,param-level,"terminus-2,gpt-5",1.017258746144939,1.0435889263128073,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Methods.time_fullmatch,strings,"{""strings.Methods.time_fullmatch"": ""class Methods:\n    def time_fullmatch(self, dtype):\n        self.s.str.fullmatch(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.Methods.time_fullmatch,time,-0.01862106093908654
3975,param-level,"terminus-2,gpt-5",1.3663665411564112,1.3596393751609956,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Methods.time_upper,strings,"{""strings.Methods.time_upper"": ""class Methods:\n    def time_upper(self, dtype):\n        self.s.str.upper()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.Methods.time_upper,time,0.004757543136786112
3980,param-level,"terminus-2,gpt-5",1.012088734909618,1.0300157447443716,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.fields.TimeGetStartEndField.time_get_start_end_field,tslibs.fields,"{""tslibs.fields.TimeGetStartEndField.time_get_start_end_field"": ""class TimeGetStartEndField:\n    def time_get_start_end_field(self, size, side, period, freqstr, month_kw):\n        get_start_end_field(self.i8data, self.attrname, freqstr, month_kw=month_kw)\n\n    def setup(self, size, side, period, freqstr, month_kw):\n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr\n    \n        self.attrname = f\""is_{period}_{side}\""""}",tslibs.fields.TimeGetStartEndField.time_get_start_end_field,time,-0.01267822477705354
3973,param-level,"terminus-2,gpt-5",1.0164370423138829,1.0196207599438951,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Methods.time_get,strings,"{""strings.Methods.time_get"": ""class Methods:\n    def time_get(self, dtype):\n        self.s.str.get(0)\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.Methods.time_get,time,-0.0022515683380567465
3976,param-level,"terminus-2,gpt-5",1.0256753129131535,1.0312078855701563,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Methods.time_wrap,strings,"{""strings.Methods.time_wrap"": ""class Methods:\n    def time_wrap(self, dtype):\n        self.s.str.wrap(10)\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.Methods.time_wrap,time,-0.003912710507074115
3971,param-level,"terminus-2,gpt-5",1.0308279016595705,1.037615166606883,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Methods.time_endswith,strings,"{""strings.Methods.time_endswith"": ""class Methods:\n    def time_endswith(self, dtype):\n        self.s.str.endswith(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.Methods.time_endswith,time,-0.004800045931621224
3970,param-level,"terminus-2,gpt-5",1.025119076680235,1.0221869382018522,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Contains.time_contains,strings,"{""strings.Contains.time_contains"": ""class Contains:\n    def time_contains(self, dtype, regex):\n        self.s.str.contains(\""A\"", regex=regex)\n\n    def setup(self, dtype, regex):\n        super().setup(dtype)""}",strings.Contains.time_contains,time,0.0020736481459566448
3981,param-level,"terminus-2,gpt-5",1.0027272859251488,0.9623454744132488,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.normalize.Normalize.time_is_date_array_normalized,tslibs.normalize,"{""tslibs.normalize.Normalize.time_is_date_array_normalized"": ""class Normalize:\n    def time_is_date_array_normalized(self, size, tz):\n        # TODO: cases with different levels of short-circuiting\n        # 10 i.e. NPY_FR_ns\n        is_date_array_normalized(self.i8data, tz, 10)\n\n    def setup(self, size, tz):\n        # use an array that will have is_date_array_normalized give True,\n        #  so we do not short-circuit early.\n        dti = pd.date_range(\""2016-01-01\"", periods=10, tz=tz).repeat(size // 10)\n        self.i8data = dti.asi8\n    \n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError""}",tslibs.normalize.Normalize.time_is_date_array_normalized,time,0.028558565425671863
3977,param-level,"terminus-2,gpt-5",0.9930445623907804,0.9911992258081832,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.DatetimeIndex.time_timeseries_is_month_start,timeseries,"{""timeseries.DatetimeIndex.time_timeseries_is_month_start"": ""class DatetimeIndex:\n    def time_timeseries_is_month_start(self, index_type):\n        self.index.is_month_start\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]""}",timeseries.DatetimeIndex.time_timeseries_is_month_start,time,0.0013050470881168484
3982,param-level,"terminus-2,gpt-5",1.0456899732678298,1.0498466373943325,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.OffestDatetimeArithmetic.time_add,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_add"": ""class OffestDatetimeArithmetic:\n    def time_add(self, offset):\n        self.date + offset\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.OffestDatetimeArithmetic.time_add,time,-0.0029396493115295117
3969,param-level,"terminus-2,gpt-5",1.0228934894927442,1.029138406425122,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,strftime.BusinessHourStrftime.time_frame_offset_str,strftime,"{""strftime.BusinessHourStrftime.time_frame_offset_str"": ""class BusinessHourStrftime:\n    def time_frame_offset_str(self, obs):\n        self.data[\""off\""].apply(str)\n\n    def setup(self, obs):\n        self.data = pd.DataFrame(\n            {\n                \""off\"": [offsets.BusinessHour()] * obs,\n            }\n        )""}",strftime.BusinessHourStrftime.time_frame_offset_str,time,-0.004416490051186664
3974,param-level,"terminus-2,gpt-5",1.2603851718908452,1.3086013614510008,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Methods.time_lower,strings,"{""strings.Methods.time_lower"": ""class Methods:\n    def time_lower(self, dtype):\n        self.s.str.lower()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.Methods.time_lower,time,-0.03409914396050609
3984,param-level,"terminus-2,gpt-5",1.0576304060117854,1.0505875506830056,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.OffestDatetimeArithmetic.time_add_np_dt64,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_add_np_dt64"": ""class OffestDatetimeArithmetic:\n    def time_add_np_dt64(self, offset):\n        offset + self.dt64\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.OffestDatetimeArithmetic.time_add_np_dt64,time,0.004980802919929134
3986,param-level,"terminus-2,gpt-5",1.0567498590741593,1.0539364109765217,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.OffestDatetimeArithmetic.time_subtract_10,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_subtract_10"": ""class OffestDatetimeArithmetic:\n    def time_subtract_10(self, offset):\n        self.date - (10 * offset)\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.OffestDatetimeArithmetic.time_subtract_10,time,0.001989708697056306
3989,param-level,"terminus-2,gpt-5",1.018202571806409,1.0292640988421993,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.PeriodProperties.time_property,tslibs.period,"{""tslibs.period.PeriodProperties.time_property"": ""class PeriodProperties:\n    def time_property(self, freq, attr):\n        getattr(self.per, attr)\n\n    def setup(self, freq, attr):\n        self.per = Period(\""2012-06-01\"", freq=freq)""}",tslibs.period.PeriodProperties.time_property,time,-0.007822862118663601
3967,param-level,"terminus-2,gpt-5",1.0271422162117505,1.025052916954476,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,stat_ops.SeriesOps.time_op,stat_ops,"{""stat_ops.SeriesOps.time_op"": ""class SeriesOps:\n    def time_op(self, op, dtype):\n        self.s_func()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesOps:\n    def setup(self, op, dtype):\n        s = pd.Series(np.random.randn(100000)).astype(dtype)\n        self.s_func = getattr(s, op)""}",stat_ops.SeriesOps.time_op,time,0.001477580804295986
3983,param-level,"terminus-2,gpt-5",1.040357876864411,1.0457376008735422,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.OffestDatetimeArithmetic.time_add_10,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_add_10"": ""class OffestDatetimeArithmetic:\n    def time_add_10(self, offset):\n        self.date + (10 * offset)\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.OffestDatetimeArithmetic.time_add_10,time,-0.0038046138678437874
3979,param-level,"terminus-2,gpt-5",0.9762346475653858,0.966058356250507,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.fields.TimeGetDateField.time_get_date_field,tslibs.fields,"{""tslibs.fields.TimeGetDateField.time_get_date_field"": ""class TimeGetDateField:\n    def time_get_date_field(self, size, field):\n        get_date_field(self.i8data, field)\n\n    def setup(self, size, field):\n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.fields.TimeGetDateField.time_get_date_field,time,0.007196811396661139
3987,param-level,"terminus-2,gpt-5",0.981357723208407,0.9876597220558692,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.OnOffset.time_on_offset,tslibs.offsets,"{""tslibs.offsets.OnOffset.time_on_offset"": ""class OnOffset:\n    def time_on_offset(self, offset):\n        for date in self.dates:\n            offset.is_on_offset(date)\n\n    def setup(self, offset):\n        self.dates = [\n            datetime(2016, m, d)\n            for m in [10, 11, 12]\n            for d in [1, 2, 3, 28, 29, 30, 31]\n            if not (m == 11 and d == 31)\n        ]""}",tslibs.offsets.OnOffset.time_on_offset,time,-0.004456859156621066
3994,param-level,"terminus-2,gpt-5",1.0002631015527346,0.9703121427233437,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr,tslibs.period,"{""tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr"": ""class TimePeriodArrToDT64Arr:\n    def time_periodarray_to_dt64arr(self, size, freq):\n        periodarr_to_dt64arr(self.i8values, freq)\n\n    def setup(self, size, freq):\n        arr = np.arange(10, dtype=\""i8\"").repeat(size // 10)\n        self.i8values = arr""}",tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr,time,0.021181724773260906
3995,param-level,"terminus-2,gpt-5",1.0075868599150823,1.0124123880355616,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.resolution.TimeResolution.time_get_resolution,tslibs.resolution,"{""tslibs.resolution.TimeResolution.time_get_resolution"": ""class TimeResolution:\n    def time_get_resolution(self, unit, size, tz):\n        get_resolution(self.i8data, tz)\n\n    def setup(self, unit, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        arr = arr.view(f\""M8[{unit}]\"").astype(\""M8[ns]\"").view(\""i8\"")\n        self.i8data = arr""}",tslibs.resolution.TimeResolution.time_get_resolution,time,-0.003412679010239891
3999,param-level,"terminus-2,gpt-5",1.0198470736385634,1.0272665785845632,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timedelta.TimedeltaConstructor.time_from_string,tslibs.timedelta,"{""tslibs.timedelta.TimedeltaConstructor.time_from_string"": ""class TimedeltaConstructor:\n    def time_from_string(self):\n        Timedelta(\""1 days\"")\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")""}",tslibs.timedelta.TimedeltaConstructor.time_from_string,time,-0.005247174643564176
3992,param-level,"terminus-2,gpt-5",1.0364622985126133,1.0745328131661918,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.PeriodUnaryMethods.time_to_timestamp,tslibs.period,"{""tslibs.period.PeriodUnaryMethods.time_to_timestamp"": ""class PeriodUnaryMethods:\n    def time_to_timestamp(self, freq):\n        self.per.to_timestamp()\n\n    def setup(self, freq):\n        self.per = Period(\""2012-06-01\"", freq=freq)""}",tslibs.period.PeriodUnaryMethods.time_to_timestamp,time,-0.026923984903520843
3991,param-level,"terminus-2,gpt-5",1.0441422483193945,1.0369797119462696,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.PeriodUnaryMethods.time_now,tslibs.period,"{""tslibs.period.PeriodUnaryMethods.time_now"": ""class PeriodUnaryMethods:\n    def time_now(self, freq):\n        self.per.now(freq)\n\n    def setup(self, freq):\n        self.per = Period(\""2012-06-01\"", freq=freq)""}",tslibs.period.PeriodUnaryMethods.time_now,time,0.005065442979579149
3990,param-level,"terminus-2,gpt-5",1.0207411616961457,1.0187289916107118,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.PeriodUnaryMethods.time_asfreq,tslibs.period,"{""tslibs.period.PeriodUnaryMethods.time_asfreq"": ""class PeriodUnaryMethods:\n    def time_asfreq(self, freq):\n        self.per.asfreq(\""A\"")\n\n    def setup(self, freq):\n        self.per = Period(\""2012-06-01\"", freq=freq)""}",tslibs.period.PeriodUnaryMethods.time_asfreq,time,0.0014230340066717555
3993,param-level,"terminus-2,gpt-5",1.010929828348516,1.0135595023440018,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr,tslibs.period,"{""tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr"": ""class TimeDT64ArrToPeriodArr:\n    def time_dt64arr_to_periodarr(self, size, freq, tz):\n        dt64arr_to_periodarr(self.i8values, freq, tz)\n\n    def setup(self, size, freq, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.arange(10, dtype=\""i8\"").repeat(size // 10)\n        self.i8values = arr""}",tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr,time,-0.0018597411566378074
3988,param-level,"terminus-2,gpt-5",1.0687924408221696,1.0892287783290096,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.PeriodConstructor.time_period_constructor,tslibs.period,"{""tslibs.period.PeriodConstructor.time_period_constructor"": ""class PeriodConstructor:\n    def time_period_constructor(self, freq, is_offset):\n        Period(\""2012-06-01\"", freq=freq)\n\n    def setup(self, freq, is_offset):\n        if is_offset:\n            self.freq = to_offset(freq)\n        else:\n            self.freq = freq""}",tslibs.period.PeriodConstructor.time_period_constructor,time,-0.01445285537966049
4003,param-level,"terminus-2,gpt-5",1.0279009276060944,1.01738764873653,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampConstruction.time_from_npdatetime64,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_from_npdatetime64"": ""class TimestampConstruction:\n    def time_from_npdatetime64(self):\n        Timestamp(self.npdatetime64)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.TimestampConstruction.time_from_npdatetime64,time,0.007435133571120456
3998,param-level,"terminus-2,gpt-5",1.0310551426126953,1.0242987605277876,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timedelta.TimedeltaConstructor.time_from_np_timedelta,tslibs.timedelta,"{""tslibs.timedelta.TimedeltaConstructor.time_from_np_timedelta"": ""class TimedeltaConstructor:\n    def time_from_np_timedelta(self):\n        Timedelta(self.nptimedelta64)\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")""}",tslibs.timedelta.TimedeltaConstructor.time_from_np_timedelta,time,0.004778205151985642
3985,param-level,"terminus-2,gpt-5",1.0484595268392467,1.0485062112037746,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.OffestDatetimeArithmetic.time_subtract,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_subtract"": ""class OffestDatetimeArithmetic:\n    def time_subtract(self, offset):\n        self.date - offset\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.OffestDatetimeArithmetic.time_subtract,time,-3.301581649782667e-05
4000,param-level,"terminus-2,gpt-5",1.0135610916926148,1.0385202816031789,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampAcrossDst.time_replace_across_dst,tslibs.timestamp,"{""tslibs.timestamp.TimestampAcrossDst.time_replace_across_dst"": ""class TimestampAcrossDst:\n    def time_replace_across_dst(self):\n        self.ts2.replace(tzinfo=self.tzinfo)\n\n    def setup(self):\n        dt = datetime(2016, 3, 27, 1)\n        self.tzinfo = pytz.timezone(\""CET\"").localize(dt, is_dst=False).tzinfo\n        self.ts2 = Timestamp(dt)""}",tslibs.timestamp.TimestampAcrossDst.time_replace_across_dst,time,-0.017651478013128795
4002,param-level,"terminus-2,gpt-5",1.0335804549781111,1.0272462397518227,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampConstruction.time_from_datetime_unaware,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_from_datetime_unaware"": ""class TimestampConstruction:\n    def time_from_datetime_unaware(self):\n        Timestamp(self.dttime_unaware)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.TimestampConstruction.time_from_datetime_unaware,time,0.004479643017177092
4001,param-level,"terminus-2,gpt-5",1.0507377625539016,1.026953965589444,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampConstruction.time_from_datetime_aware,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_from_datetime_aware"": ""class TimestampConstruction:\n    def time_from_datetime_aware(self):\n        Timestamp(self.dttime_aware)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.TimestampConstruction.time_from_datetime_aware,time,0.01682022416156835
3997,param-level,"terminus-2,gpt-5",1.0232342172789712,1.026531993986255,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timedelta.TimedeltaConstructor.time_from_iso_format,tslibs.timedelta,"{""tslibs.timedelta.TimedeltaConstructor.time_from_iso_format"": ""class TimedeltaConstructor:\n    def time_from_iso_format(self):\n        Timedelta(\""P4DT12H30M5S\"")\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")""}",tslibs.timedelta.TimedeltaConstructor.time_from_iso_format,time,-0.0023322324662543893
3996,param-level,"terminus-2,gpt-5",1.0504753429107314,1.0156490510826353,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timedelta.TimedeltaConstructor.time_from_datetime_timedelta,tslibs.timedelta,"{""tslibs.timedelta.TimedeltaConstructor.time_from_datetime_timedelta"": ""class TimedeltaConstructor:\n    def time_from_datetime_timedelta(self):\n        Timedelta(self.dttimedelta)\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")""}",tslibs.timedelta.TimedeltaConstructor.time_from_datetime_timedelta,time,0.02462962646965782
4016,param-level,"terminus-2,gpt-5",1.0140354533158242,1.0096282482119523,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampProperties.time_is_quarter_end,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_is_quarter_end"": ""class TimestampProperties:\n    def time_is_quarter_end(self, tz):\n        self.ts.is_quarter_end\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.TimestampProperties.time_is_quarter_end,time,0.0031168352926958533
4013,param-level,"terminus-2,gpt-5",1.0122450486528345,0.97438388789397,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampOps.time_tz_convert,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_tz_convert"": ""class TimestampOps:\n    def time_tz_convert(self, tz):\n        if self.ts.tz is not None:\n            self.ts.tz_convert(tz)\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.TimestampOps.time_tz_convert,time,0.026775926986467097
4009,param-level,"terminus-2,gpt-5",1.017750066029947,1.0292353035559174,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampOps.time_normalize,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_normalize"": ""class TimestampOps:\n    def time_normalize(self, tz):\n        self.ts.normalize()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.TimestampOps.time_normalize,time,-0.008122515930672147
4007,param-level,"terminus-2,gpt-5",1.035281365647862,1.028120784701606,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampOps.time_ceil,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_ceil"": ""class TimestampOps:\n    def time_ceil(self, tz):\n        self.ts.ceil(\""5T\"")\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.TimestampOps.time_ceil,time,0.005064060075145718
4010,param-level,"terminus-2,gpt-5",1.0466428170174762,1.0477511425107873,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampOps.time_replace_None,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_replace_None"": ""class TimestampOps:\n    def time_replace_None(self, tz):\n        self.ts.replace(tzinfo=None)\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.TimestampOps.time_replace_None,time,-0.0007838228382680903
4006,param-level,"terminus-2,gpt-5",1.0111660709481616,1.0164790544428826,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampConstruction.time_parse_iso8601_tz,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_parse_iso8601_tz"": ""class TimestampConstruction:\n    def time_parse_iso8601_tz(self):\n        Timestamp(\""2017-08-25 08:16:14-0500\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.TimestampConstruction.time_parse_iso8601_tz,time,-0.0037574140698168174
4004,param-level,"terminus-2,gpt-5",1.0143900976752434,1.0562877508565431,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampConstruction.time_parse_dateutil,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_parse_dateutil"": ""class TimestampConstruction:\n    def time_parse_dateutil(self):\n        Timestamp(\""2017/08/25 08:16:14 AM\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.TimestampConstruction.time_parse_dateutil,time,-0.029630589237128566
4011,param-level,"terminus-2,gpt-5",1.03328246186692,1.0396346627529898,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampOps.time_replace_tz,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_replace_tz"": ""class TimestampOps:\n    def time_replace_tz(self, tz):\n        self.ts.replace(tzinfo=pytz.timezone(\""US/Eastern\""))\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.TimestampOps.time_replace_tz,time,-0.00449236271999271
4019,param-level,"terminus-2,gpt-5",1.0171445771171386,1.0104473288567313,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampProperties.time_is_year_start,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_is_year_start"": ""class TimestampProperties:\n    def time_is_year_start(self, tz):\n        self.ts.is_year_start\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.TimestampProperties.time_is_year_start,time,0.00473638490835028
4005,param-level,"terminus-2,gpt-5",1.041839474670617,1.0390043640338635,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampConstruction.time_parse_iso8601_no_tz,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_parse_iso8601_no_tz"": ""class TimestampConstruction:\n    def time_parse_iso8601_no_tz(self):\n        Timestamp(\""2017-08-25 08:16:14\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.TimestampConstruction.time_parse_iso8601_no_tz,time,0.0020050287388638315
4014,param-level,"terminus-2,gpt-5",1.0175806384357051,0.9848410265394214,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampOps.time_tz_localize,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_tz_localize"": ""class TimestampOps:\n    def time_tz_localize(self, tz):\n        if self.ts.tz is None:\n            self.ts.tz_localize(tz)\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.TimestampOps.time_tz_localize,time,0.02315389808789513
4008,param-level,"terminus-2,gpt-5",1.0283387351275328,1.0425602629905846,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampOps.time_floor,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_floor"": ""class TimestampOps:\n    def time_floor(self, tz):\n        self.ts.floor(\""5T\"")\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.TimestampOps.time_floor,time,-0.010057657611776346
4015,param-level,"terminus-2,gpt-5",1.0105903306150494,1.015546681180151,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampProperties.time_is_leap_year,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_is_leap_year"": ""class TimestampProperties:\n    def time_is_leap_year(self, tz):\n        self.ts.is_leap_year\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.TimestampProperties.time_is_leap_year,time,-0.0035051984194494875
4012,param-level,"terminus-2,gpt-5",1.0198540312318218,1.023041485566896,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampOps.time_to_julian_date,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_to_julian_date"": ""class TimestampOps:\n    def time_to_julian_date(self, tz):\n        self.ts.to_julian_date()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.TimestampOps.time_to_julian_date,time,-0.002254210986615317
4027,param-level,"terminus-2,oracle",1.088845865478887,1.088845865478887,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algorithms.Factorize.time_factorize,algorithms,"{""algorithms.Factorize.time_factorize"": ""class Factorize:\n    def time_factorize(self, unique, sort, dtype):\n        pd.factorize(self.data, sort=sort)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Factorize:\n    def setup(self, unique, sort, dtype):\n        N = 10**5\n        string_index = tm.makeStringIndex(N)\n        string_arrow = None\n        if dtype == \""string[pyarrow]\"":\n            try:\n                string_arrow = pd.array(string_index, dtype=\""string[pyarrow]\"")\n            except ImportError:\n                raise NotImplementedError\n    \n        data = {\n            \""int\"": pd.Index(np.arange(N), dtype=\""int64\""),\n            \""uint\"": pd.Index(np.arange(N), dtype=\""uint64\""),\n            \""float\"": pd.Index(np.random.randn(N), dtype=\""float64\""),\n            \""object\"": string_index,\n            \""datetime64[ns]\"": pd.date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n            \""datetime64[ns, tz]\"": pd.date_range(\n                \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n            ),\n            \""Int64\"": pd.array(np.arange(N), dtype=\""Int64\""),\n            \""boolean\"": pd.array(np.random.randint(0, 2, N), dtype=\""boolean\""),\n            \""string[pyarrow]\"": string_arrow,\n        }[dtype]\n        if not unique:\n            data = data.repeat(5)\n        self.data = data""}",algorithms.Factorize.time_factorize,time,0.0
4026,param-level,"terminus-2,oracle",1.0219970745309055,1.0219970745309055,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algorithms.Duplicated.time_duplicated,algorithms,"{""algorithms.Duplicated.time_duplicated"": ""class Duplicated:\n    def time_duplicated(self, unique, keep, dtype):\n        self.idx.duplicated(keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Duplicated:\n    def setup(self, unique, keep, dtype):\n        N = 10**5\n        data = {\n            \""int\"": pd.Index(np.arange(N), dtype=\""int64\""),\n            \""uint\"": pd.Index(np.arange(N), dtype=\""uint64\""),\n            \""float\"": pd.Index(np.random.randn(N), dtype=\""float64\""),\n            \""string\"": tm.makeStringIndex(N),\n            \""datetime64[ns]\"": pd.date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n            \""datetime64[ns, tz]\"": pd.date_range(\n                \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n            ),\n        }[dtype]\n        if not unique:\n            data = data.repeat(5)\n        self.idx = data\n        # cache is_unique\n        self.idx.is_unique""}",algorithms.Duplicated.time_duplicated,time,0.0
4017,param-level,"terminus-2,gpt-5",1.0112033176462023,1.0098146508293273,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampProperties.time_is_quarter_start,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_is_quarter_start"": ""class TimestampProperties:\n    def time_is_quarter_start(self, tz):\n        self.ts.is_quarter_start\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.TimestampProperties.time_is_quarter_start,time,0.0009820840289073516
4018,param-level,"terminus-2,gpt-5",1.01162875302415,1.0069464222790394,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampProperties.time_is_year_end,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_is_year_end"": ""class TimestampProperties:\n    def time_is_year_end(self, tz):\n        self.ts.is_year_end\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.TimestampProperties.time_is_year_end,time,0.0033114078819734697
4031,param-level,"terminus-2,oracle",1.1108688007180527,1.1108688007180527,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.IsIn.time_isin_mismatched_dtype,algos.isin,"{""algos.isin.IsIn.time_isin_mismatched_dtype"": ""class IsIn:\n    def time_isin_mismatched_dtype(self, dtype):\n        self.series.isin(self.mismatched)\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)""}",algos.isin.IsIn.time_isin_mismatched_dtype,time,0.0
4029,param-level,"terminus-2,oracle",1.0618129829695413,1.0618129829695413,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.IsIn.time_isin_categorical,algos.isin,"{""algos.isin.IsIn.time_isin_categorical"": ""class IsIn:\n    def time_isin_categorical(self, dtype):\n        self.series.isin(self.cat_values)\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)""}",algos.isin.IsIn.time_isin_categorical,time,0.0
4032,param-level,"terminus-2,oracle",1.173784775878192,1.173784775878192,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.IsInLongSeriesLookUpDominates.time_isin,algos.isin,"{""algos.isin.IsInLongSeriesLookUpDominates.time_isin"": ""class IsInLongSeriesLookUpDominates:\n    def time_isin(self, dtypes, MaxNumber, series_type):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, MaxNumber, series_type):\n        N = 10**7\n    \n        if series_type == \""random_hits\"":\n            array = np.random.randint(0, MaxNumber, N)\n        if series_type == \""random_misses\"":\n            array = np.random.randint(0, MaxNumber, N) + MaxNumber\n        if series_type == \""monotone_hits\"":\n            array = np.repeat(np.arange(MaxNumber), N // MaxNumber)\n        if series_type == \""monotone_misses\"":\n            array = np.arange(N) + MaxNumber\n    \n        self.series = Series(array).astype(dtype)\n    \n        self.values = np.arange(MaxNumber).astype(dtype.lower())""}",algos.isin.IsInLongSeriesLookUpDominates.time_isin,time,0.0
4030,param-level,"terminus-2,oracle",1.0557779017966336,1.0557779017966336,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.IsIn.time_isin_empty,algos.isin,"{""algos.isin.IsIn.time_isin_empty"": ""class IsIn:\n    def time_isin_empty(self, dtype):\n        self.series.isin([])\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)""}",algos.isin.IsIn.time_isin_empty,time,0.0
4028,param-level,"terminus-2,oracle",1.123615183139807,1.123615183139807,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algorithms.Quantile.time_quantile,algorithms,"{""algorithms.Quantile.time_quantile"": ""class Quantile:\n    def time_quantile(self, quantile, interpolation, dtype):\n        self.idx.quantile(quantile, interpolation=interpolation)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, quantile, interpolation, dtype):\n        N = 10**5\n        data = {\n            \""int\"": np.arange(N),\n            \""uint\"": np.arange(N).astype(np.uint64),\n            \""float\"": np.random.randn(N),\n        }\n        self.idx = pd.Series(data[dtype].repeat(5))""}",algorithms.Quantile.time_quantile,time,0.0
4034,param-level,"terminus-2,oracle",0.9014908348337756,0.9014908348337756,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.FrameWithFrameWide.time_op_different_blocks,arithmetic,"{""arithmetic.FrameWithFrameWide.time_op_different_blocks"": ""class FrameWithFrameWide:\n    def time_op_different_blocks(self, op, shape):\n        # blocks (and dtypes) are not aligned\n        op(self.left, self.right)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FrameWithFrameWide:\n    def setup(self, op, shape):\n        # we choose dtypes so as to make the blocks\n        #  a) not perfectly match between right and left\n        #  b) appreciably bigger than single columns\n        n_rows, n_cols = shape\n    \n        if op is operator.floordiv:\n            # floordiv is much slower than the other operations -> use less data\n            n_rows = n_rows // 10\n    \n        # construct dataframe with 2 blocks\n        arr1 = np.random.randn(n_rows, n_cols // 2).astype(\""f8\"")\n        arr2 = np.random.randn(n_rows, n_cols // 2).astype(\""f4\"")\n        df = pd.concat([DataFrame(arr1), DataFrame(arr2)], axis=1, ignore_index=True)\n        # should already be the case, but just to be sure\n        df._consolidate_inplace()\n    \n        # TODO: GH#33198 the setting here shouldn't need two steps\n        arr1 = np.random.randn(n_rows, max(n_cols // 4, 3)).astype(\""f8\"")\n        arr2 = np.random.randn(n_rows, n_cols // 2).astype(\""i8\"")\n        arr3 = np.random.randn(n_rows, n_cols // 4).astype(\""f8\"")\n        df2 = pd.concat(\n            [DataFrame(arr1), DataFrame(arr2), DataFrame(arr3)],\n            axis=1,\n            ignore_index=True,\n        )\n        # should already be the case, but just to be sure\n        df2._consolidate_inplace()\n    \n        self.left = df\n        self.right = df2""}",arithmetic.FrameWithFrameWide.time_op_different_blocks,time,0.0
4033,param-level,"terminus-2,oracle",1.0296213811038797,1.0296213811038797,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.IsinAlmostFullWithRandomInt.time_isin,algos.isin,"{""algos.isin.IsinAlmostFullWithRandomInt.time_isin"": ""class IsinAlmostFullWithRandomInt:\n    def time_isin(self, dtype, exponent, title):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, exponent, title):\n        M = 3 * 2 ** (exponent - 2)\n        # 0.77-the maximal share of occupied buckets\n        self.series = Series(np.random.randint(0, M, M)).astype(dtype)\n    \n        values = np.random.randint(0, M, M).astype(dtype)\n        if title == \""inside\"":\n            self.values = values\n        elif title == \""outside\"":\n            self.values = values + M\n        else:\n            raise ValueError(title)""}",algos.isin.IsinAlmostFullWithRandomInt.time_isin,time,0.0
4036,param-level,"terminus-2,oracle",0.9842875183363564,0.9842875183363564,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.IndexArithmetic.time_modulo,arithmetic,"{""arithmetic.IndexArithmetic.time_modulo"": ""class IndexArithmetic:\n    def time_modulo(self, dtype):\n        self.index % 2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IndexArithmetic:\n    def setup(self, dtype):\n        N = 10**6\n        indexes = {\""int\"": \""makeIntIndex\"", \""float\"": \""makeFloatIndex\""}\n        self.index = getattr(tm, indexes[dtype])(N)""}",arithmetic.IndexArithmetic.time_modulo,time,0.0
4035,param-level,"terminus-2,oracle",0.9604287049162952,0.9604287049162952,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.FrameWithFrameWide.time_op_same_blocks,arithmetic,"{""arithmetic.FrameWithFrameWide.time_op_same_blocks"": ""class FrameWithFrameWide:\n    def time_op_same_blocks(self, op, shape):\n        # blocks (and dtypes) are aligned\n        op(self.left, self.left)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FrameWithFrameWide:\n    def setup(self, op, shape):\n        # we choose dtypes so as to make the blocks\n        #  a) not perfectly match between right and left\n        #  b) appreciably bigger than single columns\n        n_rows, n_cols = shape\n    \n        if op is operator.floordiv:\n            # floordiv is much slower than the other operations -> use less data\n            n_rows = n_rows // 10\n    \n        # construct dataframe with 2 blocks\n        arr1 = np.random.randn(n_rows, n_cols // 2).astype(\""f8\"")\n        arr2 = np.random.randn(n_rows, n_cols // 2).astype(\""f4\"")\n        df = pd.concat([DataFrame(arr1), DataFrame(arr2)], axis=1, ignore_index=True)\n        # should already be the case, but just to be sure\n        df._consolidate_inplace()\n    \n        # TODO: GH#33198 the setting here shouldn't need two steps\n        arr1 = np.random.randn(n_rows, max(n_cols // 4, 3)).astype(\""f8\"")\n        arr2 = np.random.randn(n_rows, n_cols // 2).astype(\""i8\"")\n        arr3 = np.random.randn(n_rows, n_cols // 4).astype(\""f8\"")\n        df2 = pd.concat(\n            [DataFrame(arr1), DataFrame(arr2), DataFrame(arr3)],\n            axis=1,\n            ignore_index=True,\n        )\n        # should already be the case, but just to be sure\n        df2._consolidate_inplace()\n    \n        self.left = df\n        self.right = df2""}",arithmetic.FrameWithFrameWide.time_op_same_blocks,time,0.0
4039,param-level,"terminus-2,oracle",0.7970819164535795,0.7970819164535795,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.NumericInferOps.time_divide,arithmetic,"{""arithmetic.NumericInferOps.time_divide"": ""class NumericInferOps:\n    def time_divide(self, dtype):\n        self.df[\""A\""] / self.df[\""B\""]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericInferOps:\n    def setup(self, dtype):\n        N = 5 * 10**5\n        self.df = DataFrame(\n            {\""A\"": np.arange(N).astype(dtype), \""B\"": np.arange(N).astype(dtype)}\n        )""}",arithmetic.NumericInferOps.time_divide,time,0.0
4038,param-level,"terminus-2,oracle",0.9681320119640044,0.9681320119640044,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0,arithmetic,"{""arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0"": ""class MixedFrameWithSeriesAxis:\n    def time_frame_op_with_series_axis0(self, opname):\n        getattr(self.df, opname)(self.ser, axis=0)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MixedFrameWithSeriesAxis:\n    def setup(self, opname):\n        arr = np.arange(10**6).reshape(1000, -1)\n        df = DataFrame(arr)\n        df[\""C\""] = 1.0\n        self.df = df\n        self.ser = df[0]\n        self.row = df.iloc[0]""}",arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0,time,0.0
4037,param-level,"terminus-2,oracle",0.9813729259103554,0.9813729259103554,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.IntFrameWithScalar.time_frame_op_with_scalar,arithmetic,"{""arithmetic.IntFrameWithScalar.time_frame_op_with_scalar"": ""class IntFrameWithScalar:\n    def time_frame_op_with_scalar(self, dtype, scalar, op):\n        op(self.df, scalar)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IntFrameWithScalar:\n    def setup(self, dtype, scalar, op):\n        arr = np.random.randn(20000, 100)\n        self.df = DataFrame(arr.astype(dtype))""}",arithmetic.IntFrameWithScalar.time_frame_op_with_scalar,time,0.0
4020,param-level,"terminus-2,gpt-5",1.0102244904145388,1.0106040296936736,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampProperties.time_microsecond,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_microsecond"": ""class TimestampProperties:\n    def time_microsecond(self, tz):\n        self.ts.microsecond\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.TimestampProperties.time_microsecond,time,-0.00026841533177850824
4021,param-level,"terminus-2,gpt-5",0.9799252293572036,0.9791569957625458,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampProperties.time_month_name,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_month_name"": ""class TimestampProperties:\n    def time_month_name(self, tz):\n        self.ts.month_name()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.TimestampProperties.time_month_name,time,0.0005433052296024258
4022,param-level,"terminus-2,gpt-5",1.0207834401168423,1.026001270156748,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampProperties.time_quarter,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_quarter"": ""class TimestampProperties:\n    def time_quarter(self, tz):\n        self.ts.quarter\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.TimestampProperties.time_quarter,time,-0.0036901202545302153
4024,param-level,"terminus-2,gpt-5",1.024648415455555,1.0298420080843886,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime,tslibs.tslib,"{""tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime"": ""class TimeIntsToPydatetime:\n    def time_ints_to_pydatetime(self, box, size, tz):\n        ints_to_pydatetime(self.i8data, tz, box=box)\n\n    def setup(self, box, size, tz):\n        if box == \""date\"" and tz is not None:\n            # tz is ignored, so avoid running redundant benchmarks\n            raise NotImplementedError  # skip benchmark\n        if size == 10**6 and tz is _tzs[-1]:\n            # This is cumbersomely-slow, so skip to trim runtime\n            raise NotImplementedError  # skip benchmark\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime,time,-0.0036729792283122812
4025,param-level,"terminus-2,gpt-5",1.0284023470860206,0.9598299359561444,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.tz_convert.TimeTZConvert.time_tz_localize_to_utc,tslibs.tz_convert,"{""tslibs.tz_convert.TimeTZConvert.time_tz_localize_to_utc"": ""class TimeTZConvert:\n    def time_tz_localize_to_utc(self, size, tz):\n        # effectively:\n        #  dti = DatetimeIndex(self.i8data)\n        #  dti.tz_localize(tz, ambiguous=\""NaT\"", nonexistent=\""NaT\"")\n        tz_localize_to_utc(self.i8data, tz, ambiguous=\""NaT\"", nonexistent=\""NaT\"")\n\n    def setup(self, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.tz_convert.TimeTZConvert.time_tz_localize_to_utc,time,0.04849534026158149
4040,param-level,"terminus-2,oracle",0.9600290957696874,0.9600290957696874,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.OffsetArrayArithmetic.time_add_dti_offset,arithmetic,"{""arithmetic.OffsetArrayArithmetic.time_add_dti_offset"": ""class OffsetArrayArithmetic:\n    def time_add_dti_offset(self, offset):\n        with warnings.catch_warnings(record=True):\n            self.rng + offset\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass OffsetArrayArithmetic:\n    def setup(self, offset):\n        N = 10000\n        rng = date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"")\n        self.rng = rng\n        self.ser = Series(rng)""}",arithmetic.OffsetArrayArithmetic.time_add_dti_offset,time,0.0
4023,param-level,"terminus-2,gpt-5",1.0165091003831477,1.0142606399277831,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampProperties.time_week,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_week"": ""class TimestampProperties:\n    def time_week(self, tz):\n        self.ts.week\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.TimestampProperties.time_week,time,0.00159014176475568
4041,param-level,"terminus-2,oracle",0.9221873662977405,0.9221873662977405,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.OffsetArrayArithmetic.time_add_series_offset,arithmetic,"{""arithmetic.OffsetArrayArithmetic.time_add_series_offset"": ""class OffsetArrayArithmetic:\n    def time_add_series_offset(self, offset):\n        with warnings.catch_warnings(record=True):\n            self.ser + offset\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass OffsetArrayArithmetic:\n    def setup(self, offset):\n        N = 10000\n        rng = date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"")\n        self.rng = rng\n        self.ser = Series(rng)""}",arithmetic.OffsetArrayArithmetic.time_add_series_offset,time,0.0
4042,param-level,"terminus-2,oracle",0.9735601845496592,0.9735601845496592,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.Ops2.time_frame_dot,arithmetic,"{""arithmetic.Ops2.time_frame_dot"": ""class Ops2:\n    def time_frame_dot(self):\n        self.df.dot(self.df2)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Ops2:\n    def setup(self):\n        N = 10**3\n        self.df = DataFrame(np.random.randn(N, N))\n        self.df2 = DataFrame(np.random.randn(N, N))\n    \n        self.df_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n        self.df2_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n    \n        self.s = Series(np.random.randn(N))""}",arithmetic.Ops2.time_frame_dot,time,0.0
4047,param-level,"terminus-2,oracle",0.8968622270302342,0.8968622270302342,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.TimedeltaOps.time_add_td_ts,arithmetic,"{""arithmetic.TimedeltaOps.time_add_td_ts"": ""class TimedeltaOps:\n    def time_add_td_ts(self):\n        self.td + self.ts\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TimedeltaOps:\n    def setup(self):\n        self.td = to_timedelta(np.arange(1000000))\n        self.ts = Timestamp(\""2000\"")""}",arithmetic.TimedeltaOps.time_add_td_ts,time,0.0
4048,param-level,"terminus-2,oracle",0.7674428694827238,0.7674428694827238,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.Timeseries.time_series_timestamp_compare,arithmetic,"{""arithmetic.Timeseries.time_series_timestamp_compare"": ""class Timeseries:\n    def time_series_timestamp_compare(self, tz):\n        self.s <= self.ts\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Timeseries:\n    def setup(self, tz):\n        N = 10**6\n        halfway = (N // 2) - 1\n        self.s = Series(date_range(\""20010101\"", periods=N, freq=\""T\"", tz=tz))\n        self.ts = self.s[halfway]\n    \n        self.s2 = Series(date_range(\""20010101\"", periods=N, freq=\""s\"", tz=tz))""}",arithmetic.Timeseries.time_series_timestamp_compare,time,0.0
4043,param-level,"terminus-2,oracle",0.7364712027927393,0.7364712027927393,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.Ops2.time_frame_float_div_by_zero,arithmetic,"{""arithmetic.Ops2.time_frame_float_div_by_zero"": ""class Ops2:\n    def time_frame_float_div_by_zero(self):\n        self.df / 0\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Ops2:\n    def setup(self):\n        N = 10**3\n        self.df = DataFrame(np.random.randn(N, N))\n        self.df2 = DataFrame(np.random.randn(N, N))\n    \n        self.df_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n        self.df2_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n    \n        self.s = Series(np.random.randn(N))""}",arithmetic.Ops2.time_frame_float_div_by_zero,time,0.0
4049,param-level,"terminus-2,oracle",0.8652383137552341,0.8652383137552341,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.Timeseries.time_timestamp_ops_diff,arithmetic,"{""arithmetic.Timeseries.time_timestamp_ops_diff"": ""class Timeseries:\n    def time_timestamp_ops_diff(self, tz):\n        self.s2.diff()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Timeseries:\n    def setup(self, tz):\n        N = 10**6\n        halfway = (N // 2) - 1\n        self.s = Series(date_range(\""20010101\"", periods=N, freq=\""T\"", tz=tz))\n        self.ts = self.s[halfway]\n    \n        self.s2 = Series(date_range(\""20010101\"", periods=N, freq=\""s\"", tz=tz))""}",arithmetic.Timeseries.time_timestamp_ops_diff,time,0.0
4046,param-level,"terminus-2,oracle",0.9435501271235892,0.9435501271235892,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.Ops2.time_frame_series_dot,arithmetic,"{""arithmetic.Ops2.time_frame_series_dot"": ""class Ops2:\n    def time_frame_series_dot(self):\n        self.df.dot(self.s)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Ops2:\n    def setup(self):\n        N = 10**3\n        self.df = DataFrame(np.random.randn(N, N))\n        self.df2 = DataFrame(np.random.randn(N, N))\n    \n        self.df_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n        self.df2_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n    \n        self.s = Series(np.random.randn(N))""}",arithmetic.Ops2.time_frame_series_dot,time,0.0
4050,param-level,"terminus-2,oracle",0.9280316575600353,0.9280316575600353,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.Timeseries.time_timestamp_ops_diff_with_shift,arithmetic,"{""arithmetic.Timeseries.time_timestamp_ops_diff_with_shift"": ""class Timeseries:\n    def time_timestamp_ops_diff_with_shift(self, tz):\n        self.s - self.s.shift()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Timeseries:\n    def setup(self, tz):\n        N = 10**6\n        halfway = (N // 2) - 1\n        self.s = Series(date_range(\""20010101\"", periods=N, freq=\""T\"", tz=tz))\n        self.ts = self.s[halfway]\n    \n        self.s2 = Series(date_range(\""20010101\"", periods=N, freq=\""s\"", tz=tz))""}",arithmetic.Timeseries.time_timestamp_ops_diff_with_shift,time,0.0
4045,param-level,"terminus-2,oracle",0.839985324922907,0.839985324922907,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.Ops2.time_frame_int_div_by_zero,arithmetic,"{""arithmetic.Ops2.time_frame_int_div_by_zero"": ""class Ops2:\n    def time_frame_int_div_by_zero(self):\n        self.df_int / 0\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Ops2:\n    def setup(self):\n        N = 10**3\n        self.df = DataFrame(np.random.randn(N, N))\n        self.df2 = DataFrame(np.random.randn(N, N))\n    \n        self.df_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n        self.df2_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n    \n        self.s = Series(np.random.randn(N))""}",arithmetic.Ops2.time_frame_int_div_by_zero,time,0.0
4044,param-level,"terminus-2,oracle",0.9848606535373996,0.9848606535373996,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.Ops2.time_frame_float_mod,arithmetic,"{""arithmetic.Ops2.time_frame_float_mod"": ""class Ops2:\n    def time_frame_float_mod(self):\n        self.df % self.df2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Ops2:\n    def setup(self):\n        N = 10**3\n        self.df = DataFrame(np.random.randn(N, N))\n        self.df2 = DataFrame(np.random.randn(N, N))\n    \n        self.df_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n        self.df2_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n    \n        self.s = Series(np.random.randn(N))""}",arithmetic.Ops2.time_frame_float_mod,time,0.0
4059,param-level,"terminus-2,oracle",0.9587187321438082,0.9587187321438082,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,attrs_caching.SeriesArrayAttribute.time_array,attrs_caching,"{""attrs_caching.SeriesArrayAttribute.time_array"": ""class SeriesArrayAttribute:\n    def time_array(self, dtype):\n        self.series.array\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesArrayAttribute:\n    def setup(self, dtype):\n        if dtype == \""numeric\"":\n            self.series = pd.Series([1, 2, 3])\n        elif dtype == \""object\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=object)\n        elif dtype == \""category\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=\""category\"")\n        elif dtype == \""datetime64\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3))\n        elif dtype == \""datetime64tz\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3, tz=\""UTC\""))""}",attrs_caching.SeriesArrayAttribute.time_array,time,0.0
4054,param-level,"terminus-2,oracle",0.9782162306248984,0.9782162306248984,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,array.ArrowStringArray.time_tolist,array,"{""array.ArrowStringArray.time_tolist"": ""class ArrowStringArray:\n    def time_tolist(self, multiple_chunks):\n        self.array.tolist()\n\n    def setup(self, multiple_chunks):\n        try:\n            import pyarrow as pa\n        except ImportError:\n            raise NotImplementedError\n        strings = tm.rands_array(3, 10_000)\n        if multiple_chunks:\n            chunks = [strings[i : i + 100] for i in range(0, len(strings), 100)]\n            self.array = pd.arrays.ArrowStringArray(pa.chunked_array(chunks))\n        else:\n            self.array = pd.arrays.ArrowStringArray(pa.array(strings))""}",array.ArrowStringArray.time_tolist,time,0.0
4051,param-level,"terminus-2,oracle",0.734412543616603,0.734412543616603,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.Timeseries.time_timestamp_series_compare,arithmetic,"{""arithmetic.Timeseries.time_timestamp_series_compare"": ""class Timeseries:\n    def time_timestamp_series_compare(self, tz):\n        self.ts >= self.s\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Timeseries:\n    def setup(self, tz):\n        N = 10**6\n        halfway = (N // 2) - 1\n        self.s = Series(date_range(\""20010101\"", periods=N, freq=\""T\"", tz=tz))\n        self.ts = self.s[halfway]\n    \n        self.s2 = Series(date_range(\""20010101\"", periods=N, freq=\""s\"", tz=tz))""}",arithmetic.Timeseries.time_timestamp_series_compare,time,0.0
4061,param-level,"terminus-2,oracle",0.9759908567977325,0.9759908567977325,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,attrs_caching.SeriesArrayAttribute.time_extract_array_numpy,attrs_caching,"{""attrs_caching.SeriesArrayAttribute.time_extract_array_numpy"": ""class SeriesArrayAttribute:\n    def time_extract_array_numpy(self, dtype):\n        extract_array(self.series, extract_numpy=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesArrayAttribute:\n    def setup(self, dtype):\n        if dtype == \""numeric\"":\n            self.series = pd.Series([1, 2, 3])\n        elif dtype == \""object\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=object)\n        elif dtype == \""category\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=\""category\"")\n        elif dtype == \""datetime64\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3))\n        elif dtype == \""datetime64tz\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3, tz=\""UTC\""))""}",attrs_caching.SeriesArrayAttribute.time_extract_array_numpy,time,0.0
4055,param-level,"terminus-2,oracle",0.981116408258753,0.981116408258753,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,array.BooleanArray.time_from_bool_array,array,"{""array.BooleanArray.time_from_bool_array"": ""class BooleanArray:\n    def time_from_bool_array(self):\n        pd.array(self.values_bool, dtype=\""boolean\"")\n\n    def setup(self):\n        self.values_bool = np.array([True, False, True, False])\n        self.values_float = np.array([1.0, 0.0, 1.0, 0.0])\n        self.values_integer = np.array([1, 0, 1, 0])\n        self.values_integer_like = [1, 0, 1, 0]\n        self.data = np.array([True, False, True, False])\n        self.mask = np.array([False, False, True, False])""}",array.BooleanArray.time_from_bool_array,time,0.0
4058,param-level,"terminus-2,oracle",0.8975931204235437,0.8975931204235437,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,array.StringArray.time_from_list,array,"{""array.StringArray.time_from_list"": ""class StringArray:\n    def time_from_list(self):\n        pd.array(self.values_list, dtype=\""string\"")\n\n    def setup(self):\n        N = 100_000\n        values = tm.rands_array(3, N)\n        self.values_obj = np.array(values, dtype=\""object\"")\n        self.values_str = np.array(values, dtype=\""U\"")\n        self.values_list = values.tolist()""}",array.StringArray.time_from_list,time,0.0
4057,param-level,"terminus-2,oracle",0.904333635973276,0.904333635973276,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,array.IntervalArray.time_from_tuples,array,"{""array.IntervalArray.time_from_tuples"": ""class IntervalArray:\n    def time_from_tuples(self):\n        pd.arrays.IntervalArray.from_tuples(self.tuples)\n\n    def setup(self):\n        N = 10_000\n        self.tuples = [(i, i + 1) for i in range(N)]""}",array.IntervalArray.time_from_tuples,time,0.0
4052,param-level,"terminus-2,oracle",0.9620389002167072,0.9620389002167072,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,array.ArrowExtensionArray.time_to_numpy,array,"{""array.ArrowExtensionArray.time_to_numpy"": ""class ArrowExtensionArray:\n    def time_to_numpy(self, dtype, hasna):\n        self.arr.to_numpy()\n\n    def setup(self, dtype, hasna):\n        N = 100_000\n        if dtype == \""boolean[pyarrow]\"":\n            data = np.random.choice([True, False], N, replace=True)\n        elif dtype == \""float64[pyarrow]\"":\n            data = np.random.randn(N)\n        elif dtype == \""int64[pyarrow]\"":\n            data = np.arange(N)\n        elif dtype == \""string[pyarrow]\"":\n            data = tm.rands_array(10, N)\n        elif dtype == \""timestamp[ns][pyarrow]\"":\n            data = pd.date_range(\""2000-01-01\"", freq=\""s\"", periods=N)\n        else:\n            raise NotImplementedError\n    \n        arr = pd.array(data, dtype=dtype)\n        if hasna:\n            arr[::2] = pd.NA\n        self.arr = arr""}",array.ArrowExtensionArray.time_to_numpy,time,0.0
4065,param-level,"terminus-2,oracle",0.9588389898491556,0.9588389898491556,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.CategoricalSlicing.time_getitem_list,categoricals,"{""categoricals.CategoricalSlicing.time_getitem_list"": ""class CategoricalSlicing:\n    def time_getitem_list(self, index):\n        self.data[self.list]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalSlicing:\n    def setup(self, index):\n        N = 10**6\n        categories = [\""a\"", \""b\"", \""c\""]\n        values = [0] * N + [1] * N + [2] * N\n        if index == \""monotonic_incr\"":\n            self.data = pd.Categorical.from_codes(values, categories=categories)\n        elif index == \""monotonic_decr\"":\n            self.data = pd.Categorical.from_codes(\n                list(reversed(values)), categories=categories\n            )\n        elif index == \""non_monotonic\"":\n            self.data = pd.Categorical.from_codes([0, 1, 2] * N, categories=categories)\n        else:\n            raise ValueError(f\""Invalid index param: {index}\"")\n    \n        self.scalar = 10000\n        self.list = list(range(10000))\n        self.cat_scalar = \""b\""""}",categoricals.CategoricalSlicing.time_getitem_list,time,0.0
4067,param-level,"terminus-2,oracle",0.9820553921372688,0.9820553921372688,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.Concat.time_append_overlapping_index,categoricals,"{""categoricals.Concat.time_append_overlapping_index"": ""class Concat:\n    def time_append_overlapping_index(self):\n        self.idx_a.append(self.idx_a)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Concat:\n    def setup(self):\n        N = 10**5\n        self.s = pd.Series(list(\""aabbcd\"") * N).astype(\""category\"")\n    \n        self.a = pd.Categorical(list(\""aabbcd\"") * N)\n        self.b = pd.Categorical(list(\""bbcdjk\"") * N)\n    \n        self.idx_a = pd.CategoricalIndex(range(N), range(N))\n        self.idx_b = pd.CategoricalIndex(range(N + 1), range(N + 1))\n        self.df_a = pd.DataFrame(range(N), columns=[\""a\""], index=self.idx_a)\n        self.df_b = pd.DataFrame(range(N + 1), columns=[\""a\""], index=self.idx_b)""}",categoricals.Concat.time_append_overlapping_index,time,0.0
4068,param-level,"terminus-2,oracle",0.97934611144595,0.97934611144595,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.Constructor.time_all_nan,categoricals,"{""categoricals.Constructor.time_all_nan"": ""class Constructor:\n    def time_all_nan(self):\n        pd.Categorical(self.values_all_nan)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Constructor:\n    def setup(self):\n        N = 10**5\n        self.categories = list(\""abcde\"")\n        self.cat_idx = pd.Index(self.categories)\n        self.values = np.tile(self.categories, N)\n        self.codes = np.tile(range(len(self.categories)), N)\n    \n        self.datetimes = pd.Series(\n            pd.date_range(\""1995-01-01 00:00:00\"", periods=N / 10, freq=\""s\"")\n        )\n        self.datetimes_with_nat = self.datetimes.copy()\n        self.datetimes_with_nat.iloc[-1] = pd.NaT\n    \n        self.values_some_nan = list(np.tile(self.categories + [np.nan], N))\n        self.values_all_nan = [np.nan] * len(self.values)\n        self.values_all_int8 = np.ones(N, \""int8\"")\n        self.categorical = pd.Categorical(self.values, self.categories)\n        self.series = pd.Series(self.categorical)\n        self.intervals = pd.interval_range(0, 1, periods=N // 10)""}",categoricals.Constructor.time_all_nan,time,0.0
4053,param-level,"terminus-2,oracle",0.9786944912772296,0.9786944912772296,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,array.ArrowStringArray.time_setitem_list,array,"{""array.ArrowStringArray.time_setitem_list"": ""class ArrowStringArray:\n    def time_setitem_list(self, multiple_chunks):\n        indexer = list(range(0, 50)) + list(range(-1000, 0, 50))\n        self.array[indexer] = [\""foo\""] * len(indexer)\n\n    def setup(self, multiple_chunks):\n        try:\n            import pyarrow as pa\n        except ImportError:\n            raise NotImplementedError\n        strings = tm.rands_array(3, 10_000)\n        if multiple_chunks:\n            chunks = [strings[i : i + 100] for i in range(0, len(strings), 100)]\n            self.array = pd.arrays.ArrowStringArray(pa.chunked_array(chunks))\n        else:\n            self.array = pd.arrays.ArrowStringArray(pa.array(strings))""}",array.ArrowStringArray.time_setitem_list,time,0.0
4063,param-level,"terminus-2,oracle",0.9732328002519358,0.9732328002519358,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,boolean.TimeLogicalOps.time_xor_array,boolean,"{""boolean.TimeLogicalOps.time_xor_array"": ""class TimeLogicalOps:\n    def time_xor_array(self):\n        self.left ^ self.right\n\n    def setup(self):\n        N = 10_000\n        left, right, lmask, rmask = np.random.randint(0, 2, size=(4, N)).astype(\""bool\"")\n        self.left = pd.arrays.BooleanArray(left, lmask)\n        self.right = pd.arrays.BooleanArray(right, rmask)""}",boolean.TimeLogicalOps.time_xor_array,time,0.0
4074,param-level,"terminus-2,oracle",0.9471538882319472,0.9471538882319472,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.Constructor.time_with_nan,categoricals,"{""categoricals.Constructor.time_with_nan"": ""class Constructor:\n    def time_with_nan(self):\n        pd.Categorical(self.values_some_nan)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Constructor:\n    def setup(self):\n        N = 10**5\n        self.categories = list(\""abcde\"")\n        self.cat_idx = pd.Index(self.categories)\n        self.values = np.tile(self.categories, N)\n        self.codes = np.tile(range(len(self.categories)), N)\n    \n        self.datetimes = pd.Series(\n            pd.date_range(\""1995-01-01 00:00:00\"", periods=N / 10, freq=\""s\"")\n        )\n        self.datetimes_with_nat = self.datetimes.copy()\n        self.datetimes_with_nat.iloc[-1] = pd.NaT\n    \n        self.values_some_nan = list(np.tile(self.categories + [np.nan], N))\n        self.values_all_nan = [np.nan] * len(self.values)\n        self.values_all_int8 = np.ones(N, \""int8\"")\n        self.categorical = pd.Categorical(self.values, self.categories)\n        self.series = pd.Series(self.categorical)\n        self.intervals = pd.interval_range(0, 1, periods=N // 10)""}",categoricals.Constructor.time_with_nan,time,0.0
4070,param-level,"terminus-2,oracle",0.9466848608289778,0.9466848608289778,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.Constructor.time_existing_series,categoricals,"{""categoricals.Constructor.time_existing_series"": ""class Constructor:\n    def time_existing_series(self):\n        pd.Categorical(self.series)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Constructor:\n    def setup(self):\n        N = 10**5\n        self.categories = list(\""abcde\"")\n        self.cat_idx = pd.Index(self.categories)\n        self.values = np.tile(self.categories, N)\n        self.codes = np.tile(range(len(self.categories)), N)\n    \n        self.datetimes = pd.Series(\n            pd.date_range(\""1995-01-01 00:00:00\"", periods=N / 10, freq=\""s\"")\n        )\n        self.datetimes_with_nat = self.datetimes.copy()\n        self.datetimes_with_nat.iloc[-1] = pd.NaT\n    \n        self.values_some_nan = list(np.tile(self.categories + [np.nan], N))\n        self.values_all_nan = [np.nan] * len(self.values)\n        self.values_all_int8 = np.ones(N, \""int8\"")\n        self.categorical = pd.Categorical(self.values, self.categories)\n        self.series = pd.Series(self.categorical)\n        self.intervals = pd.interval_range(0, 1, periods=N // 10)""}",categoricals.Constructor.time_existing_series,time,0.0
4073,param-level,"terminus-2,oracle",0.9665723228353378,0.9665723228353378,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.Constructor.time_interval,categoricals,"{""categoricals.Constructor.time_interval"": ""class Constructor:\n    def time_interval(self):\n        pd.Categorical(self.datetimes, categories=self.datetimes)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Constructor:\n    def setup(self):\n        N = 10**5\n        self.categories = list(\""abcde\"")\n        self.cat_idx = pd.Index(self.categories)\n        self.values = np.tile(self.categories, N)\n        self.codes = np.tile(range(len(self.categories)), N)\n    \n        self.datetimes = pd.Series(\n            pd.date_range(\""1995-01-01 00:00:00\"", periods=N / 10, freq=\""s\"")\n        )\n        self.datetimes_with_nat = self.datetimes.copy()\n        self.datetimes_with_nat.iloc[-1] = pd.NaT\n    \n        self.values_some_nan = list(np.tile(self.categories + [np.nan], N))\n        self.values_all_nan = [np.nan] * len(self.values)\n        self.values_all_int8 = np.ones(N, \""int8\"")\n        self.categorical = pd.Categorical(self.values, self.categories)\n        self.series = pd.Series(self.categorical)\n        self.intervals = pd.interval_range(0, 1, periods=N // 10)""}",categoricals.Constructor.time_interval,time,0.0
4072,param-level,"terminus-2,oracle",0.9586333781694028,0.9586333781694028,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.Constructor.time_from_codes_all_int8,categoricals,"{""categoricals.Constructor.time_from_codes_all_int8"": ""class Constructor:\n    def time_from_codes_all_int8(self):\n        pd.Categorical.from_codes(self.values_all_int8, self.categories)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Constructor:\n    def setup(self):\n        N = 10**5\n        self.categories = list(\""abcde\"")\n        self.cat_idx = pd.Index(self.categories)\n        self.values = np.tile(self.categories, N)\n        self.codes = np.tile(range(len(self.categories)), N)\n    \n        self.datetimes = pd.Series(\n            pd.date_range(\""1995-01-01 00:00:00\"", periods=N / 10, freq=\""s\"")\n        )\n        self.datetimes_with_nat = self.datetimes.copy()\n        self.datetimes_with_nat.iloc[-1] = pd.NaT\n    \n        self.values_some_nan = list(np.tile(self.categories + [np.nan], N))\n        self.values_all_nan = [np.nan] * len(self.values)\n        self.values_all_int8 = np.ones(N, \""int8\"")\n        self.categorical = pd.Categorical(self.values, self.categories)\n        self.series = pd.Series(self.categorical)\n        self.intervals = pd.interval_range(0, 1, periods=N // 10)""}",categoricals.Constructor.time_from_codes_all_int8,time,0.0
4064,param-level,"terminus-2,oracle",0.9347060925143976,0.9347060925143976,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.CategoricalSlicing.time_getitem_bool_array,categoricals,"{""categoricals.CategoricalSlicing.time_getitem_bool_array"": ""class CategoricalSlicing:\n    def time_getitem_bool_array(self, index):\n        self.data[self.data == self.cat_scalar]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalSlicing:\n    def setup(self, index):\n        N = 10**6\n        categories = [\""a\"", \""b\"", \""c\""]\n        values = [0] * N + [1] * N + [2] * N\n        if index == \""monotonic_incr\"":\n            self.data = pd.Categorical.from_codes(values, categories=categories)\n        elif index == \""monotonic_decr\"":\n            self.data = pd.Categorical.from_codes(\n                list(reversed(values)), categories=categories\n            )\n        elif index == \""non_monotonic\"":\n            self.data = pd.Categorical.from_codes([0, 1, 2] * N, categories=categories)\n        else:\n            raise ValueError(f\""Invalid index param: {index}\"")\n    \n        self.scalar = 10000\n        self.list = list(range(10000))\n        self.cat_scalar = \""b\""""}",categoricals.CategoricalSlicing.time_getitem_bool_array,time,0.0
4071,param-level,"terminus-2,oracle",0.9435737819115144,0.9435737819115144,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.Constructor.time_fastpath,categoricals,"{""categoricals.Constructor.time_fastpath"": ""class Constructor:\n    def time_fastpath(self):\n        pd.Categorical(self.codes, self.cat_idx, fastpath=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Constructor:\n    def setup(self):\n        N = 10**5\n        self.categories = list(\""abcde\"")\n        self.cat_idx = pd.Index(self.categories)\n        self.values = np.tile(self.categories, N)\n        self.codes = np.tile(range(len(self.categories)), N)\n    \n        self.datetimes = pd.Series(\n            pd.date_range(\""1995-01-01 00:00:00\"", periods=N / 10, freq=\""s\"")\n        )\n        self.datetimes_with_nat = self.datetimes.copy()\n        self.datetimes_with_nat.iloc[-1] = pd.NaT\n    \n        self.values_some_nan = list(np.tile(self.categories + [np.nan], N))\n        self.values_all_nan = [np.nan] * len(self.values)\n        self.values_all_int8 = np.ones(N, \""int8\"")\n        self.categorical = pd.Categorical(self.values, self.categories)\n        self.series = pd.Series(self.categorical)\n        self.intervals = pd.interval_range(0, 1, periods=N // 10)""}",categoricals.Constructor.time_fastpath,time,0.0
4062,param-level,"terminus-2,oracle",0.9690018695661616,0.9690018695661616,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,boolean.TimeLogicalOps.time_and_scalar,boolean,"{""boolean.TimeLogicalOps.time_and_scalar"": ""class TimeLogicalOps:\n    def time_and_scalar(self):\n        self.left & True\n        self.left & False\n\n    def setup(self):\n        N = 10_000\n        left, right, lmask, rmask = np.random.randint(0, 2, size=(4, N)).astype(\""bool\"")\n        self.left = pd.arrays.BooleanArray(left, lmask)\n        self.right = pd.arrays.BooleanArray(right, rmask)""}",boolean.TimeLogicalOps.time_and_scalar,time,0.0
4060,param-level,"terminus-2,oracle",0.9725705610801374,0.9725705610801374,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,attrs_caching.SeriesArrayAttribute.time_extract_array,attrs_caching,"{""attrs_caching.SeriesArrayAttribute.time_extract_array"": ""class SeriesArrayAttribute:\n    def time_extract_array(self, dtype):\n        extract_array(self.series)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesArrayAttribute:\n    def setup(self, dtype):\n        if dtype == \""numeric\"":\n            self.series = pd.Series([1, 2, 3])\n        elif dtype == \""object\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=object)\n        elif dtype == \""category\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=\""category\"")\n        elif dtype == \""datetime64\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3))\n        elif dtype == \""datetime64tz\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3, tz=\""UTC\""))""}",attrs_caching.SeriesArrayAttribute.time_extract_array,time,0.0
4066,param-level,"terminus-2,oracle",0.9825157394271008,0.9825157394271008,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.CategoricalSlicing.time_getitem_scalar,categoricals,"{""categoricals.CategoricalSlicing.time_getitem_scalar"": ""class CategoricalSlicing:\n    def time_getitem_scalar(self, index):\n        self.data[self.scalar]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalSlicing:\n    def setup(self, index):\n        N = 10**6\n        categories = [\""a\"", \""b\"", \""c\""]\n        values = [0] * N + [1] * N + [2] * N\n        if index == \""monotonic_incr\"":\n            self.data = pd.Categorical.from_codes(values, categories=categories)\n        elif index == \""monotonic_decr\"":\n            self.data = pd.Categorical.from_codes(\n                list(reversed(values)), categories=categories\n            )\n        elif index == \""non_monotonic\"":\n            self.data = pd.Categorical.from_codes([0, 1, 2] * N, categories=categories)\n        else:\n            raise ValueError(f\""Invalid index param: {index}\"")\n    \n        self.scalar = 10000\n        self.list = list(range(10000))\n        self.cat_scalar = \""b\""""}",categoricals.CategoricalSlicing.time_getitem_scalar,time,0.0
4056,param-level,"terminus-2,oracle",0.963676491180532,0.963676491180532,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,array.IntegerArray.time_constructor,array,"{""array.IntegerArray.time_constructor"": ""class IntegerArray:\n    def time_constructor(self):\n        pd.arrays.IntegerArray(self.data, self.mask)\n\n    def setup(self):\n        N = 250_000\n        self.values_integer = np.array([1, 0, 1, 0] * N)\n        self.data = np.array([1, 2, 3, 4] * N, dtype=\""int64\"")\n        self.mask = np.array([False, False, True, False] * N)""}",array.IntegerArray.time_constructor,time,0.0
4082,param-level,"terminus-2,oracle",0.7523904029259222,0.7523904029259222,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.RemoveCategories.time_remove_categories,categoricals,"{""categoricals.RemoveCategories.time_remove_categories"": ""class RemoveCategories:\n    def time_remove_categories(self):\n        self.ts.cat.remove_categories(self.ts.cat.categories[::2])\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass RemoveCategories:\n    def setup(self):\n        n = 5 * 10**5\n        arr = [f\""s{i:04d}\"" for i in np.random.randint(0, n // 10, size=n)]\n        self.ts = pd.Series(arr).astype(\""category\"")""}",categoricals.RemoveCategories.time_remove_categories,time,0.0
4069,param-level,"terminus-2,oracle",0.9492977282479867,0.9492977282479867,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.Constructor.time_datetimes,categoricals,"{""categoricals.Constructor.time_datetimes"": ""class Constructor:\n    def time_datetimes(self):\n        pd.Categorical(self.datetimes)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Constructor:\n    def setup(self):\n        N = 10**5\n        self.categories = list(\""abcde\"")\n        self.cat_idx = pd.Index(self.categories)\n        self.values = np.tile(self.categories, N)\n        self.codes = np.tile(range(len(self.categories)), N)\n    \n        self.datetimes = pd.Series(\n            pd.date_range(\""1995-01-01 00:00:00\"", periods=N / 10, freq=\""s\"")\n        )\n        self.datetimes_with_nat = self.datetimes.copy()\n        self.datetimes_with_nat.iloc[-1] = pd.NaT\n    \n        self.values_some_nan = list(np.tile(self.categories + [np.nan], N))\n        self.values_all_nan = [np.nan] * len(self.values)\n        self.values_all_int8 = np.ones(N, \""int8\"")\n        self.categorical = pd.Categorical(self.values, self.categories)\n        self.series = pd.Series(self.categorical)\n        self.intervals = pd.interval_range(0, 1, periods=N // 10)""}",categoricals.Constructor.time_datetimes,time,0.0
4075,param-level,"terminus-2,oracle",0.8987922682585705,0.8987922682585705,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.Indexing.time_align,categoricals,"{""categoricals.Indexing.time_align"": ""class Indexing:\n    def time_align(self):\n        pd.DataFrame({\""a\"": self.series, \""b\"": self.series[:500]})\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self):\n        N = 10**5\n        self.index = pd.CategoricalIndex(range(N), range(N))\n        self.series = pd.Series(range(N), index=self.index).sort_index()\n        self.category = self.index[500]""}",categoricals.Indexing.time_align,time,0.0
4077,param-level,"terminus-2,oracle",0.9865113562865824,0.9865113562865824,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.Indexing.time_shallow_copy,categoricals,"{""categoricals.Indexing.time_shallow_copy"": ""class Indexing:\n    def time_shallow_copy(self):\n        self.index._view()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self):\n        N = 10**5\n        self.index = pd.CategoricalIndex(range(N), range(N))\n        self.series = pd.Series(range(N), index=self.index).sort_index()\n        self.category = self.index[500]""}",categoricals.Indexing.time_shallow_copy,time,0.0
4080,param-level,"terminus-2,oracle",0.8989628497659634,0.8989628497659634,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.Rank.time_rank_int,categoricals,"{""categoricals.Rank.time_rank_int"": ""class Rank:\n    def time_rank_int(self):\n        self.s_int.rank()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Rank:\n    def setup(self):\n        N = 10**5\n        ncats = 15\n    \n        self.s_str = pd.Series(tm.makeCategoricalIndex(N, ncats)).astype(str)\n        self.s_str_cat = pd.Series(self.s_str, dtype=\""category\"")\n        with warnings.catch_warnings(record=True):\n            str_cat_type = pd.CategoricalDtype(set(self.s_str), ordered=True)\n            self.s_str_cat_ordered = self.s_str.astype(str_cat_type)\n    \n        self.s_int = pd.Series(np.random.randint(0, ncats, size=N))\n        self.s_int_cat = pd.Series(self.s_int, dtype=\""category\"")\n        with warnings.catch_warnings(record=True):\n            int_cat_type = pd.CategoricalDtype(set(self.s_int), ordered=True)\n            self.s_int_cat_ordered = self.s_int.astype(int_cat_type)""}",categoricals.Rank.time_rank_int,time,0.0
4081,param-level,"terminus-2,oracle",0.8876400520380507,0.8876400520380507,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.Rank.time_rank_string_cat,categoricals,"{""categoricals.Rank.time_rank_string_cat"": ""class Rank:\n    def time_rank_string_cat(self):\n        self.s_str_cat.rank()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Rank:\n    def setup(self):\n        N = 10**5\n        ncats = 15\n    \n        self.s_str = pd.Series(tm.makeCategoricalIndex(N, ncats)).astype(str)\n        self.s_str_cat = pd.Series(self.s_str, dtype=\""category\"")\n        with warnings.catch_warnings(record=True):\n            str_cat_type = pd.CategoricalDtype(set(self.s_str), ordered=True)\n            self.s_str_cat_ordered = self.s_str.astype(str_cat_type)\n    \n        self.s_int = pd.Series(np.random.randint(0, ncats, size=N))\n        self.s_int_cat = pd.Series(self.s_int, dtype=\""category\"")\n        with warnings.catch_warnings(record=True):\n            int_cat_type = pd.CategoricalDtype(set(self.s_int), ordered=True)\n            self.s_int_cat_ordered = self.s_int.astype(int_cat_type)""}",categoricals.Rank.time_rank_string_cat,time,0.0
4076,param-level,"terminus-2,oracle",0.9182493888704965,0.9182493888704965,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.Indexing.time_reindex_missing,categoricals,"{""categoricals.Indexing.time_reindex_missing"": ""class Indexing:\n    def time_reindex_missing(self):\n        self.index.reindex([\""a\"", \""b\"", \""c\"", \""d\""])\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self):\n        N = 10**5\n        self.index = pd.CategoricalIndex(range(N), range(N))\n        self.series = pd.Series(range(N), index=self.index).sort_index()\n        self.category = self.index[500]""}",categoricals.Indexing.time_reindex_missing,time,0.0
4079,param-level,"terminus-2,oracle",0.9777255532200974,0.9777255532200974,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.Indexing.time_unique,categoricals,"{""categoricals.Indexing.time_unique"": ""class Indexing:\n    def time_unique(self):\n        self.index.unique()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self):\n        N = 10**5\n        self.index = pd.CategoricalIndex(range(N), range(N))\n        self.series = pd.Series(range(N), index=self.index).sort_index()\n        self.category = self.index[500]""}",categoricals.Indexing.time_unique,time,0.0
4088,param-level,"terminus-2,oracle",0.9843871123238844,0.9843871123238844,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.DtypesInvalid.time_pandas_dtype_invalid,dtypes,"{""dtypes.DtypesInvalid.time_pandas_dtype_invalid"": ""class DtypesInvalid:\n    def time_pandas_dtype_invalid(self, dtype):\n        try:\n            pandas_dtype(self.data_dict[dtype])\n        except TypeError:\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)""}",dtypes.DtypesInvalid.time_pandas_dtype_invalid,time,0.0
4078,param-level,"terminus-2,oracle",0.9514097224264996,0.9514097224264996,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.Indexing.time_sort_values,categoricals,"{""categoricals.Indexing.time_sort_values"": ""class Indexing:\n    def time_sort_values(self):\n        self.index.sort_values(ascending=False)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self):\n        N = 10**5\n        self.index = pd.CategoricalIndex(range(N), range(N))\n        self.series = pd.Series(range(N), index=self.index).sort_index()\n        self.category = self.index[500]""}",categoricals.Indexing.time_sort_values,time,0.0
4093,param-level,"terminus-2,oracle",0.950460614656845,0.950460614656845,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.SelectDtypes.time_select_dtype_string_include,dtypes,"{""dtypes.SelectDtypes.time_select_dtype_string_include"": ""class SelectDtypes:\n    def time_select_dtype_string_include(self, dtype):\n        self.df_string.select_dtypes(include=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )""}",dtypes.SelectDtypes.time_select_dtype_string_include,time,0.0
4083,param-level,"terminus-2,oracle",0.9760363826929792,0.9760363826929792,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.SearchSorted.time_categorical_contains,categoricals,"{""categoricals.SearchSorted.time_categorical_contains"": ""class SearchSorted:\n    def time_categorical_contains(self):\n        self.c.searchsorted(self.key)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SearchSorted:\n    def setup(self):\n        N = 10**5\n        self.ci = tm.makeCategoricalIndex(N).sort_values()\n        self.c = self.ci.values\n        self.key = self.ci.categories[1]""}",categoricals.SearchSorted.time_categorical_contains,time,0.0
4087,param-level,"terminus-2,oracle",1.0235979911278916,1.0235979911278916,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.Dtypes.time_pandas_dtype,dtypes,"{""dtypes.Dtypes.time_pandas_dtype"": ""class Dtypes:\n    def time_pandas_dtype(self, dtype):\n        pandas_dtype(dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)""}",dtypes.Dtypes.time_pandas_dtype,time,0.0
4086,param-level,"terminus-2,oracle",0.9075099787978216,0.9075099787978216,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,ctors.SeriesConstructors.time_series_constructor,ctors,"{""ctors.SeriesConstructors.time_series_constructor"": ""class SeriesConstructors:\n    def time_series_constructor(self, data_fmt, with_index, dtype):\n        Series(self.data, index=self.index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesConstructors:\n    def setup(self, data_fmt, with_index, dtype):\n        if data_fmt in (gen_of_str, gen_of_tuples) and with_index:\n            raise NotImplementedError(\n                \""Series constructors do not support using generators with indexes\""\n            )\n        N = 10**4\n        if dtype == \""float\"":\n            arr = np.random.randn(N)\n        else:\n            arr = np.arange(N)\n        self.data = data_fmt(arr)\n        self.index = np.arange(N) if with_index else None""}",ctors.SeriesConstructors.time_series_constructor,time,0.0
4084,param-level,"terminus-2,oracle",0.696597972308575,0.696597972308575,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.SetCategories.time_set_categories,categoricals,"{""categoricals.SetCategories.time_set_categories"": ""class SetCategories:\n    def time_set_categories(self):\n        self.ts.cat.set_categories(self.ts.cat.categories[::2])\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SetCategories:\n    def setup(self):\n        n = 5 * 10**5\n        arr = [f\""s{i:04d}\"" for i in np.random.randint(0, n // 10, size=n)]\n        self.ts = pd.Series(arr).astype(\""category\"")""}",categoricals.SetCategories.time_set_categories,time,0.0
4085,param-level,"terminus-2,oracle",0.9767446348584446,0.9767446348584446,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,ctors.DatetimeIndexConstructor.time_from_list_of_dates,ctors,"{""ctors.DatetimeIndexConstructor.time_from_list_of_dates"": ""class DatetimeIndexConstructor:\n    def time_from_list_of_dates(self):\n        DatetimeIndex(self.list_of_dates)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndexConstructor:\n    def setup(self):\n        N = 20_000\n        dti = date_range(\""1900-01-01\"", periods=N)\n    \n        self.list_of_timestamps = dti.tolist()\n        self.list_of_dates = dti.date.tolist()\n        self.list_of_datetimes = dti.to_pydatetime().tolist()\n        self.list_of_str = dti.strftime(\""%Y-%m-%d\"").tolist()""}",ctors.DatetimeIndexConstructor.time_from_list_of_dates,time,0.0
4095,param-level,"terminus-2,oracle",1.1345405893436904,1.1345405893436904,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.Eval.time_and,eval,"{""eval.Eval.time_and"": ""class Eval:\n    def time_and(self, engine, threads):\n        pd.eval(\n            \""(self.df > 0) & (self.df2 > 0) & (self.df3 > 0) & (self.df4 > 0)\"",\n            engine=engine,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)""}",eval.Eval.time_and,time,0.0
4092,param-level,"terminus-2,oracle",1.1441580130964004,1.1441580130964004,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.SelectDtypes.time_select_dtype_string_exclude,dtypes,"{""dtypes.SelectDtypes.time_select_dtype_string_exclude"": ""class SelectDtypes:\n    def time_select_dtype_string_exclude(self, dtype):\n        self.df_string.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )""}",dtypes.SelectDtypes.time_select_dtype_string_exclude,time,0.0
4089,param-level,"terminus-2,oracle",0.9307347810611114,0.9307347810611114,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.SelectDtypes.time_select_dtype_bool_include,dtypes,"{""dtypes.SelectDtypes.time_select_dtype_bool_include"": ""class SelectDtypes:\n    def time_select_dtype_bool_include(self, dtype):\n        self.df_bool.select_dtypes(include=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )""}",dtypes.SelectDtypes.time_select_dtype_bool_include,time,0.0
4090,param-level,"terminus-2,oracle",0.9469967072015046,0.9469967072015046,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.SelectDtypes.time_select_dtype_float_include,dtypes,"{""dtypes.SelectDtypes.time_select_dtype_float_include"": ""class SelectDtypes:\n    def time_select_dtype_float_include(self, dtype):\n        self.df_float.select_dtypes(include=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )""}",dtypes.SelectDtypes.time_select_dtype_float_include,time,0.0
4096,param-level,"terminus-2,oracle",1.159924219526538,1.159924219526538,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.Eval.time_chained_cmp,eval,"{""eval.Eval.time_chained_cmp"": ""class Eval:\n    def time_chained_cmp(self, engine, threads):\n        pd.eval(\""self.df < self.df2 < self.df3 < self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)""}",eval.Eval.time_chained_cmp,time,0.0
4099,param-level,"terminus-2,oracle",1.1546050789147206,1.1546050789147206,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.Query.time_query_with_boolean_selection,eval,"{""eval.Query.time_query_with_boolean_selection"": ""class Query:\n    def time_query_with_boolean_selection(self):\n        self.df.query(\""(a >= @self.min_val) & (a <= @self.max_val)\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Query:\n    def setup(self):\n        N = 10**6\n        halfway = (N // 2) - 1\n        index = pd.date_range(\""20010101\"", periods=N, freq=\""T\"")\n        s = pd.Series(index)\n        self.ts = s.iloc[halfway]\n        self.df = pd.DataFrame({\""a\"": np.random.randn(N), \""dates\"": index}, index=index)\n        data = np.random.randn(N)\n        self.min_val = data.min()\n        self.max_val = data.max()""}",eval.Query.time_query_with_boolean_selection,time,0.0
4097,param-level,"terminus-2,oracle",1.3893436394248913,1.3893436394248913,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.Eval.time_mult,eval,"{""eval.Eval.time_mult"": ""class Eval:\n    def time_mult(self, engine, threads):\n        pd.eval(\""self.df * self.df2 * self.df3 * self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)""}",eval.Eval.time_mult,time,0.0
4101,param-level,"terminus-2,oracle",1.109764724574163,1.109764724574163,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromArrays.time_frame_from_arrays_sparse,frame_ctor,"{""frame_ctor.FromArrays.time_frame_from_arrays_sparse"": ""class FromArrays:\n    def time_frame_from_arrays_sparse(self):\n        self.df = DataFrame._from_arrays(\n            self.sparse_arrays,\n            index=self.index,\n            columns=self.columns,\n            verify_integrity=False,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromArrays:\n    def setup(self):\n        N_rows = 1000\n        N_cols = 1000\n        self.float_arrays = [np.random.randn(N_rows) for _ in range(N_cols)]\n        self.sparse_arrays = [\n            pd.arrays.SparseArray(np.random.randint(0, 2, N_rows), dtype=\""float64\"")\n            for _ in range(N_cols)\n        ]\n        self.int_arrays = [\n            pd.array(np.random.randint(1000, size=N_rows), dtype=\""Int64\"")\n            for _ in range(N_cols)\n        ]\n        self.index = pd.Index(range(N_rows))\n        self.columns = pd.Index(range(N_cols))""}",frame_ctor.FromArrays.time_frame_from_arrays_sparse,time,0.0
4102,param-level,"terminus-2,oracle",1.0681690669423896,1.0681690669423896,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromDicts.time_dict_of_categoricals,frame_ctor,"{""frame_ctor.FromDicts.time_dict_of_categoricals"": ""class FromDicts:\n    def time_dict_of_categoricals(self):\n        # dict of arrays that we won't consolidate\n        DataFrame(self.dict_of_categoricals)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDicts:\n    def setup(self):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n        frame = DataFrame(np.random.randn(N, K), index=self.index, columns=self.columns)\n        self.data = frame.to_dict()\n        self.dict_list = frame.to_dict(orient=\""records\"")\n        self.data2 = {i: {j: float(j) for j in range(100)} for i in range(2000)}\n    \n        # arrays which we won't consolidate\n        self.dict_of_categoricals = {i: Categorical(np.arange(N)) for i in range(K)}""}",frame_ctor.FromDicts.time_dict_of_categoricals,time,0.0
4100,param-level,"terminus-2,oracle",0.994961338034834,0.994961338034834,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,finalize.Finalize.time_finalize_micro,finalize,"{""finalize.Finalize.time_finalize_micro"": ""class Finalize:\n    def time_finalize_micro(self, param):\n        self.obj.__finalize__(self.obj, method=\""__finalize__\"")\n\n    def setup(self, param):\n        N = 1000\n        obj = param(dtype=float)\n        for i in range(N):\n            obj.attrs[i] = i\n        self.obj = obj""}",finalize.Finalize.time_finalize_micro,time,0.0
4105,param-level,"terminus-2,oracle",1.0145578631642584,1.0145578631642584,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromDictwithTimestamp.time_dict_with_timestamp_offsets,frame_ctor,"{""frame_ctor.FromDictwithTimestamp.time_dict_with_timestamp_offsets"": ""class FromDictwithTimestamp:\n    def time_dict_with_timestamp_offsets(self, offset):\n        DataFrame(self.d)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDictwithTimestamp:\n    def setup(self, offset):\n        N = 10**3\n        idx = date_range(Timestamp(\""1/1/1900\""), freq=offset, periods=N)\n        df = DataFrame(np.random.randn(N, 10), index=idx)\n        self.d = df.to_dict()""}",frame_ctor.FromDictwithTimestamp.time_dict_with_timestamp_offsets,time,0.0
4098,param-level,"terminus-2,oracle",1.1317455443362747,1.1317455443362747,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.Query.time_query_datetime_column,eval,"{""eval.Query.time_query_datetime_column"": ""class Query:\n    def time_query_datetime_column(self):\n        self.df.query(\""dates < @self.ts\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Query:\n    def setup(self):\n        N = 10**6\n        halfway = (N // 2) - 1\n        index = pd.date_range(\""20010101\"", periods=N, freq=\""T\"")\n        s = pd.Series(index)\n        self.ts = s.iloc[halfway]\n        self.df = pd.DataFrame({\""a\"": np.random.randn(N), \""dates\"": index}, index=index)\n        data = np.random.randn(N)\n        self.min_val = data.min()\n        self.max_val = data.max()""}",eval.Query.time_query_datetime_column,time,0.0
4109,param-level,"terminus-2,oracle",1.0407155721351138,1.0407155721351138,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromScalar.time_frame_from_scalar_ea_float64_na,frame_ctor,"{""frame_ctor.FromScalar.time_frame_from_scalar_ea_float64_na"": ""class FromScalar:\n    def time_frame_from_scalar_ea_float64_na(self):\n        DataFrame(\n            NA,\n            index=range(self.nrows),\n            columns=list(\""abc\""),\n            dtype=Float64Dtype(),\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromScalar:\n    def setup(self):\n        self.nrows = 100_000""}",frame_ctor.FromScalar.time_frame_from_scalar_ea_float64_na,time,0.0
4112,param-level,"terminus-2,oracle",1.0717508688341202,1.0717508688341202,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Apply.time_apply_np_mean,frame_methods,"{""frame_methods.Apply.time_apply_np_mean"": ""class Apply:\n    def time_apply_np_mean(self):\n        self.df.apply(np.mean)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self):\n        self.df = DataFrame(np.random.randn(1000, 100))\n    \n        self.s = Series(np.arange(1028.0))\n        self.df2 = DataFrame({i: self.s for i in range(1028)})\n        self.df3 = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))""}",frame_methods.Apply.time_apply_np_mean,time,0.0
4103,param-level,"terminus-2,oracle",1.0644373832418372,1.0644373832418372,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromDicts.time_list_of_dict,frame_ctor,"{""frame_ctor.FromDicts.time_list_of_dict"": ""class FromDicts:\n    def time_list_of_dict(self):\n        DataFrame(self.dict_list)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDicts:\n    def setup(self):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n        frame = DataFrame(np.random.randn(N, K), index=self.index, columns=self.columns)\n        self.data = frame.to_dict()\n        self.dict_list = frame.to_dict(orient=\""records\"")\n        self.data2 = {i: {j: float(j) for j in range(100)} for i in range(2000)}\n    \n        # arrays which we won't consolidate\n        self.dict_of_categoricals = {i: Categorical(np.arange(N)) for i in range(K)}""}",frame_ctor.FromDicts.time_list_of_dict,time,0.0
4094,param-level,"terminus-2,oracle",1.2709339453302828,1.2709339453302828,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.Eval.time_add,eval,"{""eval.Eval.time_add"": ""class Eval:\n    def time_add(self, engine, threads):\n        pd.eval(\""self.df + self.df2 + self.df3 + self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)""}",eval.Eval.time_add,time,0.0
4110,param-level,"terminus-2,oracle",1.028257179873277,1.028257179873277,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Apply.time_apply_axis_1,frame_methods,"{""frame_methods.Apply.time_apply_axis_1"": ""class Apply:\n    def time_apply_axis_1(self):\n        self.df.apply(lambda x: x + 1, axis=1)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self):\n        self.df = DataFrame(np.random.randn(1000, 100))\n    \n        self.s = Series(np.arange(1028.0))\n        self.df2 = DataFrame({i: self.s for i in range(1028)})\n        self.df3 = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))""}",frame_methods.Apply.time_apply_axis_1,time,0.0
4091,param-level,"terminus-2,oracle",1.0679607860056457,1.0679607860056457,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.SelectDtypes.time_select_dtype_int_exclude,dtypes,"{""dtypes.SelectDtypes.time_select_dtype_int_exclude"": ""class SelectDtypes:\n    def time_select_dtype_int_exclude(self, dtype):\n        self.df_int.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )""}",dtypes.SelectDtypes.time_select_dtype_int_exclude,time,0.0
4107,param-level,"terminus-2,oracle",1.048155438963408,1.048155438963408,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromRecords.time_frame_from_records_generator,frame_ctor,"{""frame_ctor.FromRecords.time_frame_from_records_generator"": ""class FromRecords:\n    def time_frame_from_records_generator(self, nrows):\n        # issue-6700\n        self.df = DataFrame.from_records(self.gen, nrows=nrows)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromRecords:\n    def setup(self, nrows):\n        N = 100000\n        self.gen = ((x, (x * 20), (x * 100)) for x in range(N))""}",frame_ctor.FromRecords.time_frame_from_records_generator,time,0.0
4106,param-level,"terminus-2,oracle",1.0154414722650773,1.0154414722650773,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromLists.time_frame_from_lists,frame_ctor,"{""frame_ctor.FromLists.time_frame_from_lists"": ""class FromLists:\n    def time_frame_from_lists(self):\n        self.df = DataFrame(self.data)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromLists:\n    def setup(self):\n        N = 1000\n        M = 100\n        self.data = [list(range(M)) for i in range(N)]""}",frame_ctor.FromLists.time_frame_from_lists,time,0.0
4116,param-level,"terminus-2,oracle",1.0482445796343702,1.0482445796343702,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Describe.time_series_describe,frame_methods,"{""frame_methods.Describe.time_series_describe"": ""class Describe:\n    def time_series_describe(self):\n        self.df[\""a\""].describe()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Describe:\n    def setup(self):\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(0, 100, 10**6),\n                \""b\"": np.random.randint(0, 100, 10**6),\n                \""c\"": np.random.randint(0, 100, 10**6),\n            }\n        )""}",frame_methods.Describe.time_series_describe,time,0.0
4114,param-level,"terminus-2,oracle",1.1014807750057138,1.1014807750057138,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Count.time_count_level_mixed_dtypes_multi,frame_methods,"{""frame_methods.Count.time_count_level_mixed_dtypes_multi"": ""class Count:\n    def time_count_level_mixed_dtypes_multi(self, axis):\n        self.df_mixed.count(axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Count:\n    def setup(self, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""\n    \n        self.df.index = MultiIndex.from_arrays([self.df.index, self.df.index])\n        self.df.columns = MultiIndex.from_arrays([self.df.columns, self.df.columns])\n        self.df_mixed.index = MultiIndex.from_arrays(\n            [self.df_mixed.index, self.df_mixed.index]\n        )\n        self.df_mixed.columns = MultiIndex.from_arrays(\n            [self.df_mixed.columns, self.df_mixed.columns]\n        )""}",frame_methods.Count.time_count_level_mixed_dtypes_multi,time,0.0
4108,param-level,"terminus-2,oracle",1.047063214802518,1.047063214802518,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromScalar.time_frame_from_scalar_ea_float64,frame_ctor,"{""frame_ctor.FromScalar.time_frame_from_scalar_ea_float64"": ""class FromScalar:\n    def time_frame_from_scalar_ea_float64(self):\n        DataFrame(\n            1.0,\n            index=range(self.nrows),\n            columns=list(\""abc\""),\n            dtype=Float64Dtype(),\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromScalar:\n    def setup(self):\n        self.nrows = 100_000""}",frame_ctor.FromScalar.time_frame_from_scalar_ea_float64,time,0.0
4117,param-level,"terminus-2,oracle",1.1480368354203323,1.1480368354203323,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Dropna.time_dropna,frame_methods,"{""frame_methods.Dropna.time_dropna"": ""class Dropna:\n    def time_dropna(self, how, axis):\n        self.df.dropna(how=how, axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Dropna:\n    def setup(self, how, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""""}",frame_methods.Dropna.time_dropna,time,0.0
4125,param-level,"terminus-2,oracle",1.0351569761026338,1.0351569761026338,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Iteration.time_iteritems_indexing,frame_methods,"{""frame_methods.Iteration.time_iteritems_indexing"": ""class Iteration:\n    def time_iteritems_indexing(self):\n        for col in self.df3:\n            self.df3[col]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))""}",frame_methods.Iteration.time_iteritems_indexing,time,0.0
4113,param-level,"terminus-2,oracle",1.0948485647922328,1.0948485647922328,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Apply.time_apply_pass_thru,frame_methods,"{""frame_methods.Apply.time_apply_pass_thru"": ""class Apply:\n    def time_apply_pass_thru(self):\n        self.df.apply(lambda x: x)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self):\n        self.df = DataFrame(np.random.randn(1000, 100))\n    \n        self.s = Series(np.arange(1028.0))\n        self.df2 = DataFrame({i: self.s for i in range(1028)})\n        self.df3 = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))""}",frame_methods.Apply.time_apply_pass_thru,time,0.0
4104,param-level,"terminus-2,oracle",1.0588739842694828,1.0588739842694828,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromDicts.time_nested_dict_int64,frame_ctor,"{""frame_ctor.FromDicts.time_nested_dict_int64"": ""class FromDicts:\n    def time_nested_dict_int64(self):\n        # nested dict, integer indexes, regression described in #621\n        DataFrame(self.data2)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDicts:\n    def setup(self):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n        frame = DataFrame(np.random.randn(N, K), index=self.index, columns=self.columns)\n        self.data = frame.to_dict()\n        self.dict_list = frame.to_dict(orient=\""records\"")\n        self.data2 = {i: {j: float(j) for j in range(100)} for i in range(2000)}\n    \n        # arrays which we won't consolidate\n        self.dict_of_categoricals = {i: Categorical(np.arange(N)) for i in range(K)}""}",frame_ctor.FromDicts.time_nested_dict_int64,time,0.0
4121,param-level,"terminus-2,oracle",1.0901762619178108,1.0901762619178108,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Fillna.time_frame_fillna,frame_methods,"{""frame_methods.Fillna.time_frame_fillna"": ""class Fillna:\n    def time_frame_fillna(self, inplace, method, dtype):\n        self.df.fillna(inplace=inplace, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Fillna:\n    def setup(self, inplace, method, dtype):\n        N, M = 10000, 100\n        if dtype in (\""datetime64[ns]\"", \""datetime64[ns, tz]\"", \""timedelta64[ns]\""):\n            data = {\n                \""datetime64[ns]\"": date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n                \""datetime64[ns, tz]\"": date_range(\n                    \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n                ),\n                \""timedelta64[ns]\"": timedelta_range(start=\""1 day\"", periods=N, freq=\""1D\""),\n            }\n            self.df = DataFrame({f\""col_{i}\"": data[dtype] for i in range(M)})\n            self.df[::2] = None\n        else:\n            values = np.random.randn(N, M)\n            values[::2] = np.nan\n            if dtype == \""Int64\"":\n                values = values.round()\n            self.df = DataFrame(values, dtype=dtype)""}",frame_methods.Fillna.time_frame_fillna,time,0.0
4111,param-level,"terminus-2,oracle",1.0553971358160663,1.0553971358160663,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Apply.time_apply_lambda_mean,frame_methods,"{""frame_methods.Apply.time_apply_lambda_mean"": ""class Apply:\n    def time_apply_lambda_mean(self):\n        self.df.apply(lambda x: x.mean())\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self):\n        self.df = DataFrame(np.random.randn(1000, 100))\n    \n        self.s = Series(np.arange(1028.0))\n        self.df2 = DataFrame({i: self.s for i in range(1028)})\n        self.df3 = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))""}",frame_methods.Apply.time_apply_lambda_mean,time,0.0
4119,param-level,"terminus-2,oracle",1.0415164232546907,1.0415164232546907,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Duplicated.time_frame_duplicated_wide,frame_methods,"{""frame_methods.Duplicated.time_frame_duplicated_wide"": ""class Duplicated:\n    def time_frame_duplicated_wide(self):\n        self.df2.duplicated()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Duplicated:\n    def setup(self):\n        n = 1 << 20\n        t = date_range(\""2015-01-01\"", freq=\""S\"", periods=(n // 64))\n        xs = np.random.randn(n // 64).round(2)\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(-1 << 8, 1 << 8, n),\n                \""b\"": np.random.choice(t, n),\n                \""c\"": np.random.choice(xs, n),\n            }\n        )\n        self.df2 = DataFrame(np.random.randn(1000, 100).astype(str)).T""}",frame_methods.Duplicated.time_frame_duplicated_wide,time,0.0
4120,param-level,"terminus-2,oracle",1.5831426169002387,1.5831426169002387,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Equals.time_frame_float_equal,frame_methods,"{""frame_methods.Equals.time_frame_float_equal"": ""class Equals:\n    def time_frame_float_equal(self):\n        self.float_df.equals(self.float_df)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Equals:\n    def setup(self):\n        N = 10**3\n        self.float_df = DataFrame(np.random.randn(N, N))\n        self.float_df_nan = self.float_df.copy()\n        self.float_df_nan.iloc[-1, -1] = np.nan\n    \n        self.object_df = DataFrame(\""foo\"", index=range(N), columns=range(N))\n        self.object_df_nan = self.object_df.copy()\n        self.object_df_nan.iloc[-1, -1] = np.nan\n    \n        self.nonunique_cols = self.object_df.copy()\n        self.nonunique_cols.columns = [\""A\""] * len(self.nonunique_cols.columns)\n        self.nonunique_cols_nan = self.nonunique_cols.copy()\n        self.nonunique_cols_nan.iloc[-1, -1] = np.nan""}",frame_methods.Equals.time_frame_float_equal,time,0.0
4130,param-level,"terminus-2,oracle",1.1137320610096797,1.1137320610096797,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.NSort.time_nsmallest_two_columns,frame_methods,"{""frame_methods.NSort.time_nsmallest_two_columns"": ""class NSort:\n    def time_nsmallest_two_columns(self, keep):\n        self.df.nsmallest(100, [\""A\"", \""B\""], keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NSort:\n    def setup(self, keep):\n        self.df = DataFrame(np.random.randn(100000, 3), columns=list(\""ABC\""))""}",frame_methods.NSort.time_nsmallest_two_columns,time,0.0
4118,param-level,"terminus-2,oracle",1.1474070058342605,1.1474070058342605,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Dropna.time_dropna_axis_mixed_dtypes,frame_methods,"{""frame_methods.Dropna.time_dropna_axis_mixed_dtypes"": ""class Dropna:\n    def time_dropna_axis_mixed_dtypes(self, how, axis):\n        self.df_mixed.dropna(how=how, axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Dropna:\n    def setup(self, how, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""""}",frame_methods.Dropna.time_dropna_axis_mixed_dtypes,time,0.0
4123,param-level,"terminus-2,oracle",1.0177514409094597,1.0177514409094597,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Iteration.time_items,frame_methods,"{""frame_methods.Iteration.time_items"": ""class Iteration:\n    def time_items(self):\n        # (monitor no-copying behaviour)\n        if hasattr(self.df, \""_item_cache\""):\n            self.df._item_cache.clear()\n        for name, col in self.df.items():\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))""}",frame_methods.Iteration.time_items,time,0.0
4128,param-level,"terminus-2,oracle",1.1236743877828543,1.1236743877828543,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.NSort.time_nlargest_two_columns,frame_methods,"{""frame_methods.NSort.time_nlargest_two_columns"": ""class NSort:\n    def time_nlargest_two_columns(self, keep):\n        self.df.nlargest(100, [\""A\"", \""B\""], keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NSort:\n    def setup(self, keep):\n        self.df = DataFrame(np.random.randn(100000, 3), columns=list(\""ABC\""))""}",frame_methods.NSort.time_nlargest_two_columns,time,0.0
4127,param-level,"terminus-2,oracle",1.07645130760125,1.07645130760125,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Iteration.time_itertuples_raw_start,frame_methods,"{""frame_methods.Iteration.time_itertuples_raw_start"": ""class Iteration:\n    def time_itertuples_raw_start(self):\n        self.df4.itertuples(index=False, name=None)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))""}",frame_methods.Iteration.time_itertuples_raw_start,time,0.0
4133,param-level,"terminus-2,oracle",1.0380832207895176,1.0380832207895176,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.ToNumpy.time_to_numpy_tall,frame_methods,"{""frame_methods.ToNumpy.time_to_numpy_tall"": ""class ToNumpy:\n    def time_to_numpy_tall(self):\n        self.df_tall.to_numpy()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToNumpy:\n    def setup(self):\n        N = 10000\n        M = 10\n        self.df_tall = DataFrame(np.random.randn(N, M))\n        self.df_wide = DataFrame(np.random.randn(M, N))\n        self.df_mixed_tall = self.df_tall.copy()\n        self.df_mixed_tall[\""foo\""] = \""bar\""\n        self.df_mixed_tall[0] = period_range(\""2000\"", periods=N)\n        self.df_mixed_tall[1] = range(N)\n        self.df_mixed_wide = self.df_wide.copy()\n        self.df_mixed_wide[\""foo\""] = \""bar\""\n        self.df_mixed_wide[0] = period_range(\""2000\"", periods=M)\n        self.df_mixed_wide[1] = range(M)""}",frame_methods.ToNumpy.time_to_numpy_tall,time,0.0
4115,param-level,"terminus-2,oracle",1.0485527998893167,1.0485527998893167,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Describe.time_dataframe_describe,frame_methods,"{""frame_methods.Describe.time_dataframe_describe"": ""class Describe:\n    def time_dataframe_describe(self):\n        self.df.describe()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Describe:\n    def setup(self):\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(0, 100, 10**6),\n                \""b\"": np.random.randint(0, 100, 10**6),\n                \""c\"": np.random.randint(0, 100, 10**6),\n            }\n        )""}",frame_methods.Describe.time_dataframe_describe,time,0.0
4124,param-level,"terminus-2,oracle",1.0356350267655572,1.0356350267655572,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Iteration.time_items_cached,frame_methods,"{""frame_methods.Iteration.time_items_cached"": ""class Iteration:\n    def time_items_cached(self):\n        for name, col in self.df.items():\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))""}",frame_methods.Iteration.time_items_cached,time,0.0
4136,param-level,"terminus-2,oracle",1.0645670397307003,1.0645670397307003,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.ToNumpy.time_values_wide,frame_methods,"{""frame_methods.ToNumpy.time_values_wide"": ""class ToNumpy:\n    def time_values_wide(self):\n        self.df_wide.values\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToNumpy:\n    def setup(self):\n        N = 10000\n        M = 10\n        self.df_tall = DataFrame(np.random.randn(N, M))\n        self.df_wide = DataFrame(np.random.randn(M, N))\n        self.df_mixed_tall = self.df_tall.copy()\n        self.df_mixed_tall[\""foo\""] = \""bar\""\n        self.df_mixed_tall[0] = period_range(\""2000\"", periods=N)\n        self.df_mixed_tall[1] = range(N)\n        self.df_mixed_wide = self.df_wide.copy()\n        self.df_mixed_wide[\""foo\""] = \""bar\""\n        self.df_mixed_wide[0] = period_range(\""2000\"", periods=M)\n        self.df_mixed_wide[1] = range(M)""}",frame_methods.ToNumpy.time_values_wide,time,0.0
4126,param-level,"terminus-2,oracle",1.1062177869730845,1.1062177869730845,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Iteration.time_iterrows,frame_methods,"{""frame_methods.Iteration.time_iterrows"": ""class Iteration:\n    def time_iterrows(self):\n        for row in self.df.iterrows():\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))""}",frame_methods.Iteration.time_iterrows,time,0.0
4122,param-level,"terminus-2,oracle",1.026015236530584,1.026015236530584,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Interpolate.time_interpolate_some_good,frame_methods,"{""frame_methods.Interpolate.time_interpolate_some_good"": ""class Interpolate:\n    def time_interpolate_some_good(self, downcast):\n        self.df2.interpolate(downcast=downcast)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Interpolate:\n    def setup(self, downcast):\n        N = 10000\n        # this is the worst case, where every column has NaNs.\n        arr = np.random.randn(N, 100)\n        # NB: we need to set values in array, not in df.values, otherwise\n        #  the benchmark will be misleading for ArrayManager\n        arr[::2] = np.nan\n    \n        self.df = DataFrame(arr)\n    \n        self.df2 = DataFrame(\n            {\n                \""A\"": np.arange(0, N),\n                \""B\"": np.random.randint(0, 100, N),\n                \""C\"": np.random.randn(N),\n                \""D\"": np.random.randn(N),\n            }\n        )\n        self.df2.loc[1::5, \""A\""] = np.nan\n        self.df2.loc[1::5, \""C\""] = np.nan""}",frame_methods.Interpolate.time_interpolate_some_good,time,0.0
4135,param-level,"terminus-2,oracle",1.060012268918662,1.060012268918662,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.ToNumpy.time_values_tall,frame_methods,"{""frame_methods.ToNumpy.time_values_tall"": ""class ToNumpy:\n    def time_values_tall(self):\n        self.df_tall.values\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToNumpy:\n    def setup(self):\n        N = 10000\n        M = 10\n        self.df_tall = DataFrame(np.random.randn(N, M))\n        self.df_wide = DataFrame(np.random.randn(M, N))\n        self.df_mixed_tall = self.df_tall.copy()\n        self.df_mixed_tall[\""foo\""] = \""bar\""\n        self.df_mixed_tall[0] = period_range(\""2000\"", periods=N)\n        self.df_mixed_tall[1] = range(N)\n        self.df_mixed_wide = self.df_wide.copy()\n        self.df_mixed_wide[\""foo\""] = \""bar\""\n        self.df_mixed_wide[0] = period_range(\""2000\"", periods=M)\n        self.df_mixed_wide[1] = range(M)""}",frame_methods.ToNumpy.time_values_tall,time,0.0
4129,param-level,"terminus-2,oracle",1.1223277154045053,1.1223277154045053,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.NSort.time_nsmallest_one_column,frame_methods,"{""frame_methods.NSort.time_nsmallest_one_column"": ""class NSort:\n    def time_nsmallest_one_column(self, keep):\n        self.df.nsmallest(100, \""A\"", keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NSort:\n    def setup(self, keep):\n        self.df = DataFrame(np.random.randn(100000, 3), columns=list(\""ABC\""))""}",frame_methods.NSort.time_nsmallest_one_column,time,0.0
4137,param-level,"terminus-2,oracle",1.1079542764811172,1.1079542764811172,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,gil.ParallelGroupbyMethods.time_parallel,gil,"{""gil.ParallelGroupbyMethods.time_parallel"": ""class ParallelGroupbyMethods:\n    def time_parallel(self, threads, method):\n        self.parallel()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ParallelGroupbyMethods:\n    def setup(self, threads, method):\n        N = 10**6\n        ngroups = 10**3\n        df = DataFrame(\n            {\""key\"": np.random.randint(0, ngroups, size=N), \""data\"": np.random.randn(N)}\n        )\n    \n        @test_parallel(num_threads=threads)\n        def parallel():\n            getattr(df.groupby(\""key\"")[\""data\""], method)()\n    \n        self.parallel = parallel\n    \n        def loop():\n            getattr(df.groupby(\""key\"")[\""data\""], method)()\n    \n        self.loop = loop""}",gil.ParallelGroupbyMethods.time_parallel,time,0.0
4141,param-level,"terminus-2,oracle",1.0188571417553025,1.0188571417553025,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.ApplyNonUniqueUnsortedIndex.time_groupby_apply_non_unique_unsorted_index,groupby,"{""groupby.ApplyNonUniqueUnsortedIndex.time_groupby_apply_non_unique_unsorted_index"": ""class ApplyNonUniqueUnsortedIndex:\n    def time_groupby_apply_non_unique_unsorted_index(self):\n        self.df.groupby(\""key\"", group_keys=False).apply(lambda x: x)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ApplyNonUniqueUnsortedIndex:\n    def setup(self):\n        # GH 46527\n        # unsorted and non-unique index\n        idx = np.arange(100)[::-1]\n        idx = Index(np.repeat(idx, 200), name=\""key\"")\n        self.df = DataFrame(np.random.randn(len(idx), 10), index=idx)""}",groupby.ApplyNonUniqueUnsortedIndex.time_groupby_apply_non_unique_unsorted_index,time,0.0
4142,param-level,"terminus-2,oracle",1.070948001002442,1.070948001002442,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.Cumulative.time_frame_transform,groupby,"{""groupby.Cumulative.time_frame_transform"": ""class Cumulative:\n    def time_frame_transform(self, dtype, method, with_nans):\n        self.df.groupby(\""key\"").transform(method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cumulative:\n    def setup(self, dtype, method, with_nans):\n        if with_nans and dtype == \""int64\"":\n            raise NotImplementedError(\""Construction of df would raise\"")\n    \n        N = 500_000\n        keys = np.random.randint(0, 100, size=N)\n        vals = np.random.randint(-10, 10, (N, 5))\n    \n        if with_nans:\n            null_vals = vals.astype(float, copy=True)\n            null_vals[::2, :] = np.nan\n            null_vals[::3, :] = np.nan\n            df = DataFrame(null_vals, columns=list(\""abcde\""), dtype=dtype)\n            df[\""key\""] = keys\n            self.df = df\n        else:\n            df = DataFrame(vals, columns=list(\""abcde\"")).astype(dtype, copy=False)\n            df[\""key\""] = keys\n            self.df = df""}",groupby.Cumulative.time_frame_transform,time,0.0
4132,param-level,"terminus-2,oracle",1.274450093142372,1.274450093142372,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.ToDict.time_to_dict_ints,frame_methods,"{""frame_methods.ToDict.time_to_dict_ints"": ""class ToDict:\n    def time_to_dict_ints(self, orient):\n        self.int_df.to_dict(orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToDict:\n    def setup(self, orient):\n        data = np.random.randint(0, 1000, size=(10000, 4))\n        self.int_df = DataFrame(data)\n        self.datetimelike_df = self.int_df.astype(\""timedelta64[ns]\"")""}",frame_methods.ToDict.time_to_dict_ints,time,0.0
4139,param-level,"terminus-2,oracle",0.960157811866578,0.960157811866578,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,gil.ParallelTake1D.time_take1d,gil,"{""gil.ParallelTake1D.time_take1d"": ""class ParallelTake1D:\n    def time_take1d(self, dtype):\n        self.parallel_take1d()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ParallelTake1D:\n    def setup(self, dtype):\n        N = 10**6\n        df = DataFrame({\""col\"": np.arange(N, dtype=dtype)})\n        indexer = np.arange(100, len(df) - 100)\n    \n        @test_parallel(num_threads=2)\n        def parallel_take1d():\n            take_nd(df[\""col\""].values, indexer)\n    \n        self.parallel_take1d = parallel_take1d""}",gil.ParallelTake1D.time_take1d,time,0.0
4144,param-level,"terminus-2,oracle",1.0599486639571911,1.0599486639571911,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.GroupByMethods.time_dtype_as_group,groupby,"{""groupby.GroupByMethods.time_dtype_as_group"": ""class GroupByMethods:\n    def time_dtype_as_group(self, dtype, method, application, ncols):\n        self.as_group_method()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupByMethods:\n    def setup(self, dtype, method, application, ncols):\n        if method in method_blocklist.get(dtype, {}):\n            raise NotImplementedError  # skip benchmark\n    \n        if ncols != 1 and method in [\""value_counts\"", \""unique\""]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if application == \""transformation\"" and method in [\n            \""describe\"",\n            \""head\"",\n            \""tail\"",\n            \""unique\"",\n            \""value_counts\"",\n            \""size\"",\n        ]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if method == \""describe\"":\n            ngroups = 20\n        elif method == \""skew\"":\n            ngroups = 100\n        else:\n            ngroups = 1000\n        size = ngroups * 2\n        rng = np.arange(ngroups).reshape(-1, 1)\n        rng = np.broadcast_to(rng, (len(rng), ncols))\n        taker = np.random.randint(0, ngroups, size=size)\n        values = rng.take(taker, axis=0)\n        if dtype == \""int\"":\n            key = np.random.randint(0, size, size=size)\n        elif dtype in (\""int16\"", \""uint\""):\n            key = np.random.randint(0, size, size=size, dtype=dtype)\n        elif dtype == \""float\"":\n            key = np.concatenate(\n                [np.random.random(ngroups) * 0.1, np.random.random(ngroups) * 10.0]\n            )\n        elif dtype == \""object\"":\n            key = [\""foo\""] * size\n        elif dtype == \""datetime\"":\n            key = date_range(\""1/1/2011\"", periods=size, freq=\""s\"")\n    \n        cols = [f\""values{n}\"" for n in range(ncols)]\n        df = DataFrame(values, columns=cols)\n        df[\""key\""] = key\n    \n        if len(cols) == 1:\n            cols = cols[0]\n    \n        if application == \""transformation\"":\n            self.as_group_method = lambda: df.groupby(\""key\"")[cols].transform(method)\n            self.as_field_method = lambda: df.groupby(cols)[\""key\""].transform(method)\n        else:\n            self.as_group_method = getattr(df.groupby(\""key\"")[cols], method)\n            self.as_field_method = getattr(df.groupby(cols)[\""key\""], method)""}",groupby.GroupByMethods.time_dtype_as_group,time,0.0
4134,param-level,"terminus-2,oracle",1.0361173915231612,1.0361173915231612,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.ToNumpy.time_to_numpy_wide,frame_methods,"{""frame_methods.ToNumpy.time_to_numpy_wide"": ""class ToNumpy:\n    def time_to_numpy_wide(self):\n        self.df_wide.to_numpy()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToNumpy:\n    def setup(self):\n        N = 10000\n        M = 10\n        self.df_tall = DataFrame(np.random.randn(N, M))\n        self.df_wide = DataFrame(np.random.randn(M, N))\n        self.df_mixed_tall = self.df_tall.copy()\n        self.df_mixed_tall[\""foo\""] = \""bar\""\n        self.df_mixed_tall[0] = period_range(\""2000\"", periods=N)\n        self.df_mixed_tall[1] = range(N)\n        self.df_mixed_wide = self.df_wide.copy()\n        self.df_mixed_wide[\""foo\""] = \""bar\""\n        self.df_mixed_wide[0] = period_range(\""2000\"", periods=M)\n        self.df_mixed_wide[1] = range(M)""}",frame_methods.ToNumpy.time_to_numpy_wide,time,0.0
4145,param-level,"terminus-2,oracle",1.0355765116122002,1.0355765116122002,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.GroupManyLabels.time_sum,groupby,"{""groupby.GroupManyLabels.time_sum"": ""class GroupManyLabels:\n    def time_sum(self, ncols):\n        self.df.groupby(self.labels).sum()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupManyLabels:\n    def setup(self, ncols):\n        N = 1000\n        data = np.random.randn(N, ncols)\n        self.labels = np.random.randint(0, 100, size=N)\n        self.df = DataFrame(data)""}",groupby.GroupManyLabels.time_sum,time,0.0
4138,param-level,"terminus-2,oracle",0.9677818173268288,0.9677818173268288,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,gil.ParallelRolling.time_rolling,gil,"{""gil.ParallelRolling.time_rolling"": ""class ParallelRolling:\n    def time_rolling(self, method):\n        self.parallel_rolling()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ParallelRolling:\n    def setup(self, method):\n        win = 100\n        arr = np.random.rand(100000)\n        if hasattr(DataFrame, \""rolling\""):\n            df = DataFrame(arr).rolling(win)\n    \n            @test_parallel(num_threads=2)\n            def parallel_rolling():\n                getattr(df, method)()\n    \n            self.parallel_rolling = parallel_rolling\n        elif have_rolling_methods:\n            rolling = {\n                \""median\"": rolling_median,\n                \""mean\"": rolling_mean,\n                \""min\"": rolling_min,\n                \""max\"": rolling_max,\n                \""var\"": rolling_var,\n                \""skew\"": rolling_skew,\n                \""kurt\"": rolling_kurt,\n                \""std\"": rolling_std,\n            }\n    \n            @test_parallel(num_threads=2)\n            def parallel_rolling():\n                rolling[method](arr, win)\n    \n            self.parallel_rolling = parallel_rolling\n        else:\n            raise NotImplementedError""}",gil.ParallelRolling.time_rolling,time,0.0
4149,param-level,"terminus-2,oracle",0.8147115270091495,0.8147115270091495,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.String.time_str_func,groupby,"{""groupby.String.time_str_func"": ""class String:\n    def time_str_func(self, dtype, method):\n        self.df.groupby(\""a\"")[self.df.columns[1:]].agg(method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass String:\n    def setup(self, dtype, method):\n        cols = list(\""abcdefghjkl\"")\n        self.df = DataFrame(\n            np.random.randint(0, 100, size=(10_000, len(cols))),\n            columns=cols,\n            dtype=dtype,\n        )""}",groupby.String.time_str_func,time,0.0
4143,param-level,"terminus-2,oracle",1.0756120963928204,1.0756120963928204,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.GroupByMethods.time_dtype_as_field,groupby,"{""groupby.GroupByMethods.time_dtype_as_field"": ""class GroupByMethods:\n    def time_dtype_as_field(self, dtype, method, application, ncols):\n        self.as_field_method()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupByMethods:\n    def setup(self, dtype, method, application, ncols):\n        if method in method_blocklist.get(dtype, {}):\n            raise NotImplementedError  # skip benchmark\n    \n        if ncols != 1 and method in [\""value_counts\"", \""unique\""]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if application == \""transformation\"" and method in [\n            \""describe\"",\n            \""head\"",\n            \""tail\"",\n            \""unique\"",\n            \""value_counts\"",\n            \""size\"",\n        ]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if method == \""describe\"":\n            ngroups = 20\n        elif method == \""skew\"":\n            ngroups = 100\n        else:\n            ngroups = 1000\n        size = ngroups * 2\n        rng = np.arange(ngroups).reshape(-1, 1)\n        rng = np.broadcast_to(rng, (len(rng), ncols))\n        taker = np.random.randint(0, ngroups, size=size)\n        values = rng.take(taker, axis=0)\n        if dtype == \""int\"":\n            key = np.random.randint(0, size, size=size)\n        elif dtype in (\""int16\"", \""uint\""):\n            key = np.random.randint(0, size, size=size, dtype=dtype)\n        elif dtype == \""float\"":\n            key = np.concatenate(\n                [np.random.random(ngroups) * 0.1, np.random.random(ngroups) * 10.0]\n            )\n        elif dtype == \""object\"":\n            key = [\""foo\""] * size\n        elif dtype == \""datetime\"":\n            key = date_range(\""1/1/2011\"", periods=size, freq=\""s\"")\n    \n        cols = [f\""values{n}\"" for n in range(ncols)]\n        df = DataFrame(values, columns=cols)\n        df[\""key\""] = key\n    \n        if len(cols) == 1:\n            cols = cols[0]\n    \n        if application == \""transformation\"":\n            self.as_group_method = lambda: df.groupby(\""key\"")[cols].transform(method)\n            self.as_field_method = lambda: df.groupby(cols)[\""key\""].transform(method)\n        else:\n            self.as_group_method = getattr(df.groupby(\""key\"")[cols], method)\n            self.as_field_method = getattr(df.groupby(cols)[\""key\""], method)""}",groupby.GroupByMethods.time_dtype_as_field,time,0.0
4147,param-level,"terminus-2,oracle",0.7453157545798069,0.7453157545798069,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.RankWithTies.time_rank_ties,groupby,"{""groupby.RankWithTies.time_rank_ties"": ""class RankWithTies:\n    def time_rank_ties(self, dtype, tie_method):\n        self.df.groupby(\""key\"").rank(method=tie_method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass RankWithTies:\n    def setup(self, dtype, tie_method):\n        N = 10**4\n        if dtype == \""datetime64\"":\n            data = np.array([Timestamp(\""2011/01/01\"")] * N, dtype=dtype)\n        else:\n            data = np.array([1] * N, dtype=dtype)\n        self.df = DataFrame({\""values\"": data, \""key\"": [\""foo\""] * N})""}",groupby.RankWithTies.time_rank_ties,time,0.0
4140,param-level,"terminus-2,oracle",1.0230055870235066,1.0230055870235066,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.Apply.time_scalar_function_multi_col,groupby,"{""groupby.Apply.time_scalar_function_multi_col"": ""class Apply:\n    def time_scalar_function_multi_col(self, factor):\n        self.df.groupby([\""key\"", \""key2\""]).apply(lambda x: 1)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, factor):\n        N = 10**factor\n        # two cases:\n        # - small groups: small data (N**4) + many labels (2000) -> average group\n        #   size of 5 (-> larger overhead of slicing method)\n        # - larger groups: larger data (N**5) + fewer labels (20) -> average group\n        #   size of 5000\n        labels = np.random.randint(0, 2000 if factor == 4 else 20, size=N)\n        labels2 = np.random.randint(0, 3, size=N)\n        df = DataFrame(\n            {\n                \""key\"": labels,\n                \""key2\"": labels2,\n                \""value1\"": np.random.randn(N),\n                \""value2\"": [\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (N // 4),\n            }\n        )\n        self.df = df""}",groupby.Apply.time_scalar_function_multi_col,time,0.0
4152,param-level,"terminus-2,oracle",1.1141947051174346,1.1141947051174346,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,hash_functions.UniqueAndFactorizeArange.time_factorize,hash_functions,"{""hash_functions.UniqueAndFactorizeArange.time_factorize"": ""class UniqueAndFactorizeArange:\n    def time_factorize(self, exponent):\n        pd.factorize(self.a2)\n\n    def setup(self, exponent):\n        a = np.arange(10**4, dtype=\""float64\"")\n        self.a2 = (a + 10**exponent).repeat(100)""}",hash_functions.UniqueAndFactorizeArange.time_factorize,time,0.0
4150,param-level,"terminus-2,oracle",1.042586536069814,1.042586536069814,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.Transform.time_transform_lambda_max_wide,groupby,"{""groupby.Transform.time_transform_lambda_max_wide"": ""class Transform:\n    def time_transform_lambda_max_wide(self):\n        self.df_wide.groupby(level=0).transform(lambda x: np.max(x, axis=0))\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]""}",groupby.Transform.time_transform_lambda_max_wide,time,0.0
4157,param-level,"terminus-2,oracle",1.0263895534781546,1.0263895534781546,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_object.Indexing.time_get_loc_sorted,index_object,"{""index_object.Indexing.time_get_loc_sorted"": ""class Indexing:\n    def time_get_loc_sorted(self, dtype):\n        self.sorted.get_loc(self.key)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self, dtype):\n        N = 10**6\n        self.idx = getattr(tm, f\""make{dtype}Index\"")(N)\n        self.array_mask = (np.arange(N) % 3) == 0\n        self.series_mask = Series(self.array_mask)\n        self.sorted = self.idx.sort_values()\n        half = N // 2\n        self.non_unique = self.idx[:half].append(self.idx[:half])\n        self.non_unique_sorted = (\n            self.sorted[:half].append(self.sorted[:half]).sort_values()\n        )\n        self.key = self.sorted[N // 4]""}",index_object.Indexing.time_get_loc_sorted,time,0.0
4151,param-level,"terminus-2,oracle",1.0353260541593894,1.0353260541593894,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,hash_functions.NumericSeriesIndexing.time_loc_slice,hash_functions,"{""hash_functions.NumericSeriesIndexing.time_loc_slice"": ""class NumericSeriesIndexing:\n    def time_loc_slice(self, index, N):\n        # trigger building of mapping\n        self.data.loc[:800]\n\n    def setup(self, dtype, N):\n        vals = np.array(list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype)\n        indices = pd.Index(vals)\n        self.data = pd.Series(np.arange(N), index=indices)""}",hash_functions.NumericSeriesIndexing.time_loc_slice,time,0.0
4148,param-level,"terminus-2,oracle",0.9242780314831636,0.9242780314831636,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.Size.time_category_size,groupby,"{""groupby.Size.time_category_size"": ""class Size:\n    def time_category_size(self):\n        self.draws.groupby(self.cats).size()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Size:\n    def setup(self):\n        n = 10**5\n        offsets = np.random.randint(n, size=n).astype(\""timedelta64[ns]\"")\n        dates = np.datetime64(\""now\"") + offsets\n        self.df = DataFrame(\n            {\n                \""key1\"": np.random.randint(0, 500, size=n),\n                \""key2\"": np.random.randint(0, 100, size=n),\n                \""value1\"": np.random.randn(n),\n                \""value2\"": np.random.randn(n),\n                \""value3\"": np.random.randn(n),\n                \""dates\"": dates,\n            }\n        )\n        self.draws = Series(np.random.randn(n))\n        labels = Series([\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (n // 4))\n        self.cats = labels.astype(\""category\"")""}",groupby.Size.time_category_size,time,0.0
4146,param-level,"terminus-2,oracle",1.1043999320046491,1.1043999320046491,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.Groups.time_series_groups,groupby,"{""groupby.Groups.time_series_groups"": ""class Groups:\n    def time_series_groups(self, data, key):\n        self.ser.groupby(self.ser).groups\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Groups:\n    def setup(self, data, key):\n        self.ser = data[key]\n\n    def setup_cache(self):\n        size = 10**6\n        data = {\n            \""int64_small\"": Series(np.random.randint(0, 100, size=size)),\n            \""int64_large\"": Series(np.random.randint(0, 10000, size=size)),\n            \""object_small\"": Series(\n                tm.makeStringIndex(100).take(np.random.randint(0, 100, size=size))\n            ),\n            \""object_large\"": Series(\n                tm.makeStringIndex(10000).take(np.random.randint(0, 10000, size=size))\n            ),\n        }\n        return data""}",groupby.Groups.time_series_groups,time,0.0
4155,param-level,"terminus-2,oracle",1.0220004458551766,1.0220004458551766,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_cached_properties.IndexCache.time_is_monotonic_increasing,index_cached_properties,"{""index_cached_properties.IndexCache.time_is_monotonic_increasing"": ""class IndexCache:\n    def time_is_monotonic_increasing(self, index_type):\n        self.idx.is_monotonic_increasing\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}""}",index_cached_properties.IndexCache.time_is_monotonic_increasing,time,0.0
4163,param-level,"terminus-2,oracle",1.038504163552764,1.038504163552764,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.IndexSingleRow.time_loc_row,indexing,"{""indexing.IndexSingleRow.time_loc_row"": ""class IndexSingleRow:\n    def time_loc_row(self, unique_cols):\n        self.df.loc[10000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IndexSingleRow:\n    def setup(self, unique_cols):\n        arr = np.arange(10**7).reshape(-1, 10)\n        df = DataFrame(arr)\n        dtypes = [\""u1\"", \""u2\"", \""u4\"", \""u8\"", \""i1\"", \""i2\"", \""i4\"", \""i8\"", \""f8\"", \""f4\""]\n        for i, d in enumerate(dtypes):\n            df[i] = df[i].astype(d)\n    \n        if not unique_cols:\n            # GH#33032 single-row lookups with non-unique columns were\n            #  15x slower than with unique columns\n            df.columns = [\""A\"", \""A\""] + list(df.columns[2:])\n    \n        self.df = df""}",indexing.IndexSingleRow.time_loc_row,time,0.0
4154,param-level,"terminus-2,oracle",1.0838227693246487,1.0838227693246487,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_cached_properties.IndexCache.time_is_monotonic_decreasing,index_cached_properties,"{""index_cached_properties.IndexCache.time_is_monotonic_decreasing"": ""class IndexCache:\n    def time_is_monotonic_decreasing(self, index_type):\n        self.idx.is_monotonic_decreasing\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}""}",index_cached_properties.IndexCache.time_is_monotonic_decreasing,time,0.0
4131,param-level,"terminus-2,oracle",1.0248017468505206,1.0248017468505206,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Quantile.time_frame_quantile,frame_methods,"{""frame_methods.Quantile.time_frame_quantile"": ""class Quantile:\n    def time_frame_quantile(self, axis):\n        self.df.quantile([0.1, 0.5], axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, axis):\n        self.df = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))""}",frame_methods.Quantile.time_frame_quantile,time,0.0
4166,param-level,"terminus-2,oracle",0.9826809808730528,0.9826809808730528,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.MethodLookup.time_lookup_loc,indexing,"{""indexing.MethodLookup.time_lookup_loc"": ""class MethodLookup:\n    def time_lookup_loc(self, s):\n        s.loc\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MethodLookup:\n    def setup_cache(self):\n        s = Series()\n        return s""}",indexing.MethodLookup.time_lookup_loc,time,0.0
4153,param-level,"terminus-2,oracle",1.0398100956535965,1.0398100956535965,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_cached_properties.IndexCache.time_engine,index_cached_properties,"{""index_cached_properties.IndexCache.time_engine"": ""class IndexCache:\n    def time_engine(self, index_type):\n        self.idx._engine\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}""}",index_cached_properties.IndexCache.time_engine,time,0.0
4158,param-level,"terminus-2,oracle",1.1140343749271642,1.1140343749271642,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_object.SetOperations.time_operation,index_object,"{""index_object.SetOperations.time_operation"": ""class SetOperations:\n    def time_operation(self, index_structure, dtype, method):\n        getattr(self.left, method)(self.right)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SetOperations:\n    def setup(self, index_structure, dtype, method):\n        N = 10**5\n        dates_left = date_range(\""1/1/2000\"", periods=N, freq=\""T\"")\n        fmt = \""%Y-%m-%d %H:%M:%S\""\n        date_str_left = Index(dates_left.strftime(fmt))\n        int_left = Index(np.arange(N))\n        ea_int_left = Index(np.arange(N), dtype=\""Int64\"")\n        str_left = tm.makeStringIndex(N)\n    \n        data = {\n            \""datetime\"": dates_left,\n            \""date_string\"": date_str_left,\n            \""int\"": int_left,\n            \""strings\"": str_left,\n            \""ea_int\"": ea_int_left,\n        }\n    \n        if index_structure == \""non_monotonic\"":\n            data = {k: mi[::-1] for k, mi in data.items()}\n    \n        data = {k: {\""left\"": idx, \""right\"": idx[:-1]} for k, idx in data.items()}\n    \n        self.left = data[dtype][\""left\""]\n        self.right = data[dtype][\""right\""]""}",index_object.SetOperations.time_operation,time,0.0
4167,param-level,"terminus-2,oracle",1.061558771647169,1.061558771647169,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.MultiIndexing.time_loc_all_null_slices,indexing,"{""indexing.MultiIndexing.time_loc_all_null_slices"": ""class MultiIndexing:\n    def time_loc_all_null_slices(self, unique_levels):\n        target = tuple([self.tgt_null_slice] * self.nlevels)\n        self.df.loc[target, :]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MultiIndexing:\n    def setup(self, unique_levels):\n        self.nlevels = 2\n        if unique_levels:\n            mi = MultiIndex.from_arrays([range(1000000)] * self.nlevels)\n        else:\n            mi = MultiIndex.from_product([range(1000)] * self.nlevels)\n        self.df = DataFrame(np.random.randn(len(mi)), index=mi)\n    \n        self.tgt_slice = slice(200, 800)\n        self.tgt_null_slice = slice(None)\n        self.tgt_list = list(range(0, 1000, 10))\n        self.tgt_scalar = 500\n    \n        bool_indexer = np.zeros(len(mi), dtype=np.bool_)\n        bool_indexer[slice(0, len(mi), 100)] = True\n        self.tgt_bool_indexer = bool_indexer""}",indexing.MultiIndexing.time_loc_all_null_slices,time,0.0
4160,param-level,"terminus-2,oracle",1.123397506290995,1.123397506290995,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.DataFrameStringIndexing.time_at_setitem,indexing,"{""indexing.DataFrameStringIndexing.time_at_setitem"": ""class DataFrameStringIndexing:\n    def time_at_setitem(self):\n        self.df.at[self.idx_scalar, self.col_scalar] = 0.0\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DataFrameStringIndexing:\n    def setup(self):\n        index = tm.makeStringIndex(1000)\n        columns = tm.makeStringIndex(30)\n        with warnings.catch_warnings(record=True):\n            self.df = DataFrame(np.random.randn(1000, 30), index=index, columns=columns)\n        self.idx_scalar = index[100]\n        self.col_scalar = columns[10]\n        self.bool_indexer = self.df[self.col_scalar] > 0\n        self.bool_obj_indexer = self.bool_indexer.astype(object)\n        self.boolean_indexer = (self.df[self.col_scalar] > 0).astype(\""boolean\"")""}",indexing.DataFrameStringIndexing.time_at_setitem,time,0.0
4162,param-level,"terminus-2,oracle",0.9839495425121074,0.9839495425121074,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.GetItemSingleColumn.time_frame_getitem_single_column_int,indexing,"{""indexing.GetItemSingleColumn.time_frame_getitem_single_column_int"": ""class GetItemSingleColumn:\n    def time_frame_getitem_single_column_int(self):\n        self.df_int_col[0]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GetItemSingleColumn:\n    def setup(self):\n        self.df_string_col = DataFrame(np.random.randn(3000, 1), columns=[\""A\""])\n        self.df_int_col = DataFrame(np.random.randn(3000, 1))""}",indexing.GetItemSingleColumn.time_frame_getitem_single_column_int,time,0.0
4169,param-level,"terminus-2,oracle",1.0473046935586134,1.0473046935586134,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NonNumericSeriesIndexing.time_getitem_label_slice,indexing,"{""indexing.NonNumericSeriesIndexing.time_getitem_label_slice"": ""class NonNumericSeriesIndexing:\n    def time_getitem_label_slice(self, index, index_structure):\n        self.s[: self.lbl]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]""}",indexing.NonNumericSeriesIndexing.time_getitem_label_slice,time,0.0
4173,param-level,"terminus-2,oracle",1.2018416031412138,1.2018416031412138,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericMaskedIndexing.time_get_indexer_dups,indexing,"{""indexing.NumericMaskedIndexing.time_get_indexer_dups"": ""class NumericMaskedIndexing:\n    def time_get_indexer_dups(self, dtype, monotonic):\n        self.data.get_indexer_for(self.indexer)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericMaskedIndexing:\n    def setup(self, dtype, monotonic):\n        indices = {\n            True: Index(self.monotonic_list, dtype=dtype),\n            False: Index(self.non_monotonic_list, dtype=dtype).append(\n                Index([NA], dtype=dtype)\n            ),\n        }\n        self.data = indices[monotonic]\n        self.indexer = np.arange(300, 1_000)\n        self.data_dups = self.data.append(self.data)""}",indexing.NumericMaskedIndexing.time_get_indexer_dups,time,0.0
4161,param-level,"terminus-2,oracle",1.0333263660754053,1.0333263660754053,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.DatetimeIndexIndexing.time_get_indexer_mismatched_tz,indexing,"{""indexing.DatetimeIndexIndexing.time_get_indexer_mismatched_tz"": ""class DatetimeIndexIndexing:\n    def time_get_indexer_mismatched_tz(self):\n        # reached via e.g.\n        #  ser = Series(range(len(dti)), index=dti)\n        #  ser[dti2]\n        self.dti.get_indexer(self.dti2)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndexIndexing:\n    def setup(self):\n        dti = date_range(\""2016-01-01\"", periods=10000, tz=\""US/Pacific\"")\n        dti2 = dti.tz_convert(\""UTC\"")\n        self.dti = dti\n        self.dti2 = dti2""}",indexing.DatetimeIndexIndexing.time_get_indexer_mismatched_tz,time,0.0
4170,param-level,"terminus-2,oracle",1.064515417167363,1.064515417167363,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NonNumericSeriesIndexing.time_getitem_list_like,indexing,"{""indexing.NonNumericSeriesIndexing.time_getitem_list_like"": ""class NonNumericSeriesIndexing:\n    def time_getitem_list_like(self, index, index_structure):\n        self.s[[self.lbl]]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]""}",indexing.NonNumericSeriesIndexing.time_getitem_list_like,time,0.0
4159,param-level,"terminus-2,oracle",0.9864520167007408,0.9864520167007408,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.CategoricalIndexIndexing.time_getitem_scalar,indexing,"{""indexing.CategoricalIndexIndexing.time_getitem_scalar"": ""class CategoricalIndexIndexing:\n    def time_getitem_scalar(self, index):\n        self.data[self.int_scalar]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalIndexIndexing:\n    def setup(self, index):\n        N = 10**5\n        values = list(\""a\"" * N + \""b\"" * N + \""c\"" * N)\n        indices = {\n            \""monotonic_incr\"": CategoricalIndex(values),\n            \""monotonic_decr\"": CategoricalIndex(reversed(values)),\n            \""non_monotonic\"": CategoricalIndex(list(\""abc\"" * N)),\n        }\n        self.data = indices[index]\n        self.data_unique = CategoricalIndex([str(i) for i in range(N * 3)])\n    \n        self.int_scalar = 10000\n        self.int_list = list(range(10000))\n    \n        self.cat_scalar = \""b\""\n        self.cat_list = [\""1\"", \""3\""]""}",indexing.CategoricalIndexIndexing.time_getitem_scalar,time,0.0
4165,param-level,"terminus-2,oracle",0.9670654987224886,0.9670654987224886,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.MethodLookup.time_lookup_iloc,indexing,"{""indexing.MethodLookup.time_lookup_iloc"": ""class MethodLookup:\n    def time_lookup_iloc(self, s):\n        s.iloc\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MethodLookup:\n    def setup_cache(self):\n        s = Series()\n        return s""}",indexing.MethodLookup.time_lookup_iloc,time,0.0
4174,param-level,"terminus-2,oracle",1.1377686919683618,1.1377686919683618,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericSeriesIndexing.time_getitem_array,indexing,"{""indexing.NumericSeriesIndexing.time_getitem_array"": ""class NumericSeriesIndexing:\n    def time_getitem_array(self, index, index_structure):\n        self.data[self.array]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.NumericSeriesIndexing.time_getitem_array,time,0.0
4175,param-level,"terminus-2,oracle",1.3297764991995344,1.3297764991995344,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericSeriesIndexing.time_getitem_list_like,indexing,"{""indexing.NumericSeriesIndexing.time_getitem_list_like"": ""class NumericSeriesIndexing:\n    def time_getitem_list_like(self, index, index_structure):\n        self.data[[800000]]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.NumericSeriesIndexing.time_getitem_list_like,time,0.0
4164,param-level,"terminus-2,oracle",1.3004972820780516,1.3004972820780516,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.IntervalIndexing.time_loc_scalar,indexing,"{""indexing.IntervalIndexing.time_loc_scalar"": ""class IntervalIndexing:\n    def time_loc_scalar(self, monotonic):\n        monotonic.loc[80000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IntervalIndexing:\n    def setup_cache(self):\n        idx = IntervalIndex.from_breaks(np.arange(1000001))\n        monotonic = Series(np.arange(1000000), index=idx)\n        return monotonic""}",indexing.IntervalIndexing.time_loc_scalar,time,0.0
4172,param-level,"terminus-2,oracle",1.2617980705467426,1.2617980705467426,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericMaskedIndexing.time_get_indexer,indexing,"{""indexing.NumericMaskedIndexing.time_get_indexer"": ""class NumericMaskedIndexing:\n    def time_get_indexer(self, dtype, monotonic):\n        self.data.get_indexer(self.indexer)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericMaskedIndexing:\n    def setup(self, dtype, monotonic):\n        indices = {\n            True: Index(self.monotonic_list, dtype=dtype),\n            False: Index(self.non_monotonic_list, dtype=dtype).append(\n                Index([NA], dtype=dtype)\n            ),\n        }\n        self.data = indices[monotonic]\n        self.indexer = np.arange(300, 1_000)\n        self.data_dups = self.data.append(self.data)""}",indexing.NumericMaskedIndexing.time_get_indexer,time,0.0
4176,param-level,"terminus-2,oracle",1.208053664760974,1.208053664760974,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericSeriesIndexing.time_getitem_lists,indexing,"{""indexing.NumericSeriesIndexing.time_getitem_lists"": ""class NumericSeriesIndexing:\n    def time_getitem_lists(self, index, index_structure):\n        self.data[self.array_list]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.NumericSeriesIndexing.time_getitem_lists,time,0.0
4168,param-level,"terminus-2,oracle",1.052174150069658,1.052174150069658,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.MultiIndexing.time_loc_all_scalars,indexing,"{""indexing.MultiIndexing.time_loc_all_scalars"": ""class MultiIndexing:\n    def time_loc_all_scalars(self, unique_levels):\n        target = tuple([self.tgt_scalar] * self.nlevels)\n        self.df.loc[target, :]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MultiIndexing:\n    def setup(self, unique_levels):\n        self.nlevels = 2\n        if unique_levels:\n            mi = MultiIndex.from_arrays([range(1000000)] * self.nlevels)\n        else:\n            mi = MultiIndex.from_product([range(1000)] * self.nlevels)\n        self.df = DataFrame(np.random.randn(len(mi)), index=mi)\n    \n        self.tgt_slice = slice(200, 800)\n        self.tgt_null_slice = slice(None)\n        self.tgt_list = list(range(0, 1000, 10))\n        self.tgt_scalar = 500\n    \n        bool_indexer = np.zeros(len(mi), dtype=np.bool_)\n        bool_indexer[slice(0, len(mi), 100)] = True\n        self.tgt_bool_indexer = bool_indexer""}",indexing.MultiIndexing.time_loc_all_scalars,time,0.0
4156,param-level,"terminus-2,oracle",1.0227030008106008,1.0227030008106008,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_object.Indexing.time_get,index_object,"{""index_object.Indexing.time_get"": ""class Indexing:\n    def time_get(self, dtype):\n        self.idx[1]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self, dtype):\n        N = 10**6\n        self.idx = getattr(tm, f\""make{dtype}Index\"")(N)\n        self.array_mask = (np.arange(N) % 3) == 0\n        self.series_mask = Series(self.array_mask)\n        self.sorted = self.idx.sort_values()\n        half = N // 2\n        self.non_unique = self.idx[:half].append(self.idx[:half])\n        self.non_unique_sorted = (\n            self.sorted[:half].append(self.sorted[:half]).sort_values()\n        )\n        self.key = self.sorted[N // 4]""}",index_object.Indexing.time_get,time,0.0
4179,param-level,"terminus-2,oracle",1.063036998406487,1.063036998406487,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericSeriesIndexing.time_iloc_slice,indexing,"{""indexing.NumericSeriesIndexing.time_iloc_slice"": ""class NumericSeriesIndexing:\n    def time_iloc_slice(self, index, index_structure):\n        self.data.iloc[:800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.NumericSeriesIndexing.time_iloc_slice,time,0.0
4189,param-level,"terminus-2,oracle",1.024725834815643,1.024725834815643,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,inference.ToNumericDowncast.time_downcast,inference,"{""inference.ToNumericDowncast.time_downcast"": ""class ToNumericDowncast:\n    def time_downcast(self, dtype, downcast):\n        to_numeric(self.data, downcast=downcast)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToNumericDowncast:\n    def setup(self, dtype, downcast):\n        self.data = self.data_dict[dtype]""}",inference.ToNumericDowncast.time_downcast,time,0.0
4183,param-level,"terminus-2,oracle",1.0429647275222034,1.0429647275222034,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.Take.time_take,indexing,"{""indexing.Take.time_take"": ""class Take:\n    def time_take(self, index):\n        self.s.take(self.indexer)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Take:\n    def setup(self, index):\n        N = 100000\n        indexes = {\n            \""int\"": Index(np.arange(N), dtype=np.int64),\n            \""datetime\"": date_range(\""2011-01-01\"", freq=\""S\"", periods=N),\n        }\n        index = indexes[index]\n        self.s = Series(np.random.rand(N), index=index)\n        self.indexer = np.random.randint(0, N, size=N)""}",indexing.Take.time_take,time,0.0
4171,param-level,"terminus-2,oracle",0.974465543171536,0.974465543171536,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NonNumericSeriesIndexing.time_getitem_scalar,indexing,"{""indexing.NonNumericSeriesIndexing.time_getitem_scalar"": ""class NonNumericSeriesIndexing:\n    def time_getitem_scalar(self, index, index_structure):\n        self.s[self.lbl]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]""}",indexing.NonNumericSeriesIndexing.time_getitem_scalar,time,0.0
4178,param-level,"terminus-2,oracle",1.2623581032741231,1.2623581032741231,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericSeriesIndexing.time_getitem_slice,indexing,"{""indexing.NumericSeriesIndexing.time_getitem_slice"": ""class NumericSeriesIndexing:\n    def time_getitem_slice(self, index, index_structure):\n        self.data[:800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.NumericSeriesIndexing.time_getitem_slice,time,0.0
4177,param-level,"terminus-2,oracle",1.1173194857077458,1.1173194857077458,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericSeriesIndexing.time_getitem_scalar,indexing,"{""indexing.NumericSeriesIndexing.time_getitem_scalar"": ""class NumericSeriesIndexing:\n    def time_getitem_scalar(self, index, index_structure):\n        self.data[800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.NumericSeriesIndexing.time_getitem_scalar,time,0.0
4191,param-level,"terminus-2,oracle",1.121376298596207,1.121376298596207,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.csv.ReadCSVCategorical.time_convert_post,io.csv,"{""io.csv.ReadCSVCategorical.time_convert_post"": ""class ReadCSVCategorical:\n    def time_convert_post(self, engine):\n        read_csv(self.fname, engine=engine).apply(Categorical)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVCategorical:\n    def setup(self, engine):\n        N = 100000\n        group1 = [\""aaaaaaaa\"", \""bbbbbbb\"", \""cccccccc\"", \""dddddddd\"", \""eeeeeeee\""]\n        df = DataFrame(np.random.choice(group1, (N, 3)), columns=list(\""abc\""))\n        df.to_csv(self.fname, index=False)""}",io.csv.ReadCSVCategorical.time_convert_post,time,0.0
4193,param-level,"terminus-2,oracle",1.03767982340548,1.03767982340548,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.csv.ReadCSVEngine.time_read_stringcsv,io.csv,"{""io.csv.ReadCSVEngine.time_read_stringcsv"": ""class ReadCSVEngine:\n    def time_read_stringcsv(self, engine):\n        read_csv(self.data(self.StringIO_input), engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVEngine:\n    def setup(self, engine):\n        data = [\""A,B,C,D,E\""] + ([\""1,2,3,4,5\""] * 100000)\n        self.StringIO_input = StringIO(\""\\n\"".join(data))\n        # simulate reading from file\n        self.BytesIO_input = BytesIO(self.StringIO_input.read().encode(\""utf-8\""))""}",io.csv.ReadCSVEngine.time_read_stringcsv,time,0.0
4180,param-level,"terminus-2,oracle",1.092596161613923,1.092596161613923,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericSeriesIndexing.time_loc_array,indexing,"{""indexing.NumericSeriesIndexing.time_loc_array"": ""class NumericSeriesIndexing:\n    def time_loc_array(self, index, index_structure):\n        self.data.loc[self.array]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.NumericSeriesIndexing.time_loc_array,time,0.0
4196,param-level,"terminus-2,oracle",1.1630325669190456,1.1630325669190456,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.hdf.HDFStoreDataFrame.time_query_store_table,io.hdf,"{""io.hdf.HDFStoreDataFrame.time_query_store_table"": ""class HDFStoreDataFrame:\n    def time_query_store_table(self):\n        self.store.select(\""table\"", where=\""index > self.start and index < self.stop\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass HDFStoreDataFrame:\n    def setup(self):\n        N = 25000\n        index = tm.makeStringIndex(N)\n        self.df = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)}, index=index\n        )\n        self.df_mixed = DataFrame(\n            {\n                \""float1\"": np.random.randn(N),\n                \""float2\"": np.random.randn(N),\n                \""string1\"": [\""foo\""] * N,\n                \""bool1\"": [True] * N,\n                \""int1\"": np.random.randint(0, N, size=N),\n            },\n            index=index,\n        )\n        self.df_wide = DataFrame(np.random.randn(N, 100))\n        self.start_wide = self.df_wide.index[10000]\n        self.stop_wide = self.df_wide.index[15000]\n        self.df2 = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)},\n            index=date_range(\""1/1/2000\"", periods=N),\n        )\n        self.start = self.df2.index[10000]\n        self.stop = self.df2.index[15000]\n        self.df_wide2 = DataFrame(\n            np.random.randn(N, 100), index=date_range(\""1/1/2000\"", periods=N)\n        )\n        self.df_dc = DataFrame(\n            np.random.randn(N, 10), columns=[f\""C{i:03d}\"" for i in range(10)]\n        )\n    \n        self.fname = \""__test__.h5\""\n    \n        self.store = HDFStore(self.fname)\n        self.store.put(\""fixed\"", self.df)\n        self.store.put(\""fixed_mixed\"", self.df_mixed)\n        self.store.append(\""table\"", self.df2)\n        self.store.append(\""table_mixed\"", self.df_mixed)\n        self.store.append(\""table_wide\"", self.df_wide)\n        self.store.append(\""table_wide2\"", self.df_wide2)""}",io.hdf.HDFStoreDataFrame.time_query_store_table,time,0.0
4185,param-level,"terminus-2,oracle",1.003317469444863,1.003317469444863,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing_engines.MaskedNumericEngineIndexing.time_get_loc_near_middle,indexing_engines,"{""indexing_engines.MaskedNumericEngineIndexing.time_get_loc_near_middle"": ""class MaskedNumericEngineIndexing:\n    def time_get_loc_near_middle(self, engine_and_dtype, index_type, unique, N):\n        # searchsorted performance may be different near the middle of a range\n        #  vs near an endpoint\n        self.data.get_loc(self.key_middle)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())[::-1]\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.zeros(N * 3, dtype=dtype.lower())\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype.lower())\n                arr[N:] = np.arange(N * 2, dtype=dtype.lower())\n    \n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n            mask[-1] = True\n    \n        self.data = engine(BaseMaskedArray(arr, mask))\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]""}",indexing_engines.MaskedNumericEngineIndexing.time_get_loc_near_middle,time,0.0
4188,param-level,"terminus-2,oracle",0.9586934385698772,0.9586934385698772,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,inference.ToDatetimeCache.time_dup_string_dates_and_format,inference,"{""inference.ToDatetimeCache.time_dup_string_dates_and_format"": ""class ToDatetimeCache:\n    def time_dup_string_dates_and_format(self, cache):\n        to_datetime(self.dup_string_dates, format=\""%Y-%m-%d\"", cache=cache)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToDatetimeCache:\n    def setup(self, cache):\n        N = 10000\n        self.unique_numeric_seconds = list(range(N))\n        self.dup_numeric_seconds = [1000] * N\n        self.dup_string_dates = [\""2000-02-11\""] * N\n        self.dup_string_with_tz = [\""2000-02-11 15:00:00-0800\""] * N""}",inference.ToDatetimeCache.time_dup_string_dates_and_format,time,0.0
4186,param-level,"terminus-2,oracle",0.9969443331739448,0.9969443331739448,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing_engines.NumericEngineIndexing.time_get_loc,indexing_engines,"{""indexing_engines.NumericEngineIndexing.time_get_loc"": ""class NumericEngineIndexing:\n    def time_get_loc(self, engine_and_dtype, index_type, unique, N):\n        self.data.get_loc(self.key_early)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)[::-1]\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.empty(N * 3, dtype=dtype)\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype)\n                arr[N:] = np.arange(N * 2, dtype=dtype)\n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype)\n    \n        self.data = engine(arr)\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]""}",indexing_engines.NumericEngineIndexing.time_get_loc,time,0.0
4184,param-level,"terminus-2,oracle",1.05464562024382,1.05464562024382,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing_engines.MaskedNumericEngineIndexing.time_get_loc,indexing_engines,"{""indexing_engines.MaskedNumericEngineIndexing.time_get_loc"": ""class MaskedNumericEngineIndexing:\n    def time_get_loc(self, engine_and_dtype, index_type, unique, N):\n        self.data.get_loc(self.key_early)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())[::-1]\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.zeros(N * 3, dtype=dtype.lower())\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype.lower())\n                arr[N:] = np.arange(N * 2, dtype=dtype.lower())\n    \n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n            mask[-1] = True\n    \n        self.data = engine(BaseMaskedArray(arr, mask))\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]""}",indexing_engines.MaskedNumericEngineIndexing.time_get_loc,time,0.0
4195,param-level,"terminus-2,oracle",1.027590940682821,1.027590940682821,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.excel.ReadExcel.time_read_excel,io.excel,"{""io.excel.ReadExcel.time_read_excel"": ""class ReadExcel:\n    def time_read_excel(self, engine):\n        if engine == \""odf\"":\n            fname = self.fname_odf\n        else:\n            fname = self.fname_excel\n        read_excel(fname, engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadExcel:\n    def setup_cache(self):\n        self.df = _generate_dataframe()\n    \n        self.df.to_excel(self.fname_excel, sheet_name=\""Sheet1\"")\n        self._create_odf()""}",io.excel.ReadExcel.time_read_excel,time,0.0
4187,param-level,"terminus-2,oracle",0.9808478773147782,0.9808478773147782,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing_engines.NumericEngineIndexing.time_get_loc_near_middle,indexing_engines,"{""indexing_engines.NumericEngineIndexing.time_get_loc_near_middle"": ""class NumericEngineIndexing:\n    def time_get_loc_near_middle(self, engine_and_dtype, index_type, unique, N):\n        # searchsorted performance may be different near the middle of a range\n        #  vs near an endpoint\n        self.data.get_loc(self.key_middle)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)[::-1]\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.empty(N * 3, dtype=dtype)\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype)\n                arr[N:] = np.arange(N * 2, dtype=dtype)\n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype)\n    \n        self.data = engine(arr)\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]""}",indexing_engines.NumericEngineIndexing.time_get_loc_near_middle,time,0.0
4181,param-level,"terminus-2,oracle",1.0313814836948474,1.0313814836948474,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericSeriesIndexing.time_loc_scalar,indexing,"{""indexing.NumericSeriesIndexing.time_loc_scalar"": ""class NumericSeriesIndexing:\n    def time_loc_scalar(self, index, index_structure):\n        self.data.loc[800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.NumericSeriesIndexing.time_loc_scalar,time,0.0
4182,param-level,"terminus-2,oracle",1.0881409032083509,1.0881409032083509,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericSeriesIndexing.time_loc_slice,indexing,"{""indexing.NumericSeriesIndexing.time_loc_slice"": ""class NumericSeriesIndexing:\n    def time_loc_slice(self, index, index_structure):\n        self.data.loc[:800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.NumericSeriesIndexing.time_loc_slice,time,0.0
4194,param-level,"terminus-2,oracle",1.0231803528110297,1.0231803528110297,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.csv.ToCSVDatetimeIndex.time_frame_date_formatting_index,io.csv,"{""io.csv.ToCSVDatetimeIndex.time_frame_date_formatting_index"": ""class ToCSVDatetimeIndex:\n    def time_frame_date_formatting_index(self):\n        self.data.to_csv(self.fname, date_format=\""%Y-%m-%d %H:%M:%S\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToCSVDatetimeIndex:\n    def setup(self):\n        rng = date_range(\""2000\"", periods=100_000, freq=\""S\"")\n        self.data = DataFrame({\""a\"": 1}, index=rng)""}",io.csv.ToCSVDatetimeIndex.time_frame_date_formatting_index,time,0.0
4192,param-level,"terminus-2,oracle",1.0503761610002444,1.0503761610002444,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.csv.ReadCSVEngine.time_read_bytescsv,io.csv,"{""io.csv.ReadCSVEngine.time_read_bytescsv"": ""class ReadCSVEngine:\n    def time_read_bytescsv(self, engine):\n        read_csv(self.data(self.BytesIO_input), engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVEngine:\n    def setup(self, engine):\n        data = [\""A,B,C,D,E\""] + ([\""1,2,3,4,5\""] * 100000)\n        self.StringIO_input = StringIO(\""\\n\"".join(data))\n        # simulate reading from file\n        self.BytesIO_input = BytesIO(self.StringIO_input.read().encode(\""utf-8\""))""}",io.csv.ReadCSVEngine.time_read_bytescsv,time,0.0
4202,param-level,"terminus-2,oracle",1.0156846993935509,1.0156846993935509,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.parsers.ConcatDateCols.time_check_concat,io.parsers,"{""io.parsers.ConcatDateCols.time_check_concat"": ""class ConcatDateCols:\n    def time_check_concat(self, value, dim):\n        concat_date_cols(self.object)\n\n    def setup(self, value, dim):\n        count_elem = 10000\n        if dim == 1:\n            self.object = (np.array([value] * count_elem),)\n        if dim == 2:\n            self.object = (\n                np.array([value] * count_elem),\n                np.array([value] * count_elem),\n            )""}",io.parsers.ConcatDateCols.time_check_concat,time,0.0
4190,param-level,"terminus-2,oracle",1.061552509171506,1.061552509171506,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.csv.ReadCSVCategorical.time_convert_direct,io.csv,"{""io.csv.ReadCSVCategorical.time_convert_direct"": ""class ReadCSVCategorical:\n    def time_convert_direct(self, engine):\n        read_csv(self.fname, engine=engine, dtype=\""category\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVCategorical:\n    def setup(self, engine):\n        N = 100000\n        group1 = [\""aaaaaaaa\"", \""bbbbbbb\"", \""cccccccc\"", \""dddddddd\"", \""eeeeeeee\""]\n        df = DataFrame(np.random.choice(group1, (N, 3)), columns=list(\""abc\""))\n        df.to_csv(self.fname, index=False)""}",io.csv.ReadCSVCategorical.time_convert_direct,time,0.0
4209,param-level,"terminus-2,oracle",1.026775439075071,1.026775439075071,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,join_merge.MergeAsof.time_by_object,join_merge,"{""join_merge.MergeAsof.time_by_object"": ""class MergeAsof:\n    def time_by_object(self, direction, tolerance):\n        merge_asof(\n            self.df1b,\n            self.df2b,\n            on=\""time\"",\n            by=\""key\"",\n            direction=direction,\n            tolerance=tolerance,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MergeAsof:\n    def setup(self, direction, tolerance):\n        one_count = 200000\n        two_count = 1000000\n    \n        df1 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, one_count / 20, one_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), one_count),\n                \""key2\"": np.random.randint(0, 25, one_count),\n                \""value1\"": np.random.randn(one_count),\n            }\n        )\n        df2 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, two_count / 20, two_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), two_count),\n                \""key2\"": np.random.randint(0, 25, two_count),\n                \""value2\"": np.random.randn(two_count),\n            }\n        )\n    \n        df1 = df1.sort_values(\""time\"")\n        df2 = df2.sort_values(\""time\"")\n    \n        df1[\""time32\""] = np.int32(df1.time)\n        df2[\""time32\""] = np.int32(df2.time)\n    \n        df1[\""timeu64\""] = np.uint64(df1.time)\n        df2[\""timeu64\""] = np.uint64(df2.time)\n    \n        self.df1a = df1[[\""time\"", \""value1\""]]\n        self.df2a = df2[[\""time\"", \""value2\""]]\n        self.df1b = df1[[\""time\"", \""key\"", \""value1\""]]\n        self.df2b = df2[[\""time\"", \""key\"", \""value2\""]]\n        self.df1c = df1[[\""time\"", \""key2\"", \""value1\""]]\n        self.df2c = df2[[\""time\"", \""key2\"", \""value2\""]]\n        self.df1d = df1[[\""time32\"", \""value1\""]]\n        self.df2d = df2[[\""time32\"", \""value2\""]]\n        self.df1e = df1[[\""time\"", \""key\"", \""key2\"", \""value1\""]]\n        self.df2e = df2[[\""time\"", \""key\"", \""key2\"", \""value2\""]]\n        self.df1f = df1[[\""timeu64\"", \""value1\""]]\n        self.df2f = df2[[\""timeu64\"", \""value2\""]]""}",join_merge.MergeAsof.time_by_object,time,0.0
4210,param-level,"terminus-2,oracle",1.0651493160553005,1.0651493160553005,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,join_merge.MergeAsof.time_on_int32,join_merge,"{""join_merge.MergeAsof.time_on_int32"": ""class MergeAsof:\n    def time_on_int32(self, direction, tolerance):\n        merge_asof(\n            self.df1d, self.df2d, on=\""time32\"", direction=direction, tolerance=tolerance\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MergeAsof:\n    def setup(self, direction, tolerance):\n        one_count = 200000\n        two_count = 1000000\n    \n        df1 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, one_count / 20, one_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), one_count),\n                \""key2\"": np.random.randint(0, 25, one_count),\n                \""value1\"": np.random.randn(one_count),\n            }\n        )\n        df2 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, two_count / 20, two_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), two_count),\n                \""key2\"": np.random.randint(0, 25, two_count),\n                \""value2\"": np.random.randn(two_count),\n            }\n        )\n    \n        df1 = df1.sort_values(\""time\"")\n        df2 = df2.sort_values(\""time\"")\n    \n        df1[\""time32\""] = np.int32(df1.time)\n        df2[\""time32\""] = np.int32(df2.time)\n    \n        df1[\""timeu64\""] = np.uint64(df1.time)\n        df2[\""timeu64\""] = np.uint64(df2.time)\n    \n        self.df1a = df1[[\""time\"", \""value1\""]]\n        self.df2a = df2[[\""time\"", \""value2\""]]\n        self.df1b = df1[[\""time\"", \""key\"", \""value1\""]]\n        self.df2b = df2[[\""time\"", \""key\"", \""value2\""]]\n        self.df1c = df1[[\""time\"", \""key2\"", \""value1\""]]\n        self.df2c = df2[[\""time\"", \""key2\"", \""value2\""]]\n        self.df1d = df1[[\""time32\"", \""value1\""]]\n        self.df2d = df2[[\""time32\"", \""value2\""]]\n        self.df1e = df1[[\""time\"", \""key\"", \""key2\"", \""value1\""]]\n        self.df2e = df2[[\""time\"", \""key\"", \""key2\"", \""value2\""]]\n        self.df1f = df1[[\""timeu64\"", \""value1\""]]\n        self.df2f = df2[[\""timeu64\"", \""value2\""]]""}",join_merge.MergeAsof.time_on_int32,time,0.0
4211,param-level,"terminus-2,oracle",1.0361836437598764,1.0361836437598764,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,libs.InferDtype.time_infer_dtype,libs,"{""libs.InferDtype.time_infer_dtype"": ""class InferDtype:\n    def time_infer_dtype(self, dtype):\n        infer_dtype(self.data_dict[dtype], skipna=False)""}",libs.InferDtype.time_infer_dtype,time,0.0
4197,param-level,"terminus-2,oracle",1.0502907382323543,1.0502907382323543,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.json.NormalizeJSON.time_normalize_json,io.json,"{""io.json.NormalizeJSON.time_normalize_json"": ""class NormalizeJSON:\n    def time_normalize_json(self, orient, frame):\n        json_normalize(self.data)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NormalizeJSON:\n    def setup(self, orient, frame):\n        data = {\n            \""hello\"": [\""thisisatest\"", 999898, \""mixed types\""],\n            \""nest1\"": {\""nest2\"": {\""nest3\"": \""nest3_value\"", \""nest3_int\"": 3445}},\n            \""nest1_list\"": {\""nest2\"": [\""blah\"", 32423, 546456.876, 92030234]},\n            \""hello2\"": \""string\"",\n        }\n        self.data = [data for i in range(10000)]""}",io.json.NormalizeJSON.time_normalize_json,time,0.0
4200,param-level,"terminus-2,oracle",1.0262777368030565,1.0262777368030565,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.json.ToJSONWide.time_to_json,io.json,"{""io.json.ToJSONWide.time_to_json"": ""class ToJSON:\n    def time_to_json(self, orient, frame):\n        getattr(self, frame).to_json(self.fname, orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSONWide:\n    def setup(self, orient, frame):\n        super().setup(orient, frame)\n        base_df = getattr(self, frame).copy()\n        df_wide = concat([base_df.iloc[:100]] * 1000, ignore_index=True, axis=1)\n        self.df_wide = df_wide""}",io.json.ToJSONWide.time_to_json,time,0.0
4206,param-level,"terminus-2,oracle",0.6705412141744088,0.6705412141744088,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,join_merge.Align.time_series_align_left_monotonic,join_merge,"{""join_merge.Align.time_series_align_left_monotonic"": ""class Align:\n    def time_series_align_left_monotonic(self):\n        self.ts1.align(self.ts2, join=\""left\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Align:\n    def setup(self):\n        size = 5 * 10**5\n        rng = np.arange(0, 10**13, 10**7)\n        stamps = np.datetime64(\""now\"").view(\""i8\"") + rng\n        idx1 = np.sort(np.random.choice(stamps, size, replace=False))\n        idx2 = np.sort(np.random.choice(stamps, size, replace=False))\n        self.ts1 = Series(np.random.randn(size), idx1)\n        self.ts2 = Series(np.random.randn(size), idx2)""}",join_merge.Align.time_series_align_left_monotonic,time,0.0
4205,param-level,"terminus-2,oracle",0.97232287374828,0.97232287374828,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.style.Render.time_format_render,io.style,"{""io.style.Render.time_format_render"": ""class Render:\n    def time_format_render(self, cols, rows):\n        self._style_format()\n        self.st._render_html(True, True)\n\n    def setup(self, cols, rows):\n        self.df = DataFrame(\n            np.random.randn(rows, cols),\n            columns=[f\""float_{i+1}\"" for i in range(cols)],\n            index=[f\""row_{i+1}\"" for i in range(rows)],\n        )""}",io.style.Render.time_format_render,time,0.0
4198,param-level,"terminus-2,oracle",1.019150027730217,1.019150027730217,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.json.ToJSON.time_to_json,io.json,"{""io.json.ToJSON.time_to_json"": ""class ToJSON:\n    def time_to_json(self, orient, frame):\n        getattr(self, frame).to_json(self.fname, orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSON:\n    def setup(self, orient, frame):\n        N = 10**5\n        ncols = 5\n        index = date_range(\""20000101\"", periods=N, freq=\""H\"")\n        timedeltas = timedelta_range(start=1, periods=N, freq=\""s\"")\n        datetimes = date_range(start=1, periods=N, freq=\""s\"")\n        ints = np.random.randint(100000000, size=N)\n        longints = sys.maxsize * np.random.randint(100000000, size=N)\n        floats = np.random.randn(N)\n        strings = tm.makeStringIndex(N)\n        self.df = DataFrame(np.random.randn(N, ncols), index=np.arange(N))\n        self.df_date_idx = DataFrame(np.random.randn(N, ncols), index=index)\n        self.df_td_int_ts = DataFrame(\n            {\n                \""td_1\"": timedeltas,\n                \""td_2\"": timedeltas,\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""ts_1\"": datetimes,\n                \""ts_2\"": datetimes,\n            },\n            index=index,\n        )\n        self.df_int_floats = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""int_3\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""float_3\"": floats,\n            },\n            index=index,\n        )\n        self.df_int_float_str = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )\n    \n        self.df_longint_float_str = DataFrame(\n            {\n                \""longint_1\"": longints,\n                \""longint_2\"": longints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )""}",io.json.ToJSON.time_to_json,time,0.0
4201,param-level,"terminus-2,oracle",1.0644492683983184,1.0644492683983184,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.json.ToJSONWide.time_to_json_wide,io.json,"{""io.json.ToJSONWide.time_to_json_wide"": ""class ToJSONWide:\n    def time_to_json_wide(self, orient, frame):\n        self.df_wide.to_json(self.fname, orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSONWide:\n    def setup(self, orient, frame):\n        super().setup(orient, frame)\n        base_df = getattr(self, frame).copy()\n        df_wide = concat([base_df.iloc[:100]] * 1000, ignore_index=True, axis=1)\n        self.df_wide = df_wide""}",io.json.ToJSONWide.time_to_json_wide,time,0.0
4199,param-level,"terminus-2,oracle",1.0151594714925496,1.0151594714925496,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.json.ToJSONLines.time_delta_int_tstamp_lines,io.json,"{""io.json.ToJSONLines.time_delta_int_tstamp_lines"": ""class ToJSONLines:\n    def time_delta_int_tstamp_lines(self):\n        self.df_td_int_ts.to_json(self.fname, orient=\""records\"", lines=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSONLines:\n    def setup(self):\n        N = 10**5\n        ncols = 5\n        index = date_range(\""20000101\"", periods=N, freq=\""H\"")\n        timedeltas = timedelta_range(start=1, periods=N, freq=\""s\"")\n        datetimes = date_range(start=1, periods=N, freq=\""s\"")\n        ints = np.random.randint(100000000, size=N)\n        longints = sys.maxsize * np.random.randint(100000000, size=N)\n        floats = np.random.randn(N)\n        strings = tm.makeStringIndex(N)\n        self.df = DataFrame(np.random.randn(N, ncols), index=np.arange(N))\n        self.df_date_idx = DataFrame(np.random.randn(N, ncols), index=index)\n        self.df_td_int_ts = DataFrame(\n            {\n                \""td_1\"": timedeltas,\n                \""td_2\"": timedeltas,\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""ts_1\"": datetimes,\n                \""ts_2\"": datetimes,\n            },\n            index=index,\n        )\n        self.df_int_floats = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""int_3\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""float_3\"": floats,\n            },\n            index=index,\n        )\n        self.df_int_float_str = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )\n        self.df_longint_float_str = DataFrame(\n            {\n                \""longint_1\"": longints,\n                \""longint_2\"": longints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )""}",io.json.ToJSONLines.time_delta_int_tstamp_lines,time,0.0
4203,param-level,"terminus-2,oracle",0.9588987691396428,0.9588987691396428,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.parsers.DoesStringLookLikeDatetime.time_check_datetimes,io.parsers,"{""io.parsers.DoesStringLookLikeDatetime.time_check_datetimes"": ""class DoesStringLookLikeDatetime:\n    def time_check_datetimes(self, value):\n        for obj in self.objects:\n            _does_string_look_like_datetime(obj)\n\n    def setup(self, value):\n        self.objects = [value] * 1000000""}",io.parsers.DoesStringLookLikeDatetime.time_check_datetimes,time,0.0
4207,param-level,"terminus-2,oracle",1.0272913623753237,1.0272913623753237,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,join_merge.Concat.time_concat_empty_left,join_merge,"{""join_merge.Concat.time_concat_empty_left"": ""class Concat:\n    def time_concat_empty_left(self, axis):\n        concat(self.empty_left, axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Concat:\n    def setup(self, axis):\n        N = 1000\n        s = Series(N, index=tm.makeStringIndex(N))\n        self.series = [s[i:-i] for i in range(1, 10)] * 50\n        self.small_frames = [DataFrame(np.random.randn(5, 4))] * 1000\n        df = DataFrame(\n            {\""A\"": range(N)}, index=date_range(\""20130101\"", periods=N, freq=\""s\"")\n        )\n        self.empty_left = [DataFrame(), df]\n        self.empty_right = [df, DataFrame()]\n        self.mixed_ndims = [df, df.head(N // 2)]""}",join_merge.Concat.time_concat_empty_left,time,0.0
4213,param-level,"terminus-2,oracle",1.021994003062137,1.021994003062137,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,libs.ScalarListLike.time_is_scalar,libs,"{""libs.ScalarListLike.time_is_scalar"": ""class ScalarListLike:\n    def time_is_scalar(self, param):\n        is_scalar(param)""}",libs.ScalarListLike.time_is_scalar,time,0.0
4208,param-level,"terminus-2,oracle",0.9154205285523832,0.9154205285523832,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,join_merge.ConcatIndexDtype.time_concat_series,join_merge,"{""join_merge.ConcatIndexDtype.time_concat_series"": ""class ConcatIndexDtype:\n    def time_concat_series(self, dtype, structure, axis, sort):\n        concat(self.series, axis=axis, sort=sort)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ConcatIndexDtype:\n    def setup(self, dtype, structure, axis, sort):\n        N = 10_000\n        if dtype == \""datetime64[ns]\"":\n            vals = date_range(\""1970-01-01\"", periods=N)\n        elif dtype in (\""int64\"", \""Int64\""):\n            vals = np.arange(N, dtype=np.int64)\n        elif dtype in (\""string[python]\"", \""string[pyarrow]\""):\n            vals = tm.makeStringIndex(N)\n        else:\n            raise NotImplementedError\n    \n        idx = Index(vals, dtype=dtype)\n    \n        if structure == \""monotonic\"":\n            idx = idx.sort_values()\n        elif structure == \""non_monotonic\"":\n            idx = idx[::-1]\n        elif structure == \""has_na\"":\n            if not idx._can_hold_na:\n                raise NotImplementedError\n            idx = Index([None], dtype=dtype).append(idx)\n        else:\n            raise NotImplementedError\n    \n        self.series = [Series(i, idx[:-i]) for i in range(1, 6)]""}",join_merge.ConcatIndexDtype.time_concat_series,time,0.0
4214,param-level,"terminus-2,oracle",1.5677687835340337,1.5677687835340337,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,multiindex_object.GetLoc.time_large_get_loc_warm,multiindex_object,"{""multiindex_object.GetLoc.time_large_get_loc_warm"": ""class GetLoc:\n    def time_large_get_loc_warm(self):\n        for _ in range(1000):\n            self.mi_large.get_loc((999, 19, \""Z\""))\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GetLoc:\n    def setup(self):\n        self.mi_large = MultiIndex.from_product(\n            [np.arange(1000), np.arange(20), list(string.ascii_letters)],\n            names=[\""one\"", \""two\"", \""three\""],\n        )\n        self.mi_med = MultiIndex.from_product(\n            [np.arange(1000), np.arange(10), list(\""A\"")], names=[\""one\"", \""two\"", \""three\""]\n        )\n        self.mi_small = MultiIndex.from_product(\n            [np.arange(100), list(\""A\""), list(\""A\"")], names=[\""one\"", \""two\"", \""three\""]\n        )""}",multiindex_object.GetLoc.time_large_get_loc_warm,time,0.0
4204,param-level,"terminus-2,oracle",1.0472118667839048,1.0472118667839048,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.sql.WriteSQLDtypes.time_read_sql_query_select_column,io.sql,"{""io.sql.WriteSQLDtypes.time_read_sql_query_select_column"": ""class WriteSQLDtypes:\n    def time_read_sql_query_select_column(self, connection, dtype):\n        read_sql_query(self.query_col, self.con)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass WriteSQLDtypes:\n    def setup(self, connection, dtype):\n        N = 10000\n        con = {\n            \""sqlalchemy\"": create_engine(\""sqlite:///:memory:\""),\n            \""sqlite\"": sqlite3.connect(\"":memory:\""),\n        }\n        self.table_name = \""test_type\""\n        self.query_col = f\""SELECT {dtype} FROM {self.table_name}\""\n        self.con = con[connection]\n        self.df = DataFrame(\n            {\n                \""float\"": np.random.randn(N),\n                \""float_with_nan\"": np.random.randn(N),\n                \""string\"": [\""foo\""] * N,\n                \""bool\"": [True] * N,\n                \""int\"": np.random.randint(0, N, size=N),\n                \""datetime\"": date_range(\""2000-01-01\"", periods=N, freq=\""s\""),\n            },\n            index=tm.makeStringIndex(N),\n        )\n        self.df.iloc[1000:3000, 1] = np.nan\n        self.df[\""date\""] = self.df[\""datetime\""].dt.date\n        self.df[\""time\""] = self.df[\""datetime\""].dt.time\n        self.df[\""datetime_string\""] = self.df[\""datetime\""].astype(str)\n        self.df.to_sql(self.table_name, self.con, if_exists=\""replace\"")""}",io.sql.WriteSQLDtypes.time_read_sql_query_select_column,time,0.0
4218,param-level,"terminus-2,oracle",1.0301011183673392,1.0301011183673392,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,period.Indexing.time_align,period,"{""period.Indexing.time_align"": ""class Indexing:\n    def time_align(self):\n        DataFrame({\""a\"": self.series, \""b\"": self.series[:500]})\n\n    def setup(self):\n        self.index = period_range(start=\""1985\"", periods=1000, freq=\""D\"")\n        self.series = Series(range(1000), index=self.index)\n        self.period = self.index[500]""}",period.Indexing.time_align,time,0.0
4212,param-level,"terminus-2,oracle",1.02516782783796,1.02516782783796,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,libs.ScalarListLike.time_is_list_like,libs,"{""libs.ScalarListLike.time_is_list_like"": ""class ScalarListLike:\n    def time_is_list_like(self, param):\n        is_list_like(param)""}",libs.ScalarListLike.time_is_list_like,time,0.0
4220,param-level,"terminus-2,oracle",1.0326909144462386,1.0326909144462386,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,period.Indexing.time_series_loc,period,"{""period.Indexing.time_series_loc"": ""class Indexing:\n    def time_series_loc(self):\n        self.series.loc[self.period]\n\n    def setup(self):\n        self.index = period_range(start=\""1985\"", periods=1000, freq=\""D\"")\n        self.series = Series(range(1000), index=self.index)\n        self.period = self.index[500]""}",period.Indexing.time_series_loc,time,0.0
4224,param-level,"terminus-2,oracle",0.9481537929203344,0.9481537929203344,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,replace.ReplaceList.time_replace_list,replace,"{""replace.ReplaceList.time_replace_list"": ""class ReplaceList:\n    def time_replace_list(self, inplace):\n        self.df.replace([np.inf, -np.inf], np.nan, inplace=inplace)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReplaceList:\n    def setup(self, inplace):\n        self.df = pd.DataFrame({\""A\"": 0, \""B\"": 0}, index=range(4 * 10**7))""}",replace.ReplaceList.time_replace_list,time,0.0
4223,param-level,"terminus-2,oracle",0.9804572743060842,0.9804572743060842,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reindex.Fillna.time_reindexed,reindex,"{""reindex.Fillna.time_reindexed"": ""class Fillna:\n    def time_reindexed(self, method):\n        self.ts_reindexed.fillna(method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Fillna:\n    def setup(self, method):\n        N = 100000\n        self.idx = date_range(\""1/1/2000\"", periods=N, freq=\""1min\"")\n        ts = Series(np.random.randn(N), index=self.idx)[::2]\n        self.ts_reindexed = ts.reindex(self.idx)\n        self.ts_float32 = self.ts_reindexed.astype(\""float32\"")""}",reindex.Fillna.time_reindexed,time,0.0
4226,param-level,"terminus-2,oracle",0.9799149945027332,0.9799149945027332,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.Cut.time_cut_interval,reshape,"{""reshape.Cut.time_cut_interval"": ""class Cut:\n    def time_cut_interval(self, bins):\n        # GH 27668\n        pd.cut(self.int_series, self.interval_bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))""}",reshape.Cut.time_cut_interval,time,0.0
4219,param-level,"terminus-2,oracle",1.0468463707505495,1.0468463707505495,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,period.Indexing.time_intersection,period,"{""period.Indexing.time_intersection"": ""class Indexing:\n    def time_intersection(self):\n        self.index[:750].intersection(self.index[250:])\n\n    def setup(self):\n        self.index = period_range(start=\""1985\"", periods=1000, freq=\""D\"")\n        self.series = Series(range(1000), index=self.index)\n        self.period = self.index[500]""}",period.Indexing.time_intersection,time,0.0
4227,param-level,"terminus-2,oracle",1.1399230301212795,1.1399230301212795,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.Cut.time_cut_timedelta,reshape,"{""reshape.Cut.time_cut_timedelta"": ""class Cut:\n    def time_cut_timedelta(self, bins):\n        pd.cut(self.timedelta_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))""}",reshape.Cut.time_cut_timedelta,time,0.0
4217,param-level,"terminus-2,oracle",0.9111576927899844,0.9111576927899844,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,multiindex_object.SetOperations.time_operation,multiindex_object,"{""multiindex_object.SetOperations.time_operation"": ""class SetOperations:\n    def time_operation(self, index_structure, dtype, method, sort):\n        getattr(self.left, method)(self.right, sort=sort)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SetOperations:\n    def setup(self, index_structure, dtype, method, sort):\n        N = 10**5\n        level1 = range(1000)\n    \n        level2 = date_range(start=\""1/1/2000\"", periods=N // 1000)\n        dates_left = MultiIndex.from_product([level1, level2])\n    \n        level2 = range(N // 1000)\n        int_left = MultiIndex.from_product([level1, level2])\n    \n        level2 = tm.makeStringIndex(N // 1000).values\n        str_left = MultiIndex.from_product([level1, level2])\n    \n        level2 = range(N // 1000)\n        ea_int_left = MultiIndex.from_product([level1, Series(level2, dtype=\""Int64\"")])\n    \n        data = {\n            \""datetime\"": dates_left,\n            \""int\"": int_left,\n            \""string\"": str_left,\n            \""ea_int\"": ea_int_left,\n        }\n    \n        if index_structure == \""non_monotonic\"":\n            data = {k: mi[::-1] for k, mi in data.items()}\n    \n        data = {k: {\""left\"": mi, \""right\"": mi[:-1]} for k, mi in data.items()}\n        self.left = data[dtype][\""left\""]\n        self.right = data[dtype][\""right\""]""}",multiindex_object.SetOperations.time_operation,time,0.0
4215,param-level,"terminus-2,oracle",1.0539896610532509,1.0539896610532509,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,multiindex_object.GetLoc.time_med_get_loc,multiindex_object,"{""multiindex_object.GetLoc.time_med_get_loc"": ""class GetLoc:\n    def time_med_get_loc(self):\n        self.mi_med.get_loc((999, 9, \""A\""))\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GetLoc:\n    def setup(self):\n        self.mi_large = MultiIndex.from_product(\n            [np.arange(1000), np.arange(20), list(string.ascii_letters)],\n            names=[\""one\"", \""two\"", \""three\""],\n        )\n        self.mi_med = MultiIndex.from_product(\n            [np.arange(1000), np.arange(10), list(\""A\"")], names=[\""one\"", \""two\"", \""three\""]\n        )\n        self.mi_small = MultiIndex.from_product(\n            [np.arange(100), list(\""A\""), list(\""A\"")], names=[\""one\"", \""two\"", \""three\""]\n        )""}",multiindex_object.GetLoc.time_med_get_loc,time,0.0
4222,param-level,"terminus-2,oracle",0.97254858342969,0.97254858342969,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,plotting.SeriesPlotting.time_series_plot,plotting,"{""plotting.SeriesPlotting.time_series_plot"": ""class SeriesPlotting:\n    def time_series_plot(self, kind):\n        self.s.plot(kind=kind)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesPlotting:\n    def setup(self, kind):\n        if kind in [\""bar\"", \""barh\"", \""pie\""]:\n            n = 100\n        elif kind in [\""kde\""]:\n            n = 10000\n        else:\n            n = 1000000\n    \n        self.s = Series(np.random.randn(n))\n        if kind in [\""area\"", \""pie\""]:\n            self.s = self.s.abs()""}",plotting.SeriesPlotting.time_series_plot,time,0.0
4230,param-level,"terminus-2,oracle",1.0285263308926698,1.0285263308926698,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.Explode.time_explode,reshape,"{""reshape.Explode.time_explode"": ""class Explode:\n    def time_explode(self, n_rows, max_list_length):\n        self.series.explode()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Explode:\n    def setup(self, n_rows, max_list_length):\n        data = [np.arange(np.random.randint(max_list_length)) for _ in range(n_rows)]\n        self.series = pd.Series(data)""}",reshape.Explode.time_explode,time,0.0
4221,param-level,"terminus-2,oracle",1.0396601642827117,1.0396601642827117,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,period.PeriodIndexConstructor.time_from_date_range,period,"{""period.PeriodIndexConstructor.time_from_date_range"": ""class PeriodIndexConstructor:\n    def time_from_date_range(self, freq, is_offset):\n        PeriodIndex(self.rng, freq=freq)\n\n    def setup(self, freq, is_offset):\n        self.rng = date_range(\""1985\"", periods=1000)\n        self.rng2 = date_range(\""1985\"", periods=1000).to_pydatetime()\n        self.ints = list(range(2000, 3000))\n        self.daily_ints = (\n            date_range(\""1/1/2000\"", periods=1000, freq=freq).strftime(\""%Y%m%d\"").map(int)\n        )\n        if is_offset:\n            self.freq = to_offset(freq)\n        else:\n            self.freq = freq""}",period.PeriodIndexConstructor.time_from_date_range,time,0.0
4216,param-level,"terminus-2,oracle",0.9662950819021658,0.9662950819021658,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,multiindex_object.Integer.time_is_monotonic,multiindex_object,"{""multiindex_object.Integer.time_is_monotonic"": ""class Integer:\n    def time_is_monotonic(self):\n        self.mi_int.is_monotonic_increasing\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Integer:\n    def setup(self):\n        self.mi_int = MultiIndex.from_product(\n            [np.arange(1000), np.arange(1000)], names=[\""one\"", \""two\""]\n        )\n        self.obj_index = np.array(\n            [\n                (0, 10),\n                (0, 11),\n                (0, 12),\n                (0, 13),\n                (0, 14),\n                (0, 15),\n                (0, 16),\n                (0, 17),\n                (0, 18),\n                (0, 19),\n            ],\n            dtype=object,\n        )\n        self.other_mi_many_mismatches = MultiIndex.from_tuples(\n            [\n                (-7, 41),\n                (-2, 3),\n                (-0.7, 5),\n                (0, 0),\n                (0, 1.5),\n                (0, 340),\n                (0, 1001),\n                (1, -4),\n                (1, 20),\n                (1, 1040),\n                (432, -5),\n                (432, 17),\n                (439, 165.5),\n                (998, -4),\n                (998, 24065),\n                (999, 865.2),\n                (999, 1000),\n                (1045, -843),\n            ]\n        )""}",multiindex_object.Integer.time_is_monotonic,time,0.0
4225,param-level,"terminus-2,oracle",1.0900392923901523,1.0900392923901523,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.Cut.time_cut_datetime,reshape,"{""reshape.Cut.time_cut_datetime"": ""class Cut:\n    def time_cut_datetime(self, bins):\n        pd.cut(self.datetime_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))""}",reshape.Cut.time_cut_datetime,time,0.0
4233,param-level,"terminus-2,oracle",1.2283349290061478,1.2283349290061478,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.PivotTable.time_pivot_table_categorical_observed,reshape,"{""reshape.PivotTable.time_pivot_table_categorical_observed"": ""class PivotTable:\n    def time_pivot_table_categorical_observed(self):\n        self.df2.pivot_table(\n            index=\""col1\"",\n            values=\""col3\"",\n            columns=\""col2\"",\n            aggfunc=np.sum,\n            fill_value=0,\n            observed=True,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")""}",reshape.PivotTable.time_pivot_table_categorical_observed,time,0.0
4228,param-level,"terminus-2,oracle",1.1130333586673145,1.1130333586673145,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.Cut.time_qcut_datetime,reshape,"{""reshape.Cut.time_qcut_datetime"": ""class Cut:\n    def time_qcut_datetime(self, bins):\n        pd.qcut(self.datetime_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))""}",reshape.Cut.time_qcut_datetime,time,0.0
4229,param-level,"terminus-2,oracle",1.0351283795476007,1.0351283795476007,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.Cut.time_qcut_float,reshape,"{""reshape.Cut.time_qcut_float"": ""class Cut:\n    def time_qcut_float(self, bins):\n        pd.qcut(self.float_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))""}",reshape.Cut.time_qcut_float,time,0.0
4237,param-level,"terminus-2,oracle",1.0345564193262995,1.0345564193262995,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.WideToLong.time_wide_to_long_big,reshape,"{""reshape.WideToLong.time_wide_to_long_big"": ""class WideToLong:\n    def time_wide_to_long_big(self):\n        wide_to_long(self.df, self.letters, i=\""id\"", j=\""year\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass WideToLong:\n    def setup(self):\n        nyrs = 20\n        nidvars = 20\n        N = 5000\n        self.letters = list(\""ABCD\"")\n        yrvars = [\n            letter + str(num)\n            for letter, num in product(self.letters, range(1, nyrs + 1))\n        ]\n        columns = [str(i) for i in range(nidvars)] + yrvars\n        self.df = DataFrame(np.random.randn(N, nidvars + len(yrvars)), columns=columns)\n        self.df[\""id\""] = self.df.index""}",reshape.WideToLong.time_wide_to_long_big,time,0.0
4235,param-level,"terminus-2,oracle",1.0645771443999537,1.0645771443999537,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.PivotTable.time_pivot_table_margins_only_column,reshape,"{""reshape.PivotTable.time_pivot_table_margins_only_column"": ""class PivotTable:\n    def time_pivot_table_margins_only_column(self):\n        self.df.pivot_table(columns=[\""key1\"", \""key2\"", \""key3\""], margins=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")""}",reshape.PivotTable.time_pivot_table_margins_only_column,time,0.0
4232,param-level,"terminus-2,oracle",1.1391285843457797,1.1391285843457797,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.PivotTable.time_pivot_table_categorical,reshape,"{""reshape.PivotTable.time_pivot_table_categorical"": ""class PivotTable:\n    def time_pivot_table_categorical(self):\n        self.df2.pivot_table(\n            index=\""col1\"", values=\""col3\"", columns=\""col2\"", aggfunc=np.sum, fill_value=0\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")""}",reshape.PivotTable.time_pivot_table_categorical,time,0.0
4244,param-level,"terminus-2,oracle",1.0835962245212107,1.0835962245212107,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.GroupbyLargeGroups.time_rolling_multiindex_creation,rolling,"{""rolling.GroupbyLargeGroups.time_rolling_multiindex_creation"": ""class GroupbyLargeGroups:\n    def time_rolling_multiindex_creation(self):\n        self.df.groupby(\""A\"").rolling(3).mean()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupbyLargeGroups:\n    def setup(self):\n        N = 100000\n        self.df = pd.DataFrame({\""A\"": [1, 2] * (N // 2), \""B\"": np.random.randn(N)})""}",rolling.GroupbyLargeGroups.time_rolling_multiindex_creation,time,0.0
4236,param-level,"terminus-2,oracle",1.4175765820288715,1.4175765820288715,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.ReshapeExtensionDtype.time_stack,reshape,"{""reshape.ReshapeExtensionDtype.time_stack"": ""class ReshapeExtensionDtype:\n    def time_stack(self, dtype):\n        self.df.stack()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReshapeExtensionDtype:\n    def setup(self, dtype):\n        lev = pd.Index(list(\""ABCDEFGHIJ\""))\n        ri = pd.Index(range(1000))\n        mi = MultiIndex.from_product([lev, ri], names=[\""foo\"", \""bar\""])\n    \n        index = date_range(\""2016-01-01\"", periods=10000, freq=\""s\"", tz=\""US/Pacific\"")\n        if dtype == \""Period[s]\"":\n            index = index.tz_localize(None).to_period(\""s\"")\n    \n        ser = pd.Series(index, index=mi)\n        df = ser.unstack(\""bar\"")\n        # roundtrips -> df.stack().equals(ser)\n    \n        self.ser = ser\n        self.df = df""}",reshape.ReshapeExtensionDtype.time_stack,time,0.0
4239,param-level,"terminus-2,oracle",1.0570098215679355,1.0570098215679355,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.EWMMethods.time_ewm,rolling,"{""rolling.EWMMethods.time_ewm"": ""class EWMMethods:\n    def time_ewm(self, constructor, kwargs_method, dtype):\n        getattr(self.ewm, self.method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass EWMMethods:\n    def setup(self, constructor, kwargs_method, dtype):\n        N = 10**5\n        kwargs, method = kwargs_method\n        arr = (100 * np.random.random(N)).astype(dtype)\n        self.method = method\n        self.ewm = getattr(pd, constructor)(arr).ewm(**kwargs)""}",rolling.EWMMethods.time_ewm,time,0.0
4238,param-level,"terminus-2,oracle",1.0542741512301976,1.0542741512301976,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Apply.time_rolling,rolling,"{""rolling.Apply.time_rolling"": ""class Apply:\n    def time_rolling(self, constructor, window, dtype, function, raw):\n        self.roll.apply(function, raw=raw)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, constructor, window, dtype, function, raw):\n        N = 10**3\n        arr = (100 * np.random.random(N)).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)""}",rolling.Apply.time_rolling,time,0.0
4245,param-level,"terminus-2,oracle",1.0461441520445638,1.0461441520445638,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Methods.time_method,rolling,"{""rolling.Methods.time_method"": ""class Methods:\n    def time_method(self, constructor, window_kwargs, dtype, method):\n        getattr(self.window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Methods:\n    def setup(self, constructor, window_kwargs, dtype, method):\n        N = 10**5\n        window, kwargs = window_kwargs\n        arr = (100 * np.random.random(N)).astype(dtype)\n        obj = getattr(pd, constructor)(arr)\n        self.window = getattr(obj, window)(**kwargs)""}",rolling.Methods.time_method,time,0.0
4240,param-level,"terminus-2,oracle",1.0448612492390277,1.0448612492390277,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.ForwardWindowMethods.time_rolling,rolling,"{""rolling.ForwardWindowMethods.time_rolling"": ""class ForwardWindowMethods:\n    def time_rolling(self, constructor, window_size, dtype, method):\n        getattr(self.roll, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ForwardWindowMethods:\n    def setup(self, constructor, window_size, dtype, method):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=window_size)\n        self.roll = getattr(pd, constructor)(arr).rolling(window=indexer)""}",rolling.ForwardWindowMethods.time_rolling,time,0.0
4251,param-level,"terminus-2,oracle",1.046121270484629,1.046121270484629,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.VariableWindowMethods.time_method,rolling,"{""rolling.VariableWindowMethods.time_method"": ""class Methods:\n    def time_method(self, constructor, window_kwargs, dtype, method):\n        getattr(self.window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass VariableWindowMethods:\n    def setup(self, constructor, window, dtype, method):\n        N = 10**5\n        arr = (100 * np.random.random(N)).astype(dtype)\n        index = pd.date_range(\""2017-01-01\"", periods=N, freq=\""5s\"")\n        self.window = getattr(pd, constructor)(arr, index=index).rolling(window)""}",rolling.VariableWindowMethods.time_method,time,0.0
4234,param-level,"terminus-2,oracle",1.085041769248503,1.085041769248503,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.PivotTable.time_pivot_table_margins,reshape,"{""reshape.PivotTable.time_pivot_table_margins"": ""class PivotTable:\n    def time_pivot_table_margins(self):\n        self.df.pivot_table(index=\""key1\"", columns=[\""key2\"", \""key3\""], margins=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")""}",reshape.PivotTable.time_pivot_table_margins,time,0.0
4243,param-level,"terminus-2,oracle",1.0461862350524451,1.0461862350524451,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.GroupbyEWMEngine.time_groupby_mean,rolling,"{""rolling.GroupbyEWMEngine.time_groupby_mean"": ""class GroupbyEWMEngine:\n    def time_groupby_mean(self, engine):\n        self.gb_ewm.mean(engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupbyEWMEngine:\n    def setup(self, engine):\n        df = pd.DataFrame({\""A\"": range(50), \""B\"": range(50)})\n        self.gb_ewm = df.groupby(\""A\"").ewm(com=1.0)""}",rolling.GroupbyEWMEngine.time_groupby_mean,time,0.0
4242,param-level,"terminus-2,oracle",1.1082312294361656,1.1082312294361656,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.GroupbyEWM.time_groupby_method,rolling,"{""rolling.GroupbyEWM.time_groupby_method"": ""class GroupbyEWM:\n    def time_groupby_method(self, method):\n        getattr(self.gb_ewm, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupbyEWM:\n    def setup(self, method):\n        df = pd.DataFrame({\""A\"": range(50), \""B\"": range(50)})\n        self.gb_ewm = df.groupby(\""A\"").ewm(com=1.0)""}",rolling.GroupbyEWM.time_groupby_method,time,0.0
4253,param-level,"terminus-2,oracle",1.1976886276828178,1.1976886276828178,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.Fillna.time_fillna,series_methods,"{""series_methods.Fillna.time_fillna"": ""class Fillna:\n    def time_fillna(self, dtype, method):\n        value = self.fill_value if method is None else None\n        self.ser.fillna(value=value, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Fillna:\n    def setup(self, dtype, method):\n        N = 10**6\n        if dtype == \""datetime64[ns]\"":\n            data = date_range(\""2000-01-01\"", freq=\""S\"", periods=N)\n            na_value = NaT\n        elif dtype in (\""float64\"", \""Float64\""):\n            data = np.random.randn(N)\n            na_value = np.nan\n        elif dtype in (\""Int64\"", \""int64[pyarrow]\""):\n            data = np.arange(N)\n            na_value = NA\n        elif dtype in (\""string\"", \""string[pyarrow]\""):\n            data = tm.rands_array(5, N)\n            na_value = NA\n        else:\n            raise NotImplementedError\n        fill_value = data[0]\n        ser = Series(data, dtype=dtype)\n        ser[::2] = na_value\n        self.ser = ser\n        self.fill_value = fill_value""}",series_methods.Fillna.time_fillna,time,0.0
4246,param-level,"terminus-2,oracle",1.0714356834437433,1.0714356834437433,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Pairwise.time_groupby,rolling,"{""rolling.Pairwise.time_groupby"": ""class Pairwise:\n    def time_groupby(self, kwargs_window, method, pairwise):\n        getattr(self.window_group, method)(self.df, pairwise=pairwise)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Pairwise:\n    def setup(self, kwargs_window, method, pairwise):\n        N = 10**4\n        n_groups = 20\n        kwargs, window = kwargs_window\n        groups = [i for _ in range(N // n_groups) for i in range(n_groups)]\n        arr = np.random.random(N)\n        self.df = pd.DataFrame(arr)\n        self.window = getattr(self.df, window)(**kwargs)\n        self.window_group = getattr(\n            pd.DataFrame({\""A\"": groups, \""B\"": arr}).groupby(\""A\""), window\n        )(**kwargs)""}",rolling.Pairwise.time_groupby,time,0.0
4231,param-level,"terminus-2,oracle",1.1113814191673432,1.1113814191673432,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.Melt.time_melt_dataframe,reshape,"{""reshape.Melt.time_melt_dataframe"": ""class Melt:\n    def time_melt_dataframe(self, dtype):\n        melt(self.df, id_vars=[\""id1\"", \""id2\""])\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Melt:\n    def setup(self, dtype):\n        self.df = DataFrame(\n            np.random.randn(100_000, 3), columns=[\""A\"", \""B\"", \""C\""], dtype=dtype\n        )\n        self.df[\""id1\""] = pd.Series(np.random.randint(0, 10, 10000))\n        self.df[\""id2\""] = pd.Series(np.random.randint(100, 1000, 10000))""}",reshape.Melt.time_melt_dataframe,time,0.0
4241,param-level,"terminus-2,oracle",1.0888663070669695,1.0888663070669695,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Groupby.time_method,rolling,"{""rolling.Groupby.time_method"": ""class Groupby:\n    def time_method(self, method, window_kwargs):\n        getattr(self.groupby_window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Groupby:\n    def setup(self, method, window_kwargs):\n        N = 1000\n        window, kwargs = window_kwargs\n        df = pd.DataFrame(\n            {\n                \""A\"": [str(i) for i in range(N)] * 10,\n                \""B\"": list(range(N)) * 10,\n            }\n        )\n        if isinstance(kwargs.get(\""window\"", None), str):\n            df.index = pd.date_range(start=\""1900-01-01\"", freq=\""1min\"", periods=N * 10)\n        self.groupby_window = getattr(df.groupby(\""A\""), window)(**kwargs)""}",rolling.Groupby.time_method,time,0.0
4248,param-level,"terminus-2,oracle",1.0647197439195957,1.0647197439195957,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Quantile.time_quantile,rolling,"{""rolling.Quantile.time_quantile"": ""class Quantile:\n    def time_quantile(self, constructor, window, dtype, percentile, interpolation):\n        self.roll.quantile(percentile, interpolation=interpolation)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, constructor, window, dtype, percentile, interpolation):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)""}",rolling.Quantile.time_quantile,time,0.0
4256,param-level,"terminus-2,oracle",1.0118678655169218,1.0118678655169218,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.SeriesConstructor.time_constructor_dict,series_methods,"{""series_methods.SeriesConstructor.time_constructor_dict"": ""class SeriesConstructor:\n    def time_constructor_dict(self):\n        Series(data=self.data, index=self.idx)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesConstructor:\n    def setup(self):\n        self.idx = date_range(\n            start=datetime(2015, 10, 26), end=datetime(2016, 1, 1), freq=\""50s\""\n        )\n        self.data = dict(zip(self.idx, range(len(self.idx))))\n        self.array = np.array([1, 2, 3])\n        self.idx2 = Index([\""a\"", \""b\"", \""c\""])""}",series_methods.SeriesConstructor.time_constructor_dict,time,0.0
4259,param-level,"terminus-2,oracle",0.91583897802989,0.91583897802989,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.Arithmetic.time_divide,sparse,"{""sparse.Arithmetic.time_divide"": ""class Arithmetic:\n    def time_divide(self, dense_proportion, fill_value):\n        self.array1 / self.array2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)""}",sparse.Arithmetic.time_divide,time,0.0
4254,param-level,"terminus-2,oracle",1.0573150212261662,1.0573150212261662,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.NanOps.time_func,series_methods,"{""series_methods.NanOps.time_func"": ""class NanOps:\n    def time_func(self, func, N, dtype):\n        self.func()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NanOps:\n    def setup(self, func, N, dtype):\n        if func == \""argmax\"" and dtype in {\""Int64\"", \""boolean\""}:\n            # Skip argmax for nullable int since this doesn't work yet (GH-24382)\n            raise NotImplementedError\n        self.s = Series([1] * N, dtype=dtype)\n        self.func = getattr(self.s, func)""}",series_methods.NanOps.time_func,time,0.0
4249,param-level,"terminus-2,oracle",1.024073566008088,1.024073566008088,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Rank.time_rank,rolling,"{""rolling.Rank.time_rank"": ""class Rank:\n    def time_rank(self, constructor, window, dtype, percentile, ascending, method):\n        self.roll.rank(pct=percentile, ascending=ascending, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Rank:\n    def setup(self, constructor, window, dtype, percentile, ascending, method):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)""}",rolling.Rank.time_rank,time,0.0
4255,param-level,"terminus-2,oracle",1.0549105958071132,1.0549105958071132,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.SearchSorted.time_searchsorted,series_methods,"{""series_methods.SearchSorted.time_searchsorted"": ""class SearchSorted:\n    def time_searchsorted(self, dtype):\n        key = \""2\"" if dtype == \""str\"" else 2\n        self.s.searchsorted(key)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SearchSorted:\n    def setup(self, dtype):\n        N = 10**5\n        data = np.array([1] * N + [2] * N + [3] * N).astype(dtype)\n        self.s = Series(data)""}",series_methods.SearchSorted.time_searchsorted,time,0.0
4247,param-level,"terminus-2,oracle",1.0529958245106887,1.0529958245106887,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Pairwise.time_pairwise,rolling,"{""rolling.Pairwise.time_pairwise"": ""class Pairwise:\n    def time_pairwise(self, kwargs_window, method, pairwise):\n        getattr(self.window, method)(self.df, pairwise=pairwise)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Pairwise:\n    def setup(self, kwargs_window, method, pairwise):\n        N = 10**4\n        n_groups = 20\n        kwargs, window = kwargs_window\n        groups = [i for _ in range(N // n_groups) for i in range(n_groups)]\n        arr = np.random.random(N)\n        self.df = pd.DataFrame(arr)\n        self.window = getattr(self.df, window)(**kwargs)\n        self.window_group = getattr(\n            pd.DataFrame({\""A\"": groups, \""B\"": arr}).groupby(\""A\""), window\n        )(**kwargs)""}",rolling.Pairwise.time_pairwise,time,0.0
4269,param-level,"terminus-2,oracle",1.027937502439999,1.027937502439999,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,stat_ops.Correlation.time_corrwith_rows,stat_ops,"{""stat_ops.Correlation.time_corrwith_rows"": ""class Correlation:\n    def time_corrwith_rows(self, method):\n        self.df.corrwith(self.df2, axis=1, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Correlation:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(500, 15))\n        self.df2 = pd.DataFrame(np.random.randn(500, 15))\n        self.df_wide = pd.DataFrame(np.random.randn(500, 100))\n        self.df_wide_nans = self.df_wide.where(np.random.random((500, 100)) < 0.9)\n        self.s = pd.Series(np.random.randn(500))\n        self.s2 = pd.Series(np.random.randn(500))""}",stat_ops.Correlation.time_corrwith_rows,time,0.0
4257,param-level,"terminus-2,oracle",0.9192684582296644,0.9192684582296644,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.ToFrame.time_to_frame,series_methods,"{""series_methods.ToFrame.time_to_frame"": ""class ToFrame:\n    def time_to_frame(self, dtype, name):\n        self.ser.to_frame(name)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToFrame:\n    def setup(self, dtype, name):\n        arr = np.arange(10**5)\n        ser = Series(arr, dtype=dtype)\n        self.ser = ser""}",series_methods.ToFrame.time_to_frame,time,0.0
4265,param-level,"terminus-2,oracle",1.0273384746739196,1.0273384746739196,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,stat_ops.Correlation.time_corr,stat_ops,"{""stat_ops.Correlation.time_corr"": ""class Correlation:\n    def time_corr(self, method):\n        self.df.corr(method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Correlation:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(500, 15))\n        self.df2 = pd.DataFrame(np.random.randn(500, 15))\n        self.df_wide = pd.DataFrame(np.random.randn(500, 100))\n        self.df_wide_nans = self.df_wide.where(np.random.random((500, 100)) < 0.9)\n        self.s = pd.Series(np.random.randn(500))\n        self.s2 = pd.Series(np.random.randn(500))""}",stat_ops.Correlation.time_corr,time,0.0
4258,param-level,"terminus-2,oracle",0.9048225669589512,0.9048225669589512,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.Arithmetic.time_add,sparse,"{""sparse.Arithmetic.time_add"": ""class Arithmetic:\n    def time_add(self, dense_proportion, fill_value):\n        self.array1 + self.array2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)""}",sparse.Arithmetic.time_add,time,0.0
4263,param-level,"terminus-2,oracle",0.8920809027583091,0.8920809027583091,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.ArithmeticBlock.time_make_union,sparse,"{""sparse.ArithmeticBlock.time_make_union"": ""class ArithmeticBlock:\n    def time_make_union(self, fill_value):\n        self.arr1.sp_index.make_union(self.arr2.sp_index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )""}",sparse.ArithmeticBlock.time_make_union,time,0.0
4260,param-level,"terminus-2,oracle",0.9198406736292096,0.9198406736292096,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.Arithmetic.time_make_union,sparse,"{""sparse.Arithmetic.time_make_union"": ""class Arithmetic:\n    def time_make_union(self, dense_proportion, fill_value):\n        self.array1.sp_index.make_union(self.array2.sp_index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)""}",sparse.Arithmetic.time_make_union,time,0.0
4267,param-level,"terminus-2,oracle",1.012040175725413,1.012040175725413,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,stat_ops.Correlation.time_corr_wide_nans,stat_ops,"{""stat_ops.Correlation.time_corr_wide_nans"": ""class Correlation:\n    def time_corr_wide_nans(self, method):\n        self.df_wide_nans.corr(method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Correlation:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(500, 15))\n        self.df2 = pd.DataFrame(np.random.randn(500, 15))\n        self.df_wide = pd.DataFrame(np.random.randn(500, 100))\n        self.df_wide_nans = self.df_wide.where(np.random.random((500, 100)) < 0.9)\n        self.s = pd.Series(np.random.randn(500))\n        self.s2 = pd.Series(np.random.randn(500))""}",stat_ops.Correlation.time_corr_wide_nans,time,0.0
4262,param-level,"terminus-2,oracle",0.9086828083158498,0.9086828083158498,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.ArithmeticBlock.time_division,sparse,"{""sparse.ArithmeticBlock.time_division"": ""class ArithmeticBlock:\n    def time_division(self, fill_value):\n        self.arr1 / self.arr2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )""}",sparse.ArithmeticBlock.time_division,time,0.0
4250,param-level,"terminus-2,oracle",1.060642811266571,1.060642811266571,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.TableMethod.time_apply,rolling,"{""rolling.TableMethod.time_apply"": ""class TableMethod:\n    def time_apply(self, method):\n        self.df.rolling(2, method=method).apply(\n            table_method_func, raw=True, engine=\""numba\""\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TableMethod:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(10, 1000))""}",rolling.TableMethod.time_apply,time,0.0
4266,param-level,"terminus-2,oracle",1.0425458957734757,1.0425458957734757,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,stat_ops.Correlation.time_corr_series,stat_ops,"{""stat_ops.Correlation.time_corr_series"": ""class Correlation:\n    def time_corr_series(self, method):\n        self.s.corr(self.s2, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Correlation:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(500, 15))\n        self.df2 = pd.DataFrame(np.random.randn(500, 15))\n        self.df_wide = pd.DataFrame(np.random.randn(500, 100))\n        self.df_wide_nans = self.df_wide.where(np.random.random((500, 100)) < 0.9)\n        self.s = pd.Series(np.random.randn(500))\n        self.s2 = pd.Series(np.random.randn(500))""}",stat_ops.Correlation.time_corr_series,time,0.0
4264,param-level,"terminus-2,oracle",1.0153968750429954,1.0153968750429954,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.Take.time_take,sparse,"{""sparse.Take.time_take"": ""class Take:\n    def time_take(self, indices, allow_fill):\n        self.sp_arr.take(indices, allow_fill=allow_fill)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Take:\n    def setup(self, indices, allow_fill):\n        N = 1_000_000\n        fill_value = 0.0\n        arr = make_array(N, 1e-5, fill_value, np.float64)\n        self.sp_arr = SparseArray(arr, fill_value=fill_value)""}",sparse.Take.time_take,time,0.0
4270,param-level,"terminus-2,oracle",1.0475539470866344,1.0475539470866344,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,stat_ops.FrameMultiIndexOps.time_op,stat_ops,"{""stat_ops.FrameMultiIndexOps.time_op"": ""class FrameMultiIndexOps:\n    def time_op(self, op):\n        self.df_func()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FrameMultiIndexOps:\n    def setup(self, op):\n        levels = [np.arange(10), np.arange(100), np.arange(100)]\n        codes = [\n            np.arange(10).repeat(10000),\n            np.tile(np.arange(100).repeat(100), 10),\n            np.tile(np.tile(np.arange(100), 100), 10),\n        ]\n        index = pd.MultiIndex(levels=levels, codes=codes)\n        df = pd.DataFrame(np.random.randn(len(index), 4), index=index)\n        self.df_func = getattr(df, op)""}",stat_ops.FrameMultiIndexOps.time_op,time,0.0
4272,param-level,"terminus-2,oracle",1.0235986125470364,1.0235986125470364,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,stat_ops.SeriesOps.time_op,stat_ops,"{""stat_ops.SeriesOps.time_op"": ""class SeriesOps:\n    def time_op(self, op, dtype):\n        self.s_func()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesOps:\n    def setup(self, op, dtype):\n        s = pd.Series(np.random.randn(100000)).astype(dtype)\n        self.s_func = getattr(s, op)""}",stat_ops.SeriesOps.time_op,time,0.0
4252,param-level,"terminus-2,oracle",1.0243815660476452,1.0243815660476452,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.Any.time_any,series_methods,"{""series_methods.Any.time_any"": ""class Any:\n    def time_any(self, N, case, dtype):\n        self.s.any()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Any:\n    def setup(self, N, case, dtype):\n        val = case == \""fast\""\n        self.s = Series([val] * N, dtype=dtype)""}",series_methods.Any.time_any,time,0.0
4273,param-level,"terminus-2,oracle",1.026777433579889,1.026777433579889,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strftime.BusinessHourStrftime.time_frame_offset_repr,strftime,"{""strftime.BusinessHourStrftime.time_frame_offset_repr"": ""class BusinessHourStrftime:\n    def time_frame_offset_repr(self, obs):\n        self.data[\""off\""].apply(repr)\n\n    def setup(self, obs):\n        self.data = pd.DataFrame(\n            {\n                \""off\"": [offsets.BusinessHour()] * obs,\n            }\n        )""}",strftime.BusinessHourStrftime.time_frame_offset_repr,time,0.0
4268,param-level,"terminus-2,oracle",1.0308942388787077,1.0308942388787077,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,stat_ops.Correlation.time_corrwith_cols,stat_ops,"{""stat_ops.Correlation.time_corrwith_cols"": ""class Correlation:\n    def time_corrwith_cols(self, method):\n        self.df.corrwith(self.df2, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Correlation:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(500, 15))\n        self.df2 = pd.DataFrame(np.random.randn(500, 15))\n        self.df_wide = pd.DataFrame(np.random.randn(500, 100))\n        self.df_wide_nans = self.df_wide.where(np.random.random((500, 100)) < 0.9)\n        self.s = pd.Series(np.random.randn(500))\n        self.s2 = pd.Series(np.random.randn(500))""}",stat_ops.Correlation.time_corrwith_cols,time,0.0
4271,param-level,"terminus-2,oracle",1.0357025052036626,1.0357025052036626,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,stat_ops.FrameOps.time_op,stat_ops,"{""stat_ops.FrameOps.time_op"": ""class FrameOps:\n    def time_op(self, op, dtype, axis):\n        self.df_func(axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FrameOps:\n    def setup(self, op, dtype, axis):\n        values = np.random.randn(100000, 4)\n        if dtype == \""Int64\"":\n            values = values.astype(int)\n        df = pd.DataFrame(values).astype(dtype)\n        self.df_func = getattr(df, op)""}",stat_ops.FrameOps.time_op,time,0.0
4279,param-level,"terminus-2,oracle",1.02486375275368,1.02486375275368,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Methods.time_findall,strings,"{""strings.Methods.time_findall"": ""class Methods:\n    def time_findall(self, dtype):\n        self.s.str.findall(\""[A-Z]+\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.Methods.time_findall,time,0.0
4261,param-level,"terminus-2,oracle",0.912691752252956,0.912691752252956,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.ArithmeticBlock.time_addition,sparse,"{""sparse.ArithmeticBlock.time_addition"": ""class ArithmeticBlock:\n    def time_addition(self, fill_value):\n        self.arr1 + self.arr2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )""}",sparse.ArithmeticBlock.time_addition,time,0.0
4280,param-level,"terminus-2,oracle",1.0329384160058914,1.0329384160058914,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Methods.time_fullmatch,strings,"{""strings.Methods.time_fullmatch"": ""class Methods:\n    def time_fullmatch(self, dtype):\n        self.s.str.fullmatch(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.Methods.time_fullmatch,time,0.0
4283,param-level,"terminus-2,oracle",1.0251988182556808,1.0251988182556808,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Methods.time_istitle,strings,"{""strings.Methods.time_istitle"": ""class Methods:\n    def time_istitle(self, dtype):\n        self.s.str.istitle()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.Methods.time_istitle,time,0.0
4290,param-level,"terminus-2,oracle",1.0368987124797229,1.0368987124797229,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timedelta.TimedeltaIndexing.time_series_loc,timedelta,"{""timedelta.TimedeltaIndexing.time_series_loc"": ""class TimedeltaIndexing:\n    def time_series_loc(self):\n        self.series.loc[self.timedelta]\n\n    def setup(self):\n        self.index = timedelta_range(start=\""1985\"", periods=1000, freq=\""D\"")\n        self.index2 = timedelta_range(start=\""1986\"", periods=1000, freq=\""D\"")\n        self.series = Series(range(1000), index=self.index)\n        self.timedelta = self.index[500]""}",timedelta.TimedeltaIndexing.time_series_loc,time,0.0
4275,param-level,"terminus-2,oracle",1.0107349830512775,1.0107349830512775,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strftime.DatetimeStrftime.time_frame_datetime_to_str,strftime,"{""strftime.DatetimeStrftime.time_frame_datetime_to_str"": ""class DatetimeStrftime:\n    def time_frame_datetime_to_str(self, obs):\n        self.data[\""dt\""].astype(str)\n\n    def setup(self, obs):\n        d = \""2018-11-29\""\n        dt = \""2018-11-26 11:18:27.0\""\n        self.data = pd.DataFrame(\n            {\n                \""dt\"": [np.datetime64(dt)] * obs,\n                \""d\"": [np.datetime64(d)] * obs,\n                \""r\"": [np.random.uniform()] * obs,\n            }\n        )""}",strftime.DatetimeStrftime.time_frame_datetime_to_str,time,0.0
4276,param-level,"terminus-2,oracle",1.0221869382018522,1.0221869382018522,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Contains.time_contains,strings,"{""strings.Contains.time_contains"": ""class Contains:\n    def time_contains(self, dtype, regex):\n        self.s.str.contains(\""A\"", regex=regex)\n\n    def setup(self, dtype, regex):\n        super().setup(dtype)""}",strings.Contains.time_contains,time,0.0
4285,param-level,"terminus-2,oracle",1.053556804380056,1.053556804380056,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Methods.time_match,strings,"{""strings.Methods.time_match"": ""class Methods:\n    def time_match(self, dtype):\n        self.s.str.match(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.Methods.time_match,time,0.0
4288,param-level,"terminus-2,oracle",1.005475445309784,1.005475445309784,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timedelta.DatetimeAccessor.time_timedelta_seconds,timedelta,"{""timedelta.DatetimeAccessor.time_timedelta_seconds"": ""class DatetimeAccessor:\n    def time_timedelta_seconds(self, series):\n        series.dt.seconds\n\n    def setup_cache(self):\n        N = 100000\n        series = Series(timedelta_range(\""1 days\"", periods=N, freq=\""h\""))\n        return series""}",timedelta.DatetimeAccessor.time_timedelta_seconds,time,0.0
4274,param-level,"terminus-2,oracle",1.029138406425122,1.029138406425122,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strftime.BusinessHourStrftime.time_frame_offset_str,strftime,"{""strftime.BusinessHourStrftime.time_frame_offset_str"": ""class BusinessHourStrftime:\n    def time_frame_offset_str(self, obs):\n        self.data[\""off\""].apply(str)\n\n    def setup(self, obs):\n        self.data = pd.DataFrame(\n            {\n                \""off\"": [offsets.BusinessHour()] * obs,\n            }\n        )""}",strftime.BusinessHourStrftime.time_frame_offset_str,time,0.0
4286,param-level,"terminus-2,oracle",1.3596393751609956,1.3596393751609956,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Methods.time_upper,strings,"{""strings.Methods.time_upper"": ""class Methods:\n    def time_upper(self, dtype):\n        self.s.str.upper()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.Methods.time_upper,time,0.0
4277,param-level,"terminus-2,oracle",1.018177527973979,1.018177527973979,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Dummies.time_get_dummies,strings,"{""strings.Dummies.time_get_dummies"": ""class Dummies:\n    def time_get_dummies(self, dtype):\n        self.s.str.get_dummies(\""|\"")\n\n    def setup(self, dtype):\n        super().setup(dtype)\n        self.s = self.s.str.join(\""|\"")""}",strings.Dummies.time_get_dummies,time,0.0
4278,param-level,"terminus-2,oracle",1.037615166606883,1.037615166606883,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Methods.time_endswith,strings,"{""strings.Methods.time_endswith"": ""class Methods:\n    def time_endswith(self, dtype):\n        self.s.str.endswith(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.Methods.time_endswith,time,0.0
4284,param-level,"terminus-2,oracle",1.3086013614510008,1.3086013614510008,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Methods.time_lower,strings,"{""strings.Methods.time_lower"": ""class Methods:\n    def time_lower(self, dtype):\n        self.s.str.lower()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.Methods.time_lower,time,0.0
4281,param-level,"terminus-2,oracle",1.0196207599438951,1.0196207599438951,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Methods.time_get,strings,"{""strings.Methods.time_get"": ""class Methods:\n    def time_get(self, dtype):\n        self.s.str.get(0)\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.Methods.time_get,time,0.0
4282,param-level,"terminus-2,oracle",1.0532729945250614,1.0532729945250614,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Methods.time_isdigit,strings,"{""strings.Methods.time_isdigit"": ""class Methods:\n    def time_isdigit(self, dtype):\n        self.s.str.isdigit()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.Methods.time_isdigit,time,0.0
4289,param-level,"terminus-2,oracle",1.0827033762685614,1.0827033762685614,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timedelta.TimedeltaIndexing.time_intersection,timedelta,"{""timedelta.TimedeltaIndexing.time_intersection"": ""class TimedeltaIndexing:\n    def time_intersection(self):\n        self.index.intersection(self.index2)\n\n    def setup(self):\n        self.index = timedelta_range(start=\""1985\"", periods=1000, freq=\""D\"")\n        self.index2 = timedelta_range(start=\""1986\"", periods=1000, freq=\""D\"")\n        self.series = Series(range(1000), index=self.index)\n        self.timedelta = self.index[500]""}",timedelta.TimedeltaIndexing.time_intersection,time,0.0
4298,param-level,"terminus-2,oracle",1.0714083894064304,1.0714083894064304,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.TzLocalize.time_infer_dst,timeseries,"{""timeseries.TzLocalize.time_infer_dst"": ""class TzLocalize:\n    def time_infer_dst(self, tz):\n        self.index.tz_localize(tz, ambiguous=\""infer\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TzLocalize:\n    def setup(self, tz):\n        dst_rng = date_range(\n            start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n        )\n        self.index = date_range(start=\""10/29/2000\"", end=\""10/29/2000 00:59:59\"", freq=\""S\"")\n        self.index = self.index.append(dst_rng)\n        self.index = self.index.append(dst_rng)\n        self.index = self.index.append(\n            date_range(start=\""10/29/2000 2:00:00\"", end=\""10/29/2000 3:00:00\"", freq=\""S\"")\n        )""}",timeseries.TzLocalize.time_infer_dst,time,0.0
4287,param-level,"terminus-2,oracle",1.0312078855701563,1.0312078855701563,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Methods.time_wrap,strings,"{""strings.Methods.time_wrap"": ""class Methods:\n    def time_wrap(self, dtype):\n        self.s.str.wrap(10)\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.Methods.time_wrap,time,0.0
4296,param-level,"terminus-2,oracle",1.010748999041344,1.010748999041344,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.DatetimeIndex.time_to_time,timeseries,"{""timeseries.DatetimeIndex.time_to_time"": ""class DatetimeIndex:\n    def time_to_time(self, index_type):\n        self.index.time\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]""}",timeseries.DatetimeIndex.time_to_time,time,0.0
4293,param-level,"terminus-2,oracle",0.96242104773653,0.96242104773653,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.DatetimeIndex.time_get,timeseries,"{""timeseries.DatetimeIndex.time_get"": ""class DatetimeIndex:\n    def time_get(self, index_type):\n        self.index[0]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]""}",timeseries.DatetimeIndex.time_get,time,0.0
4292,param-level,"terminus-2,oracle",0.9450309626574186,0.9450309626574186,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.DatetimeAccessor.time_dt_accessor_month_name,timeseries,"{""timeseries.DatetimeAccessor.time_dt_accessor_month_name"": ""class DatetimeAccessor:\n    def time_dt_accessor_month_name(self, tz):\n        self.series.dt.month_name()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeAccessor:\n    def setup(self, tz):\n        N = 100000\n        self.series = Series(date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"", tz=tz))""}",timeseries.DatetimeAccessor.time_dt_accessor_month_name,time,0.0
4294,param-level,"terminus-2,oracle",0.9911992258081832,0.9911992258081832,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.DatetimeIndex.time_timeseries_is_month_start,timeseries,"{""timeseries.DatetimeIndex.time_timeseries_is_month_start"": ""class DatetimeIndex:\n    def time_timeseries_is_month_start(self, index_type):\n        self.index.is_month_start\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]""}",timeseries.DatetimeIndex.time_timeseries_is_month_start,time,0.0
4300,param-level,"terminus-2,oracle",1.026500169451337,1.026500169451337,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.fields.TimeGetStartEndField.time_get_start_end_field,tslibs.fields,"{""tslibs.fields.TimeGetStartEndField.time_get_start_end_field"": ""class TimeGetStartEndField:\n    def time_get_start_end_field(self, size, side, period, freqstr, month_kw):\n        get_start_end_field(self.i8data, self.attrname, freqstr, month_kw=month_kw)\n\n    def setup(self, size, side, period, freqstr, month_kw):\n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr\n    \n        self.attrname = f\""is_{period}_{side}\""""}",tslibs.fields.TimeGetStartEndField.time_get_start_end_field,time,0.0
4302,param-level,"terminus-2,oracle",0.9573888041737096,0.9573888041737096,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.normalize.Normalize.time_is_date_array_normalized,tslibs.normalize,"{""tslibs.normalize.Normalize.time_is_date_array_normalized"": ""class Normalize:\n    def time_is_date_array_normalized(self, size, tz):\n        # TODO: cases with different levels of short-circuiting\n        # 10 i.e. NPY_FR_ns\n        is_date_array_normalized(self.i8data, tz, 10)\n\n    def setup(self, size, tz):\n        # use an array that will have is_date_array_normalized give True,\n        #  so we do not short-circuit early.\n        dti = pd.date_range(\""2016-01-01\"", periods=10, tz=tz).repeat(size // 10)\n        self.i8data = dti.asi8\n    \n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError""}",tslibs.normalize.Normalize.time_is_date_array_normalized,time,0.0
4301,param-level,"terminus-2,oracle",0.9843482318148696,0.9843482318148696,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.fields.TimeGetTimedeltaField.time_get_timedelta_field,tslibs.fields,"{""tslibs.fields.TimeGetTimedeltaField.time_get_timedelta_field"": ""class TimeGetTimedeltaField:\n    def time_get_timedelta_field(self, size, field):\n        get_timedelta_field(self.i8data, field)\n\n    def setup(self, size, field):\n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.fields.TimeGetTimedeltaField.time_get_timedelta_field,time,0.0
4295,param-level,"terminus-2,oracle",0.9852909091765004,0.9852909091765004,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.DatetimeIndex.time_to_date,timeseries,"{""timeseries.DatetimeIndex.time_to_date"": ""class DatetimeIndex:\n    def time_to_date(self, index_type):\n        self.index.date\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]""}",timeseries.DatetimeIndex.time_to_date,time,0.0
4299,param-level,"terminus-2,oracle",0.9810336757022337,0.9810336757022337,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.fields.TimeGetDateField.time_get_date_field,tslibs.fields,"{""tslibs.fields.TimeGetDateField.time_get_date_field"": ""class TimeGetDateField:\n    def time_get_date_field(self, size, field):\n        get_date_field(self.i8data, field)\n\n    def setup(self, size, field):\n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.fields.TimeGetDateField.time_get_date_field,time,0.0
4297,param-level,"terminus-2,oracle",1.0555519096179355,1.0555519096179355,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.ResetIndex.time_reset_datetimeindex,timeseries,"{""timeseries.ResetIndex.time_reset_datetimeindex"": ""class ResetIndex:\n    def time_reset_datetimeindex(self, tz):\n        self.df.reset_index()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ResetIndex:\n    def setup(self, tz):\n        idx = date_range(start=\""1/1/2000\"", periods=1000, freq=\""H\"", tz=tz)\n        self.df = DataFrame(np.random.randn(1000, 2), index=idx)""}",timeseries.ResetIndex.time_reset_datetimeindex,time,0.0
4291,param-level,"terminus-2,oracle",0.94904794756731,0.94904794756731,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.DatetimeAccessor.time_dt_accessor_day_name,timeseries,"{""timeseries.DatetimeAccessor.time_dt_accessor_day_name"": ""class DatetimeAccessor:\n    def time_dt_accessor_day_name(self, tz):\n        self.series.dt.day_name()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeAccessor:\n    def setup(self, tz):\n        N = 100000\n        self.series = Series(date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"", tz=tz))""}",timeseries.DatetimeAccessor.time_dt_accessor_day_name,time,0.0
4305,param-level,"terminus-2,oracle",1.0518811107510226,1.0518811107510226,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.OffestDatetimeArithmetic.time_add_10,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_add_10"": ""class OffestDatetimeArithmetic:\n    def time_add_10(self, offset):\n        self.date + (10 * offset)\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.OffestDatetimeArithmetic.time_add_10,time,0.0
4310,param-level,"terminus-2,oracle",1.0892287783290096,1.0892287783290096,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.PeriodConstructor.time_period_constructor,tslibs.period,"{""tslibs.period.PeriodConstructor.time_period_constructor"": ""class PeriodConstructor:\n    def time_period_constructor(self, freq, is_offset):\n        Period(\""2012-06-01\"", freq=freq)\n\n    def setup(self, freq, is_offset):\n        if is_offset:\n            self.freq = to_offset(freq)\n        else:\n            self.freq = freq""}",tslibs.period.PeriodConstructor.time_period_constructor,time,0.0
4304,param-level,"terminus-2,oracle",1.046463312646811,1.046463312646811,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.OffestDatetimeArithmetic.time_add,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_add"": ""class OffestDatetimeArithmetic:\n    def time_add(self, offset):\n        self.date + offset\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.OffestDatetimeArithmetic.time_add,time,0.0
4309,param-level,"terminus-2,oracle",1.001471372421829,1.001471372421829,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.OnOffset.time_on_offset,tslibs.offsets,"{""tslibs.offsets.OnOffset.time_on_offset"": ""class OnOffset:\n    def time_on_offset(self, offset):\n        for date in self.dates:\n            offset.is_on_offset(date)\n\n    def setup(self, offset):\n        self.dates = [\n            datetime(2016, m, d)\n            for m in [10, 11, 12]\n            for d in [1, 2, 3, 28, 29, 30, 31]\n            if not (m == 11 and d == 31)\n        ]""}",tslibs.offsets.OnOffset.time_on_offset,time,0.0
4306,param-level,"terminus-2,oracle",1.0458309907695014,1.0458309907695014,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.OffestDatetimeArithmetic.time_add_np_dt64,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_add_np_dt64"": ""class OffestDatetimeArithmetic:\n    def time_add_np_dt64(self, offset):\n        offset + self.dt64\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.OffestDatetimeArithmetic.time_add_np_dt64,time,0.0
4307,param-level,"terminus-2,oracle",1.0383551718777015,1.0383551718777015,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.OffestDatetimeArithmetic.time_subtract,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_subtract"": ""class OffestDatetimeArithmetic:\n    def time_subtract(self, offset):\n        self.date - offset\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.OffestDatetimeArithmetic.time_subtract,time,0.0
4303,param-level,"terminus-2,oracle",1.0016897797076685,1.0016897797076685,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.normalize.Normalize.time_normalize_i8_timestamps,tslibs.normalize,"{""tslibs.normalize.Normalize.time_normalize_i8_timestamps"": ""class Normalize:\n    def time_normalize_i8_timestamps(self, size, tz):\n        # 10 i.e. NPY_FR_ns\n        normalize_i8_timestamps(self.i8data, tz, 10)\n\n    def setup(self, size, tz):\n        # use an array that will have is_date_array_normalized give True,\n        #  so we do not short-circuit early.\n        dti = pd.date_range(\""2016-01-01\"", periods=10, tz=tz).repeat(size // 10)\n        self.i8data = dti.asi8\n    \n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError""}",tslibs.normalize.Normalize.time_normalize_i8_timestamps,time,0.0
4313,param-level,"terminus-2,oracle",1.0369797119462696,1.0369797119462696,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.PeriodUnaryMethods.time_now,tslibs.period,"{""tslibs.period.PeriodUnaryMethods.time_now"": ""class PeriodUnaryMethods:\n    def time_now(self, freq):\n        self.per.now(freq)\n\n    def setup(self, freq):\n        self.per = Period(\""2012-06-01\"", freq=freq)""}",tslibs.period.PeriodUnaryMethods.time_now,time,0.0
4312,param-level,"terminus-2,oracle",1.0187289916107118,1.0187289916107118,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.PeriodUnaryMethods.time_asfreq,tslibs.period,"{""tslibs.period.PeriodUnaryMethods.time_asfreq"": ""class PeriodUnaryMethods:\n    def time_asfreq(self, freq):\n        self.per.asfreq(\""A\"")\n\n    def setup(self, freq):\n        self.per = Period(\""2012-06-01\"", freq=freq)""}",tslibs.period.PeriodUnaryMethods.time_asfreq,time,0.0
4311,param-level,"terminus-2,oracle",1.0137567719325948,1.0137567719325948,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.PeriodProperties.time_property,tslibs.period,"{""tslibs.period.PeriodProperties.time_property"": ""class PeriodProperties:\n    def time_property(self, freq, attr):\n        getattr(self.per, attr)\n\n    def setup(self, freq, attr):\n        self.per = Period(\""2012-06-01\"", freq=freq)""}",tslibs.period.PeriodProperties.time_property,time,0.0
4314,param-level,"terminus-2,oracle",1.0745328131661918,1.0745328131661918,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.PeriodUnaryMethods.time_to_timestamp,tslibs.period,"{""tslibs.period.PeriodUnaryMethods.time_to_timestamp"": ""class PeriodUnaryMethods:\n    def time_to_timestamp(self, freq):\n        self.per.to_timestamp()\n\n    def setup(self, freq):\n        self.per = Period(\""2012-06-01\"", freq=freq)""}",tslibs.period.PeriodUnaryMethods.time_to_timestamp,time,0.0
4308,param-level,"terminus-2,oracle",1.0505874439255598,1.0505874439255598,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.OffestDatetimeArithmetic.time_subtract_10,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_subtract_10"": ""class OffestDatetimeArithmetic:\n    def time_subtract_10(self, offset):\n        self.date - (10 * offset)\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.OffestDatetimeArithmetic.time_subtract_10,time,0.0
4315,param-level,"terminus-2,oracle",1.0052824550428674,1.0052824550428674,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr,tslibs.period,"{""tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr"": ""class TimeDT64ArrToPeriodArr:\n    def time_dt64arr_to_periodarr(self, size, freq, tz):\n        dt64arr_to_periodarr(self.i8values, freq, tz)\n\n    def setup(self, size, freq, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.arange(10, dtype=\""i8\"").repeat(size // 10)\n        self.i8values = arr""}",tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr,time,0.0
4321,param-level,"terminus-2,oracle",1.0272665785845632,1.0272665785845632,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timedelta.TimedeltaConstructor.time_from_string,tslibs.timedelta,"{""tslibs.timedelta.TimedeltaConstructor.time_from_string"": ""class TimedeltaConstructor:\n    def time_from_string(self):\n        Timedelta(\""1 days\"")\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")""}",tslibs.timedelta.TimedeltaConstructor.time_from_string,time,0.0
4322,param-level,"terminus-2,oracle",1.0385202816031789,1.0385202816031789,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampAcrossDst.time_replace_across_dst,tslibs.timestamp,"{""tslibs.timestamp.TimestampAcrossDst.time_replace_across_dst"": ""class TimestampAcrossDst:\n    def time_replace_across_dst(self):\n        self.ts2.replace(tzinfo=self.tzinfo)\n\n    def setup(self):\n        dt = datetime(2016, 3, 27, 1)\n        self.tzinfo = pytz.timezone(\""CET\"").localize(dt, is_dst=False).tzinfo\n        self.ts2 = Timestamp(dt)""}",tslibs.timestamp.TimestampAcrossDst.time_replace_across_dst,time,0.0
4327,param-level,"terminus-2,oracle",1.0390043640338635,1.0390043640338635,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampConstruction.time_parse_iso8601_no_tz,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_parse_iso8601_no_tz"": ""class TimestampConstruction:\n    def time_parse_iso8601_no_tz(self):\n        Timestamp(\""2017-08-25 08:16:14\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.TimestampConstruction.time_parse_iso8601_no_tz,time,0.0
4330,param-level,"terminus-2,oracle",1.0381472749374108,1.0381472749374108,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampOps.time_floor,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_floor"": ""class TimestampOps:\n    def time_floor(self, tz):\n        self.ts.floor(\""5T\"")\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.TimestampOps.time_floor,time,0.0
4319,param-level,"terminus-2,oracle",1.026531993986255,1.026531993986255,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timedelta.TimedeltaConstructor.time_from_iso_format,tslibs.timedelta,"{""tslibs.timedelta.TimedeltaConstructor.time_from_iso_format"": ""class TimedeltaConstructor:\n    def time_from_iso_format(self):\n        Timedelta(\""P4DT12H30M5S\"")\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")""}",tslibs.timedelta.TimedeltaConstructor.time_from_iso_format,time,0.0
4320,param-level,"terminus-2,oracle",1.0242987605277876,1.0242987605277876,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timedelta.TimedeltaConstructor.time_from_np_timedelta,tslibs.timedelta,"{""tslibs.timedelta.TimedeltaConstructor.time_from_np_timedelta"": ""class TimedeltaConstructor:\n    def time_from_np_timedelta(self):\n        Timedelta(self.nptimedelta64)\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")""}",tslibs.timedelta.TimedeltaConstructor.time_from_np_timedelta,time,0.0
4318,param-level,"terminus-2,oracle",1.0156490510826353,1.0156490510826353,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timedelta.TimedeltaConstructor.time_from_datetime_timedelta,tslibs.timedelta,"{""tslibs.timedelta.TimedeltaConstructor.time_from_datetime_timedelta"": ""class TimedeltaConstructor:\n    def time_from_datetime_timedelta(self):\n        Timedelta(self.dttimedelta)\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")""}",tslibs.timedelta.TimedeltaConstructor.time_from_datetime_timedelta,time,0.0
4316,param-level,"terminus-2,oracle",0.9737964783501856,0.9737964783501856,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr,tslibs.period,"{""tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr"": ""class TimePeriodArrToDT64Arr:\n    def time_periodarray_to_dt64arr(self, size, freq):\n        periodarr_to_dt64arr(self.i8values, freq)\n\n    def setup(self, size, freq):\n        arr = np.arange(10, dtype=\""i8\"").repeat(size // 10)\n        self.i8values = arr""}",tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr,time,0.0
4325,param-level,"terminus-2,oracle",1.01738764873653,1.01738764873653,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampConstruction.time_from_npdatetime64,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_from_npdatetime64"": ""class TimestampConstruction:\n    def time_from_npdatetime64(self):\n        Timestamp(self.npdatetime64)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.TimestampConstruction.time_from_npdatetime64,time,0.0
4329,param-level,"terminus-2,oracle",1.0348650191997786,1.0348650191997786,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampOps.time_ceil,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_ceil"": ""class TimestampOps:\n    def time_ceil(self, tz):\n        self.ts.ceil(\""5T\"")\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.TimestampOps.time_ceil,time,0.0
4317,param-level,"terminus-2,oracle",1.0065466865962085,1.0065466865962085,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.resolution.TimeResolution.time_get_resolution,tslibs.resolution,"{""tslibs.resolution.TimeResolution.time_get_resolution"": ""class TimeResolution:\n    def time_get_resolution(self, unit, size, tz):\n        get_resolution(self.i8data, tz)\n\n    def setup(self, unit, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        arr = arr.view(f\""M8[{unit}]\"").astype(\""M8[ns]\"").view(\""i8\"")\n        self.i8data = arr""}",tslibs.resolution.TimeResolution.time_get_resolution,time,0.0
4324,param-level,"terminus-2,oracle",1.0272462397518227,1.0272462397518227,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampConstruction.time_from_datetime_unaware,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_from_datetime_unaware"": ""class TimestampConstruction:\n    def time_from_datetime_unaware(self):\n        Timestamp(self.dttime_unaware)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.TimestampConstruction.time_from_datetime_unaware,time,0.0
4328,param-level,"terminus-2,oracle",1.0164790544428826,1.0164790544428826,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampConstruction.time_parse_iso8601_tz,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_parse_iso8601_tz"": ""class TimestampConstruction:\n    def time_parse_iso8601_tz(self):\n        Timestamp(\""2017-08-25 08:16:14-0500\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.TimestampConstruction.time_parse_iso8601_tz,time,0.0
4337,param-level,"terminus-2,oracle",0.9912717761748578,0.9912717761748578,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampProperties.time_dayofyear,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_dayofyear"": ""class TimestampProperties:\n    def time_dayofyear(self, tz):\n        self.ts.dayofyear\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.TimestampProperties.time_dayofyear,time,0.0
4326,param-level,"terminus-2,oracle",1.0562877508565431,1.0562877508565431,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampConstruction.time_parse_dateutil,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_parse_dateutil"": ""class TimestampConstruction:\n    def time_parse_dateutil(self):\n        Timestamp(\""2017/08/25 08:16:14 AM\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.TimestampConstruction.time_parse_dateutil,time,0.0
4334,param-level,"terminus-2,oracle",1.0201454057753734,1.0201454057753734,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampOps.time_to_julian_date,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_to_julian_date"": ""class TimestampOps:\n    def time_to_julian_date(self, tz):\n        self.ts.to_julian_date()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.TimestampOps.time_to_julian_date,time,0.0
4336,param-level,"terminus-2,oracle",0.9848410265394214,0.9848410265394214,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampOps.time_tz_localize,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_tz_localize"": ""class TimestampOps:\n    def time_tz_localize(self, tz):\n        if self.ts.tz is None:\n            self.ts.tz_localize(tz)\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.TimestampOps.time_tz_localize,time,0.0
4331,param-level,"terminus-2,oracle",1.0362342669924922,1.0362342669924922,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampOps.time_normalize,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_normalize"": ""class TimestampOps:\n    def time_normalize(self, tz):\n        self.ts.normalize()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.TimestampOps.time_normalize,time,0.0
4340,param-level,"terminus-2,oracle",1.0096282482119523,1.0096282482119523,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampProperties.time_is_quarter_end,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_is_quarter_end"": ""class TimestampProperties:\n    def time_is_quarter_end(self, tz):\n        self.ts.is_quarter_end\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.TimestampProperties.time_is_quarter_end,time,0.0
4335,param-level,"terminus-2,oracle",0.97438388789397,0.97438388789397,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampOps.time_tz_convert,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_tz_convert"": ""class TimestampOps:\n    def time_tz_convert(self, tz):\n        if self.ts.tz is not None:\n            self.ts.tz_convert(tz)\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.TimestampOps.time_tz_convert,time,0.0
4339,param-level,"terminus-2,oracle",1.015546681180151,1.015546681180151,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampProperties.time_is_leap_year,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_is_leap_year"": ""class TimestampProperties:\n    def time_is_leap_year(self, tz):\n        self.ts.is_leap_year\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.TimestampProperties.time_is_leap_year,time,0.0
4344,param-level,"terminus-2,oracle",1.0101241275047386,1.0101241275047386,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampProperties.time_microsecond,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_microsecond"": ""class TimestampProperties:\n    def time_microsecond(self, tz):\n        self.ts.microsecond\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.TimestampProperties.time_microsecond,time,0.0
4345,param-level,"terminus-2,oracle",0.9791569957625458,0.9791569957625458,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampProperties.time_month_name,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_month_name"": ""class TimestampProperties:\n    def time_month_name(self, tz):\n        self.ts.month_name()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.TimestampProperties.time_month_name,time,0.0
4333,param-level,"terminus-2,oracle",1.0111371176694943,1.0111371176694943,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampOps.time_replace_tz,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_replace_tz"": ""class TimestampOps:\n    def time_replace_tz(self, tz):\n        self.ts.replace(tzinfo=pytz.timezone(\""US/Eastern\""))\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.TimestampOps.time_replace_tz,time,0.0
4338,param-level,"terminus-2,oracle",1.0088496122333308,1.0088496122333308,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampProperties.time_days_in_month,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_days_in_month"": ""class TimestampProperties:\n    def time_days_in_month(self, tz):\n        self.ts.days_in_month\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.TimestampProperties.time_days_in_month,time,0.0
4341,param-level,"terminus-2,oracle",1.0098146508293273,1.0098146508293273,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampProperties.time_is_quarter_start,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_is_quarter_start"": ""class TimestampProperties:\n    def time_is_quarter_start(self, tz):\n        self.ts.is_quarter_start\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.TimestampProperties.time_is_quarter_start,time,0.0
4323,param-level,"terminus-2,oracle",1.026953965589444,1.026953965589444,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampConstruction.time_from_datetime_aware,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_from_datetime_aware"": ""class TimestampConstruction:\n    def time_from_datetime_aware(self):\n        Timestamp(self.dttime_aware)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.TimestampConstruction.time_from_datetime_aware,time,0.0
4350,param-level,"terminus-2,oracle",1.0548652750818708,1.0548652750818708,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.tz_convert.TimeTZConvert.time_tz_convert_from_utc,tslibs.tz_convert,"{""tslibs.tz_convert.TimeTZConvert.time_tz_convert_from_utc"": ""class TimeTZConvert:\n    def time_tz_convert_from_utc(self, size, tz):\n        # effectively:\n        #  dti = DatetimeIndex(self.i8data, tz=tz)\n        #  dti.tz_localize(None)\n        if old_sig:\n            tz_convert_from_utc(self.i8data, UTC, tz)\n        else:\n            tz_convert_from_utc(self.i8data, tz)\n\n    def setup(self, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.tz_convert.TimeTZConvert.time_tz_convert_from_utc,time,0.0
4343,param-level,"terminus-2,oracle",1.0104473288567313,1.0104473288567313,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampProperties.time_is_year_start,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_is_year_start"": ""class TimestampProperties:\n    def time_is_year_start(self, tz):\n        self.ts.is_year_start\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.TimestampProperties.time_is_year_start,time,0.0
4346,param-level,"terminus-2,oracle",1.0233434816712337,1.0233434816712337,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampProperties.time_quarter,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_quarter"": ""class TimestampProperties:\n    def time_quarter(self, tz):\n        self.ts.quarter\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.TimestampProperties.time_quarter,time,0.0
4347,param-level,"terminus-2,oracle",1.0142606399277831,1.0142606399277831,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampProperties.time_week,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_week"": ""class TimestampProperties:\n    def time_week(self, tz):\n        self.ts.week\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.TimestampProperties.time_week,time,0.0
4351,param-level,"terminus-2,oracle",0.9598299359561444,0.9598299359561444,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.tz_convert.TimeTZConvert.time_tz_localize_to_utc,tslibs.tz_convert,"{""tslibs.tz_convert.TimeTZConvert.time_tz_localize_to_utc"": ""class TimeTZConvert:\n    def time_tz_localize_to_utc(self, size, tz):\n        # effectively:\n        #  dti = DatetimeIndex(self.i8data)\n        #  dti.tz_localize(tz, ambiguous=\""NaT\"", nonexistent=\""NaT\"")\n        tz_localize_to_utc(self.i8data, tz, ambiguous=\""NaT\"", nonexistent=\""NaT\"")\n\n    def setup(self, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.tz_convert.TimeTZConvert.time_tz_localize_to_utc,time,0.0
4342,param-level,"terminus-2,oracle",1.009174308037522,1.009174308037522,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampProperties.time_is_year_end,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_is_year_end"": ""class TimestampProperties:\n    def time_is_year_end(self, tz):\n        self.ts.is_year_end\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.TimestampProperties.time_is_year_end,time,0.0
4348,param-level,"terminus-2,oracle",0.9855268261361976,0.9855268261361976,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampProperties.time_weekday_name,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_weekday_name"": ""class TimestampProperties:\n    def time_weekday_name(self, tz):\n        self.ts.day_name()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.TimestampProperties.time_weekday_name,time,0.0
4332,param-level,"terminus-2,oracle",1.0477511425107873,1.0477511425107873,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampOps.time_replace_None,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_replace_None"": ""class TimestampOps:\n    def time_replace_None(self, tz):\n        self.ts.replace(tzinfo=None)\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.TimestampOps.time_replace_None,time,0.0
4349,param-level,"terminus-2,oracle",1.0124058949042414,1.0124058949042414,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime,tslibs.tslib,"{""tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime"": ""class TimeIntsToPydatetime:\n    def time_ints_to_pydatetime(self, box, size, tz):\n        ints_to_pydatetime(self.i8data, tz, box=box)\n\n    def setup(self, box, size, tz):\n        if box == \""date\"" and tz is not None:\n            # tz is ignored, so avoid running redundant benchmarks\n            raise NotImplementedError  # skip benchmark\n        if size == 10**6 and tz is _tzs[-1]:\n            # This is cumbersomely-slow, so skip to trim runtime\n            raise NotImplementedError  # skip benchmark\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime,time,0.0
11311,param-level,"terminus-2,claude",1.0742448146448818,1.03428784582855,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,cluster.KMeansBenchmark.time_fit,cluster,"{""cluster.KMeansBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",cluster.KMeansBenchmark.time_fit,time,0.028258110902639116
11315,param-level,"terminus-2,claude",0.9002117666510273,0.9384013064633068,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,metrics.PairwiseDistancesBenchmark.time_pairwise_distances,metrics,"{""metrics.PairwiseDistancesBenchmark.time_pairwise_distances"": ""class PairwiseDistancesBenchmark:\n    def time_pairwise_distances(self, *args):\n        pairwise_distances(self.X, **self.pdist_params)\n\n    def setup(self, *params):\n        representation, metric, n_jobs = params\n    \n        if representation == \""sparse\"" and metric == \""correlation\"":\n            raise NotImplementedError\n    \n        if Benchmark.data_size == \""large\"":\n            if metric in (\""manhattan\"", \""correlation\""):\n                n_samples = 8000\n            else:\n                n_samples = 24000\n        else:\n            if metric in (\""manhattan\"", \""correlation\""):\n                n_samples = 4000\n            else:\n                n_samples = 12000\n    \n        data = _random_dataset(n_samples=n_samples, representation=representation)\n        self.X, self.X_val, self.y, self.y_val = data\n    \n        self.pdist_params = {\""metric\"": metric, \""n_jobs\"": n_jobs}""}",metrics.PairwiseDistancesBenchmark.time_pairwise_distances,time,-0.027008161111937386
11321,param-level,"terminus-2,oracle",1.0589149323361604,1.0589149323361604,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,cluster.KMeansBenchmark.time_fit,cluster,"{""cluster.KMeansBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",cluster.KMeansBenchmark.time_fit,time,0.0
11314,param-level,"terminus-2,claude",1.0147383648495112,1.024858002625557,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,ensemble.RandomForestClassifierBenchmark.time_fit,ensemble,"{""ensemble.RandomForestClassifierBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RandomForestClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",ensemble.RandomForestClassifierBenchmark.time_fit,time,-0.007156745244728248
11313,param-level,"terminus-2,claude",1.4547160527174117,3.093394649383032,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,ensemble.HistGradientBoostingClassifierBenchmark.time_predict,ensemble,"{""ensemble.HistGradientBoostingClassifierBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass HistGradientBoostingClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",ensemble.HistGradientBoostingClassifierBenchmark.time_predict,time,-1.1588957543604106
11326,param-level,"terminus-2,oracle",1.108554803597796,1.108554803597796,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,ensemble.HistGradientBoostingClassifierBenchmark.time_fit,ensemble,"{""ensemble.HistGradientBoostingClassifierBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass HistGradientBoostingClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",ensemble.HistGradientBoostingClassifierBenchmark.time_fit,time,0.0
11322,param-level,"terminus-2,oracle",1.4341243541277937,1.4341243541277937,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,cluster.KMeansBenchmark.time_predict,cluster,"{""cluster.KMeansBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",cluster.KMeansBenchmark.time_predict,time,0.0
11329,param-level,"terminus-2,oracle",1.01322469325088,1.01322469325088,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,linear_model.ElasticNetBenchmark.time_fit,linear_model,"{""linear_model.ElasticNetBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass ElasticNetBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",linear_model.ElasticNetBenchmark.time_fit,time,0.0
11312,param-level,"terminus-2,claude",0.8335698226637654,0.8450338227894318,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,cluster.MiniBatchKMeansBenchmark.time_transform,cluster,"{""cluster.MiniBatchKMeansBenchmark.time_transform"": ""class Transformer:\n    def time_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchKMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",cluster.MiniBatchKMeansBenchmark.time_transform,time,-0.008107496552805089
11327,param-level,"terminus-2,oracle",3.093394649383032,3.093394649383032,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,ensemble.HistGradientBoostingClassifierBenchmark.time_predict,ensemble,"{""ensemble.HistGradientBoostingClassifierBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass HistGradientBoostingClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",ensemble.HistGradientBoostingClassifierBenchmark.time_predict,time,0.0
11325,param-level,"terminus-2,oracle",1.200577564959305,1.200577564959305,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,cluster.MiniBatchKMeansBenchmark.time_transform,cluster,"{""cluster.MiniBatchKMeansBenchmark.time_transform"": ""class Transformer:\n    def time_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchKMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",cluster.MiniBatchKMeansBenchmark.time_transform,time,0.0
11323,param-level,"terminus-2,oracle",1.4984990522758903,1.4984990522758903,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,cluster.KMeansBenchmark.time_transform,cluster,"{""cluster.KMeansBenchmark.time_transform"": ""class Transformer:\n    def time_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",cluster.KMeansBenchmark.time_transform,time,0.0
11331,param-level,"terminus-2,oracle",0.9787176714587016,0.9787176714587016,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,linear_model.LogisticRegressionBenchmark.time_fit,linear_model,"{""linear_model.LogisticRegressionBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LogisticRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",linear_model.LogisticRegressionBenchmark.time_fit,time,0.0
11333,param-level,"terminus-2,oracle",0.7443338819800812,0.7443338819800812,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,linear_model.SGDRegressorBenchmark.time_predict,linear_model,"{""linear_model.SGDRegressorBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SGDRegressorBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",linear_model.SGDRegressorBenchmark.time_predict,time,0.0
11330,param-level,"terminus-2,oracle",1.020202162067072,1.020202162067072,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,linear_model.LassoBenchmark.time_fit,linear_model,"{""linear_model.LassoBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LassoBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",linear_model.LassoBenchmark.time_fit,time,0.0
11328,param-level,"terminus-2,oracle",1.024858002625557,1.024858002625557,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,ensemble.RandomForestClassifierBenchmark.time_fit,ensemble,"{""ensemble.RandomForestClassifierBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RandomForestClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",ensemble.RandomForestClassifierBenchmark.time_fit,time,0.0
11324,param-level,"terminus-2,oracle",1.1304154849260344,1.1304154849260344,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,cluster.MiniBatchKMeansBenchmark.time_fit,cluster,"{""cluster.MiniBatchKMeansBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchKMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",cluster.MiniBatchKMeansBenchmark.time_fit,time,0.0
11332,param-level,"terminus-2,oracle",0.6686427205795704,0.6686427205795704,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,linear_model.SGDRegressorBenchmark.time_fit,linear_model,"{""linear_model.SGDRegressorBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SGDRegressorBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",linear_model.SGDRegressorBenchmark.time_fit,time,0.0
11335,param-level,"terminus-2,oracle",0.940096240203932,0.940096240203932,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,neighbors.KNeighborsClassifierBenchmark.time_fit,neighbors,"{""neighbors.KNeighborsClassifierBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KNeighborsClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",neighbors.KNeighborsClassifierBenchmark.time_fit,time,0.0
11334,param-level,"terminus-2,oracle",0.9384013064633068,0.9384013064633068,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,metrics.PairwiseDistancesBenchmark.time_pairwise_distances,metrics,"{""metrics.PairwiseDistancesBenchmark.time_pairwise_distances"": ""class PairwiseDistancesBenchmark:\n    def time_pairwise_distances(self, *args):\n        pairwise_distances(self.X, **self.pdist_params)\n\n    def setup(self, *params):\n        representation, metric, n_jobs = params\n    \n        if representation == \""sparse\"" and metric == \""correlation\"":\n            raise NotImplementedError\n    \n        if Benchmark.data_size == \""large\"":\n            if metric in (\""manhattan\"", \""correlation\""):\n                n_samples = 8000\n            else:\n                n_samples = 24000\n        else:\n            if metric in (\""manhattan\"", \""correlation\""):\n                n_samples = 4000\n            else:\n                n_samples = 12000\n    \n        data = _random_dataset(n_samples=n_samples, representation=representation)\n        self.X, self.X_val, self.y, self.y_val = data\n    \n        self.pdist_params = {\""metric\"": metric, \""n_jobs\"": n_jobs}""}",metrics.PairwiseDistancesBenchmark.time_pairwise_distances,time,0.0
11336,param-level,"terminus-2,oracle",0.9730120615379054,0.9730120615379054,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,neighbors.KNeighborsClassifierBenchmark.time_predict,neighbors,"{""neighbors.KNeighborsClassifierBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KNeighborsClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",neighbors.KNeighborsClassifierBenchmark.time_predict,time,0.0
11340,func-level,"terminus-2,claude",0.959910040211352,0.9433432111662834,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_utils,imports,"{""imports.timeraw_import_astropy_utils"": ""def timeraw_import_astropy_utils():\n    return \""\""\""\n    from astropy import utils\n    \""\""\""""}",imports.timeraw_import_astropy_utils,time,0.011716286453372395
11338,func-level,"terminus-2,claude",0.9667647284879493,0.9690919685025468,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_table,imports,"{""imports.timeraw_import_astropy_table"": ""def timeraw_import_astropy_table():\n    return \""\""\""\n    from astropy import table\n    \""\""\""""}",imports.timeraw_import_astropy_table,time,-0.0016458557387535348
11341,func-level,"terminus-2,claude",0.9698083188264875,0.9375326437838574,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_utils_iers,imports,"{""imports.timeraw_import_astropy_utils_iers"": ""def timeraw_import_astropy_utils_iers():\n    return \""\""\""\n    from astropy.utils import iers\n    \""\""\""""}",imports.timeraw_import_astropy_utils_iers,time,0.022825795645424456
11346,func-level,"terminus-2,claude",1.0550148289258674,0.9971303976901288,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.time_read,io_ascii.main,"{""io_ascii.main.AastexFloat.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.IpacString.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.SextractorFloat.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.SextractorInt.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.SextractorString.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.time_read,time,0.040936655753704816
11345,func-level,"terminus-2,claude",1.0264646683950471,1.0537416530971275,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.core.time_default_splitter_call,io_ascii.core,"{""io_ascii.core.CoreSuite.time_default_splitter_call"": ""class CoreSuite:\n    def time_default_splitter_call(self):\n        core.DefaultSplitter()(self.csv_line)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]""}",io_ascii.core.time_default_splitter_call,time,-0.019290653961867327
11342,func-level,"terminus-2,claude",0.9832837638809778,0.9471224197161896,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_visualization,imports,"{""imports.timeraw_import_astropy_visualization"": ""def timeraw_import_astropy_visualization():\n    return \""\""\""\n    from astropy import visualization\n    \""\""\""""}",imports.timeraw_import_astropy_visualization,time,0.025573793610175546
11339,func-level,"terminus-2,claude",0.969560605876141,0.9447062140645422,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_units,imports,"{""imports.timeraw_import_astropy_units"": ""def timeraw_import_astropy_units():\n    return \""\""\""\n    from astropy import units\n    \""\""\""""}",imports.timeraw_import_astropy_units,time,0.017577363374539436
11350,func-level,"terminus-2,claude",1.0135023024404557,1.019241621879223,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.time_init_int_masked_3d,table,"{""table.TimeTableInitWithMultiDimLists.time_init_int_masked_3d"": ""class TimeTableInitWithMultiDimLists:\n    def time_init_int_masked_3d(self):\n        Table([self.data_int_masked_3d])\n\n    def setup(self):\n        np_data_int = np.arange(1_000_000, dtype=np.int64)\n        np_data_float = np_data_int.astype(np.float64)\n        np_data_str = np_data_int.astype(\""U\"")\n    \n        self.data_int_1d = np_data_int.tolist()\n    \n        self.data_int_3d = np_data_int.reshape(1000, 100, 10).tolist()\n    \n        self.data_int_masked_1d = self.data_int_1d.copy()\n        self.data_int_masked_1d[-1] = np.ma.masked\n    \n        self.data_int_masked_3d = self.data_int_3d.copy()\n        self.data_int_masked_3d[-1][-1][-1] = np.ma.masked\n    \n        self.data_float_1d = np_data_float.tolist()\n    \n        self.data_str_1d = np_data_str.tolist()\n    \n        self.data_str_masked_1d = self.data_str_1d.copy()\n        self.data_str_masked_1d[-1] = np.ma.masked""}",table.time_init_int_masked_3d,time,-0.0040589246384493466
11343,func-level,"terminus-2,claude",0.955402557487482,0.93716742554461,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_wcs,imports,"{""imports.timeraw_import_astropy_wcs"": ""def timeraw_import_astropy_wcs():\n    return \""\""\""\n    from astropy import wcs\n    \""\""\""""}",imports.timeraw_import_astropy_wcs,time,0.012896132915751113
11337,func-level,"terminus-2,claude",0.9717202118900872,0.9589327323279948,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,convolve.time_convolve_fft,convolve,"{""convolve.ConvolveFFT.time_convolve_fft"": ""class ConvolveFFT:\n    def time_convolve_fft(self, ndim, size, boundary, nan_treatment):\n        convolve_fft(\n            self.array, self.kernel, boundary=boundary, nan_treatment=nan_treatment\n        )\n\n    def setup(self, ndim, size, boundary, nan_treatment):\n        np.random.seed(12345)\n    \n        self.kernel = np.random.random(kernel_shapes[ndim][size])\n        self.array = np.random.random(array_shapes[ndim][size])""}",convolve.time_convolve_fft,time,0.009043479181111997
11348,func-level,"terminus-2,claude",1.0532855922428737,1.0187313338888078,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,stats.sigma_clipping.time_3d_array_axis2,stats.sigma_clipping,"{""stats.sigma_clipping.SigmaClipBenchmarks.time_3d_array_axis2"": ""class SigmaClipBenchmarks:\n    def time_3d_array_axis2(self):\n        self.sigclip(self.data, axis=(0, 1))\n\n    def setup(self):\n        # Avoid top-level module import to make sure that the benchmarks are\n        # compatible with versions of astropy that did not have this functionality.\n        from astropy.stats import SigmaClip\n    \n        size = (4, 2048, 2048)\n    \n        with NumpyRNGContext(12345):\n            self.data = np.random.normal(size=size)\n    \n            # add outliers\n            nbad = 100000\n            zbad = np.random.randint(low=0, high=size[0] - 1, size=nbad)\n            ybad = np.random.randint(low=0, high=size[1] - 1, size=nbad)\n            xbad = np.random.randint(low=0, high=size[2] - 1, size=nbad)\n            self.data[zbad, ybad, xbad] = np.random.choice([-1, 1], size=nbad) * (\n                10 + np.random.rand(nbad)\n            )\n    \n            # The defaults use median as the cenfunc and standard\n            # deviation as the stdfunc.  The default iters is 5.\n            self.sigclip = SigmaClip(sigma=3)""}",stats.sigma_clipping.time_3d_array_axis2,time,0.024437240703016897
11347,func-level,"terminus-2,claude",1.0251839802771694,1.0075690507073685,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,modeling.fitting.time_init_LinearLSQFitter,modeling.fitting,"{""modeling.fitting.time_init_LinearLSQFitter"": ""def time_init_LinearLSQFitter():\n    fitting.LinearLSQFitter()""}",modeling.fitting.time_init_LinearLSQFitter,time,0.012457517376096839
11344,func-level,"terminus-2,claude",1.020818645377212,1.0278074337985588,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.core.time_continuation_inputter,io_ascii.core,"{""io_ascii.core.CoreSuite.time_continuation_inputter"": ""class CoreSuite:\n    def time_continuation_inputter(self):\n        core.ContinuationLinesInputter().process_lines(self.lines)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]""}",io_ascii.core.time_continuation_inputter,time,-0.0049425660688450284
11351,func-level,"terminus-2,claude",1.0812914844911257,1.070279171332443,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.time_init_lists,table,"{""table.TimeTableInitWithLists.time_init_lists"": ""class TimeTableInitWithLists:\n    def time_init_lists(self):\n        Table([self.dat, self.dat, self.dat], names=[\""time\"", \""rate\"", \""error\""])\n\n    def setup(self):\n        self.dat = list(range(100_000))""}",table.time_init_lists,time,0.007788057396522322
11353,func-level,"terminus-2,claude",1.0475555107561574,1.0436571240682462,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,units.time_quantity_add,units,"{""units.TimeQuantityOpSmallArrayDiffUnit.time_quantity_add"": ""class TimeQuantityOpSmallArrayDiffUnit:\n    def time_quantity_add(self):\n        # Same as operator.add\n        self.data + self.data2\n\n    def setup(self):\n        data = np.array([1.0, 2.0, 3.0])\n        self.data = data * u.g\n    \n        # A different but dimensionally compatible unit\n        self.data2 = 0.001 * data * u.kg""}",units.time_quantity_add,time,0.0027569919999372024
11349,func-level,"terminus-2,claude",1.0644133111058784,1.0684349181619717,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.time_init_int_1d,table,"{""table.TimeTableInitWithMultiDimLists.time_init_int_1d"": ""class TimeTableInitWithMultiDimLists:\n    def time_init_int_1d(self):\n        Table([self.data_int_1d])\n\n    def setup(self):\n        np_data_int = np.arange(1_000_000, dtype=np.int64)\n        np_data_float = np_data_int.astype(np.float64)\n        np_data_str = np_data_int.astype(\""U\"")\n    \n        self.data_int_1d = np_data_int.tolist()\n    \n        self.data_int_3d = np_data_int.reshape(1000, 100, 10).tolist()\n    \n        self.data_int_masked_1d = self.data_int_1d.copy()\n        self.data_int_masked_1d[-1] = np.ma.masked\n    \n        self.data_int_masked_3d = self.data_int_3d.copy()\n        self.data_int_masked_3d[-1][-1][-1] = np.ma.masked\n    \n        self.data_float_1d = np_data_float.tolist()\n    \n        self.data_str_1d = np_data_str.tolist()\n    \n        self.data_str_masked_1d = self.data_str_1d.copy()\n        self.data_str_masked_1d[-1] = np.ma.masked""}",table.time_init_int_1d,time,-0.0028441351174633834
11354,func-level,"terminus-2,claude",8.026759062778206,1.0080852034220826,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,units.time_quantity_np_square,units,"{""units.TimeQuantityOpLargeArray.time_quantity_np_square"": ""class TimeQuantityOpSmallArray:\n    def time_quantity_np_square(self):\n        np.power(self.data, 2)\n\nclass TimeQuantityOpLargeArray:\n    def setup(self):\n        data = np.arange(1e6) + 1\n        self.data = data * u.g\n        self.out_sq = data * u.g**2\n        self.out_sqrt = data * u.g**0.5"", ""units.TimeQuantityOpSmallArray.time_quantity_np_square"": ""class TimeQuantityOpSmallArray:\n    def time_quantity_np_square(self):\n        np.power(self.data, 2)\n\n    def setup(self):\n        data = np.array([1.0, 2.0, 3.0])\n        self.data = data * u.g\n        self.out_sq = data * u.g**2\n        self.out_sqrt = data * u.g**0.5""}",units.time_quantity_np_square,time,4.963701456404614
11352,func-level,"terminus-2,claude",1.0242751521344056,1.0210058620492353,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.time_read_rows,table,"{""table.TimeTable.time_read_rows"": ""class TimeTable:\n    def time_read_rows(self):\n        for row in self.table:\n            tuple(row)\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6""}",table.time_read_rows,time,0.002312086340290167
11356,func-level,"terminus-2,claude",0.9564521149576416,1.0312724196031475,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,wcs.time_ape14_world_to_pixel,wcs,"{""wcs.WCSTransformations.time_ape14_world_to_pixel"": ""class WCSTransformations:\n    def time_ape14_world_to_pixel(self, size):\n        wcs.world_to_pixel(self.coord)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)""}",wcs.time_ape14_world_to_pixel,time,-0.05291393539286134
11357,func-level,"terminus-2,claude",0.9573371633381628,0.9528888363018873,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,wcs.time_pix2world_x_y_0,wcs,"{""wcs.WCSTransformations.time_pix2world_x_y_0"": ""class WCSTransformations:\n    def time_pix2world_x_y_0(self, size):\n        wcs.wcs_pix2world(self.px, self.py, 0)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)""}",wcs.time_pix2world_x_y_0,time,0.0031459172816658506
11355,func-level,"terminus-2,claude",1.0520263716203884,1.043050322455329,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,units.time_quantity_view,units,"{""units.time_quantity_view"": ""def time_quantity_view():\n    q1.view(u.Quantity)""}",units.time_quantity_view,time,0.00634798385081998
11358,func-level,"terminus-2,claude",0.9809142254016988,0.949979563682008,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,wcs.time_pix2world_x_y_1,wcs,"{""wcs.WCSTransformations.time_pix2world_x_y_1"": ""class WCSTransformations:\n    def time_pix2world_x_y_1(self, size):\n        wcs.wcs_pix2world(self.px, self.py, 1)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)""}",wcs.time_pix2world_x_y_1,time,0.0218774128144914
11364,func-level,"terminus-2,gpt-5",1.058565116636493,0.9502411460411848,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_time,imports,"{""imports.timeraw_import_astropy_time"": ""def timeraw_import_astropy_time():\n    return \""\""\""\n    from astropy import time\n    \""\""\""""}",imports.timeraw_import_astropy_time,time,0.07660818288211337
11360,func-level,"terminus-2,gpt-5",1.1218778451080516,1.0791845115486305,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_io,imports,"{""imports.timeraw_import_astropy_io"": ""def timeraw_import_astropy_io():\n    return \""\""\""\n    from astropy import io\n    \""\""\""""}",imports.timeraw_import_astropy_io,time,0.030193305204682507
11368,func-level,"terminus-2,gpt-5",1.0372915399081946,1.0537416530971275,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.core.time_default_splitter_call,io_ascii.core,"{""io_ascii.core.CoreSuite.time_default_splitter_call"": ""class CoreSuite:\n    def time_default_splitter_call(self):\n        core.DefaultSplitter()(self.csv_line)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]""}",io_ascii.core.time_default_splitter_call,time,-0.011633743415086965
11363,func-level,"terminus-2,gpt-5",1.0810134963298987,1.0528219741954823,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_io_misc_hdf5,imports,"{""imports.timeraw_import_astropy_io_misc_hdf5"": ""def timeraw_import_astropy_io_misc_hdf5():\n    return \""\""\""\n    from astropy.io.misc import hdf5\n    \""\""\""""}",imports.timeraw_import_astropy_io_misc_hdf5,time,0.019937427252062552
11359,func-level,"terminus-2,gpt-5",0.9929863139631648,0.9591490792274736,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,convolve.time_convolve_fft,convolve,"{""convolve.ConvolveFFT.time_convolve_fft"": ""class ConvolveFFT:\n    def time_convolve_fft(self, ndim, size, boundary, nan_treatment):\n        convolve_fft(\n            self.array, self.kernel, boundary=boundary, nan_treatment=nan_treatment\n        )\n\n    def setup(self, ndim, size, boundary, nan_treatment):\n        np.random.seed(12345)\n    \n        self.kernel = np.random.random(kernel_shapes[ndim][size])\n        self.array = np.random.random(array_shapes[ndim][size])""}",convolve.time_convolve_fft,time,0.023930151863996624
11362,func-level,"terminus-2,gpt-5",1.1090733624811406,1.0611753029756317,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_io_misc,imports,"{""imports.timeraw_import_astropy_io_misc"": ""def timeraw_import_astropy_io_misc():\n    return \""\""\""\n    from astropy.io import misc\n    \""\""\""""}",imports.timeraw_import_astropy_io_misc,time,0.03387415806613081
11361,func-level,"terminus-2,gpt-5",1.085173471563104,1.0391439947064631,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_io_fits,imports,"{""imports.timeraw_import_astropy_io_fits"": ""def timeraw_import_astropy_io_fits():\n    return \""\""\""\n    from astropy.io import fits\n    \""\""\""""}",imports.timeraw_import_astropy_io_fits,time,0.03255267104430053
11365,func-level,"terminus-2,gpt-5",1.0735850081135054,0.969432561350119,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_timeseries,imports,"{""imports.timeraw_import_astropy_timeseries"": ""def timeraw_import_astropy_timeseries():\n    return \""\""\""\n    from astropy import timeseries\n    \""\""\""""}",imports.timeraw_import_astropy_timeseries,time,0.07365802458513891
11367,func-level,"terminus-2,gpt-5",1.0165220062328009,1.0128514245735518,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.core.time_convert_vals,io_ascii.core,"{""io_ascii.core.CoreSuite.time_convert_vals"": ""class CoreSuite:\n    def time_convert_vals(self):\n        core.TableOutputter()._convert_vals(self.cols)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]""}",io_ascii.core.time_convert_vals,time,0.0025958851904165844
11369,func-level,"terminus-2,gpt-5",1.0170891957626351,1.015350347678538,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.core.time_whitespace_splitter,io_ascii.core,"{""io_ascii.core.CoreSuite.time_whitespace_splitter"": ""class CoreSuite:\n    def time_whitespace_splitter(self):\n        core.WhitespaceSplitter().process_line(self.line)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]""}",io_ascii.core.time_whitespace_splitter,time,0.0012297369760234716
11366,func-level,"terminus-2,gpt-5",1.0560109880479434,0.9874496199734252,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_timeseries_io,imports,"{""imports.timeraw_import_astropy_timeseries_io"": ""def timeraw_import_astropy_timeseries_io():\n    return \""\""\""\n    from astropy.timeseries import io\n    \""\""\""""}",imports.timeraw_import_astropy_timeseries_io,time,0.04848753046288415
11379,func-level,"terminus-2,oracle",1.0071536445507023,1.0071536445507023,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,convolve.time_convolve,convolve,"{""convolve.Convolve.time_convolve"": ""class Convolve:\n    def time_convolve(self, ndim, size, boundary, nan_treatment):\n        convolve(\n            self.array, self.kernel, boundary=boundary, nan_treatment=nan_treatment\n        )\n\n    def setup(self, ndim, size, boundary, nan_treatment):\n        np.random.seed(12345)\n    \n        self.kernel = np.random.random(kernel_shapes[ndim][size])\n        self.array = np.random.random(array_shapes[ndim][size])""}",convolve.time_convolve,time,0.0
11380,func-level,"terminus-2,oracle",0.997006747390139,0.997006747390139,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,convolve.time_convolve_fft,convolve,"{""convolve.ConvolveFFT.time_convolve_fft"": ""class ConvolveFFT:\n    def time_convolve_fft(self, ndim, size, boundary, nan_treatment):\n        convolve_fft(\n            self.array, self.kernel, boundary=boundary, nan_treatment=nan_treatment\n        )\n\n    def setup(self, ndim, size, boundary, nan_treatment):\n        np.random.seed(12345)\n    \n        self.kernel = np.random.random(kernel_shapes[ndim][size])\n        self.array = np.random.random(array_shapes[ndim][size])""}",convolve.time_convolve_fft,time,0.0
11318,param-level,"terminus-2,gpt-5",1.026560358036901,1.024858002625557,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,ensemble.RandomForestClassifierBenchmark.time_fit,ensemble,"{""ensemble.RandomForestClassifierBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RandomForestClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",ensemble.RandomForestClassifierBenchmark.time_fit,time,0.0012039288623366359
11316,param-level,"terminus-2,gpt-5",1.0349332922636336,1.513804722063677,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,cluster.KMeansBenchmark.time_transform,cluster,"{""cluster.KMeansBenchmark.time_transform"": ""class Transformer:\n    def time_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",cluster.KMeansBenchmark.time_transform,time,-0.3386643775106389
11381,func-level,"terminus-2,oracle",0.9436728904153254,0.9436728904153254,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,coordinates.time_concatenate_array,coordinates,"{""coordinates.FrameBenchmarks.time_concatenate_array"": ""class FrameBenchmarks:\n    def time_concatenate_array(self):\n        concatenate((self.icrs_array, self.icrs_array))\n\n    def setup(self):\n        self.scalar_ra = 3.2 * u.deg\n        self.scalar_dec = 2.2 * u.deg\n    \n        self.scalar_pmra = 3.2 * u.mas / u.yr\n        self.scalar_pmdec = 2.2 * u.mas / u.yr\n    \n        self.array_ra = np.linspace(0.0, 360.0, 1000) * u.deg\n        self.array_dec = np.linspace(-90.0, 90.0, 1000) * u.deg\n    \n        np.random.seed(12345)\n        self.icrs_scalar = ICRS(ra=1 * u.deg, dec=2 * u.deg)\n        self.icrs_array = ICRS(\n            ra=np.random.random(10000) * u.deg, dec=np.random.random(10000) * u.deg\n        )\n    \n        # Some points to use for benchmarking coordinate matching.\n        # These were motivated by some tests done in astropy/astropy#7324:\n        # https://github.com/astropy/astropy/pull/7324#issuecomment-392382719\n        xyz_uniform1 = rnd.uniform(size=(3, 10000)) * u.kpc\n        xyz_uniform2 = rnd.uniform(size=(3, 10000)) * u.kpc\n        self.icrs_uniform1 = ICRS(\n            xyz_uniform1, representation_type=CartesianRepresentation\n        )\n        self.icrs_uniform2 = ICRS(\n            xyz_uniform2, representation_type=CartesianRepresentation\n        )\n    \n        phi = rnd.uniform(0, 2 * np.pi, size=10000)\n        theta = np.arccos(2 * rnd.uniform(size=10000) - 1)\n        xyz_uniform_sph1 = (\n            np.vstack(\n                (\n                    np.cos(phi) * np.sin(theta),\n                    np.sin(phi) * np.sin(theta),\n                    np.cos(theta),\n                )\n            )\n            * u.kpc\n        )\n    \n        phi = rnd.uniform(0, 2 * np.pi, size=10000)\n        theta = np.arccos(2 * rnd.uniform(size=10000) - 1)\n        xyz_uniform_sph2 = (\n            np.vstack(\n                (\n                    np.cos(phi) * np.sin(theta),\n                    np.sin(phi) * np.sin(theta),\n                    np.cos(theta),\n                )\n            )\n            * u.kpc\n        )\n        self.icrs_uniform_sph1 = ICRS(\n            xyz_uniform_sph1, representation_type=CartesianRepresentation\n        )\n        self.icrs_uniform_sph2 = ICRS(\n            xyz_uniform_sph2, representation_type=CartesianRepresentation\n        )\n    \n        xyz0 = rnd.uniform(-100, 100, size=(8, 3))\n        xyz_clustered1 = np.vstack(rnd.normal(xyz0, size=(10000, 8, 3))).T * u.kpc\n        xyz_clustered2 = np.vstack(rnd.normal(xyz0, size=(10000, 8, 3))).T * u.kpc\n        self.icrs_clustered1 = ICRS(\n            xyz_clustered1, representation_type=CartesianRepresentation\n        )\n        self.icrs_clustered2 = ICRS(\n            xyz_clustered2, representation_type=CartesianRepresentation\n        )""}",coordinates.time_concatenate_array,time,0.0
11317,param-level,"terminus-2,gpt-5",2.336977261529984,3.093394649383032,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,ensemble.HistGradientBoostingClassifierBenchmark.time_predict,ensemble,"{""ensemble.HistGradientBoostingClassifierBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass HistGradientBoostingClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",ensemble.HistGradientBoostingClassifierBenchmark.time_predict,time,-0.5349486477037116
11320,param-level,"terminus-2,gpt-5",0.9714961330391112,0.9771958841499164,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,neighbors.KNeighborsClassifierBenchmark.time_predict,neighbors,"{""neighbors.KNeighborsClassifierBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KNeighborsClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",neighbors.KNeighborsClassifierBenchmark.time_predict,time,-0.004030941379635947
11319,param-level,"terminus-2,gpt-5",1.118069927042651,1.010540831267532,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,linear_model.LogisticRegressionBenchmark.time_fit,linear_model,"{""linear_model.LogisticRegressionBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LogisticRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",linear_model.LogisticRegressionBenchmark.time_fit,time,0.07604603661606729
11382,func-level,"terminus-2,oracle",1.093442182763926,1.093442182763926,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,coordinates.time_iter_array,coordinates,"{""coordinates.SkyCoordBenchmarks.time_iter_array"": ""class SkyCoordBenchmarks:\n    def time_iter_array(self):\n        for c in self.coord_array_1e3:\n            pass\n\n    def setup(self):\n        self.coord_scalar = SkyCoord(1, 2, unit=\""deg\"", frame=\""icrs\"")\n    \n        lon, lat = np.ones((2, 1000))\n        self.coord_array_1e3 = SkyCoord(lon, lat, unit=\""deg\"", frame=\""icrs\"")\n    \n        self.lon_1e6, self.lat_1e6 = np.ones((2, int(1e6)))\n        self.coord_array_1e6 = SkyCoord(\n            self.lon_1e6, self.lat_1e6, unit=\""deg\"", frame=\""icrs\""\n        )\n    \n        self.scalar_q_ra = 1 * u.deg\n        self.scalar_q_dec = 2 * u.deg\n    \n        np.random.seed(12345)\n        self.array_q_ra = np.random.rand(int(1e6)) * 360 * u.deg\n        self.array_q_dec = (np.random.rand(int(1e6)) * 180 - 90) * u.deg\n    \n        self.scalar_repr = UnitSphericalRepresentation(\n            lat=self.scalar_q_dec, lon=self.scalar_q_ra\n        )\n        self.array_repr = UnitSphericalRepresentation(\n            lat=self.array_q_dec, lon=self.array_q_ra\n        )""}",coordinates.time_iter_array,time,0.0
11372,func-level,"terminus-2,gpt-5",0.9814727981246726,0.968112742119425,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,modeling.fitting.time_Chebyshev1D_LevMarLSQFitter,modeling.fitting,"{""modeling.fitting.time_Chebyshev1D_LevMarLSQFitter"": ""def time_Chebyshev1D_LevMarLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        y = y_base + np.random.normal(0.0, 0.2, y_base.shape)\n        fit_LevMarLSQFitter(Chebyshev1D, x, y)\n    except Warning:\n        pass""}",modeling.fitting.time_Chebyshev1D_LevMarLSQFitter,time,0.009448413016441011
11378,func-level,"terminus-2,gpt-5",0.977650517371278,0.9528888363018873,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,wcs.time_pix2world_x_y_0,wcs,"{""wcs.WCSTransformations.time_pix2world_x_y_0"": ""class WCSTransformations:\n    def time_pix2world_x_y_0(self, size):\n        wcs.wcs_pix2world(self.px, self.py, 0)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)""}",wcs.time_pix2world_x_y_0,time,0.017511797078777012
11375,func-level,"terminus-2,gpt-5",1.0591447239874952,1.0684349181619717,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,table.time_init_int_1d,table,"{""table.TimeTableInitWithMultiDimLists.time_init_int_1d"": ""class TimeTableInitWithMultiDimLists:\n    def time_init_int_1d(self):\n        Table([self.data_int_1d])\n\n    def setup(self):\n        np_data_int = np.arange(1_000_000, dtype=np.int64)\n        np_data_float = np_data_int.astype(np.float64)\n        np_data_str = np_data_int.astype(\""U\"")\n    \n        self.data_int_1d = np_data_int.tolist()\n    \n        self.data_int_3d = np_data_int.reshape(1000, 100, 10).tolist()\n    \n        self.data_int_masked_1d = self.data_int_1d.copy()\n        self.data_int_masked_1d[-1] = np.ma.masked\n    \n        self.data_int_masked_3d = self.data_int_3d.copy()\n        self.data_int_masked_3d[-1][-1][-1] = np.ma.masked\n    \n        self.data_float_1d = np_data_float.tolist()\n    \n        self.data_str_1d = np_data_str.tolist()\n    \n        self.data_str_masked_1d = self.data_str_1d.copy()\n        self.data_str_masked_1d[-1] = np.ma.masked""}",table.time_init_int_1d,time,-0.0065701514670979625
11376,func-level,"terminus-2,gpt-5",1.0761991296122446,1.070279171332443,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,table.time_init_lists,table,"{""table.TimeTableInitWithLists.time_init_lists"": ""class TimeTableInitWithLists:\n    def time_init_lists(self):\n        Table([self.dat, self.dat, self.dat], names=[\""time\"", \""rate\"", \""error\""])\n\n    def setup(self):\n        self.dat = list(range(100_000))""}",table.time_init_lists,time,0.004186674879633323
11370,func-level,"terminus-2,gpt-5",1.0457880278292635,1.0341871887841994,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.time_read,io_ascii.main,"{""io_ascii.main.CommentedHeaderFloat.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.FixedWidthNoHeaderFloat.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.FixedWidthNoHeaderString.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.IpacFloat.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.IpacInt.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.LatexFloat.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.LatexInt.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.NoHeaderInt.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.NoHeaderString.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.RdbInt.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.SextractorFloat.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.SextractorInt.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.SextractorString.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.time_read,time,0.008204270894670544
11374,func-level,"terminus-2,gpt-5",0.9690183933220252,1.0166635229568928,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,modeling.model.time_init_gaussian_with_units,modeling.model,"{""modeling.model.time_init_gaussian_with_units"": ""def time_init_gaussian_with_units():\n    models.Gaussian1D(amplitude=10 * u.Hz, mean=5 * u.m, stddev=1.2 * u.cm)""}",modeling.model.time_init_gaussian_with_units,time,-0.03369528262720478
11383,func-level,"terminus-2,oracle",1.0791845115486305,1.0791845115486305,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_io,imports,"{""imports.timeraw_import_astropy_io"": ""def timeraw_import_astropy_io():\n    return \""\""\""\n    from astropy import io\n    \""\""\""""}",imports.timeraw_import_astropy_io,time,0.0
11385,func-level,"terminus-2,oracle",1.0611753029756317,1.0611753029756317,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_io_misc,imports,"{""imports.timeraw_import_astropy_io_misc"": ""def timeraw_import_astropy_io_misc():\n    return \""\""\""\n    from astropy.io import misc\n    \""\""\""""}",imports.timeraw_import_astropy_io_misc,time,0.0
11384,func-level,"terminus-2,oracle",1.0391439947064631,1.0391439947064631,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_io_fits,imports,"{""imports.timeraw_import_astropy_io_fits"": ""def timeraw_import_astropy_io_fits():\n    return \""\""\""\n    from astropy.io import fits\n    \""\""\""""}",imports.timeraw_import_astropy_io_fits,time,0.0
11388,func-level,"terminus-2,oracle",0.9502411460411848,0.9502411460411848,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_time,imports,"{""imports.timeraw_import_astropy_time"": ""def timeraw_import_astropy_time():\n    return \""\""\""\n    from astropy import time\n    \""\""\""""}",imports.timeraw_import_astropy_time,time,0.0
11386,func-level,"terminus-2,oracle",1.0528219741954823,1.0528219741954823,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_io_misc_hdf5,imports,"{""imports.timeraw_import_astropy_io_misc_hdf5"": ""def timeraw_import_astropy_io_misc_hdf5():\n    return \""\""\""\n    from astropy.io.misc import hdf5\n    \""\""\""""}",imports.timeraw_import_astropy_io_misc_hdf5,time,0.0
11387,func-level,"terminus-2,oracle",0.9690919685025468,0.9690919685025468,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_table,imports,"{""imports.timeraw_import_astropy_table"": ""def timeraw_import_astropy_table():\n    return \""\""\""\n    from astropy import table\n    \""\""\""""}",imports.timeraw_import_astropy_table,time,0.0
11397,func-level,"terminus-2,oracle",0.9257499597310214,0.9257499597310214,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_visualization_wcsaxes,imports,"{""imports.timeraw_import_astropy_visualization_wcsaxes"": ""def timeraw_import_astropy_visualization_wcsaxes():\n    return \""\""\""\n    from astropy.visualization import wcsaxes\n    \""\""\""""}",imports.timeraw_import_astropy_visualization_wcsaxes,time,0.0
11389,func-level,"terminus-2,oracle",0.969432561350119,0.969432561350119,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_timeseries,imports,"{""imports.timeraw_import_astropy_timeseries"": ""def timeraw_import_astropy_timeseries():\n    return \""\""\""\n    from astropy import timeseries\n    \""\""\""""}",imports.timeraw_import_astropy_timeseries,time,0.0
11398,func-level,"terminus-2,oracle",0.93716742554461,0.93716742554461,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_wcs,imports,"{""imports.timeraw_import_astropy_wcs"": ""def timeraw_import_astropy_wcs():\n    return \""\""\""\n    from astropy import wcs\n    \""\""\""""}",imports.timeraw_import_astropy_wcs,time,0.0
11393,func-level,"terminus-2,oracle",0.9587349685274658,0.9587349685274658,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_units_quantity,imports,"{""imports.timeraw_import_astropy_units_quantity"": ""def timeraw_import_astropy_units_quantity():\n    return \""\""\""\n    from astropy.units import quantity\n    \""\""\""""}",imports.timeraw_import_astropy_units_quantity,time,0.0
11392,func-level,"terminus-2,oracle",0.9447062140645422,0.9447062140645422,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_units,imports,"{""imports.timeraw_import_astropy_units"": ""def timeraw_import_astropy_units():\n    return \""\""\""\n    from astropy import units\n    \""\""\""""}",imports.timeraw_import_astropy_units,time,0.0
11391,func-level,"terminus-2,oracle",0.9665951387805676,0.9665951387805676,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_timeseries_periodograms,imports,"{""imports.timeraw_import_astropy_timeseries_periodograms"": ""def timeraw_import_astropy_timeseries_periodograms():\n    return \""\""\""\n    from astropy.timeseries import periodograms\n    \""\""\""""}",imports.timeraw_import_astropy_timeseries_periodograms,time,0.0
11390,func-level,"terminus-2,oracle",0.9874496199734252,0.9874496199734252,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_timeseries_io,imports,"{""imports.timeraw_import_astropy_timeseries_io"": ""def timeraw_import_astropy_timeseries_io():\n    return \""\""\""\n    from astropy.timeseries import io\n    \""\""\""""}",imports.timeraw_import_astropy_timeseries_io,time,0.0
11395,func-level,"terminus-2,oracle",0.9375326437838574,0.9375326437838574,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_utils_iers,imports,"{""imports.timeraw_import_astropy_utils_iers"": ""def timeraw_import_astropy_utils_iers():\n    return \""\""\""\n    from astropy.utils import iers\n    \""\""\""""}",imports.timeraw_import_astropy_utils_iers,time,0.0
11396,func-level,"terminus-2,oracle",0.9471224197161896,0.9471224197161896,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_visualization,imports,"{""imports.timeraw_import_astropy_visualization"": ""def timeraw_import_astropy_visualization():\n    return \""\""\""\n    from astropy import visualization\n    \""\""\""""}",imports.timeraw_import_astropy_visualization,time,0.0
11394,func-level,"terminus-2,oracle",0.9433432111662834,0.9433432111662834,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_utils,imports,"{""imports.timeraw_import_astropy_utils"": ""def timeraw_import_astropy_utils():\n    return \""\""\""\n    from astropy import utils\n    \""\""\""""}",imports.timeraw_import_astropy_utils,time,0.0
11377,func-level,"terminus-2,gpt-5",1.025835527044144,1.0312724196031475,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,wcs.time_ape14_world_to_pixel,wcs,"{""wcs.WCSTransformations.time_ape14_world_to_pixel"": ""class WCSTransformations:\n    def time_ape14_world_to_pixel(self, size):\n        wcs.world_to_pixel(self.coord)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)""}",wcs.time_ape14_world_to_pixel,time,-0.003845044242576798
11373,func-level,"terminus-2,gpt-5",1.013860207073542,1.052227833379354,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,modeling.fitting.time_init_LevMarLSQFitter,modeling.fitting,"{""modeling.fitting.time_init_LevMarLSQFitter"": ""def time_init_LevMarLSQFitter():\n    fitting.LevMarLSQFitter()""}",modeling.fitting.time_init_LevMarLSQFitter,time,-0.027134106298311117
11404,func-level,"terminus-2,oracle",1.012710550091558,1.012710550091558,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.time_read,io_ascii.main,"{""io_ascii.main.AastexFloat.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.CommentedHeaderFloat.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.NoHeaderInt.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.RdbInt.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.SextractorString.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.time_read,time,0.0
11400,func-level,"terminus-2,oracle",1.0278074337985588,1.0278074337985588,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.core.time_continuation_inputter,io_ascii.core,"{""io_ascii.core.CoreSuite.time_continuation_inputter"": ""class CoreSuite:\n    def time_continuation_inputter(self):\n        core.ContinuationLinesInputter().process_lines(self.lines)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]""}",io_ascii.core.time_continuation_inputter,time,0.0
11406,func-level,"terminus-2,oracle",1.0145693146798205,1.0145693146798205,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.table.time_str_vals_str,io_ascii.table,"{""io_ascii.table.TableSuite.time_str_vals_str"": ""class TableSuite:\n    def time_str_vals_str(self):\n        self.table_cols[2].iter_str_vals()\n\n    def setup(self):\n        self.lst = []\n        self.lst.append([random.randint(-500, 500) for i in range(1000)])\n        self.lst.append([random.random() * 500 - 500 for i in range(1000)])\n        self.lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, self.lst):\n            col.data = x\n        self.table_cols = [table.Column(x) for x in self.lst]\n        self.outputter = core.TableOutputter()\n        self.table = table.Table()""}",io_ascii.table.time_str_vals_str,time,0.0
11401,func-level,"terminus-2,oracle",1.0128514245735518,1.0128514245735518,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.core.time_convert_vals,io_ascii.core,"{""io_ascii.core.CoreSuite.time_convert_vals"": ""class CoreSuite:\n    def time_convert_vals(self):\n        core.TableOutputter()._convert_vals(self.cols)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]""}",io_ascii.core.time_convert_vals,time,0.0
11403,func-level,"terminus-2,oracle",1.015350347678538,1.015350347678538,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.core.time_whitespace_splitter,io_ascii.core,"{""io_ascii.core.CoreSuite.time_whitespace_splitter"": ""class CoreSuite:\n    def time_whitespace_splitter(self):\n        core.WhitespaceSplitter().process_line(self.line)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]""}",io_ascii.core.time_whitespace_splitter,time,0.0
11413,func-level,"terminus-2,oracle",1.0166635229568928,1.0166635229568928,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,modeling.model.time_init_gaussian_with_units,modeling.model,"{""modeling.model.time_init_gaussian_with_units"": ""def time_init_gaussian_with_units():\n    models.Gaussian1D(amplitude=10 * u.Hz, mean=5 * u.m, stddev=1.2 * u.cm)""}",modeling.model.time_init_gaussian_with_units,time,0.0
11371,func-level,"terminus-2,gpt-5",1.0322151554512686,1.0739800298432836,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.time_write,io_ascii.main,"{""io_ascii.main.CommentedHeaderInt.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.FixedWidthNoHeaderFloat.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.FixedWidthNoHeaderString.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.FixedWidthString.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.FixedWidthTwoLineString.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.IpacString.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.LatexInt.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.LatexString.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.RdbInt.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.TabInt.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.time_write,time,-0.02953668627440943
11411,func-level,"terminus-2,oracle",1.052227833379354,1.052227833379354,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,modeling.fitting.time_init_LevMarLSQFitter,modeling.fitting,"{""modeling.fitting.time_init_LevMarLSQFitter"": ""def time_init_LevMarLSQFitter():\n    fitting.LevMarLSQFitter()""}",modeling.fitting.time_init_LevMarLSQFitter,time,0.0
11399,func-level,"terminus-2,oracle",0.9440264105865166,0.9440264105865166,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports.timeraw_import_astropy_wcs_wcsapi,imports,"{""imports.timeraw_import_astropy_wcs_wcsapi"": ""def timeraw_import_astropy_wcs_wcsapi():\n    return \""\""\""\n    from astropy.wcs import wcsapi\n    \""\""\""""}",imports.timeraw_import_astropy_wcs_wcsapi,time,0.0
11408,func-level,"terminus-2,oracle",1.059748629030831,1.059748629030831,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_fits.time_get_hierarch,io_fits,"{""io_fits.FITSHeader.time_get_hierarch"": ""class FITSHeader:\n    def time_get_hierarch(self):\n        self.hdr.get(\""HIERARCH FOO BAR 999\"")\n\n    def setup(self):\n        self.hdr = make_header()\n        self.hdr_string = self.hdr.tostring()""}",io_fits.time_get_hierarch,time,0.0
11405,func-level,"terminus-2,oracle",1.0586583810611263,1.0586583810611263,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.time_write,io_ascii.main,"{""io_ascii.main.CommentedHeaderFloat.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.CommentedHeaderInt.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.NoHeaderInt.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.TabInt.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.time_write,time,0.0
11402,func-level,"terminus-2,oracle",1.0537416530971275,1.0537416530971275,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.core.time_default_splitter_call,io_ascii.core,"{""io_ascii.core.CoreSuite.time_default_splitter_call"": ""class CoreSuite:\n    def time_default_splitter_call(self):\n        core.DefaultSplitter()(self.csv_line)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]""}",io_ascii.core.time_default_splitter_call,time,0.0
11409,func-level,"terminus-2,oracle",1.0168241644485905,1.0168241644485905,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_fits.time_get_int,io_fits,"{""io_fits.FITSHeader.time_get_int"": ""class FITSHeader:\n    def time_get_int(self):\n        self.hdr.get(\""INT999\"")\n\n    def setup(self):\n        self.hdr = make_header()\n        self.hdr_string = self.hdr.tostring()""}",io_fits.time_get_int,time,0.0
11410,func-level,"terminus-2,oracle",0.968112742119425,0.968112742119425,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,modeling.fitting.time_Chebyshev1D_LevMarLSQFitter,modeling.fitting,"{""modeling.fitting.time_Chebyshev1D_LevMarLSQFitter"": ""def time_Chebyshev1D_LevMarLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        y = y_base + np.random.normal(0.0, 0.2, y_base.shape)\n        fit_LevMarLSQFitter(Chebyshev1D, x, y)\n    except Warning:\n        pass""}",modeling.fitting.time_Chebyshev1D_LevMarLSQFitter,time,0.0
11412,func-level,"terminus-2,oracle",1.0075690507073685,1.0075690507073685,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,modeling.fitting.time_init_LinearLSQFitter,modeling.fitting,"{""modeling.fitting.time_init_LinearLSQFitter"": ""def time_init_LinearLSQFitter():\n    fitting.LinearLSQFitter()""}",modeling.fitting.time_init_LinearLSQFitter,time,0.0
11422,func-level,"terminus-2,oracle",1.006789553503843,1.006789553503843,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.time_item_get_rowfirst,table,"{""table.TimeTable.time_item_get_rowfirst"": ""class TimeTable:\n    def time_item_get_rowfirst(self):\n        self.table[300][\""b\""]\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6""}",table.time_item_get_rowfirst,time,0.0
11407,func-level,"terminus-2,oracle",1.0207959134133997,1.0207959134133997,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_fits.time_get_float,io_fits,"{""io_fits.FITSHeader.time_get_float"": ""class FITSHeader:\n    def time_get_float(self):\n        self.hdr.get(\""FLT999\"")\n\n    def setup(self):\n        self.hdr = make_header()\n        self.hdr_string = self.hdr.tostring()""}",io_fits.time_get_float,time,0.0
11415,func-level,"terminus-2,oracle",0.9806920326250612,0.9806920326250612,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.time_add_column,table,"{""table.TimeTable.time_add_column"": ""class TimeTable:\n    def time_add_column(self):\n        self.table[\""e\""] = self.extra_column\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6""}",table.time_add_column,time,0.0
11416,func-level,"terminus-2,oracle",0.9596615099407187,0.9596615099407187,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.time_column_slice_bool,table,"{""table.TimeTable.time_column_slice_bool"": ""class TimeTable:\n    def time_column_slice_bool(self):\n        self.table[\""a\""][self.bool_mask]\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6""}",table.time_column_slice_bool,time,0.0
11419,func-level,"terminus-2,oracle",1.0684349181619717,1.0684349181619717,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.time_init_int_1d,table,"{""table.TimeTableInitWithMultiDimLists.time_init_int_1d"": ""class TimeTableInitWithMultiDimLists:\n    def time_init_int_1d(self):\n        Table([self.data_int_1d])\n\n    def setup(self):\n        np_data_int = np.arange(1_000_000, dtype=np.int64)\n        np_data_float = np_data_int.astype(np.float64)\n        np_data_str = np_data_int.astype(\""U\"")\n    \n        self.data_int_1d = np_data_int.tolist()\n    \n        self.data_int_3d = np_data_int.reshape(1000, 100, 10).tolist()\n    \n        self.data_int_masked_1d = self.data_int_1d.copy()\n        self.data_int_masked_1d[-1] = np.ma.masked\n    \n        self.data_int_masked_3d = self.data_int_3d.copy()\n        self.data_int_masked_3d[-1][-1][-1] = np.ma.masked\n    \n        self.data_float_1d = np_data_float.tolist()\n    \n        self.data_str_1d = np_data_str.tolist()\n    \n        self.data_str_masked_1d = self.data_str_1d.copy()\n        self.data_str_masked_1d[-1] = np.ma.masked""}",table.time_init_int_1d,time,0.0
11417,func-level,"terminus-2,oracle",0.955421083240806,0.955421083240806,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.time_hstack,table,"{""table.TimeTable.time_hstack"": ""class TimeTable:\n    def time_hstack(self):\n        hstack([self.table, self.other_table_2])\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6""}",table.time_hstack,time,0.0
11420,func-level,"terminus-2,oracle",1.019241621879223,1.019241621879223,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.time_init_int_masked_3d,table,"{""table.TimeTableInitWithMultiDimLists.time_init_int_masked_3d"": ""class TimeTableInitWithMultiDimLists:\n    def time_init_int_masked_3d(self):\n        Table([self.data_int_masked_3d])\n\n    def setup(self):\n        np_data_int = np.arange(1_000_000, dtype=np.int64)\n        np_data_float = np_data_int.astype(np.float64)\n        np_data_str = np_data_int.astype(\""U\"")\n    \n        self.data_int_1d = np_data_int.tolist()\n    \n        self.data_int_3d = np_data_int.reshape(1000, 100, 10).tolist()\n    \n        self.data_int_masked_1d = self.data_int_1d.copy()\n        self.data_int_masked_1d[-1] = np.ma.masked\n    \n        self.data_int_masked_3d = self.data_int_3d.copy()\n        self.data_int_masked_3d[-1][-1][-1] = np.ma.masked\n    \n        self.data_float_1d = np_data_float.tolist()\n    \n        self.data_str_1d = np_data_str.tolist()\n    \n        self.data_str_masked_1d = self.data_str_1d.copy()\n        self.data_str_masked_1d[-1] = np.ma.masked""}",table.time_init_int_masked_3d,time,0.0
11421,func-level,"terminus-2,oracle",1.070279171332443,1.070279171332443,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.time_init_lists,table,"{""table.TimeTableInitWithLists.time_init_lists"": ""class TimeTableInitWithLists:\n    def time_init_lists(self):\n        Table([self.dat, self.dat, self.dat], names=[\""time\"", \""rate\"", \""error\""])\n\n    def setup(self):\n        self.dat = list(range(100_000))""}",table.time_init_lists,time,0.0
11424,func-level,"terminus-2,oracle",1.0210058620492353,1.0210058620492353,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.time_read_rows,table,"{""table.TimeTable.time_read_rows"": ""class TimeTable:\n    def time_read_rows(self):\n        for row in self.table:\n            tuple(row)\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6""}",table.time_read_rows,time,0.0
11423,func-level,"terminus-2,oracle",0.9314596873228048,0.9314596873228048,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.time_join_inner,table,"{""table.TimeMaskedTable.time_join_inner"": ""class TimeTable:\n    def time_join_inner(self):\n        join(self.table, self.other_table, keys=\""i\"", join_type=\""inner\"")\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6""}",table.time_join_inner,time,0.0
11414,func-level,"terminus-2,oracle",1.0187313338888078,1.0187313338888078,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,stats.sigma_clipping.time_3d_array_axis2,stats.sigma_clipping,"{""stats.sigma_clipping.SigmaClipBenchmarks.time_3d_array_axis2"": ""class SigmaClipBenchmarks:\n    def time_3d_array_axis2(self):\n        self.sigclip(self.data, axis=(0, 1))\n\n    def setup(self):\n        # Avoid top-level module import to make sure that the benchmarks are\n        # compatible with versions of astropy that did not have this functionality.\n        from astropy.stats import SigmaClip\n    \n        size = (4, 2048, 2048)\n    \n        with NumpyRNGContext(12345):\n            self.data = np.random.normal(size=size)\n    \n            # add outliers\n            nbad = 100000\n            zbad = np.random.randint(low=0, high=size[0] - 1, size=nbad)\n            ybad = np.random.randint(low=0, high=size[1] - 1, size=nbad)\n            xbad = np.random.randint(low=0, high=size[2] - 1, size=nbad)\n            self.data[zbad, ybad, xbad] = np.random.choice([-1, 1], size=nbad) * (\n                10 + np.random.rand(nbad)\n            )\n    \n            # The defaults use median as the cenfunc and standard\n            # deviation as the stdfunc.  The default iters is 5.\n            self.sigclip = SigmaClip(sigma=3)""}",stats.sigma_clipping.time_3d_array_axis2,time,0.0
11429,func-level,"terminus-2,oracle",1.015357168017734,1.015357168017734,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,units.time_unit_to,units,"{""units.time_unit_to"": ""def time_unit_to():\n    u.m.to(u.pc)""}",units.time_unit_to,time,0.0
11430,func-level,"terminus-2,oracle",1.0136094389155583,1.0136094389155583,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,votable.time_small_binary2,votable,"{""votable.TimeVOTableSmallOverhead.time_small_binary2"": ""class TimeVOTableSmallOverhead:\n    def time_small_binary2(self):\n        parse(io.BytesIO(self.binary2_data))\n\n    def setup(self):\n        table = Table(\n            [\n                ra_data[:SMALL_SIZE],\n                dec_data[:SMALL_SIZE],\n                mag_data[:SMALL_SIZE]\n            ],\n            names=['ra', 'dec', 'mag']\n        )\n    \n        self.binary_data = create_votable_bytes(table, 'binary')\n        self.binary2_data = create_votable_bytes(table, 'binary2')""}",votable.time_small_binary2,time,0.0
11425,func-level,"terminus-2,oracle",1.0436571240682462,1.0436571240682462,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,units.time_quantity_add,units,"{""units.TimeQuantityOpSmallArrayDiffUnit.time_quantity_add"": ""class TimeQuantityOpSmallArrayDiffUnit:\n    def time_quantity_add(self):\n        # Same as operator.add\n        self.data + self.data2\n\n    def setup(self):\n        data = np.array([1.0, 2.0, 3.0])\n        self.data = data * u.g\n    \n        # A different but dimensionally compatible unit\n        self.data2 = 0.001 * data * u.kg""}",units.time_quantity_add,time,0.0
11418,func-level,"terminus-2,oracle",0.9383809606086992,0.9383809606086992,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.time_init_from_np_array_copy,table,"{""table.TimeTable.time_init_from_np_array_copy"": ""class TimeTable:\n    def time_init_from_np_array_copy(self):\n        Table(self.np_table, copy=True)\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6""}",table.time_init_from_np_array_copy,time,0.0
11428,func-level,"terminus-2,oracle",1.043050322455329,1.043050322455329,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,units.time_quantity_view,units,"{""units.time_quantity_view"": ""def time_quantity_view():\n    q1.view(u.Quantity)""}",units.time_quantity_view,time,0.0
11433,func-level,"terminus-2,oracle",0.949979563682008,0.949979563682008,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,wcs.time_pix2world_x_y_1,wcs,"{""wcs.WCSTransformations.time_pix2world_x_y_1"": ""class WCSTransformations:\n    def time_pix2world_x_y_1(self, size):\n        wcs.wcs_pix2world(self.px, self.py, 1)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)""}",wcs.time_pix2world_x_y_1,time,0.0
11427,func-level,"terminus-2,oracle",1.0080852034220826,1.0080852034220826,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,units.time_quantity_np_square,units,"{""units.TimeQuantityOpLargeArray.time_quantity_np_square"": ""class TimeQuantityOpSmallArray:\n    def time_quantity_np_square(self):\n        np.power(self.data, 2)\n\nclass TimeQuantityOpLargeArray:\n    def setup(self):\n        data = np.arange(1e6) + 1\n        self.data = data * u.g\n        self.out_sq = data * u.g**2\n        self.out_sqrt = data * u.g**0.5""}",units.time_quantity_np_square,time,0.0
11435,func-level,"terminus-2,oracle",0.916563197620162,0.916563197620162,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,wcs.time_world2pix_x_y_1,wcs,"{""wcs.WCSTransformations.time_world2pix_x_y_1"": ""class WCSTransformations:\n    def time_world2pix_x_y_1(self, size):\n        wcs.wcs_world2pix(self.wx, self.wy, 1)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)""}",wcs.time_world2pix_x_y_1,time,0.0
11431,func-level,"terminus-2,oracle",1.0312724196031475,1.0312724196031475,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,wcs.time_ape14_world_to_pixel,wcs,"{""wcs.WCSTransformations.time_ape14_world_to_pixel"": ""class WCSTransformations:\n    def time_ape14_world_to_pixel(self, size):\n        wcs.world_to_pixel(self.coord)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)""}",wcs.time_ape14_world_to_pixel,time,0.0
11426,func-level,"terminus-2,oracle",1.016057738104207,1.016057738104207,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,units.time_quantity_np_add,units,"{""units.TimeQuantityOpSmallArrayDiffUnit.time_quantity_np_add"": ""class TimeQuantityOpSmallArrayDiffUnit:\n    def time_quantity_np_add(self):\n        np.add(self.data, self.data2)\n\n    def setup(self):\n        data = np.array([1.0, 2.0, 3.0])\n        self.data = data * u.g\n    \n        # A different but dimensionally compatible unit\n        self.data2 = 0.001 * data * u.kg""}",units.time_quantity_np_add,time,0.0
11432,func-level,"terminus-2,oracle",0.9528888363018873,0.9528888363018873,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,wcs.time_pix2world_x_y_0,wcs,"{""wcs.WCSTransformations.time_pix2world_x_y_0"": ""class WCSTransformations:\n    def time_pix2world_x_y_0(self, size):\n        wcs.wcs_pix2world(self.px, self.py, 0)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)""}",wcs.time_pix2world_x_y_0,time,0.0
11434,func-level,"terminus-2,oracle",0.925068101922638,0.925068101922638,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,wcs.time_world2pix_x_y_0,wcs,"{""wcs.WCSTransformations.time_world2pix_x_y_0"": ""class WCSTransformations:\n    def time_world2pix_x_y_0(self, size):\n        wcs.wcs_world2pix(self.wx, self.wy, 0)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)""}",wcs.time_world2pix_x_y_0,time,0.0
14787,func-level,"terminus-2,claude",1.1304151036932122,1.0755927325105377,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.time_isin,algos.isin,"{""algos.isin.IsInLongSeriesLookUpDominates.time_isin"": ""class IsInLongSeriesLookUpDominates:\n    def time_isin(self, dtypes, MaxNumber, series_type):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, MaxNumber, series_type):\n        N = 10**7\n    \n        if series_type == \""random_hits\"":\n            array = np.random.randint(0, MaxNumber, N)\n        if series_type == \""random_misses\"":\n            array = np.random.randint(0, MaxNumber, N) + MaxNumber\n        if series_type == \""monotone_hits\"":\n            array = np.repeat(np.arange(MaxNumber), N // MaxNumber)\n        if series_type == \""monotone_misses\"":\n            array = np.arange(N) + MaxNumber\n    \n        self.series = Series(array).astype(dtype)\n    \n        self.values = np.arange(MaxNumber).astype(dtype.lower())"", ""algos.isin.IsInLongSeriesValuesDominate.time_isin"": ""class IsInLongSeriesValuesDominate:\n    def time_isin(self, dtypes, series_type):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, series_type):\n        N = 10**7\n    \n        if series_type == \""random\"":\n            vals = np.random.randint(0, 10 * N, N)\n        if series_type == \""monotone\"":\n            vals = np.arange(N)\n    \n        self.values = vals.astype(dtype.lower())\n        M = 10**6 + 1\n        self.series = Series(np.arange(M)).astype(dtype)"", ""algos.isin.IsinAlmostFullWithRandomInt.time_isin"": ""class IsinAlmostFullWithRandomInt:\n    def time_isin(self, dtype, exponent, title):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, exponent, title):\n        M = 3 * 2 ** (exponent - 2)\n        # 0.77-the maximal share of occupied buckets\n        self.series = Series(np.random.randint(0, M, M)).astype(dtype)\n    \n        values = np.random.randint(0, M, M).astype(dtype)\n        if title == \""inside\"":\n            self.values = values\n        elif title == \""outside\"":\n            self.values = values + M\n        else:\n            raise ValueError(title)"", ""algos.isin.IsinWithArange.time_isin"": ""class IsinWithArange:\n    def time_isin(self, dtype, M, offset_factor):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, M, offset_factor):\n        offset = int(M * offset_factor)\n        tmp = Series(np.random.randint(offset, M + offset, 10**6))\n        self.series = tmp.astype(dtype)\n        self.values = np.arange(M).astype(dtype)"", ""algos.isin.IsinWithRandomFloat.time_isin"": ""class IsinWithRandomFloat:\n    def time_isin(self, dtype, size, title):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, size, title):\n        self.values = np.random.rand(size)\n        self.series = Series(self.values).astype(dtype)\n        np.random.shuffle(self.values)\n    \n        if title == \""outside\"":\n            self.values = self.values + 0.1""}",algos.isin.time_isin,time,0.0387711253059933
14786,func-level,"terminus-2,claude",1.133311625383533,1.1089147165785465,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algorithms.time_quantile,algorithms,"{""algorithms.Quantile.time_quantile"": ""class Quantile:\n    def time_quantile(self, quantile, interpolation, dtype):\n        self.idx.quantile(quantile, interpolation=interpolation)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, quantile, interpolation, dtype):\n        N = 10**5\n        data = {\n            \""int\"": np.arange(N),\n            \""uint\"": np.arange(N).astype(np.uint64),\n            \""float\"": np.random.randn(N),\n        }\n        self.idx = pd.Series(data[dtype].repeat(5))""}",algorithms.time_quantile,time,0.01725382518032989
14788,func-level,"terminus-2,claude",1.0528985669144937,1.0396141997096964,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.time_isin_empty,algos.isin,"{""algos.isin.IsIn.time_isin_empty"": ""class IsIn:\n    def time_isin_empty(self, dtype):\n        self.series.isin([])\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)""}",algos.isin.time_isin_empty,time,0.009394884869022143
14791,func-level,"terminus-2,claude",0.9964518521955192,0.9848606535373996,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.time_frame_float_mod,arithmetic,"{""arithmetic.Ops2.time_frame_float_mod"": ""class Ops2:\n    def time_frame_float_mod(self):\n        self.df % self.df2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Ops2:\n    def setup(self):\n        N = 10**3\n        self.df = DataFrame(np.random.randn(N, N))\n        self.df2 = DataFrame(np.random.randn(N, N))\n    \n        self.df_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n        self.df2_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n    \n        self.s = Series(np.random.randn(N))""}",arithmetic.time_frame_float_mod,time,0.008197453082121386
14796,func-level,"terminus-2,claude",1.0548761297850124,0.9279782369189892,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,ctors.time_series_constructor,ctors,"{""ctors.SeriesConstructors.time_series_constructor"": ""class SeriesConstructors:\n    def time_series_constructor(self, data_fmt, with_index, dtype):\n        Series(self.data, index=self.index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesConstructors:\n    def setup(self, data_fmt, with_index, dtype):\n        if data_fmt in (gen_of_str, gen_of_tuples) and with_index:\n            raise NotImplementedError(\n                \""Series constructors do not support using generators with indexes\""\n            )\n        N = 10**4\n        if dtype == \""float\"":\n            arr = np.random.randn(N)\n        else:\n            arr = np.arange(N)\n        self.data = data_fmt(arr)\n        self.index = np.arange(N) if with_index else None""}",ctors.time_series_constructor,time,0.08974391291797969
14789,func-level,"terminus-2,claude",1.0925336030979893,1.1108688007180527,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.time_isin_mismatched_dtype,algos.isin,"{""algos.isin.IsIn.time_isin_mismatched_dtype"": ""class IsIn:\n    def time_isin_mismatched_dtype(self, dtype):\n        self.series.isin(self.mismatched)\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)""}",algos.isin.time_isin_mismatched_dtype,time,-0.012966900721402698
14795,func-level,"terminus-2,claude",1.0186548518363805,0.9767446348584446,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,ctors.time_from_list_of_dates,ctors,"{""ctors.DatetimeIndexConstructor.time_from_list_of_dates"": ""class DatetimeIndexConstructor:\n    def time_from_list_of_dates(self):\n        DatetimeIndex(self.list_of_dates)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndexConstructor:\n    def setup(self):\n        N = 20_000\n        dti = date_range(\""1900-01-01\"", periods=N)\n    \n        self.list_of_timestamps = dti.tolist()\n        self.list_of_dates = dti.date.tolist()\n        self.list_of_datetimes = dti.to_pydatetime().tolist()\n        self.list_of_str = dti.strftime(\""%Y-%m-%d\"").tolist()""}",ctors.time_from_list_of_dates,time,0.029639474524707103
14793,func-level,"terminus-2,claude",0.9898216294453116,0.9878651791606782,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,attrs_caching.time_extract_array,attrs_caching,"{""attrs_caching.SeriesArrayAttribute.time_extract_array"": ""class SeriesArrayAttribute:\n    def time_extract_array(self, dtype):\n        extract_array(self.series)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesArrayAttribute:\n    def setup(self, dtype):\n        if dtype == \""numeric\"":\n            self.series = pd.Series([1, 2, 3])\n        elif dtype == \""object\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=object)\n        elif dtype == \""category\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=\""category\"")\n        elif dtype == \""datetime64\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3))\n        elif dtype == \""datetime64tz\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3, tz=\""UTC\""))""}",attrs_caching.time_extract_array,time,0.0013836282069543328
14785,func-level,"terminus-2,claude",1.1648842382224,1.121587361662498,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algorithms.time_factorize,algorithms,"{""algorithms.Factorize.time_factorize"": ""class Factorize:\n    def time_factorize(self, unique, sort, dtype):\n        pd.factorize(self.data, sort=sort)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Factorize:\n    def setup(self, unique, sort, dtype):\n        N = 10**5\n        string_index = tm.makeStringIndex(N)\n        string_arrow = None\n        if dtype == \""string[pyarrow]\"":\n            try:\n                string_arrow = pd.array(string_index, dtype=\""string[pyarrow]\"")\n            except ImportError:\n                raise NotImplementedError\n    \n        data = {\n            \""int\"": pd.Index(np.arange(N), dtype=\""int64\""),\n            \""uint\"": pd.Index(np.arange(N), dtype=\""uint64\""),\n            \""float\"": pd.Index(np.random.randn(N), dtype=\""float64\""),\n            \""object\"": string_index,\n            \""datetime64[ns]\"": pd.date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n            \""datetime64[ns, tz]\"": pd.date_range(\n                \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n            ),\n            \""Int64\"": pd.array(np.arange(N), dtype=\""Int64\""),\n            \""boolean\"": pd.array(np.random.randint(0, 2, N), dtype=\""boolean\""),\n            \""string[pyarrow]\"": string_arrow,\n        }[dtype]\n        if not unique:\n            data = data.repeat(5)\n        self.data = data""}",algorithms.time_factorize,time,0.03062013900983168
14792,func-level,"terminus-2,claude",0.9824268958157192,0.9813637113697236,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,array.time_to_numpy,array,"{""array.ArrowExtensionArray.time_to_numpy"": ""class ArrowExtensionArray:\n    def time_to_numpy(self, dtype, hasna):\n        self.arr.to_numpy()\n\n    def setup(self, dtype, hasna):\n        N = 100_000\n        if dtype == \""boolean[pyarrow]\"":\n            data = np.random.choice([True, False], N, replace=True)\n        elif dtype == \""float64[pyarrow]\"":\n            data = np.random.randn(N)\n        elif dtype == \""int64[pyarrow]\"":\n            data = np.arange(N)\n        elif dtype == \""string[pyarrow]\"":\n            data = tm.rands_array(10, N)\n        elif dtype == \""timestamp[ns][pyarrow]\"":\n            data = pd.date_range(\""2000-01-01\"", freq=\""s\"", periods=N)\n        else:\n            raise NotImplementedError\n    \n        arr = pd.array(data, dtype=dtype)\n        if hasna:\n            arr[::2] = pd.NA\n        self.arr = arr""}",array.time_to_numpy,time,0.0007518984766588572
14794,func-level,"terminus-2,claude",0.973824571908645,0.9825157394271008,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.time_getitem_scalar,categoricals,"{""categoricals.CategoricalSlicing.time_getitem_scalar"": ""class CategoricalSlicing:\n    def time_getitem_scalar(self, index):\n        self.data[self.scalar]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalSlicing:\n    def setup(self, index):\n        N = 10**6\n        categories = [\""a\"", \""b\"", \""c\""]\n        values = [0] * N + [1] * N + [2] * N\n        if index == \""monotonic_incr\"":\n            self.data = pd.Categorical.from_codes(values, categories=categories)\n        elif index == \""monotonic_decr\"":\n            self.data = pd.Categorical.from_codes(\n                list(reversed(values)), categories=categories\n            )\n        elif index == \""non_monotonic\"":\n            self.data = pd.Categorical.from_codes([0, 1, 2] * N, categories=categories)\n        else:\n            raise ValueError(f\""Invalid index param: {index}\"")\n    \n        self.scalar = 10000\n        self.list = list(range(10000))\n        self.cat_scalar = \""b\""""}",categoricals.time_getitem_scalar,time,-0.006146511682076222
14800,func-level,"terminus-2,claude",1.084285370782898,1.0679607860056457,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.time_select_dtype_int_exclude,dtypes,"{""dtypes.SelectDtypes.time_select_dtype_int_exclude"": ""class SelectDtypes:\n    def time_select_dtype_int_exclude(self, dtype):\n        self.df_int.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )""}",dtypes.time_select_dtype_int_exclude,time,0.011544968017858844
14802,func-level,"terminus-2,claude",1.3953496003618713,1.2709339453302828,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.time_add,eval,"{""eval.Eval.time_add"": ""class Eval:\n    def time_add(self, engine, threads):\n        pd.eval(\""self.df + self.df2 + self.df3 + self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)""}",eval.time_add,time,0.08798844061639921
14797,func-level,"terminus-2,claude",1.0270767187934915,1.0630451419651972,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.time_pandas_dtype,dtypes,"{""dtypes.Dtypes.time_pandas_dtype"": ""class Dtypes:\n    def time_pandas_dtype(self, dtype):\n        pandas_dtype(dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)""}",dtypes.time_pandas_dtype,time,-0.02543735726428973
14799,func-level,"terminus-2,claude",1.0425175197787877,0.9747697591590926,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.time_select_dtype_float_include,dtypes,"{""dtypes.SelectDtypes.time_select_dtype_float_include"": ""class SelectDtypes:\n    def time_select_dtype_float_include(self, dtype):\n        self.df_float.select_dtypes(include=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )""}",dtypes.time_select_dtype_float_include,time,0.04791213622326388
14803,func-level,"terminus-2,claude",1.0934879570027516,1.1345405893436904,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.time_and,eval,"{""eval.Eval.time_and"": ""class Eval:\n    def time_and(self, engine, threads):\n        pd.eval(\n            \""(self.df > 0) & (self.df2 > 0) & (self.df3 > 0) & (self.df4 > 0)\"",\n            engine=engine,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)""}",eval.time_and,time,-0.029032979024709204
14790,func-level,"terminus-2,claude",1.0161320571669217,0.8638397455809634,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.time_add_series_offset,arithmetic,"{""arithmetic.OffsetArrayArithmetic.time_add_series_offset"": ""class OffsetArrayArithmetic:\n    def time_add_series_offset(self, offset):\n        with warnings.catch_warnings(record=True):\n            self.ser + offset\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass OffsetArrayArithmetic:\n    def setup(self, offset):\n        N = 10000\n        rng = date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"")\n        self.rng = rng\n        self.ser = Series(rng)""}",arithmetic.time_add_series_offset,time,0.10770319065485025
14804,func-level,"terminus-2,claude",1.1503155650897716,1.159924219526538,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.time_chained_cmp,eval,"{""eval.Eval.time_chained_cmp"": ""class Eval:\n    def time_chained_cmp(self, engine, threads):\n        pd.eval(\""self.df < self.df2 < self.df3 < self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)""}",eval.time_chained_cmp,time,-0.00679537088880226
14814,func-level,"terminus-2,claude",1.0370228351786577,1.028257179873277,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_apply_axis_1,frame_methods,"{""frame_methods.Apply.time_apply_axis_1"": ""class Apply:\n    def time_apply_axis_1(self):\n        self.df.apply(lambda x: x + 1, axis=1)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self):\n        self.df = DataFrame(np.random.randn(1000, 100))\n    \n        self.s = Series(np.arange(1028.0))\n        self.df2 = DataFrame({i: self.s for i in range(1028)})\n        self.df3 = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))""}",frame_methods.time_apply_axis_1,time,0.006199190456422037
14801,func-level,"terminus-2,claude",1.1128212073620574,1.3767683410933085,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.time_select_dtype_string_exclude,dtypes,"{""dtypes.SelectDtypes.time_select_dtype_string_exclude"": ""class SelectDtypes:\n    def time_select_dtype_string_exclude(self, dtype):\n        self.df_string.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )""}",dtypes.time_select_dtype_string_exclude,time,-0.18666699698108283
14810,func-level,"terminus-2,claude",1.0370344496148716,1.0154414722650773,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.time_frame_from_lists,frame_ctor,"{""frame_ctor.FromLists.time_frame_from_lists"": ""class FromLists:\n    def time_frame_from_lists(self):\n        self.df = DataFrame(self.data)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromLists:\n    def setup(self):\n        N = 1000\n        M = 100\n        self.data = [list(range(M)) for i in range(N)]""}",frame_ctor.time_frame_from_lists,time,0.015270846782032717
14806,func-level,"terminus-2,claude",1.2037987398290126,1.1317455443362747,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.time_query_datetime_column,eval,"{""eval.Query.time_query_datetime_column"": ""class Query:\n    def time_query_datetime_column(self):\n        self.df.query(\""dates < @self.ts\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Query:\n    def setup(self):\n        N = 10**6\n        halfway = (N // 2) - 1\n        index = pd.date_range(\""20010101\"", periods=N, freq=\""T\"")\n        s = pd.Series(index)\n        self.ts = s.iloc[halfway]\n        self.df = pd.DataFrame({\""a\"": np.random.randn(N), \""dates\"": index}, index=index)\n        data = np.random.randn(N)\n        self.min_val = data.min()\n        self.max_val = data.max()""}",eval.time_query_datetime_column,time,0.05095699822683022
14798,func-level,"terminus-2,claude",1.0285055100576934,0.9843871123238844,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.time_pandas_dtype_invalid,dtypes,"{""dtypes.DtypesInvalid.time_pandas_dtype_invalid"": ""class DtypesInvalid:\n    def time_pandas_dtype_invalid(self, dtype):\n        try:\n            pandas_dtype(self.data_dict[dtype])\n        except TypeError:\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)""}",dtypes.time_pandas_dtype_invalid,time,0.031201129939044548
14812,func-level,"terminus-2,claude",1.2917013945587692,1.0644373832418372,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.time_list_of_dict,frame_ctor,"{""frame_ctor.FromDicts.time_list_of_dict"": ""class FromDicts:\n    def time_list_of_dict(self):\n        DataFrame(self.dict_list)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDicts:\n    def setup(self):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n        frame = DataFrame(np.random.randn(N, K), index=self.index, columns=self.columns)\n        self.data = frame.to_dict()\n        self.dict_list = frame.to_dict(orient=\""records\"")\n        self.data2 = {i: {j: float(j) for j in range(100)} for i in range(2000)}\n    \n        # arrays which we won't consolidate\n        self.dict_of_categoricals = {i: Categorical(np.arange(N)) for i in range(K)}""}",frame_ctor.time_list_of_dict,time,0.1607241947078727
14805,func-level,"terminus-2,claude",1.3328781077964085,1.3893436394248913,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.time_mult,eval,"{""eval.Eval.time_mult"": ""class Eval:\n    def time_mult(self, engine, threads):\n        pd.eval(\""self.df * self.df2 * self.df3 * self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)""}",eval.time_mult,time,-0.03993319068492417
14808,func-level,"terminus-2,claude",1.0801749345590252,1.0681690669423896,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.time_dict_of_categoricals,frame_ctor,"{""frame_ctor.FromDicts.time_dict_of_categoricals"": ""class FromDicts:\n    def time_dict_of_categoricals(self):\n        # dict of arrays that we won't consolidate\n        DataFrame(self.dict_of_categoricals)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDicts:\n    def setup(self):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n        frame = DataFrame(np.random.randn(N, K), index=self.index, columns=self.columns)\n        self.data = frame.to_dict()\n        self.dict_list = frame.to_dict(orient=\""records\"")\n        self.data2 = {i: {j: float(j) for j in range(100)} for i in range(2000)}\n    \n        # arrays which we won't consolidate\n        self.dict_of_categoricals = {i: Categorical(np.arange(N)) for i in range(K)}""}",frame_ctor.time_dict_of_categoricals,time,0.008490712600166685
14807,func-level,"terminus-2,claude",1.2329444195455332,1.1546050789147206,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.time_query_with_boolean_selection,eval,"{""eval.Query.time_query_with_boolean_selection"": ""class Query:\n    def time_query_with_boolean_selection(self):\n        self.df.query(\""(a >= @self.min_val) & (a <= @self.max_val)\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Query:\n    def setup(self):\n        N = 10**6\n        halfway = (N // 2) - 1\n        index = pd.date_range(\""20010101\"", periods=N, freq=\""T\"")\n        s = pd.Series(index)\n        self.ts = s.iloc[halfway]\n        self.df = pd.DataFrame({\""a\"": np.random.randn(N), \""dates\"": index}, index=index)\n        data = np.random.randn(N)\n        self.min_val = data.min()\n        self.max_val = data.max()""}",eval.time_query_with_boolean_selection,time,0.055402645424902836
14813,func-level,"terminus-2,claude",1.086827676208385,1.0588739842694828,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.time_nested_dict_int64,frame_ctor,"{""frame_ctor.FromDicts.time_nested_dict_int64"": ""class FromDicts:\n    def time_nested_dict_int64(self):\n        # nested dict, integer indexes, regression described in #621\n        DataFrame(self.data2)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDicts:\n    def setup(self):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n        frame = DataFrame(np.random.randn(N, K), index=self.index, columns=self.columns)\n        self.data = frame.to_dict()\n        self.dict_list = frame.to_dict(orient=\""records\"")\n        self.data2 = {i: {j: float(j) for j in range(100)} for i in range(2000)}\n    \n        # arrays which we won't consolidate\n        self.dict_of_categoricals = {i: Categorical(np.arange(N)) for i in range(K)}""}",frame_ctor.time_nested_dict_int64,time,0.019769230508417463
14809,func-level,"terminus-2,claude",1.190751097121914,1.109764724574163,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.time_frame_from_arrays_sparse,frame_ctor,"{""frame_ctor.FromArrays.time_frame_from_arrays_sparse"": ""class FromArrays:\n    def time_frame_from_arrays_sparse(self):\n        self.df = DataFrame._from_arrays(\n            self.sparse_arrays,\n            index=self.index,\n            columns=self.columns,\n            verify_integrity=False,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromArrays:\n    def setup(self):\n        N_rows = 1000\n        N_cols = 1000\n        self.float_arrays = [np.random.randn(N_rows) for _ in range(N_cols)]\n        self.sparse_arrays = [\n            pd.arrays.SparseArray(np.random.randint(0, 2, N_rows), dtype=\""float64\"")\n            for _ in range(N_cols)\n        ]\n        self.int_arrays = [\n            pd.array(np.random.randint(1000, size=N_rows), dtype=\""Int64\"")\n            for _ in range(N_cols)\n        ]\n        self.index = pd.Index(range(N_rows))\n        self.columns = pd.Index(range(N_cols))""}",frame_ctor.time_frame_from_arrays_sparse,time,0.05727466233928633
14817,func-level,"terminus-2,claude",1.0948167539051892,1.0485527998893167,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_dataframe_describe,frame_methods,"{""frame_methods.Describe.time_dataframe_describe"": ""class Describe:\n    def time_dataframe_describe(self):\n        self.df.describe()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Describe:\n    def setup(self):\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(0, 100, 10**6),\n                \""b\"": np.random.randint(0, 100, 10**6),\n                \""c\"": np.random.randint(0, 100, 10**6),\n            }\n        )""}",frame_methods.time_dataframe_describe,time,0.03271849647515733
14816,func-level,"terminus-2,claude",1.2679011711536292,1.1014807750057138,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_count_level_mixed_dtypes_multi,frame_methods,"{""frame_methods.Count.time_count_level_mixed_dtypes_multi"": ""class Count:\n    def time_count_level_mixed_dtypes_multi(self, axis):\n        self.df_mixed.count(axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Count:\n    def setup(self, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""\n    \n        self.df.index = MultiIndex.from_arrays([self.df.index, self.df.index])\n        self.df.columns = MultiIndex.from_arrays([self.df.columns, self.df.columns])\n        self.df_mixed.index = MultiIndex.from_arrays(\n            [self.df_mixed.index, self.df_mixed.index]\n        )\n        self.df_mixed.columns = MultiIndex.from_arrays(\n            [self.df_mixed.columns, self.df_mixed.columns]\n        )""}",frame_methods.time_count_level_mixed_dtypes_multi,time,0.11769476389527254
14821,func-level,"terminus-2,claude",1.2349416916784892,1.097394403466586,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_frame_fillna,frame_methods,"{""frame_methods.Fillna.time_frame_fillna"": ""class Fillna:\n    def time_frame_fillna(self, inplace, method, dtype):\n        self.df.fillna(inplace=inplace, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Fillna:\n    def setup(self, inplace, method, dtype):\n        N, M = 10000, 100\n        if dtype in (\""datetime64[ns]\"", \""datetime64[ns, tz]\"", \""timedelta64[ns]\""):\n            data = {\n                \""datetime64[ns]\"": date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n                \""datetime64[ns, tz]\"": date_range(\n                    \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n                ),\n                \""timedelta64[ns]\"": timedelta_range(start=\""1 day\"", periods=N, freq=\""1D\""),\n            }\n            self.df = DataFrame({f\""col_{i}\"": data[dtype] for i in range(M)})\n            self.df[::2] = None\n        else:\n            values = np.random.randn(N, M)\n            values[::2] = np.nan\n            if dtype == \""Int64\"":\n                values = values.round()\n            self.df = DataFrame(values, dtype=dtype)""}",frame_methods.time_frame_fillna,time,0.09727530990940829
14811,func-level,"terminus-2,claude",1.0673483349349584,1.048155438963408,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.time_frame_from_records_generator,frame_ctor,"{""frame_ctor.FromRecords.time_frame_from_records_generator"": ""class FromRecords:\n    def time_frame_from_records_generator(self, nrows):\n        # issue-6700\n        self.df = DataFrame.from_records(self.gen, nrows=nrows)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromRecords:\n    def setup(self, nrows):\n        N = 100000\n        self.gen = ((x, (x * 20), (x * 100)) for x in range(N))""}",frame_ctor.time_frame_from_records_generator,time,0.013573476641831955
14820,func-level,"terminus-2,claude",1.0458341879438715,1.0415164232546907,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_frame_duplicated_wide,frame_methods,"{""frame_methods.Duplicated.time_frame_duplicated_wide"": ""class Duplicated:\n    def time_frame_duplicated_wide(self):\n        self.df2.duplicated()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Duplicated:\n    def setup(self):\n        n = 1 << 20\n        t = date_range(\""2015-01-01\"", freq=\""S\"", periods=(n // 64))\n        xs = np.random.randn(n // 64).round(2)\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(-1 << 8, 1 << 8, n),\n                \""b\"": np.random.choice(t, n),\n                \""c\"": np.random.choice(xs, n),\n            }\n        )\n        self.df2 = DataFrame(np.random.randn(1000, 100).astype(str)).T""}",frame_methods.time_frame_duplicated_wide,time,0.003053581816959526
14815,func-level,"terminus-2,claude",1.0516602928577283,1.0553971358160663,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_apply_lambda_mean,frame_methods,"{""frame_methods.Apply.time_apply_lambda_mean"": ""class Apply:\n    def time_apply_lambda_mean(self):\n        self.df.apply(lambda x: x.mean())\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self):\n        self.df = DataFrame(np.random.randn(1000, 100))\n    \n        self.s = Series(np.arange(1028.0))\n        self.df2 = DataFrame({i: self.s for i in range(1028)})\n        self.df3 = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))""}",frame_methods.time_apply_lambda_mean,time,-0.002642746080861374
14818,func-level,"terminus-2,claude",1.2321341011815543,1.1480368354203323,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_dropna,frame_methods,"{""frame_methods.Dropna.time_dropna"": ""class Dropna:\n    def time_dropna(self, how, axis):\n        self.df.dropna(how=how, axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Dropna:\n    def setup(self, how, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""""}",frame_methods.time_dropna,time,0.05947472826111882
14823,func-level,"terminus-2,claude",1.0527241543064776,1.07645130760125,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_itertuples_raw_start,frame_methods,"{""frame_methods.Iteration.time_itertuples_raw_start"": ""class Iteration:\n    def time_itertuples_raw_start(self):\n        self.df4.itertuples(index=False, name=None)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))""}",frame_methods.time_itertuples_raw_start,time,-0.016780164989230895
14832,func-level,"terminus-2,claude",1.0790536747171422,1.070948001002442,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.time_frame_transform,groupby,"{""groupby.Cumulative.time_frame_transform"": ""class Cumulative:\n    def time_frame_transform(self, dtype, method, with_nans):\n        self.df.groupby(\""key\"").transform(method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cumulative:\n    def setup(self, dtype, method, with_nans):\n        if with_nans and dtype == \""int64\"":\n            raise NotImplementedError(\""Construction of df would raise\"")\n    \n        N = 500_000\n        keys = np.random.randint(0, 100, size=N)\n        vals = np.random.randint(-10, 10, (N, 5))\n    \n        if with_nans:\n            null_vals = vals.astype(float, copy=True)\n            null_vals[::2, :] = np.nan\n            null_vals[::3, :] = np.nan\n            df = DataFrame(null_vals, columns=list(\""abcde\""), dtype=dtype)\n            df[\""key\""] = keys\n            self.df = df\n        else:\n            df = DataFrame(vals, columns=list(\""abcde\"")).astype(dtype, copy=False)\n            df[\""key\""] = keys\n            self.df = df""}",groupby.time_frame_transform,time,0.005732442513932305
14828,func-level,"terminus-2,claude",1.0575302948020138,1.1079542764811172,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,gil.time_parallel,gil,"{""gil.ParallelGroupbyMethods.time_parallel"": ""class ParallelGroupbyMethods:\n    def time_parallel(self, threads, method):\n        self.parallel()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ParallelGroupbyMethods:\n    def setup(self, threads, method):\n        N = 10**6\n        ngroups = 10**3\n        df = DataFrame(\n            {\""key\"": np.random.randint(0, ngroups, size=N), \""data\"": np.random.randn(N)}\n        )\n    \n        @test_parallel(num_threads=threads)\n        def parallel():\n            getattr(df.groupby(\""key\"")[\""data\""], method)()\n    \n        self.parallel = parallel\n    \n        def loop():\n            getattr(df.groupby(\""key\"")[\""data\""], method)()\n    \n        self.loop = loop""}",gil.time_parallel,time,-0.03566052452553285
14824,func-level,"terminus-2,claude",1.095662227171521,1.1236743877828543,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_nlargest_two_columns,frame_methods,"{""frame_methods.NSort.time_nlargest_two_columns"": ""class NSort:\n    def time_nlargest_two_columns(self, keep):\n        self.df.nlargest(100, [\""A\"", \""B\""], keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NSort:\n    def setup(self, keep):\n        self.df = DataFrame(np.random.randn(100000, 3), columns=list(\""ABC\""))""}",frame_methods.time_nlargest_two_columns,time,-0.019810580347477562
14825,func-level,"terminus-2,claude",1.153540011059381,1.116944134539179,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_nsmallest_one_column,frame_methods,"{""frame_methods.NSort.time_nsmallest_one_column"": ""class NSort:\n    def time_nsmallest_one_column(self, keep):\n        self.df.nsmallest(100, \""A\"", keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NSort:\n    def setup(self, keep):\n        self.df = DataFrame(np.random.randn(100000, 3), columns=list(\""ABC\""))""}",frame_methods.time_nsmallest_one_column,time,0.025881100792222
14829,func-level,"terminus-2,claude",1.0284597004830336,0.9242780314831636,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.time_category_size,groupby,"{""groupby.Size.time_category_size"": ""class Size:\n    def time_category_size(self):\n        self.draws.groupby(self.cats).size()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Size:\n    def setup(self):\n        n = 10**5\n        offsets = np.random.randint(n, size=n).astype(\""timedelta64[ns]\"")\n        dates = np.datetime64(\""now\"") + offsets\n        self.df = DataFrame(\n            {\n                \""key1\"": np.random.randint(0, 500, size=n),\n                \""key2\"": np.random.randint(0, 100, size=n),\n                \""value1\"": np.random.randn(n),\n                \""value2\"": np.random.randn(n),\n                \""value3\"": np.random.randn(n),\n                \""dates\"": dates,\n            }\n        )\n        self.draws = Series(np.random.randn(n))\n        labels = Series([\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (n // 4))\n        self.cats = labels.astype(\""category\"")""}",groupby.time_category_size,time,0.07367869094757427
14826,func-level,"terminus-2,claude",1.158283804858569,1.1137320610096797,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_nsmallest_two_columns,frame_methods,"{""frame_methods.NSort.time_nsmallest_two_columns"": ""class NSort:\n    def time_nsmallest_two_columns(self, keep):\n        self.df.nsmallest(100, [\""A\"", \""B\""], keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NSort:\n    def setup(self, keep):\n        self.df = DataFrame(np.random.randn(100000, 3), columns=list(\""ABC\""))""}",frame_methods.time_nsmallest_two_columns,time,0.031507598195819934
14827,func-level,"terminus-2,claude",1.102871712480657,1.0482445796343702,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_series_describe,frame_methods,"{""frame_methods.Describe.time_series_describe"": ""class Describe:\n    def time_series_describe(self):\n        self.df[\""a\""].describe()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Describe:\n    def setup(self):\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(0, 100, 10**6),\n                \""b\"": np.random.randint(0, 100, 10**6),\n                \""c\"": np.random.randint(0, 100, 10**6),\n            }\n        )""}",frame_methods.time_series_describe,time,0.038633050103456044
14836,func-level,"terminus-2,claude",1.0688795064751455,0.8448937156170028,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.time_str_func,groupby,"{""groupby.String.time_str_func"": ""class String:\n    def time_str_func(self, dtype, method):\n        self.df.groupby(\""a\"")[self.df.columns[1:]].agg(method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass String:\n    def setup(self, dtype, method):\n        cols = list(\""abcdefghjkl\"")\n        self.df = DataFrame(\n            np.random.randint(0, 100, size=(10_000, len(cols))),\n            columns=cols,\n            dtype=dtype,\n        )""}",groupby.time_str_func,time,0.15840579268609814
14819,func-level,"terminus-2,claude",1.2352836128246911,1.1474070058342605,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_dropna_axis_mixed_dtypes,frame_methods,"{""frame_methods.Dropna.time_dropna_axis_mixed_dtypes"": ""class Dropna:\n    def time_dropna_axis_mixed_dtypes(self, how, axis):\n        self.df_mixed.dropna(how=how, axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Dropna:\n    def setup(self, how, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""""}",frame_methods.time_dropna_axis_mixed_dtypes,time,0.06214752969620273
14834,func-level,"terminus-2,claude",1.0661276571731069,0.7453157545798069,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.time_rank_ties,groupby,"{""groupby.RankWithTies.time_rank_ties"": ""class RankWithTies:\n    def time_rank_ties(self, dtype, tie_method):\n        self.df.groupby(\""key\"").rank(method=tie_method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass RankWithTies:\n    def setup(self, dtype, tie_method):\n        N = 10**4\n        if dtype == \""datetime64\"":\n            data = np.array([Timestamp(\""2011/01/01\"")] * N, dtype=dtype)\n        else:\n            data = np.array([1] * N, dtype=dtype)\n        self.df = DataFrame({\""values\"": data, \""key\"": [\""foo\""] * N})""}",groupby.time_rank_ties,time,0.22688253365862798
14822,func-level,"terminus-2,claude",1.0352487998803463,1.0248017468505206,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_frame_quantile,frame_methods,"{""frame_methods.Quantile.time_frame_quantile"": ""class Quantile:\n    def time_frame_quantile(self, axis):\n        self.df.quantile([0.1, 0.5], axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, axis):\n        self.df = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))""}",frame_methods.time_frame_quantile,time,0.007388297758009715
14830,func-level,"terminus-2,claude",1.0692863976783202,1.074014587749526,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.time_dtype_as_field,groupby,"{""groupby.GroupByMethods.time_dtype_as_field"": ""class GroupByMethods:\n    def time_dtype_as_field(self, dtype, method, application, ncols):\n        self.as_field_method()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupByMethods:\n    def setup(self, dtype, method, application, ncols):\n        if method in method_blocklist.get(dtype, {}):\n            raise NotImplementedError  # skip benchmark\n    \n        if ncols != 1 and method in [\""value_counts\"", \""unique\""]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if application == \""transformation\"" and method in [\n            \""describe\"",\n            \""head\"",\n            \""tail\"",\n            \""unique\"",\n            \""value_counts\"",\n            \""size\"",\n        ]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if method == \""describe\"":\n            ngroups = 20\n        elif method == \""skew\"":\n            ngroups = 100\n        else:\n            ngroups = 1000\n        size = ngroups * 2\n        rng = np.arange(ngroups).reshape(-1, 1)\n        rng = np.broadcast_to(rng, (len(rng), ncols))\n        taker = np.random.randint(0, ngroups, size=size)\n        values = rng.take(taker, axis=0)\n        if dtype == \""int\"":\n            key = np.random.randint(0, size, size=size)\n        elif dtype in (\""int16\"", \""uint\""):\n            key = np.random.randint(0, size, size=size, dtype=dtype)\n        elif dtype == \""float\"":\n            key = np.concatenate(\n                [np.random.random(ngroups) * 0.1, np.random.random(ngroups) * 10.0]\n            )\n        elif dtype == \""object\"":\n            key = [\""foo\""] * size\n        elif dtype == \""datetime\"":\n            key = date_range(\""1/1/2011\"", periods=size, freq=\""s\"")\n    \n        cols = [f\""values{n}\"" for n in range(ncols)]\n        df = DataFrame(values, columns=cols)\n        df[\""key\""] = key\n    \n        if len(cols) == 1:\n            cols = cols[0]\n    \n        if application == \""transformation\"":\n            self.as_group_method = lambda: df.groupby(\""key\"")[cols].transform(method)\n            self.as_field_method = lambda: df.groupby(cols)[\""key\""].transform(method)\n        else:\n            self.as_group_method = getattr(df.groupby(\""key\"")[cols], method)\n            self.as_field_method = getattr(df.groupby(cols)[\""key\""], method)""}",groupby.time_dtype_as_field,time,-0.0033438402200889174
14831,func-level,"terminus-2,claude",1.0829825703007137,1.071055066756287,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.time_dtype_as_group,groupby,"{""groupby.GroupByMethods.time_dtype_as_group"": ""class GroupByMethods:\n    def time_dtype_as_group(self, dtype, method, application, ncols):\n        self.as_group_method()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupByMethods:\n    def setup(self, dtype, method, application, ncols):\n        if method in method_blocklist.get(dtype, {}):\n            raise NotImplementedError  # skip benchmark\n    \n        if ncols != 1 and method in [\""value_counts\"", \""unique\""]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if application == \""transformation\"" and method in [\n            \""describe\"",\n            \""head\"",\n            \""tail\"",\n            \""unique\"",\n            \""value_counts\"",\n            \""size\"",\n        ]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if method == \""describe\"":\n            ngroups = 20\n        elif method == \""skew\"":\n            ngroups = 100\n        else:\n            ngroups = 1000\n        size = ngroups * 2\n        rng = np.arange(ngroups).reshape(-1, 1)\n        rng = np.broadcast_to(rng, (len(rng), ncols))\n        taker = np.random.randint(0, ngroups, size=size)\n        values = rng.take(taker, axis=0)\n        if dtype == \""int\"":\n            key = np.random.randint(0, size, size=size)\n        elif dtype in (\""int16\"", \""uint\""):\n            key = np.random.randint(0, size, size=size, dtype=dtype)\n        elif dtype == \""float\"":\n            key = np.concatenate(\n                [np.random.random(ngroups) * 0.1, np.random.random(ngroups) * 10.0]\n            )\n        elif dtype == \""object\"":\n            key = [\""foo\""] * size\n        elif dtype == \""datetime\"":\n            key = date_range(\""1/1/2011\"", periods=size, freq=\""s\"")\n    \n        cols = [f\""values{n}\"" for n in range(ncols)]\n        df = DataFrame(values, columns=cols)\n        df[\""key\""] = key\n    \n        if len(cols) == 1:\n            cols = cols[0]\n    \n        if application == \""transformation\"":\n            self.as_group_method = lambda: df.groupby(\""key\"")[cols].transform(method)\n            self.as_field_method = lambda: df.groupby(cols)[\""key\""].transform(method)\n        else:\n            self.as_group_method = getattr(df.groupby(\""key\"")[cols], method)\n            self.as_field_method = getattr(df.groupby(cols)[\""key\""], method)""}",groupby.time_dtype_as_group,time,0.00843529246423382
14835,func-level,"terminus-2,claude",1.0664601671525424,1.0230055870235066,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.time_scalar_function_multi_col,groupby,"{""groupby.Apply.time_scalar_function_multi_col"": ""class Apply:\n    def time_scalar_function_multi_col(self, factor):\n        self.df.groupby([\""key\"", \""key2\""]).apply(lambda x: 1)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, factor):\n        N = 10**factor\n        # two cases:\n        # - small groups: small data (N**4) + many labels (2000) -> average group\n        #   size of 5 (-> larger overhead of slicing method)\n        # - larger groups: larger data (N**5) + fewer labels (20) -> average group\n        #   size of 5000\n        labels = np.random.randint(0, 2000 if factor == 4 else 20, size=N)\n        labels2 = np.random.randint(0, 3, size=N)\n        df = DataFrame(\n            {\n                \""key\"": labels,\n                \""key2\"": labels2,\n                \""value1\"": np.random.randn(N),\n                \""value2\"": [\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (N // 4),\n            }\n        )\n        self.df = df""}",groupby.time_scalar_function_multi_col,time,0.030731669115301136
14839,func-level,"terminus-2,claude",1.129020629433639,1.1141947051174346,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,hash_functions.time_factorize,hash_functions,"{""hash_functions.UniqueAndFactorizeArange.time_factorize"": ""class UniqueAndFactorizeArange:\n    def time_factorize(self, exponent):\n        pd.factorize(self.a2)\n\n    def setup(self, exponent):\n        a = np.arange(10**4, dtype=\""float64\"")\n        self.a2 = (a + 10**exponent).repeat(100)""}",hash_functions.time_factorize,time,0.010485094990243516
14838,func-level,"terminus-2,claude",1.0393049894783648,1.042586536069814,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.time_transform_lambda_max_wide,groupby,"{""groupby.Transform.time_transform_lambda_max_wide"": ""class Transform:\n    def time_transform_lambda_max_wide(self):\n        self.df_wide.groupby(level=0).transform(lambda x: np.max(x, axis=0))\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]""}",groupby.time_transform_lambda_max_wide,time,-0.0023207543079556596
14842,func-level,"terminus-2,claude",1.1050428638006742,1.0838227693246487,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_cached_properties.time_is_monotonic_decreasing,index_cached_properties,"{""index_cached_properties.IndexCache.time_is_monotonic_decreasing"": ""class IndexCache:\n    def time_is_monotonic_decreasing(self, index_type):\n        self.idx.is_monotonic_decreasing\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}""}",index_cached_properties.time_is_monotonic_decreasing,time,0.015007138950513059
14844,func-level,"terminus-2,claude",1.1700522916098166,1.123397506290995,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_at_setitem,indexing,"{""indexing.DataFrameStringIndexing.time_at_setitem"": ""class DataFrameStringIndexing:\n    def time_at_setitem(self):\n        self.df.at[self.idx_scalar, self.col_scalar] = 0.0\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DataFrameStringIndexing:\n    def setup(self):\n        index = tm.makeStringIndex(1000)\n        columns = tm.makeStringIndex(30)\n        with warnings.catch_warnings(record=True):\n            self.df = DataFrame(np.random.randn(1000, 30), index=index, columns=columns)\n        self.idx_scalar = index[100]\n        self.col_scalar = columns[10]\n        self.bool_indexer = self.df[self.col_scalar] > 0\n        self.bool_obj_indexer = self.bool_indexer.astype(object)\n        self.boolean_indexer = (self.df[self.col_scalar] > 0).astype(\""boolean\"")""}",indexing.time_at_setitem,time,0.032994897679506016
14840,func-level,"terminus-2,claude",1.16151632024249,1.0227767447743357,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,hash_functions.time_loc_slice,hash_functions,"{""hash_functions.NumericSeriesIndexing.time_loc_slice"": ""class NumericSeriesIndexing:\n    def time_loc_slice(self, index, N):\n        # trigger building of mapping\n        self.data.loc[:800]\n\n    def setup(self, dtype, N):\n        vals = np.array(list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype)\n        indices = pd.Index(vals)\n        self.data = pd.Series(np.arange(N), index=indices)"", ""hash_functions.NumericSeriesIndexingShuffled.time_loc_slice"": ""class NumericSeriesIndexingShuffled:\n    def time_loc_slice(self, index, N):\n        # trigger building of mapping\n        self.data.loc[:800]\n\n    def setup(self, dtype, N):\n        vals = np.array(list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype)\n        np.random.shuffle(vals)\n        indices = pd.Index(vals)\n        self.data = pd.Series(np.arange(N), index=indices)""}",hash_functions.time_loc_slice,time,0.09811851164650241
14837,func-level,"terminus-2,claude",1.0514012644312607,1.0355765116122002,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.time_sum,groupby,"{""groupby.Datelike.time_sum"": ""class Datelike:\n    def time_sum(self, grouper):\n        self.df.groupby(self.grouper).sum()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Datelike:\n    def setup(self, grouper):\n        N = 10**4\n        rng_map = {\n            \""period_range\"": period_range,\n            \""date_range\"": date_range,\n            \""date_range_tz\"": partial(date_range, tz=\""US/Central\""),\n        }\n        self.grouper = rng_map[grouper](\""1900-01-01\"", freq=\""D\"", periods=N)\n        self.df = DataFrame(np.random.randn(10**4, 2))"", ""groupby.GroupManyLabels.time_sum"": ""class GroupManyLabels:\n    def time_sum(self, ncols):\n        self.df.groupby(self.labels).sum()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupManyLabels:\n    def setup(self, ncols):\n        N = 1000\n        data = np.random.randn(N, ncols)\n        self.labels = np.random.randint(0, 100, size=N)\n        self.df = DataFrame(data)""}",groupby.time_sum,time,0.01119148007005692
14843,func-level,"terminus-2,claude",1.0785916293887383,1.0220004458551766,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_cached_properties.time_is_monotonic_increasing,index_cached_properties,"{""index_cached_properties.IndexCache.time_is_monotonic_increasing"": ""class IndexCache:\n    def time_is_monotonic_increasing(self, index_type):\n        self.idx.is_monotonic_increasing\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}""}",index_cached_properties.time_is_monotonic_increasing,time,0.040022053418360494
14849,func-level,"terminus-2,claude",1.091964196574294,1.0473046935586134,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_getitem_label_slice,indexing,"{""indexing.NonNumericSeriesIndexing.time_getitem_label_slice"": ""class NonNumericSeriesIndexing:\n    def time_getitem_label_slice(self, index, index_structure):\n        self.s[: self.lbl]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]""}",indexing.time_getitem_label_slice,time,0.03158380694178265
14841,func-level,"terminus-2,claude",1.04918511585981,1.0398100956535965,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_cached_properties.time_engine,index_cached_properties,"{""index_cached_properties.IndexCache.time_engine"": ""class IndexCache:\n    def time_engine(self, index_type):\n        self.idx._engine\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}""}",index_cached_properties.time_engine,time,0.006630141588552689
14833,func-level,"terminus-2,claude",1.1539903005891918,1.0188571417553025,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.time_groupby_apply_non_unique_unsorted_index,groupby,"{""groupby.ApplyNonUniqueUnsortedIndex.time_groupby_apply_non_unique_unsorted_index"": ""class ApplyNonUniqueUnsortedIndex:\n    def time_groupby_apply_non_unique_unsorted_index(self):\n        self.df.groupby(\""key\"", group_keys=False).apply(lambda x: x)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ApplyNonUniqueUnsortedIndex:\n    def setup(self):\n        # GH 46527\n        # unsorted and non-unique index\n        idx = np.arange(100)[::-1]\n        idx = Index(np.repeat(idx, 200), name=\""key\"")\n        self.df = DataFrame(np.random.randn(len(idx), 10), index=idx)""}",groupby.time_groupby_apply_non_unique_unsorted_index,time,0.09556800483301933
14847,func-level,"terminus-2,claude",1.2826399838873093,1.2018416031412138,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_get_indexer_dups,indexing,"{""indexing.NumericMaskedIndexing.time_get_indexer_dups"": ""class NumericMaskedIndexing:\n    def time_get_indexer_dups(self, dtype, monotonic):\n        self.data.get_indexer_for(self.indexer)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericMaskedIndexing:\n    def setup(self, dtype, monotonic):\n        indices = {\n            True: Index(self.monotonic_list, dtype=dtype),\n            False: Index(self.non_monotonic_list, dtype=dtype).append(\n                Index([NA], dtype=dtype)\n            ),\n        }\n        self.data = indices[monotonic]\n        self.indexer = np.arange(300, 1_000)\n        self.data_dups = self.data.append(self.data)""}",indexing.time_get_indexer_dups,time,0.05714171198450882
14845,func-level,"terminus-2,claude",0.983835109845662,0.9839495425121074,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_frame_getitem_single_column_int,indexing,"{""indexing.GetItemSingleColumn.time_frame_getitem_single_column_int"": ""class GetItemSingleColumn:\n    def time_frame_getitem_single_column_int(self):\n        self.df_int_col[0]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GetItemSingleColumn:\n    def setup(self):\n        self.df_string_col = DataFrame(np.random.randn(3000, 1), columns=[\""A\""])\n        self.df_int_col = DataFrame(np.random.randn(3000, 1))""}",indexing.time_frame_getitem_single_column_int,time,-8.092833553422667e-05
14848,func-level,"terminus-2,claude",1.152386552649178,1.1377686919683618,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_getitem_array,indexing,"{""indexing.NumericSeriesIndexing.time_getitem_array"": ""class NumericSeriesIndexing:\n    def time_getitem_array(self, index, index_structure):\n        self.data[self.array]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.time_getitem_array,time,0.010337949562104862
14856,func-level,"terminus-2,claude",1.0230200839080663,1.038504163552764,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_loc_row,indexing,"{""indexing.IndexSingleRow.time_loc_row"": ""class IndexSingleRow:\n    def time_loc_row(self, unique_cols):\n        self.df.loc[10000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IndexSingleRow:\n    def setup(self, unique_cols):\n        arr = np.arange(10**7).reshape(-1, 10)\n        df = DataFrame(arr)\n        dtypes = [\""u1\"", \""u2\"", \""u4\"", \""u8\"", \""i1\"", \""i2\"", \""i4\"", \""i8\"", \""f8\"", \""f4\""]\n        for i, d in enumerate(dtypes):\n            df[i] = df[i].astype(d)\n    \n        if not unique_cols:\n            # GH#33032 single-row lookups with non-unique columns were\n            #  15x slower than with unique columns\n            df.columns = [\""A\"", \""A\""] + list(df.columns[2:])\n    \n        self.df = df""}",indexing.time_loc_row,time,-0.010950551375316676
14846,func-level,"terminus-2,claude",1.2656880762130045,1.2617980705467426,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_get_indexer,indexing,"{""indexing.NumericMaskedIndexing.time_get_indexer"": ""class NumericMaskedIndexing:\n    def time_get_indexer(self, dtype, monotonic):\n        self.data.get_indexer(self.indexer)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericMaskedIndexing:\n    def setup(self, dtype, monotonic):\n        indices = {\n            True: Index(self.monotonic_list, dtype=dtype),\n            False: Index(self.non_monotonic_list, dtype=dtype).append(\n                Index([NA], dtype=dtype)\n            ),\n        }\n        self.data = indices[monotonic]\n        self.indexer = np.arange(300, 1_000)\n        self.data_dups = self.data.append(self.data)""}",indexing.time_get_indexer,time,0.002751064827625073
14854,func-level,"terminus-2,claude",1.1974388419001345,1.052174150069658,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_loc_all_scalars,indexing,"{""indexing.MultiIndexing.time_loc_all_scalars"": ""class MultiIndexing:\n    def time_loc_all_scalars(self, unique_levels):\n        target = tuple([self.tgt_scalar] * self.nlevels)\n        self.df.loc[target, :]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MultiIndexing:\n    def setup(self, unique_levels):\n        self.nlevels = 2\n        if unique_levels:\n            mi = MultiIndex.from_arrays([range(1000000)] * self.nlevels)\n        else:\n            mi = MultiIndex.from_product([range(1000)] * self.nlevels)\n        self.df = DataFrame(np.random.randn(len(mi)), index=mi)\n    \n        self.tgt_slice = slice(200, 800)\n        self.tgt_null_slice = slice(None)\n        self.tgt_list = list(range(0, 1000, 10))\n        self.tgt_scalar = 500\n    \n        bool_indexer = np.zeros(len(mi), dtype=np.bool_)\n        bool_indexer[slice(0, len(mi), 100)] = True\n        self.tgt_bool_indexer = bool_indexer""}",indexing.time_loc_all_scalars,time,0.10273316253923376
14855,func-level,"terminus-2,claude",1.1321112935328206,1.092596161613923,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_loc_array,indexing,"{""indexing.NumericSeriesIndexing.time_loc_array"": ""class NumericSeriesIndexing:\n    def time_loc_array(self, index, index_structure):\n        self.data.loc[self.array]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.time_loc_array,time,0.027945637849291056
14852,func-level,"terminus-2,claude",1.0542725224825586,1.0368646001272837,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_getitem_scalar,indexing,"{""indexing.CategoricalIndexIndexing.time_getitem_scalar"": ""class CategoricalIndexIndexing:\n    def time_getitem_scalar(self, index):\n        self.data[self.int_scalar]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalIndexIndexing:\n    def setup(self, index):\n        N = 10**5\n        values = list(\""a\"" * N + \""b\"" * N + \""c\"" * N)\n        indices = {\n            \""monotonic_incr\"": CategoricalIndex(values),\n            \""monotonic_decr\"": CategoricalIndex(reversed(values)),\n            \""non_monotonic\"": CategoricalIndex(list(\""abc\"" * N)),\n        }\n        self.data = indices[index]\n        self.data_unique = CategoricalIndex([str(i) for i in range(N * 3)])\n    \n        self.int_scalar = 10000\n        self.int_list = list(range(10000))\n    \n        self.cat_scalar = \""b\""\n        self.cat_list = [\""1\"", \""3\""]"", ""indexing.NonNumericSeriesIndexing.time_getitem_scalar"": ""class NonNumericSeriesIndexing:\n    def time_getitem_scalar(self, index, index_structure):\n        self.s[self.lbl]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]"", ""indexing.NumericSeriesIndexing.time_getitem_scalar"": ""class NumericSeriesIndexing:\n    def time_getitem_scalar(self, index, index_structure):\n        self.data[800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.time_getitem_scalar,time,0.01231111906313649
14853,func-level,"terminus-2,claude",1.2122411735831442,1.2623581032741231,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_getitem_slice,indexing,"{""indexing.NumericSeriesIndexing.time_getitem_slice"": ""class NumericSeriesIndexing:\n    def time_getitem_slice(self, index, index_structure):\n        self.data[:800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.time_getitem_slice,time,-0.035443373190225544
14857,func-level,"terminus-2,claude",0.9667672142142164,0.9670654987224886,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_lookup_iloc,indexing,"{""indexing.MethodLookup.time_lookup_iloc"": ""class MethodLookup:\n    def time_lookup_iloc(self, s):\n        s.iloc\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MethodLookup:\n    def setup_cache(self):\n        s = Series()\n        return s""}",indexing.time_lookup_iloc,time,-0.00021095085450649514
14858,func-level,"terminus-2,claude",0.9753593178122134,0.9826809808730528,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_lookup_loc,indexing,"{""indexing.MethodLookup.time_lookup_loc"": ""class MethodLookup:\n    def time_lookup_loc(self, s):\n        s.loc\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MethodLookup:\n    def setup_cache(self):\n        s = Series()\n        return s""}",indexing.time_lookup_loc,time,-0.00517797953383265
14859,func-level,"terminus-2,claude",0.9915260675365308,1.0259419388693445,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing_engines.time_get_loc,indexing_engines,"{""indexing_engines.MaskedNumericEngineIndexing.time_get_loc"": ""class MaskedNumericEngineIndexing:\n    def time_get_loc(self, engine_and_dtype, index_type, unique, N):\n        self.data.get_loc(self.key_early)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())[::-1]\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.zeros(N * 3, dtype=dtype.lower())\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype.lower())\n                arr[N:] = np.arange(N * 2, dtype=dtype.lower())\n    \n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n            mask[-1] = True\n    \n        self.data = engine(BaseMaskedArray(arr, mask))\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]"", ""indexing_engines.NumericEngineIndexing.time_get_loc"": ""class NumericEngineIndexing:\n    def time_get_loc(self, engine_and_dtype, index_type, unique, N):\n        self.data.get_loc(self.key_early)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)[::-1]\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.empty(N * 3, dtype=dtype)\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype)\n                arr[N:] = np.arange(N * 2, dtype=dtype)\n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype)\n    \n        self.data = engine(arr)\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]""}",indexing_engines.time_get_loc,time,-0.02433937152249905
14851,func-level,"terminus-2,claude",1.2380816076528536,1.208053664760974,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_getitem_lists,indexing,"{""indexing.NumericSeriesIndexing.time_getitem_lists"": ""class NumericSeriesIndexing:\n    def time_getitem_lists(self, index, index_structure):\n        self.data[self.array_list]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.time_getitem_lists,time,0.021236168947580968
14865,func-level,"terminus-2,claude",0.9618587767167588,0.97232287374828,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.style.time_format_render,io.style,"{""io.style.Render.time_format_render"": ""class Render:\n    def time_format_render(self, cols, rows):\n        self._style_format()\n        self.st._render_html(True, True)\n\n    def setup(self, cols, rows):\n        self.df = DataFrame(\n            np.random.randn(rows, cols),\n            columns=[f\""float_{i+1}\"" for i in range(cols)],\n            index=[f\""row_{i+1}\"" for i in range(rows)],\n        )""}",io.style.time_format_render,time,-0.007400351507440722
14861,func-level,"terminus-2,claude",1.0232202472077538,1.0287380341736552,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,inference.time_downcast,inference,"{""inference.ToNumericDowncast.time_downcast"": ""class ToNumericDowncast:\n    def time_downcast(self, dtype, downcast):\n        to_numeric(self.data, downcast=downcast)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToNumericDowncast:\n    def setup(self, dtype, downcast):\n        self.data = self.data_dict[dtype]""}",inference.time_downcast,time,-0.003902253865559638
14850,func-level,"terminus-2,claude",1.1458274884729205,1.3297764991995344,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_getitem_list_like,indexing,"{""indexing.NonNumericSeriesIndexing.time_getitem_list_like"": ""class NonNumericSeriesIndexing:\n    def time_getitem_list_like(self, index, index_structure):\n        self.s[[self.lbl]]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]"", ""indexing.NumericSeriesIndexing.time_getitem_list_like"": ""class NumericSeriesIndexing:\n    def time_getitem_list_like(self, index, index_structure):\n        self.data[[800000]]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.time_getitem_list_like,time,-0.13009123813763365
14860,func-level,"terminus-2,claude",0.9730508909665608,0.987339115176013,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing_engines.time_get_loc_near_middle,indexing_engines,"{""indexing_engines.MaskedNumericEngineIndexing.time_get_loc_near_middle"": ""class MaskedNumericEngineIndexing:\n    def time_get_loc_near_middle(self, engine_and_dtype, index_type, unique, N):\n        # searchsorted performance may be different near the middle of a range\n        #  vs near an endpoint\n        self.data.get_loc(self.key_middle)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())[::-1]\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.zeros(N * 3, dtype=dtype.lower())\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype.lower())\n                arr[N:] = np.arange(N * 2, dtype=dtype.lower())\n    \n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n            mask[-1] = True\n    \n        self.data = engine(BaseMaskedArray(arr, mask))\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]"", ""indexing_engines.NumericEngineIndexing.time_get_loc_near_middle"": ""class NumericEngineIndexing:\n    def time_get_loc_near_middle(self, engine_and_dtype, index_type, unique, N):\n        # searchsorted performance may be different near the middle of a range\n        #  vs near an endpoint\n        self.data.get_loc(self.key_middle)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)[::-1]\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.empty(N * 3, dtype=dtype)\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype)\n                arr[N:] = np.arange(N * 2, dtype=dtype)\n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype)\n    \n        self.data = engine(arr)\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]""}",indexing_engines.time_get_loc_near_middle,time,-0.010104826173587125
14862,func-level,"terminus-2,claude",1.0874673740712428,1.121376298596207,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.csv.time_convert_post,io.csv,"{""io.csv.ReadCSVCategorical.time_convert_post"": ""class ReadCSVCategorical:\n    def time_convert_post(self, engine):\n        read_csv(self.fname, engine=engine).apply(Categorical)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVCategorical:\n    def setup(self, engine):\n        N = 100000\n        group1 = [\""aaaaaaaa\"", \""bbbbbbb\"", \""cccccccc\"", \""dddddddd\"", \""eeeeeeee\""]\n        df = DataFrame(np.random.choice(group1, (N, 3)), columns=list(\""abc\""))\n        df.to_csv(self.fname, index=False)""}",io.csv.time_convert_post,time,-0.02398085185641032
14870,func-level,"terminus-2,claude",0.9865002009491602,0.9821791176867636,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.time_cut_interval,reshape,"{""reshape.Cut.time_cut_interval"": ""class Cut:\n    def time_cut_interval(self, bins):\n        # GH 27668\n        pd.cut(self.int_series, self.interval_bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))""}",reshape.time_cut_interval,time,0.003055928756999055
14864,func-level,"terminus-2,claude",1.035572857603586,1.065536048258366,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.json.time_normalize_json,io.json,"{""io.json.NormalizeJSON.time_normalize_json"": ""class NormalizeJSON:\n    def time_normalize_json(self, orient, frame):\n        json_normalize(self.data)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NormalizeJSON:\n    def setup(self, orient, frame):\n        data = {\n            \""hello\"": [\""thisisatest\"", 999898, \""mixed types\""],\n            \""nest1\"": {\""nest2\"": {\""nest3\"": \""nest3_value\"", \""nest3_int\"": 3445}},\n            \""nest1_list\"": {\""nest2\"": [\""blah\"", 32423, 546456.876, 92030234]},\n            \""hello2\"": \""string\"",\n        }\n        self.data = [data for i in range(10000)]""}",io.json.time_normalize_json,time,-0.02119037528626586
14867,func-level,"terminus-2,claude",1.0410915933272462,1.0651493160553005,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,join_merge.time_on_int32,join_merge,"{""join_merge.MergeAsof.time_on_int32"": ""class MergeAsof:\n    def time_on_int32(self, direction, tolerance):\n        merge_asof(\n            self.df1d, self.df2d, on=\""time32\"", direction=direction, tolerance=tolerance\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MergeAsof:\n    def setup(self, direction, tolerance):\n        one_count = 200000\n        two_count = 1000000\n    \n        df1 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, one_count / 20, one_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), one_count),\n                \""key2\"": np.random.randint(0, 25, one_count),\n                \""value1\"": np.random.randn(one_count),\n            }\n        )\n        df2 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, two_count / 20, two_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), two_count),\n                \""key2\"": np.random.randint(0, 25, two_count),\n                \""value2\"": np.random.randn(two_count),\n            }\n        )\n    \n        df1 = df1.sort_values(\""time\"")\n        df2 = df2.sort_values(\""time\"")\n    \n        df1[\""time32\""] = np.int32(df1.time)\n        df2[\""time32\""] = np.int32(df2.time)\n    \n        df1[\""timeu64\""] = np.uint64(df1.time)\n        df2[\""timeu64\""] = np.uint64(df2.time)\n    \n        self.df1a = df1[[\""time\"", \""value1\""]]\n        self.df2a = df2[[\""time\"", \""value2\""]]\n        self.df1b = df1[[\""time\"", \""key\"", \""value1\""]]\n        self.df2b = df2[[\""time\"", \""key\"", \""value2\""]]\n        self.df1c = df1[[\""time\"", \""key2\"", \""value1\""]]\n        self.df2c = df2[[\""time\"", \""key2\"", \""value2\""]]\n        self.df1d = df1[[\""time32\"", \""value1\""]]\n        self.df2d = df2[[\""time32\"", \""value2\""]]\n        self.df1e = df1[[\""time\"", \""key\"", \""key2\"", \""value1\""]]\n        self.df2e = df2[[\""time\"", \""key\"", \""key2\"", \""value2\""]]\n        self.df1f = df1[[\""timeu64\"", \""value1\""]]\n        self.df2f = df2[[\""timeu64\"", \""value2\""]]""}",join_merge.time_on_int32,time,-0.017013948181085068
14863,func-level,"terminus-2,claude",1.0255708562611685,1.0503761610002444,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.csv.time_read_bytescsv,io.csv,"{""io.csv.ReadCSVEngine.time_read_bytescsv"": ""class ReadCSVEngine:\n    def time_read_bytescsv(self, engine):\n        read_csv(self.data(self.BytesIO_input), engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVEngine:\n    def setup(self, engine):\n        data = [\""A,B,C,D,E\""] + ([\""1,2,3,4,5\""] * 100000)\n        self.StringIO_input = StringIO(\""\\n\"".join(data))\n        # simulate reading from file\n        self.BytesIO_input = BytesIO(self.StringIO_input.read().encode(\""utf-8\""))""}",io.csv.time_read_bytescsv,time,-0.017542648330322447
14866,func-level,"terminus-2,claude",1.023823510221133,1.029046138693322,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,join_merge.time_by_object,join_merge,"{""join_merge.MergeAsof.time_by_object"": ""class MergeAsof:\n    def time_by_object(self, direction, tolerance):\n        merge_asof(\n            self.df1b,\n            self.df2b,\n            on=\""time\"",\n            by=\""key\"",\n            direction=direction,\n            tolerance=tolerance,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MergeAsof:\n    def setup(self, direction, tolerance):\n        one_count = 200000\n        two_count = 1000000\n    \n        df1 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, one_count / 20, one_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), one_count),\n                \""key2\"": np.random.randint(0, 25, one_count),\n                \""value1\"": np.random.randn(one_count),\n            }\n        )\n        df2 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, two_count / 20, two_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), two_count),\n                \""key2\"": np.random.randint(0, 25, two_count),\n                \""value2\"": np.random.randn(two_count),\n            }\n        )\n    \n        df1 = df1.sort_values(\""time\"")\n        df2 = df2.sort_values(\""time\"")\n    \n        df1[\""time32\""] = np.int32(df1.time)\n        df2[\""time32\""] = np.int32(df2.time)\n    \n        df1[\""timeu64\""] = np.uint64(df1.time)\n        df2[\""timeu64\""] = np.uint64(df2.time)\n    \n        self.df1a = df1[[\""time\"", \""value1\""]]\n        self.df2a = df2[[\""time\"", \""value2\""]]\n        self.df1b = df1[[\""time\"", \""key\"", \""value1\""]]\n        self.df2b = df2[[\""time\"", \""key\"", \""value2\""]]\n        self.df1c = df1[[\""time\"", \""key2\"", \""value1\""]]\n        self.df2c = df2[[\""time\"", \""key2\"", \""value2\""]]\n        self.df1d = df1[[\""time32\"", \""value1\""]]\n        self.df2d = df2[[\""time32\"", \""value2\""]]\n        self.df1e = df1[[\""time\"", \""key\"", \""key2\"", \""value1\""]]\n        self.df2e = df2[[\""time\"", \""key\"", \""key2\"", \""value2\""]]\n        self.df1f = df1[[\""timeu64\"", \""value1\""]]\n        self.df2f = df2[[\""timeu64\"", \""value2\""]]""}",join_merge.time_by_object,time,-0.003693513770996476
14868,func-level,"terminus-2,claude",1.0094226647694298,1.0420834684650648,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,libs.time_is_list_like,libs,"{""libs.ScalarListLike.time_is_list_like"": ""class ScalarListLike:\n    def time_is_list_like(self, param):\n        is_list_like(param)""}",libs.time_is_list_like,time,-0.0230981638582992
14879,func-level,"terminus-2,claude",1.0135964294916813,1.024648637049011,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.time_rank,rolling,"{""rolling.Rank.time_rank"": ""class Rank:\n    def time_rank(self, constructor, window, dtype, percentile, ascending, method):\n        self.roll.rank(pct=percentile, ascending=ascending, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Rank:\n    def setup(self, constructor, window, dtype, percentile, ascending, method):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)""}",rolling.time_rank,time,-0.00781627125695179
14876,func-level,"terminus-2,claude",1.0158911939097353,1.0706396709748176,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.time_method,rolling,"{""rolling.Groupby.time_method"": ""class Groupby:\n    def time_method(self, method, window_kwargs):\n        getattr(self.groupby_window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Groupby:\n    def setup(self, method, window_kwargs):\n        N = 1000\n        window, kwargs = window_kwargs\n        df = pd.DataFrame(\n            {\n                \""A\"": [str(i) for i in range(N)] * 10,\n                \""B\"": list(range(N)) * 10,\n            }\n        )\n        if isinstance(kwargs.get(\""window\"", None), str):\n            df.index = pd.date_range(start=\""1900-01-01\"", freq=\""1min\"", periods=N * 10)\n        self.groupby_window = getattr(df.groupby(\""A\""), window)(**kwargs)"", ""rolling.Methods.time_method"": ""class Methods:\n    def time_method(self, constructor, window_kwargs, dtype, method):\n        getattr(self.window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Methods:\n    def setup(self, constructor, window_kwargs, dtype, method):\n        N = 10**5\n        window, kwargs = window_kwargs\n        arr = (100 * np.random.random(N)).astype(dtype)\n        obj = getattr(pd, constructor)(arr)\n        self.window = getattr(obj, window)(**kwargs)"", ""rolling.VariableWindowMethods.time_method"": ""class Methods:\n    def time_method(self, constructor, window_kwargs, dtype, method):\n        getattr(self.window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass VariableWindowMethods:\n    def setup(self, constructor, window, dtype, method):\n        N = 10**5\n        arr = (100 * np.random.random(N)).astype(dtype)\n        index = pd.date_range(\""2017-01-01\"", periods=N, freq=\""5s\"")\n        self.window = getattr(pd, constructor)(arr, index=index).rolling(window)""}",rolling.time_method,time,-0.03871886638266078
14878,func-level,"terminus-2,claude",1.0336586160589174,1.0647197439195957,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.time_quantile,rolling,"{""rolling.Quantile.time_quantile"": ""class Quantile:\n    def time_quantile(self, constructor, window, dtype, percentile, interpolation):\n        self.roll.quantile(percentile, interpolation=interpolation)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, constructor, window, dtype, percentile, interpolation):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)""}",rolling.time_quantile,time,-0.0219668513866183
14875,func-level,"terminus-2,claude",1.0720491588329937,1.0919853885459143,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.time_ewm,rolling,"{""rolling.EWMMethods.time_ewm"": ""class EWMMethods:\n    def time_ewm(self, constructor, kwargs_method, dtype):\n        getattr(self.ewm, self.method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass EWMMethods:\n    def setup(self, constructor, kwargs_method, dtype):\n        N = 10**5\n        kwargs, method = kwargs_method\n        arr = (100 * np.random.random(N)).astype(dtype)\n        self.method = method\n        self.ewm = getattr(pd, constructor)(arr).ewm(**kwargs)""}",rolling.time_ewm,time,-0.014099172357086734
14874,func-level,"terminus-2,claude",1.0201766673229071,1.0345564193262995,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.time_wide_to_long_big,reshape,"{""reshape.WideToLong.time_wide_to_long_big"": ""class WideToLong:\n    def time_wide_to_long_big(self):\n        wide_to_long(self.df, self.letters, i=\""id\"", j=\""year\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass WideToLong:\n    def setup(self):\n        nyrs = 20\n        nidvars = 20\n        N = 5000\n        self.letters = list(\""ABCD\"")\n        yrvars = [\n            letter + str(num)\n            for letter, num in product(self.letters, range(1, nyrs + 1))\n        ]\n        columns = [str(i) for i in range(nidvars)] + yrvars\n        self.df = DataFrame(np.random.randn(N, nidvars + len(yrvars)), columns=columns)\n        self.df[\""id\""] = self.df.index""}",reshape.time_wide_to_long_big,time,-0.01016955587227183
14881,func-level,"terminus-2,claude",1.032162152737261,1.069441903454608,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.time_func,series_methods,"{""series_methods.NanOps.time_func"": ""class NanOps:\n    def time_func(self, func, N, dtype):\n        self.func()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NanOps:\n    def setup(self, func, N, dtype):\n        if func == \""argmax\"" and dtype in {\""Int64\"", \""boolean\""}:\n            # Skip argmax for nullable int since this doesn't work yet (GH-24382)\n            raise NotImplementedError\n        self.s = Series([1] * N, dtype=dtype)\n        self.func = getattr(self.s, func)""}",series_methods.time_func,time,-0.02636474591042929
14884,func-level,"terminus-2,claude",0.9029970317607616,0.912691752252956,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.time_addition,sparse,"{""sparse.ArithmeticBlock.time_addition"": ""class ArithmeticBlock:\n    def time_addition(self, fill_value):\n        self.arr1 + self.arr2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )""}",sparse.time_addition,time,-0.006856237971848891
14869,func-level,"terminus-2,claude",0.9558450517631384,0.9662950819021658,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,multiindex_object.time_is_monotonic,multiindex_object,"{""multiindex_object.Integer.time_is_monotonic"": ""class Integer:\n    def time_is_monotonic(self):\n        self.mi_int.is_monotonic_increasing\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Integer:\n    def setup(self):\n        self.mi_int = MultiIndex.from_product(\n            [np.arange(1000), np.arange(1000)], names=[\""one\"", \""two\""]\n        )\n        self.obj_index = np.array(\n            [\n                (0, 10),\n                (0, 11),\n                (0, 12),\n                (0, 13),\n                (0, 14),\n                (0, 15),\n                (0, 16),\n                (0, 17),\n                (0, 18),\n                (0, 19),\n            ],\n            dtype=object,\n        )\n        self.other_mi_many_mismatches = MultiIndex.from_tuples(\n            [\n                (-7, 41),\n                (-2, 3),\n                (-0.7, 5),\n                (0, 0),\n                (0, 1.5),\n                (0, 340),\n                (0, 1001),\n                (1, -4),\n                (1, 20),\n                (1, 1040),\n                (432, -5),\n                (432, 17),\n                (439, 165.5),\n                (998, -4),\n                (998, 24065),\n                (999, 865.2),\n                (999, 1000),\n                (1045, -843),\n            ]\n        )""}",multiindex_object.time_is_monotonic,time,-0.00739040321006185
14871,func-level,"terminus-2,claude",1.1272804856936771,1.1568802304914567,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.time_cut_timedelta,reshape,"{""reshape.Cut.time_cut_timedelta"": ""class Cut:\n    def time_cut_timedelta(self, bins):\n        pd.cut(self.timedelta_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))""}",reshape.time_cut_timedelta,time,-0.020933341441145406
14873,func-level,"terminus-2,claude",1.1209021120866982,1.1395950020018186,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.time_qcut_datetime,reshape,"{""reshape.Cut.time_qcut_datetime"": ""class Cut:\n    def time_qcut_datetime(self, bins):\n        pd.qcut(self.datetime_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))""}",reshape.time_qcut_datetime,time,-0.01321986556939209
14882,func-level,"terminus-2,claude",1.065313482244743,1.1004792458334434,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.time_searchsorted,series_methods,"{""series_methods.SearchSorted.time_searchsorted"": ""class SearchSorted:\n    def time_searchsorted(self, dtype):\n        key = \""2\"" if dtype == \""str\"" else 2\n        self.s.searchsorted(key)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SearchSorted:\n    def setup(self, dtype):\n        N = 10**5\n        data = np.array([1] * N + [2] * N + [3] * N).astype(dtype)\n        self.s = Series(data)""}",series_methods.time_searchsorted,time,-0.02486970550827467
14877,func-level,"terminus-2,claude",1.038201593307916,1.056450567090326,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.time_pairwise,rolling,"{""rolling.Pairwise.time_pairwise"": ""class Pairwise:\n    def time_pairwise(self, kwargs_window, method, pairwise):\n        getattr(self.window, method)(self.df, pairwise=pairwise)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Pairwise:\n    def setup(self, kwargs_window, method, pairwise):\n        N = 10**4\n        n_groups = 20\n        kwargs, window = kwargs_window\n        groups = [i for _ in range(N // n_groups) for i in range(n_groups)]\n        arr = np.random.random(N)\n        self.df = pd.DataFrame(arr)\n        self.window = getattr(self.df, window)(**kwargs)\n        self.window_group = getattr(\n            pd.DataFrame({\""A\"": groups, \""B\"": arr}).groupby(\""A\""), window\n        )(**kwargs)""}",rolling.time_pairwise,time,-0.012905922052623664
14880,func-level,"terminus-2,claude",1.031638733452637,1.0514397522446848,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.time_rolling,rolling,"{""rolling.Apply.time_rolling"": ""class Apply:\n    def time_rolling(self, constructor, window, dtype, function, raw):\n        self.roll.apply(function, raw=raw)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, constructor, window, dtype, function, raw):\n        N = 10**3\n        arr = (100 * np.random.random(N)).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)"", ""rolling.ForwardWindowMethods.time_rolling"": ""class ForwardWindowMethods:\n    def time_rolling(self, constructor, window_size, dtype, method):\n        getattr(self.roll, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ForwardWindowMethods:\n    def setup(self, constructor, window_size, dtype, method):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=window_size)\n        self.roll = getattr(pd, constructor)(arr).rolling(window=indexer)""}",rolling.time_rolling,time,-0.01400354935788393
14885,func-level,"terminus-2,claude",0.901870976407824,0.91583897802989,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.time_divide,sparse,"{""sparse.Arithmetic.time_divide"": ""class Arithmetic:\n    def time_divide(self, dense_proportion, fill_value):\n        self.array1 / self.array2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)""}",sparse.time_divide,time,-0.009878360411644992
14872,func-level,"terminus-2,claude",0.9920469880372312,1.0221675206886116,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.time_explode,reshape,"{""reshape.Explode.time_explode"": ""class Explode:\n    def time_explode(self, n_rows, max_list_length):\n        self.series.explode()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Explode:\n    def setup(self, n_rows, max_list_length):\n        data = [np.arange(np.random.randint(max_list_length)) for _ in range(n_rows)]\n        self.series = pd.Series(data)""}",reshape.time_explode,time,-0.021301649682730107
14891,func-level,"terminus-2,claude",1.2990149339732082,1.3086013614510008,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.time_lower,strings,"{""strings.Methods.time_lower"": ""class Methods:\n    def time_lower(self, dtype):\n        self.s.str.lower()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.time_lower,time,-0.006779651681607208
14888,func-level,"terminus-2,claude",0.9810111873552728,1.0204791059132996,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strftime.time_frame_offset_repr,strftime,"{""strftime.BusinessHourStrftime.time_frame_offset_repr"": ""class BusinessHourStrftime:\n    def time_frame_offset_repr(self, obs):\n        self.data[\""off\""].apply(repr)\n\n    def setup(self, obs):\n        self.data = pd.DataFrame(\n            {\n                \""off\"": [offsets.BusinessHour()] * obs,\n            }\n        )""}",strftime.time_frame_offset_repr,time,-0.027912247919396595
14887,func-level,"terminus-2,claude",0.898652079323169,0.9104927395620972,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.time_make_union,sparse,"{""sparse.Arithmetic.time_make_union"": ""class Arithmetic:\n    def time_make_union(self, dense_proportion, fill_value):\n        self.array1.sp_index.make_union(self.array2.sp_index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)"", ""sparse.ArithmeticBlock.time_make_union"": ""class ArithmeticBlock:\n    def time_make_union(self, fill_value):\n        self.arr1.sp_index.make_union(self.arr2.sp_index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )""}",sparse.time_make_union,time,-0.008373875699383422
14892,func-level,"terminus-2,claude",1.3072067435402686,1.3596393751609956,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.time_upper,strings,"{""strings.Methods.time_upper"": ""class Methods:\n    def time_upper(self, dtype):\n        self.s.str.upper()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.time_upper,time,-0.03708106903870374
14889,func-level,"terminus-2,claude",1.0363817620577986,1.037615166606883,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.time_endswith,strings,"{""strings.Methods.time_endswith"": ""class Methods:\n    def time_endswith(self, dtype):\n        self.s.str.endswith(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.time_endswith,time,-0.0008722804448969389
14883,func-level,"terminus-2,claude",0.907003308295164,0.9048225669589512,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.time_add,sparse,"{""sparse.Arithmetic.time_add"": ""class Arithmetic:\n    def time_add(self, dense_proportion, fill_value):\n        self.array1 + self.array2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)""}",sparse.time_add,time,0.001542249884167405
14898,func-level,"terminus-2,claude",0.9939602532093392,0.9749632137875638,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.fields.time_get_date_field,tslibs.fields,"{""tslibs.fields.TimeGetDateField.time_get_date_field"": ""class TimeGetDateField:\n    def time_get_date_field(self, size, field):\n        get_date_field(self.i8data, field)\n\n    def setup(self, size, field):\n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.fields.time_get_date_field,time,0.013434964230392817
14890,func-level,"terminus-2,claude",1.008567272346436,1.0198460204179998,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.time_get_dummies,strings,"{""strings.Dummies.time_get_dummies"": ""class Dummies:\n    def time_get_dummies(self, dtype):\n        self.s.str.get_dummies(\""|\"")\n\n    def setup(self, dtype):\n        super().setup(dtype)\n        self.s = self.s.str.join(\""|\"")""}",strings.time_get_dummies,time,-0.00797648378469852
14897,func-level,"terminus-2,claude",0.9863214042910328,0.9852909091765004,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.time_to_date,timeseries,"{""timeseries.DatetimeIndex.time_to_date"": ""class DatetimeIndex:\n    def time_to_date(self, index_type):\n        self.index.date\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]""}",timeseries.time_to_date,time,0.0007287801375759182
14886,func-level,"terminus-2,claude",0.9082765254154672,0.9086828083158498,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.time_division,sparse,"{""sparse.ArithmeticBlock.time_division"": ""class ArithmeticBlock:\n    def time_division(self, fill_value):\n        self.arr1 / self.arr2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )""}",sparse.time_division,time,-0.0002873287838632121
14900,func-level,"terminus-2,claude",0.965368946532643,0.964714718516657,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.normalize.time_is_date_array_normalized,tslibs.normalize,"{""tslibs.normalize.Normalize.time_is_date_array_normalized"": ""class Normalize:\n    def time_is_date_array_normalized(self, size, tz):\n        # TODO: cases with different levels of short-circuiting\n        # 10 i.e. NPY_FR_ns\n        is_date_array_normalized(self.i8data, tz, 10)\n\n    def setup(self, size, tz):\n        # use an array that will have is_date_array_normalized give True,\n        #  so we do not short-circuit early.\n        dti = pd.date_range(\""2016-01-01\"", periods=10, tz=tz).repeat(size // 10)\n        self.i8data = dti.asi8\n    \n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError""}",tslibs.normalize.time_is_date_array_normalized,time,0.00046267893634089447
14894,func-level,"terminus-2,claude",0.9578109586223608,0.9594607562437548,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.time_dt_accessor_month_name,timeseries,"{""timeseries.DatetimeAccessor.time_dt_accessor_month_name"": ""class DatetimeAccessor:\n    def time_dt_accessor_month_name(self, tz):\n        self.series.dt.month_name()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeAccessor:\n    def setup(self, tz):\n        N = 100000\n        self.series = Series(date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"", tz=tz))""}",timeseries.time_dt_accessor_month_name,time,-0.0011667592796280452
14902,func-level,"terminus-2,claude",1.037556228698402,1.0460564224178994,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.time_add_10,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_add_10"": ""class OffestDatetimeArithmetic:\n    def time_add_10(self, offset):\n        self.date + (10 * offset)\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.time_add_10,time,-0.006011452418315069
14896,func-level,"terminus-2,claude",0.978481232734824,0.9911992258081832,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.time_timeseries_is_month_start,timeseries,"{""timeseries.DatetimeIndex.time_timeseries_is_month_start"": ""class DatetimeIndex:\n    def time_timeseries_is_month_start(self, index_type):\n        self.index.is_month_start\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]""}",timeseries.time_timeseries_is_month_start,time,-0.00899433739275756
14895,func-level,"terminus-2,claude",1.0789464905606996,1.0714083894064304,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.time_infer_dst,timeseries,"{""timeseries.TzLocalize.time_infer_dst"": ""class TzLocalize:\n    def time_infer_dst(self, tz):\n        self.index.tz_localize(tz, ambiguous=\""infer\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TzLocalize:\n    def setup(self, tz):\n        dst_rng = date_range(\n            start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n        )\n        self.index = date_range(start=\""10/29/2000\"", end=\""10/29/2000 00:59:59\"", freq=\""S\"")\n        self.index = self.index.append(dst_rng)\n        self.index = self.index.append(dst_rng)\n        self.index = self.index.append(\n            date_range(start=\""10/29/2000 2:00:00\"", end=\""10/29/2000 3:00:00\"", freq=\""S\"")\n        )""}",timeseries.time_infer_dst,time,0.005331047492411052
14904,func-level,"terminus-2,claude",0.9846621274957932,0.9637683780969988,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.time_on_offset,tslibs.offsets,"{""tslibs.offsets.OnOffset.time_on_offset"": ""class OnOffset:\n    def time_on_offset(self, offset):\n        for date in self.dates:\n            offset.is_on_offset(date)\n\n    def setup(self, offset):\n        self.dates = [\n            datetime(2016, m, d)\n            for m in [10, 11, 12]\n            for d in [1, 2, 3, 28, 29, 30, 31]\n            if not (m == 11 and d == 31)\n        ]""}",tslibs.offsets.time_on_offset,time,0.01477634328061839
14899,func-level,"terminus-2,claude",1.0175278213569792,1.0255275966642323,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.fields.time_get_start_end_field,tslibs.fields,"{""tslibs.fields.TimeGetStartEndField.time_get_start_end_field"": ""class TimeGetStartEndField:\n    def time_get_start_end_field(self, size, side, period, freqstr, month_kw):\n        get_start_end_field(self.i8data, self.attrname, freqstr, month_kw=month_kw)\n\n    def setup(self, size, side, period, freqstr, month_kw):\n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr\n    \n        self.attrname = f\""is_{period}_{side}\""""}",tslibs.fields.time_get_start_end_field,time,-0.005657549722244098
14907,func-level,"terminus-2,claude",0.976999345527098,1.0187289916107118,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.time_asfreq,tslibs.period,"{""tslibs.period.PeriodUnaryMethods.time_asfreq"": ""class PeriodUnaryMethods:\n    def time_asfreq(self, freq):\n        self.per.asfreq(\""A\"")\n\n    def setup(self, freq):\n        self.per = Period(\""2012-06-01\"", freq=freq)""}",tslibs.period.time_asfreq,time,-0.029511772336360573
14893,func-level,"terminus-2,claude",1.0340796936974983,1.0312078855701563,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.time_wrap,strings,"{""strings.Methods.time_wrap"": ""class Methods:\n    def time_wrap(self, dtype):\n        self.s.str.wrap(10)\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.time_wrap,time,0.0020309817025049516
14906,func-level,"terminus-2,claude",1.0476579684743583,1.0596632947327829,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.time_subtract_10,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_subtract_10"": ""class OffestDatetimeArithmetic:\n    def time_subtract_10(self, offset):\n        self.date - (10 * offset)\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.time_subtract_10,time,-0.008490329744288927
14912,func-level,"terminus-2,claude",0.9991201711043596,1.0436114744293197,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_ceil,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_ceil"": ""class TimestampOps:\n    def time_ceil(self, tz):\n        self.ts.ceil(\""5T\"")\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.time_ceil,time,-0.031464853836605446
14905,func-level,"terminus-2,claude",1.0343418984428585,1.0701985069308426,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.time_subtract,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_subtract"": ""class OffestDatetimeArithmetic:\n    def time_subtract(self, offset):\n        self.date - offset\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.time_subtract,time,-0.025358280401686077
14911,func-level,"terminus-2,claude",1.036665999451161,1.0156490510826353,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timedelta.time_from_datetime_timedelta,tslibs.timedelta,"{""tslibs.timedelta.TimedeltaConstructor.time_from_datetime_timedelta"": ""class TimedeltaConstructor:\n    def time_from_datetime_timedelta(self):\n        Timedelta(self.dttimedelta)\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")""}",tslibs.timedelta.time_from_datetime_timedelta,time,0.014863471264869615
14909,func-level,"terminus-2,claude",0.9681999031708676,0.9754416429948614,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.time_periodarray_to_dt64arr,tslibs.period,"{""tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr"": ""class TimePeriodArrToDT64Arr:\n    def time_periodarray_to_dt64arr(self, size, freq):\n        periodarr_to_dt64arr(self.i8values, freq)\n\n    def setup(self, size, freq):\n        arr = np.arange(10, dtype=\""i8\"").repeat(size // 10)\n        self.i8values = arr""}",tslibs.period.time_periodarray_to_dt64arr,time,-0.005121456735497728
14908,func-level,"terminus-2,claude",0.9770122721104428,0.9966201154783566,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.time_dt64arr_to_periodarr,tslibs.period,"{""tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr"": ""class TimeDT64ArrToPeriodArr:\n    def time_dt64arr_to_periodarr(self, size, freq, tz):\n        dt64arr_to_periodarr(self.i8values, freq, tz)\n\n    def setup(self, size, freq, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.arange(10, dtype=\""i8\"").repeat(size // 10)\n        self.i8values = arr""}",tslibs.period.time_dt64arr_to_periodarr,time,-0.013866933074903656
14901,func-level,"terminus-2,claude",1.0258982717045702,1.0497694699987314,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.time_add,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_add"": ""class OffestDatetimeArithmetic:\n    def time_add(self, offset):\n        self.date + offset\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.time_add,time,-0.016882035568713784
14919,func-level,"terminus-2,claude",1.000635883812254,1.040393835731974,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_normalize,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_normalize"": ""class TimestampOps:\n    def time_normalize(self, tz):\n        self.ts.normalize()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.time_normalize,time,-0.02811736345100423
14916,func-level,"terminus-2,claude",0.99044397149246,1.0073551653987558,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_is_quarter_start,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_is_quarter_start"": ""class TimestampProperties:\n    def time_is_quarter_start(self, tz):\n        self.ts.is_quarter_start\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.time_is_quarter_start,time,-0.011959825959190812
14914,func-level,"terminus-2,claude",0.976740856511139,1.0250200716088496,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_floor,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_floor"": ""class TimestampOps:\n    def time_floor(self, tz):\n        self.ts.floor(\""5T\"")\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.time_floor,time,-0.03414371647645729
14915,func-level,"terminus-2,claude",1.0333647046128065,1.01738764873653,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_from_npdatetime64,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_from_npdatetime64"": ""class TimestampConstruction:\n    def time_from_npdatetime64(self):\n        Timestamp(self.npdatetime64)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.time_from_npdatetime64,time,0.011299190860167188
14920,func-level,"terminus-2,claude",1.0485715149287966,1.0390043640338635,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_parse_iso8601_no_tz,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_parse_iso8601_no_tz"": ""class TimestampConstruction:\n    def time_parse_iso8601_no_tz(self):\n        Timestamp(\""2017-08-25 08:16:14\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.time_parse_iso8601_no_tz,time,0.006766019020461879
14910,func-level,"terminus-2,claude",0.9740867278162968,0.9958619199845872,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.resolution.time_get_resolution,tslibs.resolution,"{""tslibs.resolution.TimeResolution.time_get_resolution"": ""class TimeResolution:\n    def time_get_resolution(self, unit, size, tz):\n        get_resolution(self.i8data, tz)\n\n    def setup(self, unit, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        arr = arr.view(f\""M8[{unit}]\"").astype(\""M8[ns]\"").view(\""i8\"")\n        self.i8data = arr""}",tslibs.resolution.time_get_resolution,time,-0.015399711575877214
14917,func-level,"terminus-2,claude",0.9916259670054186,1.0090970559195138,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_microsecond,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_microsecond"": ""class TimestampProperties:\n    def time_microsecond(self, tz):\n        self.ts.microsecond\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.time_microsecond,time,-0.01235579131124131
14903,func-level,"terminus-2,claude",1.0657905849412628,1.0665039317249991,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.time_add_np_dt64,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_add_np_dt64"": ""class OffestDatetimeArithmetic:\n    def time_add_np_dt64(self, offset):\n        offset + self.dt64\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.time_add_np_dt64,time,-0.0005044885316381593
14924,func-level,"terminus-2,claude",0.9874857971476948,0.9815589348407184,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_weekday_name,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_weekday_name"": ""class TimestampProperties:\n    def time_weekday_name(self, tz):\n        self.ts.day_name()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.time_weekday_name,time,0.004191557501397713
14921,func-level,"terminus-2,claude",0.9803632786892996,0.9834207220656008,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_replace_tz,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_replace_tz"": ""class TimestampOps:\n    def time_replace_tz(self, tz):\n        self.ts.replace(tzinfo=pytz.timezone(\""US/Eastern\""))\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.time_replace_tz,time,-0.00216226547121726
14913,func-level,"terminus-2,claude",0.9859417141392418,1.0088496122333308,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_days_in_month,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_days_in_month"": ""class TimestampProperties:\n    def time_days_in_month(self, tz):\n        self.ts.days_in_month\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.time_days_in_month,time,-0.01620077658705021
14918,func-level,"terminus-2,claude",0.982610059394668,0.9791569957625458,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_month_name,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_month_name"": ""class TimestampProperties:\n    def time_month_name(self, tz):\n        self.ts.month_name()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.time_month_name,time,0.002442053488063767
14922,func-level,"terminus-2,claude",1.007877364357194,1.0141674302365769,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_to_julian_date,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_to_julian_date"": ""class TimestampOps:\n    def time_to_julian_date(self, tz):\n        self.ts.to_julian_date()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.time_to_julian_date,time,-0.004448419999563469
14925,func-level,"terminus-2,claude",0.98887136318111,1.018084972226594,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.tslib.time_ints_to_pydatetime,tslibs.tslib,"{""tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime"": ""class TimeIntsToPydatetime:\n    def time_ints_to_pydatetime(self, box, size, tz):\n        ints_to_pydatetime(self.i8data, tz, box=box)\n\n    def setup(self, box, size, tz):\n        if box == \""date\"" and tz is not None:\n            # tz is ignored, so avoid running redundant benchmarks\n            raise NotImplementedError  # skip benchmark\n        if size == 10**6 and tz is _tzs[-1]:\n            # This is cumbersomely-slow, so skip to trim runtime\n            raise NotImplementedError  # skip benchmark\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.tslib.time_ints_to_pydatetime,time,-0.020660260993977435
14923,func-level,"terminus-2,claude",0.993524248271918,0.97861892483466,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_tz_localize,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_tz_localize"": ""class TimestampOps:\n    def time_tz_localize(self, tz):\n        if self.ts.tz is None:\n            self.ts.tz_localize(tz)\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.time_tz_localize,time,0.010541247126773717
14926,func-level,"terminus-2,claude",0.9819732726671584,0.992241436005673,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.tz_convert.time_tz_convert_from_utc,tslibs.tz_convert,"{""tslibs.tz_convert.TimeTZConvert.time_tz_convert_from_utc"": ""class TimeTZConvert:\n    def time_tz_convert_from_utc(self, size, tz):\n        # effectively:\n        #  dti = DatetimeIndex(self.i8data, tz=tz)\n        #  dti.tz_localize(None)\n        if old_sig:\n            tz_convert_from_utc(self.i8data, UTC, tz)\n        else:\n            tz_convert_from_utc(self.i8data, tz)\n\n    def setup(self, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.tz_convert.time_tz_convert_from_utc,time,-0.00726178453926072
14932,func-level,"terminus-2,gpt-5",1.0677645872985508,1.0572102104876364,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.time_isin_empty,algos.isin,"{""algos.isin.IsIn.time_isin_empty"": ""class IsIn:\n    def time_isin_empty(self, dtype):\n        self.series.isin([])\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)""}",algos.isin.time_isin_empty,time,0.007464198593291672
14929,func-level,"terminus-2,gpt-5",1.1345145036953457,1.123615183139807,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,algorithms.time_quantile,algorithms,"{""algorithms.Quantile.time_quantile"": ""class Quantile:\n    def time_quantile(self, quantile, interpolation, dtype):\n        self.idx.quantile(quantile, interpolation=interpolation)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, quantile, interpolation, dtype):\n        N = 10**5\n        data = {\n            \""int\"": np.arange(N),\n            \""uint\"": np.arange(N).astype(np.uint64),\n            \""float\"": np.random.randn(N),\n        }\n        self.idx = pd.Series(data[dtype].repeat(5))""}",algorithms.time_quantile,time,0.007708147493308891
14930,func-level,"terminus-2,gpt-5",1.1247224406520675,1.173784775878192,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.time_isin,algos.isin,"{""algos.isin.IsIn.time_isin"": ""class IsIn:\n    def time_isin(self, dtype):\n        self.series.isin(self.values)\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)"", ""algos.isin.IsInForObjects.time_isin"": ""class IsInForObjects:\n    def time_isin(self, series_type, vals_type):\n        self.series.isin(self.values)\n\n    def setup(self, series_type, vals_type):\n        N_many = 10**5\n    \n        if series_type == \""nans\"":\n            ser_vals = np.full(10**4, np.nan)\n        elif series_type == \""short\"":\n            ser_vals = np.arange(2)\n        elif series_type == \""long\"":\n            ser_vals = np.arange(N_many)\n        elif series_type == \""long_floats\"":\n            ser_vals = np.arange(N_many, dtype=np.float_)\n    \n        self.series = Series(ser_vals).astype(object)\n    \n        if vals_type == \""nans\"":\n            values = np.full(10**4, np.nan)\n        elif vals_type == \""short\"":\n            values = np.arange(2)\n        elif vals_type == \""long\"":\n            values = np.arange(N_many)\n        elif vals_type == \""long_floats\"":\n            values = np.arange(N_many, dtype=np.float_)\n    \n        self.values = values.astype(object)"", ""algos.isin.IsInLongSeriesLookUpDominates.time_isin"": ""class IsInLongSeriesLookUpDominates:\n    def time_isin(self, dtypes, MaxNumber, series_type):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, MaxNumber, series_type):\n        N = 10**7\n    \n        if series_type == \""random_hits\"":\n            array = np.random.randint(0, MaxNumber, N)\n        if series_type == \""random_misses\"":\n            array = np.random.randint(0, MaxNumber, N) + MaxNumber\n        if series_type == \""monotone_hits\"":\n            array = np.repeat(np.arange(MaxNumber), N // MaxNumber)\n        if series_type == \""monotone_misses\"":\n            array = np.arange(N) + MaxNumber\n    \n        self.series = Series(array).astype(dtype)\n    \n        self.values = np.arange(MaxNumber).astype(dtype.lower())"", ""algos.isin.IsInLongSeriesValuesDominate.time_isin"": ""class IsInLongSeriesValuesDominate:\n    def time_isin(self, dtypes, series_type):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, series_type):\n        N = 10**7\n    \n        if series_type == \""random\"":\n            vals = np.random.randint(0, 10 * N, N)\n        if series_type == \""monotone\"":\n            vals = np.arange(N)\n    \n        self.values = vals.astype(dtype.lower())\n        M = 10**6 + 1\n        self.series = Series(np.arange(M)).astype(dtype)"", ""algos.isin.IsinAlmostFullWithRandomInt.time_isin"": ""class IsinAlmostFullWithRandomInt:\n    def time_isin(self, dtype, exponent, title):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, exponent, title):\n        M = 3 * 2 ** (exponent - 2)\n        # 0.77-the maximal share of occupied buckets\n        self.series = Series(np.random.randint(0, M, M)).astype(dtype)\n    \n        values = np.random.randint(0, M, M).astype(dtype)\n        if title == \""inside\"":\n            self.values = values\n        elif title == \""outside\"":\n            self.values = values + M\n        else:\n            raise ValueError(title)"", ""algos.isin.IsinWithArange.time_isin"": ""class IsinWithArange:\n    def time_isin(self, dtype, M, offset_factor):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, M, offset_factor):\n        offset = int(M * offset_factor)\n        tmp = Series(np.random.randint(offset, M + offset, 10**6))\n        self.series = tmp.astype(dtype)\n        self.values = np.arange(M).astype(dtype)""}",algos.isin.time_isin,time,-0.03469754966486887
14927,func-level,"terminus-2,gpt-5",1.3530931976211034,1.0265993124974522,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,algorithms.time_duplicated,algorithms,"{""algorithms.Duplicated.time_duplicated"": ""class Duplicated:\n    def time_duplicated(self, unique, keep, dtype):\n        self.idx.duplicated(keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Duplicated:\n    def setup(self, unique, keep, dtype):\n        N = 10**5\n        data = {\n            \""int\"": pd.Index(np.arange(N), dtype=\""int64\""),\n            \""uint\"": pd.Index(np.arange(N), dtype=\""uint64\""),\n            \""float\"": pd.Index(np.random.randn(N), dtype=\""float64\""),\n            \""string\"": tm.makeStringIndex(N),\n            \""datetime64[ns]\"": pd.date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n            \""datetime64[ns, tz]\"": pd.date_range(\n                \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n            ),\n        }[dtype]\n        if not unique:\n            data = data.repeat(5)\n        self.idx = data\n        # cache is_unique\n        self.idx.is_unique"", ""algorithms.DuplicatedMaskedArray.time_duplicated"": ""class DuplicatedMaskedArray:\n    def time_duplicated(self, unique, keep, dtype):\n        self.ser.duplicated(keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DuplicatedMaskedArray:\n    def setup(self, unique, keep, dtype):\n        N = 10**5\n        data = pd.Series(np.arange(N), dtype=dtype)\n        data[list(range(1, N, 100))] = pd.NA\n        if not unique:\n            data = data.repeat(5)\n        self.ser = data\n        # cache is_unique\n        self.ser.is_unique""}",algorithms.time_duplicated,time,0.23090090885689618
14938,func-level,"terminus-2,gpt-5",0.980583029376074,0.948302164804101,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,array.time_to_numpy,array,"{""array.ArrowExtensionArray.time_to_numpy"": ""class ArrowExtensionArray:\n    def time_to_numpy(self, dtype, hasna):\n        self.arr.to_numpy()\n\n    def setup(self, dtype, hasna):\n        N = 100_000\n        if dtype == \""boolean[pyarrow]\"":\n            data = np.random.choice([True, False], N, replace=True)\n        elif dtype == \""float64[pyarrow]\"":\n            data = np.random.randn(N)\n        elif dtype == \""int64[pyarrow]\"":\n            data = np.arange(N)\n        elif dtype == \""string[pyarrow]\"":\n            data = tm.rands_array(10, N)\n        elif dtype == \""timestamp[ns][pyarrow]\"":\n            data = pd.date_range(\""2000-01-01\"", freq=\""s\"", periods=N)\n        else:\n            raise NotImplementedError\n    \n        arr = pd.array(data, dtype=dtype)\n        if hasna:\n            arr[::2] = pd.NA\n        self.arr = arr""}",array.time_to_numpy,time,0.02282946575104172
14934,func-level,"terminus-2,gpt-5",0.99927294112674,0.969296484273031,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.time_add_dti_offset,arithmetic,"{""arithmetic.OffsetArrayArithmetic.time_add_dti_offset"": ""class OffsetArrayArithmetic:\n    def time_add_dti_offset(self, offset):\n        with warnings.catch_warnings(record=True):\n            self.rng + offset\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass OffsetArrayArithmetic:\n    def setup(self, offset):\n        N = 10000\n        rng = date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"")\n        self.rng = rng\n        self.ser = Series(rng)""}",arithmetic.time_add_dti_offset,time,0.021199757322283658
14928,func-level,"terminus-2,gpt-5",1.2250480322656268,1.1702881536723302,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,algorithms.time_factorize,algorithms,"{""algorithms.Factorize.time_factorize"": ""class Factorize:\n    def time_factorize(self, unique, sort, dtype):\n        pd.factorize(self.data, sort=sort)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Factorize:\n    def setup(self, unique, sort, dtype):\n        N = 10**5\n        string_index = tm.makeStringIndex(N)\n        string_arrow = None\n        if dtype == \""string[pyarrow]\"":\n            try:\n                string_arrow = pd.array(string_index, dtype=\""string[pyarrow]\"")\n            except ImportError:\n                raise NotImplementedError\n    \n        data = {\n            \""int\"": pd.Index(np.arange(N), dtype=\""int64\""),\n            \""uint\"": pd.Index(np.arange(N), dtype=\""uint64\""),\n            \""float\"": pd.Index(np.random.randn(N), dtype=\""float64\""),\n            \""object\"": string_index,\n            \""datetime64[ns]\"": pd.date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n            \""datetime64[ns, tz]\"": pd.date_range(\n                \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n            ),\n            \""Int64\"": pd.array(np.arange(N), dtype=\""Int64\""),\n            \""boolean\"": pd.array(np.random.randint(0, 2, N), dtype=\""boolean\""),\n            \""string[pyarrow]\"": string_arrow,\n        }[dtype]\n        if not unique:\n            data = data.repeat(5)\n        self.data = data""}",algorithms.time_factorize,time,0.03872692969822954
14936,func-level,"terminus-2,gpt-5",1.0634911940872729,0.9570005702528782,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.time_frame_op_with_scalar,arithmetic,"{""arithmetic.IntFrameWithScalar.time_frame_op_with_scalar"": ""class IntFrameWithScalar:\n    def time_frame_op_with_scalar(self, dtype, scalar, op):\n        op(self.df, scalar)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IntFrameWithScalar:\n    def setup(self, dtype, scalar, op):\n        arr = np.random.randn(20000, 100)\n        self.df = DataFrame(arr.astype(dtype))""}",arithmetic.time_frame_op_with_scalar,time,0.07531161515869496
14931,func-level,"terminus-2,gpt-5",1.04541781423117,1.0618129829695413,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.time_isin_categorical,algos.isin,"{""algos.isin.IsIn.time_isin_categorical"": ""class IsIn:\n    def time_isin_categorical(self, dtype):\n        self.series.isin(self.cat_values)\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)""}",algos.isin.time_isin_categorical,time,-0.011594885953586431
14937,func-level,"terminus-2,gpt-5",0.9777050864705156,0.981116408258753,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,array.time_from_bool_array,array,"{""array.BooleanArray.time_from_bool_array"": ""class BooleanArray:\n    def time_from_bool_array(self):\n        pd.array(self.values_bool, dtype=\""boolean\"")\n\n    def setup(self):\n        self.values_bool = np.array([True, False, True, False])\n        self.values_float = np.array([1.0, 0.0, 1.0, 0.0])\n        self.values_integer = np.array([1, 0, 1, 0])\n        self.values_integer_like = [1, 0, 1, 0]\n        self.data = np.array([True, False, True, False])\n        self.mask = np.array([False, False, True, False])""}",array.time_from_bool_array,time,-0.0024125330892768334
14941,func-level,"terminus-2,gpt-5",1.0997558588050889,0.901924432099614,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,ctors.time_series_constructor,ctors,"{""ctors.SeriesConstructors.time_series_constructor"": ""class SeriesConstructors:\n    def time_series_constructor(self, data_fmt, with_index, dtype):\n        Series(self.data, index=self.index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesConstructors:\n    def setup(self, data_fmt, with_index, dtype):\n        if data_fmt in (gen_of_str, gen_of_tuples) and with_index:\n            raise NotImplementedError(\n                \""Series constructors do not support using generators with indexes\""\n            )\n        N = 10**4\n        if dtype == \""float\"":\n            arr = np.random.randn(N)\n        else:\n            arr = np.arange(N)\n        self.data = data_fmt(arr)\n        self.index = np.arange(N) if with_index else None""}",ctors.time_series_constructor,time,0.13990907122027924
14933,func-level,"terminus-2,gpt-5",1.1104912903573572,1.1108688007180527,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.time_isin_mismatched_dtype,algos.isin,"{""algos.isin.IsIn.time_isin_mismatched_dtype"": ""class IsIn:\n    def time_isin_mismatched_dtype(self, dtype):\n        self.series.isin(self.mismatched)\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)""}",algos.isin.time_isin_mismatched_dtype,time,-0.0002669804531085957
14940,func-level,"terminus-2,gpt-5",0.9751943239358802,0.9825157394271008,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.time_getitem_scalar,categoricals,"{""categoricals.CategoricalSlicing.time_getitem_scalar"": ""class CategoricalSlicing:\n    def time_getitem_scalar(self, index):\n        self.data[self.scalar]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalSlicing:\n    def setup(self, index):\n        N = 10**6\n        categories = [\""a\"", \""b\"", \""c\""]\n        values = [0] * N + [1] * N + [2] * N\n        if index == \""monotonic_incr\"":\n            self.data = pd.Categorical.from_codes(values, categories=categories)\n        elif index == \""monotonic_decr\"":\n            self.data = pd.Categorical.from_codes(\n                list(reversed(values)), categories=categories\n            )\n        elif index == \""non_monotonic\"":\n            self.data = pd.Categorical.from_codes([0, 1, 2] * N, categories=categories)\n        else:\n            raise ValueError(f\""Invalid index param: {index}\"")\n    \n        self.scalar = 10000\n        self.list = list(range(10000))\n        self.cat_scalar = \""b\""""}",categoricals.time_getitem_scalar,time,-0.005177804449236607
14939,func-level,"terminus-2,gpt-5",0.9856389943622818,0.9690018695661616,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,boolean.time_and_scalar,boolean,"{""boolean.TimeLogicalOps.time_and_scalar"": ""class TimeLogicalOps:\n    def time_and_scalar(self):\n        self.left & True\n        self.left & False\n\n    def setup(self):\n        N = 10_000\n        left, right, lmask, rmask = np.random.randint(0, 2, size=(4, N)).astype(\""bool\"")\n        self.left = pd.arrays.BooleanArray(left, lmask)\n        self.right = pd.arrays.BooleanArray(right, rmask)""}",boolean.time_and_scalar,time,0.011766000563026957
14942,func-level,"terminus-2,gpt-5",1.0155700319684131,0.9843871123238844,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.time_pandas_dtype_invalid,dtypes,"{""dtypes.DtypesInvalid.time_pandas_dtype_invalid"": ""class DtypesInvalid:\n    def time_pandas_dtype_invalid(self, dtype):\n        try:\n            pandas_dtype(self.data_dict[dtype])\n        except TypeError:\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)""}",dtypes.time_pandas_dtype_invalid,time,0.022052984189907133
14935,func-level,"terminus-2,gpt-5",1.122802610650385,0.7970819164535795,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.time_divide,arithmetic,"{""arithmetic.NumericInferOps.time_divide"": ""class NumericInferOps:\n    def time_divide(self, dtype):\n        self.df[\""A\""] / self.df[\""B\""]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericInferOps:\n    def setup(self, dtype):\n        N = 5 * 10**5\n        self.df = DataFrame(\n            {\""A\"": np.arange(N).astype(dtype), \""B\"": np.arange(N).astype(dtype)}\n        )""}",arithmetic.time_divide,time,0.23035409773465737
14944,func-level,"terminus-2,gpt-5",1.1083670438653708,1.3767683410933085,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.time_select_dtype_string_exclude,dtypes,"{""dtypes.SelectDtypes.time_select_dtype_string_exclude"": ""class SelectDtypes:\n    def time_select_dtype_string_exclude(self, dtype):\n        self.df_string.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )""}",dtypes.time_select_dtype_string_exclude,time,-0.1898170418868018
14948,func-level,"terminus-2,gpt-5",1.4722037190624588,1.3893436394248913,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.time_mult,eval,"{""eval.Eval.time_mult"": ""class Eval:\n    def time_mult(self, engine, threads):\n        pd.eval(\""self.df * self.df2 * self.df3 * self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)""}",eval.time_mult,time,0.05859977343533776
14945,func-level,"terminus-2,gpt-5",1.3115797104729858,1.2709339453302828,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.time_add,eval,"{""eval.Eval.time_add"": ""class Eval:\n    def time_add(self, engine, threads):\n        pd.eval(\""self.df + self.df2 + self.df3 + self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)""}",eval.time_add,time,0.02874523701747031
14949,func-level,"terminus-2,gpt-5",1.1780682023749691,1.1317455443362747,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.time_query_datetime_column,eval,"{""eval.Query.time_query_datetime_column"": ""class Query:\n    def time_query_datetime_column(self):\n        self.df.query(\""dates < @self.ts\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Query:\n    def setup(self):\n        N = 10**6\n        halfway = (N // 2) - 1\n        index = pd.date_range(\""20010101\"", periods=N, freq=\""T\"")\n        s = pd.Series(index)\n        self.ts = s.iloc[halfway]\n        self.df = pd.DataFrame({\""a\"": np.random.randn(N), \""dates\"": index}, index=index)\n        data = np.random.randn(N)\n        self.min_val = data.min()\n        self.max_val = data.max()""}",eval.time_query_datetime_column,time,0.03276001275720962
14947,func-level,"terminus-2,gpt-5",1.1556759264583172,1.159924219526538,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.time_chained_cmp,eval,"{""eval.Eval.time_chained_cmp"": ""class Eval:\n    def time_chained_cmp(self, engine, threads):\n        pd.eval(\""self.df < self.df2 < self.df3 < self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)""}",eval.time_chained_cmp,time,-0.0030044505432962037
14950,func-level,"terminus-2,gpt-5",1.17618474881113,1.1546050789147206,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.time_query_with_boolean_selection,eval,"{""eval.Query.time_query_with_boolean_selection"": ""class Query:\n    def time_query_with_boolean_selection(self):\n        self.df.query(\""(a >= @self.min_val) & (a <= @self.max_val)\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Query:\n    def setup(self):\n        N = 10**6\n        halfway = (N // 2) - 1\n        index = pd.date_range(\""20010101\"", periods=N, freq=\""T\"")\n        s = pd.Series(index)\n        self.ts = s.iloc[halfway]\n        self.df = pd.DataFrame({\""a\"": np.random.randn(N), \""dates\"": index}, index=index)\n        data = np.random.randn(N)\n        self.min_val = data.min()\n        self.max_val = data.max()""}",eval.time_query_with_boolean_selection,time,0.015261435570303728
14951,func-level,"terminus-2,gpt-5",0.9836834185219744,0.994961338034834,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,finalize.time_finalize_micro,finalize,"{""finalize.Finalize.time_finalize_micro"": ""class Finalize:\n    def time_finalize_micro(self, param):\n        self.obj.__finalize__(self.obj, method=\""__finalize__\"")\n\n    def setup(self, param):\n        N = 1000\n        obj = param(dtype=float)\n        for i in range(N):\n            obj.attrs[i] = i\n        self.obj = obj""}",finalize.time_finalize_micro,time,-0.0079758978167324
14954,func-level,"terminus-2,gpt-5",1.2988358280316128,1.0644373832418372,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.time_list_of_dict,frame_ctor,"{""frame_ctor.FromDicts.time_list_of_dict"": ""class FromDicts:\n    def time_list_of_dict(self):\n        DataFrame(self.dict_list)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDicts:\n    def setup(self):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n        frame = DataFrame(np.random.randn(N, K), index=self.index, columns=self.columns)\n        self.data = frame.to_dict()\n        self.dict_list = frame.to_dict(orient=\""records\"")\n        self.data2 = {i: {j: float(j) for j in range(100)} for i in range(2000)}\n    \n        # arrays which we won't consolidate\n        self.dict_of_categoricals = {i: Categorical(np.arange(N)) for i in range(K)}""}",frame_ctor.time_list_of_dict,time,0.16576976293477763
14946,func-level,"terminus-2,gpt-5",1.2009407550352635,1.1345405893436904,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.time_and,eval,"{""eval.Eval.time_and"": ""class Eval:\n    def time_and(self, engine, threads):\n        pd.eval(\n            \""(self.df > 0) & (self.df2 > 0) & (self.df3 > 0) & (self.df4 > 0)\"",\n            engine=engine,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)""}",eval.time_and,time,0.04695909879177733
14955,func-level,"terminus-2,gpt-5",1.2331877612548838,1.1014807750057138,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_count_level_mixed_dtypes_multi,frame_methods,"{""frame_methods.Count.time_count_level_mixed_dtypes_multi"": ""class Count:\n    def time_count_level_mixed_dtypes_multi(self, axis):\n        self.df_mixed.count(axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Count:\n    def setup(self, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""\n    \n        self.df.index = MultiIndex.from_arrays([self.df.index, self.df.index])\n        self.df.columns = MultiIndex.from_arrays([self.df.columns, self.df.columns])\n        self.df_mixed.index = MultiIndex.from_arrays(\n            [self.df_mixed.index, self.df_mixed.index]\n        )\n        self.df_mixed.columns = MultiIndex.from_arrays(\n            [self.df_mixed.columns, self.df_mixed.columns]\n        )""}",frame_methods.time_count_level_mixed_dtypes_multi,time,0.09314496905881894
14957,func-level,"terminus-2,gpt-5",1.22214795531945,1.1480368354203323,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_dropna,frame_methods,"{""frame_methods.Dropna.time_dropna"": ""class Dropna:\n    def time_dropna(self, how, axis):\n        self.df.dropna(how=how, axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Dropna:\n    def setup(self, how, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""""}",frame_methods.time_dropna,time,0.05241239031055006
14943,func-level,"terminus-2,gpt-5",1.0719707549674986,1.0679607860056457,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.time_select_dtype_int_exclude,dtypes,"{""dtypes.SelectDtypes.time_select_dtype_int_exclude"": ""class SelectDtypes:\n    def time_select_dtype_int_exclude(self, dtype):\n        self.df_int.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )""}",dtypes.time_select_dtype_int_exclude,time,0.0028359044991887595
14953,func-level,"terminus-2,gpt-5",1.0535484176124674,1.0407155721351138,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.time_frame_from_scalar_ea_float64_na,frame_ctor,"{""frame_ctor.FromScalar.time_frame_from_scalar_ea_float64_na"": ""class FromScalar:\n    def time_frame_from_scalar_ea_float64_na(self):\n        DataFrame(\n            NA,\n            index=range(self.nrows),\n            columns=list(\""abc\""),\n            dtype=Float64Dtype(),\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromScalar:\n    def setup(self):\n        self.nrows = 100_000""}",frame_ctor.time_frame_from_scalar_ea_float64_na,time,0.009075562572385884
14958,func-level,"terminus-2,gpt-5",1.1908681192596289,1.170278022390865,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_dropna_axis_mixed_dtypes,frame_methods,"{""frame_methods.Dropna.time_dropna_axis_mixed_dtypes"": ""class Dropna:\n    def time_dropna_axis_mixed_dtypes(self, how, axis):\n        self.df_mixed.dropna(how=how, axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Dropna:\n    def setup(self, how, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""""}",frame_methods.time_dropna_axis_mixed_dtypes,time,0.014561596088234643
14959,func-level,"terminus-2,gpt-5",1.0356180912954065,1.0415164232546907,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_frame_duplicated_wide,frame_methods,"{""frame_methods.Duplicated.time_frame_duplicated_wide"": ""class Duplicated:\n    def time_frame_duplicated_wide(self):\n        self.df2.duplicated()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Duplicated:\n    def setup(self):\n        n = 1 << 20\n        t = date_range(\""2015-01-01\"", freq=\""S\"", periods=(n // 64))\n        xs = np.random.randn(n // 64).round(2)\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(-1 << 8, 1 << 8, n),\n                \""b\"": np.random.choice(t, n),\n                \""c\"": np.random.choice(xs, n),\n            }\n        )\n        self.df2 = DataFrame(np.random.randn(1000, 100).astype(str)).T""}",frame_methods.time_frame_duplicated_wide,time,-0.004171380452110496
14952,func-level,"terminus-2,gpt-5",1.1568948063018856,1.109764724574163,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.time_frame_from_arrays_sparse,frame_ctor,"{""frame_ctor.FromArrays.time_frame_from_arrays_sparse"": ""class FromArrays:\n    def time_frame_from_arrays_sparse(self):\n        self.df = DataFrame._from_arrays(\n            self.sparse_arrays,\n            index=self.index,\n            columns=self.columns,\n            verify_integrity=False,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromArrays:\n    def setup(self):\n        N_rows = 1000\n        N_cols = 1000\n        self.float_arrays = [np.random.randn(N_rows) for _ in range(N_cols)]\n        self.sparse_arrays = [\n            pd.arrays.SparseArray(np.random.randint(0, 2, N_rows), dtype=\""float64\"")\n            for _ in range(N_cols)\n        ]\n        self.int_arrays = [\n            pd.array(np.random.randint(1000, size=N_rows), dtype=\""Int64\"")\n            for _ in range(N_cols)\n        ]\n        self.index = pd.Index(range(N_rows))\n        self.columns = pd.Index(range(N_cols))""}",frame_ctor.time_frame_from_arrays_sparse,time,0.033331033753693416
14964,func-level,"terminus-2,gpt-5",1.1032608803409794,1.0482445796343702,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_series_describe,frame_methods,"{""frame_methods.Describe.time_series_describe"": ""class Describe:\n    def time_series_describe(self):\n        self.df[\""a\""].describe()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Describe:\n    def setup(self):\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(0, 100, 10**6),\n                \""b\"": np.random.randint(0, 100, 10**6),\n                \""c\"": np.random.randint(0, 100, 10**6),\n            }\n        )""}",frame_methods.time_series_describe,time,0.03890827489859203
14965,func-level,"terminus-2,gpt-5",1.1189741944212834,1.274450093142372,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_to_dict_ints,frame_methods,"{""frame_methods.ToDict.time_to_dict_ints"": ""class ToDict:\n    def time_to_dict_ints(self, orient):\n        self.int_df.to_dict(orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToDict:\n    def setup(self, orient):\n        data = np.random.randint(0, 1000, size=(10000, 4))\n        self.int_df = DataFrame(data)\n        self.datetimelike_df = self.int_df.astype(\""timedelta64[ns]\"")""}",frame_methods.time_to_dict_ints,time,-0.10995466670515451
14956,func-level,"terminus-2,gpt-5",1.1142605285625165,1.0485527998893167,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_dataframe_describe,frame_methods,"{""frame_methods.Describe.time_dataframe_describe"": ""class Describe:\n    def time_dataframe_describe(self):\n        self.df.describe()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Describe:\n    def setup(self):\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(0, 100, 10**6),\n                \""b\"": np.random.randint(0, 100, 10**6),\n                \""c\"": np.random.randint(0, 100, 10**6),\n            }\n        )""}",frame_methods.time_dataframe_describe,time,0.04646939793012712
14960,func-level,"terminus-2,gpt-5",1.1691773958960416,1.0972896468407467,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_frame_fillna,frame_methods,"{""frame_methods.Fillna.time_frame_fillna"": ""class Fillna:\n    def time_frame_fillna(self, inplace, method, dtype):\n        self.df.fillna(inplace=inplace, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Fillna:\n    def setup(self, inplace, method, dtype):\n        N, M = 10000, 100\n        if dtype in (\""datetime64[ns]\"", \""datetime64[ns, tz]\"", \""timedelta64[ns]\""):\n            data = {\n                \""datetime64[ns]\"": date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n                \""datetime64[ns, tz]\"": date_range(\n                    \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n                ),\n                \""timedelta64[ns]\"": timedelta_range(start=\""1 day\"", periods=N, freq=\""1D\""),\n            }\n            self.df = DataFrame({f\""col_{i}\"": data[dtype] for i in range(M)})\n            self.df[::2] = None\n        else:\n            values = np.random.randn(N, M)\n            values[::2] = np.nan\n            if dtype == \""Int64\"":\n                values = values.round()\n            self.df = DataFrame(values, dtype=dtype)""}",frame_methods.time_frame_fillna,time,0.05083999225975597
14961,func-level,"terminus-2,gpt-5",1.7318344749928167,1.5831426169002387,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_frame_float_equal,frame_methods,"{""frame_methods.Equals.time_frame_float_equal"": ""class Equals:\n    def time_frame_float_equal(self):\n        self.float_df.equals(self.float_df)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Equals:\n    def setup(self):\n        N = 10**3\n        self.float_df = DataFrame(np.random.randn(N, N))\n        self.float_df_nan = self.float_df.copy()\n        self.float_df_nan.iloc[-1, -1] = np.nan\n    \n        self.object_df = DataFrame(\""foo\"", index=range(N), columns=range(N))\n        self.object_df_nan = self.object_df.copy()\n        self.object_df_nan.iloc[-1, -1] = np.nan\n    \n        self.nonunique_cols = self.object_df.copy()\n        self.nonunique_cols.columns = [\""A\""] * len(self.nonunique_cols.columns)\n        self.nonunique_cols_nan = self.nonunique_cols.copy()\n        self.nonunique_cols_nan.iloc[-1, -1] = np.nan""}",frame_methods.time_frame_float_equal,time,0.10515690105557145
14962,func-level,"terminus-2,gpt-5",1.0375599233103503,1.0356350267655572,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_items_cached,frame_methods,"{""frame_methods.Iteration.time_items_cached"": ""class Iteration:\n    def time_items_cached(self):\n        for name, col in self.df.items():\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))""}",frame_methods.time_items_cached,time,0.0013613129736867813
14970,func-level,"terminus-2,gpt-5",1.177381398721525,1.0188571417553025,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.time_groupby_apply_non_unique_unsorted_index,groupby,"{""groupby.ApplyNonUniqueUnsortedIndex.time_groupby_apply_non_unique_unsorted_index"": ""class ApplyNonUniqueUnsortedIndex:\n    def time_groupby_apply_non_unique_unsorted_index(self):\n        self.df.groupby(\""key\"", group_keys=False).apply(lambda x: x)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ApplyNonUniqueUnsortedIndex:\n    def setup(self):\n        # GH 46527\n        # unsorted and non-unique index\n        idx = np.arange(100)[::-1]\n        idx = Index(np.repeat(idx, 200), name=\""key\"")\n        self.df = DataFrame(np.random.randn(len(idx), 10), index=idx)""}",groupby.time_groupby_apply_non_unique_unsorted_index,time,0.11211050704824782
14963,func-level,"terminus-2,gpt-5",1.0116562576417258,1.1062177869730845,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_iterrows,frame_methods,"{""frame_methods.Iteration.time_iterrows"": ""class Iteration:\n    def time_iterrows(self):\n        for row in self.df.iterrows():\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))""}",frame_methods.time_iterrows,time,-0.06687519754692973
14974,func-level,"terminus-2,gpt-5",1.047004930851596,1.042586536069814,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.time_transform_lambda_max_wide,groupby,"{""groupby.Transform.time_transform_lambda_max_wide"": ""class Transform:\n    def time_transform_lambda_max_wide(self):\n        self.df_wide.groupby(level=0).transform(lambda x: np.max(x, axis=0))\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]""}",groupby.time_transform_lambda_max_wide,time,0.0031247487848528746
14968,func-level,"terminus-2,gpt-5",1.07488800404684,1.0647825340230623,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.time_dtype_as_group,groupby,"{""groupby.GroupByMethods.time_dtype_as_group"": ""class GroupByMethods:\n    def time_dtype_as_group(self, dtype, method, application, ncols):\n        self.as_group_method()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupByMethods:\n    def setup(self, dtype, method, application, ncols):\n        if method in method_blocklist.get(dtype, {}):\n            raise NotImplementedError  # skip benchmark\n    \n        if ncols != 1 and method in [\""value_counts\"", \""unique\""]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if application == \""transformation\"" and method in [\n            \""describe\"",\n            \""head\"",\n            \""tail\"",\n            \""unique\"",\n            \""value_counts\"",\n            \""size\"",\n        ]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if method == \""describe\"":\n            ngroups = 20\n        elif method == \""skew\"":\n            ngroups = 100\n        else:\n            ngroups = 1000\n        size = ngroups * 2\n        rng = np.arange(ngroups).reshape(-1, 1)\n        rng = np.broadcast_to(rng, (len(rng), ncols))\n        taker = np.random.randint(0, ngroups, size=size)\n        values = rng.take(taker, axis=0)\n        if dtype == \""int\"":\n            key = np.random.randint(0, size, size=size)\n        elif dtype in (\""int16\"", \""uint\""):\n            key = np.random.randint(0, size, size=size, dtype=dtype)\n        elif dtype == \""float\"":\n            key = np.concatenate(\n                [np.random.random(ngroups) * 0.1, np.random.random(ngroups) * 10.0]\n            )\n        elif dtype == \""object\"":\n            key = [\""foo\""] * size\n        elif dtype == \""datetime\"":\n            key = date_range(\""1/1/2011\"", periods=size, freq=\""s\"")\n    \n        cols = [f\""values{n}\"" for n in range(ncols)]\n        df = DataFrame(values, columns=cols)\n        df[\""key\""] = key\n    \n        if len(cols) == 1:\n            cols = cols[0]\n    \n        if application == \""transformation\"":\n            self.as_group_method = lambda: df.groupby(\""key\"")[cols].transform(method)\n            self.as_field_method = lambda: df.groupby(cols)[\""key\""].transform(method)\n        else:\n            self.as_group_method = getattr(df.groupby(\""key\"")[cols], method)\n            self.as_field_method = getattr(df.groupby(cols)[\""key\""], method)""}",groupby.time_dtype_as_group,time,0.007146725617947459
14972,func-level,"terminus-2,gpt-5",1.119538568783104,1.0230055870235066,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.time_scalar_function_multi_col,groupby,"{""groupby.Apply.time_scalar_function_multi_col"": ""class Apply:\n    def time_scalar_function_multi_col(self, factor):\n        self.df.groupby([\""key\"", \""key2\""]).apply(lambda x: 1)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, factor):\n        N = 10**factor\n        # two cases:\n        # - small groups: small data (N**4) + many labels (2000) -> average group\n        #   size of 5 (-> larger overhead of slicing method)\n        # - larger groups: larger data (N**5) + fewer labels (20) -> average group\n        #   size of 5000\n        labels = np.random.randint(0, 2000 if factor == 4 else 20, size=N)\n        labels2 = np.random.randint(0, 3, size=N)\n        df = DataFrame(\n            {\n                \""key\"": labels,\n                \""key2\"": labels2,\n                \""value1\"": np.random.randn(N),\n                \""value2\"": [\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (N // 4),\n            }\n        )\n        self.df = df""}",groupby.time_scalar_function_multi_col,time,0.06826943547354841
14967,func-level,"terminus-2,gpt-5",1.0619004043364555,1.0764056266782285,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.time_dtype_as_field,groupby,"{""groupby.GroupByMethods.time_dtype_as_field"": ""class GroupByMethods:\n    def time_dtype_as_field(self, dtype, method, application, ncols):\n        self.as_field_method()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupByMethods:\n    def setup(self, dtype, method, application, ncols):\n        if method in method_blocklist.get(dtype, {}):\n            raise NotImplementedError  # skip benchmark\n    \n        if ncols != 1 and method in [\""value_counts\"", \""unique\""]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if application == \""transformation\"" and method in [\n            \""describe\"",\n            \""head\"",\n            \""tail\"",\n            \""unique\"",\n            \""value_counts\"",\n            \""size\"",\n        ]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if method == \""describe\"":\n            ngroups = 20\n        elif method == \""skew\"":\n            ngroups = 100\n        else:\n            ngroups = 1000\n        size = ngroups * 2\n        rng = np.arange(ngroups).reshape(-1, 1)\n        rng = np.broadcast_to(rng, (len(rng), ncols))\n        taker = np.random.randint(0, ngroups, size=size)\n        values = rng.take(taker, axis=0)\n        if dtype == \""int\"":\n            key = np.random.randint(0, size, size=size)\n        elif dtype in (\""int16\"", \""uint\""):\n            key = np.random.randint(0, size, size=size, dtype=dtype)\n        elif dtype == \""float\"":\n            key = np.concatenate(\n                [np.random.random(ngroups) * 0.1, np.random.random(ngroups) * 10.0]\n            )\n        elif dtype == \""object\"":\n            key = [\""foo\""] * size\n        elif dtype == \""datetime\"":\n            key = date_range(\""1/1/2011\"", periods=size, freq=\""s\"")\n    \n        cols = [f\""values{n}\"" for n in range(ncols)]\n        df = DataFrame(values, columns=cols)\n        df[\""key\""] = key\n    \n        if len(cols) == 1:\n            cols = cols[0]\n    \n        if application == \""transformation\"":\n            self.as_group_method = lambda: df.groupby(\""key\"")[cols].transform(method)\n            self.as_field_method = lambda: df.groupby(cols)[\""key\""].transform(method)\n        else:\n            self.as_group_method = getattr(df.groupby(\""key\"")[cols], method)\n            self.as_field_method = getattr(df.groupby(cols)[\""key\""], method)""}",groupby.time_dtype_as_field,time,-0.01025829019927368
14973,func-level,"terminus-2,gpt-5",1.1111333736972115,0.7678419434164367,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.time_str_func,groupby,"{""groupby.String.time_str_func"": ""class String:\n    def time_str_func(self, dtype, method):\n        self.df.groupby(\""a\"")[self.df.columns[1:]].agg(method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass String:\n    def setup(self, dtype, method):\n        cols = list(\""abcdefghjkl\"")\n        self.df = DataFrame(\n            np.random.randint(0, 100, size=(10_000, len(cols))),\n            columns=cols,\n            dtype=dtype,\n        )""}",groupby.time_str_func,time,0.2427803608774928
14966,func-level,"terminus-2,gpt-5",1.1072106193677194,1.139579860075716,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,gil.time_parallel,gil,"{""gil.ParallelFactorize.time_parallel"": ""class ParallelFactorize:\n    def time_parallel(self, threads):\n        self.parallel()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ParallelFactorize:\n    def setup(self, threads):\n        strings = tm.makeStringIndex(100000)\n    \n        @test_parallel(num_threads=threads)\n        def parallel():\n            factorize(strings)\n    \n        self.parallel = parallel\n    \n        def loop():\n            factorize(strings)\n    \n        self.loop = loop"", ""gil.ParallelGroupbyMethods.time_parallel"": ""class ParallelGroupbyMethods:\n    def time_parallel(self, threads, method):\n        self.parallel()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ParallelGroupbyMethods:\n    def setup(self, threads, method):\n        N = 10**6\n        ngroups = 10**3\n        df = DataFrame(\n            {\""key\"": np.random.randint(0, ngroups, size=N), \""data\"": np.random.randn(N)}\n        )\n    \n        @test_parallel(num_threads=threads)\n        def parallel():\n            getattr(df.groupby(\""key\"")[\""data\""], method)()\n    \n        self.parallel = parallel\n    \n        def loop():\n            getattr(df.groupby(\""key\"")[\""data\""], method)()\n    \n        self.loop = loop""}",gil.time_parallel,time,-0.02289196655445314
14977,func-level,"terminus-2,gpt-5",1.276018616164088,1.0398100956535965,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_cached_properties.time_engine,index_cached_properties,"{""index_cached_properties.IndexCache.time_engine"": ""class IndexCache:\n    def time_engine(self, index_type):\n        self.idx._engine\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}""}",index_cached_properties.time_engine,time,0.16704987306258245
14971,func-level,"terminus-2,gpt-5",1.0521344820479284,0.7453157545798069,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.time_rank_ties,groupby,"{""groupby.RankWithTies.time_rank_ties"": ""class RankWithTies:\n    def time_rank_ties(self, dtype, tie_method):\n        self.df.groupby(\""key\"").rank(method=tie_method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass RankWithTies:\n    def setup(self, dtype, tie_method):\n        N = 10**4\n        if dtype == \""datetime64\"":\n            data = np.array([Timestamp(\""2011/01/01\"")] * N, dtype=dtype)\n        else:\n            data = np.array([1] * N, dtype=dtype)\n        self.df = DataFrame({\""values\"": data, \""key\"": [\""foo\""] * N})""}",groupby.time_rank_ties,time,0.2169863702037634
14976,func-level,"terminus-2,gpt-5",1.1638270210400568,1.0353260541593894,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,hash_functions.time_loc_slice,hash_functions,"{""hash_functions.NumericSeriesIndexing.time_loc_slice"": ""class NumericSeriesIndexing:\n    def time_loc_slice(self, index, N):\n        # trigger building of mapping\n        self.data.loc[:800]\n\n    def setup(self, dtype, N):\n        vals = np.array(list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype)\n        indices = pd.Index(vals)\n        self.data = pd.Series(np.arange(N), index=indices)"", ""hash_functions.NumericSeriesIndexingShuffled.time_loc_slice"": ""class NumericSeriesIndexingShuffled:\n    def time_loc_slice(self, index, N):\n        # trigger building of mapping\n        self.data.loc[:800]\n\n    def setup(self, dtype, N):\n        vals = np.array(list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype)\n        np.random.shuffle(vals)\n        indices = pd.Index(vals)\n        self.data = pd.Series(np.arange(N), index=indices)""}",hash_functions.time_loc_slice,time,0.09087762862847763
14969,func-level,"terminus-2,gpt-5",1.0701570395163813,1.070948001002442,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.time_frame_transform,groupby,"{""groupby.Cumulative.time_frame_transform"": ""class Cumulative:\n    def time_frame_transform(self, dtype, method, with_nans):\n        self.df.groupby(\""key\"").transform(method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cumulative:\n    def setup(self, dtype, method, with_nans):\n        if with_nans and dtype == \""int64\"":\n            raise NotImplementedError(\""Construction of df would raise\"")\n    \n        N = 500_000\n        keys = np.random.randint(0, 100, size=N)\n        vals = np.random.randint(-10, 10, (N, 5))\n    \n        if with_nans:\n            null_vals = vals.astype(float, copy=True)\n            null_vals[::2, :] = np.nan\n            null_vals[::3, :] = np.nan\n            df = DataFrame(null_vals, columns=list(\""abcde\""), dtype=dtype)\n            df[\""key\""] = keys\n            self.df = df\n        else:\n            df = DataFrame(vals, columns=list(\""abcde\"")).astype(dtype, copy=False)\n            df[\""key\""] = keys\n            self.df = df""}",groupby.time_frame_transform,time,-0.0005593787030131648
14975,func-level,"terminus-2,gpt-5",1.1057859716008107,1.1141947051174346,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,hash_functions.time_factorize,hash_functions,"{""hash_functions.UniqueAndFactorizeArange.time_factorize"": ""class UniqueAndFactorizeArange:\n    def time_factorize(self, exponent):\n        pd.factorize(self.a2)\n\n    def setup(self, exponent):\n        a = np.arange(10**4, dtype=\""float64\"")\n        self.a2 = (a + 10**exponent).repeat(100)""}",hash_functions.time_factorize,time,-0.005946770520950394
14980,func-level,"terminus-2,gpt-5",1.0444604293552595,1.0263895534781546,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_object.time_get_loc_sorted,index_object,"{""index_object.Indexing.time_get_loc_sorted"": ""class Indexing:\n    def time_get_loc_sorted(self, dtype):\n        self.sorted.get_loc(self.key)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self, dtype):\n        N = 10**6\n        self.idx = getattr(tm, f\""make{dtype}Index\"")(N)\n        self.array_mask = (np.arange(N) % 3) == 0\n        self.series_mask = Series(self.array_mask)\n        self.sorted = self.idx.sort_values()\n        half = N // 2\n        self.non_unique = self.idx[:half].append(self.idx[:half])\n        self.non_unique_sorted = (\n            self.sorted[:half].append(self.sorted[:half]).sort_values()\n        )\n        self.key = self.sorted[N // 4]""}",index_object.time_get_loc_sorted,time,0.012779968795689446
14978,func-level,"terminus-2,gpt-5",1.108824880447787,1.0838227693246487,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_cached_properties.time_is_monotonic_decreasing,index_cached_properties,"{""index_cached_properties.IndexCache.time_is_monotonic_decreasing"": ""class IndexCache:\n    def time_is_monotonic_decreasing(self, index_type):\n        self.idx.is_monotonic_decreasing\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}""}",index_cached_properties.time_is_monotonic_decreasing,time,0.017681832477466915
14983,func-level,"terminus-2,gpt-5",1.2294133914530463,1.2018416031412138,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_get_indexer_dups,indexing,"{""indexing.NumericMaskedIndexing.time_get_indexer_dups"": ""class NumericMaskedIndexing:\n    def time_get_indexer_dups(self, dtype, monotonic):\n        self.data.get_indexer_for(self.indexer)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericMaskedIndexing:\n    def setup(self, dtype, monotonic):\n        indices = {\n            True: Index(self.monotonic_list, dtype=dtype),\n            False: Index(self.non_monotonic_list, dtype=dtype).append(\n                Index([NA], dtype=dtype)\n            ),\n        }\n        self.data = indices[monotonic]\n        self.indexer = np.arange(300, 1_000)\n        self.data_dups = self.data.append(self.data)""}",indexing.time_get_indexer_dups,time,0.01949914307767503
14984,func-level,"terminus-2,gpt-5",1.0684170052273432,1.0333263660754053,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_get_indexer_mismatched_tz,indexing,"{""indexing.DatetimeIndexIndexing.time_get_indexer_mismatched_tz"": ""class DatetimeIndexIndexing:\n    def time_get_indexer_mismatched_tz(self):\n        # reached via e.g.\n        #  ser = Series(range(len(dti)), index=dti)\n        #  ser[dti2]\n        self.dti.get_indexer(self.dti2)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndexIndexing:\n    def setup(self):\n        dti = date_range(\""2016-01-01\"", periods=10000, tz=\""US/Pacific\"")\n        dti2 = dti.tz_convert(\""UTC\"")\n        self.dti = dti\n        self.dti2 = dti2""}",indexing.time_get_indexer_mismatched_tz,time,0.024816576486519016
14985,func-level,"terminus-2,gpt-5",1.0952383674801311,1.0708721027634038,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_getitem_array,indexing,"{""indexing.NumericSeriesIndexing.time_getitem_array"": ""class NumericSeriesIndexing:\n    def time_getitem_array(self, index, index_structure):\n        self.data[self.array]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.time_getitem_array,time,0.017232153265012245
14979,func-level,"terminus-2,gpt-5",1.1488001500199212,1.0220004458551766,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_cached_properties.time_is_monotonic_increasing,index_cached_properties,"{""index_cached_properties.IndexCache.time_is_monotonic_increasing"": ""class IndexCache:\n    def time_is_monotonic_increasing(self, index_type):\n        self.idx.is_monotonic_increasing\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}""}",index_cached_properties.time_is_monotonic_increasing,time,0.08967447253518007
14986,func-level,"terminus-2,gpt-5",1.0484895577529203,1.0473046935586134,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_getitem_label_slice,indexing,"{""indexing.NonNumericSeriesIndexing.time_getitem_label_slice"": ""class NonNumericSeriesIndexing:\n    def time_getitem_label_slice(self, index, index_structure):\n        self.s[: self.lbl]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]""}",indexing.time_getitem_label_slice,time,0.000837952046893109
14987,func-level,"terminus-2,gpt-5",1.110944526395339,1.1635971122694453,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_getitem_list_like,indexing,"{""indexing.NonNumericSeriesIndexing.time_getitem_list_like"": ""class NonNumericSeriesIndexing:\n    def time_getitem_list_like(self, index, index_structure):\n        self.s[[self.lbl]]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]"", ""indexing.NumericSeriesIndexing.time_getitem_list_like"": ""class NumericSeriesIndexing:\n    def time_getitem_list_like(self, index, index_structure):\n        self.data[[800000]]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.time_getitem_list_like,time,-0.037236623673342475
14989,func-level,"terminus-2,gpt-5",1.0450318161993282,1.0513697873029555,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_getitem_scalar,indexing,"{""indexing.CategoricalIndexIndexing.time_getitem_scalar"": ""class CategoricalIndexIndexing:\n    def time_getitem_scalar(self, index):\n        self.data[self.int_scalar]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalIndexIndexing:\n    def setup(self, index):\n        N = 10**5\n        values = list(\""a\"" * N + \""b\"" * N + \""c\"" * N)\n        indices = {\n            \""monotonic_incr\"": CategoricalIndex(values),\n            \""monotonic_decr\"": CategoricalIndex(reversed(values)),\n            \""non_monotonic\"": CategoricalIndex(list(\""abc\"" * N)),\n        }\n        self.data = indices[index]\n        self.data_unique = CategoricalIndex([str(i) for i in range(N * 3)])\n    \n        self.int_scalar = 10000\n        self.int_list = list(range(10000))\n    \n        self.cat_scalar = \""b\""\n        self.cat_list = [\""1\"", \""3\""]"", ""indexing.NonNumericSeriesIndexing.time_getitem_scalar"": ""class NonNumericSeriesIndexing:\n    def time_getitem_scalar(self, index, index_structure):\n        self.s[self.lbl]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]"", ""indexing.NumericSeriesIndexing.time_getitem_scalar"": ""class NumericSeriesIndexing:\n    def time_getitem_scalar(self, index, index_structure):\n        self.data[800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.time_getitem_scalar,time,-0.004482299224630359
14982,func-level,"terminus-2,gpt-5",1.2642587347976593,1.2617980705467426,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_get_indexer,indexing,"{""indexing.NumericMaskedIndexing.time_get_indexer"": ""class NumericMaskedIndexing:\n    def time_get_indexer(self, dtype, monotonic):\n        self.data.get_indexer(self.indexer)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericMaskedIndexing:\n    def setup(self, dtype, monotonic):\n        indices = {\n            True: Index(self.monotonic_list, dtype=dtype),\n            False: Index(self.non_monotonic_list, dtype=dtype).append(\n                Index([NA], dtype=dtype)\n            ),\n        }\n        self.data = indices[monotonic]\n        self.indexer = np.arange(300, 1_000)\n        self.data_dups = self.data.append(self.data)""}",indexing.time_get_indexer,time,0.0017402151703795065
14991,func-level,"terminus-2,gpt-5",1.2339709997730812,1.061558771647169,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_loc_all_null_slices,indexing,"{""indexing.MultiIndexing.time_loc_all_null_slices"": ""class MultiIndexing:\n    def time_loc_all_null_slices(self, unique_levels):\n        target = tuple([self.tgt_null_slice] * self.nlevels)\n        self.df.loc[target, :]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MultiIndexing:\n    def setup(self, unique_levels):\n        self.nlevels = 2\n        if unique_levels:\n            mi = MultiIndex.from_arrays([range(1000000)] * self.nlevels)\n        else:\n            mi = MultiIndex.from_product([range(1000)] * self.nlevels)\n        self.df = DataFrame(np.random.randn(len(mi)), index=mi)\n    \n        self.tgt_slice = slice(200, 800)\n        self.tgt_null_slice = slice(None)\n        self.tgt_list = list(range(0, 1000, 10))\n        self.tgt_scalar = 500\n    \n        bool_indexer = np.zeros(len(mi), dtype=np.bool_)\n        bool_indexer[slice(0, len(mi), 100)] = True\n        self.tgt_bool_indexer = bool_indexer""}",indexing.time_loc_all_null_slices,time,0.12193226883020664
14990,func-level,"terminus-2,gpt-5",1.2689658857262036,1.2623581032741231,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_getitem_slice,indexing,"{""indexing.NumericSeriesIndexing.time_getitem_slice"": ""class NumericSeriesIndexing:\n    def time_getitem_slice(self, index, index_structure):\n        self.data[:800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.time_getitem_slice,time,0.004673113473890022
14988,func-level,"terminus-2,gpt-5",1.2361869970793182,1.208053664760974,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_getitem_lists,indexing,"{""indexing.NumericSeriesIndexing.time_getitem_lists"": ""class NumericSeriesIndexing:\n    def time_getitem_lists(self, index, index_structure):\n        self.data[self.array_list]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.time_getitem_lists,time,0.01989627462400576
14981,func-level,"terminus-2,gpt-5",1.273250890439918,1.16908494765572,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_object.time_operation,index_object,"{""index_object.SetOperations.time_operation"": ""class SetOperations:\n    def time_operation(self, index_structure, dtype, method):\n        getattr(self.left, method)(self.right)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SetOperations:\n    def setup(self, index_structure, dtype, method):\n        N = 10**5\n        dates_left = date_range(\""1/1/2000\"", periods=N, freq=\""T\"")\n        fmt = \""%Y-%m-%d %H:%M:%S\""\n        date_str_left = Index(dates_left.strftime(fmt))\n        int_left = Index(np.arange(N))\n        ea_int_left = Index(np.arange(N), dtype=\""Int64\"")\n        str_left = tm.makeStringIndex(N)\n    \n        data = {\n            \""datetime\"": dates_left,\n            \""date_string\"": date_str_left,\n            \""int\"": int_left,\n            \""strings\"": str_left,\n            \""ea_int\"": ea_int_left,\n        }\n    \n        if index_structure == \""non_monotonic\"":\n            data = {k: mi[::-1] for k, mi in data.items()}\n    \n        data = {k: {\""left\"": idx, \""right\"": idx[:-1]} for k, idx in data.items()}\n    \n        self.left = data[dtype][\""left\""]\n        self.right = data[dtype][\""right\""]""}",index_object.time_operation,time,0.07366756915431266
14994,func-level,"terminus-2,gpt-5",1.0189707786432889,1.038504163552764,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_loc_row,indexing,"{""indexing.IndexSingleRow.time_loc_row"": ""class IndexSingleRow:\n    def time_loc_row(self, unique_cols):\n        self.df.loc[10000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IndexSingleRow:\n    def setup(self, unique_cols):\n        arr = np.arange(10**7).reshape(-1, 10)\n        df = DataFrame(arr)\n        dtypes = [\""u1\"", \""u2\"", \""u4\"", \""u8\"", \""i1\"", \""i2\"", \""i4\"", \""i8\"", \""f8\"", \""f4\""]\n        for i, d in enumerate(dtypes):\n            df[i] = df[i].astype(d)\n    \n        if not unique_cols:\n            # GH#33032 single-row lookups with non-unique columns were\n            #  15x slower than with unique columns\n            df.columns = [\""A\"", \""A\""] + list(df.columns[2:])\n    \n        self.df = df""}",indexing.time_loc_row,time,-0.013814275042061673
15000,func-level,"terminus-2,gpt-5",1.0571727132763542,1.03767982340548,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.csv.time_read_stringcsv,io.csv,"{""io.csv.ReadCSVEngine.time_read_stringcsv"": ""class ReadCSVEngine:\n    def time_read_stringcsv(self, engine):\n        read_csv(self.data(self.StringIO_input), engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVEngine:\n    def setup(self, engine):\n        data = [\""A,B,C,D,E\""] + ([\""1,2,3,4,5\""] * 100000)\n        self.StringIO_input = StringIO(\""\\n\"".join(data))\n        # simulate reading from file\n        self.BytesIO_input = BytesIO(self.StringIO_input.read().encode(\""utf-8\""))""}",io.csv.time_read_stringcsv,time,0.013785636400901106
14993,func-level,"terminus-2,gpt-5",1.118500458692621,1.092596161613923,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_loc_array,indexing,"{""indexing.NumericSeriesIndexing.time_loc_array"": ""class NumericSeriesIndexing:\n    def time_loc_array(self, index, index_structure):\n        self.data.loc[self.array]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.time_loc_array,time,0.018319870635571438
14995,func-level,"terminus-2,gpt-5",1.0557091446616276,1.0881409032083509,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_loc_slice,indexing,"{""indexing.NumericSeriesIndexing.time_loc_slice"": ""class NumericSeriesIndexing:\n    def time_loc_slice(self, index, index_structure):\n        self.data.loc[:800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.time_loc_slice,time,-0.022936180018899088
14999,func-level,"terminus-2,gpt-5",1.0656415947558855,1.0503761610002444,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.csv.time_read_bytescsv,io.csv,"{""io.csv.ReadCSVEngine.time_read_bytescsv"": ""class ReadCSVEngine:\n    def time_read_bytescsv(self, engine):\n        read_csv(self.data(self.BytesIO_input), engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVEngine:\n    def setup(self, engine):\n        data = [\""A,B,C,D,E\""] + ([\""1,2,3,4,5\""] * 100000)\n        self.StringIO_input = StringIO(\""\\n\"".join(data))\n        # simulate reading from file\n        self.BytesIO_input = BytesIO(self.StringIO_input.read().encode(\""utf-8\""))""}",io.csv.time_read_bytescsv,time,0.010795922033692394
14996,func-level,"terminus-2,gpt-5",1.0829583470702462,1.0811531589465706,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing_engines.time_get_loc,indexing_engines,"{""indexing_engines.MaskedNumericEngineIndexing.time_get_loc"": ""class MaskedNumericEngineIndexing:\n    def time_get_loc(self, engine_and_dtype, index_type, unique, N):\n        self.data.get_loc(self.key_early)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())[::-1]\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.zeros(N * 3, dtype=dtype.lower())\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype.lower())\n                arr[N:] = np.arange(N * 2, dtype=dtype.lower())\n    \n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n            mask[-1] = True\n    \n        self.data = engine(BaseMaskedArray(arr, mask))\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]"", ""indexing_engines.NumericEngineIndexing.time_get_loc"": ""class NumericEngineIndexing:\n    def time_get_loc(self, engine_and_dtype, index_type, unique, N):\n        self.data.get_loc(self.key_early)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)[::-1]\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.empty(N * 3, dtype=dtype)\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype)\n                arr[N:] = np.arange(N * 2, dtype=dtype)\n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype)\n    \n        self.data = engine(arr)\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]"", ""indexing_engines.ObjectEngineIndexing.time_get_loc"": ""class ObjectEngineIndexing:\n    def time_get_loc(self, index_type):\n        self.data.get_loc(\""b\"")\n\n    def setup(self, index_type):\n        N = 10**5\n        values = list(\""a\"" * N + \""b\"" * N + \""c\"" * N)\n        arr = {\n            \""monotonic_incr\"": np.array(values, dtype=object),\n            \""monotonic_decr\"": np.array(list(reversed(values)), dtype=object),\n            \""non_monotonic\"": np.array(list(\""abc\"") * N, dtype=object),\n        }[index_type]\n    \n        self.data = libindex.ObjectEngine(arr)\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(\""b\"")""}",indexing_engines.time_get_loc,time,0.0012766535528115862
14998,func-level,"terminus-2,gpt-5",1.1446884577534786,1.121376298596207,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.csv.time_convert_post,io.csv,"{""io.csv.ReadCSVCategorical.time_convert_post"": ""class ReadCSVCategorical:\n    def time_convert_post(self, engine):\n        read_csv(self.fname, engine=engine).apply(Categorical)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVCategorical:\n    def setup(self, engine):\n        N = 100000\n        group1 = [\""aaaaaaaa\"", \""bbbbbbb\"", \""cccccccc\"", \""dddddddd\"", \""eeeeeeee\""]\n        df = DataFrame(np.random.choice(group1, (N, 3)), columns=list(\""abc\""))\n        df.to_csv(self.fname, index=False)""}",io.csv.time_convert_post,time,0.016486675500192068
14992,func-level,"terminus-2,gpt-5",1.1234814963034754,1.052174150069658,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_loc_all_scalars,indexing,"{""indexing.MultiIndexing.time_loc_all_scalars"": ""class MultiIndexing:\n    def time_loc_all_scalars(self, unique_levels):\n        target = tuple([self.tgt_scalar] * self.nlevels)\n        self.df.loc[target, :]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MultiIndexing:\n    def setup(self, unique_levels):\n        self.nlevels = 2\n        if unique_levels:\n            mi = MultiIndex.from_arrays([range(1000000)] * self.nlevels)\n        else:\n            mi = MultiIndex.from_product([range(1000)] * self.nlevels)\n        self.df = DataFrame(np.random.randn(len(mi)), index=mi)\n    \n        self.tgt_slice = slice(200, 800)\n        self.tgt_null_slice = slice(None)\n        self.tgt_list = list(range(0, 1000, 10))\n        self.tgt_scalar = 500\n    \n        bool_indexer = np.zeros(len(mi), dtype=np.bool_)\n        bool_indexer[slice(0, len(mi), 100)] = True\n        self.tgt_bool_indexer = bool_indexer""}",indexing.time_loc_all_scalars,time,0.05042952350340694
15001,func-level,"terminus-2,gpt-5",1.2186384323505546,1.1630325669190456,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.hdf.time_query_store_table,io.hdf,"{""io.hdf.HDFStoreDataFrame.time_query_store_table"": ""class HDFStoreDataFrame:\n    def time_query_store_table(self):\n        self.store.select(\""table\"", where=\""index > self.start and index < self.stop\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass HDFStoreDataFrame:\n    def setup(self):\n        N = 25000\n        index = tm.makeStringIndex(N)\n        self.df = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)}, index=index\n        )\n        self.df_mixed = DataFrame(\n            {\n                \""float1\"": np.random.randn(N),\n                \""float2\"": np.random.randn(N),\n                \""string1\"": [\""foo\""] * N,\n                \""bool1\"": [True] * N,\n                \""int1\"": np.random.randint(0, N, size=N),\n            },\n            index=index,\n        )\n        self.df_wide = DataFrame(np.random.randn(N, 100))\n        self.start_wide = self.df_wide.index[10000]\n        self.stop_wide = self.df_wide.index[15000]\n        self.df2 = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)},\n            index=date_range(\""1/1/2000\"", periods=N),\n        )\n        self.start = self.df2.index[10000]\n        self.stop = self.df2.index[15000]\n        self.df_wide2 = DataFrame(\n            np.random.randn(N, 100), index=date_range(\""1/1/2000\"", periods=N)\n        )\n        self.df_dc = DataFrame(\n            np.random.randn(N, 10), columns=[f\""C{i:03d}\"" for i in range(10)]\n        )\n    \n        self.fname = \""__test__.h5\""\n    \n        self.store = HDFStore(self.fname)\n        self.store.put(\""fixed\"", self.df)\n        self.store.put(\""fixed_mixed\"", self.df_mixed)\n        self.store.append(\""table\"", self.df2)\n        self.store.append(\""table_mixed\"", self.df_mixed)\n        self.store.append(\""table_wide\"", self.df_wide)\n        self.store.append(\""table_wide2\"", self.df_wide2)""}",io.hdf.time_query_store_table,time,0.03932522307744627
15004,func-level,"terminus-2,gpt-5",1.0329203791619326,1.0234459712234998,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.json.time_to_json,io.json,"{""io.json.ToJSON.time_to_json"": ""class ToJSON:\n    def time_to_json(self, orient, frame):\n        getattr(self, frame).to_json(self.fname, orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSON:\n    def setup(self, orient, frame):\n        N = 10**5\n        ncols = 5\n        index = date_range(\""20000101\"", periods=N, freq=\""H\"")\n        timedeltas = timedelta_range(start=1, periods=N, freq=\""s\"")\n        datetimes = date_range(start=1, periods=N, freq=\""s\"")\n        ints = np.random.randint(100000000, size=N)\n        longints = sys.maxsize * np.random.randint(100000000, size=N)\n        floats = np.random.randn(N)\n        strings = tm.makeStringIndex(N)\n        self.df = DataFrame(np.random.randn(N, ncols), index=np.arange(N))\n        self.df_date_idx = DataFrame(np.random.randn(N, ncols), index=index)\n        self.df_td_int_ts = DataFrame(\n            {\n                \""td_1\"": timedeltas,\n                \""td_2\"": timedeltas,\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""ts_1\"": datetimes,\n                \""ts_2\"": datetimes,\n            },\n            index=index,\n        )\n        self.df_int_floats = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""int_3\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""float_3\"": floats,\n            },\n            index=index,\n        )\n        self.df_int_float_str = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )\n    \n        self.df_longint_float_str = DataFrame(\n            {\n                \""longint_1\"": longints,\n                \""longint_2\"": longints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )"", ""io.json.ToJSONWide.time_to_json"": ""class ToJSON:\n    def time_to_json(self, orient, frame):\n        getattr(self, frame).to_json(self.fname, orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSONWide:\n    def setup(self, orient, frame):\n        super().setup(orient, frame)\n        base_df = getattr(self, frame).copy()\n        df_wide = concat([base_df.iloc[:100]] * 1000, ignore_index=True, axis=1)\n        self.df_wide = df_wide""}",io.json.time_to_json,time,0.006700429942314565
15003,func-level,"terminus-2,gpt-5",1.0318322167913276,1.0532229372599846,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.json.time_normalize_json,io.json,"{""io.json.NormalizeJSON.time_normalize_json"": ""class NormalizeJSON:\n    def time_normalize_json(self, orient, frame):\n        json_normalize(self.data)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NormalizeJSON:\n    def setup(self, orient, frame):\n        data = {\n            \""hello\"": [\""thisisatest\"", 999898, \""mixed types\""],\n            \""nest1\"": {\""nest2\"": {\""nest3\"": \""nest3_value\"", \""nest3_int\"": 3445}},\n            \""nest1_list\"": {\""nest2\"": [\""blah\"", 32423, 546456.876, 92030234]},\n            \""hello2\"": \""string\"",\n        }\n        self.data = [data for i in range(10000)]""}",io.json.time_normalize_json,time,-0.015127807969347226
15006,func-level,"terminus-2,gpt-5",1.0545492694538523,1.0472118667839048,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.sql.time_read_sql_query_select_column,io.sql,"{""io.sql.WriteSQLDtypes.time_read_sql_query_select_column"": ""class WriteSQLDtypes:\n    def time_read_sql_query_select_column(self, connection, dtype):\n        read_sql_query(self.query_col, self.con)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass WriteSQLDtypes:\n    def setup(self, connection, dtype):\n        N = 10000\n        con = {\n            \""sqlalchemy\"": create_engine(\""sqlite:///:memory:\""),\n            \""sqlite\"": sqlite3.connect(\"":memory:\""),\n        }\n        self.table_name = \""test_type\""\n        self.query_col = f\""SELECT {dtype} FROM {self.table_name}\""\n        self.con = con[connection]\n        self.df = DataFrame(\n            {\n                \""float\"": np.random.randn(N),\n                \""float_with_nan\"": np.random.randn(N),\n                \""string\"": [\""foo\""] * N,\n                \""bool\"": [True] * N,\n                \""int\"": np.random.randint(0, N, size=N),\n                \""datetime\"": date_range(\""2000-01-01\"", periods=N, freq=\""s\""),\n            },\n            index=tm.makeStringIndex(N),\n        )\n        self.df.iloc[1000:3000, 1] = np.nan\n        self.df[\""date\""] = self.df[\""datetime\""].dt.date\n        self.df[\""time\""] = self.df[\""datetime\""].dt.time\n        self.df[\""datetime_string\""] = self.df[\""datetime\""].astype(str)\n        self.df.to_sql(self.table_name, self.con, if_exists=\""replace\"")""}",io.sql.time_read_sql_query_select_column,time,0.0051891107991142394
15002,func-level,"terminus-2,gpt-5",1.034878133189342,1.0151594714925496,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.json.time_delta_int_tstamp_lines,io.json,"{""io.json.ToJSONLines.time_delta_int_tstamp_lines"": ""class ToJSONLines:\n    def time_delta_int_tstamp_lines(self):\n        self.df_td_int_ts.to_json(self.fname, orient=\""records\"", lines=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSONLines:\n    def setup(self):\n        N = 10**5\n        ncols = 5\n        index = date_range(\""20000101\"", periods=N, freq=\""H\"")\n        timedeltas = timedelta_range(start=1, periods=N, freq=\""s\"")\n        datetimes = date_range(start=1, periods=N, freq=\""s\"")\n        ints = np.random.randint(100000000, size=N)\n        longints = sys.maxsize * np.random.randint(100000000, size=N)\n        floats = np.random.randn(N)\n        strings = tm.makeStringIndex(N)\n        self.df = DataFrame(np.random.randn(N, ncols), index=np.arange(N))\n        self.df_date_idx = DataFrame(np.random.randn(N, ncols), index=index)\n        self.df_td_int_ts = DataFrame(\n            {\n                \""td_1\"": timedeltas,\n                \""td_2\"": timedeltas,\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""ts_1\"": datetimes,\n                \""ts_2\"": datetimes,\n            },\n            index=index,\n        )\n        self.df_int_floats = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""int_3\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""float_3\"": floats,\n            },\n            index=index,\n        )\n        self.df_int_float_str = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )\n        self.df_longint_float_str = DataFrame(\n            {\n                \""longint_1\"": longints,\n                \""longint_2\"": longints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )""}",io.json.time_delta_int_tstamp_lines,time,0.013945305301833362
15005,func-level,"terminus-2,gpt-5",0.9564595661905344,0.9525199004060194,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.parsers.time_check_datetimes,io.parsers,"{""io.parsers.DoesStringLookLikeDatetime.time_check_datetimes"": ""class DoesStringLookLikeDatetime:\n    def time_check_datetimes(self, value):\n        for obj in self.objects:\n            _does_string_look_like_datetime(obj)\n\n    def setup(self, value):\n        self.objects = [value] * 1000000""}",io.parsers.time_check_datetimes,time,0.002786185137563642
14997,func-level,"terminus-2,gpt-5",1.0491460156384689,1.0153873906209916,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing_engines.time_get_loc_near_middle,indexing_engines,"{""indexing_engines.MaskedNumericEngineIndexing.time_get_loc_near_middle"": ""class MaskedNumericEngineIndexing:\n    def time_get_loc_near_middle(self, engine_and_dtype, index_type, unique, N):\n        # searchsorted performance may be different near the middle of a range\n        #  vs near an endpoint\n        self.data.get_loc(self.key_middle)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())[::-1]\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.zeros(N * 3, dtype=dtype.lower())\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype.lower())\n                arr[N:] = np.arange(N * 2, dtype=dtype.lower())\n    \n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n            mask[-1] = True\n    \n        self.data = engine(BaseMaskedArray(arr, mask))\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]"", ""indexing_engines.NumericEngineIndexing.time_get_loc_near_middle"": ""class NumericEngineIndexing:\n    def time_get_loc_near_middle(self, engine_and_dtype, index_type, unique, N):\n        # searchsorted performance may be different near the middle of a range\n        #  vs near an endpoint\n        self.data.get_loc(self.key_middle)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)[::-1]\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.empty(N * 3, dtype=dtype)\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype)\n                arr[N:] = np.arange(N * 2, dtype=dtype)\n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype)\n    \n        self.data = engine(arr)\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]""}",indexing_engines.time_get_loc_near_middle,time,0.02387455800387361
15007,func-level,"terminus-2,gpt-5",1.0612458037563528,1.0651493160553005,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,join_merge.time_on_int32,join_merge,"{""join_merge.MergeAsof.time_on_int32"": ""class MergeAsof:\n    def time_on_int32(self, direction, tolerance):\n        merge_asof(\n            self.df1d, self.df2d, on=\""time32\"", direction=direction, tolerance=tolerance\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MergeAsof:\n    def setup(self, direction, tolerance):\n        one_count = 200000\n        two_count = 1000000\n    \n        df1 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, one_count / 20, one_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), one_count),\n                \""key2\"": np.random.randint(0, 25, one_count),\n                \""value1\"": np.random.randn(one_count),\n            }\n        )\n        df2 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, two_count / 20, two_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), two_count),\n                \""key2\"": np.random.randint(0, 25, two_count),\n                \""value2\"": np.random.randn(two_count),\n            }\n        )\n    \n        df1 = df1.sort_values(\""time\"")\n        df2 = df2.sort_values(\""time\"")\n    \n        df1[\""time32\""] = np.int32(df1.time)\n        df2[\""time32\""] = np.int32(df2.time)\n    \n        df1[\""timeu64\""] = np.uint64(df1.time)\n        df2[\""timeu64\""] = np.uint64(df2.time)\n    \n        self.df1a = df1[[\""time\"", \""value1\""]]\n        self.df2a = df2[[\""time\"", \""value2\""]]\n        self.df1b = df1[[\""time\"", \""key\"", \""value1\""]]\n        self.df2b = df2[[\""time\"", \""key\"", \""value2\""]]\n        self.df1c = df1[[\""time\"", \""key2\"", \""value1\""]]\n        self.df2c = df2[[\""time\"", \""key2\"", \""value2\""]]\n        self.df1d = df1[[\""time32\"", \""value1\""]]\n        self.df2d = df2[[\""time32\"", \""value2\""]]\n        self.df1e = df1[[\""time\"", \""key\"", \""key2\"", \""value1\""]]\n        self.df2e = df2[[\""time\"", \""key\"", \""key2\"", \""value2\""]]\n        self.df1f = df1[[\""timeu64\"", \""value1\""]]\n        self.df2f = df2[[\""timeu64\"", \""value2\""]]""}",join_merge.time_on_int32,time,-0.002760616901660324
15011,func-level,"terminus-2,gpt-5",1.0433120478808708,1.0285263308926698,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.time_explode,reshape,"{""reshape.Explode.time_explode"": ""class Explode:\n    def time_explode(self, n_rows, max_list_length):\n        self.series.explode()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Explode:\n    def setup(self, n_rows, max_list_length):\n        data = [np.arange(np.random.randint(max_list_length)) for _ in range(n_rows)]\n        self.series = pd.Series(data)""}",reshape.time_explode,time,0.010456659821924386
15013,func-level,"terminus-2,gpt-5",1.1295411658667804,1.1391285843457797,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.time_pivot_table_categorical,reshape,"{""reshape.PivotTable.time_pivot_table_categorical"": ""class PivotTable:\n    def time_pivot_table_categorical(self):\n        self.df2.pivot_table(\n            index=\""col1\"", values=\""col3\"", columns=\""col2\"", aggfunc=np.sum, fill_value=0\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")""}",reshape.time_pivot_table_categorical,time,-0.006780352531116863
15009,func-level,"terminus-2,gpt-5",0.922564721858806,0.9662950819021658,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,multiindex_object.time_is_monotonic,multiindex_object,"{""multiindex_object.Integer.time_is_monotonic"": ""class Integer:\n    def time_is_monotonic(self):\n        self.mi_int.is_monotonic_increasing\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Integer:\n    def setup(self):\n        self.mi_int = MultiIndex.from_product(\n            [np.arange(1000), np.arange(1000)], names=[\""one\"", \""two\""]\n        )\n        self.obj_index = np.array(\n            [\n                (0, 10),\n                (0, 11),\n                (0, 12),\n                (0, 13),\n                (0, 14),\n                (0, 15),\n                (0, 16),\n                (0, 17),\n                (0, 18),\n                (0, 19),\n            ],\n            dtype=object,\n        )\n        self.other_mi_many_mismatches = MultiIndex.from_tuples(\n            [\n                (-7, 41),\n                (-2, 3),\n                (-0.7, 5),\n                (0, 0),\n                (0, 1.5),\n                (0, 340),\n                (0, 1001),\n                (1, -4),\n                (1, 20),\n                (1, 1040),\n                (432, -5),\n                (432, 17),\n                (439, 165.5),\n                (998, -4),\n                (998, 24065),\n                (999, 865.2),\n                (999, 1000),\n                (1045, -843),\n            ]\n        )""}",multiindex_object.time_is_monotonic,time,-0.030926704415388855
15014,func-level,"terminus-2,gpt-5",1.2298699254682994,1.2283349290061478,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.time_pivot_table_categorical_observed,reshape,"{""reshape.PivotTable.time_pivot_table_categorical_observed"": ""class PivotTable:\n    def time_pivot_table_categorical_observed(self):\n        self.df2.pivot_table(\n            index=\""col1\"",\n            values=\""col3\"",\n            columns=\""col2\"",\n            aggfunc=np.sum,\n            fill_value=0,\n            observed=True,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")""}",reshape.time_pivot_table_categorical_observed,time,0.0010855703409841622
15010,func-level,"terminus-2,gpt-5",1.134055805843431,1.123214383263225,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.time_cut_timedelta,reshape,"{""reshape.Cut.time_cut_timedelta"": ""class Cut:\n    def time_cut_timedelta(self, bins):\n        pd.cut(self.timedelta_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))""}",reshape.time_cut_timedelta,time,0.007667201258985727
15018,func-level,"terminus-2,gpt-5",1.0324992733073688,1.024862591515639,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.time_qcut_float,reshape,"{""reshape.Cut.time_qcut_float"": ""class Cut:\n    def time_qcut_float(self, bins):\n        pd.qcut(self.float_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))""}",reshape.time_qcut_float,time,0.005400765057800365
15008,func-level,"terminus-2,gpt-5",1.0027976053603749,1.0420834684650648,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,libs.time_is_list_like,libs,"{""libs.ScalarListLike.time_is_list_like"": ""class ScalarListLike:\n    def time_is_list_like(self, param):\n        is_list_like(param)""}",libs.time_is_list_like,time,-0.027783495830756687
15015,func-level,"terminus-2,gpt-5",1.105180383236151,1.085041769248503,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.time_pivot_table_margins,reshape,"{""reshape.PivotTable.time_pivot_table_margins"": ""class PivotTable:\n    def time_pivot_table_margins(self):\n        self.df.pivot_table(index=\""key1\"", columns=[\""key2\"", \""key3\""], margins=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")""}",reshape.time_pivot_table_margins,time,0.014242301264248972
15020,func-level,"terminus-2,gpt-5",1.064465515071442,1.0345564193262995,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.time_wide_to_long_big,reshape,"{""reshape.WideToLong.time_wide_to_long_big"": ""class WideToLong:\n    def time_wide_to_long_big(self):\n        wide_to_long(self.df, self.letters, i=\""id\"", j=\""year\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass WideToLong:\n    def setup(self):\n        nyrs = 20\n        nidvars = 20\n        N = 5000\n        self.letters = list(\""ABCD\"")\n        yrvars = [\n            letter + str(num)\n            for letter, num in product(self.letters, range(1, nyrs + 1))\n        ]\n        columns = [str(i) for i in range(nidvars)] + yrvars\n        self.df = DataFrame(np.random.randn(N, nidvars + len(yrvars)), columns=columns)\n        self.df[\""id\""] = self.df.index""}",reshape.time_wide_to_long_big,time,0.021152118631642465
15016,func-level,"terminus-2,gpt-5",1.0568596542514763,1.0645771443999537,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.time_pivot_table_margins_only_column,reshape,"{""reshape.PivotTable.time_pivot_table_margins_only_column"": ""class PivotTable:\n    def time_pivot_table_margins_only_column(self):\n        self.df.pivot_table(columns=[\""key1\"", \""key2\"", \""key3\""], margins=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")""}",reshape.time_pivot_table_margins_only_column,time,-0.00545791382494861
15019,func-level,"terminus-2,gpt-5",1.2730663879986164,1.4175765820288715,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.time_stack,reshape,"{""reshape.ReshapeExtensionDtype.time_stack"": ""class ReshapeExtensionDtype:\n    def time_stack(self, dtype):\n        self.df.stack()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReshapeExtensionDtype:\n    def setup(self, dtype):\n        lev = pd.Index(list(\""ABCDEFGHIJ\""))\n        ri = pd.Index(range(1000))\n        mi = MultiIndex.from_product([lev, ri], names=[\""foo\"", \""bar\""])\n    \n        index = date_range(\""2016-01-01\"", periods=10000, freq=\""s\"", tz=\""US/Pacific\"")\n        if dtype == \""Period[s]\"":\n            index = index.tz_localize(None).to_period(\""s\"")\n    \n        ser = pd.Series(index, index=mi)\n        df = ser.unstack(\""bar\"")\n        # roundtrips -> df.stack().equals(ser)\n    \n        self.ser = ser\n        self.df = df"", ""reshape.SimpleReshape.time_stack"": ""class SimpleReshape:\n    def time_stack(self):\n        self.udf.stack()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SimpleReshape:\n    def setup(self):\n        arrays = [np.arange(100).repeat(100), np.roll(np.tile(np.arange(100), 100), 25)]\n        index = MultiIndex.from_arrays(arrays)\n        self.df = DataFrame(np.random.randn(10000, 4), index=index)\n        self.udf = self.df.unstack(1)""}",reshape.time_stack,time,-0.10219957144996826
15023,func-level,"terminus-2,gpt-5",1.0528150478903315,1.0461862350524451,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.time_groupby_mean,rolling,"{""rolling.GroupbyEWMEngine.time_groupby_mean"": ""class GroupbyEWMEngine:\n    def time_groupby_mean(self, engine):\n        self.gb_ewm.mean(engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupbyEWMEngine:\n    def setup(self, engine):\n        df = pd.DataFrame({\""A\"": range(50), \""B\"": range(50)})\n        self.gb_ewm = df.groupby(\""A\"").ewm(com=1.0)""}",rolling.time_groupby_mean,time,0.004687986448293077
15026,func-level,"terminus-2,gpt-5",1.0564919755569504,1.0529958245106887,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.time_pairwise,rolling,"{""rolling.Pairwise.time_pairwise"": ""class Pairwise:\n    def time_pairwise(self, kwargs_window, method, pairwise):\n        getattr(self.window, method)(self.df, pairwise=pairwise)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Pairwise:\n    def setup(self, kwargs_window, method, pairwise):\n        N = 10**4\n        n_groups = 20\n        kwargs, window = kwargs_window\n        groups = [i for _ in range(N // n_groups) for i in range(n_groups)]\n        arr = np.random.random(N)\n        self.df = pd.DataFrame(arr)\n        self.window = getattr(self.df, window)(**kwargs)\n        self.window_group = getattr(\n            pd.DataFrame({\""A\"": groups, \""B\"": arr}).groupby(\""A\""), window\n        )(**kwargs)""}",rolling.time_pairwise,time,0.002472525492405703
15030,func-level,"terminus-2,gpt-5",1.100478784705483,1.0835962245212107,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.time_rolling_multiindex_creation,rolling,"{""rolling.GroupbyLargeGroups.time_rolling_multiindex_creation"": ""class GroupbyLargeGroups:\n    def time_rolling_multiindex_creation(self):\n        self.df.groupby(\""A\"").rolling(3).mean()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupbyLargeGroups:\n    def setup(self):\n        N = 100000\n        self.df = pd.DataFrame({\""A\"": [1, 2] * (N // 2), \""B\"": np.random.randn(N)})""}",rolling.time_rolling_multiindex_creation,time,0.01193957580217282
15027,func-level,"terminus-2,gpt-5",1.035932610315794,1.0647197439195957,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.time_quantile,rolling,"{""rolling.Quantile.time_quantile"": ""class Quantile:\n    def time_quantile(self, constructor, window, dtype, percentile, interpolation):\n        self.roll.quantile(percentile, interpolation=interpolation)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, constructor, window, dtype, percentile, interpolation):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)""}",rolling.time_quantile,time,-0.020358651770722557
15025,func-level,"terminus-2,gpt-5",1.055183755944308,1.0581820415478922,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.time_method,rolling,"{""rolling.Groupby.time_method"": ""class Groupby:\n    def time_method(self, method, window_kwargs):\n        getattr(self.groupby_window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Groupby:\n    def setup(self, method, window_kwargs):\n        N = 1000\n        window, kwargs = window_kwargs\n        df = pd.DataFrame(\n            {\n                \""A\"": [str(i) for i in range(N)] * 10,\n                \""B\"": list(range(N)) * 10,\n            }\n        )\n        if isinstance(kwargs.get(\""window\"", None), str):\n            df.index = pd.date_range(start=\""1900-01-01\"", freq=\""1min\"", periods=N * 10)\n        self.groupby_window = getattr(df.groupby(\""A\""), window)(**kwargs)"", ""rolling.Methods.time_method"": ""class Methods:\n    def time_method(self, constructor, window_kwargs, dtype, method):\n        getattr(self.window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Methods:\n    def setup(self, constructor, window_kwargs, dtype, method):\n        N = 10**5\n        window, kwargs = window_kwargs\n        arr = (100 * np.random.random(N)).astype(dtype)\n        obj = getattr(pd, constructor)(arr)\n        self.window = getattr(obj, window)(**kwargs)"", ""rolling.VariableWindowMethods.time_method"": ""class Methods:\n    def time_method(self, constructor, window_kwargs, dtype, method):\n        getattr(self.window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass VariableWindowMethods:\n    def setup(self, constructor, window, dtype, method):\n        N = 10**5\n        arr = (100 * np.random.random(N)).astype(dtype)\n        index = pd.date_range(\""2017-01-01\"", periods=N, freq=\""5s\"")\n        self.window = getattr(pd, constructor)(arr, index=index).rolling(window)""}",rolling.time_method,time,-0.0021204282910779154
15022,func-level,"terminus-2,gpt-5",1.057727120900387,1.0570098215679355,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.time_ewm,rolling,"{""rolling.EWMMethods.time_ewm"": ""class EWMMethods:\n    def time_ewm(self, constructor, kwargs_method, dtype):\n        getattr(self.ewm, self.method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass EWMMethods:\n    def setup(self, constructor, kwargs_method, dtype):\n        N = 10**5\n        kwargs, method = kwargs_method\n        arr = (100 * np.random.random(N)).astype(dtype)\n        self.method = method\n        self.ewm = getattr(pd, constructor)(arr).ewm(**kwargs)""}",rolling.time_ewm,time,0.0005072838277590868
15028,func-level,"terminus-2,gpt-5",1.0158117231477402,1.0235342956125444,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.time_rank,rolling,"{""rolling.Rank.time_rank"": ""class Rank:\n    def time_rank(self, constructor, window, dtype, percentile, ascending, method):\n        self.roll.rank(pct=percentile, ascending=ascending, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Rank:\n    def setup(self, constructor, window, dtype, percentile, ascending, method):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)""}",rolling.time_rank,time,-0.005461508108065229
15024,func-level,"terminus-2,gpt-5",1.0827575908690534,1.1082312294361656,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.time_groupby_method,rolling,"{""rolling.GroupbyEWM.time_groupby_method"": ""class GroupbyEWM:\n    def time_groupby_method(self, method):\n        getattr(self.gb_ewm, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupbyEWM:\n    def setup(self, method):\n        df = pd.DataFrame({\""A\"": range(50), \""B\"": range(50)})\n        self.gb_ewm = df.groupby(\""A\"").ewm(com=1.0)""}",rolling.time_groupby_method,time,-0.018015303088481018
15017,func-level,"terminus-2,gpt-5",1.1210531193239583,1.1130333586673145,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.time_qcut_datetime,reshape,"{""reshape.Cut.time_qcut_datetime"": ""class Cut:\n    def time_qcut_datetime(self, bins):\n        pd.qcut(self.datetime_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))""}",reshape.time_qcut_datetime,time,0.005671683632704309
15021,func-level,"terminus-2,gpt-5",1.059443156912705,1.060642811266571,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.time_apply,rolling,"{""rolling.TableMethod.time_apply"": ""class TableMethod:\n    def time_apply(self, method):\n        self.df.rolling(2, method=method).apply(\n            table_method_func, raw=True, engine=\""numba\""\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TableMethod:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(10, 1000))""}",rolling.time_apply,time,-0.0008484118485614789
15012,func-level,"terminus-2,gpt-5",1.1141466394257598,1.1113814191673432,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.time_melt_dataframe,reshape,"{""reshape.Melt.time_melt_dataframe"": ""class Melt:\n    def time_melt_dataframe(self, dtype):\n        melt(self.df, id_vars=[\""id1\"", \""id2\""])\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Melt:\n    def setup(self, dtype):\n        self.df = DataFrame(\n            np.random.randn(100_000, 3), columns=[\""A\"", \""B\"", \""C\""], dtype=dtype\n        )\n        self.df[\""id1\""] = pd.Series(np.random.randint(0, 10, 10000))\n        self.df[\""id2\""] = pd.Series(np.random.randint(100, 1000, 10000))""}",reshape.time_melt_dataframe,time,0.0019556013142973997
15032,func-level,"terminus-2,gpt-5",1.1084518137171673,1.1976886276828178,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.time_fillna,series_methods,"{""series_methods.Fillna.time_fillna"": ""class Fillna:\n    def time_fillna(self, dtype, method):\n        value = self.fill_value if method is None else None\n        self.ser.fillna(value=value, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Fillna:\n    def setup(self, dtype, method):\n        N = 10**6\n        if dtype == \""datetime64[ns]\"":\n            data = date_range(\""2000-01-01\"", freq=\""S\"", periods=N)\n            na_value = NaT\n        elif dtype in (\""float64\"", \""Float64\""):\n            data = np.random.randn(N)\n            na_value = np.nan\n        elif dtype in (\""Int64\"", \""int64[pyarrow]\""):\n            data = np.arange(N)\n            na_value = NA\n        elif dtype in (\""string\"", \""string[pyarrow]\""):\n            data = tm.rands_array(5, N)\n            na_value = NA\n        else:\n            raise NotImplementedError\n        fill_value = data[0]\n        ser = Series(data, dtype=dtype)\n        ser[::2] = na_value\n        self.ser = ser\n        self.fill_value = fill_value""}",series_methods.time_fillna,time,-0.0631094865386496
15035,func-level,"terminus-2,gpt-5",0.9126102627268862,0.9048225669589512,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.time_add,sparse,"{""sparse.Arithmetic.time_add"": ""class Arithmetic:\n    def time_add(self, dense_proportion, fill_value):\n        self.array1 + self.array2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)""}",sparse.time_add,time,0.005507564192316069
15029,func-level,"terminus-2,gpt-5",1.0396835159161013,1.053185363100278,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.time_rolling,rolling,"{""rolling.Apply.time_rolling"": ""class Apply:\n    def time_rolling(self, constructor, window, dtype, function, raw):\n        self.roll.apply(function, raw=raw)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, constructor, window, dtype, function, raw):\n        N = 10**3\n        arr = (100 * np.random.random(N)).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)"", ""rolling.ForwardWindowMethods.time_rolling"": ""class ForwardWindowMethods:\n    def time_rolling(self, constructor, window_size, dtype, method):\n        getattr(self.roll, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ForwardWindowMethods:\n    def setup(self, constructor, window_size, dtype, method):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=window_size)\n        self.roll = getattr(pd, constructor)(arr).rolling(window=indexer)""}",rolling.time_rolling,time,-0.00954868966349137
15044,func-level,"terminus-2,gpt-5",1.025119076680235,1.0221869382018522,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.time_contains,strings,"{""strings.Contains.time_contains"": ""class Contains:\n    def time_contains(self, dtype, regex):\n        self.s.str.contains(\""A\"", regex=regex)\n\n    def setup(self, dtype, regex):\n        super().setup(dtype)""}",strings.time_contains,time,0.0020736481459566448
15034,func-level,"terminus-2,gpt-5",1.0410190222734892,1.0623573846427314,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.time_searchsorted,series_methods,"{""series_methods.SearchSorted.time_searchsorted"": ""class SearchSorted:\n    def time_searchsorted(self, dtype):\n        key = \""2\"" if dtype == \""str\"" else 2\n        self.s.searchsorted(key)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SearchSorted:\n    def setup(self, dtype):\n        N = 10**5\n        data = np.array([1] * N + [2] * N + [3] * N).astype(dtype)\n        self.s = Series(data)""}",series_methods.time_searchsorted,time,-0.01509077961049664
15040,func-level,"terminus-2,gpt-5",1.0250425473131854,1.0425458957734757,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,stat_ops.time_corr_series,stat_ops,"{""stat_ops.Correlation.time_corr_series"": ""class Correlation:\n    def time_corr_series(self, method):\n        self.s.corr(self.s2, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Correlation:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(500, 15))\n        self.df2 = pd.DataFrame(np.random.randn(500, 15))\n        self.df_wide = pd.DataFrame(np.random.randn(500, 100))\n        self.df_wide_nans = self.df_wide.where(np.random.random((500, 100)) < 0.9)\n        self.s = pd.Series(np.random.randn(500))\n        self.s2 = pd.Series(np.random.randn(500))""}",stat_ops.time_corr_series,time,-0.012378605700346695
15045,func-level,"terminus-2,gpt-5",1.0308279016595705,1.037615166606883,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.time_endswith,strings,"{""strings.Methods.time_endswith"": ""class Methods:\n    def time_endswith(self, dtype):\n        self.s.str.endswith(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.time_endswith,time,-0.004800045931621224
15036,func-level,"terminus-2,gpt-5",0.9122514023207678,0.912691752252956,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.time_addition,sparse,"{""sparse.ArithmeticBlock.time_addition"": ""class ArithmeticBlock:\n    def time_addition(self, fill_value):\n        self.arr1 + self.arr2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )""}",sparse.time_addition,time,-0.0003114214513352978
15033,func-level,"terminus-2,gpt-5",1.0584044800057109,1.0597253376016218,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.time_func,series_methods,"{""series_methods.NanOps.time_func"": ""class NanOps:\n    def time_func(self, func, N, dtype):\n        self.func()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NanOps:\n    def setup(self, func, N, dtype):\n        if func == \""argmax\"" and dtype in {\""Int64\"", \""boolean\""}:\n            # Skip argmax for nullable int since this doesn't work yet (GH-24382)\n            raise NotImplementedError\n        self.s = Series([1] * N, dtype=dtype)\n        self.func = getattr(self.s, func)""}",series_methods.time_func,time,-0.0009341284270940173
15039,func-level,"terminus-2,gpt-5",0.911450853589588,0.9104927395620972,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.time_make_union,sparse,"{""sparse.Arithmetic.time_make_union"": ""class Arithmetic:\n    def time_make_union(self, dense_proportion, fill_value):\n        self.array1.sp_index.make_union(self.array2.sp_index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)"", ""sparse.ArithmeticBlock.time_make_union"": ""class ArithmeticBlock:\n    def time_make_union(self, fill_value):\n        self.arr1.sp_index.make_union(self.arr2.sp_index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )""}",sparse.time_make_union,time,0.0006775912499934918
15043,func-level,"terminus-2,gpt-5",1.0228934894927442,1.029138406425122,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,strftime.time_frame_offset_str,strftime,"{""strftime.BusinessHourStrftime.time_frame_offset_str"": ""class BusinessHourStrftime:\n    def time_frame_offset_str(self, obs):\n        self.data[\""off\""].apply(str)\n\n    def setup(self, obs):\n        self.data = pd.DataFrame(\n            {\n                \""off\"": [offsets.BusinessHour()] * obs,\n            }\n        )""}",strftime.time_frame_offset_str,time,-0.004416490051186664
15031,func-level,"terminus-2,gpt-5",1.0252508663975362,1.0118678655169218,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.time_constructor_dict,series_methods,"{""series_methods.SeriesConstructor.time_constructor_dict"": ""class SeriesConstructor:\n    def time_constructor_dict(self):\n        Series(data=self.data, index=self.idx)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesConstructor:\n    def setup(self):\n        self.idx = date_range(\n            start=datetime(2015, 10, 26), end=datetime(2016, 1, 1), freq=\""50s\""\n        )\n        self.data = dict(zip(self.idx, range(len(self.idx))))\n        self.array = np.array([1, 2, 3])\n        self.idx2 = Index([\""a\"", \""b\"", \""c\""])""}",series_methods.time_constructor_dict,time,0.009464639943857421
15041,func-level,"terminus-2,gpt-5",1.0522608491131085,1.036917833151236,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,stat_ops.time_op,stat_ops,"{""stat_ops.FrameMultiIndexOps.time_op"": ""class FrameMultiIndexOps:\n    def time_op(self, op):\n        self.df_func()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FrameMultiIndexOps:\n    def setup(self, op):\n        levels = [np.arange(10), np.arange(100), np.arange(100)]\n        codes = [\n            np.arange(10).repeat(10000),\n            np.tile(np.arange(100).repeat(100), 10),\n            np.tile(np.tile(np.arange(100), 100), 10),\n        ]\n        index = pd.MultiIndex(levels=levels, codes=codes)\n        df = pd.DataFrame(np.random.randn(len(index), 4), index=index)\n        self.df_func = getattr(df, op)"", ""stat_ops.FrameOps.time_op"": ""class FrameOps:\n    def time_op(self, op, dtype, axis):\n        self.df_func(axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FrameOps:\n    def setup(self, op, dtype, axis):\n        values = np.random.randn(100000, 4)\n        if dtype == \""Int64\"":\n            values = values.astype(int)\n        df = pd.DataFrame(values).astype(dtype)\n        self.df_func = getattr(df, op)"", ""stat_ops.SeriesOps.time_op"": ""class SeriesOps:\n    def time_op(self, op, dtype):\n        self.s_func()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesOps:\n    def setup(self, op, dtype):\n        s = pd.Series(np.random.randn(100000)).astype(dtype)\n        self.s_func = getattr(s, op)""}",stat_ops.time_op,time,0.010850789223389234
15037,func-level,"terminus-2,gpt-5",0.9091904913610988,0.91583897802989,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.time_divide,sparse,"{""sparse.Arithmetic.time_divide"": ""class Arithmetic:\n    def time_divide(self, dense_proportion, fill_value):\n        self.array1 / self.array2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)""}",sparse.time_divide,time,-0.004701900048650109
15042,func-level,"terminus-2,gpt-5",1.0278443416482657,1.033114634096658,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,strftime.time_frame_offset_repr,strftime,"{""strftime.BusinessHourStrftime.time_frame_offset_repr"": ""class BusinessHourStrftime:\n    def time_frame_offset_repr(self, obs):\n        self.data[\""off\""].apply(repr)\n\n    def setup(self, obs):\n        self.data = pd.DataFrame(\n            {\n                \""off\"": [offsets.BusinessHour()] * obs,\n            }\n        )""}",strftime.time_frame_offset_repr,time,-0.0037272223821727566
15046,func-level,"terminus-2,gpt-5",1.017258746144939,1.0435889263128073,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.time_fullmatch,strings,"{""strings.Methods.time_fullmatch"": ""class Methods:\n    def time_fullmatch(self, dtype):\n        self.s.str.fullmatch(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.time_fullmatch,time,-0.01862106093908654
15038,func-level,"terminus-2,gpt-5",0.909030324994456,0.9086828083158498,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.time_division,sparse,"{""sparse.ArithmeticBlock.time_division"": ""class ArithmeticBlock:\n    def time_division(self, fill_value):\n        self.arr1 / self.arr2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )""}",sparse.time_division,time,0.0002457685138657843
15047,func-level,"terminus-2,gpt-5",1.0164370423138829,1.0196207599438951,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.time_get,strings,"{""strings.Methods.time_get"": ""class Methods:\n    def time_get(self, dtype):\n        self.s.str.get(0)\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.time_get,time,-0.0022515683380567465
15051,func-level,"terminus-2,gpt-5",0.9930445623907804,0.9911992258081832,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.time_timeseries_is_month_start,timeseries,"{""timeseries.DatetimeIndex.time_timeseries_is_month_start"": ""class DatetimeIndex:\n    def time_timeseries_is_month_start(self, index_type):\n        self.index.is_month_start\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]""}",timeseries.time_timeseries_is_month_start,time,0.0013050470881168484
15048,func-level,"terminus-2,gpt-5",1.2603851718908452,1.3086013614510008,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.time_lower,strings,"{""strings.Methods.time_lower"": ""class Methods:\n    def time_lower(self, dtype):\n        self.s.str.lower()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.time_lower,time,-0.03409914396050609
15050,func-level,"terminus-2,gpt-5",1.0256753129131535,1.0312078855701563,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.time_wrap,strings,"{""strings.Methods.time_wrap"": ""class Methods:\n    def time_wrap(self, dtype):\n        self.s.str.wrap(10)\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.time_wrap,time,-0.003912710507074115
15054,func-level,"terminus-2,gpt-5",1.012088734909618,1.0300157447443716,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.fields.time_get_start_end_field,tslibs.fields,"{""tslibs.fields.TimeGetStartEndField.time_get_start_end_field"": ""class TimeGetStartEndField:\n    def time_get_start_end_field(self, size, side, period, freqstr, month_kw):\n        get_start_end_field(self.i8data, self.attrname, freqstr, month_kw=month_kw)\n\n    def setup(self, size, side, period, freqstr, month_kw):\n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr\n    \n        self.attrname = f\""is_{period}_{side}\""""}",tslibs.fields.time_get_start_end_field,time,-0.01267822477705354
15052,func-level,"terminus-2,gpt-5",1.0147936886724054,1.010748999041344,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.time_to_time,timeseries,"{""timeseries.DatetimeIndex.time_to_time"": ""class DatetimeIndex:\n    def time_to_time(self, index_type):\n        self.index.time\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]""}",timeseries.time_to_time,time,0.0028604594279075706
15053,func-level,"terminus-2,gpt-5",0.9762346475653858,0.966058356250507,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.fields.time_get_date_field,tslibs.fields,"{""tslibs.fields.TimeGetDateField.time_get_date_field"": ""class TimeGetDateField:\n    def time_get_date_field(self, size, field):\n        get_date_field(self.i8data, field)\n\n    def setup(self, size, field):\n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.fields.time_get_date_field,time,0.007196811396661139
15049,func-level,"terminus-2,gpt-5",1.3663665411564112,1.3596393751609956,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.time_upper,strings,"{""strings.Methods.time_upper"": ""class Methods:\n    def time_upper(self, dtype):\n        self.s.str.upper()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.time_upper,time,0.004757543136786112
15058,func-level,"terminus-2,gpt-5",1.0576304060117854,1.0505875506830056,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.time_add_np_dt64,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_add_np_dt64"": ""class OffestDatetimeArithmetic:\n    def time_add_np_dt64(self, offset):\n        offset + self.dt64\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.time_add_np_dt64,time,0.004980802919929134
15062,func-level,"terminus-2,gpt-5",1.0207411616961457,1.0187289916107118,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.time_asfreq,tslibs.period,"{""tslibs.period.PeriodUnaryMethods.time_asfreq"": ""class PeriodUnaryMethods:\n    def time_asfreq(self, freq):\n        self.per.asfreq(\""A\"")\n\n    def setup(self, freq):\n        self.per = Period(\""2012-06-01\"", freq=freq)""}",tslibs.period.time_asfreq,time,0.0014230340066717555
15057,func-level,"terminus-2,gpt-5",1.040357876864411,1.0457376008735422,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.time_add_10,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_add_10"": ""class OffestDatetimeArithmetic:\n    def time_add_10(self, offset):\n        self.date + (10 * offset)\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.time_add_10,time,-0.0038046138678437874
15059,func-level,"terminus-2,gpt-5",0.981357723208407,0.9876597220558692,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.time_on_offset,tslibs.offsets,"{""tslibs.offsets.OnOffset.time_on_offset"": ""class OnOffset:\n    def time_on_offset(self, offset):\n        for date in self.dates:\n            offset.is_on_offset(date)\n\n    def setup(self, offset):\n        self.dates = [\n            datetime(2016, m, d)\n            for m in [10, 11, 12]\n            for d in [1, 2, 3, 28, 29, 30, 31]\n            if not (m == 11 and d == 31)\n        ]""}",tslibs.offsets.time_on_offset,time,-0.004456859156621066
15055,func-level,"terminus-2,gpt-5",1.0027272859251488,0.9623454744132488,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.normalize.time_is_date_array_normalized,tslibs.normalize,"{""tslibs.normalize.Normalize.time_is_date_array_normalized"": ""class Normalize:\n    def time_is_date_array_normalized(self, size, tz):\n        # TODO: cases with different levels of short-circuiting\n        # 10 i.e. NPY_FR_ns\n        is_date_array_normalized(self.i8data, tz, 10)\n\n    def setup(self, size, tz):\n        # use an array that will have is_date_array_normalized give True,\n        #  so we do not short-circuit early.\n        dti = pd.date_range(\""2016-01-01\"", periods=10, tz=tz).repeat(size // 10)\n        self.i8data = dti.asi8\n    \n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError""}",tslibs.normalize.time_is_date_array_normalized,time,0.028558565425671863
15060,func-level,"terminus-2,gpt-5",1.0484595268392467,1.0485062112037746,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.time_subtract,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_subtract"": ""class OffestDatetimeArithmetic:\n    def time_subtract(self, offset):\n        self.date - offset\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.time_subtract,time,-3.301581649782667e-05
15067,func-level,"terminus-2,gpt-5",1.018202571806409,1.0292640988421993,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.time_property,tslibs.period,"{""tslibs.period.PeriodProperties.time_property"": ""class PeriodProperties:\n    def time_property(self, freq, attr):\n        getattr(self.per, attr)\n\n    def setup(self, freq, attr):\n        self.per = Period(\""2012-06-01\"", freq=freq)""}",tslibs.period.time_property,time,-0.007822862118663601
15066,func-level,"terminus-2,gpt-5",1.0002631015527346,0.9703121427233437,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.time_periodarray_to_dt64arr,tslibs.period,"{""tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr"": ""class TimePeriodArrToDT64Arr:\n    def time_periodarray_to_dt64arr(self, size, freq):\n        periodarr_to_dt64arr(self.i8values, freq)\n\n    def setup(self, size, freq):\n        arr = np.arange(10, dtype=\""i8\"").repeat(size // 10)\n        self.i8values = arr""}",tslibs.period.time_periodarray_to_dt64arr,time,0.021181724773260906
15063,func-level,"terminus-2,gpt-5",1.010929828348516,1.0135595023440018,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.time_dt64arr_to_periodarr,tslibs.period,"{""tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr"": ""class TimeDT64ArrToPeriodArr:\n    def time_dt64arr_to_periodarr(self, size, freq, tz):\n        dt64arr_to_periodarr(self.i8values, freq, tz)\n\n    def setup(self, size, freq, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.arange(10, dtype=\""i8\"").repeat(size // 10)\n        self.i8values = arr""}",tslibs.period.time_dt64arr_to_periodarr,time,-0.0018597411566378074
15065,func-level,"terminus-2,gpt-5",1.0687924408221696,1.0892287783290096,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.time_period_constructor,tslibs.period,"{""tslibs.period.PeriodConstructor.time_period_constructor"": ""class PeriodConstructor:\n    def time_period_constructor(self, freq, is_offset):\n        Period(\""2012-06-01\"", freq=freq)\n\n    def setup(self, freq, is_offset):\n        if is_offset:\n            self.freq = to_offset(freq)\n        else:\n            self.freq = freq""}",tslibs.period.time_period_constructor,time,-0.01445285537966049
15064,func-level,"terminus-2,gpt-5",1.0441422483193945,1.0369797119462696,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.time_now,tslibs.period,"{""tslibs.period.PeriodUnaryMethods.time_now"": ""class PeriodUnaryMethods:\n    def time_now(self, freq):\n        self.per.now(freq)\n\n    def setup(self, freq):\n        self.per = Period(\""2012-06-01\"", freq=freq)""}",tslibs.period.time_now,time,0.005065442979579149
15056,func-level,"terminus-2,gpt-5",1.0456899732678298,1.0498466373943325,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.time_add,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_add"": ""class OffestDatetimeArithmetic:\n    def time_add(self, offset):\n        self.date + offset\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.time_add,time,-0.0029396493115295117
15070,func-level,"terminus-2,gpt-5",1.0504753429107314,1.0156490510826353,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timedelta.time_from_datetime_timedelta,tslibs.timedelta,"{""tslibs.timedelta.TimedeltaConstructor.time_from_datetime_timedelta"": ""class TimedeltaConstructor:\n    def time_from_datetime_timedelta(self):\n        Timedelta(self.dttimedelta)\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")""}",tslibs.timedelta.time_from_datetime_timedelta,time,0.02462962646965782
15071,func-level,"terminus-2,gpt-5",1.0232342172789712,1.026531993986255,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timedelta.time_from_iso_format,tslibs.timedelta,"{""tslibs.timedelta.TimedeltaConstructor.time_from_iso_format"": ""class TimedeltaConstructor:\n    def time_from_iso_format(self):\n        Timedelta(\""P4DT12H30M5S\"")\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")""}",tslibs.timedelta.time_from_iso_format,time,-0.0023322324662543893
15072,func-level,"terminus-2,gpt-5",1.0310551426126953,1.0242987605277876,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timedelta.time_from_np_timedelta,tslibs.timedelta,"{""tslibs.timedelta.TimedeltaConstructor.time_from_np_timedelta"": ""class TimedeltaConstructor:\n    def time_from_np_timedelta(self):\n        Timedelta(self.nptimedelta64)\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")""}",tslibs.timedelta.time_from_np_timedelta,time,0.004778205151985642
15075,func-level,"terminus-2,gpt-5",1.0283387351275328,1.0425602629905846,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_floor,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_floor"": ""class TimestampOps:\n    def time_floor(self, tz):\n        self.ts.floor(\""5T\"")\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.time_floor,time,-0.010057657611776346
15061,func-level,"terminus-2,gpt-5",1.0567498590741593,1.0539364109765217,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.time_subtract_10,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_subtract_10"": ""class OffestDatetimeArithmetic:\n    def time_subtract_10(self, offset):\n        self.date - (10 * offset)\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.time_subtract_10,time,0.001989708697056306
15068,func-level,"terminus-2,gpt-5",1.0364622985126133,1.0745328131661918,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.time_to_timestamp,tslibs.period,"{""tslibs.period.PeriodUnaryMethods.time_to_timestamp"": ""class PeriodUnaryMethods:\n    def time_to_timestamp(self, freq):\n        self.per.to_timestamp()\n\n    def setup(self, freq):\n        self.per = Period(\""2012-06-01\"", freq=freq)""}",tslibs.period.time_to_timestamp,time,-0.026923984903520843
15074,func-level,"terminus-2,gpt-5",1.035281365647862,1.028120784701606,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_ceil,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_ceil"": ""class TimestampOps:\n    def time_ceil(self, tz):\n        self.ts.ceil(\""5T\"")\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.time_ceil,time,0.005064060075145718
15069,func-level,"terminus-2,gpt-5",1.0075868599150823,1.0124123880355616,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.resolution.time_get_resolution,tslibs.resolution,"{""tslibs.resolution.TimeResolution.time_get_resolution"": ""class TimeResolution:\n    def time_get_resolution(self, unit, size, tz):\n        get_resolution(self.i8data, tz)\n\n    def setup(self, unit, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        arr = arr.view(f\""M8[{unit}]\"").astype(\""M8[ns]\"").view(\""i8\"")\n        self.i8data = arr""}",tslibs.resolution.time_get_resolution,time,-0.003412679010239891
15080,func-level,"terminus-2,gpt-5",1.0140354533158242,1.0096282482119523,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_is_quarter_end,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_is_quarter_end"": ""class TimestampProperties:\n    def time_is_quarter_end(self, tz):\n        self.ts.is_quarter_end\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.time_is_quarter_end,time,0.0031168352926958533
15084,func-level,"terminus-2,gpt-5",1.0102244904145388,1.0106040296936736,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_microsecond,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_microsecond"": ""class TimestampProperties:\n    def time_microsecond(self, tz):\n        self.ts.microsecond\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.time_microsecond,time,-0.00026841533177850824
15079,func-level,"terminus-2,gpt-5",1.0105903306150494,1.015546681180151,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_is_leap_year,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_is_leap_year"": ""class TimestampProperties:\n    def time_is_leap_year(self, tz):\n        self.ts.is_leap_year\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.time_is_leap_year,time,-0.0035051984194494875
15073,func-level,"terminus-2,gpt-5",1.0198470736385634,1.0272665785845632,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timedelta.time_from_string,tslibs.timedelta,"{""tslibs.timedelta.TimedeltaConstructor.time_from_string"": ""class TimedeltaConstructor:\n    def time_from_string(self):\n        Timedelta(\""1 days\"")\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")""}",tslibs.timedelta.time_from_string,time,-0.005247174643564176
15081,func-level,"terminus-2,gpt-5",1.0112033176462023,1.0098146508293273,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_is_quarter_start,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_is_quarter_start"": ""class TimestampProperties:\n    def time_is_quarter_start(self, tz):\n        self.ts.is_quarter_start\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.time_is_quarter_start,time,0.0009820840289073516
15076,func-level,"terminus-2,gpt-5",1.0507377625539016,1.026953965589444,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_from_datetime_aware,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_from_datetime_aware"": ""class TimestampConstruction:\n    def time_from_datetime_aware(self):\n        Timestamp(self.dttime_aware)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.time_from_datetime_aware,time,0.01682022416156835
15077,func-level,"terminus-2,gpt-5",1.0335804549781111,1.0272462397518227,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_from_datetime_unaware,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_from_datetime_unaware"": ""class TimestampConstruction:\n    def time_from_datetime_unaware(self):\n        Timestamp(self.dttime_unaware)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.time_from_datetime_unaware,time,0.004479643017177092
15078,func-level,"terminus-2,gpt-5",1.0279009276060944,1.01738764873653,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_from_npdatetime64,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_from_npdatetime64"": ""class TimestampConstruction:\n    def time_from_npdatetime64(self):\n        Timestamp(self.npdatetime64)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.time_from_npdatetime64,time,0.007435133571120456
15088,func-level,"terminus-2,gpt-5",1.041839474670617,1.0390043640338635,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_parse_iso8601_no_tz,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_parse_iso8601_no_tz"": ""class TimestampConstruction:\n    def time_parse_iso8601_no_tz(self):\n        Timestamp(\""2017-08-25 08:16:14\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.time_parse_iso8601_no_tz,time,0.0020050287388638315
15083,func-level,"terminus-2,gpt-5",1.0171445771171386,1.0104473288567313,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_is_year_start,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_is_year_start"": ""class TimestampProperties:\n    def time_is_year_start(self, tz):\n        self.ts.is_year_start\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.time_is_year_start,time,0.00473638490835028
15091,func-level,"terminus-2,gpt-5",1.0466428170174762,1.0477511425107873,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_replace_None,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_replace_None"": ""class TimestampOps:\n    def time_replace_None(self, tz):\n        self.ts.replace(tzinfo=None)\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.time_replace_None,time,-0.0007838228382680903
15082,func-level,"terminus-2,gpt-5",1.01162875302415,1.0069464222790394,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_is_year_end,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_is_year_end"": ""class TimestampProperties:\n    def time_is_year_end(self, tz):\n        self.ts.is_year_end\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.time_is_year_end,time,0.0033114078819734697
15086,func-level,"terminus-2,gpt-5",1.017750066029947,1.0292353035559174,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_normalize,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_normalize"": ""class TimestampOps:\n    def time_normalize(self, tz):\n        self.ts.normalize()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.time_normalize,time,-0.008122515930672147
15093,func-level,"terminus-2,gpt-5",1.03328246186692,1.0396346627529898,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_replace_tz,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_replace_tz"": ""class TimestampOps:\n    def time_replace_tz(self, tz):\n        self.ts.replace(tzinfo=pytz.timezone(\""US/Eastern\""))\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.time_replace_tz,time,-0.00449236271999271
15085,func-level,"terminus-2,gpt-5",0.9799252293572036,0.9791569957625458,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_month_name,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_month_name"": ""class TimestampProperties:\n    def time_month_name(self, tz):\n        self.ts.month_name()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.time_month_name,time,0.0005433052296024258
15087,func-level,"terminus-2,gpt-5",1.0143900976752434,1.0562877508565431,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_parse_dateutil,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_parse_dateutil"": ""class TimestampConstruction:\n    def time_parse_dateutil(self):\n        Timestamp(\""2017/08/25 08:16:14 AM\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.time_parse_dateutil,time,-0.029630589237128566
15089,func-level,"terminus-2,gpt-5",1.0111660709481616,1.0164790544428826,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_parse_iso8601_tz,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_parse_iso8601_tz"": ""class TimestampConstruction:\n    def time_parse_iso8601_tz(self):\n        Timestamp(\""2017-08-25 08:16:14-0500\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.time_parse_iso8601_tz,time,-0.0037574140698168174
15103,func-level,"terminus-2,oracle",1.0755927325105377,1.0755927325105377,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.time_isin,algos.isin,"{""algos.isin.IsInLongSeriesLookUpDominates.time_isin"": ""class IsInLongSeriesLookUpDominates:\n    def time_isin(self, dtypes, MaxNumber, series_type):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, MaxNumber, series_type):\n        N = 10**7\n    \n        if series_type == \""random_hits\"":\n            array = np.random.randint(0, MaxNumber, N)\n        if series_type == \""random_misses\"":\n            array = np.random.randint(0, MaxNumber, N) + MaxNumber\n        if series_type == \""monotone_hits\"":\n            array = np.repeat(np.arange(MaxNumber), N // MaxNumber)\n        if series_type == \""monotone_misses\"":\n            array = np.arange(N) + MaxNumber\n    \n        self.series = Series(array).astype(dtype)\n    \n        self.values = np.arange(MaxNumber).astype(dtype.lower())"", ""algos.isin.IsinAlmostFullWithRandomInt.time_isin"": ""class IsinAlmostFullWithRandomInt:\n    def time_isin(self, dtype, exponent, title):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, exponent, title):\n        M = 3 * 2 ** (exponent - 2)\n        # 0.77-the maximal share of occupied buckets\n        self.series = Series(np.random.randint(0, M, M)).astype(dtype)\n    \n        values = np.random.randint(0, M, M).astype(dtype)\n        if title == \""inside\"":\n            self.values = values\n        elif title == \""outside\"":\n            self.values = values + M\n        else:\n            raise ValueError(title)""}",algos.isin.time_isin,time,0.0
15100,func-level,"terminus-2,oracle",1.0219970745309055,1.0219970745309055,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algorithms.time_duplicated,algorithms,"{""algorithms.Duplicated.time_duplicated"": ""class Duplicated:\n    def time_duplicated(self, unique, keep, dtype):\n        self.idx.duplicated(keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Duplicated:\n    def setup(self, unique, keep, dtype):\n        N = 10**5\n        data = {\n            \""int\"": pd.Index(np.arange(N), dtype=\""int64\""),\n            \""uint\"": pd.Index(np.arange(N), dtype=\""uint64\""),\n            \""float\"": pd.Index(np.random.randn(N), dtype=\""float64\""),\n            \""string\"": tm.makeStringIndex(N),\n            \""datetime64[ns]\"": pd.date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n            \""datetime64[ns, tz]\"": pd.date_range(\n                \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n            ),\n        }[dtype]\n        if not unique:\n            data = data.repeat(5)\n        self.idx = data\n        # cache is_unique\n        self.idx.is_unique""}",algorithms.time_duplicated,time,0.0
15090,func-level,"terminus-2,gpt-5",1.0207834401168423,1.026001270156748,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_quarter,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_quarter"": ""class TimestampProperties:\n    def time_quarter(self, tz):\n        self.ts.quarter\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.time_quarter,time,-0.0036901202545302153
15102,func-level,"terminus-2,oracle",1.123615183139807,1.123615183139807,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algorithms.time_quantile,algorithms,"{""algorithms.Quantile.time_quantile"": ""class Quantile:\n    def time_quantile(self, quantile, interpolation, dtype):\n        self.idx.quantile(quantile, interpolation=interpolation)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, quantile, interpolation, dtype):\n        N = 10**5\n        data = {\n            \""int\"": np.arange(N),\n            \""uint\"": np.arange(N).astype(np.uint64),\n            \""float\"": np.random.randn(N),\n        }\n        self.idx = pd.Series(data[dtype].repeat(5))""}",algorithms.time_quantile,time,0.0
15101,func-level,"terminus-2,oracle",1.088845865478887,1.088845865478887,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algorithms.time_factorize,algorithms,"{""algorithms.Factorize.time_factorize"": ""class Factorize:\n    def time_factorize(self, unique, sort, dtype):\n        pd.factorize(self.data, sort=sort)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Factorize:\n    def setup(self, unique, sort, dtype):\n        N = 10**5\n        string_index = tm.makeStringIndex(N)\n        string_arrow = None\n        if dtype == \""string[pyarrow]\"":\n            try:\n                string_arrow = pd.array(string_index, dtype=\""string[pyarrow]\"")\n            except ImportError:\n                raise NotImplementedError\n    \n        data = {\n            \""int\"": pd.Index(np.arange(N), dtype=\""int64\""),\n            \""uint\"": pd.Index(np.arange(N), dtype=\""uint64\""),\n            \""float\"": pd.Index(np.random.randn(N), dtype=\""float64\""),\n            \""object\"": string_index,\n            \""datetime64[ns]\"": pd.date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n            \""datetime64[ns, tz]\"": pd.date_range(\n                \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n            ),\n            \""Int64\"": pd.array(np.arange(N), dtype=\""Int64\""),\n            \""boolean\"": pd.array(np.random.randint(0, 2, N), dtype=\""boolean\""),\n            \""string[pyarrow]\"": string_arrow,\n        }[dtype]\n        if not unique:\n            data = data.repeat(5)\n        self.data = data""}",algorithms.time_factorize,time,0.0
15106,func-level,"terminus-2,oracle",1.1108688007180527,1.1108688007180527,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.time_isin_mismatched_dtype,algos.isin,"{""algos.isin.IsIn.time_isin_mismatched_dtype"": ""class IsIn:\n    def time_isin_mismatched_dtype(self, dtype):\n        self.series.isin(self.mismatched)\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)""}",algos.isin.time_isin_mismatched_dtype,time,0.0
15094,func-level,"terminus-2,gpt-5",1.0198540312318218,1.023041485566896,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_to_julian_date,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_to_julian_date"": ""class TimestampOps:\n    def time_to_julian_date(self, tz):\n        self.ts.to_julian_date()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.time_to_julian_date,time,-0.002254210986615317
15104,func-level,"terminus-2,oracle",1.0618129829695413,1.0618129829695413,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.time_isin_categorical,algos.isin,"{""algos.isin.IsIn.time_isin_categorical"": ""class IsIn:\n    def time_isin_categorical(self, dtype):\n        self.series.isin(self.cat_values)\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)""}",algos.isin.time_isin_categorical,time,0.0
15110,func-level,"terminus-2,oracle",0.7970819164535795,0.7970819164535795,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.time_divide,arithmetic,"{""arithmetic.NumericInferOps.time_divide"": ""class NumericInferOps:\n    def time_divide(self, dtype):\n        self.df[\""A\""] / self.df[\""B\""]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericInferOps:\n    def setup(self, dtype):\n        N = 5 * 10**5\n        self.df = DataFrame(\n            {\""A\"": np.arange(N).astype(dtype), \""B\"": np.arange(N).astype(dtype)}\n        )""}",arithmetic.time_divide,time,0.0
15105,func-level,"terminus-2,oracle",1.0557779017966336,1.0557779017966336,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.time_isin_empty,algos.isin,"{""algos.isin.IsIn.time_isin_empty"": ""class IsIn:\n    def time_isin_empty(self, dtype):\n        self.series.isin([])\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)""}",algos.isin.time_isin_empty,time,0.0
15107,func-level,"terminus-2,oracle",0.9600290957696874,0.9600290957696874,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.time_add_dti_offset,arithmetic,"{""arithmetic.OffsetArrayArithmetic.time_add_dti_offset"": ""class OffsetArrayArithmetic:\n    def time_add_dti_offset(self, offset):\n        with warnings.catch_warnings(record=True):\n            self.rng + offset\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass OffsetArrayArithmetic:\n    def setup(self, offset):\n        N = 10000\n        rng = date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"")\n        self.rng = rng\n        self.ser = Series(rng)""}",arithmetic.time_add_dti_offset,time,0.0
15099,func-level,"terminus-2,gpt-5",1.0284023470860206,0.9598299359561444,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.tz_convert.time_tz_localize_to_utc,tslibs.tz_convert,"{""tslibs.tz_convert.TimeTZConvert.time_tz_localize_to_utc"": ""class TimeTZConvert:\n    def time_tz_localize_to_utc(self, size, tz):\n        # effectively:\n        #  dti = DatetimeIndex(self.i8data)\n        #  dti.tz_localize(tz, ambiguous=\""NaT\"", nonexistent=\""NaT\"")\n        tz_localize_to_utc(self.i8data, tz, ambiguous=\""NaT\"", nonexistent=\""NaT\"")\n\n    def setup(self, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.tz_convert.time_tz_localize_to_utc,time,0.04849534026158149
15109,func-level,"terminus-2,oracle",0.8968622270302342,0.8968622270302342,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.time_add_td_ts,arithmetic,"{""arithmetic.TimedeltaOps.time_add_td_ts"": ""class TimedeltaOps:\n    def time_add_td_ts(self):\n        self.td + self.ts\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TimedeltaOps:\n    def setup(self):\n        self.td = to_timedelta(np.arange(1000000))\n        self.ts = Timestamp(\""2000\"")""}",arithmetic.time_add_td_ts,time,0.0
15092,func-level,"terminus-2,gpt-5",1.0135610916926148,1.0385202816031789,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_replace_across_dst,tslibs.timestamp,"{""tslibs.timestamp.TimestampAcrossDst.time_replace_across_dst"": ""class TimestampAcrossDst:\n    def time_replace_across_dst(self):\n        self.ts2.replace(tzinfo=self.tzinfo)\n\n    def setup(self):\n        dt = datetime(2016, 3, 27, 1)\n        self.tzinfo = pytz.timezone(\""CET\"").localize(dt, is_dst=False).tzinfo\n        self.ts2 = Timestamp(dt)""}",tslibs.timestamp.time_replace_across_dst,time,-0.017651478013128795
15111,func-level,"terminus-2,oracle",0.9735601845496592,0.9735601845496592,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.time_frame_dot,arithmetic,"{""arithmetic.Ops2.time_frame_dot"": ""class Ops2:\n    def time_frame_dot(self):\n        self.df.dot(self.df2)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Ops2:\n    def setup(self):\n        N = 10**3\n        self.df = DataFrame(np.random.randn(N, N))\n        self.df2 = DataFrame(np.random.randn(N, N))\n    \n        self.df_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n        self.df2_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n    \n        self.s = Series(np.random.randn(N))""}",arithmetic.time_frame_dot,time,0.0
15108,func-level,"terminus-2,oracle",0.9221873662977405,0.9221873662977405,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.time_add_series_offset,arithmetic,"{""arithmetic.OffsetArrayArithmetic.time_add_series_offset"": ""class OffsetArrayArithmetic:\n    def time_add_series_offset(self, offset):\n        with warnings.catch_warnings(record=True):\n            self.ser + offset\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass OffsetArrayArithmetic:\n    def setup(self, offset):\n        N = 10000\n        rng = date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"")\n        self.rng = rng\n        self.ser = Series(rng)""}",arithmetic.time_add_series_offset,time,0.0
15096,func-level,"terminus-2,gpt-5",1.0175806384357051,0.9848410265394214,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_tz_localize,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_tz_localize"": ""class TimestampOps:\n    def time_tz_localize(self, tz):\n        if self.ts.tz is None:\n            self.ts.tz_localize(tz)\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.time_tz_localize,time,0.02315389808789513
15097,func-level,"terminus-2,gpt-5",1.0165091003831477,1.0142606399277831,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_week,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_week"": ""class TimestampProperties:\n    def time_week(self, tz):\n        self.ts.week\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.time_week,time,0.00159014176475568
15095,func-level,"terminus-2,gpt-5",1.0122450486528345,0.97438388789397,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_tz_convert,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_tz_convert"": ""class TimestampOps:\n    def time_tz_convert(self, tz):\n        if self.ts.tz is not None:\n            self.ts.tz_convert(tz)\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.time_tz_convert,time,0.026775926986467097
15113,func-level,"terminus-2,oracle",0.9848606535373996,0.9848606535373996,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.time_frame_float_mod,arithmetic,"{""arithmetic.Ops2.time_frame_float_mod"": ""class Ops2:\n    def time_frame_float_mod(self):\n        self.df % self.df2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Ops2:\n    def setup(self):\n        N = 10**3\n        self.df = DataFrame(np.random.randn(N, N))\n        self.df2 = DataFrame(np.random.randn(N, N))\n    \n        self.df_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n        self.df2_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n    \n        self.s = Series(np.random.randn(N))""}",arithmetic.time_frame_float_mod,time,0.0
15117,func-level,"terminus-2,oracle",0.9435501271235892,0.9435501271235892,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.time_frame_series_dot,arithmetic,"{""arithmetic.Ops2.time_frame_series_dot"": ""class Ops2:\n    def time_frame_series_dot(self):\n        self.df.dot(self.s)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Ops2:\n    def setup(self):\n        N = 10**3\n        self.df = DataFrame(np.random.randn(N, N))\n        self.df2 = DataFrame(np.random.randn(N, N))\n    \n        self.df_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n        self.df2_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n    \n        self.s = Series(np.random.randn(N))""}",arithmetic.time_frame_series_dot,time,0.0
15098,func-level,"terminus-2,gpt-5",1.024648415455555,1.0298420080843886,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.tslib.time_ints_to_pydatetime,tslibs.tslib,"{""tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime"": ""class TimeIntsToPydatetime:\n    def time_ints_to_pydatetime(self, box, size, tz):\n        ints_to_pydatetime(self.i8data, tz, box=box)\n\n    def setup(self, box, size, tz):\n        if box == \""date\"" and tz is not None:\n            # tz is ignored, so avoid running redundant benchmarks\n            raise NotImplementedError  # skip benchmark\n        if size == 10**6 and tz is _tzs[-1]:\n            # This is cumbersomely-slow, so skip to trim runtime\n            raise NotImplementedError  # skip benchmark\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.tslib.time_ints_to_pydatetime,time,-0.0036729792283122812
15112,func-level,"terminus-2,oracle",0.7364712027927393,0.7364712027927393,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.time_frame_float_div_by_zero,arithmetic,"{""arithmetic.Ops2.time_frame_float_div_by_zero"": ""class Ops2:\n    def time_frame_float_div_by_zero(self):\n        self.df / 0\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Ops2:\n    def setup(self):\n        N = 10**3\n        self.df = DataFrame(np.random.randn(N, N))\n        self.df2 = DataFrame(np.random.randn(N, N))\n    \n        self.df_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n        self.df2_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n    \n        self.s = Series(np.random.randn(N))""}",arithmetic.time_frame_float_div_by_zero,time,0.0
15114,func-level,"terminus-2,oracle",0.839985324922907,0.839985324922907,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.time_frame_int_div_by_zero,arithmetic,"{""arithmetic.Ops2.time_frame_int_div_by_zero"": ""class Ops2:\n    def time_frame_int_div_by_zero(self):\n        self.df_int / 0\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Ops2:\n    def setup(self):\n        N = 10**3\n        self.df = DataFrame(np.random.randn(N, N))\n        self.df2 = DataFrame(np.random.randn(N, N))\n    \n        self.df_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n        self.df2_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n    \n        self.s = Series(np.random.randn(N))""}",arithmetic.time_frame_int_div_by_zero,time,0.0
15115,func-level,"terminus-2,oracle",0.9813729259103554,0.9813729259103554,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.time_frame_op_with_scalar,arithmetic,"{""arithmetic.IntFrameWithScalar.time_frame_op_with_scalar"": ""class IntFrameWithScalar:\n    def time_frame_op_with_scalar(self, dtype, scalar, op):\n        op(self.df, scalar)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IntFrameWithScalar:\n    def setup(self, dtype, scalar, op):\n        arr = np.random.randn(20000, 100)\n        self.df = DataFrame(arr.astype(dtype))""}",arithmetic.time_frame_op_with_scalar,time,0.0
15121,func-level,"terminus-2,oracle",0.7674428694827238,0.7674428694827238,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.time_series_timestamp_compare,arithmetic,"{""arithmetic.Timeseries.time_series_timestamp_compare"": ""class Timeseries:\n    def time_series_timestamp_compare(self, tz):\n        self.s <= self.ts\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Timeseries:\n    def setup(self, tz):\n        N = 10**6\n        halfway = (N // 2) - 1\n        self.s = Series(date_range(\""20010101\"", periods=N, freq=\""T\"", tz=tz))\n        self.ts = self.s[halfway]\n    \n        self.s2 = Series(date_range(\""20010101\"", periods=N, freq=\""s\"", tz=tz))""}",arithmetic.time_series_timestamp_compare,time,0.0
15119,func-level,"terminus-2,oracle",0.9014908348337756,0.9014908348337756,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.time_op_different_blocks,arithmetic,"{""arithmetic.FrameWithFrameWide.time_op_different_blocks"": ""class FrameWithFrameWide:\n    def time_op_different_blocks(self, op, shape):\n        # blocks (and dtypes) are not aligned\n        op(self.left, self.right)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FrameWithFrameWide:\n    def setup(self, op, shape):\n        # we choose dtypes so as to make the blocks\n        #  a) not perfectly match between right and left\n        #  b) appreciably bigger than single columns\n        n_rows, n_cols = shape\n    \n        if op is operator.floordiv:\n            # floordiv is much slower than the other operations -> use less data\n            n_rows = n_rows // 10\n    \n        # construct dataframe with 2 blocks\n        arr1 = np.random.randn(n_rows, n_cols // 2).astype(\""f8\"")\n        arr2 = np.random.randn(n_rows, n_cols // 2).astype(\""f4\"")\n        df = pd.concat([DataFrame(arr1), DataFrame(arr2)], axis=1, ignore_index=True)\n        # should already be the case, but just to be sure\n        df._consolidate_inplace()\n    \n        # TODO: GH#33198 the setting here shouldn't need two steps\n        arr1 = np.random.randn(n_rows, max(n_cols // 4, 3)).astype(\""f8\"")\n        arr2 = np.random.randn(n_rows, n_cols // 2).astype(\""i8\"")\n        arr3 = np.random.randn(n_rows, n_cols // 4).astype(\""f8\"")\n        df2 = pd.concat(\n            [DataFrame(arr1), DataFrame(arr2), DataFrame(arr3)],\n            axis=1,\n            ignore_index=True,\n        )\n        # should already be the case, but just to be sure\n        df2._consolidate_inplace()\n    \n        self.left = df\n        self.right = df2""}",arithmetic.time_op_different_blocks,time,0.0
15116,func-level,"terminus-2,oracle",0.9681320119640044,0.9681320119640044,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.time_frame_op_with_series_axis0,arithmetic,"{""arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0"": ""class MixedFrameWithSeriesAxis:\n    def time_frame_op_with_series_axis0(self, opname):\n        getattr(self.df, opname)(self.ser, axis=0)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MixedFrameWithSeriesAxis:\n    def setup(self, opname):\n        arr = np.arange(10**6).reshape(1000, -1)\n        df = DataFrame(arr)\n        df[\""C\""] = 1.0\n        self.df = df\n        self.ser = df[0]\n        self.row = df.iloc[0]""}",arithmetic.time_frame_op_with_series_axis0,time,0.0
15122,func-level,"terminus-2,oracle",0.8652383137552341,0.8652383137552341,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.time_timestamp_ops_diff,arithmetic,"{""arithmetic.Timeseries.time_timestamp_ops_diff"": ""class Timeseries:\n    def time_timestamp_ops_diff(self, tz):\n        self.s2.diff()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Timeseries:\n    def setup(self, tz):\n        N = 10**6\n        halfway = (N // 2) - 1\n        self.s = Series(date_range(\""20010101\"", periods=N, freq=\""T\"", tz=tz))\n        self.ts = self.s[halfway]\n    \n        self.s2 = Series(date_range(\""20010101\"", periods=N, freq=\""s\"", tz=tz))""}",arithmetic.time_timestamp_ops_diff,time,0.0
15125,func-level,"terminus-2,oracle",0.963676491180532,0.963676491180532,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,array.time_constructor,array,"{""array.IntegerArray.time_constructor"": ""class IntegerArray:\n    def time_constructor(self):\n        pd.arrays.IntegerArray(self.data, self.mask)\n\n    def setup(self):\n        N = 250_000\n        self.values_integer = np.array([1, 0, 1, 0] * N)\n        self.data = np.array([1, 2, 3, 4] * N, dtype=\""int64\"")\n        self.mask = np.array([False, False, True, False] * N)""}",array.time_constructor,time,0.0
15118,func-level,"terminus-2,oracle",0.9842875183363564,0.9842875183363564,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.time_modulo,arithmetic,"{""arithmetic.IndexArithmetic.time_modulo"": ""class IndexArithmetic:\n    def time_modulo(self, dtype):\n        self.index % 2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IndexArithmetic:\n    def setup(self, dtype):\n        N = 10**6\n        indexes = {\""int\"": \""makeIntIndex\"", \""float\"": \""makeFloatIndex\""}\n        self.index = getattr(tm, indexes[dtype])(N)""}",arithmetic.time_modulo,time,0.0
15120,func-level,"terminus-2,oracle",0.9604287049162952,0.9604287049162952,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.time_op_same_blocks,arithmetic,"{""arithmetic.FrameWithFrameWide.time_op_same_blocks"": ""class FrameWithFrameWide:\n    def time_op_same_blocks(self, op, shape):\n        # blocks (and dtypes) are aligned\n        op(self.left, self.left)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FrameWithFrameWide:\n    def setup(self, op, shape):\n        # we choose dtypes so as to make the blocks\n        #  a) not perfectly match between right and left\n        #  b) appreciably bigger than single columns\n        n_rows, n_cols = shape\n    \n        if op is operator.floordiv:\n            # floordiv is much slower than the other operations -> use less data\n            n_rows = n_rows // 10\n    \n        # construct dataframe with 2 blocks\n        arr1 = np.random.randn(n_rows, n_cols // 2).astype(\""f8\"")\n        arr2 = np.random.randn(n_rows, n_cols // 2).astype(\""f4\"")\n        df = pd.concat([DataFrame(arr1), DataFrame(arr2)], axis=1, ignore_index=True)\n        # should already be the case, but just to be sure\n        df._consolidate_inplace()\n    \n        # TODO: GH#33198 the setting here shouldn't need two steps\n        arr1 = np.random.randn(n_rows, max(n_cols // 4, 3)).astype(\""f8\"")\n        arr2 = np.random.randn(n_rows, n_cols // 2).astype(\""i8\"")\n        arr3 = np.random.randn(n_rows, n_cols // 4).astype(\""f8\"")\n        df2 = pd.concat(\n            [DataFrame(arr1), DataFrame(arr2), DataFrame(arr3)],\n            axis=1,\n            ignore_index=True,\n        )\n        # should already be the case, but just to be sure\n        df2._consolidate_inplace()\n    \n        self.left = df\n        self.right = df2""}",arithmetic.time_op_same_blocks,time,0.0
15123,func-level,"terminus-2,oracle",0.9280316575600353,0.9280316575600353,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.time_timestamp_ops_diff_with_shift,arithmetic,"{""arithmetic.Timeseries.time_timestamp_ops_diff_with_shift"": ""class Timeseries:\n    def time_timestamp_ops_diff_with_shift(self, tz):\n        self.s - self.s.shift()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Timeseries:\n    def setup(self, tz):\n        N = 10**6\n        halfway = (N // 2) - 1\n        self.s = Series(date_range(\""20010101\"", periods=N, freq=\""T\"", tz=tz))\n        self.ts = self.s[halfway]\n    \n        self.s2 = Series(date_range(\""20010101\"", periods=N, freq=\""s\"", tz=tz))""}",arithmetic.time_timestamp_ops_diff_with_shift,time,0.0
15129,func-level,"terminus-2,oracle",0.9786944912772296,0.9786944912772296,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,array.time_setitem_list,array,"{""array.ArrowStringArray.time_setitem_list"": ""class ArrowStringArray:\n    def time_setitem_list(self, multiple_chunks):\n        indexer = list(range(0, 50)) + list(range(-1000, 0, 50))\n        self.array[indexer] = [\""foo\""] * len(indexer)\n\n    def setup(self, multiple_chunks):\n        try:\n            import pyarrow as pa\n        except ImportError:\n            raise NotImplementedError\n        strings = tm.rands_array(3, 10_000)\n        if multiple_chunks:\n            chunks = [strings[i : i + 100] for i in range(0, len(strings), 100)]\n            self.array = pd.arrays.ArrowStringArray(pa.chunked_array(chunks))\n        else:\n            self.array = pd.arrays.ArrowStringArray(pa.array(strings))""}",array.time_setitem_list,time,0.0
15127,func-level,"terminus-2,oracle",0.8975931204235437,0.8975931204235437,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,array.time_from_list,array,"{""array.StringArray.time_from_list"": ""class StringArray:\n    def time_from_list(self):\n        pd.array(self.values_list, dtype=\""string\"")\n\n    def setup(self):\n        N = 100_000\n        values = tm.rands_array(3, N)\n        self.values_obj = np.array(values, dtype=\""object\"")\n        self.values_str = np.array(values, dtype=\""U\"")\n        self.values_list = values.tolist()""}",array.time_from_list,time,0.0
15126,func-level,"terminus-2,oracle",0.981116408258753,0.981116408258753,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,array.time_from_bool_array,array,"{""array.BooleanArray.time_from_bool_array"": ""class BooleanArray:\n    def time_from_bool_array(self):\n        pd.array(self.values_bool, dtype=\""boolean\"")\n\n    def setup(self):\n        self.values_bool = np.array([True, False, True, False])\n        self.values_float = np.array([1.0, 0.0, 1.0, 0.0])\n        self.values_integer = np.array([1, 0, 1, 0])\n        self.values_integer_like = [1, 0, 1, 0]\n        self.data = np.array([True, False, True, False])\n        self.mask = np.array([False, False, True, False])""}",array.time_from_bool_array,time,0.0
15128,func-level,"terminus-2,oracle",0.904333635973276,0.904333635973276,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,array.time_from_tuples,array,"{""array.IntervalArray.time_from_tuples"": ""class IntervalArray:\n    def time_from_tuples(self):\n        pd.arrays.IntervalArray.from_tuples(self.tuples)\n\n    def setup(self):\n        N = 10_000\n        self.tuples = [(i, i + 1) for i in range(N)]""}",array.time_from_tuples,time,0.0
15124,func-level,"terminus-2,oracle",0.734412543616603,0.734412543616603,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.time_timestamp_series_compare,arithmetic,"{""arithmetic.Timeseries.time_timestamp_series_compare"": ""class Timeseries:\n    def time_timestamp_series_compare(self, tz):\n        self.ts >= self.s\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Timeseries:\n    def setup(self, tz):\n        N = 10**6\n        halfway = (N // 2) - 1\n        self.s = Series(date_range(\""20010101\"", periods=N, freq=\""T\"", tz=tz))\n        self.ts = self.s[halfway]\n    \n        self.s2 = Series(date_range(\""20010101\"", periods=N, freq=\""s\"", tz=tz))""}",arithmetic.time_timestamp_series_compare,time,0.0
15130,func-level,"terminus-2,oracle",0.9620389002167072,0.9620389002167072,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,array.time_to_numpy,array,"{""array.ArrowExtensionArray.time_to_numpy"": ""class ArrowExtensionArray:\n    def time_to_numpy(self, dtype, hasna):\n        self.arr.to_numpy()\n\n    def setup(self, dtype, hasna):\n        N = 100_000\n        if dtype == \""boolean[pyarrow]\"":\n            data = np.random.choice([True, False], N, replace=True)\n        elif dtype == \""float64[pyarrow]\"":\n            data = np.random.randn(N)\n        elif dtype == \""int64[pyarrow]\"":\n            data = np.arange(N)\n        elif dtype == \""string[pyarrow]\"":\n            data = tm.rands_array(10, N)\n        elif dtype == \""timestamp[ns][pyarrow]\"":\n            data = pd.date_range(\""2000-01-01\"", freq=\""s\"", periods=N)\n        else:\n            raise NotImplementedError\n    \n        arr = pd.array(data, dtype=dtype)\n        if hasna:\n            arr[::2] = pd.NA\n        self.arr = arr""}",array.time_to_numpy,time,0.0
15133,func-level,"terminus-2,oracle",0.9725705610801374,0.9725705610801374,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,attrs_caching.time_extract_array,attrs_caching,"{""attrs_caching.SeriesArrayAttribute.time_extract_array"": ""class SeriesArrayAttribute:\n    def time_extract_array(self, dtype):\n        extract_array(self.series)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesArrayAttribute:\n    def setup(self, dtype):\n        if dtype == \""numeric\"":\n            self.series = pd.Series([1, 2, 3])\n        elif dtype == \""object\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=object)\n        elif dtype == \""category\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=\""category\"")\n        elif dtype == \""datetime64\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3))\n        elif dtype == \""datetime64tz\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3, tz=\""UTC\""))""}",attrs_caching.time_extract_array,time,0.0
15134,func-level,"terminus-2,oracle",0.9759908567977325,0.9759908567977325,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,attrs_caching.time_extract_array_numpy,attrs_caching,"{""attrs_caching.SeriesArrayAttribute.time_extract_array_numpy"": ""class SeriesArrayAttribute:\n    def time_extract_array_numpy(self, dtype):\n        extract_array(self.series, extract_numpy=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesArrayAttribute:\n    def setup(self, dtype):\n        if dtype == \""numeric\"":\n            self.series = pd.Series([1, 2, 3])\n        elif dtype == \""object\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=object)\n        elif dtype == \""category\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=\""category\"")\n        elif dtype == \""datetime64\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3))\n        elif dtype == \""datetime64tz\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3, tz=\""UTC\""))""}",attrs_caching.time_extract_array_numpy,time,0.0
15131,func-level,"terminus-2,oracle",0.9782162306248984,0.9782162306248984,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,array.time_tolist,array,"{""array.ArrowStringArray.time_tolist"": ""class ArrowStringArray:\n    def time_tolist(self, multiple_chunks):\n        self.array.tolist()\n\n    def setup(self, multiple_chunks):\n        try:\n            import pyarrow as pa\n        except ImportError:\n            raise NotImplementedError\n        strings = tm.rands_array(3, 10_000)\n        if multiple_chunks:\n            chunks = [strings[i : i + 100] for i in range(0, len(strings), 100)]\n            self.array = pd.arrays.ArrowStringArray(pa.chunked_array(chunks))\n        else:\n            self.array = pd.arrays.ArrowStringArray(pa.array(strings))""}",array.time_tolist,time,0.0
15138,func-level,"terminus-2,oracle",0.97934611144595,0.97934611144595,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.time_all_nan,categoricals,"{""categoricals.Constructor.time_all_nan"": ""class Constructor:\n    def time_all_nan(self):\n        pd.Categorical(self.values_all_nan)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Constructor:\n    def setup(self):\n        N = 10**5\n        self.categories = list(\""abcde\"")\n        self.cat_idx = pd.Index(self.categories)\n        self.values = np.tile(self.categories, N)\n        self.codes = np.tile(range(len(self.categories)), N)\n    \n        self.datetimes = pd.Series(\n            pd.date_range(\""1995-01-01 00:00:00\"", periods=N / 10, freq=\""s\"")\n        )\n        self.datetimes_with_nat = self.datetimes.copy()\n        self.datetimes_with_nat.iloc[-1] = pd.NaT\n    \n        self.values_some_nan = list(np.tile(self.categories + [np.nan], N))\n        self.values_all_nan = [np.nan] * len(self.values)\n        self.values_all_int8 = np.ones(N, \""int8\"")\n        self.categorical = pd.Categorical(self.values, self.categories)\n        self.series = pd.Series(self.categorical)\n        self.intervals = pd.interval_range(0, 1, periods=N // 10)""}",categoricals.time_all_nan,time,0.0
15146,func-level,"terminus-2,oracle",0.9588389898491556,0.9588389898491556,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.time_getitem_list,categoricals,"{""categoricals.CategoricalSlicing.time_getitem_list"": ""class CategoricalSlicing:\n    def time_getitem_list(self, index):\n        self.data[self.list]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalSlicing:\n    def setup(self, index):\n        N = 10**6\n        categories = [\""a\"", \""b\"", \""c\""]\n        values = [0] * N + [1] * N + [2] * N\n        if index == \""monotonic_incr\"":\n            self.data = pd.Categorical.from_codes(values, categories=categories)\n        elif index == \""monotonic_decr\"":\n            self.data = pd.Categorical.from_codes(\n                list(reversed(values)), categories=categories\n            )\n        elif index == \""non_monotonic\"":\n            self.data = pd.Categorical.from_codes([0, 1, 2] * N, categories=categories)\n        else:\n            raise ValueError(f\""Invalid index param: {index}\"")\n    \n        self.scalar = 10000\n        self.list = list(range(10000))\n        self.cat_scalar = \""b\""""}",categoricals.time_getitem_list,time,0.0
15132,func-level,"terminus-2,oracle",0.9587187321438082,0.9587187321438082,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,attrs_caching.time_array,attrs_caching,"{""attrs_caching.SeriesArrayAttribute.time_array"": ""class SeriesArrayAttribute:\n    def time_array(self, dtype):\n        self.series.array\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesArrayAttribute:\n    def setup(self, dtype):\n        if dtype == \""numeric\"":\n            self.series = pd.Series([1, 2, 3])\n        elif dtype == \""object\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=object)\n        elif dtype == \""category\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=\""category\"")\n        elif dtype == \""datetime64\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3))\n        elif dtype == \""datetime64tz\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3, tz=\""UTC\""))""}",attrs_caching.time_array,time,0.0
15139,func-level,"terminus-2,oracle",0.9820553921372688,0.9820553921372688,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.time_append_overlapping_index,categoricals,"{""categoricals.Concat.time_append_overlapping_index"": ""class Concat:\n    def time_append_overlapping_index(self):\n        self.idx_a.append(self.idx_a)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Concat:\n    def setup(self):\n        N = 10**5\n        self.s = pd.Series(list(\""aabbcd\"") * N).astype(\""category\"")\n    \n        self.a = pd.Categorical(list(\""aabbcd\"") * N)\n        self.b = pd.Categorical(list(\""bbcdjk\"") * N)\n    \n        self.idx_a = pd.CategoricalIndex(range(N), range(N))\n        self.idx_b = pd.CategoricalIndex(range(N + 1), range(N + 1))\n        self.df_a = pd.DataFrame(range(N), columns=[\""a\""], index=self.idx_a)\n        self.df_b = pd.DataFrame(range(N + 1), columns=[\""a\""], index=self.idx_b)""}",categoricals.time_append_overlapping_index,time,0.0
15135,func-level,"terminus-2,oracle",0.9690018695661616,0.9690018695661616,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,boolean.time_and_scalar,boolean,"{""boolean.TimeLogicalOps.time_and_scalar"": ""class TimeLogicalOps:\n    def time_and_scalar(self):\n        self.left & True\n        self.left & False\n\n    def setup(self):\n        N = 10_000\n        left, right, lmask, rmask = np.random.randint(0, 2, size=(4, N)).astype(\""bool\"")\n        self.left = pd.arrays.BooleanArray(left, lmask)\n        self.right = pd.arrays.BooleanArray(right, rmask)""}",boolean.time_and_scalar,time,0.0
15137,func-level,"terminus-2,oracle",0.8987922682585705,0.8987922682585705,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.time_align,categoricals,"{""categoricals.Indexing.time_align"": ""class Indexing:\n    def time_align(self):\n        pd.DataFrame({\""a\"": self.series, \""b\"": self.series[:500]})\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self):\n        N = 10**5\n        self.index = pd.CategoricalIndex(range(N), range(N))\n        self.series = pd.Series(range(N), index=self.index).sort_index()\n        self.category = self.index[500]""}",categoricals.time_align,time,0.0
15141,func-level,"terminus-2,oracle",0.9492977282479867,0.9492977282479867,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.time_datetimes,categoricals,"{""categoricals.Constructor.time_datetimes"": ""class Constructor:\n    def time_datetimes(self):\n        pd.Categorical(self.datetimes)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Constructor:\n    def setup(self):\n        N = 10**5\n        self.categories = list(\""abcde\"")\n        self.cat_idx = pd.Index(self.categories)\n        self.values = np.tile(self.categories, N)\n        self.codes = np.tile(range(len(self.categories)), N)\n    \n        self.datetimes = pd.Series(\n            pd.date_range(\""1995-01-01 00:00:00\"", periods=N / 10, freq=\""s\"")\n        )\n        self.datetimes_with_nat = self.datetimes.copy()\n        self.datetimes_with_nat.iloc[-1] = pd.NaT\n    \n        self.values_some_nan = list(np.tile(self.categories + [np.nan], N))\n        self.values_all_nan = [np.nan] * len(self.values)\n        self.values_all_int8 = np.ones(N, \""int8\"")\n        self.categorical = pd.Categorical(self.values, self.categories)\n        self.series = pd.Series(self.categorical)\n        self.intervals = pd.interval_range(0, 1, periods=N // 10)""}",categoricals.time_datetimes,time,0.0
15136,func-level,"terminus-2,oracle",0.9732328002519358,0.9732328002519358,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,boolean.time_xor_array,boolean,"{""boolean.TimeLogicalOps.time_xor_array"": ""class TimeLogicalOps:\n    def time_xor_array(self):\n        self.left ^ self.right\n\n    def setup(self):\n        N = 10_000\n        left, right, lmask, rmask = np.random.randint(0, 2, size=(4, N)).astype(\""bool\"")\n        self.left = pd.arrays.BooleanArray(left, lmask)\n        self.right = pd.arrays.BooleanArray(right, rmask)""}",boolean.time_xor_array,time,0.0
15145,func-level,"terminus-2,oracle",0.9347060925143976,0.9347060925143976,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.time_getitem_bool_array,categoricals,"{""categoricals.CategoricalSlicing.time_getitem_bool_array"": ""class CategoricalSlicing:\n    def time_getitem_bool_array(self, index):\n        self.data[self.data == self.cat_scalar]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalSlicing:\n    def setup(self, index):\n        N = 10**6\n        categories = [\""a\"", \""b\"", \""c\""]\n        values = [0] * N + [1] * N + [2] * N\n        if index == \""monotonic_incr\"":\n            self.data = pd.Categorical.from_codes(values, categories=categories)\n        elif index == \""monotonic_decr\"":\n            self.data = pd.Categorical.from_codes(\n                list(reversed(values)), categories=categories\n            )\n        elif index == \""non_monotonic\"":\n            self.data = pd.Categorical.from_codes([0, 1, 2] * N, categories=categories)\n        else:\n            raise ValueError(f\""Invalid index param: {index}\"")\n    \n        self.scalar = 10000\n        self.list = list(range(10000))\n        self.cat_scalar = \""b\""""}",categoricals.time_getitem_bool_array,time,0.0
15144,func-level,"terminus-2,oracle",0.9586333781694028,0.9586333781694028,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.time_from_codes_all_int8,categoricals,"{""categoricals.Constructor.time_from_codes_all_int8"": ""class Constructor:\n    def time_from_codes_all_int8(self):\n        pd.Categorical.from_codes(self.values_all_int8, self.categories)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Constructor:\n    def setup(self):\n        N = 10**5\n        self.categories = list(\""abcde\"")\n        self.cat_idx = pd.Index(self.categories)\n        self.values = np.tile(self.categories, N)\n        self.codes = np.tile(range(len(self.categories)), N)\n    \n        self.datetimes = pd.Series(\n            pd.date_range(\""1995-01-01 00:00:00\"", periods=N / 10, freq=\""s\"")\n        )\n        self.datetimes_with_nat = self.datetimes.copy()\n        self.datetimes_with_nat.iloc[-1] = pd.NaT\n    \n        self.values_some_nan = list(np.tile(self.categories + [np.nan], N))\n        self.values_all_nan = [np.nan] * len(self.values)\n        self.values_all_int8 = np.ones(N, \""int8\"")\n        self.categorical = pd.Categorical(self.values, self.categories)\n        self.series = pd.Series(self.categorical)\n        self.intervals = pd.interval_range(0, 1, periods=N // 10)""}",categoricals.time_from_codes_all_int8,time,0.0
15142,func-level,"terminus-2,oracle",0.9466848608289778,0.9466848608289778,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.time_existing_series,categoricals,"{""categoricals.Constructor.time_existing_series"": ""class Constructor:\n    def time_existing_series(self):\n        pd.Categorical(self.series)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Constructor:\n    def setup(self):\n        N = 10**5\n        self.categories = list(\""abcde\"")\n        self.cat_idx = pd.Index(self.categories)\n        self.values = np.tile(self.categories, N)\n        self.codes = np.tile(range(len(self.categories)), N)\n    \n        self.datetimes = pd.Series(\n            pd.date_range(\""1995-01-01 00:00:00\"", periods=N / 10, freq=\""s\"")\n        )\n        self.datetimes_with_nat = self.datetimes.copy()\n        self.datetimes_with_nat.iloc[-1] = pd.NaT\n    \n        self.values_some_nan = list(np.tile(self.categories + [np.nan], N))\n        self.values_all_nan = [np.nan] * len(self.values)\n        self.values_all_int8 = np.ones(N, \""int8\"")\n        self.categorical = pd.Categorical(self.values, self.categories)\n        self.series = pd.Series(self.categorical)\n        self.intervals = pd.interval_range(0, 1, periods=N // 10)""}",categoricals.time_existing_series,time,0.0
15150,func-level,"terminus-2,oracle",0.8876400520380507,0.8876400520380507,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.time_rank_string_cat,categoricals,"{""categoricals.Rank.time_rank_string_cat"": ""class Rank:\n    def time_rank_string_cat(self):\n        self.s_str_cat.rank()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Rank:\n    def setup(self):\n        N = 10**5\n        ncats = 15\n    \n        self.s_str = pd.Series(tm.makeCategoricalIndex(N, ncats)).astype(str)\n        self.s_str_cat = pd.Series(self.s_str, dtype=\""category\"")\n        with warnings.catch_warnings(record=True):\n            str_cat_type = pd.CategoricalDtype(set(self.s_str), ordered=True)\n            self.s_str_cat_ordered = self.s_str.astype(str_cat_type)\n    \n        self.s_int = pd.Series(np.random.randint(0, ncats, size=N))\n        self.s_int_cat = pd.Series(self.s_int, dtype=\""category\"")\n        with warnings.catch_warnings(record=True):\n            int_cat_type = pd.CategoricalDtype(set(self.s_int), ordered=True)\n            self.s_int_cat_ordered = self.s_int.astype(int_cat_type)""}",categoricals.time_rank_string_cat,time,0.0
15149,func-level,"terminus-2,oracle",0.8989628497659634,0.8989628497659634,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.time_rank_int,categoricals,"{""categoricals.Rank.time_rank_int"": ""class Rank:\n    def time_rank_int(self):\n        self.s_int.rank()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Rank:\n    def setup(self):\n        N = 10**5\n        ncats = 15\n    \n        self.s_str = pd.Series(tm.makeCategoricalIndex(N, ncats)).astype(str)\n        self.s_str_cat = pd.Series(self.s_str, dtype=\""category\"")\n        with warnings.catch_warnings(record=True):\n            str_cat_type = pd.CategoricalDtype(set(self.s_str), ordered=True)\n            self.s_str_cat_ordered = self.s_str.astype(str_cat_type)\n    \n        self.s_int = pd.Series(np.random.randint(0, ncats, size=N))\n        self.s_int_cat = pd.Series(self.s_int, dtype=\""category\"")\n        with warnings.catch_warnings(record=True):\n            int_cat_type = pd.CategoricalDtype(set(self.s_int), ordered=True)\n            self.s_int_cat_ordered = self.s_int.astype(int_cat_type)""}",categoricals.time_rank_int,time,0.0
15140,func-level,"terminus-2,oracle",0.9760363826929792,0.9760363826929792,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.time_categorical_contains,categoricals,"{""categoricals.SearchSorted.time_categorical_contains"": ""class SearchSorted:\n    def time_categorical_contains(self):\n        self.c.searchsorted(self.key)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SearchSorted:\n    def setup(self):\n        N = 10**5\n        self.ci = tm.makeCategoricalIndex(N).sort_values()\n        self.c = self.ci.values\n        self.key = self.ci.categories[1]""}",categoricals.time_categorical_contains,time,0.0
15147,func-level,"terminus-2,oracle",0.9825157394271008,0.9825157394271008,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.time_getitem_scalar,categoricals,"{""categoricals.CategoricalSlicing.time_getitem_scalar"": ""class CategoricalSlicing:\n    def time_getitem_scalar(self, index):\n        self.data[self.scalar]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalSlicing:\n    def setup(self, index):\n        N = 10**6\n        categories = [\""a\"", \""b\"", \""c\""]\n        values = [0] * N + [1] * N + [2] * N\n        if index == \""monotonic_incr\"":\n            self.data = pd.Categorical.from_codes(values, categories=categories)\n        elif index == \""monotonic_decr\"":\n            self.data = pd.Categorical.from_codes(\n                list(reversed(values)), categories=categories\n            )\n        elif index == \""non_monotonic\"":\n            self.data = pd.Categorical.from_codes([0, 1, 2] * N, categories=categories)\n        else:\n            raise ValueError(f\""Invalid index param: {index}\"")\n    \n        self.scalar = 10000\n        self.list = list(range(10000))\n        self.cat_scalar = \""b\""""}",categoricals.time_getitem_scalar,time,0.0
15148,func-level,"terminus-2,oracle",0.9665723228353378,0.9665723228353378,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.time_interval,categoricals,"{""categoricals.Constructor.time_interval"": ""class Constructor:\n    def time_interval(self):\n        pd.Categorical(self.datetimes, categories=self.datetimes)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Constructor:\n    def setup(self):\n        N = 10**5\n        self.categories = list(\""abcde\"")\n        self.cat_idx = pd.Index(self.categories)\n        self.values = np.tile(self.categories, N)\n        self.codes = np.tile(range(len(self.categories)), N)\n    \n        self.datetimes = pd.Series(\n            pd.date_range(\""1995-01-01 00:00:00\"", periods=N / 10, freq=\""s\"")\n        )\n        self.datetimes_with_nat = self.datetimes.copy()\n        self.datetimes_with_nat.iloc[-1] = pd.NaT\n    \n        self.values_some_nan = list(np.tile(self.categories + [np.nan], N))\n        self.values_all_nan = [np.nan] * len(self.values)\n        self.values_all_int8 = np.ones(N, \""int8\"")\n        self.categorical = pd.Categorical(self.values, self.categories)\n        self.series = pd.Series(self.categorical)\n        self.intervals = pd.interval_range(0, 1, periods=N // 10)""}",categoricals.time_interval,time,0.0
15151,func-level,"terminus-2,oracle",0.9182493888704965,0.9182493888704965,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.time_reindex_missing,categoricals,"{""categoricals.Indexing.time_reindex_missing"": ""class Indexing:\n    def time_reindex_missing(self):\n        self.index.reindex([\""a\"", \""b\"", \""c\"", \""d\""])\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self):\n        N = 10**5\n        self.index = pd.CategoricalIndex(range(N), range(N))\n        self.series = pd.Series(range(N), index=self.index).sort_index()\n        self.category = self.index[500]""}",categoricals.time_reindex_missing,time,0.0
15143,func-level,"terminus-2,oracle",0.9435737819115144,0.9435737819115144,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.time_fastpath,categoricals,"{""categoricals.Constructor.time_fastpath"": ""class Constructor:\n    def time_fastpath(self):\n        pd.Categorical(self.codes, self.cat_idx, fastpath=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Constructor:\n    def setup(self):\n        N = 10**5\n        self.categories = list(\""abcde\"")\n        self.cat_idx = pd.Index(self.categories)\n        self.values = np.tile(self.categories, N)\n        self.codes = np.tile(range(len(self.categories)), N)\n    \n        self.datetimes = pd.Series(\n            pd.date_range(\""1995-01-01 00:00:00\"", periods=N / 10, freq=\""s\"")\n        )\n        self.datetimes_with_nat = self.datetimes.copy()\n        self.datetimes_with_nat.iloc[-1] = pd.NaT\n    \n        self.values_some_nan = list(np.tile(self.categories + [np.nan], N))\n        self.values_all_nan = [np.nan] * len(self.values)\n        self.values_all_int8 = np.ones(N, \""int8\"")\n        self.categorical = pd.Categorical(self.values, self.categories)\n        self.series = pd.Series(self.categorical)\n        self.intervals = pd.interval_range(0, 1, periods=N // 10)""}",categoricals.time_fastpath,time,0.0
15153,func-level,"terminus-2,oracle",0.696597972308575,0.696597972308575,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.time_set_categories,categoricals,"{""categoricals.SetCategories.time_set_categories"": ""class SetCategories:\n    def time_set_categories(self):\n        self.ts.cat.set_categories(self.ts.cat.categories[::2])\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SetCategories:\n    def setup(self):\n        n = 5 * 10**5\n        arr = [f\""s{i:04d}\"" for i in np.random.randint(0, n // 10, size=n)]\n        self.ts = pd.Series(arr).astype(\""category\"")""}",categoricals.time_set_categories,time,0.0
15152,func-level,"terminus-2,oracle",0.7523904029259222,0.7523904029259222,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.time_remove_categories,categoricals,"{""categoricals.RemoveCategories.time_remove_categories"": ""class RemoveCategories:\n    def time_remove_categories(self):\n        self.ts.cat.remove_categories(self.ts.cat.categories[::2])\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass RemoveCategories:\n    def setup(self):\n        n = 5 * 10**5\n        arr = [f\""s{i:04d}\"" for i in np.random.randint(0, n // 10, size=n)]\n        self.ts = pd.Series(arr).astype(\""category\"")""}",categoricals.time_remove_categories,time,0.0
15160,func-level,"terminus-2,oracle",1.0235979911278916,1.0235979911278916,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.time_pandas_dtype,dtypes,"{""dtypes.Dtypes.time_pandas_dtype"": ""class Dtypes:\n    def time_pandas_dtype(self, dtype):\n        pandas_dtype(dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)""}",dtypes.time_pandas_dtype,time,0.0
15155,func-level,"terminus-2,oracle",0.9514097224264996,0.9514097224264996,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.time_sort_values,categoricals,"{""categoricals.Indexing.time_sort_values"": ""class Indexing:\n    def time_sort_values(self):\n        self.index.sort_values(ascending=False)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self):\n        N = 10**5\n        self.index = pd.CategoricalIndex(range(N), range(N))\n        self.series = pd.Series(range(N), index=self.index).sort_index()\n        self.category = self.index[500]""}",categoricals.time_sort_values,time,0.0
15156,func-level,"terminus-2,oracle",0.9777255532200974,0.9777255532200974,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.time_unique,categoricals,"{""categoricals.Indexing.time_unique"": ""class Indexing:\n    def time_unique(self):\n        self.index.unique()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self):\n        N = 10**5\n        self.index = pd.CategoricalIndex(range(N), range(N))\n        self.series = pd.Series(range(N), index=self.index).sort_index()\n        self.category = self.index[500]""}",categoricals.time_unique,time,0.0
15162,func-level,"terminus-2,oracle",0.9307347810611114,0.9307347810611114,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.time_select_dtype_bool_include,dtypes,"{""dtypes.SelectDtypes.time_select_dtype_bool_include"": ""class SelectDtypes:\n    def time_select_dtype_bool_include(self, dtype):\n        self.df_bool.select_dtypes(include=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )""}",dtypes.time_select_dtype_bool_include,time,0.0
15158,func-level,"terminus-2,oracle",0.9767446348584446,0.9767446348584446,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,ctors.time_from_list_of_dates,ctors,"{""ctors.DatetimeIndexConstructor.time_from_list_of_dates"": ""class DatetimeIndexConstructor:\n    def time_from_list_of_dates(self):\n        DatetimeIndex(self.list_of_dates)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndexConstructor:\n    def setup(self):\n        N = 20_000\n        dti = date_range(\""1900-01-01\"", periods=N)\n    \n        self.list_of_timestamps = dti.tolist()\n        self.list_of_dates = dti.date.tolist()\n        self.list_of_datetimes = dti.to_pydatetime().tolist()\n        self.list_of_str = dti.strftime(\""%Y-%m-%d\"").tolist()""}",ctors.time_from_list_of_dates,time,0.0
15169,func-level,"terminus-2,oracle",1.159924219526538,1.159924219526538,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.time_chained_cmp,eval,"{""eval.Eval.time_chained_cmp"": ""class Eval:\n    def time_chained_cmp(self, engine, threads):\n        pd.eval(\""self.df < self.df2 < self.df3 < self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)""}",eval.time_chained_cmp,time,0.0
15159,func-level,"terminus-2,oracle",0.9075099787978216,0.9075099787978216,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,ctors.time_series_constructor,ctors,"{""ctors.SeriesConstructors.time_series_constructor"": ""class SeriesConstructors:\n    def time_series_constructor(self, data_fmt, with_index, dtype):\n        Series(self.data, index=self.index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesConstructors:\n    def setup(self, data_fmt, with_index, dtype):\n        if data_fmt in (gen_of_str, gen_of_tuples) and with_index:\n            raise NotImplementedError(\n                \""Series constructors do not support using generators with indexes\""\n            )\n        N = 10**4\n        if dtype == \""float\"":\n            arr = np.random.randn(N)\n        else:\n            arr = np.arange(N)\n        self.data = data_fmt(arr)\n        self.index = np.arange(N) if with_index else None""}",ctors.time_series_constructor,time,0.0
15163,func-level,"terminus-2,oracle",0.9469967072015046,0.9469967072015046,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.time_select_dtype_float_include,dtypes,"{""dtypes.SelectDtypes.time_select_dtype_float_include"": ""class SelectDtypes:\n    def time_select_dtype_float_include(self, dtype):\n        self.df_float.select_dtypes(include=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )""}",dtypes.time_select_dtype_float_include,time,0.0
15164,func-level,"terminus-2,oracle",1.0679607860056457,1.0679607860056457,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.time_select_dtype_int_exclude,dtypes,"{""dtypes.SelectDtypes.time_select_dtype_int_exclude"": ""class SelectDtypes:\n    def time_select_dtype_int_exclude(self, dtype):\n        self.df_int.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )""}",dtypes.time_select_dtype_int_exclude,time,0.0
15174,func-level,"terminus-2,oracle",1.0681690669423896,1.0681690669423896,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.time_dict_of_categoricals,frame_ctor,"{""frame_ctor.FromDicts.time_dict_of_categoricals"": ""class FromDicts:\n    def time_dict_of_categoricals(self):\n        # dict of arrays that we won't consolidate\n        DataFrame(self.dict_of_categoricals)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDicts:\n    def setup(self):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n        frame = DataFrame(np.random.randn(N, K), index=self.index, columns=self.columns)\n        self.data = frame.to_dict()\n        self.dict_list = frame.to_dict(orient=\""records\"")\n        self.data2 = {i: {j: float(j) for j in range(100)} for i in range(2000)}\n    \n        # arrays which we won't consolidate\n        self.dict_of_categoricals = {i: Categorical(np.arange(N)) for i in range(K)}""}",frame_ctor.time_dict_of_categoricals,time,0.0
15154,func-level,"terminus-2,oracle",0.9865113562865824,0.9865113562865824,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.time_shallow_copy,categoricals,"{""categoricals.Indexing.time_shallow_copy"": ""class Indexing:\n    def time_shallow_copy(self):\n        self.index._view()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self):\n        N = 10**5\n        self.index = pd.CategoricalIndex(range(N), range(N))\n        self.series = pd.Series(range(N), index=self.index).sort_index()\n        self.category = self.index[500]""}",categoricals.time_shallow_copy,time,0.0
15167,func-level,"terminus-2,oracle",1.2709339453302828,1.2709339453302828,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.time_add,eval,"{""eval.Eval.time_add"": ""class Eval:\n    def time_add(self, engine, threads):\n        pd.eval(\""self.df + self.df2 + self.df3 + self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)""}",eval.time_add,time,0.0
15176,func-level,"terminus-2,oracle",1.109764724574163,1.109764724574163,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.time_frame_from_arrays_sparse,frame_ctor,"{""frame_ctor.FromArrays.time_frame_from_arrays_sparse"": ""class FromArrays:\n    def time_frame_from_arrays_sparse(self):\n        self.df = DataFrame._from_arrays(\n            self.sparse_arrays,\n            index=self.index,\n            columns=self.columns,\n            verify_integrity=False,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromArrays:\n    def setup(self):\n        N_rows = 1000\n        N_cols = 1000\n        self.float_arrays = [np.random.randn(N_rows) for _ in range(N_cols)]\n        self.sparse_arrays = [\n            pd.arrays.SparseArray(np.random.randint(0, 2, N_rows), dtype=\""float64\"")\n            for _ in range(N_cols)\n        ]\n        self.int_arrays = [\n            pd.array(np.random.randint(1000, size=N_rows), dtype=\""Int64\"")\n            for _ in range(N_cols)\n        ]\n        self.index = pd.Index(range(N_rows))\n        self.columns = pd.Index(range(N_cols))""}",frame_ctor.time_frame_from_arrays_sparse,time,0.0
15157,func-level,"terminus-2,oracle",0.9471538882319472,0.9471538882319472,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.time_with_nan,categoricals,"{""categoricals.Constructor.time_with_nan"": ""class Constructor:\n    def time_with_nan(self):\n        pd.Categorical(self.values_some_nan)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Constructor:\n    def setup(self):\n        N = 10**5\n        self.categories = list(\""abcde\"")\n        self.cat_idx = pd.Index(self.categories)\n        self.values = np.tile(self.categories, N)\n        self.codes = np.tile(range(len(self.categories)), N)\n    \n        self.datetimes = pd.Series(\n            pd.date_range(\""1995-01-01 00:00:00\"", periods=N / 10, freq=\""s\"")\n        )\n        self.datetimes_with_nat = self.datetimes.copy()\n        self.datetimes_with_nat.iloc[-1] = pd.NaT\n    \n        self.values_some_nan = list(np.tile(self.categories + [np.nan], N))\n        self.values_all_nan = [np.nan] * len(self.values)\n        self.values_all_int8 = np.ones(N, \""int8\"")\n        self.categorical = pd.Categorical(self.values, self.categories)\n        self.series = pd.Series(self.categorical)\n        self.intervals = pd.interval_range(0, 1, periods=N // 10)""}",categoricals.time_with_nan,time,0.0
15175,func-level,"terminus-2,oracle",1.0145578631642584,1.0145578631642584,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.time_dict_with_timestamp_offsets,frame_ctor,"{""frame_ctor.FromDictwithTimestamp.time_dict_with_timestamp_offsets"": ""class FromDictwithTimestamp:\n    def time_dict_with_timestamp_offsets(self, offset):\n        DataFrame(self.d)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDictwithTimestamp:\n    def setup(self, offset):\n        N = 10**3\n        idx = date_range(Timestamp(\""1/1/1900\""), freq=offset, periods=N)\n        df = DataFrame(np.random.randn(N, 10), index=idx)\n        self.d = df.to_dict()""}",frame_ctor.time_dict_with_timestamp_offsets,time,0.0
15166,func-level,"terminus-2,oracle",0.950460614656845,0.950460614656845,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.time_select_dtype_string_include,dtypes,"{""dtypes.SelectDtypes.time_select_dtype_string_include"": ""class SelectDtypes:\n    def time_select_dtype_string_include(self, dtype):\n        self.df_string.select_dtypes(include=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )""}",dtypes.time_select_dtype_string_include,time,0.0
15177,func-level,"terminus-2,oracle",1.0154414722650773,1.0154414722650773,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.time_frame_from_lists,frame_ctor,"{""frame_ctor.FromLists.time_frame_from_lists"": ""class FromLists:\n    def time_frame_from_lists(self):\n        self.df = DataFrame(self.data)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromLists:\n    def setup(self):\n        N = 1000\n        M = 100\n        self.data = [list(range(M)) for i in range(N)]""}",frame_ctor.time_frame_from_lists,time,0.0
15170,func-level,"terminus-2,oracle",1.3893436394248913,1.3893436394248913,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.time_mult,eval,"{""eval.Eval.time_mult"": ""class Eval:\n    def time_mult(self, engine, threads):\n        pd.eval(\""self.df * self.df2 * self.df3 * self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)""}",eval.time_mult,time,0.0
15172,func-level,"terminus-2,oracle",1.1546050789147206,1.1546050789147206,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.time_query_with_boolean_selection,eval,"{""eval.Query.time_query_with_boolean_selection"": ""class Query:\n    def time_query_with_boolean_selection(self):\n        self.df.query(\""(a >= @self.min_val) & (a <= @self.max_val)\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Query:\n    def setup(self):\n        N = 10**6\n        halfway = (N // 2) - 1\n        index = pd.date_range(\""20010101\"", periods=N, freq=\""T\"")\n        s = pd.Series(index)\n        self.ts = s.iloc[halfway]\n        self.df = pd.DataFrame({\""a\"": np.random.randn(N), \""dates\"": index}, index=index)\n        data = np.random.randn(N)\n        self.min_val = data.min()\n        self.max_val = data.max()""}",eval.time_query_with_boolean_selection,time,0.0
15161,func-level,"terminus-2,oracle",0.9843871123238844,0.9843871123238844,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.time_pandas_dtype_invalid,dtypes,"{""dtypes.DtypesInvalid.time_pandas_dtype_invalid"": ""class DtypesInvalid:\n    def time_pandas_dtype_invalid(self, dtype):\n        try:\n            pandas_dtype(self.data_dict[dtype])\n        except TypeError:\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)""}",dtypes.time_pandas_dtype_invalid,time,0.0
15173,func-level,"terminus-2,oracle",0.994961338034834,0.994961338034834,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,finalize.time_finalize_micro,finalize,"{""finalize.Finalize.time_finalize_micro"": ""class Finalize:\n    def time_finalize_micro(self, param):\n        self.obj.__finalize__(self.obj, method=\""__finalize__\"")\n\n    def setup(self, param):\n        N = 1000\n        obj = param(dtype=float)\n        for i in range(N):\n            obj.attrs[i] = i\n        self.obj = obj""}",finalize.time_finalize_micro,time,0.0
15171,func-level,"terminus-2,oracle",1.1317455443362747,1.1317455443362747,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.time_query_datetime_column,eval,"{""eval.Query.time_query_datetime_column"": ""class Query:\n    def time_query_datetime_column(self):\n        self.df.query(\""dates < @self.ts\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Query:\n    def setup(self):\n        N = 10**6\n        halfway = (N // 2) - 1\n        index = pd.date_range(\""20010101\"", periods=N, freq=\""T\"")\n        s = pd.Series(index)\n        self.ts = s.iloc[halfway]\n        self.df = pd.DataFrame({\""a\"": np.random.randn(N), \""dates\"": index}, index=index)\n        data = np.random.randn(N)\n        self.min_val = data.min()\n        self.max_val = data.max()""}",eval.time_query_datetime_column,time,0.0
15178,func-level,"terminus-2,oracle",1.048155438963408,1.048155438963408,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.time_frame_from_records_generator,frame_ctor,"{""frame_ctor.FromRecords.time_frame_from_records_generator"": ""class FromRecords:\n    def time_frame_from_records_generator(self, nrows):\n        # issue-6700\n        self.df = DataFrame.from_records(self.gen, nrows=nrows)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromRecords:\n    def setup(self, nrows):\n        N = 100000\n        self.gen = ((x, (x * 20), (x * 100)) for x in range(N))""}",frame_ctor.time_frame_from_records_generator,time,0.0
15165,func-level,"terminus-2,oracle",1.1441580130964004,1.1441580130964004,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.time_select_dtype_string_exclude,dtypes,"{""dtypes.SelectDtypes.time_select_dtype_string_exclude"": ""class SelectDtypes:\n    def time_select_dtype_string_exclude(self, dtype):\n        self.df_string.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )""}",dtypes.time_select_dtype_string_exclude,time,0.0
15168,func-level,"terminus-2,oracle",1.1345405893436904,1.1345405893436904,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.time_and,eval,"{""eval.Eval.time_and"": ""class Eval:\n    def time_and(self, engine, threads):\n        pd.eval(\n            \""(self.df > 0) & (self.df2 > 0) & (self.df3 > 0) & (self.df4 > 0)\"",\n            engine=engine,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)""}",eval.time_and,time,0.0
15181,func-level,"terminus-2,oracle",1.0644373832418372,1.0644373832418372,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.time_list_of_dict,frame_ctor,"{""frame_ctor.FromDicts.time_list_of_dict"": ""class FromDicts:\n    def time_list_of_dict(self):\n        DataFrame(self.dict_list)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDicts:\n    def setup(self):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n        frame = DataFrame(np.random.randn(N, K), index=self.index, columns=self.columns)\n        self.data = frame.to_dict()\n        self.dict_list = frame.to_dict(orient=\""records\"")\n        self.data2 = {i: {j: float(j) for j in range(100)} for i in range(2000)}\n    \n        # arrays which we won't consolidate\n        self.dict_of_categoricals = {i: Categorical(np.arange(N)) for i in range(K)}""}",frame_ctor.time_list_of_dict,time,0.0
15182,func-level,"terminus-2,oracle",1.0588739842694828,1.0588739842694828,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.time_nested_dict_int64,frame_ctor,"{""frame_ctor.FromDicts.time_nested_dict_int64"": ""class FromDicts:\n    def time_nested_dict_int64(self):\n        # nested dict, integer indexes, regression described in #621\n        DataFrame(self.data2)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDicts:\n    def setup(self):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n        frame = DataFrame(np.random.randn(N, K), index=self.index, columns=self.columns)\n        self.data = frame.to_dict()\n        self.dict_list = frame.to_dict(orient=\""records\"")\n        self.data2 = {i: {j: float(j) for j in range(100)} for i in range(2000)}\n    \n        # arrays which we won't consolidate\n        self.dict_of_categoricals = {i: Categorical(np.arange(N)) for i in range(K)}""}",frame_ctor.time_nested_dict_int64,time,0.0
15183,func-level,"terminus-2,oracle",1.028257179873277,1.028257179873277,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_apply_axis_1,frame_methods,"{""frame_methods.Apply.time_apply_axis_1"": ""class Apply:\n    def time_apply_axis_1(self):\n        self.df.apply(lambda x: x + 1, axis=1)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self):\n        self.df = DataFrame(np.random.randn(1000, 100))\n    \n        self.s = Series(np.arange(1028.0))\n        self.df2 = DataFrame({i: self.s for i in range(1028)})\n        self.df3 = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))""}",frame_methods.time_apply_axis_1,time,0.0
15185,func-level,"terminus-2,oracle",1.0717508688341202,1.0717508688341202,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_apply_np_mean,frame_methods,"{""frame_methods.Apply.time_apply_np_mean"": ""class Apply:\n    def time_apply_np_mean(self):\n        self.df.apply(np.mean)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self):\n        self.df = DataFrame(np.random.randn(1000, 100))\n    \n        self.s = Series(np.arange(1028.0))\n        self.df2 = DataFrame({i: self.s for i in range(1028)})\n        self.df3 = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))""}",frame_methods.time_apply_np_mean,time,0.0
15186,func-level,"terminus-2,oracle",1.0948485647922328,1.0948485647922328,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_apply_pass_thru,frame_methods,"{""frame_methods.Apply.time_apply_pass_thru"": ""class Apply:\n    def time_apply_pass_thru(self):\n        self.df.apply(lambda x: x)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self):\n        self.df = DataFrame(np.random.randn(1000, 100))\n    \n        self.s = Series(np.arange(1028.0))\n        self.df2 = DataFrame({i: self.s for i in range(1028)})\n        self.df3 = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))""}",frame_methods.time_apply_pass_thru,time,0.0
15193,func-level,"terminus-2,oracle",1.5831426169002387,1.5831426169002387,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_frame_float_equal,frame_methods,"{""frame_methods.Equals.time_frame_float_equal"": ""class Equals:\n    def time_frame_float_equal(self):\n        self.float_df.equals(self.float_df)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Equals:\n    def setup(self):\n        N = 10**3\n        self.float_df = DataFrame(np.random.randn(N, N))\n        self.float_df_nan = self.float_df.copy()\n        self.float_df_nan.iloc[-1, -1] = np.nan\n    \n        self.object_df = DataFrame(\""foo\"", index=range(N), columns=range(N))\n        self.object_df_nan = self.object_df.copy()\n        self.object_df_nan.iloc[-1, -1] = np.nan\n    \n        self.nonunique_cols = self.object_df.copy()\n        self.nonunique_cols.columns = [\""A\""] * len(self.nonunique_cols.columns)\n        self.nonunique_cols_nan = self.nonunique_cols.copy()\n        self.nonunique_cols_nan.iloc[-1, -1] = np.nan""}",frame_methods.time_frame_float_equal,time,0.0
15188,func-level,"terminus-2,oracle",1.0485527998893167,1.0485527998893167,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_dataframe_describe,frame_methods,"{""frame_methods.Describe.time_dataframe_describe"": ""class Describe:\n    def time_dataframe_describe(self):\n        self.df.describe()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Describe:\n    def setup(self):\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(0, 100, 10**6),\n                \""b\"": np.random.randint(0, 100, 10**6),\n                \""c\"": np.random.randint(0, 100, 10**6),\n            }\n        )""}",frame_methods.time_dataframe_describe,time,0.0
15192,func-level,"terminus-2,oracle",1.0901762619178108,1.0901762619178108,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_frame_fillna,frame_methods,"{""frame_methods.Fillna.time_frame_fillna"": ""class Fillna:\n    def time_frame_fillna(self, inplace, method, dtype):\n        self.df.fillna(inplace=inplace, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Fillna:\n    def setup(self, inplace, method, dtype):\n        N, M = 10000, 100\n        if dtype in (\""datetime64[ns]\"", \""datetime64[ns, tz]\"", \""timedelta64[ns]\""):\n            data = {\n                \""datetime64[ns]\"": date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n                \""datetime64[ns, tz]\"": date_range(\n                    \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n                ),\n                \""timedelta64[ns]\"": timedelta_range(start=\""1 day\"", periods=N, freq=\""1D\""),\n            }\n            self.df = DataFrame({f\""col_{i}\"": data[dtype] for i in range(M)})\n            self.df[::2] = None\n        else:\n            values = np.random.randn(N, M)\n            values[::2] = np.nan\n            if dtype == \""Int64\"":\n                values = values.round()\n            self.df = DataFrame(values, dtype=dtype)""}",frame_methods.time_frame_fillna,time,0.0
15180,func-level,"terminus-2,oracle",1.0407155721351138,1.0407155721351138,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.time_frame_from_scalar_ea_float64_na,frame_ctor,"{""frame_ctor.FromScalar.time_frame_from_scalar_ea_float64_na"": ""class FromScalar:\n    def time_frame_from_scalar_ea_float64_na(self):\n        DataFrame(\n            NA,\n            index=range(self.nrows),\n            columns=list(\""abc\""),\n            dtype=Float64Dtype(),\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromScalar:\n    def setup(self):\n        self.nrows = 100_000""}",frame_ctor.time_frame_from_scalar_ea_float64_na,time,0.0
15184,func-level,"terminus-2,oracle",1.0553971358160663,1.0553971358160663,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_apply_lambda_mean,frame_methods,"{""frame_methods.Apply.time_apply_lambda_mean"": ""class Apply:\n    def time_apply_lambda_mean(self):\n        self.df.apply(lambda x: x.mean())\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self):\n        self.df = DataFrame(np.random.randn(1000, 100))\n    \n        self.s = Series(np.arange(1028.0))\n        self.df2 = DataFrame({i: self.s for i in range(1028)})\n        self.df3 = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))""}",frame_methods.time_apply_lambda_mean,time,0.0
15179,func-level,"terminus-2,oracle",1.047063214802518,1.047063214802518,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.time_frame_from_scalar_ea_float64,frame_ctor,"{""frame_ctor.FromScalar.time_frame_from_scalar_ea_float64"": ""class FromScalar:\n    def time_frame_from_scalar_ea_float64(self):\n        DataFrame(\n            1.0,\n            index=range(self.nrows),\n            columns=list(\""abc\""),\n            dtype=Float64Dtype(),\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromScalar:\n    def setup(self):\n        self.nrows = 100_000""}",frame_ctor.time_frame_from_scalar_ea_float64,time,0.0
15187,func-level,"terminus-2,oracle",1.1014807750057138,1.1014807750057138,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_count_level_mixed_dtypes_multi,frame_methods,"{""frame_methods.Count.time_count_level_mixed_dtypes_multi"": ""class Count:\n    def time_count_level_mixed_dtypes_multi(self, axis):\n        self.df_mixed.count(axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Count:\n    def setup(self, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""\n    \n        self.df.index = MultiIndex.from_arrays([self.df.index, self.df.index])\n        self.df.columns = MultiIndex.from_arrays([self.df.columns, self.df.columns])\n        self.df_mixed.index = MultiIndex.from_arrays(\n            [self.df_mixed.index, self.df_mixed.index]\n        )\n        self.df_mixed.columns = MultiIndex.from_arrays(\n            [self.df_mixed.columns, self.df_mixed.columns]\n        )""}",frame_methods.time_count_level_mixed_dtypes_multi,time,0.0
15191,func-level,"terminus-2,oracle",1.0415164232546907,1.0415164232546907,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_frame_duplicated_wide,frame_methods,"{""frame_methods.Duplicated.time_frame_duplicated_wide"": ""class Duplicated:\n    def time_frame_duplicated_wide(self):\n        self.df2.duplicated()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Duplicated:\n    def setup(self):\n        n = 1 << 20\n        t = date_range(\""2015-01-01\"", freq=\""S\"", periods=(n // 64))\n        xs = np.random.randn(n // 64).round(2)\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(-1 << 8, 1 << 8, n),\n                \""b\"": np.random.choice(t, n),\n                \""c\"": np.random.choice(xs, n),\n            }\n        )\n        self.df2 = DataFrame(np.random.randn(1000, 100).astype(str)).T""}",frame_methods.time_frame_duplicated_wide,time,0.0
15189,func-level,"terminus-2,oracle",1.1480368354203323,1.1480368354203323,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_dropna,frame_methods,"{""frame_methods.Dropna.time_dropna"": ""class Dropna:\n    def time_dropna(self, how, axis):\n        self.df.dropna(how=how, axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Dropna:\n    def setup(self, how, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""""}",frame_methods.time_dropna,time,0.0
15196,func-level,"terminus-2,oracle",1.0177514409094597,1.0177514409094597,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_items,frame_methods,"{""frame_methods.Iteration.time_items"": ""class Iteration:\n    def time_items(self):\n        # (monitor no-copying behaviour)\n        if hasattr(self.df, \""_item_cache\""):\n            self.df._item_cache.clear()\n        for name, col in self.df.items():\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))""}",frame_methods.time_items,time,0.0
15190,func-level,"terminus-2,oracle",1.1474070058342605,1.1474070058342605,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_dropna_axis_mixed_dtypes,frame_methods,"{""frame_methods.Dropna.time_dropna_axis_mixed_dtypes"": ""class Dropna:\n    def time_dropna_axis_mixed_dtypes(self, how, axis):\n        self.df_mixed.dropna(how=how, axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Dropna:\n    def setup(self, how, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""""}",frame_methods.time_dropna_axis_mixed_dtypes,time,0.0
15195,func-level,"terminus-2,oracle",1.026015236530584,1.026015236530584,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_interpolate_some_good,frame_methods,"{""frame_methods.Interpolate.time_interpolate_some_good"": ""class Interpolate:\n    def time_interpolate_some_good(self, downcast):\n        self.df2.interpolate(downcast=downcast)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Interpolate:\n    def setup(self, downcast):\n        N = 10000\n        # this is the worst case, where every column has NaNs.\n        arr = np.random.randn(N, 100)\n        # NB: we need to set values in array, not in df.values, otherwise\n        #  the benchmark will be misleading for ArrayManager\n        arr[::2] = np.nan\n    \n        self.df = DataFrame(arr)\n    \n        self.df2 = DataFrame(\n            {\n                \""A\"": np.arange(0, N),\n                \""B\"": np.random.randint(0, 100, N),\n                \""C\"": np.random.randn(N),\n                \""D\"": np.random.randn(N),\n            }\n        )\n        self.df2.loc[1::5, \""A\""] = np.nan\n        self.df2.loc[1::5, \""C\""] = np.nan""}",frame_methods.time_interpolate_some_good,time,0.0
15204,func-level,"terminus-2,oracle",1.0482445796343702,1.0482445796343702,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_series_describe,frame_methods,"{""frame_methods.Describe.time_series_describe"": ""class Describe:\n    def time_series_describe(self):\n        self.df[\""a\""].describe()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Describe:\n    def setup(self):\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(0, 100, 10**6),\n                \""b\"": np.random.randint(0, 100, 10**6),\n                \""c\"": np.random.randint(0, 100, 10**6),\n            }\n        )""}",frame_methods.time_series_describe,time,0.0
15205,func-level,"terminus-2,oracle",1.274450093142372,1.274450093142372,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_to_dict_ints,frame_methods,"{""frame_methods.ToDict.time_to_dict_ints"": ""class ToDict:\n    def time_to_dict_ints(self, orient):\n        self.int_df.to_dict(orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToDict:\n    def setup(self, orient):\n        data = np.random.randint(0, 1000, size=(10000, 4))\n        self.int_df = DataFrame(data)\n        self.datetimelike_df = self.int_df.astype(\""timedelta64[ns]\"")""}",frame_methods.time_to_dict_ints,time,0.0
15203,func-level,"terminus-2,oracle",1.1137320610096797,1.1137320610096797,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_nsmallest_two_columns,frame_methods,"{""frame_methods.NSort.time_nsmallest_two_columns"": ""class NSort:\n    def time_nsmallest_two_columns(self, keep):\n        self.df.nsmallest(100, [\""A\"", \""B\""], keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NSort:\n    def setup(self, keep):\n        self.df = DataFrame(np.random.randn(100000, 3), columns=list(\""ABC\""))""}",frame_methods.time_nsmallest_two_columns,time,0.0
15194,func-level,"terminus-2,oracle",1.0248017468505206,1.0248017468505206,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_frame_quantile,frame_methods,"{""frame_methods.Quantile.time_frame_quantile"": ""class Quantile:\n    def time_frame_quantile(self, axis):\n        self.df.quantile([0.1, 0.5], axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, axis):\n        self.df = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))""}",frame_methods.time_frame_quantile,time,0.0
15201,func-level,"terminus-2,oracle",1.1236743877828543,1.1236743877828543,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_nlargest_two_columns,frame_methods,"{""frame_methods.NSort.time_nlargest_two_columns"": ""class NSort:\n    def time_nlargest_two_columns(self, keep):\n        self.df.nlargest(100, [\""A\"", \""B\""], keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NSort:\n    def setup(self, keep):\n        self.df = DataFrame(np.random.randn(100000, 3), columns=list(\""ABC\""))""}",frame_methods.time_nlargest_two_columns,time,0.0
15202,func-level,"terminus-2,oracle",1.1223277154045053,1.1223277154045053,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_nsmallest_one_column,frame_methods,"{""frame_methods.NSort.time_nsmallest_one_column"": ""class NSort:\n    def time_nsmallest_one_column(self, keep):\n        self.df.nsmallest(100, \""A\"", keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NSort:\n    def setup(self, keep):\n        self.df = DataFrame(np.random.randn(100000, 3), columns=list(\""ABC\""))""}",frame_methods.time_nsmallest_one_column,time,0.0
15200,func-level,"terminus-2,oracle",1.07645130760125,1.07645130760125,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_itertuples_raw_start,frame_methods,"{""frame_methods.Iteration.time_itertuples_raw_start"": ""class Iteration:\n    def time_itertuples_raw_start(self):\n        self.df4.itertuples(index=False, name=None)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))""}",frame_methods.time_itertuples_raw_start,time,0.0
15206,func-level,"terminus-2,oracle",1.0380832207895176,1.0380832207895176,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_to_numpy_tall,frame_methods,"{""frame_methods.ToNumpy.time_to_numpy_tall"": ""class ToNumpy:\n    def time_to_numpy_tall(self):\n        self.df_tall.to_numpy()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToNumpy:\n    def setup(self):\n        N = 10000\n        M = 10\n        self.df_tall = DataFrame(np.random.randn(N, M))\n        self.df_wide = DataFrame(np.random.randn(M, N))\n        self.df_mixed_tall = self.df_tall.copy()\n        self.df_mixed_tall[\""foo\""] = \""bar\""\n        self.df_mixed_tall[0] = period_range(\""2000\"", periods=N)\n        self.df_mixed_tall[1] = range(N)\n        self.df_mixed_wide = self.df_wide.copy()\n        self.df_mixed_wide[\""foo\""] = \""bar\""\n        self.df_mixed_wide[0] = period_range(\""2000\"", periods=M)\n        self.df_mixed_wide[1] = range(M)""}",frame_methods.time_to_numpy_tall,time,0.0
15207,func-level,"terminus-2,oracle",1.0361173915231612,1.0361173915231612,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_to_numpy_wide,frame_methods,"{""frame_methods.ToNumpy.time_to_numpy_wide"": ""class ToNumpy:\n    def time_to_numpy_wide(self):\n        self.df_wide.to_numpy()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToNumpy:\n    def setup(self):\n        N = 10000\n        M = 10\n        self.df_tall = DataFrame(np.random.randn(N, M))\n        self.df_wide = DataFrame(np.random.randn(M, N))\n        self.df_mixed_tall = self.df_tall.copy()\n        self.df_mixed_tall[\""foo\""] = \""bar\""\n        self.df_mixed_tall[0] = period_range(\""2000\"", periods=N)\n        self.df_mixed_tall[1] = range(N)\n        self.df_mixed_wide = self.df_wide.copy()\n        self.df_mixed_wide[\""foo\""] = \""bar\""\n        self.df_mixed_wide[0] = period_range(\""2000\"", periods=M)\n        self.df_mixed_wide[1] = range(M)""}",frame_methods.time_to_numpy_wide,time,0.0
15199,func-level,"terminus-2,oracle",1.1062177869730845,1.1062177869730845,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_iterrows,frame_methods,"{""frame_methods.Iteration.time_iterrows"": ""class Iteration:\n    def time_iterrows(self):\n        for row in self.df.iterrows():\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))""}",frame_methods.time_iterrows,time,0.0
15208,func-level,"terminus-2,oracle",1.060012268918662,1.060012268918662,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_values_tall,frame_methods,"{""frame_methods.ToNumpy.time_values_tall"": ""class ToNumpy:\n    def time_values_tall(self):\n        self.df_tall.values\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToNumpy:\n    def setup(self):\n        N = 10000\n        M = 10\n        self.df_tall = DataFrame(np.random.randn(N, M))\n        self.df_wide = DataFrame(np.random.randn(M, N))\n        self.df_mixed_tall = self.df_tall.copy()\n        self.df_mixed_tall[\""foo\""] = \""bar\""\n        self.df_mixed_tall[0] = period_range(\""2000\"", periods=N)\n        self.df_mixed_tall[1] = range(N)\n        self.df_mixed_wide = self.df_wide.copy()\n        self.df_mixed_wide[\""foo\""] = \""bar\""\n        self.df_mixed_wide[0] = period_range(\""2000\"", periods=M)\n        self.df_mixed_wide[1] = range(M)""}",frame_methods.time_values_tall,time,0.0
15197,func-level,"terminus-2,oracle",1.0356350267655572,1.0356350267655572,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_items_cached,frame_methods,"{""frame_methods.Iteration.time_items_cached"": ""class Iteration:\n    def time_items_cached(self):\n        for name, col in self.df.items():\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))""}",frame_methods.time_items_cached,time,0.0
15198,func-level,"terminus-2,oracle",1.0351569761026338,1.0351569761026338,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_iteritems_indexing,frame_methods,"{""frame_methods.Iteration.time_iteritems_indexing"": ""class Iteration:\n    def time_iteritems_indexing(self):\n        for col in self.df3:\n            self.df3[col]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))""}",frame_methods.time_iteritems_indexing,time,0.0
15209,func-level,"terminus-2,oracle",1.0645670397307003,1.0645670397307003,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.time_values_wide,frame_methods,"{""frame_methods.ToNumpy.time_values_wide"": ""class ToNumpy:\n    def time_values_wide(self):\n        self.df_wide.values\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToNumpy:\n    def setup(self):\n        N = 10000\n        M = 10\n        self.df_tall = DataFrame(np.random.randn(N, M))\n        self.df_wide = DataFrame(np.random.randn(M, N))\n        self.df_mixed_tall = self.df_tall.copy()\n        self.df_mixed_tall[\""foo\""] = \""bar\""\n        self.df_mixed_tall[0] = period_range(\""2000\"", periods=N)\n        self.df_mixed_tall[1] = range(N)\n        self.df_mixed_wide = self.df_wide.copy()\n        self.df_mixed_wide[\""foo\""] = \""bar\""\n        self.df_mixed_wide[0] = period_range(\""2000\"", periods=M)\n        self.df_mixed_wide[1] = range(M)""}",frame_methods.time_values_wide,time,0.0
15210,func-level,"terminus-2,oracle",1.1079542764811172,1.1079542764811172,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,gil.time_parallel,gil,"{""gil.ParallelGroupbyMethods.time_parallel"": ""class ParallelGroupbyMethods:\n    def time_parallel(self, threads, method):\n        self.parallel()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ParallelGroupbyMethods:\n    def setup(self, threads, method):\n        N = 10**6\n        ngroups = 10**3\n        df = DataFrame(\n            {\""key\"": np.random.randint(0, ngroups, size=N), \""data\"": np.random.randn(N)}\n        )\n    \n        @test_parallel(num_threads=threads)\n        def parallel():\n            getattr(df.groupby(\""key\"")[\""data\""], method)()\n    \n        self.parallel = parallel\n    \n        def loop():\n            getattr(df.groupby(\""key\"")[\""data\""], method)()\n    \n        self.loop = loop""}",gil.time_parallel,time,0.0
15213,func-level,"terminus-2,oracle",0.9242780314831636,0.9242780314831636,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.time_category_size,groupby,"{""groupby.Size.time_category_size"": ""class Size:\n    def time_category_size(self):\n        self.draws.groupby(self.cats).size()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Size:\n    def setup(self):\n        n = 10**5\n        offsets = np.random.randint(n, size=n).astype(\""timedelta64[ns]\"")\n        dates = np.datetime64(\""now\"") + offsets\n        self.df = DataFrame(\n            {\n                \""key1\"": np.random.randint(0, 500, size=n),\n                \""key2\"": np.random.randint(0, 100, size=n),\n                \""value1\"": np.random.randn(n),\n                \""value2\"": np.random.randn(n),\n                \""value3\"": np.random.randn(n),\n                \""dates\"": dates,\n            }\n        )\n        self.draws = Series(np.random.randn(n))\n        labels = Series([\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (n // 4))\n        self.cats = labels.astype(\""category\"")""}",groupby.time_category_size,time,0.0
15212,func-level,"terminus-2,oracle",0.960157811866578,0.960157811866578,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,gil.time_take1d,gil,"{""gil.ParallelTake1D.time_take1d"": ""class ParallelTake1D:\n    def time_take1d(self, dtype):\n        self.parallel_take1d()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ParallelTake1D:\n    def setup(self, dtype):\n        N = 10**6\n        df = DataFrame({\""col\"": np.arange(N, dtype=dtype)})\n        indexer = np.arange(100, len(df) - 100)\n    \n        @test_parallel(num_threads=2)\n        def parallel_take1d():\n            take_nd(df[\""col\""].values, indexer)\n    \n        self.parallel_take1d = parallel_take1d""}",gil.time_take1d,time,0.0
15227,func-level,"terminus-2,oracle",1.0838227693246487,1.0838227693246487,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_cached_properties.time_is_monotonic_decreasing,index_cached_properties,"{""index_cached_properties.IndexCache.time_is_monotonic_decreasing"": ""class IndexCache:\n    def time_is_monotonic_decreasing(self, index_type):\n        self.idx.is_monotonic_decreasing\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}""}",index_cached_properties.time_is_monotonic_decreasing,time,0.0
15215,func-level,"terminus-2,oracle",1.0599486639571911,1.0599486639571911,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.time_dtype_as_group,groupby,"{""groupby.GroupByMethods.time_dtype_as_group"": ""class GroupByMethods:\n    def time_dtype_as_group(self, dtype, method, application, ncols):\n        self.as_group_method()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupByMethods:\n    def setup(self, dtype, method, application, ncols):\n        if method in method_blocklist.get(dtype, {}):\n            raise NotImplementedError  # skip benchmark\n    \n        if ncols != 1 and method in [\""value_counts\"", \""unique\""]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if application == \""transformation\"" and method in [\n            \""describe\"",\n            \""head\"",\n            \""tail\"",\n            \""unique\"",\n            \""value_counts\"",\n            \""size\"",\n        ]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if method == \""describe\"":\n            ngroups = 20\n        elif method == \""skew\"":\n            ngroups = 100\n        else:\n            ngroups = 1000\n        size = ngroups * 2\n        rng = np.arange(ngroups).reshape(-1, 1)\n        rng = np.broadcast_to(rng, (len(rng), ncols))\n        taker = np.random.randint(0, ngroups, size=size)\n        values = rng.take(taker, axis=0)\n        if dtype == \""int\"":\n            key = np.random.randint(0, size, size=size)\n        elif dtype in (\""int16\"", \""uint\""):\n            key = np.random.randint(0, size, size=size, dtype=dtype)\n        elif dtype == \""float\"":\n            key = np.concatenate(\n                [np.random.random(ngroups) * 0.1, np.random.random(ngroups) * 10.0]\n            )\n        elif dtype == \""object\"":\n            key = [\""foo\""] * size\n        elif dtype == \""datetime\"":\n            key = date_range(\""1/1/2011\"", periods=size, freq=\""s\"")\n    \n        cols = [f\""values{n}\"" for n in range(ncols)]\n        df = DataFrame(values, columns=cols)\n        df[\""key\""] = key\n    \n        if len(cols) == 1:\n            cols = cols[0]\n    \n        if application == \""transformation\"":\n            self.as_group_method = lambda: df.groupby(\""key\"")[cols].transform(method)\n            self.as_field_method = lambda: df.groupby(cols)[\""key\""].transform(method)\n        else:\n            self.as_group_method = getattr(df.groupby(\""key\"")[cols], method)\n            self.as_field_method = getattr(df.groupby(cols)[\""key\""], method)""}",groupby.time_dtype_as_group,time,0.0
15214,func-level,"terminus-2,oracle",1.0756120963928204,1.0756120963928204,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.time_dtype_as_field,groupby,"{""groupby.GroupByMethods.time_dtype_as_field"": ""class GroupByMethods:\n    def time_dtype_as_field(self, dtype, method, application, ncols):\n        self.as_field_method()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupByMethods:\n    def setup(self, dtype, method, application, ncols):\n        if method in method_blocklist.get(dtype, {}):\n            raise NotImplementedError  # skip benchmark\n    \n        if ncols != 1 and method in [\""value_counts\"", \""unique\""]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if application == \""transformation\"" and method in [\n            \""describe\"",\n            \""head\"",\n            \""tail\"",\n            \""unique\"",\n            \""value_counts\"",\n            \""size\"",\n        ]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if method == \""describe\"":\n            ngroups = 20\n        elif method == \""skew\"":\n            ngroups = 100\n        else:\n            ngroups = 1000\n        size = ngroups * 2\n        rng = np.arange(ngroups).reshape(-1, 1)\n        rng = np.broadcast_to(rng, (len(rng), ncols))\n        taker = np.random.randint(0, ngroups, size=size)\n        values = rng.take(taker, axis=0)\n        if dtype == \""int\"":\n            key = np.random.randint(0, size, size=size)\n        elif dtype in (\""int16\"", \""uint\""):\n            key = np.random.randint(0, size, size=size, dtype=dtype)\n        elif dtype == \""float\"":\n            key = np.concatenate(\n                [np.random.random(ngroups) * 0.1, np.random.random(ngroups) * 10.0]\n            )\n        elif dtype == \""object\"":\n            key = [\""foo\""] * size\n        elif dtype == \""datetime\"":\n            key = date_range(\""1/1/2011\"", periods=size, freq=\""s\"")\n    \n        cols = [f\""values{n}\"" for n in range(ncols)]\n        df = DataFrame(values, columns=cols)\n        df[\""key\""] = key\n    \n        if len(cols) == 1:\n            cols = cols[0]\n    \n        if application == \""transformation\"":\n            self.as_group_method = lambda: df.groupby(\""key\"")[cols].transform(method)\n            self.as_field_method = lambda: df.groupby(cols)[\""key\""].transform(method)\n        else:\n            self.as_group_method = getattr(df.groupby(\""key\"")[cols], method)\n            self.as_field_method = getattr(df.groupby(cols)[\""key\""], method)""}",groupby.time_dtype_as_field,time,0.0
15219,func-level,"terminus-2,oracle",1.0230055870235066,1.0230055870235066,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.time_scalar_function_multi_col,groupby,"{""groupby.Apply.time_scalar_function_multi_col"": ""class Apply:\n    def time_scalar_function_multi_col(self, factor):\n        self.df.groupby([\""key\"", \""key2\""]).apply(lambda x: 1)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, factor):\n        N = 10**factor\n        # two cases:\n        # - small groups: small data (N**4) + many labels (2000) -> average group\n        #   size of 5 (-> larger overhead of slicing method)\n        # - larger groups: larger data (N**5) + fewer labels (20) -> average group\n        #   size of 5000\n        labels = np.random.randint(0, 2000 if factor == 4 else 20, size=N)\n        labels2 = np.random.randint(0, 3, size=N)\n        df = DataFrame(\n            {\n                \""key\"": labels,\n                \""key2\"": labels2,\n                \""value1\"": np.random.randn(N),\n                \""value2\"": [\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (N // 4),\n            }\n        )\n        self.df = df""}",groupby.time_scalar_function_multi_col,time,0.0
15225,func-level,"terminus-2,oracle",1.0353260541593894,1.0353260541593894,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,hash_functions.time_loc_slice,hash_functions,"{""hash_functions.NumericSeriesIndexing.time_loc_slice"": ""class NumericSeriesIndexing:\n    def time_loc_slice(self, index, N):\n        # trigger building of mapping\n        self.data.loc[:800]\n\n    def setup(self, dtype, N):\n        vals = np.array(list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype)\n        indices = pd.Index(vals)\n        self.data = pd.Series(np.arange(N), index=indices)""}",hash_functions.time_loc_slice,time,0.0
15211,func-level,"terminus-2,oracle",0.9677818173268288,0.9677818173268288,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,gil.time_rolling,gil,"{""gil.ParallelRolling.time_rolling"": ""class ParallelRolling:\n    def time_rolling(self, method):\n        self.parallel_rolling()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ParallelRolling:\n    def setup(self, method):\n        win = 100\n        arr = np.random.rand(100000)\n        if hasattr(DataFrame, \""rolling\""):\n            df = DataFrame(arr).rolling(win)\n    \n            @test_parallel(num_threads=2)\n            def parallel_rolling():\n                getattr(df, method)()\n    \n            self.parallel_rolling = parallel_rolling\n        elif have_rolling_methods:\n            rolling = {\n                \""median\"": rolling_median,\n                \""mean\"": rolling_mean,\n                \""min\"": rolling_min,\n                \""max\"": rolling_max,\n                \""var\"": rolling_var,\n                \""skew\"": rolling_skew,\n                \""kurt\"": rolling_kurt,\n                \""std\"": rolling_std,\n            }\n    \n            @test_parallel(num_threads=2)\n            def parallel_rolling():\n                rolling[method](arr, win)\n    \n            self.parallel_rolling = parallel_rolling\n        else:\n            raise NotImplementedError""}",gil.time_rolling,time,0.0
15217,func-level,"terminus-2,oracle",1.0188571417553025,1.0188571417553025,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.time_groupby_apply_non_unique_unsorted_index,groupby,"{""groupby.ApplyNonUniqueUnsortedIndex.time_groupby_apply_non_unique_unsorted_index"": ""class ApplyNonUniqueUnsortedIndex:\n    def time_groupby_apply_non_unique_unsorted_index(self):\n        self.df.groupby(\""key\"", group_keys=False).apply(lambda x: x)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ApplyNonUniqueUnsortedIndex:\n    def setup(self):\n        # GH 46527\n        # unsorted and non-unique index\n        idx = np.arange(100)[::-1]\n        idx = Index(np.repeat(idx, 200), name=\""key\"")\n        self.df = DataFrame(np.random.randn(len(idx), 10), index=idx)""}",groupby.time_groupby_apply_non_unique_unsorted_index,time,0.0
15221,func-level,"terminus-2,oracle",0.8147115270091495,0.8147115270091495,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.time_str_func,groupby,"{""groupby.String.time_str_func"": ""class String:\n    def time_str_func(self, dtype, method):\n        self.df.groupby(\""a\"")[self.df.columns[1:]].agg(method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass String:\n    def setup(self, dtype, method):\n        cols = list(\""abcdefghjkl\"")\n        self.df = DataFrame(\n            np.random.randint(0, 100, size=(10_000, len(cols))),\n            columns=cols,\n            dtype=dtype,\n        )""}",groupby.time_str_func,time,0.0
15222,func-level,"terminus-2,oracle",1.0355765116122002,1.0355765116122002,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.time_sum,groupby,"{""groupby.GroupManyLabels.time_sum"": ""class GroupManyLabels:\n    def time_sum(self, ncols):\n        self.df.groupby(self.labels).sum()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupManyLabels:\n    def setup(self, ncols):\n        N = 1000\n        data = np.random.randn(N, ncols)\n        self.labels = np.random.randint(0, 100, size=N)\n        self.df = DataFrame(data)""}",groupby.time_sum,time,0.0
15223,func-level,"terminus-2,oracle",1.042586536069814,1.042586536069814,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.time_transform_lambda_max_wide,groupby,"{""groupby.Transform.time_transform_lambda_max_wide"": ""class Transform:\n    def time_transform_lambda_max_wide(self):\n        self.df_wide.groupby(level=0).transform(lambda x: np.max(x, axis=0))\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]""}",groupby.time_transform_lambda_max_wide,time,0.0
15218,func-level,"terminus-2,oracle",0.7453157545798069,0.7453157545798069,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.time_rank_ties,groupby,"{""groupby.RankWithTies.time_rank_ties"": ""class RankWithTies:\n    def time_rank_ties(self, dtype, tie_method):\n        self.df.groupby(\""key\"").rank(method=tie_method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass RankWithTies:\n    def setup(self, dtype, tie_method):\n        N = 10**4\n        if dtype == \""datetime64\"":\n            data = np.array([Timestamp(\""2011/01/01\"")] * N, dtype=dtype)\n        else:\n            data = np.array([1] * N, dtype=dtype)\n        self.df = DataFrame({\""values\"": data, \""key\"": [\""foo\""] * N})""}",groupby.time_rank_ties,time,0.0
15226,func-level,"terminus-2,oracle",1.0398100956535965,1.0398100956535965,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_cached_properties.time_engine,index_cached_properties,"{""index_cached_properties.IndexCache.time_engine"": ""class IndexCache:\n    def time_engine(self, index_type):\n        self.idx._engine\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}""}",index_cached_properties.time_engine,time,0.0
15224,func-level,"terminus-2,oracle",1.1141947051174346,1.1141947051174346,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,hash_functions.time_factorize,hash_functions,"{""hash_functions.UniqueAndFactorizeArange.time_factorize"": ""class UniqueAndFactorizeArange:\n    def time_factorize(self, exponent):\n        pd.factorize(self.a2)\n\n    def setup(self, exponent):\n        a = np.arange(10**4, dtype=\""float64\"")\n        self.a2 = (a + 10**exponent).repeat(100)""}",hash_functions.time_factorize,time,0.0
15220,func-level,"terminus-2,oracle",1.1043999320046491,1.1043999320046491,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.time_series_groups,groupby,"{""groupby.Groups.time_series_groups"": ""class Groups:\n    def time_series_groups(self, data, key):\n        self.ser.groupby(self.ser).groups\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Groups:\n    def setup(self, data, key):\n        self.ser = data[key]\n\n    def setup_cache(self):\n        size = 10**6\n        data = {\n            \""int64_small\"": Series(np.random.randint(0, 100, size=size)),\n            \""int64_large\"": Series(np.random.randint(0, 10000, size=size)),\n            \""object_small\"": Series(\n                tm.makeStringIndex(100).take(np.random.randint(0, 100, size=size))\n            ),\n            \""object_large\"": Series(\n                tm.makeStringIndex(10000).take(np.random.randint(0, 10000, size=size))\n            ),\n        }\n        return data""}",groupby.time_series_groups,time,0.0
15239,func-level,"terminus-2,oracle",1.1635971122694453,1.1635971122694453,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_getitem_list_like,indexing,"{""indexing.NonNumericSeriesIndexing.time_getitem_list_like"": ""class NonNumericSeriesIndexing:\n    def time_getitem_list_like(self, index, index_structure):\n        self.s[[self.lbl]]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]"", ""indexing.NumericSeriesIndexing.time_getitem_list_like"": ""class NumericSeriesIndexing:\n    def time_getitem_list_like(self, index, index_structure):\n        self.data[[800000]]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.time_getitem_list_like,time,0.0
15216,func-level,"terminus-2,oracle",1.070948001002442,1.070948001002442,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.time_frame_transform,groupby,"{""groupby.Cumulative.time_frame_transform"": ""class Cumulative:\n    def time_frame_transform(self, dtype, method, with_nans):\n        self.df.groupby(\""key\"").transform(method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cumulative:\n    def setup(self, dtype, method, with_nans):\n        if with_nans and dtype == \""int64\"":\n            raise NotImplementedError(\""Construction of df would raise\"")\n    \n        N = 500_000\n        keys = np.random.randint(0, 100, size=N)\n        vals = np.random.randint(-10, 10, (N, 5))\n    \n        if with_nans:\n            null_vals = vals.astype(float, copy=True)\n            null_vals[::2, :] = np.nan\n            null_vals[::3, :] = np.nan\n            df = DataFrame(null_vals, columns=list(\""abcde\""), dtype=dtype)\n            df[\""key\""] = keys\n            self.df = df\n        else:\n            df = DataFrame(vals, columns=list(\""abcde\"")).astype(dtype, copy=False)\n            df[\""key\""] = keys\n            self.df = df""}",groupby.time_frame_transform,time,0.0
15231,func-level,"terminus-2,oracle",1.1140343749271642,1.1140343749271642,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_object.time_operation,index_object,"{""index_object.SetOperations.time_operation"": ""class SetOperations:\n    def time_operation(self, index_structure, dtype, method):\n        getattr(self.left, method)(self.right)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SetOperations:\n    def setup(self, index_structure, dtype, method):\n        N = 10**5\n        dates_left = date_range(\""1/1/2000\"", periods=N, freq=\""T\"")\n        fmt = \""%Y-%m-%d %H:%M:%S\""\n        date_str_left = Index(dates_left.strftime(fmt))\n        int_left = Index(np.arange(N))\n        ea_int_left = Index(np.arange(N), dtype=\""Int64\"")\n        str_left = tm.makeStringIndex(N)\n    \n        data = {\n            \""datetime\"": dates_left,\n            \""date_string\"": date_str_left,\n            \""int\"": int_left,\n            \""strings\"": str_left,\n            \""ea_int\"": ea_int_left,\n        }\n    \n        if index_structure == \""non_monotonic\"":\n            data = {k: mi[::-1] for k, mi in data.items()}\n    \n        data = {k: {\""left\"": idx, \""right\"": idx[:-1]} for k, idx in data.items()}\n    \n        self.left = data[dtype][\""left\""]\n        self.right = data[dtype][\""right\""]""}",index_object.time_operation,time,0.0
15232,func-level,"terminus-2,oracle",1.123397506290995,1.123397506290995,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_at_setitem,indexing,"{""indexing.DataFrameStringIndexing.time_at_setitem"": ""class DataFrameStringIndexing:\n    def time_at_setitem(self):\n        self.df.at[self.idx_scalar, self.col_scalar] = 0.0\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DataFrameStringIndexing:\n    def setup(self):\n        index = tm.makeStringIndex(1000)\n        columns = tm.makeStringIndex(30)\n        with warnings.catch_warnings(record=True):\n            self.df = DataFrame(np.random.randn(1000, 30), index=index, columns=columns)\n        self.idx_scalar = index[100]\n        self.col_scalar = columns[10]\n        self.bool_indexer = self.df[self.col_scalar] > 0\n        self.bool_obj_indexer = self.bool_indexer.astype(object)\n        self.boolean_indexer = (self.df[self.col_scalar] > 0).astype(\""boolean\"")""}",indexing.time_at_setitem,time,0.0
15229,func-level,"terminus-2,oracle",1.0227030008106008,1.0227030008106008,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_object.time_get,index_object,"{""index_object.Indexing.time_get"": ""class Indexing:\n    def time_get(self, dtype):\n        self.idx[1]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self, dtype):\n        N = 10**6\n        self.idx = getattr(tm, f\""make{dtype}Index\"")(N)\n        self.array_mask = (np.arange(N) % 3) == 0\n        self.series_mask = Series(self.array_mask)\n        self.sorted = self.idx.sort_values()\n        half = N // 2\n        self.non_unique = self.idx[:half].append(self.idx[:half])\n        self.non_unique_sorted = (\n            self.sorted[:half].append(self.sorted[:half]).sort_values()\n        )\n        self.key = self.sorted[N // 4]""}",index_object.time_get,time,0.0
15237,func-level,"terminus-2,oracle",1.1377686919683618,1.1377686919683618,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_getitem_array,indexing,"{""indexing.NumericSeriesIndexing.time_getitem_array"": ""class NumericSeriesIndexing:\n    def time_getitem_array(self, index, index_structure):\n        self.data[self.array]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.time_getitem_array,time,0.0
15228,func-level,"terminus-2,oracle",1.0220004458551766,1.0220004458551766,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_cached_properties.time_is_monotonic_increasing,index_cached_properties,"{""index_cached_properties.IndexCache.time_is_monotonic_increasing"": ""class IndexCache:\n    def time_is_monotonic_increasing(self, index_type):\n        self.idx.is_monotonic_increasing\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}""}",index_cached_properties.time_is_monotonic_increasing,time,0.0
15234,func-level,"terminus-2,oracle",1.2617980705467426,1.2617980705467426,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_get_indexer,indexing,"{""indexing.NumericMaskedIndexing.time_get_indexer"": ""class NumericMaskedIndexing:\n    def time_get_indexer(self, dtype, monotonic):\n        self.data.get_indexer(self.indexer)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericMaskedIndexing:\n    def setup(self, dtype, monotonic):\n        indices = {\n            True: Index(self.monotonic_list, dtype=dtype),\n            False: Index(self.non_monotonic_list, dtype=dtype).append(\n                Index([NA], dtype=dtype)\n            ),\n        }\n        self.data = indices[monotonic]\n        self.indexer = np.arange(300, 1_000)\n        self.data_dups = self.data.append(self.data)""}",indexing.time_get_indexer,time,0.0
15233,func-level,"terminus-2,oracle",0.9839495425121074,0.9839495425121074,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_frame_getitem_single_column_int,indexing,"{""indexing.GetItemSingleColumn.time_frame_getitem_single_column_int"": ""class GetItemSingleColumn:\n    def time_frame_getitem_single_column_int(self):\n        self.df_int_col[0]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GetItemSingleColumn:\n    def setup(self):\n        self.df_string_col = DataFrame(np.random.randn(3000, 1), columns=[\""A\""])\n        self.df_int_col = DataFrame(np.random.randn(3000, 1))""}",indexing.time_frame_getitem_single_column_int,time,0.0
15230,func-level,"terminus-2,oracle",1.0263895534781546,1.0263895534781546,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_object.time_get_loc_sorted,index_object,"{""index_object.Indexing.time_get_loc_sorted"": ""class Indexing:\n    def time_get_loc_sorted(self, dtype):\n        self.sorted.get_loc(self.key)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self, dtype):\n        N = 10**6\n        self.idx = getattr(tm, f\""make{dtype}Index\"")(N)\n        self.array_mask = (np.arange(N) % 3) == 0\n        self.series_mask = Series(self.array_mask)\n        self.sorted = self.idx.sort_values()\n        half = N // 2\n        self.non_unique = self.idx[:half].append(self.idx[:half])\n        self.non_unique_sorted = (\n            self.sorted[:half].append(self.sorted[:half]).sort_values()\n        )\n        self.key = self.sorted[N // 4]""}",index_object.time_get_loc_sorted,time,0.0
15236,func-level,"terminus-2,oracle",1.0333263660754053,1.0333263660754053,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_get_indexer_mismatched_tz,indexing,"{""indexing.DatetimeIndexIndexing.time_get_indexer_mismatched_tz"": ""class DatetimeIndexIndexing:\n    def time_get_indexer_mismatched_tz(self):\n        # reached via e.g.\n        #  ser = Series(range(len(dti)), index=dti)\n        #  ser[dti2]\n        self.dti.get_indexer(self.dti2)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndexIndexing:\n    def setup(self):\n        dti = date_range(\""2016-01-01\"", periods=10000, tz=\""US/Pacific\"")\n        dti2 = dti.tz_convert(\""UTC\"")\n        self.dti = dti\n        self.dti2 = dti2""}",indexing.time_get_indexer_mismatched_tz,time,0.0
15235,func-level,"terminus-2,oracle",1.2018416031412138,1.2018416031412138,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_get_indexer_dups,indexing,"{""indexing.NumericMaskedIndexing.time_get_indexer_dups"": ""class NumericMaskedIndexing:\n    def time_get_indexer_dups(self, dtype, monotonic):\n        self.data.get_indexer_for(self.indexer)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericMaskedIndexing:\n    def setup(self, dtype, monotonic):\n        indices = {\n            True: Index(self.monotonic_list, dtype=dtype),\n            False: Index(self.non_monotonic_list, dtype=dtype).append(\n                Index([NA], dtype=dtype)\n            ),\n        }\n        self.data = indices[monotonic]\n        self.indexer = np.arange(300, 1_000)\n        self.data_dups = self.data.append(self.data)""}",indexing.time_get_indexer_dups,time,0.0
15241,func-level,"terminus-2,oracle",1.0304730152008894,1.0304730152008894,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_getitem_scalar,indexing,"{""indexing.CategoricalIndexIndexing.time_getitem_scalar"": ""class CategoricalIndexIndexing:\n    def time_getitem_scalar(self, index):\n        self.data[self.int_scalar]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalIndexIndexing:\n    def setup(self, index):\n        N = 10**5\n        values = list(\""a\"" * N + \""b\"" * N + \""c\"" * N)\n        indices = {\n            \""monotonic_incr\"": CategoricalIndex(values),\n            \""monotonic_decr\"": CategoricalIndex(reversed(values)),\n            \""non_monotonic\"": CategoricalIndex(list(\""abc\"" * N)),\n        }\n        self.data = indices[index]\n        self.data_unique = CategoricalIndex([str(i) for i in range(N * 3)])\n    \n        self.int_scalar = 10000\n        self.int_list = list(range(10000))\n    \n        self.cat_scalar = \""b\""\n        self.cat_list = [\""1\"", \""3\""]"", ""indexing.NonNumericSeriesIndexing.time_getitem_scalar"": ""class NonNumericSeriesIndexing:\n    def time_getitem_scalar(self, index, index_structure):\n        self.s[self.lbl]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]"", ""indexing.NumericSeriesIndexing.time_getitem_scalar"": ""class NumericSeriesIndexing:\n    def time_getitem_scalar(self, index, index_structure):\n        self.data[800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.time_getitem_scalar,time,0.0
15244,func-level,"terminus-2,oracle",1.061558771647169,1.061558771647169,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_loc_all_null_slices,indexing,"{""indexing.MultiIndexing.time_loc_all_null_slices"": ""class MultiIndexing:\n    def time_loc_all_null_slices(self, unique_levels):\n        target = tuple([self.tgt_null_slice] * self.nlevels)\n        self.df.loc[target, :]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MultiIndexing:\n    def setup(self, unique_levels):\n        self.nlevels = 2\n        if unique_levels:\n            mi = MultiIndex.from_arrays([range(1000000)] * self.nlevels)\n        else:\n            mi = MultiIndex.from_product([range(1000)] * self.nlevels)\n        self.df = DataFrame(np.random.randn(len(mi)), index=mi)\n    \n        self.tgt_slice = slice(200, 800)\n        self.tgt_null_slice = slice(None)\n        self.tgt_list = list(range(0, 1000, 10))\n        self.tgt_scalar = 500\n    \n        bool_indexer = np.zeros(len(mi), dtype=np.bool_)\n        bool_indexer[slice(0, len(mi), 100)] = True\n        self.tgt_bool_indexer = bool_indexer""}",indexing.time_loc_all_null_slices,time,0.0
15247,func-level,"terminus-2,oracle",1.038504163552764,1.038504163552764,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_loc_row,indexing,"{""indexing.IndexSingleRow.time_loc_row"": ""class IndexSingleRow:\n    def time_loc_row(self, unique_cols):\n        self.df.loc[10000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IndexSingleRow:\n    def setup(self, unique_cols):\n        arr = np.arange(10**7).reshape(-1, 10)\n        df = DataFrame(arr)\n        dtypes = [\""u1\"", \""u2\"", \""u4\"", \""u8\"", \""i1\"", \""i2\"", \""i4\"", \""i8\"", \""f8\"", \""f4\""]\n        for i, d in enumerate(dtypes):\n            df[i] = df[i].astype(d)\n    \n        if not unique_cols:\n            # GH#33032 single-row lookups with non-unique columns were\n            #  15x slower than with unique columns\n            df.columns = [\""A\"", \""A\""] + list(df.columns[2:])\n    \n        self.df = df""}",indexing.time_loc_row,time,0.0
15245,func-level,"terminus-2,oracle",1.052174150069658,1.052174150069658,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_loc_all_scalars,indexing,"{""indexing.MultiIndexing.time_loc_all_scalars"": ""class MultiIndexing:\n    def time_loc_all_scalars(self, unique_levels):\n        target = tuple([self.tgt_scalar] * self.nlevels)\n        self.df.loc[target, :]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MultiIndexing:\n    def setup(self, unique_levels):\n        self.nlevels = 2\n        if unique_levels:\n            mi = MultiIndex.from_arrays([range(1000000)] * self.nlevels)\n        else:\n            mi = MultiIndex.from_product([range(1000)] * self.nlevels)\n        self.df = DataFrame(np.random.randn(len(mi)), index=mi)\n    \n        self.tgt_slice = slice(200, 800)\n        self.tgt_null_slice = slice(None)\n        self.tgt_list = list(range(0, 1000, 10))\n        self.tgt_scalar = 500\n    \n        bool_indexer = np.zeros(len(mi), dtype=np.bool_)\n        bool_indexer[slice(0, len(mi), 100)] = True\n        self.tgt_bool_indexer = bool_indexer""}",indexing.time_loc_all_scalars,time,0.0
15246,func-level,"terminus-2,oracle",1.092596161613923,1.092596161613923,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_loc_array,indexing,"{""indexing.NumericSeriesIndexing.time_loc_array"": ""class NumericSeriesIndexing:\n    def time_loc_array(self, index, index_structure):\n        self.data.loc[self.array]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.time_loc_array,time,0.0
15254,func-level,"terminus-2,oracle",0.99878274439641,0.99878274439641,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing_engines.time_get_loc_near_middle,indexing_engines,"{""indexing_engines.MaskedNumericEngineIndexing.time_get_loc_near_middle"": ""class MaskedNumericEngineIndexing:\n    def time_get_loc_near_middle(self, engine_and_dtype, index_type, unique, N):\n        # searchsorted performance may be different near the middle of a range\n        #  vs near an endpoint\n        self.data.get_loc(self.key_middle)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())[::-1]\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.zeros(N * 3, dtype=dtype.lower())\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype.lower())\n                arr[N:] = np.arange(N * 2, dtype=dtype.lower())\n    \n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n            mask[-1] = True\n    \n        self.data = engine(BaseMaskedArray(arr, mask))\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]"", ""indexing_engines.NumericEngineIndexing.time_get_loc_near_middle"": ""class NumericEngineIndexing:\n    def time_get_loc_near_middle(self, engine_and_dtype, index_type, unique, N):\n        # searchsorted performance may be different near the middle of a range\n        #  vs near an endpoint\n        self.data.get_loc(self.key_middle)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)[::-1]\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.empty(N * 3, dtype=dtype)\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype)\n                arr[N:] = np.arange(N * 2, dtype=dtype)\n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype)\n    \n        self.data = engine(arr)\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]""}",indexing_engines.time_get_loc_near_middle,time,0.0
15243,func-level,"terminus-2,oracle",1.063036998406487,1.063036998406487,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_iloc_slice,indexing,"{""indexing.NumericSeriesIndexing.time_iloc_slice"": ""class NumericSeriesIndexing:\n    def time_iloc_slice(self, index, index_structure):\n        self.data.iloc[:800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.time_iloc_slice,time,0.0
15250,func-level,"terminus-2,oracle",0.9670654987224886,0.9670654987224886,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_lookup_iloc,indexing,"{""indexing.MethodLookup.time_lookup_iloc"": ""class MethodLookup:\n    def time_lookup_iloc(self, s):\n        s.iloc\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MethodLookup:\n    def setup_cache(self):\n        s = Series()\n        return s""}",indexing.time_lookup_iloc,time,0.0
15240,func-level,"terminus-2,oracle",1.208053664760974,1.208053664760974,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_getitem_lists,indexing,"{""indexing.NumericSeriesIndexing.time_getitem_lists"": ""class NumericSeriesIndexing:\n    def time_getitem_lists(self, index, index_structure):\n        self.data[self.array_list]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.time_getitem_lists,time,0.0
15242,func-level,"terminus-2,oracle",1.2623581032741231,1.2623581032741231,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_getitem_slice,indexing,"{""indexing.NumericSeriesIndexing.time_getitem_slice"": ""class NumericSeriesIndexing:\n    def time_getitem_slice(self, index, index_structure):\n        self.data[:800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.time_getitem_slice,time,0.0
15238,func-level,"terminus-2,oracle",1.0473046935586134,1.0473046935586134,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_getitem_label_slice,indexing,"{""indexing.NonNumericSeriesIndexing.time_getitem_label_slice"": ""class NonNumericSeriesIndexing:\n    def time_getitem_label_slice(self, index, index_structure):\n        self.s[: self.lbl]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]""}",indexing.time_getitem_label_slice,time,0.0
15251,func-level,"terminus-2,oracle",0.9826809808730528,0.9826809808730528,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_lookup_loc,indexing,"{""indexing.MethodLookup.time_lookup_loc"": ""class MethodLookup:\n    def time_lookup_loc(self, s):\n        s.loc\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MethodLookup:\n    def setup_cache(self):\n        s = Series()\n        return s""}",indexing.time_lookup_loc,time,0.0
15253,func-level,"terminus-2,oracle",1.0392181948025452,1.0392181948025452,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing_engines.time_get_loc,indexing_engines,"{""indexing_engines.MaskedNumericEngineIndexing.time_get_loc"": ""class MaskedNumericEngineIndexing:\n    def time_get_loc(self, engine_and_dtype, index_type, unique, N):\n        self.data.get_loc(self.key_early)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())[::-1]\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.zeros(N * 3, dtype=dtype.lower())\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype.lower())\n                arr[N:] = np.arange(N * 2, dtype=dtype.lower())\n    \n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n            mask[-1] = True\n    \n        self.data = engine(BaseMaskedArray(arr, mask))\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]"", ""indexing_engines.NumericEngineIndexing.time_get_loc"": ""class NumericEngineIndexing:\n    def time_get_loc(self, engine_and_dtype, index_type, unique, N):\n        self.data.get_loc(self.key_early)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)[::-1]\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.empty(N * 3, dtype=dtype)\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype)\n                arr[N:] = np.arange(N * 2, dtype=dtype)\n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype)\n    \n        self.data = engine(arr)\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]""}",indexing_engines.time_get_loc,time,0.0
15248,func-level,"terminus-2,oracle",1.1581488748562412,1.1581488748562412,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_loc_scalar,indexing,"{""indexing.IntervalIndexing.time_loc_scalar"": ""class IntervalIndexing:\n    def time_loc_scalar(self, monotonic):\n        monotonic.loc[80000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IntervalIndexing:\n    def setup_cache(self):\n        idx = IntervalIndex.from_breaks(np.arange(1000001))\n        monotonic = Series(np.arange(1000000), index=idx)\n        return monotonic"", ""indexing.NumericSeriesIndexing.time_loc_scalar"": ""class NumericSeriesIndexing:\n    def time_loc_scalar(self, index, index_structure):\n        self.data.loc[800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.time_loc_scalar,time,0.0
15255,func-level,"terminus-2,oracle",1.024725834815643,1.024725834815643,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,inference.time_downcast,inference,"{""inference.ToNumericDowncast.time_downcast"": ""class ToNumericDowncast:\n    def time_downcast(self, dtype, downcast):\n        to_numeric(self.data, downcast=downcast)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToNumericDowncast:\n    def setup(self, dtype, downcast):\n        self.data = self.data_dict[dtype]""}",inference.time_downcast,time,0.0
15261,func-level,"terminus-2,oracle",1.03767982340548,1.03767982340548,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.csv.time_read_stringcsv,io.csv,"{""io.csv.ReadCSVEngine.time_read_stringcsv"": ""class ReadCSVEngine:\n    def time_read_stringcsv(self, engine):\n        read_csv(self.data(self.StringIO_input), engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVEngine:\n    def setup(self, engine):\n        data = [\""A,B,C,D,E\""] + ([\""1,2,3,4,5\""] * 100000)\n        self.StringIO_input = StringIO(\""\\n\"".join(data))\n        # simulate reading from file\n        self.BytesIO_input = BytesIO(self.StringIO_input.read().encode(\""utf-8\""))""}",io.csv.time_read_stringcsv,time,0.0
15249,func-level,"terminus-2,oracle",1.0881409032083509,1.0881409032083509,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_loc_slice,indexing,"{""indexing.NumericSeriesIndexing.time_loc_slice"": ""class NumericSeriesIndexing:\n    def time_loc_slice(self, index, index_structure):\n        self.data.loc[:800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.time_loc_slice,time,0.0
15256,func-level,"terminus-2,oracle",0.9586934385698772,0.9586934385698772,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,inference.time_dup_string_dates_and_format,inference,"{""inference.ToDatetimeCache.time_dup_string_dates_and_format"": ""class ToDatetimeCache:\n    def time_dup_string_dates_and_format(self, cache):\n        to_datetime(self.dup_string_dates, format=\""%Y-%m-%d\"", cache=cache)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToDatetimeCache:\n    def setup(self, cache):\n        N = 10000\n        self.unique_numeric_seconds = list(range(N))\n        self.dup_numeric_seconds = [1000] * N\n        self.dup_string_dates = [\""2000-02-11\""] * N\n        self.dup_string_with_tz = [\""2000-02-11 15:00:00-0800\""] * N""}",inference.time_dup_string_dates_and_format,time,0.0
15260,func-level,"terminus-2,oracle",1.0503761610002444,1.0503761610002444,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.csv.time_read_bytescsv,io.csv,"{""io.csv.ReadCSVEngine.time_read_bytescsv"": ""class ReadCSVEngine:\n    def time_read_bytescsv(self, engine):\n        read_csv(self.data(self.BytesIO_input), engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVEngine:\n    def setup(self, engine):\n        data = [\""A,B,C,D,E\""] + ([\""1,2,3,4,5\""] * 100000)\n        self.StringIO_input = StringIO(\""\\n\"".join(data))\n        # simulate reading from file\n        self.BytesIO_input = BytesIO(self.StringIO_input.read().encode(\""utf-8\""))""}",io.csv.time_read_bytescsv,time,0.0
15252,func-level,"terminus-2,oracle",1.0429647275222034,1.0429647275222034,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.time_take,indexing,"{""indexing.Take.time_take"": ""class Take:\n    def time_take(self, index):\n        self.s.take(self.indexer)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Take:\n    def setup(self, index):\n        N = 100000\n        indexes = {\n            \""int\"": Index(np.arange(N), dtype=np.int64),\n            \""datetime\"": date_range(\""2011-01-01\"", freq=\""S\"", periods=N),\n        }\n        index = indexes[index]\n        self.s = Series(np.random.rand(N), index=index)\n        self.indexer = np.random.randint(0, N, size=N)""}",indexing.time_take,time,0.0
15258,func-level,"terminus-2,oracle",1.121376298596207,1.121376298596207,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.csv.time_convert_post,io.csv,"{""io.csv.ReadCSVCategorical.time_convert_post"": ""class ReadCSVCategorical:\n    def time_convert_post(self, engine):\n        read_csv(self.fname, engine=engine).apply(Categorical)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVCategorical:\n    def setup(self, engine):\n        N = 100000\n        group1 = [\""aaaaaaaa\"", \""bbbbbbb\"", \""cccccccc\"", \""dddddddd\"", \""eeeeeeee\""]\n        df = DataFrame(np.random.choice(group1, (N, 3)), columns=list(\""abc\""))\n        df.to_csv(self.fname, index=False)""}",io.csv.time_convert_post,time,0.0
15269,func-level,"terminus-2,oracle",0.9588987691396428,0.9588987691396428,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.parsers.time_check_datetimes,io.parsers,"{""io.parsers.DoesStringLookLikeDatetime.time_check_datetimes"": ""class DoesStringLookLikeDatetime:\n    def time_check_datetimes(self, value):\n        for obj in self.objects:\n            _does_string_look_like_datetime(obj)\n\n    def setup(self, value):\n        self.objects = [value] * 1000000""}",io.parsers.time_check_datetimes,time,0.0
15262,func-level,"terminus-2,oracle",1.027590940682821,1.027590940682821,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.excel.time_read_excel,io.excel,"{""io.excel.ReadExcel.time_read_excel"": ""class ReadExcel:\n    def time_read_excel(self, engine):\n        if engine == \""odf\"":\n            fname = self.fname_odf\n        else:\n            fname = self.fname_excel\n        read_excel(fname, engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadExcel:\n    def setup_cache(self):\n        self.df = _generate_dataframe()\n    \n        self.df.to_excel(self.fname_excel, sheet_name=\""Sheet1\"")\n        self._create_odf()""}",io.excel.time_read_excel,time,0.0
15267,func-level,"terminus-2,oracle",1.0644492683983184,1.0644492683983184,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.json.time_to_json_wide,io.json,"{""io.json.ToJSONWide.time_to_json_wide"": ""class ToJSONWide:\n    def time_to_json_wide(self, orient, frame):\n        self.df_wide.to_json(self.fname, orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSONWide:\n    def setup(self, orient, frame):\n        super().setup(orient, frame)\n        base_df = getattr(self, frame).copy()\n        df_wide = concat([base_df.iloc[:100]] * 1000, ignore_index=True, axis=1)\n        self.df_wide = df_wide""}",io.json.time_to_json_wide,time,0.0
15265,func-level,"terminus-2,oracle",1.0502907382323543,1.0502907382323543,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.json.time_normalize_json,io.json,"{""io.json.NormalizeJSON.time_normalize_json"": ""class NormalizeJSON:\n    def time_normalize_json(self, orient, frame):\n        json_normalize(self.data)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NormalizeJSON:\n    def setup(self, orient, frame):\n        data = {\n            \""hello\"": [\""thisisatest\"", 999898, \""mixed types\""],\n            \""nest1\"": {\""nest2\"": {\""nest3\"": \""nest3_value\"", \""nest3_int\"": 3445}},\n            \""nest1_list\"": {\""nest2\"": [\""blah\"", 32423, 546456.876, 92030234]},\n            \""hello2\"": \""string\"",\n        }\n        self.data = [data for i in range(10000)]""}",io.json.time_normalize_json,time,0.0
15273,func-level,"terminus-2,oracle",1.0272913623753237,1.0272913623753237,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,join_merge.time_concat_empty_left,join_merge,"{""join_merge.Concat.time_concat_empty_left"": ""class Concat:\n    def time_concat_empty_left(self, axis):\n        concat(self.empty_left, axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Concat:\n    def setup(self, axis):\n        N = 1000\n        s = Series(N, index=tm.makeStringIndex(N))\n        self.series = [s[i:-i] for i in range(1, 10)] * 50\n        self.small_frames = [DataFrame(np.random.randn(5, 4))] * 1000\n        df = DataFrame(\n            {\""A\"": range(N)}, index=date_range(\""20130101\"", periods=N, freq=\""s\"")\n        )\n        self.empty_left = [DataFrame(), df]\n        self.empty_right = [df, DataFrame()]\n        self.mixed_ndims = [df, df.head(N // 2)]""}",join_merge.time_concat_empty_left,time,0.0
15259,func-level,"terminus-2,oracle",1.0231803528110297,1.0231803528110297,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.csv.time_frame_date_formatting_index,io.csv,"{""io.csv.ToCSVDatetimeIndex.time_frame_date_formatting_index"": ""class ToCSVDatetimeIndex:\n    def time_frame_date_formatting_index(self):\n        self.data.to_csv(self.fname, date_format=\""%Y-%m-%d %H:%M:%S\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToCSVDatetimeIndex:\n    def setup(self):\n        rng = date_range(\""2000\"", periods=100_000, freq=\""S\"")\n        self.data = DataFrame({\""a\"": 1}, index=rng)""}",io.csv.time_frame_date_formatting_index,time,0.0
15274,func-level,"terminus-2,oracle",0.9154205285523832,0.9154205285523832,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,join_merge.time_concat_series,join_merge,"{""join_merge.ConcatIndexDtype.time_concat_series"": ""class ConcatIndexDtype:\n    def time_concat_series(self, dtype, structure, axis, sort):\n        concat(self.series, axis=axis, sort=sort)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ConcatIndexDtype:\n    def setup(self, dtype, structure, axis, sort):\n        N = 10_000\n        if dtype == \""datetime64[ns]\"":\n            vals = date_range(\""1970-01-01\"", periods=N)\n        elif dtype in (\""int64\"", \""Int64\""):\n            vals = np.arange(N, dtype=np.int64)\n        elif dtype in (\""string[python]\"", \""string[pyarrow]\""):\n            vals = tm.makeStringIndex(N)\n        else:\n            raise NotImplementedError\n    \n        idx = Index(vals, dtype=dtype)\n    \n        if structure == \""monotonic\"":\n            idx = idx.sort_values()\n        elif structure == \""non_monotonic\"":\n            idx = idx[::-1]\n        elif structure == \""has_na\"":\n            if not idx._can_hold_na:\n                raise NotImplementedError\n            idx = Index([None], dtype=dtype).append(idx)\n        else:\n            raise NotImplementedError\n    \n        self.series = [Series(i, idx[:-i]) for i in range(1, 6)]""}",join_merge.time_concat_series,time,0.0
15257,func-level,"terminus-2,oracle",1.061552509171506,1.061552509171506,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.csv.time_convert_direct,io.csv,"{""io.csv.ReadCSVCategorical.time_convert_direct"": ""class ReadCSVCategorical:\n    def time_convert_direct(self, engine):\n        read_csv(self.fname, engine=engine, dtype=\""category\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVCategorical:\n    def setup(self, engine):\n        N = 100000\n        group1 = [\""aaaaaaaa\"", \""bbbbbbb\"", \""cccccccc\"", \""dddddddd\"", \""eeeeeeee\""]\n        df = DataFrame(np.random.choice(group1, (N, 3)), columns=list(\""abc\""))\n        df.to_csv(self.fname, index=False)""}",io.csv.time_convert_direct,time,0.0
15270,func-level,"terminus-2,oracle",1.0472118667839048,1.0472118667839048,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.sql.time_read_sql_query_select_column,io.sql,"{""io.sql.WriteSQLDtypes.time_read_sql_query_select_column"": ""class WriteSQLDtypes:\n    def time_read_sql_query_select_column(self, connection, dtype):\n        read_sql_query(self.query_col, self.con)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass WriteSQLDtypes:\n    def setup(self, connection, dtype):\n        N = 10000\n        con = {\n            \""sqlalchemy\"": create_engine(\""sqlite:///:memory:\""),\n            \""sqlite\"": sqlite3.connect(\"":memory:\""),\n        }\n        self.table_name = \""test_type\""\n        self.query_col = f\""SELECT {dtype} FROM {self.table_name}\""\n        self.con = con[connection]\n        self.df = DataFrame(\n            {\n                \""float\"": np.random.randn(N),\n                \""float_with_nan\"": np.random.randn(N),\n                \""string\"": [\""foo\""] * N,\n                \""bool\"": [True] * N,\n                \""int\"": np.random.randint(0, N, size=N),\n                \""datetime\"": date_range(\""2000-01-01\"", periods=N, freq=\""s\""),\n            },\n            index=tm.makeStringIndex(N),\n        )\n        self.df.iloc[1000:3000, 1] = np.nan\n        self.df[\""date\""] = self.df[\""datetime\""].dt.date\n        self.df[\""time\""] = self.df[\""datetime\""].dt.time\n        self.df[\""datetime_string\""] = self.df[\""datetime\""].astype(str)\n        self.df.to_sql(self.table_name, self.con, if_exists=\""replace\"")""}",io.sql.time_read_sql_query_select_column,time,0.0
15268,func-level,"terminus-2,oracle",1.0156846993935509,1.0156846993935509,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.parsers.time_check_concat,io.parsers,"{""io.parsers.ConcatDateCols.time_check_concat"": ""class ConcatDateCols:\n    def time_check_concat(self, value, dim):\n        concat_date_cols(self.object)\n\n    def setup(self, value, dim):\n        count_elem = 10000\n        if dim == 1:\n            self.object = (np.array([value] * count_elem),)\n        if dim == 2:\n            self.object = (\n                np.array([value] * count_elem),\n                np.array([value] * count_elem),\n            )""}",io.parsers.time_check_concat,time,0.0
15275,func-level,"terminus-2,oracle",1.0651493160553005,1.0651493160553005,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,join_merge.time_on_int32,join_merge,"{""join_merge.MergeAsof.time_on_int32"": ""class MergeAsof:\n    def time_on_int32(self, direction, tolerance):\n        merge_asof(\n            self.df1d, self.df2d, on=\""time32\"", direction=direction, tolerance=tolerance\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MergeAsof:\n    def setup(self, direction, tolerance):\n        one_count = 200000\n        two_count = 1000000\n    \n        df1 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, one_count / 20, one_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), one_count),\n                \""key2\"": np.random.randint(0, 25, one_count),\n                \""value1\"": np.random.randn(one_count),\n            }\n        )\n        df2 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, two_count / 20, two_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), two_count),\n                \""key2\"": np.random.randint(0, 25, two_count),\n                \""value2\"": np.random.randn(two_count),\n            }\n        )\n    \n        df1 = df1.sort_values(\""time\"")\n        df2 = df2.sort_values(\""time\"")\n    \n        df1[\""time32\""] = np.int32(df1.time)\n        df2[\""time32\""] = np.int32(df2.time)\n    \n        df1[\""timeu64\""] = np.uint64(df1.time)\n        df2[\""timeu64\""] = np.uint64(df2.time)\n    \n        self.df1a = df1[[\""time\"", \""value1\""]]\n        self.df2a = df2[[\""time\"", \""value2\""]]\n        self.df1b = df1[[\""time\"", \""key\"", \""value1\""]]\n        self.df2b = df2[[\""time\"", \""key\"", \""value2\""]]\n        self.df1c = df1[[\""time\"", \""key2\"", \""value1\""]]\n        self.df2c = df2[[\""time\"", \""key2\"", \""value2\""]]\n        self.df1d = df1[[\""time32\"", \""value1\""]]\n        self.df2d = df2[[\""time32\"", \""value2\""]]\n        self.df1e = df1[[\""time\"", \""key\"", \""key2\"", \""value1\""]]\n        self.df2e = df2[[\""time\"", \""key\"", \""key2\"", \""value2\""]]\n        self.df1f = df1[[\""timeu64\"", \""value1\""]]\n        self.df2f = df2[[\""timeu64\"", \""value2\""]]""}",join_merge.time_on_int32,time,0.0
15263,func-level,"terminus-2,oracle",1.1630325669190456,1.1630325669190456,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.hdf.time_query_store_table,io.hdf,"{""io.hdf.HDFStoreDataFrame.time_query_store_table"": ""class HDFStoreDataFrame:\n    def time_query_store_table(self):\n        self.store.select(\""table\"", where=\""index > self.start and index < self.stop\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass HDFStoreDataFrame:\n    def setup(self):\n        N = 25000\n        index = tm.makeStringIndex(N)\n        self.df = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)}, index=index\n        )\n        self.df_mixed = DataFrame(\n            {\n                \""float1\"": np.random.randn(N),\n                \""float2\"": np.random.randn(N),\n                \""string1\"": [\""foo\""] * N,\n                \""bool1\"": [True] * N,\n                \""int1\"": np.random.randint(0, N, size=N),\n            },\n            index=index,\n        )\n        self.df_wide = DataFrame(np.random.randn(N, 100))\n        self.start_wide = self.df_wide.index[10000]\n        self.stop_wide = self.df_wide.index[15000]\n        self.df2 = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)},\n            index=date_range(\""1/1/2000\"", periods=N),\n        )\n        self.start = self.df2.index[10000]\n        self.stop = self.df2.index[15000]\n        self.df_wide2 = DataFrame(\n            np.random.randn(N, 100), index=date_range(\""1/1/2000\"", periods=N)\n        )\n        self.df_dc = DataFrame(\n            np.random.randn(N, 10), columns=[f\""C{i:03d}\"" for i in range(10)]\n        )\n    \n        self.fname = \""__test__.h5\""\n    \n        self.store = HDFStore(self.fname)\n        self.store.put(\""fixed\"", self.df)\n        self.store.put(\""fixed_mixed\"", self.df_mixed)\n        self.store.append(\""table\"", self.df2)\n        self.store.append(\""table_mixed\"", self.df_mixed)\n        self.store.append(\""table_wide\"", self.df_wide)\n        self.store.append(\""table_wide2\"", self.df_wide2)""}",io.hdf.time_query_store_table,time,0.0
15266,func-level,"terminus-2,oracle",1.023896312079389,1.023896312079389,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.json.time_to_json,io.json,"{""io.json.ToJSON.time_to_json"": ""class ToJSON:\n    def time_to_json(self, orient, frame):\n        getattr(self, frame).to_json(self.fname, orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSON:\n    def setup(self, orient, frame):\n        N = 10**5\n        ncols = 5\n        index = date_range(\""20000101\"", periods=N, freq=\""H\"")\n        timedeltas = timedelta_range(start=1, periods=N, freq=\""s\"")\n        datetimes = date_range(start=1, periods=N, freq=\""s\"")\n        ints = np.random.randint(100000000, size=N)\n        longints = sys.maxsize * np.random.randint(100000000, size=N)\n        floats = np.random.randn(N)\n        strings = tm.makeStringIndex(N)\n        self.df = DataFrame(np.random.randn(N, ncols), index=np.arange(N))\n        self.df_date_idx = DataFrame(np.random.randn(N, ncols), index=index)\n        self.df_td_int_ts = DataFrame(\n            {\n                \""td_1\"": timedeltas,\n                \""td_2\"": timedeltas,\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""ts_1\"": datetimes,\n                \""ts_2\"": datetimes,\n            },\n            index=index,\n        )\n        self.df_int_floats = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""int_3\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""float_3\"": floats,\n            },\n            index=index,\n        )\n        self.df_int_float_str = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )\n    \n        self.df_longint_float_str = DataFrame(\n            {\n                \""longint_1\"": longints,\n                \""longint_2\"": longints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )"", ""io.json.ToJSONWide.time_to_json"": ""class ToJSON:\n    def time_to_json(self, orient, frame):\n        getattr(self, frame).to_json(self.fname, orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSONWide:\n    def setup(self, orient, frame):\n        super().setup(orient, frame)\n        base_df = getattr(self, frame).copy()\n        df_wide = concat([base_df.iloc[:100]] * 1000, ignore_index=True, axis=1)\n        self.df_wide = df_wide""}",io.json.time_to_json,time,0.0
15271,func-level,"terminus-2,oracle",0.97232287374828,0.97232287374828,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.style.time_format_render,io.style,"{""io.style.Render.time_format_render"": ""class Render:\n    def time_format_render(self, cols, rows):\n        self._style_format()\n        self.st._render_html(True, True)\n\n    def setup(self, cols, rows):\n        self.df = DataFrame(\n            np.random.randn(rows, cols),\n            columns=[f\""float_{i+1}\"" for i in range(cols)],\n            index=[f\""row_{i+1}\"" for i in range(rows)],\n        )""}",io.style.time_format_render,time,0.0
15276,func-level,"terminus-2,oracle",0.6705412141744088,0.6705412141744088,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,join_merge.time_series_align_left_monotonic,join_merge,"{""join_merge.Align.time_series_align_left_monotonic"": ""class Align:\n    def time_series_align_left_monotonic(self):\n        self.ts1.align(self.ts2, join=\""left\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Align:\n    def setup(self):\n        size = 5 * 10**5\n        rng = np.arange(0, 10**13, 10**7)\n        stamps = np.datetime64(\""now\"").view(\""i8\"") + rng\n        idx1 = np.sort(np.random.choice(stamps, size, replace=False))\n        idx2 = np.sort(np.random.choice(stamps, size, replace=False))\n        self.ts1 = Series(np.random.randn(size), idx1)\n        self.ts2 = Series(np.random.randn(size), idx2)""}",join_merge.time_series_align_left_monotonic,time,0.0
15264,func-level,"terminus-2,oracle",1.0151594714925496,1.0151594714925496,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.json.time_delta_int_tstamp_lines,io.json,"{""io.json.ToJSONLines.time_delta_int_tstamp_lines"": ""class ToJSONLines:\n    def time_delta_int_tstamp_lines(self):\n        self.df_td_int_ts.to_json(self.fname, orient=\""records\"", lines=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSONLines:\n    def setup(self):\n        N = 10**5\n        ncols = 5\n        index = date_range(\""20000101\"", periods=N, freq=\""H\"")\n        timedeltas = timedelta_range(start=1, periods=N, freq=\""s\"")\n        datetimes = date_range(start=1, periods=N, freq=\""s\"")\n        ints = np.random.randint(100000000, size=N)\n        longints = sys.maxsize * np.random.randint(100000000, size=N)\n        floats = np.random.randn(N)\n        strings = tm.makeStringIndex(N)\n        self.df = DataFrame(np.random.randn(N, ncols), index=np.arange(N))\n        self.df_date_idx = DataFrame(np.random.randn(N, ncols), index=index)\n        self.df_td_int_ts = DataFrame(\n            {\n                \""td_1\"": timedeltas,\n                \""td_2\"": timedeltas,\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""ts_1\"": datetimes,\n                \""ts_2\"": datetimes,\n            },\n            index=index,\n        )\n        self.df_int_floats = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""int_3\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""float_3\"": floats,\n            },\n            index=index,\n        )\n        self.df_int_float_str = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )\n        self.df_longint_float_str = DataFrame(\n            {\n                \""longint_1\"": longints,\n                \""longint_2\"": longints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )""}",io.json.time_delta_int_tstamp_lines,time,0.0
15272,func-level,"terminus-2,oracle",1.026775439075071,1.026775439075071,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,join_merge.time_by_object,join_merge,"{""join_merge.MergeAsof.time_by_object"": ""class MergeAsof:\n    def time_by_object(self, direction, tolerance):\n        merge_asof(\n            self.df1b,\n            self.df2b,\n            on=\""time\"",\n            by=\""key\"",\n            direction=direction,\n            tolerance=tolerance,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MergeAsof:\n    def setup(self, direction, tolerance):\n        one_count = 200000\n        two_count = 1000000\n    \n        df1 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, one_count / 20, one_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), one_count),\n                \""key2\"": np.random.randint(0, 25, one_count),\n                \""value1\"": np.random.randn(one_count),\n            }\n        )\n        df2 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, two_count / 20, two_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), two_count),\n                \""key2\"": np.random.randint(0, 25, two_count),\n                \""value2\"": np.random.randn(two_count),\n            }\n        )\n    \n        df1 = df1.sort_values(\""time\"")\n        df2 = df2.sort_values(\""time\"")\n    \n        df1[\""time32\""] = np.int32(df1.time)\n        df2[\""time32\""] = np.int32(df2.time)\n    \n        df1[\""timeu64\""] = np.uint64(df1.time)\n        df2[\""timeu64\""] = np.uint64(df2.time)\n    \n        self.df1a = df1[[\""time\"", \""value1\""]]\n        self.df2a = df2[[\""time\"", \""value2\""]]\n        self.df1b = df1[[\""time\"", \""key\"", \""value1\""]]\n        self.df2b = df2[[\""time\"", \""key\"", \""value2\""]]\n        self.df1c = df1[[\""time\"", \""key2\"", \""value1\""]]\n        self.df2c = df2[[\""time\"", \""key2\"", \""value2\""]]\n        self.df1d = df1[[\""time32\"", \""value1\""]]\n        self.df2d = df2[[\""time32\"", \""value2\""]]\n        self.df1e = df1[[\""time\"", \""key\"", \""key2\"", \""value1\""]]\n        self.df2e = df2[[\""time\"", \""key\"", \""key2\"", \""value2\""]]\n        self.df1f = df1[[\""timeu64\"", \""value1\""]]\n        self.df2f = df2[[\""timeu64\"", \""value2\""]]""}",join_merge.time_by_object,time,0.0
15279,func-level,"terminus-2,oracle",1.021994003062137,1.021994003062137,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,libs.time_is_scalar,libs,"{""libs.ScalarListLike.time_is_scalar"": ""class ScalarListLike:\n    def time_is_scalar(self, param):\n        is_scalar(param)""}",libs.time_is_scalar,time,0.0
15284,func-level,"terminus-2,oracle",1.0301011183673392,1.0301011183673392,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,period.time_align,period,"{""period.Indexing.time_align"": ""class Indexing:\n    def time_align(self):\n        DataFrame({\""a\"": self.series, \""b\"": self.series[:500]})\n\n    def setup(self):\n        self.index = period_range(start=\""1985\"", periods=1000, freq=\""D\"")\n        self.series = Series(range(1000), index=self.index)\n        self.period = self.index[500]""}",period.time_align,time,0.0
15282,func-level,"terminus-2,oracle",1.0539896610532509,1.0539896610532509,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,multiindex_object.time_med_get_loc,multiindex_object,"{""multiindex_object.GetLoc.time_med_get_loc"": ""class GetLoc:\n    def time_med_get_loc(self):\n        self.mi_med.get_loc((999, 9, \""A\""))\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GetLoc:\n    def setup(self):\n        self.mi_large = MultiIndex.from_product(\n            [np.arange(1000), np.arange(20), list(string.ascii_letters)],\n            names=[\""one\"", \""two\"", \""three\""],\n        )\n        self.mi_med = MultiIndex.from_product(\n            [np.arange(1000), np.arange(10), list(\""A\"")], names=[\""one\"", \""two\"", \""three\""]\n        )\n        self.mi_small = MultiIndex.from_product(\n            [np.arange(100), list(\""A\""), list(\""A\"")], names=[\""one\"", \""two\"", \""three\""]\n        )""}",multiindex_object.time_med_get_loc,time,0.0
15286,func-level,"terminus-2,oracle",1.0468463707505495,1.0468463707505495,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,period.time_intersection,period,"{""period.Indexing.time_intersection"": ""class Indexing:\n    def time_intersection(self):\n        self.index[:750].intersection(self.index[250:])\n\n    def setup(self):\n        self.index = period_range(start=\""1985\"", periods=1000, freq=\""D\"")\n        self.series = Series(range(1000), index=self.index)\n        self.period = self.index[500]""}",period.time_intersection,time,0.0
15290,func-level,"terminus-2,oracle",0.9481537929203344,0.9481537929203344,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,replace.time_replace_list,replace,"{""replace.ReplaceList.time_replace_list"": ""class ReplaceList:\n    def time_replace_list(self, inplace):\n        self.df.replace([np.inf, -np.inf], np.nan, inplace=inplace)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReplaceList:\n    def setup(self, inplace):\n        self.df = pd.DataFrame({\""A\"": 0, \""B\"": 0}, index=range(4 * 10**7))""}",replace.time_replace_list,time,0.0
15292,func-level,"terminus-2,oracle",0.9799149945027332,0.9799149945027332,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.time_cut_interval,reshape,"{""reshape.Cut.time_cut_interval"": ""class Cut:\n    def time_cut_interval(self, bins):\n        # GH 27668\n        pd.cut(self.int_series, self.interval_bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))""}",reshape.time_cut_interval,time,0.0
15280,func-level,"terminus-2,oracle",0.9662950819021658,0.9662950819021658,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,multiindex_object.time_is_monotonic,multiindex_object,"{""multiindex_object.Integer.time_is_monotonic"": ""class Integer:\n    def time_is_monotonic(self):\n        self.mi_int.is_monotonic_increasing\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Integer:\n    def setup(self):\n        self.mi_int = MultiIndex.from_product(\n            [np.arange(1000), np.arange(1000)], names=[\""one\"", \""two\""]\n        )\n        self.obj_index = np.array(\n            [\n                (0, 10),\n                (0, 11),\n                (0, 12),\n                (0, 13),\n                (0, 14),\n                (0, 15),\n                (0, 16),\n                (0, 17),\n                (0, 18),\n                (0, 19),\n            ],\n            dtype=object,\n        )\n        self.other_mi_many_mismatches = MultiIndex.from_tuples(\n            [\n                (-7, 41),\n                (-2, 3),\n                (-0.7, 5),\n                (0, 0),\n                (0, 1.5),\n                (0, 340),\n                (0, 1001),\n                (1, -4),\n                (1, 20),\n                (1, 1040),\n                (432, -5),\n                (432, 17),\n                (439, 165.5),\n                (998, -4),\n                (998, 24065),\n                (999, 865.2),\n                (999, 1000),\n                (1045, -843),\n            ]\n        )""}",multiindex_object.time_is_monotonic,time,0.0
15281,func-level,"terminus-2,oracle",1.5677687835340337,1.5677687835340337,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,multiindex_object.time_large_get_loc_warm,multiindex_object,"{""multiindex_object.GetLoc.time_large_get_loc_warm"": ""class GetLoc:\n    def time_large_get_loc_warm(self):\n        for _ in range(1000):\n            self.mi_large.get_loc((999, 19, \""Z\""))\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GetLoc:\n    def setup(self):\n        self.mi_large = MultiIndex.from_product(\n            [np.arange(1000), np.arange(20), list(string.ascii_letters)],\n            names=[\""one\"", \""two\"", \""three\""],\n        )\n        self.mi_med = MultiIndex.from_product(\n            [np.arange(1000), np.arange(10), list(\""A\"")], names=[\""one\"", \""two\"", \""three\""]\n        )\n        self.mi_small = MultiIndex.from_product(\n            [np.arange(100), list(\""A\""), list(\""A\"")], names=[\""one\"", \""two\"", \""three\""]\n        )""}",multiindex_object.time_large_get_loc_warm,time,0.0
15277,func-level,"terminus-2,oracle",1.0361836437598764,1.0361836437598764,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,libs.time_infer_dtype,libs,"{""libs.InferDtype.time_infer_dtype"": ""class InferDtype:\n    def time_infer_dtype(self, dtype):\n        infer_dtype(self.data_dict[dtype], skipna=False)""}",libs.time_infer_dtype,time,0.0
15278,func-level,"terminus-2,oracle",1.02516782783796,1.02516782783796,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,libs.time_is_list_like,libs,"{""libs.ScalarListLike.time_is_list_like"": ""class ScalarListLike:\n    def time_is_list_like(self, param):\n        is_list_like(param)""}",libs.time_is_list_like,time,0.0
15283,func-level,"terminus-2,oracle",0.9111576927899844,0.9111576927899844,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,multiindex_object.time_operation,multiindex_object,"{""multiindex_object.SetOperations.time_operation"": ""class SetOperations:\n    def time_operation(self, index_structure, dtype, method, sort):\n        getattr(self.left, method)(self.right, sort=sort)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SetOperations:\n    def setup(self, index_structure, dtype, method, sort):\n        N = 10**5\n        level1 = range(1000)\n    \n        level2 = date_range(start=\""1/1/2000\"", periods=N // 1000)\n        dates_left = MultiIndex.from_product([level1, level2])\n    \n        level2 = range(N // 1000)\n        int_left = MultiIndex.from_product([level1, level2])\n    \n        level2 = tm.makeStringIndex(N // 1000).values\n        str_left = MultiIndex.from_product([level1, level2])\n    \n        level2 = range(N // 1000)\n        ea_int_left = MultiIndex.from_product([level1, Series(level2, dtype=\""Int64\"")])\n    \n        data = {\n            \""datetime\"": dates_left,\n            \""int\"": int_left,\n            \""string\"": str_left,\n            \""ea_int\"": ea_int_left,\n        }\n    \n        if index_structure == \""non_monotonic\"":\n            data = {k: mi[::-1] for k, mi in data.items()}\n    \n        data = {k: {\""left\"": mi, \""right\"": mi[:-1]} for k, mi in data.items()}\n        self.left = data[dtype][\""left\""]\n        self.right = data[dtype][\""right\""]""}",multiindex_object.time_operation,time,0.0
15294,func-level,"terminus-2,oracle",1.0285263308926698,1.0285263308926698,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.time_explode,reshape,"{""reshape.Explode.time_explode"": ""class Explode:\n    def time_explode(self, n_rows, max_list_length):\n        self.series.explode()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Explode:\n    def setup(self, n_rows, max_list_length):\n        data = [np.arange(np.random.randint(max_list_length)) for _ in range(n_rows)]\n        self.series = pd.Series(data)""}",reshape.time_explode,time,0.0
15291,func-level,"terminus-2,oracle",1.0900392923901523,1.0900392923901523,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.time_cut_datetime,reshape,"{""reshape.Cut.time_cut_datetime"": ""class Cut:\n    def time_cut_datetime(self, bins):\n        pd.cut(self.datetime_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))""}",reshape.time_cut_datetime,time,0.0
15289,func-level,"terminus-2,oracle",0.9804572743060842,0.9804572743060842,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reindex.time_reindexed,reindex,"{""reindex.Fillna.time_reindexed"": ""class Fillna:\n    def time_reindexed(self, method):\n        self.ts_reindexed.fillna(method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Fillna:\n    def setup(self, method):\n        N = 100000\n        self.idx = date_range(\""1/1/2000\"", periods=N, freq=\""1min\"")\n        ts = Series(np.random.randn(N), index=self.idx)[::2]\n        self.ts_reindexed = ts.reindex(self.idx)\n        self.ts_float32 = self.ts_reindexed.astype(\""float32\"")""}",reindex.time_reindexed,time,0.0
15287,func-level,"terminus-2,oracle",1.0326909144462386,1.0326909144462386,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,period.time_series_loc,period,"{""period.Indexing.time_series_loc"": ""class Indexing:\n    def time_series_loc(self):\n        self.series.loc[self.period]\n\n    def setup(self):\n        self.index = period_range(start=\""1985\"", periods=1000, freq=\""D\"")\n        self.series = Series(range(1000), index=self.index)\n        self.period = self.index[500]""}",period.time_series_loc,time,0.0
15285,func-level,"terminus-2,oracle",1.0396601642827117,1.0396601642827117,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,period.time_from_date_range,period,"{""period.PeriodIndexConstructor.time_from_date_range"": ""class PeriodIndexConstructor:\n    def time_from_date_range(self, freq, is_offset):\n        PeriodIndex(self.rng, freq=freq)\n\n    def setup(self, freq, is_offset):\n        self.rng = date_range(\""1985\"", periods=1000)\n        self.rng2 = date_range(\""1985\"", periods=1000).to_pydatetime()\n        self.ints = list(range(2000, 3000))\n        self.daily_ints = (\n            date_range(\""1/1/2000\"", periods=1000, freq=freq).strftime(\""%Y%m%d\"").map(int)\n        )\n        if is_offset:\n            self.freq = to_offset(freq)\n        else:\n            self.freq = freq""}",period.time_from_date_range,time,0.0
15297,func-level,"terminus-2,oracle",1.2283349290061478,1.2283349290061478,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.time_pivot_table_categorical_observed,reshape,"{""reshape.PivotTable.time_pivot_table_categorical_observed"": ""class PivotTable:\n    def time_pivot_table_categorical_observed(self):\n        self.df2.pivot_table(\n            index=\""col1\"",\n            values=\""col3\"",\n            columns=\""col2\"",\n            aggfunc=np.sum,\n            fill_value=0,\n            observed=True,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")""}",reshape.time_pivot_table_categorical_observed,time,0.0
15288,func-level,"terminus-2,oracle",0.97254858342969,0.97254858342969,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,plotting.time_series_plot,plotting,"{""plotting.SeriesPlotting.time_series_plot"": ""class SeriesPlotting:\n    def time_series_plot(self, kind):\n        self.s.plot(kind=kind)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesPlotting:\n    def setup(self, kind):\n        if kind in [\""bar\"", \""barh\"", \""pie\""]:\n            n = 100\n        elif kind in [\""kde\""]:\n            n = 10000\n        else:\n            n = 1000000\n    \n        self.s = Series(np.random.randn(n))\n        if kind in [\""area\"", \""pie\""]:\n            self.s = self.s.abs()""}",plotting.time_series_plot,time,0.0
15298,func-level,"terminus-2,oracle",1.085041769248503,1.085041769248503,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.time_pivot_table_margins,reshape,"{""reshape.PivotTable.time_pivot_table_margins"": ""class PivotTable:\n    def time_pivot_table_margins(self):\n        self.df.pivot_table(index=\""key1\"", columns=[\""key2\"", \""key3\""], margins=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")""}",reshape.time_pivot_table_margins,time,0.0
15301,func-level,"terminus-2,oracle",1.0351283795476007,1.0351283795476007,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.time_qcut_float,reshape,"{""reshape.Cut.time_qcut_float"": ""class Cut:\n    def time_qcut_float(self, bins):\n        pd.qcut(self.float_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))""}",reshape.time_qcut_float,time,0.0
15296,func-level,"terminus-2,oracle",1.1391285843457797,1.1391285843457797,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.time_pivot_table_categorical,reshape,"{""reshape.PivotTable.time_pivot_table_categorical"": ""class PivotTable:\n    def time_pivot_table_categorical(self):\n        self.df2.pivot_table(\n            index=\""col1\"", values=\""col3\"", columns=\""col2\"", aggfunc=np.sum, fill_value=0\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")""}",reshape.time_pivot_table_categorical,time,0.0
15305,func-level,"terminus-2,oracle",1.0570098215679355,1.0570098215679355,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.time_ewm,rolling,"{""rolling.EWMMethods.time_ewm"": ""class EWMMethods:\n    def time_ewm(self, constructor, kwargs_method, dtype):\n        getattr(self.ewm, self.method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass EWMMethods:\n    def setup(self, constructor, kwargs_method, dtype):\n        N = 10**5\n        kwargs, method = kwargs_method\n        arr = (100 * np.random.random(N)).astype(dtype)\n        self.method = method\n        self.ewm = getattr(pd, constructor)(arr).ewm(**kwargs)""}",rolling.time_ewm,time,0.0
15299,func-level,"terminus-2,oracle",1.0645771443999537,1.0645771443999537,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.time_pivot_table_margins_only_column,reshape,"{""reshape.PivotTable.time_pivot_table_margins_only_column"": ""class PivotTable:\n    def time_pivot_table_margins_only_column(self):\n        self.df.pivot_table(columns=[\""key1\"", \""key2\"", \""key3\""], margins=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")""}",reshape.time_pivot_table_margins_only_column,time,0.0
15303,func-level,"terminus-2,oracle",1.0345564193262995,1.0345564193262995,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.time_wide_to_long_big,reshape,"{""reshape.WideToLong.time_wide_to_long_big"": ""class WideToLong:\n    def time_wide_to_long_big(self):\n        wide_to_long(self.df, self.letters, i=\""id\"", j=\""year\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass WideToLong:\n    def setup(self):\n        nyrs = 20\n        nidvars = 20\n        N = 5000\n        self.letters = list(\""ABCD\"")\n        yrvars = [\n            letter + str(num)\n            for letter, num in product(self.letters, range(1, nyrs + 1))\n        ]\n        columns = [str(i) for i in range(nidvars)] + yrvars\n        self.df = DataFrame(np.random.randn(N, nidvars + len(yrvars)), columns=columns)\n        self.df[\""id\""] = self.df.index""}",reshape.time_wide_to_long_big,time,0.0
15311,func-level,"terminus-2,oracle",1.0647197439195957,1.0647197439195957,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.time_quantile,rolling,"{""rolling.Quantile.time_quantile"": ""class Quantile:\n    def time_quantile(self, constructor, window, dtype, percentile, interpolation):\n        self.roll.quantile(percentile, interpolation=interpolation)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, constructor, window, dtype, percentile, interpolation):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)""}",rolling.time_quantile,time,0.0
15308,func-level,"terminus-2,oracle",1.1082312294361656,1.1082312294361656,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.time_groupby_method,rolling,"{""rolling.GroupbyEWM.time_groupby_method"": ""class GroupbyEWM:\n    def time_groupby_method(self, method):\n        getattr(self.gb_ewm, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupbyEWM:\n    def setup(self, method):\n        df = pd.DataFrame({\""A\"": range(50), \""B\"": range(50)})\n        self.gb_ewm = df.groupby(\""A\"").ewm(com=1.0)""}",rolling.time_groupby_method,time,0.0
15295,func-level,"terminus-2,oracle",1.1113814191673432,1.1113814191673432,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.time_melt_dataframe,reshape,"{""reshape.Melt.time_melt_dataframe"": ""class Melt:\n    def time_melt_dataframe(self, dtype):\n        melt(self.df, id_vars=[\""id1\"", \""id2\""])\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Melt:\n    def setup(self, dtype):\n        self.df = DataFrame(\n            np.random.randn(100_000, 3), columns=[\""A\"", \""B\"", \""C\""], dtype=dtype\n        )\n        self.df[\""id1\""] = pd.Series(np.random.randint(0, 10, 10000))\n        self.df[\""id2\""] = pd.Series(np.random.randint(100, 1000, 10000))""}",reshape.time_melt_dataframe,time,0.0
15304,func-level,"terminus-2,oracle",1.060642811266571,1.060642811266571,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.time_apply,rolling,"{""rolling.TableMethod.time_apply"": ""class TableMethod:\n    def time_apply(self, method):\n        self.df.rolling(2, method=method).apply(\n            table_method_func, raw=True, engine=\""numba\""\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TableMethod:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(10, 1000))""}",rolling.time_apply,time,0.0
15293,func-level,"terminus-2,oracle",1.1399230301212795,1.1399230301212795,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.time_cut_timedelta,reshape,"{""reshape.Cut.time_cut_timedelta"": ""class Cut:\n    def time_cut_timedelta(self, bins):\n        pd.cut(self.timedelta_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))""}",reshape.time_cut_timedelta,time,0.0
15309,func-level,"terminus-2,oracle",1.0555323259758504,1.0555323259758504,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.time_method,rolling,"{""rolling.Groupby.time_method"": ""class Groupby:\n    def time_method(self, method, window_kwargs):\n        getattr(self.groupby_window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Groupby:\n    def setup(self, method, window_kwargs):\n        N = 1000\n        window, kwargs = window_kwargs\n        df = pd.DataFrame(\n            {\n                \""A\"": [str(i) for i in range(N)] * 10,\n                \""B\"": list(range(N)) * 10,\n            }\n        )\n        if isinstance(kwargs.get(\""window\"", None), str):\n            df.index = pd.date_range(start=\""1900-01-01\"", freq=\""1min\"", periods=N * 10)\n        self.groupby_window = getattr(df.groupby(\""A\""), window)(**kwargs)"", ""rolling.Methods.time_method"": ""class Methods:\n    def time_method(self, constructor, window_kwargs, dtype, method):\n        getattr(self.window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Methods:\n    def setup(self, constructor, window_kwargs, dtype, method):\n        N = 10**5\n        window, kwargs = window_kwargs\n        arr = (100 * np.random.random(N)).astype(dtype)\n        obj = getattr(pd, constructor)(arr)\n        self.window = getattr(obj, window)(**kwargs)"", ""rolling.VariableWindowMethods.time_method"": ""class Methods:\n    def time_method(self, constructor, window_kwargs, dtype, method):\n        getattr(self.window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass VariableWindowMethods:\n    def setup(self, constructor, window, dtype, method):\n        N = 10**5\n        arr = (100 * np.random.random(N)).astype(dtype)\n        index = pd.date_range(\""2017-01-01\"", periods=N, freq=\""5s\"")\n        self.window = getattr(pd, constructor)(arr, index=index).rolling(window)""}",rolling.time_method,time,0.0
15300,func-level,"terminus-2,oracle",1.1130333586673145,1.1130333586673145,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.time_qcut_datetime,reshape,"{""reshape.Cut.time_qcut_datetime"": ""class Cut:\n    def time_qcut_datetime(self, bins):\n        pd.qcut(self.datetime_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))""}",reshape.time_qcut_datetime,time,0.0
15306,func-level,"terminus-2,oracle",1.0714356834437433,1.0714356834437433,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.time_groupby,rolling,"{""rolling.Pairwise.time_groupby"": ""class Pairwise:\n    def time_groupby(self, kwargs_window, method, pairwise):\n        getattr(self.window_group, method)(self.df, pairwise=pairwise)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Pairwise:\n    def setup(self, kwargs_window, method, pairwise):\n        N = 10**4\n        n_groups = 20\n        kwargs, window = kwargs_window\n        groups = [i for _ in range(N // n_groups) for i in range(n_groups)]\n        arr = np.random.random(N)\n        self.df = pd.DataFrame(arr)\n        self.window = getattr(self.df, window)(**kwargs)\n        self.window_group = getattr(\n            pd.DataFrame({\""A\"": groups, \""B\"": arr}).groupby(\""A\""), window\n        )(**kwargs)""}",rolling.time_groupby,time,0.0
15315,func-level,"terminus-2,oracle",1.0243815660476452,1.0243815660476452,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.time_any,series_methods,"{""series_methods.Any.time_any"": ""class Any:\n    def time_any(self, N, case, dtype):\n        self.s.any()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Any:\n    def setup(self, N, case, dtype):\n        val = case == \""fast\""\n        self.s = Series([val] * N, dtype=dtype)""}",series_methods.time_any,time,0.0
15302,func-level,"terminus-2,oracle",1.4175765820288715,1.4175765820288715,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.time_stack,reshape,"{""reshape.ReshapeExtensionDtype.time_stack"": ""class ReshapeExtensionDtype:\n    def time_stack(self, dtype):\n        self.df.stack()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReshapeExtensionDtype:\n    def setup(self, dtype):\n        lev = pd.Index(list(\""ABCDEFGHIJ\""))\n        ri = pd.Index(range(1000))\n        mi = MultiIndex.from_product([lev, ri], names=[\""foo\"", \""bar\""])\n    \n        index = date_range(\""2016-01-01\"", periods=10000, freq=\""s\"", tz=\""US/Pacific\"")\n        if dtype == \""Period[s]\"":\n            index = index.tz_localize(None).to_period(\""s\"")\n    \n        ser = pd.Series(index, index=mi)\n        df = ser.unstack(\""bar\"")\n        # roundtrips -> df.stack().equals(ser)\n    \n        self.ser = ser\n        self.df = df""}",reshape.time_stack,time,0.0
15310,func-level,"terminus-2,oracle",1.0529958245106887,1.0529958245106887,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.time_pairwise,rolling,"{""rolling.Pairwise.time_pairwise"": ""class Pairwise:\n    def time_pairwise(self, kwargs_window, method, pairwise):\n        getattr(self.window, method)(self.df, pairwise=pairwise)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Pairwise:\n    def setup(self, kwargs_window, method, pairwise):\n        N = 10**4\n        n_groups = 20\n        kwargs, window = kwargs_window\n        groups = [i for _ in range(N // n_groups) for i in range(n_groups)]\n        arr = np.random.random(N)\n        self.df = pd.DataFrame(arr)\n        self.window = getattr(self.df, window)(**kwargs)\n        self.window_group = getattr(\n            pd.DataFrame({\""A\"": groups, \""B\"": arr}).groupby(\""A\""), window\n        )(**kwargs)""}",rolling.time_pairwise,time,0.0
15307,func-level,"terminus-2,oracle",1.0461862350524451,1.0461862350524451,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.time_groupby_mean,rolling,"{""rolling.GroupbyEWMEngine.time_groupby_mean"": ""class GroupbyEWMEngine:\n    def time_groupby_mean(self, engine):\n        self.gb_ewm.mean(engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupbyEWMEngine:\n    def setup(self, engine):\n        df = pd.DataFrame({\""A\"": range(50), \""B\"": range(50)})\n        self.gb_ewm = df.groupby(\""A\"").ewm(com=1.0)""}",rolling.time_groupby_mean,time,0.0
15314,func-level,"terminus-2,oracle",1.0835962245212107,1.0835962245212107,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.time_rolling_multiindex_creation,rolling,"{""rolling.GroupbyLargeGroups.time_rolling_multiindex_creation"": ""class GroupbyLargeGroups:\n    def time_rolling_multiindex_creation(self):\n        self.df.groupby(\""A\"").rolling(3).mean()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupbyLargeGroups:\n    def setup(self):\n        N = 100000\n        self.df = pd.DataFrame({\""A\"": [1, 2] * (N // 2), \""B\"": np.random.randn(N)})""}",rolling.time_rolling_multiindex_creation,time,0.0
15313,func-level,"terminus-2,oracle",1.0504988571573846,1.0504988571573846,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.time_rolling,rolling,"{""rolling.Apply.time_rolling"": ""class Apply:\n    def time_rolling(self, constructor, window, dtype, function, raw):\n        self.roll.apply(function, raw=raw)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, constructor, window, dtype, function, raw):\n        N = 10**3\n        arr = (100 * np.random.random(N)).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)"", ""rolling.ForwardWindowMethods.time_rolling"": ""class ForwardWindowMethods:\n    def time_rolling(self, constructor, window_size, dtype, method):\n        getattr(self.roll, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ForwardWindowMethods:\n    def setup(self, constructor, window_size, dtype, method):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=window_size)\n        self.roll = getattr(pd, constructor)(arr).rolling(window=indexer)""}",rolling.time_rolling,time,0.0
15321,func-level,"terminus-2,oracle",0.9048225669589512,0.9048225669589512,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.time_add,sparse,"{""sparse.Arithmetic.time_add"": ""class Arithmetic:\n    def time_add(self, dense_proportion, fill_value):\n        self.array1 + self.array2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)""}",sparse.time_add,time,0.0
15312,func-level,"terminus-2,oracle",1.024073566008088,1.024073566008088,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.time_rank,rolling,"{""rolling.Rank.time_rank"": ""class Rank:\n    def time_rank(self, constructor, window, dtype, percentile, ascending, method):\n        self.roll.rank(pct=percentile, ascending=ascending, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Rank:\n    def setup(self, constructor, window, dtype, percentile, ascending, method):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)""}",rolling.time_rank,time,0.0
15317,func-level,"terminus-2,oracle",1.1976886276828178,1.1976886276828178,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.time_fillna,series_methods,"{""series_methods.Fillna.time_fillna"": ""class Fillna:\n    def time_fillna(self, dtype, method):\n        value = self.fill_value if method is None else None\n        self.ser.fillna(value=value, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Fillna:\n    def setup(self, dtype, method):\n        N = 10**6\n        if dtype == \""datetime64[ns]\"":\n            data = date_range(\""2000-01-01\"", freq=\""S\"", periods=N)\n            na_value = NaT\n        elif dtype in (\""float64\"", \""Float64\""):\n            data = np.random.randn(N)\n            na_value = np.nan\n        elif dtype in (\""Int64\"", \""int64[pyarrow]\""):\n            data = np.arange(N)\n            na_value = NA\n        elif dtype in (\""string\"", \""string[pyarrow]\""):\n            data = tm.rands_array(5, N)\n            na_value = NA\n        else:\n            raise NotImplementedError\n        fill_value = data[0]\n        ser = Series(data, dtype=dtype)\n        ser[::2] = na_value\n        self.ser = ser\n        self.fill_value = fill_value""}",series_methods.time_fillna,time,0.0
15316,func-level,"terminus-2,oracle",1.0118678655169218,1.0118678655169218,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.time_constructor_dict,series_methods,"{""series_methods.SeriesConstructor.time_constructor_dict"": ""class SeriesConstructor:\n    def time_constructor_dict(self):\n        Series(data=self.data, index=self.idx)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesConstructor:\n    def setup(self):\n        self.idx = date_range(\n            start=datetime(2015, 10, 26), end=datetime(2016, 1, 1), freq=\""50s\""\n        )\n        self.data = dict(zip(self.idx, range(len(self.idx))))\n        self.array = np.array([1, 2, 3])\n        self.idx2 = Index([\""a\"", \""b\"", \""c\""])""}",series_methods.time_constructor_dict,time,0.0
15319,func-level,"terminus-2,oracle",1.0549105958071132,1.0549105958071132,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.time_searchsorted,series_methods,"{""series_methods.SearchSorted.time_searchsorted"": ""class SearchSorted:\n    def time_searchsorted(self, dtype):\n        key = \""2\"" if dtype == \""str\"" else 2\n        self.s.searchsorted(key)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SearchSorted:\n    def setup(self, dtype):\n        N = 10**5\n        data = np.array([1] * N + [2] * N + [3] * N).astype(dtype)\n        self.s = Series(data)""}",series_methods.time_searchsorted,time,0.0
15327,func-level,"terminus-2,oracle",1.0273384746739196,1.0273384746739196,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,stat_ops.time_corr,stat_ops,"{""stat_ops.Correlation.time_corr"": ""class Correlation:\n    def time_corr(self, method):\n        self.df.corr(method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Correlation:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(500, 15))\n        self.df2 = pd.DataFrame(np.random.randn(500, 15))\n        self.df_wide = pd.DataFrame(np.random.randn(500, 100))\n        self.df_wide_nans = self.df_wide.where(np.random.random((500, 100)) < 0.9)\n        self.s = pd.Series(np.random.randn(500))\n        self.s2 = pd.Series(np.random.randn(500))""}",stat_ops.time_corr,time,0.0
15320,func-level,"terminus-2,oracle",0.9192684582296644,0.9192684582296644,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.time_to_frame,series_methods,"{""series_methods.ToFrame.time_to_frame"": ""class ToFrame:\n    def time_to_frame(self, dtype, name):\n        self.ser.to_frame(name)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToFrame:\n    def setup(self, dtype, name):\n        arr = np.arange(10**5)\n        ser = Series(arr, dtype=dtype)\n        self.ser = ser""}",series_methods.time_to_frame,time,0.0
15318,func-level,"terminus-2,oracle",1.0573150212261662,1.0573150212261662,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.time_func,series_methods,"{""series_methods.NanOps.time_func"": ""class NanOps:\n    def time_func(self, func, N, dtype):\n        self.func()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NanOps:\n    def setup(self, func, N, dtype):\n        if func == \""argmax\"" and dtype in {\""Int64\"", \""boolean\""}:\n            # Skip argmax for nullable int since this doesn't work yet (GH-24382)\n            raise NotImplementedError\n        self.s = Series([1] * N, dtype=dtype)\n        self.func = getattr(self.s, func)""}",series_methods.time_func,time,0.0
15322,func-level,"terminus-2,oracle",0.912691752252956,0.912691752252956,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.time_addition,sparse,"{""sparse.ArithmeticBlock.time_addition"": ""class ArithmeticBlock:\n    def time_addition(self, fill_value):\n        self.arr1 + self.arr2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )""}",sparse.time_addition,time,0.0
15328,func-level,"terminus-2,oracle",1.0425458957734757,1.0425458957734757,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,stat_ops.time_corr_series,stat_ops,"{""stat_ops.Correlation.time_corr_series"": ""class Correlation:\n    def time_corr_series(self, method):\n        self.s.corr(self.s2, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Correlation:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(500, 15))\n        self.df2 = pd.DataFrame(np.random.randn(500, 15))\n        self.df_wide = pd.DataFrame(np.random.randn(500, 100))\n        self.df_wide_nans = self.df_wide.where(np.random.random((500, 100)) < 0.9)\n        self.s = pd.Series(np.random.randn(500))\n        self.s2 = pd.Series(np.random.randn(500))""}",stat_ops.time_corr_series,time,0.0
15325,func-level,"terminus-2,oracle",0.9104927395620972,0.9104927395620972,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.time_make_union,sparse,"{""sparse.Arithmetic.time_make_union"": ""class Arithmetic:\n    def time_make_union(self, dense_proportion, fill_value):\n        self.array1.sp_index.make_union(self.array2.sp_index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)"", ""sparse.ArithmeticBlock.time_make_union"": ""class ArithmeticBlock:\n    def time_make_union(self, fill_value):\n        self.arr1.sp_index.make_union(self.arr2.sp_index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )""}",sparse.time_make_union,time,0.0
15329,func-level,"terminus-2,oracle",1.012040175725413,1.012040175725413,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,stat_ops.time_corr_wide_nans,stat_ops,"{""stat_ops.Correlation.time_corr_wide_nans"": ""class Correlation:\n    def time_corr_wide_nans(self, method):\n        self.df_wide_nans.corr(method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Correlation:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(500, 15))\n        self.df2 = pd.DataFrame(np.random.randn(500, 15))\n        self.df_wide = pd.DataFrame(np.random.randn(500, 100))\n        self.df_wide_nans = self.df_wide.where(np.random.random((500, 100)) < 0.9)\n        self.s = pd.Series(np.random.randn(500))\n        self.s2 = pd.Series(np.random.randn(500))""}",stat_ops.time_corr_wide_nans,time,0.0
15331,func-level,"terminus-2,oracle",1.027937502439999,1.027937502439999,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,stat_ops.time_corrwith_rows,stat_ops,"{""stat_ops.Correlation.time_corrwith_rows"": ""class Correlation:\n    def time_corrwith_rows(self, method):\n        self.df.corrwith(self.df2, axis=1, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Correlation:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(500, 15))\n        self.df2 = pd.DataFrame(np.random.randn(500, 15))\n        self.df_wide = pd.DataFrame(np.random.randn(500, 100))\n        self.df_wide_nans = self.df_wide.where(np.random.random((500, 100)) < 0.9)\n        self.s = pd.Series(np.random.randn(500))\n        self.s2 = pd.Series(np.random.randn(500))""}",stat_ops.time_corrwith_rows,time,0.0
15332,func-level,"terminus-2,oracle",1.0320174211687203,1.0320174211687203,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,stat_ops.time_op,stat_ops,"{""stat_ops.FrameMultiIndexOps.time_op"": ""class FrameMultiIndexOps:\n    def time_op(self, op):\n        self.df_func()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FrameMultiIndexOps:\n    def setup(self, op):\n        levels = [np.arange(10), np.arange(100), np.arange(100)]\n        codes = [\n            np.arange(10).repeat(10000),\n            np.tile(np.arange(100).repeat(100), 10),\n            np.tile(np.tile(np.arange(100), 100), 10),\n        ]\n        index = pd.MultiIndex(levels=levels, codes=codes)\n        df = pd.DataFrame(np.random.randn(len(index), 4), index=index)\n        self.df_func = getattr(df, op)"", ""stat_ops.FrameOps.time_op"": ""class FrameOps:\n    def time_op(self, op, dtype, axis):\n        self.df_func(axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FrameOps:\n    def setup(self, op, dtype, axis):\n        values = np.random.randn(100000, 4)\n        if dtype == \""Int64\"":\n            values = values.astype(int)\n        df = pd.DataFrame(values).astype(dtype)\n        self.df_func = getattr(df, op)"", ""stat_ops.SeriesOps.time_op"": ""class SeriesOps:\n    def time_op(self, op, dtype):\n        self.s_func()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesOps:\n    def setup(self, op, dtype):\n        s = pd.Series(np.random.randn(100000)).astype(dtype)\n        self.s_func = getattr(s, op)""}",stat_ops.time_op,time,0.0
15336,func-level,"terminus-2,oracle",1.0221869382018522,1.0221869382018522,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.time_contains,strings,"{""strings.Contains.time_contains"": ""class Contains:\n    def time_contains(self, dtype, regex):\n        self.s.str.contains(\""A\"", regex=regex)\n\n    def setup(self, dtype, regex):\n        super().setup(dtype)""}",strings.time_contains,time,0.0
15334,func-level,"terminus-2,oracle",1.026777433579889,1.026777433579889,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strftime.time_frame_offset_repr,strftime,"{""strftime.BusinessHourStrftime.time_frame_offset_repr"": ""class BusinessHourStrftime:\n    def time_frame_offset_repr(self, obs):\n        self.data[\""off\""].apply(repr)\n\n    def setup(self, obs):\n        self.data = pd.DataFrame(\n            {\n                \""off\"": [offsets.BusinessHour()] * obs,\n            }\n        )""}",strftime.time_frame_offset_repr,time,0.0
15324,func-level,"terminus-2,oracle",0.9086828083158498,0.9086828083158498,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.time_division,sparse,"{""sparse.ArithmeticBlock.time_division"": ""class ArithmeticBlock:\n    def time_division(self, fill_value):\n        self.arr1 / self.arr2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )""}",sparse.time_division,time,0.0
15326,func-level,"terminus-2,oracle",1.0153968750429954,1.0153968750429954,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.time_take,sparse,"{""sparse.Take.time_take"": ""class Take:\n    def time_take(self, indices, allow_fill):\n        self.sp_arr.take(indices, allow_fill=allow_fill)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Take:\n    def setup(self, indices, allow_fill):\n        N = 1_000_000\n        fill_value = 0.0\n        arr = make_array(N, 1e-5, fill_value, np.float64)\n        self.sp_arr = SparseArray(arr, fill_value=fill_value)""}",sparse.time_take,time,0.0
15330,func-level,"terminus-2,oracle",1.0308942388787077,1.0308942388787077,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,stat_ops.time_corrwith_cols,stat_ops,"{""stat_ops.Correlation.time_corrwith_cols"": ""class Correlation:\n    def time_corrwith_cols(self, method):\n        self.df.corrwith(self.df2, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Correlation:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(500, 15))\n        self.df2 = pd.DataFrame(np.random.randn(500, 15))\n        self.df_wide = pd.DataFrame(np.random.randn(500, 100))\n        self.df_wide_nans = self.df_wide.where(np.random.random((500, 100)) < 0.9)\n        self.s = pd.Series(np.random.randn(500))\n        self.s2 = pd.Series(np.random.randn(500))""}",stat_ops.time_corrwith_cols,time,0.0
15338,func-level,"terminus-2,oracle",1.02486375275368,1.02486375275368,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.time_findall,strings,"{""strings.Methods.time_findall"": ""class Methods:\n    def time_findall(self, dtype):\n        self.s.str.findall(\""[A-Z]+\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.time_findall,time,0.0
15333,func-level,"terminus-2,oracle",1.0107349830512775,1.0107349830512775,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strftime.time_frame_datetime_to_str,strftime,"{""strftime.DatetimeStrftime.time_frame_datetime_to_str"": ""class DatetimeStrftime:\n    def time_frame_datetime_to_str(self, obs):\n        self.data[\""dt\""].astype(str)\n\n    def setup(self, obs):\n        d = \""2018-11-29\""\n        dt = \""2018-11-26 11:18:27.0\""\n        self.data = pd.DataFrame(\n            {\n                \""dt\"": [np.datetime64(dt)] * obs,\n                \""d\"": [np.datetime64(d)] * obs,\n                \""r\"": [np.random.uniform()] * obs,\n            }\n        )""}",strftime.time_frame_datetime_to_str,time,0.0
15335,func-level,"terminus-2,oracle",1.029138406425122,1.029138406425122,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strftime.time_frame_offset_str,strftime,"{""strftime.BusinessHourStrftime.time_frame_offset_str"": ""class BusinessHourStrftime:\n    def time_frame_offset_str(self, obs):\n        self.data[\""off\""].apply(str)\n\n    def setup(self, obs):\n        self.data = pd.DataFrame(\n            {\n                \""off\"": [offsets.BusinessHour()] * obs,\n            }\n        )""}",strftime.time_frame_offset_str,time,0.0
15340,func-level,"terminus-2,oracle",1.0196207599438951,1.0196207599438951,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.time_get,strings,"{""strings.Methods.time_get"": ""class Methods:\n    def time_get(self, dtype):\n        self.s.str.get(0)\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.time_get,time,0.0
15347,func-level,"terminus-2,oracle",1.0312078855701563,1.0312078855701563,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.time_wrap,strings,"{""strings.Methods.time_wrap"": ""class Methods:\n    def time_wrap(self, dtype):\n        self.s.str.wrap(10)\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.time_wrap,time,0.0
15339,func-level,"terminus-2,oracle",1.0329384160058914,1.0329384160058914,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.time_fullmatch,strings,"{""strings.Methods.time_fullmatch"": ""class Methods:\n    def time_fullmatch(self, dtype):\n        self.s.str.fullmatch(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.time_fullmatch,time,0.0
15343,func-level,"terminus-2,oracle",1.0251988182556808,1.0251988182556808,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.time_istitle,strings,"{""strings.Methods.time_istitle"": ""class Methods:\n    def time_istitle(self, dtype):\n        self.s.str.istitle()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.time_istitle,time,0.0
15323,func-level,"terminus-2,oracle",0.91583897802989,0.91583897802989,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.time_divide,sparse,"{""sparse.Arithmetic.time_divide"": ""class Arithmetic:\n    def time_divide(self, dense_proportion, fill_value):\n        self.array1 / self.array2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)""}",sparse.time_divide,time,0.0
15348,func-level,"terminus-2,oracle",1.0827033762685614,1.0827033762685614,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timedelta.time_intersection,timedelta,"{""timedelta.TimedeltaIndexing.time_intersection"": ""class TimedeltaIndexing:\n    def time_intersection(self):\n        self.index.intersection(self.index2)\n\n    def setup(self):\n        self.index = timedelta_range(start=\""1985\"", periods=1000, freq=\""D\"")\n        self.index2 = timedelta_range(start=\""1986\"", periods=1000, freq=\""D\"")\n        self.series = Series(range(1000), index=self.index)\n        self.timedelta = self.index[500]""}",timedelta.time_intersection,time,0.0
15341,func-level,"terminus-2,oracle",1.018177527973979,1.018177527973979,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.time_get_dummies,strings,"{""strings.Dummies.time_get_dummies"": ""class Dummies:\n    def time_get_dummies(self, dtype):\n        self.s.str.get_dummies(\""|\"")\n\n    def setup(self, dtype):\n        super().setup(dtype)\n        self.s = self.s.str.join(\""|\"")""}",strings.time_get_dummies,time,0.0
15344,func-level,"terminus-2,oracle",1.3086013614510008,1.3086013614510008,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.time_lower,strings,"{""strings.Methods.time_lower"": ""class Methods:\n    def time_lower(self, dtype):\n        self.s.str.lower()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.time_lower,time,0.0
15337,func-level,"terminus-2,oracle",1.037615166606883,1.037615166606883,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.time_endswith,strings,"{""strings.Methods.time_endswith"": ""class Methods:\n    def time_endswith(self, dtype):\n        self.s.str.endswith(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.time_endswith,time,0.0
15350,func-level,"terminus-2,oracle",1.005475445309784,1.005475445309784,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timedelta.time_timedelta_seconds,timedelta,"{""timedelta.DatetimeAccessor.time_timedelta_seconds"": ""class DatetimeAccessor:\n    def time_timedelta_seconds(self, series):\n        series.dt.seconds\n\n    def setup_cache(self):\n        N = 100000\n        series = Series(timedelta_range(\""1 days\"", periods=N, freq=\""h\""))\n        return series""}",timedelta.time_timedelta_seconds,time,0.0
15342,func-level,"terminus-2,oracle",1.0532729945250614,1.0532729945250614,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.time_isdigit,strings,"{""strings.Methods.time_isdigit"": ""class Methods:\n    def time_isdigit(self, dtype):\n        self.s.str.isdigit()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.time_isdigit,time,0.0
15349,func-level,"terminus-2,oracle",1.0368987124797229,1.0368987124797229,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timedelta.time_series_loc,timedelta,"{""timedelta.TimedeltaIndexing.time_series_loc"": ""class TimedeltaIndexing:\n    def time_series_loc(self):\n        self.series.loc[self.timedelta]\n\n    def setup(self):\n        self.index = timedelta_range(start=\""1985\"", periods=1000, freq=\""D\"")\n        self.index2 = timedelta_range(start=\""1986\"", periods=1000, freq=\""D\"")\n        self.series = Series(range(1000), index=self.index)\n        self.timedelta = self.index[500]""}",timedelta.time_series_loc,time,0.0
15345,func-level,"terminus-2,oracle",1.053556804380056,1.053556804380056,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.time_match,strings,"{""strings.Methods.time_match"": ""class Methods:\n    def time_match(self, dtype):\n        self.s.str.match(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.time_match,time,0.0
15353,func-level,"terminus-2,oracle",0.96242104773653,0.96242104773653,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.time_get,timeseries,"{""timeseries.DatetimeIndex.time_get"": ""class DatetimeIndex:\n    def time_get(self, index_type):\n        self.index[0]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]""}",timeseries.time_get,time,0.0
15361,func-level,"terminus-2,oracle",0.9843482318148696,0.9843482318148696,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.fields.time_get_timedelta_field,tslibs.fields,"{""tslibs.fields.TimeGetTimedeltaField.time_get_timedelta_field"": ""class TimeGetTimedeltaField:\n    def time_get_timedelta_field(self, size, field):\n        get_timedelta_field(self.i8data, field)\n\n    def setup(self, size, field):\n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.fields.time_get_timedelta_field,time,0.0
15354,func-level,"terminus-2,oracle",1.0714083894064304,1.0714083894064304,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.time_infer_dst,timeseries,"{""timeseries.TzLocalize.time_infer_dst"": ""class TzLocalize:\n    def time_infer_dst(self, tz):\n        self.index.tz_localize(tz, ambiguous=\""infer\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TzLocalize:\n    def setup(self, tz):\n        dst_rng = date_range(\n            start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n        )\n        self.index = date_range(start=\""10/29/2000\"", end=\""10/29/2000 00:59:59\"", freq=\""S\"")\n        self.index = self.index.append(dst_rng)\n        self.index = self.index.append(dst_rng)\n        self.index = self.index.append(\n            date_range(start=\""10/29/2000 2:00:00\"", end=\""10/29/2000 3:00:00\"", freq=\""S\"")\n        )""}",timeseries.time_infer_dst,time,0.0
15357,func-level,"terminus-2,oracle",0.9852909091765004,0.9852909091765004,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.time_to_date,timeseries,"{""timeseries.DatetimeIndex.time_to_date"": ""class DatetimeIndex:\n    def time_to_date(self, index_type):\n        self.index.date\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]""}",timeseries.time_to_date,time,0.0
15346,func-level,"terminus-2,oracle",1.3596393751609956,1.3596393751609956,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.time_upper,strings,"{""strings.Methods.time_upper"": ""class Methods:\n    def time_upper(self, dtype):\n        self.s.str.upper()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.time_upper,time,0.0
15358,func-level,"terminus-2,oracle",1.010748999041344,1.010748999041344,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.time_to_time,timeseries,"{""timeseries.DatetimeIndex.time_to_time"": ""class DatetimeIndex:\n    def time_to_time(self, index_type):\n        self.index.time\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]""}",timeseries.time_to_time,time,0.0
15360,func-level,"terminus-2,oracle",1.026500169451337,1.026500169451337,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.fields.time_get_start_end_field,tslibs.fields,"{""tslibs.fields.TimeGetStartEndField.time_get_start_end_field"": ""class TimeGetStartEndField:\n    def time_get_start_end_field(self, size, side, period, freqstr, month_kw):\n        get_start_end_field(self.i8data, self.attrname, freqstr, month_kw=month_kw)\n\n    def setup(self, size, side, period, freqstr, month_kw):\n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr\n    \n        self.attrname = f\""is_{period}_{side}\""""}",tslibs.fields.time_get_start_end_field,time,0.0
15363,func-level,"terminus-2,oracle",1.0016897797076685,1.0016897797076685,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.normalize.time_normalize_i8_timestamps,tslibs.normalize,"{""tslibs.normalize.Normalize.time_normalize_i8_timestamps"": ""class Normalize:\n    def time_normalize_i8_timestamps(self, size, tz):\n        # 10 i.e. NPY_FR_ns\n        normalize_i8_timestamps(self.i8data, tz, 10)\n\n    def setup(self, size, tz):\n        # use an array that will have is_date_array_normalized give True,\n        #  so we do not short-circuit early.\n        dti = pd.date_range(\""2016-01-01\"", periods=10, tz=tz).repeat(size // 10)\n        self.i8data = dti.asi8\n    \n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError""}",tslibs.normalize.time_normalize_i8_timestamps,time,0.0
15355,func-level,"terminus-2,oracle",1.0555519096179355,1.0555519096179355,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.time_reset_datetimeindex,timeseries,"{""timeseries.ResetIndex.time_reset_datetimeindex"": ""class ResetIndex:\n    def time_reset_datetimeindex(self, tz):\n        self.df.reset_index()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ResetIndex:\n    def setup(self, tz):\n        idx = date_range(start=\""1/1/2000\"", periods=1000, freq=\""H\"", tz=tz)\n        self.df = DataFrame(np.random.randn(1000, 2), index=idx)""}",timeseries.time_reset_datetimeindex,time,0.0
15352,func-level,"terminus-2,oracle",0.9450309626574186,0.9450309626574186,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.time_dt_accessor_month_name,timeseries,"{""timeseries.DatetimeAccessor.time_dt_accessor_month_name"": ""class DatetimeAccessor:\n    def time_dt_accessor_month_name(self, tz):\n        self.series.dt.month_name()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeAccessor:\n    def setup(self, tz):\n        N = 100000\n        self.series = Series(date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"", tz=tz))""}",timeseries.time_dt_accessor_month_name,time,0.0
15351,func-level,"terminus-2,oracle",0.94904794756731,0.94904794756731,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.time_dt_accessor_day_name,timeseries,"{""timeseries.DatetimeAccessor.time_dt_accessor_day_name"": ""class DatetimeAccessor:\n    def time_dt_accessor_day_name(self, tz):\n        self.series.dt.day_name()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeAccessor:\n    def setup(self, tz):\n        N = 100000\n        self.series = Series(date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"", tz=tz))""}",timeseries.time_dt_accessor_day_name,time,0.0
15362,func-level,"terminus-2,oracle",0.9573888041737096,0.9573888041737096,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.normalize.time_is_date_array_normalized,tslibs.normalize,"{""tslibs.normalize.Normalize.time_is_date_array_normalized"": ""class Normalize:\n    def time_is_date_array_normalized(self, size, tz):\n        # TODO: cases with different levels of short-circuiting\n        # 10 i.e. NPY_FR_ns\n        is_date_array_normalized(self.i8data, tz, 10)\n\n    def setup(self, size, tz):\n        # use an array that will have is_date_array_normalized give True,\n        #  so we do not short-circuit early.\n        dti = pd.date_range(\""2016-01-01\"", periods=10, tz=tz).repeat(size // 10)\n        self.i8data = dti.asi8\n    \n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError""}",tslibs.normalize.time_is_date_array_normalized,time,0.0
15365,func-level,"terminus-2,oracle",1.0518811107510226,1.0518811107510226,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.time_add_10,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_add_10"": ""class OffestDatetimeArithmetic:\n    def time_add_10(self, offset):\n        self.date + (10 * offset)\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.time_add_10,time,0.0
15367,func-level,"terminus-2,oracle",1.001471372421829,1.001471372421829,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.time_on_offset,tslibs.offsets,"{""tslibs.offsets.OnOffset.time_on_offset"": ""class OnOffset:\n    def time_on_offset(self, offset):\n        for date in self.dates:\n            offset.is_on_offset(date)\n\n    def setup(self, offset):\n        self.dates = [\n            datetime(2016, m, d)\n            for m in [10, 11, 12]\n            for d in [1, 2, 3, 28, 29, 30, 31]\n            if not (m == 11 and d == 31)\n        ]""}",tslibs.offsets.time_on_offset,time,0.0
15370,func-level,"terminus-2,oracle",1.0187289916107118,1.0187289916107118,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.time_asfreq,tslibs.period,"{""tslibs.period.PeriodUnaryMethods.time_asfreq"": ""class PeriodUnaryMethods:\n    def time_asfreq(self, freq):\n        self.per.asfreq(\""A\"")\n\n    def setup(self, freq):\n        self.per = Period(\""2012-06-01\"", freq=freq)""}",tslibs.period.time_asfreq,time,0.0
15359,func-level,"terminus-2,oracle",0.9810336757022337,0.9810336757022337,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.fields.time_get_date_field,tslibs.fields,"{""tslibs.fields.TimeGetDateField.time_get_date_field"": ""class TimeGetDateField:\n    def time_get_date_field(self, size, field):\n        get_date_field(self.i8data, field)\n\n    def setup(self, size, field):\n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.fields.time_get_date_field,time,0.0
15376,func-level,"terminus-2,oracle",1.0745328131661918,1.0745328131661918,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.time_to_timestamp,tslibs.period,"{""tslibs.period.PeriodUnaryMethods.time_to_timestamp"": ""class PeriodUnaryMethods:\n    def time_to_timestamp(self, freq):\n        self.per.to_timestamp()\n\n    def setup(self, freq):\n        self.per = Period(\""2012-06-01\"", freq=freq)""}",tslibs.period.time_to_timestamp,time,0.0
15366,func-level,"terminus-2,oracle",1.0458309907695014,1.0458309907695014,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.time_add_np_dt64,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_add_np_dt64"": ""class OffestDatetimeArithmetic:\n    def time_add_np_dt64(self, offset):\n        offset + self.dt64\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.time_add_np_dt64,time,0.0
15368,func-level,"terminus-2,oracle",1.0383551718777015,1.0383551718777015,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.time_subtract,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_subtract"": ""class OffestDatetimeArithmetic:\n    def time_subtract(self, offset):\n        self.date - offset\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.time_subtract,time,0.0
15364,func-level,"terminus-2,oracle",1.046463312646811,1.046463312646811,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.time_add,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_add"": ""class OffestDatetimeArithmetic:\n    def time_add(self, offset):\n        self.date + offset\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.time_add,time,0.0
15377,func-level,"terminus-2,oracle",1.0065466865962085,1.0065466865962085,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.resolution.time_get_resolution,tslibs.resolution,"{""tslibs.resolution.TimeResolution.time_get_resolution"": ""class TimeResolution:\n    def time_get_resolution(self, unit, size, tz):\n        get_resolution(self.i8data, tz)\n\n    def setup(self, unit, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        arr = arr.view(f\""M8[{unit}]\"").astype(\""M8[ns]\"").view(\""i8\"")\n        self.i8data = arr""}",tslibs.resolution.time_get_resolution,time,0.0
15375,func-level,"terminus-2,oracle",1.0137567719325948,1.0137567719325948,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.time_property,tslibs.period,"{""tslibs.period.PeriodProperties.time_property"": ""class PeriodProperties:\n    def time_property(self, freq, attr):\n        getattr(self.per, attr)\n\n    def setup(self, freq, attr):\n        self.per = Period(\""2012-06-01\"", freq=freq)""}",tslibs.period.time_property,time,0.0
15372,func-level,"terminus-2,oracle",1.0369797119462696,1.0369797119462696,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.time_now,tslibs.period,"{""tslibs.period.PeriodUnaryMethods.time_now"": ""class PeriodUnaryMethods:\n    def time_now(self, freq):\n        self.per.now(freq)\n\n    def setup(self, freq):\n        self.per = Period(\""2012-06-01\"", freq=freq)""}",tslibs.period.time_now,time,0.0
15371,func-level,"terminus-2,oracle",1.0052824550428674,1.0052824550428674,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.time_dt64arr_to_periodarr,tslibs.period,"{""tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr"": ""class TimeDT64ArrToPeriodArr:\n    def time_dt64arr_to_periodarr(self, size, freq, tz):\n        dt64arr_to_periodarr(self.i8values, freq, tz)\n\n    def setup(self, size, freq, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.arange(10, dtype=\""i8\"").repeat(size // 10)\n        self.i8values = arr""}",tslibs.period.time_dt64arr_to_periodarr,time,0.0
15379,func-level,"terminus-2,oracle",1.026531993986255,1.026531993986255,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timedelta.time_from_iso_format,tslibs.timedelta,"{""tslibs.timedelta.TimedeltaConstructor.time_from_iso_format"": ""class TimedeltaConstructor:\n    def time_from_iso_format(self):\n        Timedelta(\""P4DT12H30M5S\"")\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")""}",tslibs.timedelta.time_from_iso_format,time,0.0
15369,func-level,"terminus-2,oracle",1.0505874439255598,1.0505874439255598,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.time_subtract_10,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_subtract_10"": ""class OffestDatetimeArithmetic:\n    def time_subtract_10(self, offset):\n        self.date - (10 * offset)\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.time_subtract_10,time,0.0
15383,func-level,"terminus-2,oracle",0.9912717761748578,0.9912717761748578,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_dayofyear,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_dayofyear"": ""class TimestampProperties:\n    def time_dayofyear(self, tz):\n        self.ts.dayofyear\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.time_dayofyear,time,0.0
15374,func-level,"terminus-2,oracle",0.9737964783501856,0.9737964783501856,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.time_periodarray_to_dt64arr,tslibs.period,"{""tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr"": ""class TimePeriodArrToDT64Arr:\n    def time_periodarray_to_dt64arr(self, size, freq):\n        periodarr_to_dt64arr(self.i8values, freq)\n\n    def setup(self, size, freq):\n        arr = np.arange(10, dtype=\""i8\"").repeat(size // 10)\n        self.i8values = arr""}",tslibs.period.time_periodarray_to_dt64arr,time,0.0
15378,func-level,"terminus-2,oracle",1.0156490510826353,1.0156490510826353,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timedelta.time_from_datetime_timedelta,tslibs.timedelta,"{""tslibs.timedelta.TimedeltaConstructor.time_from_datetime_timedelta"": ""class TimedeltaConstructor:\n    def time_from_datetime_timedelta(self):\n        Timedelta(self.dttimedelta)\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")""}",tslibs.timedelta.time_from_datetime_timedelta,time,0.0
15380,func-level,"terminus-2,oracle",1.0242987605277876,1.0242987605277876,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timedelta.time_from_np_timedelta,tslibs.timedelta,"{""tslibs.timedelta.TimedeltaConstructor.time_from_np_timedelta"": ""class TimedeltaConstructor:\n    def time_from_np_timedelta(self):\n        Timedelta(self.nptimedelta64)\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")""}",tslibs.timedelta.time_from_np_timedelta,time,0.0
15381,func-level,"terminus-2,oracle",1.0272665785845632,1.0272665785845632,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timedelta.time_from_string,tslibs.timedelta,"{""tslibs.timedelta.TimedeltaConstructor.time_from_string"": ""class TimedeltaConstructor:\n    def time_from_string(self):\n        Timedelta(\""1 days\"")\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")""}",tslibs.timedelta.time_from_string,time,0.0
15373,func-level,"terminus-2,oracle",1.0892287783290096,1.0892287783290096,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.time_period_constructor,tslibs.period,"{""tslibs.period.PeriodConstructor.time_period_constructor"": ""class PeriodConstructor:\n    def time_period_constructor(self, freq, is_offset):\n        Period(\""2012-06-01\"", freq=freq)\n\n    def setup(self, freq, is_offset):\n        if is_offset:\n            self.freq = to_offset(freq)\n        else:\n            self.freq = freq""}",tslibs.period.time_period_constructor,time,0.0
15385,func-level,"terminus-2,oracle",1.0381472749374108,1.0381472749374108,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_floor,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_floor"": ""class TimestampOps:\n    def time_floor(self, tz):\n        self.ts.floor(\""5T\"")\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.time_floor,time,0.0
15387,func-level,"terminus-2,oracle",1.0272462397518227,1.0272462397518227,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_from_datetime_unaware,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_from_datetime_unaware"": ""class TimestampConstruction:\n    def time_from_datetime_unaware(self):\n        Timestamp(self.dttime_unaware)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.time_from_datetime_unaware,time,0.0
15388,func-level,"terminus-2,oracle",1.01738764873653,1.01738764873653,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_from_npdatetime64,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_from_npdatetime64"": ""class TimestampConstruction:\n    def time_from_npdatetime64(self):\n        Timestamp(self.npdatetime64)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.time_from_npdatetime64,time,0.0
15382,func-level,"terminus-2,oracle",1.0348650191997786,1.0348650191997786,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_ceil,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_ceil"": ""class TimestampOps:\n    def time_ceil(self, tz):\n        self.ts.ceil(\""5T\"")\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.time_ceil,time,0.0
15390,func-level,"terminus-2,oracle",1.0096282482119523,1.0096282482119523,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_is_quarter_end,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_is_quarter_end"": ""class TimestampProperties:\n    def time_is_quarter_end(self, tz):\n        self.ts.is_quarter_end\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.time_is_quarter_end,time,0.0
15391,func-level,"terminus-2,oracle",1.0098146508293273,1.0098146508293273,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_is_quarter_start,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_is_quarter_start"": ""class TimestampProperties:\n    def time_is_quarter_start(self, tz):\n        self.ts.is_quarter_start\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.time_is_quarter_start,time,0.0
15386,func-level,"terminus-2,oracle",1.026953965589444,1.026953965589444,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_from_datetime_aware,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_from_datetime_aware"": ""class TimestampConstruction:\n    def time_from_datetime_aware(self):\n        Timestamp(self.dttime_aware)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.time_from_datetime_aware,time,0.0
15396,func-level,"terminus-2,oracle",1.0362342669924922,1.0362342669924922,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_normalize,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_normalize"": ""class TimestampOps:\n    def time_normalize(self, tz):\n        self.ts.normalize()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.time_normalize,time,0.0
15389,func-level,"terminus-2,oracle",1.015546681180151,1.015546681180151,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_is_leap_year,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_is_leap_year"": ""class TimestampProperties:\n    def time_is_leap_year(self, tz):\n        self.ts.is_leap_year\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.time_is_leap_year,time,0.0
15384,func-level,"terminus-2,oracle",1.0088496122333308,1.0088496122333308,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_days_in_month,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_days_in_month"": ""class TimestampProperties:\n    def time_days_in_month(self, tz):\n        self.ts.days_in_month\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.time_days_in_month,time,0.0
15395,func-level,"terminus-2,oracle",0.9791569957625458,0.9791569957625458,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_month_name,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_month_name"": ""class TimestampProperties:\n    def time_month_name(self, tz):\n        self.ts.month_name()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.time_month_name,time,0.0
15356,func-level,"terminus-2,oracle",0.9911992258081832,0.9911992258081832,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.time_timeseries_is_month_start,timeseries,"{""timeseries.DatetimeIndex.time_timeseries_is_month_start"": ""class DatetimeIndex:\n    def time_timeseries_is_month_start(self, index_type):\n        self.index.is_month_start\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]""}",timeseries.time_timeseries_is_month_start,time,0.0
15394,func-level,"terminus-2,oracle",1.0101241275047386,1.0101241275047386,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_microsecond,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_microsecond"": ""class TimestampProperties:\n    def time_microsecond(self, tz):\n        self.ts.microsecond\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.time_microsecond,time,0.0
15393,func-level,"terminus-2,oracle",1.0104473288567313,1.0104473288567313,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_is_year_start,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_is_year_start"": ""class TimestampProperties:\n    def time_is_year_start(self, tz):\n        self.ts.is_year_start\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.time_is_year_start,time,0.0
15410,func-level,"terminus-2,oracle",1.0548652750818708,1.0548652750818708,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.tz_convert.time_tz_convert_from_utc,tslibs.tz_convert,"{""tslibs.tz_convert.TimeTZConvert.time_tz_convert_from_utc"": ""class TimeTZConvert:\n    def time_tz_convert_from_utc(self, size, tz):\n        # effectively:\n        #  dti = DatetimeIndex(self.i8data, tz=tz)\n        #  dti.tz_localize(None)\n        if old_sig:\n            tz_convert_from_utc(self.i8data, UTC, tz)\n        else:\n            tz_convert_from_utc(self.i8data, tz)\n\n    def setup(self, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.tz_convert.time_tz_convert_from_utc,time,0.0
15399,func-level,"terminus-2,oracle",1.0164790544428826,1.0164790544428826,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_parse_iso8601_tz,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_parse_iso8601_tz"": ""class TimestampConstruction:\n    def time_parse_iso8601_tz(self):\n        Timestamp(\""2017-08-25 08:16:14-0500\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.time_parse_iso8601_tz,time,0.0
15398,func-level,"terminus-2,oracle",1.0390043640338635,1.0390043640338635,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_parse_iso8601_no_tz,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_parse_iso8601_no_tz"": ""class TimestampConstruction:\n    def time_parse_iso8601_no_tz(self):\n        Timestamp(\""2017-08-25 08:16:14\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.time_parse_iso8601_no_tz,time,0.0
15392,func-level,"terminus-2,oracle",1.009174308037522,1.009174308037522,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_is_year_end,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_is_year_end"": ""class TimestampProperties:\n    def time_is_year_end(self, tz):\n        self.ts.is_year_end\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.time_is_year_end,time,0.0
15402,func-level,"terminus-2,oracle",1.0385202816031789,1.0385202816031789,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_replace_across_dst,tslibs.timestamp,"{""tslibs.timestamp.TimestampAcrossDst.time_replace_across_dst"": ""class TimestampAcrossDst:\n    def time_replace_across_dst(self):\n        self.ts2.replace(tzinfo=self.tzinfo)\n\n    def setup(self):\n        dt = datetime(2016, 3, 27, 1)\n        self.tzinfo = pytz.timezone(\""CET\"").localize(dt, is_dst=False).tzinfo\n        self.ts2 = Timestamp(dt)""}",tslibs.timestamp.time_replace_across_dst,time,0.0
15406,func-level,"terminus-2,oracle",0.9848410265394214,0.9848410265394214,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_tz_localize,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_tz_localize"": ""class TimestampOps:\n    def time_tz_localize(self, tz):\n        if self.ts.tz is None:\n            self.ts.tz_localize(tz)\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.time_tz_localize,time,0.0
15397,func-level,"terminus-2,oracle",1.0562877508565431,1.0562877508565431,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_parse_dateutil,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_parse_dateutil"": ""class TimestampConstruction:\n    def time_parse_dateutil(self):\n        Timestamp(\""2017/08/25 08:16:14 AM\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.time_parse_dateutil,time,0.0
15403,func-level,"terminus-2,oracle",1.0111371176694943,1.0111371176694943,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_replace_tz,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_replace_tz"": ""class TimestampOps:\n    def time_replace_tz(self, tz):\n        self.ts.replace(tzinfo=pytz.timezone(\""US/Eastern\""))\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.time_replace_tz,time,0.0
15401,func-level,"terminus-2,oracle",1.0477511425107873,1.0477511425107873,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_replace_None,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_replace_None"": ""class TimestampOps:\n    def time_replace_None(self, tz):\n        self.ts.replace(tzinfo=None)\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.time_replace_None,time,0.0
15405,func-level,"terminus-2,oracle",0.97438388789397,0.97438388789397,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_tz_convert,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_tz_convert"": ""class TimestampOps:\n    def time_tz_convert(self, tz):\n        if self.ts.tz is not None:\n            self.ts.tz_convert(tz)\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.time_tz_convert,time,0.0
15400,func-level,"terminus-2,oracle",1.0233434816712337,1.0233434816712337,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_quarter,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_quarter"": ""class TimestampProperties:\n    def time_quarter(self, tz):\n        self.ts.quarter\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.time_quarter,time,0.0
15407,func-level,"terminus-2,oracle",1.0142606399277831,1.0142606399277831,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_week,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_week"": ""class TimestampProperties:\n    def time_week(self, tz):\n        self.ts.week\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.time_week,time,0.0
15408,func-level,"terminus-2,oracle",0.9855268261361976,0.9855268261361976,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_weekday_name,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_weekday_name"": ""class TimestampProperties:\n    def time_weekday_name(self, tz):\n        self.ts.day_name()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.time_weekday_name,time,0.0
15409,func-level,"terminus-2,oracle",1.0124058949042414,1.0124058949042414,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.tslib.time_ints_to_pydatetime,tslibs.tslib,"{""tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime"": ""class TimeIntsToPydatetime:\n    def time_ints_to_pydatetime(self, box, size, tz):\n        ints_to_pydatetime(self.i8data, tz, box=box)\n\n    def setup(self, box, size, tz):\n        if box == \""date\"" and tz is not None:\n            # tz is ignored, so avoid running redundant benchmarks\n            raise NotImplementedError  # skip benchmark\n        if size == 10**6 and tz is _tzs[-1]:\n            # This is cumbersomely-slow, so skip to trim runtime\n            raise NotImplementedError  # skip benchmark\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.tslib.time_ints_to_pydatetime,time,0.0
15404,func-level,"terminus-2,oracle",1.0201454057753734,1.0201454057753734,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.time_to_julian_date,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_to_julian_date"": ""class TimestampOps:\n    def time_to_julian_date(self, tz):\n        self.ts.to_julian_date()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.time_to_julian_date,time,0.0
15411,func-level,"terminus-2,oracle",0.9598299359561444,0.9598299359561444,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.tz_convert.time_tz_localize_to_utc,tslibs.tz_convert,"{""tslibs.tz_convert.TimeTZConvert.time_tz_localize_to_utc"": ""class TimeTZConvert:\n    def time_tz_localize_to_utc(self, size, tz):\n        # effectively:\n        #  dti = DatetimeIndex(self.i8data)\n        #  dti.tz_localize(tz, ambiguous=\""NaT\"", nonexistent=\""NaT\"")\n        tz_localize_to_utc(self.i8data, tz, ambiguous=\""NaT\"", nonexistent=\""NaT\"")\n\n    def setup(self, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.tz_convert.time_tz_localize_to_utc,time,0.0
21903,func-level,"terminus-2,claude",0.9002117666510273,0.9384013064633068,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,metrics.time_pairwise_distances,metrics,"{""metrics.PairwiseDistancesBenchmark.time_pairwise_distances"": ""class PairwiseDistancesBenchmark:\n    def time_pairwise_distances(self, *args):\n        pairwise_distances(self.X, **self.pdist_params)\n\n    def setup(self, *params):\n        representation, metric, n_jobs = params\n    \n        if representation == \""sparse\"" and metric == \""correlation\"":\n            raise NotImplementedError\n    \n        if Benchmark.data_size == \""large\"":\n            if metric in (\""manhattan\"", \""correlation\""):\n                n_samples = 8000\n            else:\n                n_samples = 24000\n        else:\n            if metric in (\""manhattan\"", \""correlation\""):\n                n_samples = 4000\n            else:\n                n_samples = 12000\n    \n        data = _random_dataset(n_samples=n_samples, representation=representation)\n        self.X, self.X_val, self.y, self.y_val = data\n    \n        self.pdist_params = {\""metric\"": metric, \""n_jobs\"": n_jobs}""}",metrics.time_pairwise_distances,time,-0.027008161111937386
21900,func-level,"terminus-2,claude",0.8335698226637654,0.8450338227894318,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,cluster.time_transform,cluster,"{""cluster.MiniBatchKMeansBenchmark.time_transform"": ""class Transformer:\n    def time_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchKMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",cluster.time_transform,time,-0.008107496552805089
21902,func-level,"terminus-2,claude",1.182808465037166,3.093394649383032,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,ensemble.time_predict,ensemble,"{""ensemble.HistGradientBoostingClassifierBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass HistGradientBoostingClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""ensemble.RandomForestClassifierBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RandomForestClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",ensemble.time_predict,time,-1.351192492465252
21910,func-level,"terminus-2,oracle",1.4341243541277937,1.4341243541277937,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,cluster.time_predict,cluster,"{""cluster.KMeansBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",cluster.time_predict,time,0.0
21909,func-level,"terminus-2,oracle",1.076354495272499,1.076354495272499,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,cluster.time_fit,cluster,"{""cluster.KMeansBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""cluster.MiniBatchKMeansBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchKMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",cluster.time_fit,time,0.0
21913,func-level,"terminus-2,oracle",3.093394649383032,3.093394649383032,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,ensemble.time_predict,ensemble,"{""ensemble.HistGradientBoostingClassifierBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass HistGradientBoostingClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",ensemble.time_predict,time,0.0
21916,func-level,"terminus-2,oracle",0.9384013064633068,0.9384013064633068,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,metrics.time_pairwise_distances,metrics,"{""metrics.PairwiseDistancesBenchmark.time_pairwise_distances"": ""class PairwiseDistancesBenchmark:\n    def time_pairwise_distances(self, *args):\n        pairwise_distances(self.X, **self.pdist_params)\n\n    def setup(self, *params):\n        representation, metric, n_jobs = params\n    \n        if representation == \""sparse\"" and metric == \""correlation\"":\n            raise NotImplementedError\n    \n        if Benchmark.data_size == \""large\"":\n            if metric in (\""manhattan\"", \""correlation\""):\n                n_samples = 8000\n            else:\n                n_samples = 24000\n        else:\n            if metric in (\""manhattan\"", \""correlation\""):\n                n_samples = 4000\n            else:\n                n_samples = 12000\n    \n        data = _random_dataset(n_samples=n_samples, representation=representation)\n        self.X, self.X_val, self.y, self.y_val = data\n    \n        self.pdist_params = {\""metric\"": metric, \""n_jobs\"": n_jobs}""}",metrics.time_pairwise_distances,time,0.0
21899,func-level,"terminus-2,claude",1.0742448146448818,1.03428784582855,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,cluster.time_fit,cluster,"{""cluster.KMeansBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",cluster.time_fit,time,0.028258110902639116
21911,func-level,"terminus-2,oracle",1.3412920425005193,1.3412920425005193,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,cluster.time_transform,cluster,"{""cluster.KMeansBenchmark.time_transform"": ""class Transformer:\n    def time_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""cluster.MiniBatchKMeansBenchmark.time_transform"": ""class Transformer:\n    def time_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchKMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",cluster.time_transform,time,0.0
21918,func-level,"terminus-2,oracle",0.9730120615379054,0.9730120615379054,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,neighbors.time_predict,neighbors,"{""neighbors.KNeighborsClassifierBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KNeighborsClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",neighbors.time_predict,time,0.0
21901,func-level,"terminus-2,claude",1.0147383648495112,1.024858002625557,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,ensemble.time_fit,ensemble,"{""ensemble.RandomForestClassifierBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RandomForestClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",ensemble.time_fit,time,-0.007156745244728248
21920,class-level,"terminus-2,claude",0.9674152093259594,0.9464337069256658,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports,imports,"{""imports.timeraw_import_astropy_table"": ""def timeraw_import_astropy_table():\n    return \""\""\""\n    from astropy import table\n    \""\""\"""", ""imports.timeraw_import_astropy_units"": ""def timeraw_import_astropy_units():\n    return \""\""\""\n    from astropy import units\n    \""\""\"""", ""imports.timeraw_import_astropy_utils"": ""def timeraw_import_astropy_utils():\n    return \""\""\""\n    from astropy import utils\n    \""\""\"""", ""imports.timeraw_import_astropy_utils_iers"": ""def timeraw_import_astropy_utils_iers():\n    return \""\""\""\n    from astropy.utils import iers\n    \""\""\"""", ""imports.timeraw_import_astropy_visualization"": ""def timeraw_import_astropy_visualization():\n    return \""\""\""\n    from astropy import visualization\n    \""\""\"""", ""imports.timeraw_import_astropy_wcs"": ""def timeraw_import_astropy_wcs():\n    return \""\""\""\n    from astropy import wcs\n    \""\""\""""}",imports,time,0.014838403394832828
21914,func-level,"terminus-2,oracle",0.930235842825598,0.930235842825598,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,linear_model.time_fit,linear_model,"{""linear_model.ElasticNetBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass ElasticNetBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""linear_model.LassoBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LassoBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""linear_model.LogisticRegressionBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LogisticRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""linear_model.SGDRegressorBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SGDRegressorBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",linear_model.time_fit,time,0.0
21924,class-level,"terminus-2,claude",1.055341850348677,1.0075690507073685,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,modeling.fitting,modeling.fitting,"{""modeling.fitting.time_Chebyshev2D_LinearLSQFitter"": ""def time_Chebyshev2D_LinearLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        z = z_base + np.random.normal(0.0, 0.2, z_base.shape)\n        fit_LinearLSQFitter(Chebyshev2D, x_grid, y_grid, z)\n    except Warning:\n        pass"", ""modeling.fitting.time_Gaussian2D_LevMarLSQFitter"": ""def time_Gaussian2D_LevMarLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        z = z_base + np.random.normal(0.0, 0.2, z_base.shape)\n        fit_LevMarLSQFitter(Gaussian2D, x_grid, y_grid, z)\n    except Warning:\n        pass"", ""modeling.fitting.time_Gaussian2D_SimplexLSQFitter"": ""def time_Gaussian2D_SimplexLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        z = z_base + np.random.normal(0.0, 0.2, z_base.shape)\n        fit_SimplexLSQFitter(Gaussian2D, x_grid, y_grid, z)\n    except Warning:\n        pass"", ""modeling.fitting.time_Polynomial2D_LinearLSQFitter"": ""def time_Polynomial2D_LinearLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        z = z_base + np.random.normal(0.0, 0.2, z_base.shape)\n        fit_LinearLSQFitter(Polynomial2D, x_grid, y_grid, z)\n    except Warning:\n        pass"", ""modeling.fitting.time_init_LinearLSQFitter"": ""def time_init_LinearLSQFitter():\n    fitting.LinearLSQFitter()"", ""modeling.fitting.time_init_SimplexLSQFitter"": ""def time_init_SimplexLSQFitter():\n    fitting.SimplexLSQFitter()"", ""modeling.fitting.time_large_gauss_combined_2d_LevMarLSQFitter"": ""def time_large_gauss_combined_2d_LevMarLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        z = z_base + np.random.normal(0.0, 0.2, z_base.shape)\n        fit_LevMarLSQFitter(large_gauss_combined_2d, x_grid, y_grid, z)\n    except Warning:\n        pass"", ""modeling.fitting.time_physical_model_with_units_LevMarLSQFitter"": ""def time_physical_model_with_units_LevMarLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        fit_LevMarLSQFitter(black_body, wav, fnu)\n    except Warning:\n        pass"", ""modeling.fitting.time_uncertanty_Linear1D_LinearLSQFitter"": ""def time_uncertanty_Linear1D_LinearLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        fit_LinearLSQFitter(\n            line_init, x_points, y_points, weights=1.0 / y_unc\n        )\n    except Warning:\n        pass""}",modeling.fitting,time,0.03378557258932704
21922,class-level,"terminus-2,claude",0.9397125921072624,0.931172319629449,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.AastexFloat,io_ascii.main,"{""io_ascii.main.AastexFloat.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.AastexFloat,time,0.006039796660405527
21926,class-level,"terminus-2,claude",1.0144500860306134,1.0210058620492353,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.TimeTable,table,"{""table.TimeTable.time_copy_table"": ""class TimeTable:\n    def time_copy_table(self):\n        self.table.copy()\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6"", ""table.TimeTable.time_read_rows"": ""class TimeTable:\n    def time_read_rows(self):\n        for row in self.table:\n            tuple(row)\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6"", ""table.TimeTable.time_sort"": ""class TimeTable:\n    def time_sort(self):\n        self.table.sort(\""a\"")\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6"", ""table.TimeTable.time_table_slice_bool"": ""class TimeTable:\n    def time_table_slice_bool(self):\n        self.table[self.bool_mask]\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6""}",table.TimeTable,time,-0.004636333817978709
21915,func-level,"terminus-2,oracle",0.7443338819800812,0.7443338819800812,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,linear_model.time_predict,linear_model,"{""linear_model.SGDRegressorBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SGDRegressorBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",linear_model.time_predict,time,0.0
21917,func-level,"terminus-2,oracle",0.940096240203932,0.940096240203932,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,neighbors.time_fit,neighbors,"{""neighbors.KNeighborsClassifierBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KNeighborsClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",neighbors.time_fit,time,0.0
21912,func-level,"terminus-2,oracle",1.0658852010494393,1.0658852010494393,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,ensemble.time_fit,ensemble,"{""ensemble.HistGradientBoostingClassifierBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass HistGradientBoostingClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""ensemble.RandomForestClassifierBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RandomForestClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",ensemble.time_fit,time,0.0
21919,class-level,"terminus-2,claude",0.9717202118900872,0.9589327323279948,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,convolve.ConvolveFFT,convolve,"{""convolve.ConvolveFFT.time_convolve_fft"": ""class ConvolveFFT:\n    def time_convolve_fft(self, ndim, size, boundary, nan_treatment):\n        convolve_fft(\n            self.array, self.kernel, boundary=boundary, nan_treatment=nan_treatment\n        )\n\n    def setup(self, ndim, size, boundary, nan_treatment):\n        np.random.seed(12345)\n    \n        self.kernel = np.random.random(kernel_shapes[ndim][size])\n        self.array = np.random.random(array_shapes[ndim][size])""}",convolve.ConvolveFFT,time,0.009043479181111997
21929,class-level,"terminus-2,claude",24.791590696445784,1.0080852034220826,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,units.TimeQuantityOpLargeArray,units,"{""units.TimeQuantityOpLargeArray.time_quantity_np_square"": ""class TimeQuantityOpSmallArray:\n    def time_quantity_np_square(self):\n        np.power(self.data, 2)\n\nclass TimeQuantityOpLargeArray:\n    def setup(self):\n        data = np.arange(1e6) + 1\n        self.data = data * u.g\n        self.out_sq = data * u.g**2\n        self.out_sqrt = data * u.g**0.5"", ""units.TimeQuantityOpLargeArray.time_quantity_np_square_out"": ""class TimeQuantityOpSmallArray:\n    def time_quantity_np_square_out(self):\n        np.power(self.data, 2, out=self.out_sq)\n\nclass TimeQuantityOpLargeArray:\n    def setup(self):\n        data = np.arange(1e6) + 1\n        self.data = data * u.g\n        self.out_sq = data * u.g**2\n        self.out_sqrt = data * u.g**0.5""}",units.TimeQuantityOpLargeArray,time,16.82001802901252
21921,class-level,"terminus-2,claude",1.0236377642108565,1.040693761082678,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.core.CoreSuite,io_ascii.core,"{""io_ascii.core.CoreSuite.time_continuation_inputter"": ""class CoreSuite:\n    def time_continuation_inputter(self):\n        core.ContinuationLinesInputter().process_lines(self.lines)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]"", ""io_ascii.core.CoreSuite.time_default_splitter_call"": ""class CoreSuite:\n    def time_default_splitter_call(self):\n        core.DefaultSplitter()(self.csv_line)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]""}",io_ascii.core.CoreSuite,time,-0.012062232582617806
21930,class-level,"terminus-2,claude",0.9603964822858276,1.0436571240682462,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,units.TimeQuantityOpSmallArrayDiffUnit,units,"{""units.TimeQuantityOpSmallArrayDiffUnit.time_quantity_add"": ""class TimeQuantityOpSmallArrayDiffUnit:\n    def time_quantity_add(self):\n        # Same as operator.add\n        self.data + self.data2\n\n    def setup(self):\n        data = np.array([1.0, 2.0, 3.0])\n        self.data = data * u.g\n    \n        # A different but dimensionally compatible unit\n        self.data2 = 0.001 * data * u.kg"", ""units.TimeQuantityOpSmallArrayDiffUnit.time_quantity_mul"": ""class TimeQuantityOpSmallArrayDiffUnit:\n    def time_quantity_mul(self):\n        # Same as operator.mul\n        self.data * self.data2\n\n    def setup(self):\n        data = np.array([1.0, 2.0, 3.0])\n        self.data = data * u.g\n    \n        # A different but dimensionally compatible unit\n        self.data2 = 0.001 * data * u.kg"", ""units.TimeQuantityOpSmallArrayDiffUnit.time_quantity_np_multiply"": ""class TimeQuantityOpSmallArrayDiffUnit:\n    def time_quantity_np_multiply(self):\n        np.multiply(self.data, self.data2)\n\n    def setup(self):\n        data = np.array([1.0, 2.0, 3.0])\n        self.data = data * u.g\n    \n        # A different but dimensionally compatible unit\n        self.data2 = 0.001 * data * u.kg"", ""units.TimeQuantityOpSmallArrayDiffUnit.time_quantity_np_truediv"": ""class TimeQuantityOpSmallArrayDiffUnit:\n    def time_quantity_np_truediv(self):\n        np.true_divide(self.data, self.data2)\n\n    def setup(self):\n        data = np.array([1.0, 2.0, 3.0])\n        self.data = data * u.g\n    \n        # A different but dimensionally compatible unit\n        self.data2 = 0.001 * data * u.kg""}",units.TimeQuantityOpSmallArrayDiffUnit,time,-0.05888305642320972
21923,class-level,"terminus-2,claude",1.0764251935311682,1.0677605090251543,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.SextractorString,io_ascii.main,"{""io_ascii.main.SextractorString.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.SextractorString,time,0.006127782536077706
21927,class-level,"terminus-2,claude",1.0812914844911257,1.070279171332443,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.TimeTableInitWithLists,table,"{""table.TimeTableInitWithLists.time_init_lists"": ""class TimeTableInitWithLists:\n    def time_init_lists(self):\n        Table([self.dat, self.dat, self.dat], names=[\""time\"", \""rate\"", \""error\""])\n\n    def setup(self):\n        self.dat = list(range(100_000))""}",table.TimeTableInitWithLists,time,0.007788057396522322
21932,class-level,"terminus-2,claude",0.9798645361369982,0.9773346618089332,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,wcs.WCSTransformations,wcs,"{""wcs.WCSTransformations.time_ape14_world_to_pixel"": ""class WCSTransformations:\n    def time_ape14_world_to_pixel(self, size):\n        wcs.world_to_pixel(self.coord)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)"", ""wcs.WCSTransformations.time_pix2world_x_y_0"": ""class WCSTransformations:\n    def time_pix2world_x_y_0(self, size):\n        wcs.wcs_pix2world(self.px, self.py, 0)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)"", ""wcs.WCSTransformations.time_pix2world_x_y_1"": ""class WCSTransformations:\n    def time_pix2world_x_y_1(self, size):\n        wcs.wcs_pix2world(self.px, self.py, 1)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)"", ""wcs.WCSTransformations.time_world2pix_x_y_0"": ""class WCSTransformations:\n    def time_world2pix_x_y_0(self, size):\n        wcs.wcs_world2pix(self.wx, self.wy, 0)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)"", ""wcs.WCSTransformations.time_world2pix_x_y_1"": ""class WCSTransformations:\n    def time_world2pix_x_y_1(self, size):\n        wcs.wcs_world2pix(self.wx, self.wy, 1)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)""}",wcs.WCSTransformations,time,0.0017891614767078868
21928,class-level,"terminus-2,claude",1.02875167470587,1.0435484362787395,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.TimeTableInitWithMultiDimLists,table,"{""table.TimeTableInitWithMultiDimLists.time_init_int_1d"": ""class TimeTableInitWithMultiDimLists:\n    def time_init_int_1d(self):\n        Table([self.data_int_1d])\n\n    def setup(self):\n        np_data_int = np.arange(1_000_000, dtype=np.int64)\n        np_data_float = np_data_int.astype(np.float64)\n        np_data_str = np_data_int.astype(\""U\"")\n    \n        self.data_int_1d = np_data_int.tolist()\n    \n        self.data_int_3d = np_data_int.reshape(1000, 100, 10).tolist()\n    \n        self.data_int_masked_1d = self.data_int_1d.copy()\n        self.data_int_masked_1d[-1] = np.ma.masked\n    \n        self.data_int_masked_3d = self.data_int_3d.copy()\n        self.data_int_masked_3d[-1][-1][-1] = np.ma.masked\n    \n        self.data_float_1d = np_data_float.tolist()\n    \n        self.data_str_1d = np_data_str.tolist()\n    \n        self.data_str_masked_1d = self.data_str_1d.copy()\n        self.data_str_masked_1d[-1] = np.ma.masked"", ""table.TimeTableInitWithMultiDimLists.time_init_int_3d"": ""class TimeTableInitWithMultiDimLists:\n    def time_init_int_3d(self):\n        Table([self.data_int_3d])\n\n    def setup(self):\n        np_data_int = np.arange(1_000_000, dtype=np.int64)\n        np_data_float = np_data_int.astype(np.float64)\n        np_data_str = np_data_int.astype(\""U\"")\n    \n        self.data_int_1d = np_data_int.tolist()\n    \n        self.data_int_3d = np_data_int.reshape(1000, 100, 10).tolist()\n    \n        self.data_int_masked_1d = self.data_int_1d.copy()\n        self.data_int_masked_1d[-1] = np.ma.masked\n    \n        self.data_int_masked_3d = self.data_int_3d.copy()\n        self.data_int_masked_3d[-1][-1][-1] = np.ma.masked\n    \n        self.data_float_1d = np_data_float.tolist()\n    \n        self.data_str_1d = np_data_str.tolist()\n    \n        self.data_str_masked_1d = self.data_str_1d.copy()\n        self.data_str_masked_1d[-1] = np.ma.masked"", ""table.TimeTableInitWithMultiDimLists.time_init_int_masked_1d"": ""class TimeTableInitWithMultiDimLists:\n    def time_init_int_masked_1d(self):\n        Table([self.data_int_masked_1d])\n\n    def setup(self):\n        np_data_int = np.arange(1_000_000, dtype=np.int64)\n        np_data_float = np_data_int.astype(np.float64)\n        np_data_str = np_data_int.astype(\""U\"")\n    \n        self.data_int_1d = np_data_int.tolist()\n    \n        self.data_int_3d = np_data_int.reshape(1000, 100, 10).tolist()\n    \n        self.data_int_masked_1d = self.data_int_1d.copy()\n        self.data_int_masked_1d[-1] = np.ma.masked\n    \n        self.data_int_masked_3d = self.data_int_3d.copy()\n        self.data_int_masked_3d[-1][-1][-1] = np.ma.masked\n    \n        self.data_float_1d = np_data_float.tolist()\n    \n        self.data_str_1d = np_data_str.tolist()\n    \n        self.data_str_masked_1d = self.data_str_1d.copy()\n        self.data_str_masked_1d[-1] = np.ma.masked"", ""table.TimeTableInitWithMultiDimLists.time_init_int_masked_3d"": ""class TimeTableInitWithMultiDimLists:\n    def time_init_int_masked_3d(self):\n        Table([self.data_int_masked_3d])\n\n    def setup(self):\n        np_data_int = np.arange(1_000_000, dtype=np.int64)\n        np_data_float = np_data_int.astype(np.float64)\n        np_data_str = np_data_int.astype(\""U\"")\n    \n        self.data_int_1d = np_data_int.tolist()\n    \n        self.data_int_3d = np_data_int.reshape(1000, 100, 10).tolist()\n    \n        self.data_int_masked_1d = self.data_int_1d.copy()\n        self.data_int_masked_1d[-1] = np.ma.masked\n    \n        self.data_int_masked_3d = self.data_int_3d.copy()\n        self.data_int_masked_3d[-1][-1][-1] = np.ma.masked\n    \n        self.data_float_1d = np_data_float.tolist()\n    \n        self.data_str_1d = np_data_str.tolist()\n    \n        self.data_str_masked_1d = self.data_str_1d.copy()\n        self.data_str_masked_1d[-1] = np.ma.masked""}",table.TimeTableInitWithMultiDimLists,time,-0.010464470702170768
21925,class-level,"terminus-2,claude",1.0502744755235731,1.0187313338888078,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,stats.sigma_clipping.SigmaClipBenchmarks,stats.sigma_clipping,"{""stats.sigma_clipping.SigmaClipBenchmarks.time_2d_array"": ""class SigmaClipBenchmarks:\n    def time_2d_array(self):\n        self.sigclip(self.data[0])\n\n    def setup(self):\n        # Avoid top-level module import to make sure that the benchmarks are\n        # compatible with versions of astropy that did not have this functionality.\n        from astropy.stats import SigmaClip\n    \n        size = (4, 2048, 2048)\n    \n        with NumpyRNGContext(12345):\n            self.data = np.random.normal(size=size)\n    \n            # add outliers\n            nbad = 100000\n            zbad = np.random.randint(low=0, high=size[0] - 1, size=nbad)\n            ybad = np.random.randint(low=0, high=size[1] - 1, size=nbad)\n            xbad = np.random.randint(low=0, high=size[2] - 1, size=nbad)\n            self.data[zbad, ybad, xbad] = np.random.choice([-1, 1], size=nbad) * (\n                10 + np.random.rand(nbad)\n            )\n    \n            # The defaults use median as the cenfunc and standard\n            # deviation as the stdfunc.  The default iters is 5.\n            self.sigclip = SigmaClip(sigma=3)"", ""stats.sigma_clipping.SigmaClipBenchmarks.time_2d_array_axis"": ""class SigmaClipBenchmarks:\n    def time_2d_array_axis(self):\n        self.sigclip(self.data[0], axis=0)\n\n    def setup(self):\n        # Avoid top-level module import to make sure that the benchmarks are\n        # compatible with versions of astropy that did not have this functionality.\n        from astropy.stats import SigmaClip\n    \n        size = (4, 2048, 2048)\n    \n        with NumpyRNGContext(12345):\n            self.data = np.random.normal(size=size)\n    \n            # add outliers\n            nbad = 100000\n            zbad = np.random.randint(low=0, high=size[0] - 1, size=nbad)\n            ybad = np.random.randint(low=0, high=size[1] - 1, size=nbad)\n            xbad = np.random.randint(low=0, high=size[2] - 1, size=nbad)\n            self.data[zbad, ybad, xbad] = np.random.choice([-1, 1], size=nbad) * (\n                10 + np.random.rand(nbad)\n            )\n    \n            # The defaults use median as the cenfunc and standard\n            # deviation as the stdfunc.  The default iters is 5.\n            self.sigclip = SigmaClip(sigma=3)"", ""stats.sigma_clipping.SigmaClipBenchmarks.time_3d_array"": ""class SigmaClipBenchmarks:\n    def time_3d_array(self):\n        self.sigclip(self.data[:, :1024, :1024])\n\n    def setup(self):\n        # Avoid top-level module import to make sure that the benchmarks are\n        # compatible with versions of astropy that did not have this functionality.\n        from astropy.stats import SigmaClip\n    \n        size = (4, 2048, 2048)\n    \n        with NumpyRNGContext(12345):\n            self.data = np.random.normal(size=size)\n    \n            # add outliers\n            nbad = 100000\n            zbad = np.random.randint(low=0, high=size[0] - 1, size=nbad)\n            ybad = np.random.randint(low=0, high=size[1] - 1, size=nbad)\n            xbad = np.random.randint(low=0, high=size[2] - 1, size=nbad)\n            self.data[zbad, ybad, xbad] = np.random.choice([-1, 1], size=nbad) * (\n                10 + np.random.rand(nbad)\n            )\n    \n            # The defaults use median as the cenfunc and standard\n            # deviation as the stdfunc.  The default iters is 5.\n            self.sigclip = SigmaClip(sigma=3)"", ""stats.sigma_clipping.SigmaClipBenchmarks.time_3d_array_axis"": ""class SigmaClipBenchmarks:\n    def time_3d_array_axis(self):\n        self.sigclip(self.data, axis=0)\n\n    def setup(self):\n        # Avoid top-level module import to make sure that the benchmarks are\n        # compatible with versions of astropy that did not have this functionality.\n        from astropy.stats import SigmaClip\n    \n        size = (4, 2048, 2048)\n    \n        with NumpyRNGContext(12345):\n            self.data = np.random.normal(size=size)\n    \n            # add outliers\n            nbad = 100000\n            zbad = np.random.randint(low=0, high=size[0] - 1, size=nbad)\n            ybad = np.random.randint(low=0, high=size[1] - 1, size=nbad)\n            xbad = np.random.randint(low=0, high=size[2] - 1, size=nbad)\n            self.data[zbad, ybad, xbad] = np.random.choice([-1, 1], size=nbad) * (\n                10 + np.random.rand(nbad)\n            )\n    \n            # The defaults use median as the cenfunc and standard\n            # deviation as the stdfunc.  The default iters is 5.\n            self.sigclip = SigmaClip(sigma=3)"", ""stats.sigma_clipping.SigmaClipBenchmarks.time_3d_array_axis2"": ""class SigmaClipBenchmarks:\n    def time_3d_array_axis2(self):\n        self.sigclip(self.data, axis=(0, 1))\n\n    def setup(self):\n        # Avoid top-level module import to make sure that the benchmarks are\n        # compatible with versions of astropy that did not have this functionality.\n        from astropy.stats import SigmaClip\n    \n        size = (4, 2048, 2048)\n    \n        with NumpyRNGContext(12345):\n            self.data = np.random.normal(size=size)\n    \n            # add outliers\n            nbad = 100000\n            zbad = np.random.randint(low=0, high=size[0] - 1, size=nbad)\n            ybad = np.random.randint(low=0, high=size[1] - 1, size=nbad)\n            xbad = np.random.randint(low=0, high=size[2] - 1, size=nbad)\n            self.data[zbad, ybad, xbad] = np.random.choice([-1, 1], size=nbad) * (\n                10 + np.random.rand(nbad)\n            )\n    \n            # The defaults use median as the cenfunc and standard\n            # deviation as the stdfunc.  The default iters is 5.\n            self.sigclip = SigmaClip(sigma=3)""}",stats.sigma_clipping.SigmaClipBenchmarks,time,0.022307738072677052
21931,class-level,"terminus-2,claude",1.0441708892267163,1.043050322455329,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,units,units,"{""units.time_compose_complex"": ""def time_compose_complex():\n    # Composing a complex unit can be very inefficient\n    (u.kg / u.s**3 * u.au**2.5 / u.yr**0.5 / u.sr**2).compose()"", ""units.time_quantity_view"": ""def time_quantity_view():\n    q1.view(u.Quantity)""}",units,time,0.0007924800363418586
21937,class-level,"terminus-2,gpt-5",1.048329577037415,1.092478111708183,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.CommentedHeaderInt,io_ascii.main,"{""io_ascii.main.CommentedHeaderInt.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.CommentedHeaderInt,time,-0.03122244319007635
21938,class-level,"terminus-2,gpt-5",1.0387164237717323,1.0141282493958388,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.NoHeaderInt,io_ascii.main,"{""io_ascii.main.NoHeaderInt.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.NoHeaderInt,time,0.017389090789175075
21935,class-level,"terminus-2,gpt-5",1.0235888675616638,1.027145428415974,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.core.CoreSuite,io_ascii.core,"{""io_ascii.core.CoreSuite.time_convert_vals"": ""class CoreSuite:\n    def time_convert_vals(self):\n        core.TableOutputter()._convert_vals(self.cols)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]"", ""io_ascii.core.CoreSuite.time_default_splitter_call"": ""class CoreSuite:\n    def time_default_splitter_call(self):\n        core.DefaultSplitter()(self.csv_line)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]"", ""io_ascii.core.CoreSuite.time_whitespace_splitter"": ""class CoreSuite:\n    def time_whitespace_splitter(self):\n        core.WhitespaceSplitter().process_line(self.line)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]""}",io_ascii.core.CoreSuite,time,-0.002515248128932263
21939,class-level,"terminus-2,gpt-5",1.025643282440689,1.0248349391596268,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.RdbInt,io_ascii.main,"{""io_ascii.main.RdbInt.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.RdbInt.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.RdbInt,time,0.000571671344456975
21936,class-level,"terminus-2,gpt-5",1.0205798553151009,1.030803669803022,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.CommentedHeaderFloat,io_ascii.main,"{""io_ascii.main.CommentedHeaderFloat.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.CommentedHeaderFloat,time,-0.007230420429930128
21940,class-level,"terminus-2,gpt-5",1.084152241692071,1.0677605090251543,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.SextractorString,io_ascii.main,"{""io_ascii.main.SextractorString.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.SextractorString,time,0.011592455917197127
21933,class-level,"terminus-2,gpt-5",0.9929863139631648,0.9591490792274736,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,convolve.ConvolveFFT,convolve,"{""convolve.ConvolveFFT.time_convolve_fft"": ""class ConvolveFFT:\n    def time_convolve_fft(self, ndim, size, boundary, nan_treatment):\n        convolve_fft(\n            self.array, self.kernel, boundary=boundary, nan_treatment=nan_treatment\n        )\n\n    def setup(self, ndim, size, boundary, nan_treatment):\n        np.random.seed(12345)\n    \n        self.kernel = np.random.random(kernel_shapes[ndim][size])\n        self.array = np.random.random(array_shapes[ndim][size])""}",convolve.ConvolveFFT,time,0.023930151863996624
21947,class-level,"terminus-2,oracle",1.0071536445507023,1.0071536445507023,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,convolve.Convolve,convolve,"{""convolve.Convolve.time_convolve"": ""class Convolve:\n    def time_convolve(self, ndim, size, boundary, nan_treatment):\n        convolve(\n            self.array, self.kernel, boundary=boundary, nan_treatment=nan_treatment\n        )\n\n    def setup(self, ndim, size, boundary, nan_treatment):\n        np.random.seed(12345)\n    \n        self.kernel = np.random.random(kernel_shapes[ndim][size])\n        self.array = np.random.random(array_shapes[ndim][size])""}",convolve.Convolve,time,0.0
21942,class-level,"terminus-2,gpt-5",1.036180212091713,1.009294393676725,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,modeling.fitting,modeling.fitting,"{""modeling.fitting.time_Chebyshev1D_LevMarLSQFitter"": ""def time_Chebyshev1D_LevMarLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        y = y_base + np.random.normal(0.0, 0.2, y_base.shape)\n        fit_LevMarLSQFitter(Chebyshev1D, x, y)\n    except Warning:\n        pass"", ""modeling.fitting.time_Chebyshev2D_LinearLSQFitter"": ""def time_Chebyshev2D_LinearLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        z = z_base + np.random.normal(0.0, 0.2, z_base.shape)\n        fit_LinearLSQFitter(Chebyshev2D, x_grid, y_grid, z)\n    except Warning:\n        pass"", ""modeling.fitting.time_Gaussian2D_LevMarLSQFitter"": ""def time_Gaussian2D_LevMarLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        z = z_base + np.random.normal(0.0, 0.2, z_base.shape)\n        fit_LevMarLSQFitter(Gaussian2D, x_grid, y_grid, z)\n    except Warning:\n        pass"", ""modeling.fitting.time_Gaussian2D_SimplexLSQFitter"": ""def time_Gaussian2D_SimplexLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        z = z_base + np.random.normal(0.0, 0.2, z_base.shape)\n        fit_SimplexLSQFitter(Gaussian2D, x_grid, y_grid, z)\n    except Warning:\n        pass"", ""modeling.fitting.time_Polynomial2D_LinearLSQFitter"": ""def time_Polynomial2D_LinearLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        z = z_base + np.random.normal(0.0, 0.2, z_base.shape)\n        fit_LinearLSQFitter(Polynomial2D, x_grid, y_grid, z)\n    except Warning:\n        pass"", ""modeling.fitting.time_init_LevMarLSQFitter"": ""def time_init_LevMarLSQFitter():\n    fitting.LevMarLSQFitter()"", ""modeling.fitting.time_init_SimplexLSQFitter"": ""def time_init_SimplexLSQFitter():\n    fitting.SimplexLSQFitter()"", ""modeling.fitting.time_large_gauss_combined_1d_LevMarLSQFitter"": ""def time_large_gauss_combined_1d_LevMarLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        y = y_base + np.random.normal(0.0, 0.2, y_base.shape)\n        fit_LevMarLSQFitter(large_gauss_combined_1d, x, y)\n    except Warning:\n        pass"", ""modeling.fitting.time_large_gauss_combined_2d_LevMarLSQFitter"": ""def time_large_gauss_combined_2d_LevMarLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        z = z_base + np.random.normal(0.0, 0.2, z_base.shape)\n        fit_LevMarLSQFitter(large_gauss_combined_2d, x_grid, y_grid, z)\n    except Warning:\n        pass""}",modeling.fitting,time,0.019014015852183912
21944,class-level,"terminus-2,gpt-5",1.0761991296122446,1.070279171332443,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,table.TimeTableInitWithLists,table,"{""table.TimeTableInitWithLists.time_init_lists"": ""class TimeTableInitWithLists:\n    def time_init_lists(self):\n        Table([self.dat, self.dat, self.dat], names=[\""time\"", \""rate\"", \""error\""])\n\n    def setup(self):\n        self.dat = list(range(100_000))""}",table.TimeTableInitWithLists,time,0.004186674879633323
21941,class-level,"terminus-2,gpt-5",1.065910977414392,1.055795161606203,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.TabInt,io_ascii.main,"{""io_ascii.main.TabInt.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.TabInt,time,0.007154042297163437
21949,class-level,"terminus-2,oracle",0.9436728904153254,0.9436728904153254,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,coordinates.FrameBenchmarks,coordinates,"{""coordinates.FrameBenchmarks.time_concatenate_array"": ""class FrameBenchmarks:\n    def time_concatenate_array(self):\n        concatenate((self.icrs_array, self.icrs_array))\n\n    def setup(self):\n        self.scalar_ra = 3.2 * u.deg\n        self.scalar_dec = 2.2 * u.deg\n    \n        self.scalar_pmra = 3.2 * u.mas / u.yr\n        self.scalar_pmdec = 2.2 * u.mas / u.yr\n    \n        self.array_ra = np.linspace(0.0, 360.0, 1000) * u.deg\n        self.array_dec = np.linspace(-90.0, 90.0, 1000) * u.deg\n    \n        np.random.seed(12345)\n        self.icrs_scalar = ICRS(ra=1 * u.deg, dec=2 * u.deg)\n        self.icrs_array = ICRS(\n            ra=np.random.random(10000) * u.deg, dec=np.random.random(10000) * u.deg\n        )\n    \n        # Some points to use for benchmarking coordinate matching.\n        # These were motivated by some tests done in astropy/astropy#7324:\n        # https://github.com/astropy/astropy/pull/7324#issuecomment-392382719\n        xyz_uniform1 = rnd.uniform(size=(3, 10000)) * u.kpc\n        xyz_uniform2 = rnd.uniform(size=(3, 10000)) * u.kpc\n        self.icrs_uniform1 = ICRS(\n            xyz_uniform1, representation_type=CartesianRepresentation\n        )\n        self.icrs_uniform2 = ICRS(\n            xyz_uniform2, representation_type=CartesianRepresentation\n        )\n    \n        phi = rnd.uniform(0, 2 * np.pi, size=10000)\n        theta = np.arccos(2 * rnd.uniform(size=10000) - 1)\n        xyz_uniform_sph1 = (\n            np.vstack(\n                (\n                    np.cos(phi) * np.sin(theta),\n                    np.sin(phi) * np.sin(theta),\n                    np.cos(theta),\n                )\n            )\n            * u.kpc\n        )\n    \n        phi = rnd.uniform(0, 2 * np.pi, size=10000)\n        theta = np.arccos(2 * rnd.uniform(size=10000) - 1)\n        xyz_uniform_sph2 = (\n            np.vstack(\n                (\n                    np.cos(phi) * np.sin(theta),\n                    np.sin(phi) * np.sin(theta),\n                    np.cos(theta),\n                )\n            )\n            * u.kpc\n        )\n        self.icrs_uniform_sph1 = ICRS(\n            xyz_uniform_sph1, representation_type=CartesianRepresentation\n        )\n        self.icrs_uniform_sph2 = ICRS(\n            xyz_uniform_sph2, representation_type=CartesianRepresentation\n        )\n    \n        xyz0 = rnd.uniform(-100, 100, size=(8, 3))\n        xyz_clustered1 = np.vstack(rnd.normal(xyz0, size=(10000, 8, 3))).T * u.kpc\n        xyz_clustered2 = np.vstack(rnd.normal(xyz0, size=(10000, 8, 3))).T * u.kpc\n        self.icrs_clustered1 = ICRS(\n            xyz_clustered1, representation_type=CartesianRepresentation\n        )\n        self.icrs_clustered2 = ICRS(\n            xyz_clustered2, representation_type=CartesianRepresentation\n        )""}",coordinates.FrameBenchmarks,time,0.0
21951,class-level,"terminus-2,oracle",0.9761447922304972,0.9761447922304972,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports,imports,"{""imports.timeraw_import_astropy_io"": ""def timeraw_import_astropy_io():\n    return \""\""\""\n    from astropy import io\n    \""\""\"""", ""imports.timeraw_import_astropy_io_fits"": ""def timeraw_import_astropy_io_fits():\n    return \""\""\""\n    from astropy.io import fits\n    \""\""\"""", ""imports.timeraw_import_astropy_io_misc"": ""def timeraw_import_astropy_io_misc():\n    return \""\""\""\n    from astropy.io import misc\n    \""\""\"""", ""imports.timeraw_import_astropy_io_misc_hdf5"": ""def timeraw_import_astropy_io_misc_hdf5():\n    return \""\""\""\n    from astropy.io.misc import hdf5\n    \""\""\"""", ""imports.timeraw_import_astropy_table"": ""def timeraw_import_astropy_table():\n    return \""\""\""\n    from astropy import table\n    \""\""\"""", ""imports.timeraw_import_astropy_time"": ""def timeraw_import_astropy_time():\n    return \""\""\""\n    from astropy import time\n    \""\""\"""", ""imports.timeraw_import_astropy_timeseries"": ""def timeraw_import_astropy_timeseries():\n    return \""\""\""\n    from astropy import timeseries\n    \""\""\"""", ""imports.timeraw_import_astropy_timeseries_io"": ""def timeraw_import_astropy_timeseries_io():\n    return \""\""\""\n    from astropy.timeseries import io\n    \""\""\"""", ""imports.timeraw_import_astropy_timeseries_periodograms"": ""def timeraw_import_astropy_timeseries_periodograms():\n    return \""\""\""\n    from astropy.timeseries import periodograms\n    \""\""\"""", ""imports.timeraw_import_astropy_units"": ""def timeraw_import_astropy_units():\n    return \""\""\""\n    from astropy import units\n    \""\""\"""", ""imports.timeraw_import_astropy_units_quantity"": ""def timeraw_import_astropy_units_quantity():\n    return \""\""\""\n    from astropy.units import quantity\n    \""\""\"""", ""imports.timeraw_import_astropy_utils"": ""def timeraw_import_astropy_utils():\n    return \""\""\""\n    from astropy import utils\n    \""\""\"""", ""imports.timeraw_import_astropy_utils_iers"": ""def timeraw_import_astropy_utils_iers():\n    return \""\""\""\n    from astropy.utils import iers\n    \""\""\"""", ""imports.timeraw_import_astropy_visualization"": ""def timeraw_import_astropy_visualization():\n    return \""\""\""\n    from astropy import visualization\n    \""\""\"""", ""imports.timeraw_import_astropy_visualization_wcsaxes"": ""def timeraw_import_astropy_visualization_wcsaxes():\n    return \""\""\""\n    from astropy.visualization import wcsaxes\n    \""\""\"""", ""imports.timeraw_import_astropy_wcs"": ""def timeraw_import_astropy_wcs():\n    return \""\""\""\n    from astropy import wcs\n    \""\""\"""", ""imports.timeraw_import_astropy_wcs_wcsapi"": ""def timeraw_import_astropy_wcs_wcsapi():\n    return \""\""\""\n    from astropy.wcs import wcsapi\n    \""\""\""""}",imports,time,0.0
21948,class-level,"terminus-2,oracle",0.997006747390139,0.997006747390139,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,convolve.ConvolveFFT,convolve,"{""convolve.ConvolveFFT.time_convolve_fft"": ""class ConvolveFFT:\n    def time_convolve_fft(self, ndim, size, boundary, nan_treatment):\n        convolve_fft(\n            self.array, self.kernel, boundary=boundary, nan_treatment=nan_treatment\n        )\n\n    def setup(self, ndim, size, boundary, nan_treatment):\n        np.random.seed(12345)\n    \n        self.kernel = np.random.random(kernel_shapes[ndim][size])\n        self.array = np.random.random(array_shapes[ndim][size])""}",convolve.ConvolveFFT,time,0.0
21954,class-level,"terminus-2,oracle",1.0268629023348834,1.0268629023348834,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.CommentedHeaderFloat,io_ascii.main,"{""io_ascii.main.CommentedHeaderFloat.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.CommentedHeaderFloat.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.CommentedHeaderFloat,time,0.0
21950,class-level,"terminus-2,oracle",1.093442182763926,1.093442182763926,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,coordinates.SkyCoordBenchmarks,coordinates,"{""coordinates.SkyCoordBenchmarks.time_iter_array"": ""class SkyCoordBenchmarks:\n    def time_iter_array(self):\n        for c in self.coord_array_1e3:\n            pass\n\n    def setup(self):\n        self.coord_scalar = SkyCoord(1, 2, unit=\""deg\"", frame=\""icrs\"")\n    \n        lon, lat = np.ones((2, 1000))\n        self.coord_array_1e3 = SkyCoord(lon, lat, unit=\""deg\"", frame=\""icrs\"")\n    \n        self.lon_1e6, self.lat_1e6 = np.ones((2, int(1e6)))\n        self.coord_array_1e6 = SkyCoord(\n            self.lon_1e6, self.lat_1e6, unit=\""deg\"", frame=\""icrs\""\n        )\n    \n        self.scalar_q_ra = 1 * u.deg\n        self.scalar_q_dec = 2 * u.deg\n    \n        np.random.seed(12345)\n        self.array_q_ra = np.random.rand(int(1e6)) * 360 * u.deg\n        self.array_q_dec = (np.random.rand(int(1e6)) * 180 - 90) * u.deg\n    \n        self.scalar_repr = UnitSphericalRepresentation(\n            lat=self.scalar_q_dec, lon=self.scalar_q_ra\n        )\n        self.array_repr = UnitSphericalRepresentation(\n            lat=self.array_q_dec, lon=self.array_q_ra\n        )""}",coordinates.SkyCoordBenchmarks,time,0.0
21956,class-level,"terminus-2,oracle",1.0390523480434497,1.0390523480434497,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.NoHeaderInt,io_ascii.main,"{""io_ascii.main.NoHeaderInt.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.NoHeaderInt.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.NoHeaderInt,time,0.0
21952,class-level,"terminus-2,oracle",1.0273108897764325,1.0273108897764325,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.core.CoreSuite,io_ascii.core,"{""io_ascii.core.CoreSuite.time_continuation_inputter"": ""class CoreSuite:\n    def time_continuation_inputter(self):\n        core.ContinuationLinesInputter().process_lines(self.lines)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]"", ""io_ascii.core.CoreSuite.time_convert_vals"": ""class CoreSuite:\n    def time_convert_vals(self):\n        core.TableOutputter()._convert_vals(self.cols)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]"", ""io_ascii.core.CoreSuite.time_default_splitter_call"": ""class CoreSuite:\n    def time_default_splitter_call(self):\n        core.DefaultSplitter()(self.csv_line)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]"", ""io_ascii.core.CoreSuite.time_whitespace_splitter"": ""class CoreSuite:\n    def time_whitespace_splitter(self):\n        core.WhitespaceSplitter().process_line(self.line)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]""}",io_ascii.core.CoreSuite,time,0.0
21953,class-level,"terminus-2,oracle",0.931172319629449,0.931172319629449,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.AastexFloat,io_ascii.main,"{""io_ascii.main.AastexFloat.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.AastexFloat,time,0.0
21934,class-level,"terminus-2,gpt-5",1.0804701481035563,1.0188527292566272,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,imports,imports,"{""imports.timeraw_import_astropy_constants"": ""def timeraw_import_astropy_constants():\n    return \""\""\""\n    from astropy import constants\n    \""\""\"""", ""imports.timeraw_import_astropy_coordinates"": ""def timeraw_import_astropy_coordinates():\n    return \""\""\""\n    from astropy import coordinates\n    \""\""\"""", ""imports.timeraw_import_astropy_cosmology"": ""def timeraw_import_astropy_cosmology():\n    return \""\""\""\n    from astropy import cosmology\n    \""\""\"""", ""imports.timeraw_import_astropy_io"": ""def timeraw_import_astropy_io():\n    return \""\""\""\n    from astropy import io\n    \""\""\"""", ""imports.timeraw_import_astropy_io_ascii"": ""def timeraw_import_astropy_io_ascii():\n    return \""\""\""\n    from astropy.io import ascii\n    \""\""\"""", ""imports.timeraw_import_astropy_io_fits"": ""def timeraw_import_astropy_io_fits():\n    return \""\""\""\n    from astropy.io import fits\n    \""\""\"""", ""imports.timeraw_import_astropy_io_misc"": ""def timeraw_import_astropy_io_misc():\n    return \""\""\""\n    from astropy.io import misc\n    \""\""\"""", ""imports.timeraw_import_astropy_io_misc_hdf5"": ""def timeraw_import_astropy_io_misc_hdf5():\n    return \""\""\""\n    from astropy.io.misc import hdf5\n    \""\""\"""", ""imports.timeraw_import_astropy_io_misc_pandas"": ""def timeraw_import_astropy_io_misc_pandas():\n    return \""\""\""\n    from astropy.io.misc import pandas\n    \""\""\"""", ""imports.timeraw_import_astropy_io_misc_yaml"": ""def timeraw_import_astropy_io_misc_yaml():\n    return \""\""\""\n    from astropy.io.misc import yaml\n    \""\""\"""", ""imports.timeraw_import_astropy_io_votable"": ""def timeraw_import_astropy_io_votable():\n    return \""\""\""\n    from astropy.io import votable\n    \""\""\"""", ""imports.timeraw_import_astropy_modeling"": ""def timeraw_import_astropy_modeling():\n    return \""\""\""\n    from astropy import modeling\n    \""\""\"""", ""imports.timeraw_import_astropy_nddata"": ""def timeraw_import_astropy_nddata():\n    return \""\""\""\n    from astropy import nddata\n    \""\""\"""", ""imports.timeraw_import_astropy_time"": ""def timeraw_import_astropy_time():\n    return \""\""\""\n    from astropy import time\n    \""\""\"""", ""imports.timeraw_import_astropy_timeseries"": ""def timeraw_import_astropy_timeseries():\n    return \""\""\""\n    from astropy import timeseries\n    \""\""\"""", ""imports.timeraw_import_astropy_timeseries_io"": ""def timeraw_import_astropy_timeseries_io():\n    return \""\""\""\n    from astropy.timeseries import io\n    \""\""\""""}",imports,time,0.043576675280713704
21961,class-level,"terminus-2,oracle",1.0322761218682588,1.0322761218682588,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_fits.FITSHeader,io_fits,"{""io_fits.FITSHeader.time_get_float"": ""class FITSHeader:\n    def time_get_float(self):\n        self.hdr.get(\""FLT999\"")\n\n    def setup(self):\n        self.hdr = make_header()\n        self.hdr_string = self.hdr.tostring()"", ""io_fits.FITSHeader.time_get_hierarch"": ""class FITSHeader:\n    def time_get_hierarch(self):\n        self.hdr.get(\""HIERARCH FOO BAR 999\"")\n\n    def setup(self):\n        self.hdr = make_header()\n        self.hdr_string = self.hdr.tostring()"", ""io_fits.FITSHeader.time_get_int"": ""class FITSHeader:\n    def time_get_int(self):\n        self.hdr.get(\""INT999\"")\n\n    def setup(self):\n        self.hdr = make_header()\n        self.hdr_string = self.hdr.tostring()""}",io_fits.FITSHeader,time,0.0
21960,class-level,"terminus-2,oracle",1.0145693146798205,1.0145693146798205,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.table.TableSuite,io_ascii.table,"{""io_ascii.table.TableSuite.time_str_vals_str"": ""class TableSuite:\n    def time_str_vals_str(self):\n        self.table_cols[2].iter_str_vals()\n\n    def setup(self):\n        self.lst = []\n        self.lst.append([random.randint(-500, 500) for i in range(1000)])\n        self.lst.append([random.random() * 500 - 500 for i in range(1000)])\n        self.lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, self.lst):\n            col.data = x\n        self.table_cols = [table.Column(x) for x in self.lst]\n        self.outputter = core.TableOutputter()\n        self.table = table.Table()""}",io_ascii.table.TableSuite,time,0.0
21955,class-level,"terminus-2,oracle",1.092478111708183,1.092478111708183,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.CommentedHeaderInt,io_ascii.main,"{""io_ascii.main.CommentedHeaderInt.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.CommentedHeaderInt,time,0.0
21962,class-level,"terminus-2,oracle",1.008718951331412,1.008718951331412,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,modeling.fitting,modeling.fitting,"{""modeling.fitting.time_Chebyshev1D_LevMarLSQFitter"": ""def time_Chebyshev1D_LevMarLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        y = y_base + np.random.normal(0.0, 0.2, y_base.shape)\n        fit_LevMarLSQFitter(Chebyshev1D, x, y)\n    except Warning:\n        pass"", ""modeling.fitting.time_init_LevMarLSQFitter"": ""def time_init_LevMarLSQFitter():\n    fitting.LevMarLSQFitter()"", ""modeling.fitting.time_init_LinearLSQFitter"": ""def time_init_LinearLSQFitter():\n    fitting.LinearLSQFitter()""}",modeling.fitting,time,0.0
21957,class-level,"terminus-2,oracle",1.0248349391596268,1.0248349391596268,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.RdbInt,io_ascii.main,"{""io_ascii.main.RdbInt.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.RdbInt,time,0.0
21958,class-level,"terminus-2,oracle",1.0677605090251543,1.0677605090251543,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.SextractorString,io_ascii.main,"{""io_ascii.main.SextractorString.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.SextractorString,time,0.0
21943,class-level,"terminus-2,gpt-5",0.9850053604045756,1.0166635229568928,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,modeling.model,modeling.model,"{""modeling.model.time_eval_gaussian_no_units_big"": ""def time_eval_gaussian_no_units_big():\n    gauss1d_no_units(x_no_units_big)"", ""modeling.model.time_eval_gaussian_with_units_small"": ""def time_eval_gaussian_with_units_small():\n    gauss1d_with_units(x_with_units_small)"", ""modeling.model.time_init_gaussian_with_units"": ""def time_init_gaussian_with_units():\n    models.Gaussian1D(amplitude=10 * u.Hz, mean=5 * u.m, stddev=1.2 * u.cm)""}",modeling.model,time,-0.022389082427381277
21946,class-level,"terminus-2,gpt-5",1.0082041248679634,0.991306196805949,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,wcs.WCSTransformations,wcs,"{""wcs.WCSTransformations.time_ape14_world_to_pixel"": ""class WCSTransformations:\n    def time_ape14_world_to_pixel(self, size):\n        wcs.world_to_pixel(self.coord)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)"", ""wcs.WCSTransformations.time_pix2world_x_y_0"": ""class WCSTransformations:\n    def time_pix2world_x_y_0(self, size):\n        wcs.wcs_pix2world(self.px, self.py, 0)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)"", ""wcs.WCSTransformations.time_world2pix_x_y_0"": ""class WCSTransformations:\n    def time_world2pix_x_y_0(self, size):\n        wcs.wcs_world2pix(self.wx, self.wy, 0)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)""}",wcs.WCSTransformations,time,0.011950444173984725
21959,class-level,"terminus-2,oracle",1.055795161606203,1.055795161606203,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main.TabInt,io_ascii.main,"{""io_ascii.main.TabInt.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main.TabInt,time,0.0
21967,class-level,"terminus-2,oracle",1.070279171332443,1.070279171332443,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.TimeTableInitWithLists,table,"{""table.TimeTableInitWithLists.time_init_lists"": ""class TimeTableInitWithLists:\n    def time_init_lists(self):\n        Table([self.dat, self.dat, self.dat], names=[\""time\"", \""rate\"", \""error\""])\n\n    def setup(self):\n        self.dat = list(range(100_000))""}",table.TimeTableInitWithLists,time,0.0
21968,class-level,"terminus-2,oracle",1.0435484362787395,1.0435484362787395,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.TimeTableInitWithMultiDimLists,table,"{""table.TimeTableInitWithMultiDimLists.time_init_int_1d"": ""class TimeTableInitWithMultiDimLists:\n    def time_init_int_1d(self):\n        Table([self.data_int_1d])\n\n    def setup(self):\n        np_data_int = np.arange(1_000_000, dtype=np.int64)\n        np_data_float = np_data_int.astype(np.float64)\n        np_data_str = np_data_int.astype(\""U\"")\n    \n        self.data_int_1d = np_data_int.tolist()\n    \n        self.data_int_3d = np_data_int.reshape(1000, 100, 10).tolist()\n    \n        self.data_int_masked_1d = self.data_int_1d.copy()\n        self.data_int_masked_1d[-1] = np.ma.masked\n    \n        self.data_int_masked_3d = self.data_int_3d.copy()\n        self.data_int_masked_3d[-1][-1][-1] = np.ma.masked\n    \n        self.data_float_1d = np_data_float.tolist()\n    \n        self.data_str_1d = np_data_str.tolist()\n    \n        self.data_str_masked_1d = self.data_str_1d.copy()\n        self.data_str_masked_1d[-1] = np.ma.masked"", ""table.TimeTableInitWithMultiDimLists.time_init_int_masked_3d"": ""class TimeTableInitWithMultiDimLists:\n    def time_init_int_masked_3d(self):\n        Table([self.data_int_masked_3d])\n\n    def setup(self):\n        np_data_int = np.arange(1_000_000, dtype=np.int64)\n        np_data_float = np_data_int.astype(np.float64)\n        np_data_str = np_data_int.astype(\""U\"")\n    \n        self.data_int_1d = np_data_int.tolist()\n    \n        self.data_int_3d = np_data_int.reshape(1000, 100, 10).tolist()\n    \n        self.data_int_masked_1d = self.data_int_1d.copy()\n        self.data_int_masked_1d[-1] = np.ma.masked\n    \n        self.data_int_masked_3d = self.data_int_3d.copy()\n        self.data_int_masked_3d[-1][-1][-1] = np.ma.masked\n    \n        self.data_float_1d = np_data_float.tolist()\n    \n        self.data_str_1d = np_data_str.tolist()\n    \n        self.data_str_masked_1d = self.data_str_1d.copy()\n        self.data_str_masked_1d[-1] = np.ma.masked""}",table.TimeTableInitWithMultiDimLists,time,0.0
21963,class-level,"terminus-2,oracle",1.0166635229568928,1.0166635229568928,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,modeling.model,modeling.model,"{""modeling.model.time_init_gaussian_with_units"": ""def time_init_gaussian_with_units():\n    models.Gaussian1D(amplitude=10 * u.Hz, mean=5 * u.m, stddev=1.2 * u.cm)""}",modeling.model,time,0.0
21969,class-level,"terminus-2,oracle",1.0080852034220826,1.0080852034220826,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,units.TimeQuantityOpLargeArray,units,"{""units.TimeQuantityOpLargeArray.time_quantity_np_square"": ""class TimeQuantityOpSmallArray:\n    def time_quantity_np_square(self):\n        np.power(self.data, 2)\n\nclass TimeQuantityOpLargeArray:\n    def setup(self):\n        data = np.arange(1e6) + 1\n        self.data = data * u.g\n        self.out_sq = data * u.g**2\n        self.out_sqrt = data * u.g**0.5""}",units.TimeQuantityOpLargeArray,time,0.0
21966,class-level,"terminus-2,oracle",0.9765592798655938,0.9765592798655938,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.TimeTable,table,"{""table.TimeTable.time_add_column"": ""class TimeTable:\n    def time_add_column(self):\n        self.table[\""e\""] = self.extra_column\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6"", ""table.TimeTable.time_column_slice_bool"": ""class TimeTable:\n    def time_column_slice_bool(self):\n        self.table[\""a\""][self.bool_mask]\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6"", ""table.TimeTable.time_hstack"": ""class TimeTable:\n    def time_hstack(self):\n        hstack([self.table, self.other_table_2])\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6"", ""table.TimeTable.time_init_from_np_array_copy"": ""class TimeTable:\n    def time_init_from_np_array_copy(self):\n        Table(self.np_table, copy=True)\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6"", ""table.TimeTable.time_item_get_rowfirst"": ""class TimeTable:\n    def time_item_get_rowfirst(self):\n        self.table[300][\""b\""]\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6"", ""table.TimeTable.time_read_rows"": ""class TimeTable:\n    def time_read_rows(self):\n        for row in self.table:\n            tuple(row)\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6""}",table.TimeTable,time,0.0
21945,class-level,"terminus-2,gpt-5",1.0158093477479655,1.0684349181619717,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,table.TimeTableInitWithMultiDimLists,table,"{""table.TimeTableInitWithMultiDimLists.time_init_float_1d"": ""class TimeTableInitWithMultiDimLists:\n    def time_init_float_1d(self):\n        Table([self.data_float_1d])\n\n    def setup(self):\n        np_data_int = np.arange(1_000_000, dtype=np.int64)\n        np_data_float = np_data_int.astype(np.float64)\n        np_data_str = np_data_int.astype(\""U\"")\n    \n        self.data_int_1d = np_data_int.tolist()\n    \n        self.data_int_3d = np_data_int.reshape(1000, 100, 10).tolist()\n    \n        self.data_int_masked_1d = self.data_int_1d.copy()\n        self.data_int_masked_1d[-1] = np.ma.masked\n    \n        self.data_int_masked_3d = self.data_int_3d.copy()\n        self.data_int_masked_3d[-1][-1][-1] = np.ma.masked\n    \n        self.data_float_1d = np_data_float.tolist()\n    \n        self.data_str_1d = np_data_str.tolist()\n    \n        self.data_str_masked_1d = self.data_str_1d.copy()\n        self.data_str_masked_1d[-1] = np.ma.masked"", ""table.TimeTableInitWithMultiDimLists.time_init_int_1d"": ""class TimeTableInitWithMultiDimLists:\n    def time_init_int_1d(self):\n        Table([self.data_int_1d])\n\n    def setup(self):\n        np_data_int = np.arange(1_000_000, dtype=np.int64)\n        np_data_float = np_data_int.astype(np.float64)\n        np_data_str = np_data_int.astype(\""U\"")\n    \n        self.data_int_1d = np_data_int.tolist()\n    \n        self.data_int_3d = np_data_int.reshape(1000, 100, 10).tolist()\n    \n        self.data_int_masked_1d = self.data_int_1d.copy()\n        self.data_int_masked_1d[-1] = np.ma.masked\n    \n        self.data_int_masked_3d = self.data_int_3d.copy()\n        self.data_int_masked_3d[-1][-1][-1] = np.ma.masked\n    \n        self.data_float_1d = np_data_float.tolist()\n    \n        self.data_str_1d = np_data_str.tolist()\n    \n        self.data_str_masked_1d = self.data_str_1d.copy()\n        self.data_str_masked_1d[-1] = np.ma.masked"", ""table.TimeTableInitWithMultiDimLists.time_init_int_masked_1d"": ""class TimeTableInitWithMultiDimLists:\n    def time_init_int_masked_1d(self):\n        Table([self.data_int_masked_1d])\n\n    def setup(self):\n        np_data_int = np.arange(1_000_000, dtype=np.int64)\n        np_data_float = np_data_int.astype(np.float64)\n        np_data_str = np_data_int.astype(\""U\"")\n    \n        self.data_int_1d = np_data_int.tolist()\n    \n        self.data_int_3d = np_data_int.reshape(1000, 100, 10).tolist()\n    \n        self.data_int_masked_1d = self.data_int_1d.copy()\n        self.data_int_masked_1d[-1] = np.ma.masked\n    \n        self.data_int_masked_3d = self.data_int_3d.copy()\n        self.data_int_masked_3d[-1][-1][-1] = np.ma.masked\n    \n        self.data_float_1d = np_data_float.tolist()\n    \n        self.data_str_1d = np_data_str.tolist()\n    \n        self.data_str_masked_1d = self.data_str_1d.copy()\n        self.data_str_masked_1d[-1] = np.ma.masked"", ""table.TimeTableInitWithMultiDimLists.time_init_str_masked_1d"": ""class TimeTableInitWithMultiDimLists:\n    def time_init_str_masked_1d(self):\n        Table([self.data_str_masked_1d])\n\n    def setup(self):\n        np_data_int = np.arange(1_000_000, dtype=np.int64)\n        np_data_float = np_data_int.astype(np.float64)\n        np_data_str = np_data_int.astype(\""U\"")\n    \n        self.data_int_1d = np_data_int.tolist()\n    \n        self.data_int_3d = np_data_int.reshape(1000, 100, 10).tolist()\n    \n        self.data_int_masked_1d = self.data_int_1d.copy()\n        self.data_int_masked_1d[-1] = np.ma.masked\n    \n        self.data_int_masked_3d = self.data_int_3d.copy()\n        self.data_int_masked_3d[-1][-1][-1] = np.ma.masked\n    \n        self.data_float_1d = np_data_float.tolist()\n    \n        self.data_str_1d = np_data_str.tolist()\n    \n        self.data_str_masked_1d = self.data_str_1d.copy()\n        self.data_str_masked_1d[-1] = np.ma.masked""}",table.TimeTableInitWithMultiDimLists,time,-0.03721751797313025
21964,class-level,"terminus-2,oracle",1.0187313338888078,1.0187313338888078,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,stats.sigma_clipping.SigmaClipBenchmarks,stats.sigma_clipping,"{""stats.sigma_clipping.SigmaClipBenchmarks.time_3d_array_axis2"": ""class SigmaClipBenchmarks:\n    def time_3d_array_axis2(self):\n        self.sigclip(self.data, axis=(0, 1))\n\n    def setup(self):\n        # Avoid top-level module import to make sure that the benchmarks are\n        # compatible with versions of astropy that did not have this functionality.\n        from astropy.stats import SigmaClip\n    \n        size = (4, 2048, 2048)\n    \n        with NumpyRNGContext(12345):\n            self.data = np.random.normal(size=size)\n    \n            # add outliers\n            nbad = 100000\n            zbad = np.random.randint(low=0, high=size[0] - 1, size=nbad)\n            ybad = np.random.randint(low=0, high=size[1] - 1, size=nbad)\n            xbad = np.random.randint(low=0, high=size[2] - 1, size=nbad)\n            self.data[zbad, ybad, xbad] = np.random.choice([-1, 1], size=nbad) * (\n                10 + np.random.rand(nbad)\n            )\n    \n            # The defaults use median as the cenfunc and standard\n            # deviation as the stdfunc.  The default iters is 5.\n            self.sigclip = SigmaClip(sigma=3)""}",stats.sigma_clipping.SigmaClipBenchmarks,time,0.0
21972,class-level,"terminus-2,oracle",1.0136094389155583,1.0136094389155583,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,votable.TimeVOTableSmallOverhead,votable,"{""votable.TimeVOTableSmallOverhead.time_small_binary2"": ""class TimeVOTableSmallOverhead:\n    def time_small_binary2(self):\n        parse(io.BytesIO(self.binary2_data))\n\n    def setup(self):\n        table = Table(\n            [\n                ra_data[:SMALL_SIZE],\n                dec_data[:SMALL_SIZE],\n                mag_data[:SMALL_SIZE]\n            ],\n            names=['ra', 'dec', 'mag']\n        )\n    \n        self.binary_data = create_votable_bytes(table, 'binary')\n        self.binary2_data = create_votable_bytes(table, 'binary2')""}",votable.TimeVOTableSmallOverhead,time,0.0
21965,class-level,"terminus-2,oracle",0.9314596873228048,0.9314596873228048,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table.TimeMaskedTable,table,"{""table.TimeMaskedTable.time_join_inner"": ""class TimeTable:\n    def time_join_inner(self):\n        join(self.table, self.other_table, keys=\""i\"", join_type=\""inner\"")\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6""}",table.TimeMaskedTable,time,0.0
21970,class-level,"terminus-2,oracle",1.0297649716499022,1.0297649716499022,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,units.TimeQuantityOpSmallArrayDiffUnit,units,"{""units.TimeQuantityOpSmallArrayDiffUnit.time_quantity_add"": ""class TimeQuantityOpSmallArrayDiffUnit:\n    def time_quantity_add(self):\n        # Same as operator.add\n        self.data + self.data2\n\n    def setup(self):\n        data = np.array([1.0, 2.0, 3.0])\n        self.data = data * u.g\n    \n        # A different but dimensionally compatible unit\n        self.data2 = 0.001 * data * u.kg"", ""units.TimeQuantityOpSmallArrayDiffUnit.time_quantity_np_add"": ""class TimeQuantityOpSmallArrayDiffUnit:\n    def time_quantity_np_add(self):\n        np.add(self.data, self.data2)\n\n    def setup(self):\n        data = np.array([1.0, 2.0, 3.0])\n        self.data = data * u.g\n    \n        # A different but dimensionally compatible unit\n        self.data2 = 0.001 * data * u.kg""}",units.TimeQuantityOpSmallArrayDiffUnit,time,0.0
21973,class-level,"terminus-2,oracle",0.9543181972536148,0.9543181972536148,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,wcs.WCSTransformations,wcs,"{""wcs.WCSTransformations.time_ape14_world_to_pixel"": ""class WCSTransformations:\n    def time_ape14_world_to_pixel(self, size):\n        wcs.world_to_pixel(self.coord)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)"", ""wcs.WCSTransformations.time_pix2world_x_y_0"": ""class WCSTransformations:\n    def time_pix2world_x_y_0(self, size):\n        wcs.wcs_pix2world(self.px, self.py, 0)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)"", ""wcs.WCSTransformations.time_pix2world_x_y_1"": ""class WCSTransformations:\n    def time_pix2world_x_y_1(self, size):\n        wcs.wcs_pix2world(self.px, self.py, 1)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)"", ""wcs.WCSTransformations.time_world2pix_x_y_0"": ""class WCSTransformations:\n    def time_world2pix_x_y_0(self, size):\n        wcs.wcs_world2pix(self.wx, self.wy, 0)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)"", ""wcs.WCSTransformations.time_world2pix_x_y_1"": ""class WCSTransformations:\n    def time_world2pix_x_y_1(self, size):\n        wcs.wcs_world2pix(self.wx, self.wy, 1)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)""}",wcs.WCSTransformations,time,0.0
21971,class-level,"terminus-2,oracle",1.0291105973160646,1.0291105973160646,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,units,units,"{""units.time_quantity_view"": ""def time_quantity_view():\n    q1.view(u.Quantity)"", ""units.time_unit_to"": ""def time_unit_to():\n    u.m.to(u.pc)""}",units,time,0.0
21904,func-level,"terminus-2,gpt-5",1.0349332922636336,1.513804722063677,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,cluster.time_transform,cluster,"{""cluster.KMeansBenchmark.time_transform"": ""class Transformer:\n    def time_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",cluster.time_transform,time,-0.3386643775106389
21908,func-level,"terminus-2,gpt-5",0.9714961330391112,0.9771958841499164,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,neighbors.time_predict,neighbors,"{""neighbors.KNeighborsClassifierBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KNeighborsClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",neighbors.time_predict,time,-0.004030941379635947
21905,func-level,"terminus-2,gpt-5",1.0374202159278103,1.024858002625557,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,ensemble.time_fit,ensemble,"{""ensemble.GradientBoostingClassifierBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass GradientBoostingClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""ensemble.RandomForestClassifierBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RandomForestClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",ensemble.time_fit,time,0.008884167823375798
21906,func-level,"terminus-2,gpt-5",2.336977261529984,3.093394649383032,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,ensemble.time_predict,ensemble,"{""ensemble.HistGradientBoostingClassifierBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass HistGradientBoostingClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",ensemble.time_predict,time,-0.5349486477037116
21907,func-level,"terminus-2,gpt-5",1.1045398158057067,1.010540831267532,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,linear_model.time_fit,linear_model,"{""linear_model.LassoBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LassoBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""linear_model.LogisticRegressionBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LogisticRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""linear_model.SGDRegressorBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SGDRegressorBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",linear_model.time_fit,time,0.06647735823067524
24346,class-level,"terminus-2,claude",1.0567441225682002,1.0569874832137922,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.IsIn,algos.isin,"{""algos.isin.IsIn.time_isin_categorical"": ""class IsIn:\n    def time_isin_categorical(self, dtype):\n        self.series.isin(self.cat_values)\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)"", ""algos.isin.IsIn.time_isin_empty"": ""class IsIn:\n    def time_isin_empty(self, dtype):\n        self.series.isin([])\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)"", ""algos.isin.IsIn.time_isin_mismatched_dtype"": ""class IsIn:\n    def time_isin_mismatched_dtype(self, dtype):\n        self.series.isin(self.mismatched)\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)""}",algos.isin.IsIn,time,-0.00017210795303539608
24350,class-level,"terminus-2,claude",0.9964518521955192,0.9848606535373996,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.Ops2,arithmetic,"{""arithmetic.Ops2.time_frame_float_mod"": ""class Ops2:\n    def time_frame_float_mod(self):\n        self.df % self.df2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Ops2:\n    def setup(self):\n        N = 10**3\n        self.df = DataFrame(np.random.randn(N, N))\n        self.df2 = DataFrame(np.random.randn(N, N))\n    \n        self.df_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n        self.df2_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n    \n        self.s = Series(np.random.randn(N))""}",arithmetic.Ops2,time,0.008197453082121386
24348,class-level,"terminus-2,claude",1.0990278911993103,1.0296213811038797,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.IsinAlmostFullWithRandomInt,algos.isin,"{""algos.isin.IsinAlmostFullWithRandomInt.time_isin"": ""class IsinAlmostFullWithRandomInt:\n    def time_isin(self, dtype, exponent, title):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, exponent, title):\n        M = 3 * 2 ** (exponent - 2)\n        # 0.77-the maximal share of occupied buckets\n        self.series = Series(np.random.randint(0, M, M)).astype(dtype)\n    \n        values = np.random.randint(0, M, M).astype(dtype)\n        if title == \""inside\"":\n            self.values = values\n        elif title == \""outside\"":\n            self.values = values + M\n        else:\n            raise ValueError(title)""}",algos.isin.IsinAlmostFullWithRandomInt,time,0.04908522637583497
24345,class-level,"terminus-2,claude",1.133311625383533,1.1089147165785465,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algorithms.Quantile,algorithms,"{""algorithms.Quantile.time_quantile"": ""class Quantile:\n    def time_quantile(self, quantile, interpolation, dtype):\n        self.idx.quantile(quantile, interpolation=interpolation)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, quantile, interpolation, dtype):\n        N = 10**5\n        data = {\n            \""int\"": np.arange(N),\n            \""uint\"": np.arange(N).astype(np.uint64),\n            \""float\"": np.random.randn(N),\n        }\n        self.idx = pd.Series(data[dtype].repeat(5))""}",algorithms.Quantile,time,0.01725382518032989
24353,class-level,"terminus-2,claude",0.9979792989814374,0.9825157394271008,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.CategoricalSlicing,categoricals,"{""categoricals.CategoricalSlicing.time_getitem_list_like"": ""class CategoricalSlicing:\n    def time_getitem_list_like(self, index):\n        self.data[[self.scalar]]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalSlicing:\n    def setup(self, index):\n        N = 10**6\n        categories = [\""a\"", \""b\"", \""c\""]\n        values = [0] * N + [1] * N + [2] * N\n        if index == \""monotonic_incr\"":\n            self.data = pd.Categorical.from_codes(values, categories=categories)\n        elif index == \""monotonic_decr\"":\n            self.data = pd.Categorical.from_codes(\n                list(reversed(values)), categories=categories\n            )\n        elif index == \""non_monotonic\"":\n            self.data = pd.Categorical.from_codes([0, 1, 2] * N, categories=categories)\n        else:\n            raise ValueError(f\""Invalid index param: {index}\"")\n    \n        self.scalar = 10000\n        self.list = list(range(10000))\n        self.cat_scalar = \""b\"""", ""categoricals.CategoricalSlicing.time_getitem_scalar"": ""class CategoricalSlicing:\n    def time_getitem_scalar(self, index):\n        self.data[self.scalar]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalSlicing:\n    def setup(self, index):\n        N = 10**6\n        categories = [\""a\"", \""b\"", \""c\""]\n        values = [0] * N + [1] * N + [2] * N\n        if index == \""monotonic_incr\"":\n            self.data = pd.Categorical.from_codes(values, categories=categories)\n        elif index == \""monotonic_decr\"":\n            self.data = pd.Categorical.from_codes(\n                list(reversed(values)), categories=categories\n            )\n        elif index == \""non_monotonic\"":\n            self.data = pd.Categorical.from_codes([0, 1, 2] * N, categories=categories)\n        else:\n            raise ValueError(f\""Invalid index param: {index}\"")\n    \n        self.scalar = 10000\n        self.list = list(range(10000))\n        self.cat_scalar = \""b\"""", ""categoricals.CategoricalSlicing.time_getitem_slice"": ""class CategoricalSlicing:\n    def time_getitem_slice(self, index):\n        self.data[: self.scalar]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalSlicing:\n    def setup(self, index):\n        N = 10**6\n        categories = [\""a\"", \""b\"", \""c\""]\n        values = [0] * N + [1] * N + [2] * N\n        if index == \""monotonic_incr\"":\n            self.data = pd.Categorical.from_codes(values, categories=categories)\n        elif index == \""monotonic_decr\"":\n            self.data = pd.Categorical.from_codes(\n                list(reversed(values)), categories=categories\n            )\n        elif index == \""non_monotonic\"":\n            self.data = pd.Categorical.from_codes([0, 1, 2] * N, categories=categories)\n        else:\n            raise ValueError(f\""Invalid index param: {index}\"")\n    \n        self.scalar = 10000\n        self.list = list(range(10000))\n        self.cat_scalar = \""b\""""}",categoricals.CategoricalSlicing,time,0.010936039288781223
24351,class-level,"terminus-2,claude",0.9824268958157192,0.9813637113697236,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,array.ArrowExtensionArray,array,"{""array.ArrowExtensionArray.time_to_numpy"": ""class ArrowExtensionArray:\n    def time_to_numpy(self, dtype, hasna):\n        self.arr.to_numpy()\n\n    def setup(self, dtype, hasna):\n        N = 100_000\n        if dtype == \""boolean[pyarrow]\"":\n            data = np.random.choice([True, False], N, replace=True)\n        elif dtype == \""float64[pyarrow]\"":\n            data = np.random.randn(N)\n        elif dtype == \""int64[pyarrow]\"":\n            data = np.arange(N)\n        elif dtype == \""string[pyarrow]\"":\n            data = tm.rands_array(10, N)\n        elif dtype == \""timestamp[ns][pyarrow]\"":\n            data = pd.date_range(\""2000-01-01\"", freq=\""s\"", periods=N)\n        else:\n            raise NotImplementedError\n    \n        arr = pd.array(data, dtype=dtype)\n        if hasna:\n            arr[::2] = pd.NA\n        self.arr = arr""}",array.ArrowExtensionArray,time,0.0007518984766588572
24347,class-level,"terminus-2,claude",1.1934580578541278,1.173784775878192,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.IsInLongSeriesLookUpDominates,algos.isin,"{""algos.isin.IsInLongSeriesLookUpDominates.time_isin"": ""class IsInLongSeriesLookUpDominates:\n    def time_isin(self, dtypes, MaxNumber, series_type):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, MaxNumber, series_type):\n        N = 10**7\n    \n        if series_type == \""random_hits\"":\n            array = np.random.randint(0, MaxNumber, N)\n        if series_type == \""random_misses\"":\n            array = np.random.randint(0, MaxNumber, N) + MaxNumber\n        if series_type == \""monotone_hits\"":\n            array = np.repeat(np.arange(MaxNumber), N // MaxNumber)\n        if series_type == \""monotone_misses\"":\n            array = np.arange(N) + MaxNumber\n    \n        self.series = Series(array).astype(dtype)\n    \n        self.values = np.arange(MaxNumber).astype(dtype.lower())""}",algos.isin.IsInLongSeriesLookUpDominates,time,0.013913212147054996
24352,class-level,"terminus-2,claude",0.9898216294453116,0.9878651791606782,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,attrs_caching.SeriesArrayAttribute,attrs_caching,"{""attrs_caching.SeriesArrayAttribute.time_extract_array"": ""class SeriesArrayAttribute:\n    def time_extract_array(self, dtype):\n        extract_array(self.series)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesArrayAttribute:\n    def setup(self, dtype):\n        if dtype == \""numeric\"":\n            self.series = pd.Series([1, 2, 3])\n        elif dtype == \""object\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=object)\n        elif dtype == \""category\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=\""category\"")\n        elif dtype == \""datetime64\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3))\n        elif dtype == \""datetime64tz\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3, tz=\""UTC\""))""}",attrs_caching.SeriesArrayAttribute,time,0.0013836282069543328
24344,class-level,"terminus-2,claude",1.1648842382224,1.121587361662498,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algorithms.Factorize,algorithms,"{""algorithms.Factorize.time_factorize"": ""class Factorize:\n    def time_factorize(self, unique, sort, dtype):\n        pd.factorize(self.data, sort=sort)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Factorize:\n    def setup(self, unique, sort, dtype):\n        N = 10**5\n        string_index = tm.makeStringIndex(N)\n        string_arrow = None\n        if dtype == \""string[pyarrow]\"":\n            try:\n                string_arrow = pd.array(string_index, dtype=\""string[pyarrow]\"")\n            except ImportError:\n                raise NotImplementedError\n    \n        data = {\n            \""int\"": pd.Index(np.arange(N), dtype=\""int64\""),\n            \""uint\"": pd.Index(np.arange(N), dtype=\""uint64\""),\n            \""float\"": pd.Index(np.random.randn(N), dtype=\""float64\""),\n            \""object\"": string_index,\n            \""datetime64[ns]\"": pd.date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n            \""datetime64[ns, tz]\"": pd.date_range(\n                \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n            ),\n            \""Int64\"": pd.array(np.arange(N), dtype=\""Int64\""),\n            \""boolean\"": pd.array(np.random.randint(0, 2, N), dtype=\""boolean\""),\n            \""string[pyarrow]\"": string_arrow,\n        }[dtype]\n        if not unique:\n            data = data.repeat(5)\n        self.data = data""}",algorithms.Factorize,time,0.03062013900983168
24355,class-level,"terminus-2,claude",1.0548761297850124,0.9279782369189892,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,ctors.SeriesConstructors,ctors,"{""ctors.SeriesConstructors.time_series_constructor"": ""class SeriesConstructors:\n    def time_series_constructor(self, data_fmt, with_index, dtype):\n        Series(self.data, index=self.index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesConstructors:\n    def setup(self, data_fmt, with_index, dtype):\n        if data_fmt in (gen_of_str, gen_of_tuples) and with_index:\n            raise NotImplementedError(\n                \""Series constructors do not support using generators with indexes\""\n            )\n        N = 10**4\n        if dtype == \""float\"":\n            arr = np.random.randn(N)\n        else:\n            arr = np.arange(N)\n        self.data = data_fmt(arr)\n        self.index = np.arange(N) if with_index else None""}",ctors.SeriesConstructors,time,0.08974391291797969
24360,class-level,"terminus-2,claude",1.1757617678832697,1.143118171288414,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.Query,eval,"{""eval.Query.time_query_datetime_column"": ""class Query:\n    def time_query_datetime_column(self):\n        self.df.query(\""dates < @self.ts\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Query:\n    def setup(self):\n        N = 10**6\n        halfway = (N // 2) - 1\n        index = pd.date_range(\""20010101\"", periods=N, freq=\""T\"")\n        s = pd.Series(index)\n        self.ts = s.iloc[halfway]\n        self.df = pd.DataFrame({\""a\"": np.random.randn(N), \""dates\"": index}, index=index)\n        data = np.random.randn(N)\n        self.min_val = data.min()\n        self.max_val = data.max()"", ""eval.Query.time_query_datetime_index"": ""class Query:\n    def time_query_datetime_index(self):\n        self.df.query(\""index < @self.ts\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Query:\n    def setup(self):\n        N = 10**6\n        halfway = (N // 2) - 1\n        index = pd.date_range(\""20010101\"", periods=N, freq=\""T\"")\n        s = pd.Series(index)\n        self.ts = s.iloc[halfway]\n        self.df = pd.DataFrame({\""a\"": np.random.randn(N), \""dates\"": index}, index=index)\n        data = np.random.randn(N)\n        self.min_val = data.min()\n        self.max_val = data.max()"", ""eval.Query.time_query_with_boolean_selection"": ""class Query:\n    def time_query_with_boolean_selection(self):\n        self.df.query(\""(a >= @self.min_val) & (a <= @self.max_val)\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Query:\n    def setup(self):\n        N = 10**6\n        halfway = (N // 2) - 1\n        index = pd.date_range(\""20010101\"", periods=N, freq=\""T\"")\n        s = pd.Series(index)\n        self.ts = s.iloc[halfway]\n        self.df = pd.DataFrame({\""a\"": np.random.randn(N), \""dates\"": index}, index=index)\n        data = np.random.randn(N)\n        self.min_val = data.min()\n        self.max_val = data.max()""}",eval.Query,time,0.023085994762981413
24349,class-level,"terminus-2,claude",1.0335206561296777,0.8638397455809634,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.OffsetArrayArithmetic,arithmetic,"{""arithmetic.OffsetArrayArithmetic.time_add_dti_offset"": ""class OffsetArrayArithmetic:\n    def time_add_dti_offset(self, offset):\n        with warnings.catch_warnings(record=True):\n            self.rng + offset\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass OffsetArrayArithmetic:\n    def setup(self, offset):\n        N = 10000\n        rng = date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"")\n        self.rng = rng\n        self.ser = Series(rng)"", ""arithmetic.OffsetArrayArithmetic.time_add_series_offset"": ""class OffsetArrayArithmetic:\n    def time_add_series_offset(self, offset):\n        with warnings.catch_warnings(record=True):\n            self.ser + offset\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass OffsetArrayArithmetic:\n    def setup(self, offset):\n        N = 10000\n        rng = date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"")\n        self.rng = rng\n        self.ser = Series(rng)""}",arithmetic.OffsetArrayArithmetic,time,0.12000064395241462
24365,class-level,"terminus-2,claude",1.0443159189364772,1.041738778447151,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Apply,frame_methods,"{""frame_methods.Apply.time_apply_axis_1"": ""class Apply:\n    def time_apply_axis_1(self):\n        self.df.apply(lambda x: x + 1, axis=1)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self):\n        self.df = DataFrame(np.random.randn(1000, 100))\n    \n        self.s = Series(np.arange(1028.0))\n        self.df2 = DataFrame({i: self.s for i in range(1028)})\n        self.df3 = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))"", ""frame_methods.Apply.time_apply_lambda_mean"": ""class Apply:\n    def time_apply_lambda_mean(self):\n        self.df.apply(lambda x: x.mean())\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self):\n        self.df = DataFrame(np.random.randn(1000, 100))\n    \n        self.s = Series(np.arange(1028.0))\n        self.df2 = DataFrame({i: self.s for i in range(1028)})\n        self.df3 = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))""}",frame_methods.Apply,time,0.0018225887477554626
24361,class-level,"terminus-2,claude",1.1995468577833477,1.109764724574163,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromArrays,frame_ctor,"{""frame_ctor.FromArrays.time_frame_from_arrays_float"": ""class FromArrays:\n    def time_frame_from_arrays_float(self):\n        self.df = DataFrame._from_arrays(\n            self.float_arrays,\n            index=self.index,\n            columns=self.columns,\n            verify_integrity=False,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromArrays:\n    def setup(self):\n        N_rows = 1000\n        N_cols = 1000\n        self.float_arrays = [np.random.randn(N_rows) for _ in range(N_cols)]\n        self.sparse_arrays = [\n            pd.arrays.SparseArray(np.random.randint(0, 2, N_rows), dtype=\""float64\"")\n            for _ in range(N_cols)\n        ]\n        self.int_arrays = [\n            pd.array(np.random.randint(1000, size=N_rows), dtype=\""Int64\"")\n            for _ in range(N_cols)\n        ]\n        self.index = pd.Index(range(N_rows))\n        self.columns = pd.Index(range(N_cols))"", ""frame_ctor.FromArrays.time_frame_from_arrays_int"": ""class FromArrays:\n    def time_frame_from_arrays_int(self):\n        self.df = DataFrame._from_arrays(\n            self.int_arrays,\n            index=self.index,\n            columns=self.columns,\n            verify_integrity=False,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromArrays:\n    def setup(self):\n        N_rows = 1000\n        N_cols = 1000\n        self.float_arrays = [np.random.randn(N_rows) for _ in range(N_cols)]\n        self.sparse_arrays = [\n            pd.arrays.SparseArray(np.random.randint(0, 2, N_rows), dtype=\""float64\"")\n            for _ in range(N_cols)\n        ]\n        self.int_arrays = [\n            pd.array(np.random.randint(1000, size=N_rows), dtype=\""Int64\"")\n            for _ in range(N_cols)\n        ]\n        self.index = pd.Index(range(N_rows))\n        self.columns = pd.Index(range(N_cols))"", ""frame_ctor.FromArrays.time_frame_from_arrays_sparse"": ""class FromArrays:\n    def time_frame_from_arrays_sparse(self):\n        self.df = DataFrame._from_arrays(\n            self.sparse_arrays,\n            index=self.index,\n            columns=self.columns,\n            verify_integrity=False,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromArrays:\n    def setup(self):\n        N_rows = 1000\n        N_cols = 1000\n        self.float_arrays = [np.random.randn(N_rows) for _ in range(N_cols)]\n        self.sparse_arrays = [\n            pd.arrays.SparseArray(np.random.randint(0, 2, N_rows), dtype=\""float64\"")\n            for _ in range(N_cols)\n        ]\n        self.int_arrays = [\n            pd.array(np.random.randint(1000, size=N_rows), dtype=\""Int64\"")\n            for _ in range(N_cols)\n        ]\n        self.index = pd.Index(range(N_rows))\n        self.columns = pd.Index(range(N_cols))""}",frame_ctor.FromArrays,time,0.06349514371229463
24362,class-level,"terminus-2,claude",1.1488737440064496,1.0638199520587115,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromDicts,frame_ctor,"{""frame_ctor.FromDicts.time_dict_of_categoricals"": ""class FromDicts:\n    def time_dict_of_categoricals(self):\n        # dict of arrays that we won't consolidate\n        DataFrame(self.dict_of_categoricals)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDicts:\n    def setup(self):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n        frame = DataFrame(np.random.randn(N, K), index=self.index, columns=self.columns)\n        self.data = frame.to_dict()\n        self.dict_list = frame.to_dict(orient=\""records\"")\n        self.data2 = {i: {j: float(j) for j in range(100)} for i in range(2000)}\n    \n        # arrays which we won't consolidate\n        self.dict_of_categoricals = {i: Categorical(np.arange(N)) for i in range(K)}"", ""frame_ctor.FromDicts.time_list_of_dict"": ""class FromDicts:\n    def time_list_of_dict(self):\n        DataFrame(self.dict_list)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDicts:\n    def setup(self):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n        frame = DataFrame(np.random.randn(N, K), index=self.index, columns=self.columns)\n        self.data = frame.to_dict()\n        self.dict_list = frame.to_dict(orient=\""records\"")\n        self.data2 = {i: {j: float(j) for j in range(100)} for i in range(2000)}\n    \n        # arrays which we won't consolidate\n        self.dict_of_categoricals = {i: Categorical(np.arange(N)) for i in range(K)}"", ""frame_ctor.FromDicts.time_nested_dict_int64"": ""class FromDicts:\n    def time_nested_dict_int64(self):\n        # nested dict, integer indexes, regression described in #621\n        DataFrame(self.data2)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDicts:\n    def setup(self):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n        frame = DataFrame(np.random.randn(N, K), index=self.index, columns=self.columns)\n        self.data = frame.to_dict()\n        self.dict_list = frame.to_dict(orient=\""records\"")\n        self.data2 = {i: {j: float(j) for j in range(100)} for i in range(2000)}\n    \n        # arrays which we won't consolidate\n        self.dict_of_categoricals = {i: Categorical(np.arange(N)) for i in range(K)}""}",frame_ctor.FromDicts,time,0.0601511965684145
24354,class-level,"terminus-2,claude",1.0186548518363805,0.9767446348584446,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,ctors.DatetimeIndexConstructor,ctors,"{""ctors.DatetimeIndexConstructor.time_from_list_of_dates"": ""class DatetimeIndexConstructor:\n    def time_from_list_of_dates(self):\n        DatetimeIndex(self.list_of_dates)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndexConstructor:\n    def setup(self):\n        N = 20_000\n        dti = date_range(\""1900-01-01\"", periods=N)\n    \n        self.list_of_timestamps = dti.tolist()\n        self.list_of_dates = dti.date.tolist()\n        self.list_of_datetimes = dti.to_pydatetime().tolist()\n        self.list_of_str = dti.strftime(\""%Y-%m-%d\"").tolist()""}",ctors.DatetimeIndexConstructor,time,0.029639474524707103
24367,class-level,"terminus-2,claude",1.0988368524180148,1.0483986784350785,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Describe,frame_methods,"{""frame_methods.Describe.time_dataframe_describe"": ""class Describe:\n    def time_dataframe_describe(self):\n        self.df.describe()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Describe:\n    def setup(self):\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(0, 100, 10**6),\n                \""b\"": np.random.randint(0, 100, 10**6),\n                \""c\"": np.random.randint(0, 100, 10**6),\n            }\n        )"", ""frame_methods.Describe.time_series_describe"": ""class Describe:\n    def time_series_describe(self):\n        self.df[\""a\""].describe()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Describe:\n    def setup(self):\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(0, 100, 10**6),\n                \""b\"": np.random.randint(0, 100, 10**6),\n                \""c\"": np.random.randint(0, 100, 10**6),\n            }\n        )""}",frame_methods.Describe,time,0.03567056151551363
24358,class-level,"terminus-2,claude",1.064101855723977,1.0899004816418656,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.SelectDtypes,dtypes,"{""dtypes.SelectDtypes.time_select_dtype_bool_exclude"": ""class SelectDtypes:\n    def time_select_dtype_bool_exclude(self, dtype):\n        self.df_bool.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_bool_include"": ""class SelectDtypes:\n    def time_select_dtype_bool_include(self, dtype):\n        self.df_bool.select_dtypes(include=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_float_exclude"": ""class SelectDtypes:\n    def time_select_dtype_float_exclude(self, dtype):\n        self.df_float.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_float_include"": ""class SelectDtypes:\n    def time_select_dtype_float_include(self, dtype):\n        self.df_float.select_dtypes(include=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_int_exclude"": ""class SelectDtypes:\n    def time_select_dtype_int_exclude(self, dtype):\n        self.df_int.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_int_include"": ""class SelectDtypes:\n    def time_select_dtype_int_include(self, dtype):\n        self.df_int.select_dtypes(include=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_string_exclude"": ""class SelectDtypes:\n    def time_select_dtype_string_exclude(self, dtype):\n        self.df_string.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_string_include"": ""class SelectDtypes:\n    def time_select_dtype_string_include(self, dtype):\n        self.df_string.select_dtypes(include=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )""}",dtypes.SelectDtypes,time,-0.0182451385557911
24366,class-level,"terminus-2,claude",1.237014289509044,1.1014807750057138,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Count,frame_methods,"{""frame_methods.Count.time_count_level_mixed_dtypes_multi"": ""class Count:\n    def time_count_level_mixed_dtypes_multi(self, axis):\n        self.df_mixed.count(axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Count:\n    def setup(self, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""\n    \n        self.df.index = MultiIndex.from_arrays([self.df.index, self.df.index])\n        self.df.columns = MultiIndex.from_arrays([self.df.columns, self.df.columns])\n        self.df_mixed.index = MultiIndex.from_arrays(\n            [self.df_mixed.index, self.df_mixed.index]\n        )\n        self.df_mixed.columns = MultiIndex.from_arrays(\n            [self.df_mixed.columns, self.df_mixed.columns]\n        )"", ""frame_methods.Count.time_count_level_multi"": ""class Count:\n    def time_count_level_multi(self, axis):\n        self.df.count(axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Count:\n    def setup(self, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""\n    \n        self.df.index = MultiIndex.from_arrays([self.df.index, self.df.index])\n        self.df.columns = MultiIndex.from_arrays([self.df.columns, self.df.columns])\n        self.df_mixed.index = MultiIndex.from_arrays(\n            [self.df_mixed.index, self.df_mixed.index]\n        )\n        self.df_mixed.columns = MultiIndex.from_arrays(\n            [self.df_mixed.columns, self.df_mixed.columns]\n        )""}",frame_methods.Count,time,0.09585114179867756
24357,class-level,"terminus-2,claude",1.0285055100576934,0.9843871123238844,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.DtypesInvalid,dtypes,"{""dtypes.DtypesInvalid.time_pandas_dtype_invalid"": ""class DtypesInvalid:\n    def time_pandas_dtype_invalid(self, dtype):\n        try:\n            pandas_dtype(self.data_dict[dtype])\n        except TypeError:\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)""}",dtypes.DtypesInvalid,time,0.031201129939044548
24356,class-level,"terminus-2,claude",1.0270767187934915,1.0630451419651972,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.Dtypes,dtypes,"{""dtypes.Dtypes.time_pandas_dtype"": ""class Dtypes:\n    def time_pandas_dtype(self, dtype):\n        pandas_dtype(dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)""}",dtypes.Dtypes,time,-0.02543735726428973
24371,class-level,"terminus-2,claude",1.0474250388242552,1.07645130760125,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Iteration,frame_methods,"{""frame_methods.Iteration.time_itertuples_raw_start"": ""class Iteration:\n    def time_itertuples_raw_start(self):\n        self.df4.itertuples(index=False, name=None)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))"", ""frame_methods.Iteration.time_itertuples_read_first"": ""class Iteration:\n    def time_itertuples_read_first(self):\n        next(self.df4.itertuples())\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))"", ""frame_methods.Iteration.time_itertuples_start"": ""class Iteration:\n    def time_itertuples_start(self):\n        self.df4.itertuples()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))""}",frame_methods.Iteration,time,-0.020527771412301894
24359,class-level,"terminus-2,claude",1.2072379034754768,1.21933316247237,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.Eval,eval,"{""eval.Eval.time_add"": ""class Eval:\n    def time_add(self, engine, threads):\n        pd.eval(\""self.df + self.df2 + self.df3 + self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)"", ""eval.Eval.time_and"": ""class Eval:\n    def time_and(self, engine, threads):\n        pd.eval(\n            \""(self.df > 0) & (self.df2 > 0) & (self.df3 > 0) & (self.df4 > 0)\"",\n            engine=engine,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)"", ""eval.Eval.time_chained_cmp"": ""class Eval:\n    def time_chained_cmp(self, engine, threads):\n        pd.eval(\""self.df < self.df2 < self.df3 < self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)"", ""eval.Eval.time_mult"": ""class Eval:\n    def time_mult(self, engine, threads):\n        pd.eval(\""self.df * self.df2 * self.df3 * self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)""}",eval.Eval,time,-0.008553931398085737
24370,class-level,"terminus-2,claude",1.2349416916784892,1.097394403466586,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Fillna,frame_methods,"{""frame_methods.Fillna.time_frame_fillna"": ""class Fillna:\n    def time_frame_fillna(self, inplace, method, dtype):\n        self.df.fillna(inplace=inplace, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Fillna:\n    def setup(self, inplace, method, dtype):\n        N, M = 10000, 100\n        if dtype in (\""datetime64[ns]\"", \""datetime64[ns, tz]\"", \""timedelta64[ns]\""):\n            data = {\n                \""datetime64[ns]\"": date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n                \""datetime64[ns, tz]\"": date_range(\n                    \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n                ),\n                \""timedelta64[ns]\"": timedelta_range(start=\""1 day\"", periods=N, freq=\""1D\""),\n            }\n            self.df = DataFrame({f\""col_{i}\"": data[dtype] for i in range(M)})\n            self.df[::2] = None\n        else:\n            values = np.random.randn(N, M)\n            values[::2] = np.nan\n            if dtype == \""Int64\"":\n                values = values.round()\n            self.df = DataFrame(values, dtype=dtype)""}",frame_methods.Fillna,time,0.09727530990940829
24369,class-level,"terminus-2,claude",1.0458341879438715,1.0415164232546907,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Duplicated,frame_methods,"{""frame_methods.Duplicated.time_frame_duplicated_wide"": ""class Duplicated:\n    def time_frame_duplicated_wide(self):\n        self.df2.duplicated()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Duplicated:\n    def setup(self):\n        n = 1 << 20\n        t = date_range(\""2015-01-01\"", freq=\""S\"", periods=(n // 64))\n        xs = np.random.randn(n // 64).round(2)\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(-1 << 8, 1 << 8, n),\n                \""b\"": np.random.choice(t, n),\n                \""c\"": np.random.choice(xs, n),\n            }\n        )\n        self.df2 = DataFrame(np.random.randn(1000, 100).astype(str)).T""}",frame_methods.Duplicated,time,0.003053581816959526
24363,class-level,"terminus-2,claude",1.0370344496148716,1.0154414722650773,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromLists,frame_ctor,"{""frame_ctor.FromLists.time_frame_from_lists"": ""class FromLists:\n    def time_frame_from_lists(self):\n        self.df = DataFrame(self.data)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromLists:\n    def setup(self):\n        N = 1000\n        M = 100\n        self.data = [list(range(M)) for i in range(N)]""}",frame_ctor.FromLists,time,0.015270846782032717
24372,class-level,"terminus-2,claude",1.146489888207465,1.116999466999951,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.NSort,frame_methods,"{""frame_methods.NSort.time_nlargest_one_column"": ""class NSort:\n    def time_nlargest_one_column(self, keep):\n        self.df.nlargest(100, \""A\"", keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NSort:\n    def setup(self, keep):\n        self.df = DataFrame(np.random.randn(100000, 3), columns=list(\""ABC\""))"", ""frame_methods.NSort.time_nlargest_two_columns"": ""class NSort:\n    def time_nlargest_two_columns(self, keep):\n        self.df.nlargest(100, [\""A\"", \""B\""], keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NSort:\n    def setup(self, keep):\n        self.df = DataFrame(np.random.randn(100000, 3), columns=list(\""ABC\""))"", ""frame_methods.NSort.time_nsmallest_one_column"": ""class NSort:\n    def time_nsmallest_one_column(self, keep):\n        self.df.nsmallest(100, \""A\"", keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NSort:\n    def setup(self, keep):\n        self.df = DataFrame(np.random.randn(100000, 3), columns=list(\""ABC\""))"", ""frame_methods.NSort.time_nsmallest_two_columns"": ""class NSort:\n    def time_nsmallest_two_columns(self, keep):\n        self.df.nsmallest(100, [\""A\"", \""B\""], keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NSort:\n    def setup(self, keep):\n        self.df = DataFrame(np.random.randn(100000, 3), columns=list(\""ABC\""))""}",frame_methods.NSort,time,0.02085602631365912
24379,class-level,"terminus-2,claude",1.0525076631231682,1.0355765116122002,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.GroupManyLabels,groupby,"{""groupby.GroupManyLabels.time_sum"": ""class GroupManyLabels:\n    def time_sum(self, ncols):\n        self.df.groupby(self.labels).sum()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupManyLabels:\n    def setup(self, ncols):\n        N = 1000\n        data = np.random.randn(N, ncols)\n        self.labels = np.random.randint(0, 100, size=N)\n        self.df = DataFrame(data)""}",groupby.GroupManyLabels,time,0.011973940248209353
24368,class-level,"terminus-2,claude",1.2337078519617417,1.1476588961940375,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Dropna,frame_methods,"{""frame_methods.Dropna.time_dropna"": ""class Dropna:\n    def time_dropna(self, how, axis):\n        self.df.dropna(how=how, axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Dropna:\n    def setup(self, how, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\"""", ""frame_methods.Dropna.time_dropna_axis_mixed_dtypes"": ""class Dropna:\n    def time_dropna_axis_mixed_dtypes(self, how, axis):\n        self.df_mixed.dropna(how=how, axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Dropna:\n    def setup(self, how, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""""}",frame_methods.Dropna,time,0.06085498993472717
24377,class-level,"terminus-2,claude",1.0790536747171422,1.070948001002442,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.Cumulative,groupby,"{""groupby.Cumulative.time_frame_transform"": ""class Cumulative:\n    def time_frame_transform(self, dtype, method, with_nans):\n        self.df.groupby(\""key\"").transform(method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cumulative:\n    def setup(self, dtype, method, with_nans):\n        if with_nans and dtype == \""int64\"":\n            raise NotImplementedError(\""Construction of df would raise\"")\n    \n        N = 500_000\n        keys = np.random.randint(0, 100, size=N)\n        vals = np.random.randint(-10, 10, (N, 5))\n    \n        if with_nans:\n            null_vals = vals.astype(float, copy=True)\n            null_vals[::2, :] = np.nan\n            null_vals[::3, :] = np.nan\n            df = DataFrame(null_vals, columns=list(\""abcde\""), dtype=dtype)\n            df[\""key\""] = keys\n            self.df = df\n        else:\n            df = DataFrame(vals, columns=list(\""abcde\"")).astype(dtype, copy=False)\n            df[\""key\""] = keys\n            self.df = df""}",groupby.Cumulative,time,0.005732442513932305
24375,class-level,"terminus-2,claude",1.120471843140477,1.0230055870235066,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.Apply,groupby,"{""groupby.Apply.time_copy_function_multi_col"": ""class Apply:\n    def time_copy_function_multi_col(self, factor):\n        self.df.groupby([\""key\"", \""key2\""]).apply(self.df_copy_function)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, factor):\n        N = 10**factor\n        # two cases:\n        # - small groups: small data (N**4) + many labels (2000) -> average group\n        #   size of 5 (-> larger overhead of slicing method)\n        # - larger groups: larger data (N**5) + fewer labels (20) -> average group\n        #   size of 5000\n        labels = np.random.randint(0, 2000 if factor == 4 else 20, size=N)\n        labels2 = np.random.randint(0, 3, size=N)\n        df = DataFrame(\n            {\n                \""key\"": labels,\n                \""key2\"": labels2,\n                \""value1\"": np.random.randn(N),\n                \""value2\"": [\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (N // 4),\n            }\n        )\n        self.df = df"", ""groupby.Apply.time_copy_overhead_single_col"": ""class Apply:\n    def time_copy_overhead_single_col(self, factor):\n        self.df.groupby(\""key\"").apply(self.df_copy_function)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, factor):\n        N = 10**factor\n        # two cases:\n        # - small groups: small data (N**4) + many labels (2000) -> average group\n        #   size of 5 (-> larger overhead of slicing method)\n        # - larger groups: larger data (N**5) + fewer labels (20) -> average group\n        #   size of 5000\n        labels = np.random.randint(0, 2000 if factor == 4 else 20, size=N)\n        labels2 = np.random.randint(0, 3, size=N)\n        df = DataFrame(\n            {\n                \""key\"": labels,\n                \""key2\"": labels2,\n                \""value1\"": np.random.randn(N),\n                \""value2\"": [\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (N // 4),\n            }\n        )\n        self.df = df"", ""groupby.Apply.time_scalar_function_multi_col"": ""class Apply:\n    def time_scalar_function_multi_col(self, factor):\n        self.df.groupby([\""key\"", \""key2\""]).apply(lambda x: 1)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, factor):\n        N = 10**factor\n        # two cases:\n        # - small groups: small data (N**4) + many labels (2000) -> average group\n        #   size of 5 (-> larger overhead of slicing method)\n        # - larger groups: larger data (N**5) + fewer labels (20) -> average group\n        #   size of 5000\n        labels = np.random.randint(0, 2000 if factor == 4 else 20, size=N)\n        labels2 = np.random.randint(0, 3, size=N)\n        df = DataFrame(\n            {\n                \""key\"": labels,\n                \""key2\"": labels2,\n                \""value1\"": np.random.randn(N),\n                \""value2\"": [\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (N // 4),\n            }\n        )\n        self.df = df"", ""groupby.Apply.time_scalar_function_single_col"": ""class Apply:\n    def time_scalar_function_single_col(self, factor):\n        self.df.groupby(\""key\"").apply(lambda x: 1)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, factor):\n        N = 10**factor\n        # two cases:\n        # - small groups: small data (N**4) + many labels (2000) -> average group\n        #   size of 5 (-> larger overhead of slicing method)\n        # - larger groups: larger data (N**5) + fewer labels (20) -> average group\n        #   size of 5000\n        labels = np.random.randint(0, 2000 if factor == 4 else 20, size=N)\n        labels2 = np.random.randint(0, 3, size=N)\n        df = DataFrame(\n            {\n                \""key\"": labels,\n                \""key2\"": labels2,\n                \""value1\"": np.random.randn(N),\n                \""value2\"": [\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (N // 4),\n            }\n        )\n        self.df = df""}",groupby.Apply,time,0.06892945977154905
24376,class-level,"terminus-2,claude",1.1539903005891918,1.0188571417553025,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.ApplyNonUniqueUnsortedIndex,groupby,"{""groupby.ApplyNonUniqueUnsortedIndex.time_groupby_apply_non_unique_unsorted_index"": ""class ApplyNonUniqueUnsortedIndex:\n    def time_groupby_apply_non_unique_unsorted_index(self):\n        self.df.groupby(\""key\"", group_keys=False).apply(lambda x: x)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ApplyNonUniqueUnsortedIndex:\n    def setup(self):\n        # GH 46527\n        # unsorted and non-unique index\n        idx = np.arange(100)[::-1]\n        idx = Index(np.repeat(idx, 200), name=\""key\"")\n        self.df = DataFrame(np.random.randn(len(idx), 10), index=idx)""}",groupby.ApplyNonUniqueUnsortedIndex,time,0.09556800483301933
24374,class-level,"terminus-2,claude",1.0575302948020138,1.1079542764811172,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,gil.ParallelGroupbyMethods,gil,"{""gil.ParallelGroupbyMethods.time_parallel"": ""class ParallelGroupbyMethods:\n    def time_parallel(self, threads, method):\n        self.parallel()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ParallelGroupbyMethods:\n    def setup(self, threads, method):\n        N = 10**6\n        ngroups = 10**3\n        df = DataFrame(\n            {\""key\"": np.random.randint(0, ngroups, size=N), \""data\"": np.random.randn(N)}\n        )\n    \n        @test_parallel(num_threads=threads)\n        def parallel():\n            getattr(df.groupby(\""key\"")[\""data\""], method)()\n    \n        self.parallel = parallel\n    \n        def loop():\n            getattr(df.groupby(\""key\"")[\""data\""], method)()\n    \n        self.loop = loop""}",gil.ParallelGroupbyMethods,time,-0.03566052452553285
24378,class-level,"terminus-2,claude",1.0764388410989287,1.0725338064505447,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.GroupByMethods,groupby,"{""groupby.GroupByMethods.time_dtype_as_field"": ""class GroupByMethods:\n    def time_dtype_as_field(self, dtype, method, application, ncols):\n        self.as_field_method()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupByMethods:\n    def setup(self, dtype, method, application, ncols):\n        if method in method_blocklist.get(dtype, {}):\n            raise NotImplementedError  # skip benchmark\n    \n        if ncols != 1 and method in [\""value_counts\"", \""unique\""]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if application == \""transformation\"" and method in [\n            \""describe\"",\n            \""head\"",\n            \""tail\"",\n            \""unique\"",\n            \""value_counts\"",\n            \""size\"",\n        ]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if method == \""describe\"":\n            ngroups = 20\n        elif method == \""skew\"":\n            ngroups = 100\n        else:\n            ngroups = 1000\n        size = ngroups * 2\n        rng = np.arange(ngroups).reshape(-1, 1)\n        rng = np.broadcast_to(rng, (len(rng), ncols))\n        taker = np.random.randint(0, ngroups, size=size)\n        values = rng.take(taker, axis=0)\n        if dtype == \""int\"":\n            key = np.random.randint(0, size, size=size)\n        elif dtype in (\""int16\"", \""uint\""):\n            key = np.random.randint(0, size, size=size, dtype=dtype)\n        elif dtype == \""float\"":\n            key = np.concatenate(\n                [np.random.random(ngroups) * 0.1, np.random.random(ngroups) * 10.0]\n            )\n        elif dtype == \""object\"":\n            key = [\""foo\""] * size\n        elif dtype == \""datetime\"":\n            key = date_range(\""1/1/2011\"", periods=size, freq=\""s\"")\n    \n        cols = [f\""values{n}\"" for n in range(ncols)]\n        df = DataFrame(values, columns=cols)\n        df[\""key\""] = key\n    \n        if len(cols) == 1:\n            cols = cols[0]\n    \n        if application == \""transformation\"":\n            self.as_group_method = lambda: df.groupby(\""key\"")[cols].transform(method)\n            self.as_field_method = lambda: df.groupby(cols)[\""key\""].transform(method)\n        else:\n            self.as_group_method = getattr(df.groupby(\""key\"")[cols], method)\n            self.as_field_method = getattr(df.groupby(cols)[\""key\""], method)"", ""groupby.GroupByMethods.time_dtype_as_group"": ""class GroupByMethods:\n    def time_dtype_as_group(self, dtype, method, application, ncols):\n        self.as_group_method()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupByMethods:\n    def setup(self, dtype, method, application, ncols):\n        if method in method_blocklist.get(dtype, {}):\n            raise NotImplementedError  # skip benchmark\n    \n        if ncols != 1 and method in [\""value_counts\"", \""unique\""]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if application == \""transformation\"" and method in [\n            \""describe\"",\n            \""head\"",\n            \""tail\"",\n            \""unique\"",\n            \""value_counts\"",\n            \""size\"",\n        ]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if method == \""describe\"":\n            ngroups = 20\n        elif method == \""skew\"":\n            ngroups = 100\n        else:\n            ngroups = 1000\n        size = ngroups * 2\n        rng = np.arange(ngroups).reshape(-1, 1)\n        rng = np.broadcast_to(rng, (len(rng), ncols))\n        taker = np.random.randint(0, ngroups, size=size)\n        values = rng.take(taker, axis=0)\n        if dtype == \""int\"":\n            key = np.random.randint(0, size, size=size)\n        elif dtype in (\""int16\"", \""uint\""):\n            key = np.random.randint(0, size, size=size, dtype=dtype)\n        elif dtype == \""float\"":\n            key = np.concatenate(\n                [np.random.random(ngroups) * 0.1, np.random.random(ngroups) * 10.0]\n            )\n        elif dtype == \""object\"":\n            key = [\""foo\""] * size\n        elif dtype == \""datetime\"":\n            key = date_range(\""1/1/2011\"", periods=size, freq=\""s\"")\n    \n        cols = [f\""values{n}\"" for n in range(ncols)]\n        df = DataFrame(values, columns=cols)\n        df[\""key\""] = key\n    \n        if len(cols) == 1:\n            cols = cols[0]\n    \n        if application == \""transformation\"":\n            self.as_group_method = lambda: df.groupby(\""key\"")[cols].transform(method)\n            self.as_field_method = lambda: df.groupby(cols)[\""key\""].transform(method)\n        else:\n            self.as_group_method = getattr(df.groupby(\""key\"")[cols], method)\n            self.as_field_method = getattr(df.groupby(cols)[\""key\""], method)""}",groupby.GroupByMethods,time,0.0027616935278528907
24381,class-level,"terminus-2,claude",1.0534916624908344,0.9242780314831636,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.Size,groupby,"{""groupby.Size.time_category_size"": ""class Size:\n    def time_category_size(self):\n        self.draws.groupby(self.cats).size()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Size:\n    def setup(self):\n        n = 10**5\n        offsets = np.random.randint(n, size=n).astype(\""timedelta64[ns]\"")\n        dates = np.datetime64(\""now\"") + offsets\n        self.df = DataFrame(\n            {\n                \""key1\"": np.random.randint(0, 500, size=n),\n                \""key2\"": np.random.randint(0, 100, size=n),\n                \""value1\"": np.random.randn(n),\n                \""value2\"": np.random.randn(n),\n                \""value3\"": np.random.randn(n),\n                \""dates\"": dates,\n            }\n        )\n        self.draws = Series(np.random.randn(n))\n        labels = Series([\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (n // 4))\n        self.cats = labels.astype(\""category\"")"", ""groupby.Size.time_multi_size"": ""class Size:\n    def time_multi_size(self):\n        self.df.groupby([\""key1\"", \""key2\""]).size()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Size:\n    def setup(self):\n        n = 10**5\n        offsets = np.random.randint(n, size=n).astype(\""timedelta64[ns]\"")\n        dates = np.datetime64(\""now\"") + offsets\n        self.df = DataFrame(\n            {\n                \""key1\"": np.random.randint(0, 500, size=n),\n                \""key2\"": np.random.randint(0, 100, size=n),\n                \""value1\"": np.random.randn(n),\n                \""value2\"": np.random.randn(n),\n                \""value3\"": np.random.randn(n),\n                \""dates\"": dates,\n            }\n        )\n        self.draws = Series(np.random.randn(n))\n        labels = Series([\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (n // 4))\n        self.cats = labels.astype(\""category\"")""}",groupby.Size,time,0.09138163437600481
24373,class-level,"terminus-2,claude",1.0352487998803463,1.0248017468505206,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Quantile,frame_methods,"{""frame_methods.Quantile.time_frame_quantile"": ""class Quantile:\n    def time_frame_quantile(self, axis):\n        self.df.quantile([0.1, 0.5], axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, axis):\n        self.df = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))""}",frame_methods.Quantile,time,0.007388297758009715
24364,class-level,"terminus-2,claude",1.0673483349349584,1.048155438963408,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromRecords,frame_ctor,"{""frame_ctor.FromRecords.time_frame_from_records_generator"": ""class FromRecords:\n    def time_frame_from_records_generator(self, nrows):\n        # issue-6700\n        self.df = DataFrame.from_records(self.gen, nrows=nrows)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromRecords:\n    def setup(self, nrows):\n        N = 100000\n        self.gen = ((x, (x * 20), (x * 100)) for x in range(N))""}",frame_ctor.FromRecords,time,0.013573476641831955
24385,class-level,"terminus-2,claude",1.129020629433639,1.1141947051174346,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,hash_functions.UniqueAndFactorizeArange,hash_functions,"{""hash_functions.UniqueAndFactorizeArange.time_factorize"": ""class UniqueAndFactorizeArange:\n    def time_factorize(self, exponent):\n        pd.factorize(self.a2)\n\n    def setup(self, exponent):\n        a = np.arange(10**4, dtype=\""float64\"")\n        self.a2 = (a + 10**exponent).repeat(100)""}",hash_functions.UniqueAndFactorizeArange,time,0.010485094990243516
24382,class-level,"terminus-2,claude",1.0688795064751455,0.8448937156170028,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.String,groupby,"{""groupby.String.time_str_func"": ""class String:\n    def time_str_func(self, dtype, method):\n        self.df.groupby(\""a\"")[self.df.columns[1:]].agg(method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass String:\n    def setup(self, dtype, method):\n        cols = list(\""abcdefghjkl\"")\n        self.df = DataFrame(\n            np.random.randint(0, 100, size=(10_000, len(cols))),\n            columns=cols,\n            dtype=dtype,\n        )""}",groupby.String,time,0.15840579268609814
24395,class-level,"terminus-2,claude",1.1725839980407926,1.1879096921700036,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericSeriesIndexing,indexing,"{""indexing.NumericSeriesIndexing.time_getitem_array"": ""class NumericSeriesIndexing:\n    def time_getitem_array(self, index, index_structure):\n        self.data[self.array]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_getitem_list_like"": ""class NumericSeriesIndexing:\n    def time_getitem_list_like(self, index, index_structure):\n        self.data[[800000]]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_getitem_lists"": ""class NumericSeriesIndexing:\n    def time_getitem_lists(self, index, index_structure):\n        self.data[self.array_list]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_getitem_scalar"": ""class NumericSeriesIndexing:\n    def time_getitem_scalar(self, index, index_structure):\n        self.data[800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_getitem_slice"": ""class NumericSeriesIndexing:\n    def time_getitem_slice(self, index, index_structure):\n        self.data[:800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_iloc_slice"": ""class NumericSeriesIndexing:\n    def time_iloc_slice(self, index, index_structure):\n        self.data.iloc[:800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_loc_array"": ""class NumericSeriesIndexing:\n    def time_loc_array(self, index, index_structure):\n        self.data.loc[self.array]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_loc_list_like"": ""class NumericSeriesIndexing:\n    def time_loc_list_like(self, index, index_structure):\n        self.data.loc[[800000]]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_loc_scalar"": ""class NumericSeriesIndexing:\n    def time_loc_scalar(self, index, index_structure):\n        self.data.loc[800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.NumericSeriesIndexing,time,-0.010838538988126614
24387,class-level,"terminus-2,claude",1.0046067485647476,0.9864520167007408,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.CategoricalIndexIndexing,indexing,"{""indexing.CategoricalIndexIndexing.time_get_indexer_list"": ""class CategoricalIndexIndexing:\n    def time_get_indexer_list(self, index):\n        self.data_unique.get_indexer(self.cat_list)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalIndexIndexing:\n    def setup(self, index):\n        N = 10**5\n        values = list(\""a\"" * N + \""b\"" * N + \""c\"" * N)\n        indices = {\n            \""monotonic_incr\"": CategoricalIndex(values),\n            \""monotonic_decr\"": CategoricalIndex(reversed(values)),\n            \""non_monotonic\"": CategoricalIndex(list(\""abc\"" * N)),\n        }\n        self.data = indices[index]\n        self.data_unique = CategoricalIndex([str(i) for i in range(N * 3)])\n    \n        self.int_scalar = 10000\n        self.int_list = list(range(10000))\n    \n        self.cat_scalar = \""b\""\n        self.cat_list = [\""1\"", \""3\""]"", ""indexing.CategoricalIndexIndexing.time_getitem_list"": ""class CategoricalIndexIndexing:\n    def time_getitem_list(self, index):\n        self.data[self.int_list]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalIndexIndexing:\n    def setup(self, index):\n        N = 10**5\n        values = list(\""a\"" * N + \""b\"" * N + \""c\"" * N)\n        indices = {\n            \""monotonic_incr\"": CategoricalIndex(values),\n            \""monotonic_decr\"": CategoricalIndex(reversed(values)),\n            \""non_monotonic\"": CategoricalIndex(list(\""abc\"" * N)),\n        }\n        self.data = indices[index]\n        self.data_unique = CategoricalIndex([str(i) for i in range(N * 3)])\n    \n        self.int_scalar = 10000\n        self.int_list = list(range(10000))\n    \n        self.cat_scalar = \""b\""\n        self.cat_list = [\""1\"", \""3\""]"", ""indexing.CategoricalIndexIndexing.time_getitem_scalar"": ""class CategoricalIndexIndexing:\n    def time_getitem_scalar(self, index):\n        self.data[self.int_scalar]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalIndexIndexing:\n    def setup(self, index):\n        N = 10**5\n        values = list(\""a\"" * N + \""b\"" * N + \""c\"" * N)\n        indices = {\n            \""monotonic_incr\"": CategoricalIndex(values),\n            \""monotonic_decr\"": CategoricalIndex(reversed(values)),\n            \""non_monotonic\"": CategoricalIndex(list(\""abc\"" * N)),\n        }\n        self.data = indices[index]\n        self.data_unique = CategoricalIndex([str(i) for i in range(N * 3)])\n    \n        self.int_scalar = 10000\n        self.int_list = list(range(10000))\n    \n        self.cat_scalar = \""b\""\n        self.cat_list = [\""1\"", \""3\""]""}",indexing.CategoricalIndexIndexing,time,0.012839272888265097
24393,class-level,"terminus-2,claude",1.049748217587794,1.0181540018946085,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NonNumericSeriesIndexing,indexing,"{""indexing.NonNumericSeriesIndexing.time_getitem_label_slice"": ""class NonNumericSeriesIndexing:\n    def time_getitem_label_slice(self, index, index_structure):\n        self.s[: self.lbl]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]"", ""indexing.NonNumericSeriesIndexing.time_getitem_list_like"": ""class NonNumericSeriesIndexing:\n    def time_getitem_list_like(self, index, index_structure):\n        self.s[[self.lbl]]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]"", ""indexing.NonNumericSeriesIndexing.time_getitem_pos_slice"": ""class NonNumericSeriesIndexing:\n    def time_getitem_pos_slice(self, index, index_structure):\n        self.s[:80000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]"", ""indexing.NonNumericSeriesIndexing.time_getitem_scalar"": ""class NonNumericSeriesIndexing:\n    def time_getitem_scalar(self, index, index_structure):\n        self.s[self.lbl]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]""}",indexing.NonNumericSeriesIndexing,time,0.022343858340300875
24388,class-level,"terminus-2,claude",1.1700522916098166,1.123397506290995,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.DataFrameStringIndexing,indexing,"{""indexing.DataFrameStringIndexing.time_at_setitem"": ""class DataFrameStringIndexing:\n    def time_at_setitem(self):\n        self.df.at[self.idx_scalar, self.col_scalar] = 0.0\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DataFrameStringIndexing:\n    def setup(self):\n        index = tm.makeStringIndex(1000)\n        columns = tm.makeStringIndex(30)\n        with warnings.catch_warnings(record=True):\n            self.df = DataFrame(np.random.randn(1000, 30), index=index, columns=columns)\n        self.idx_scalar = index[100]\n        self.col_scalar = columns[10]\n        self.bool_indexer = self.df[self.col_scalar] > 0\n        self.bool_obj_indexer = self.bool_indexer.astype(object)\n        self.boolean_indexer = (self.df[self.col_scalar] > 0).astype(\""boolean\"")""}",indexing.DataFrameStringIndexing,time,0.032994897679506016
24380,class-level,"terminus-2,claude",1.0661276571731069,0.7453157545798069,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.RankWithTies,groupby,"{""groupby.RankWithTies.time_rank_ties"": ""class RankWithTies:\n    def time_rank_ties(self, dtype, tie_method):\n        self.df.groupby(\""key\"").rank(method=tie_method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass RankWithTies:\n    def setup(self, dtype, tie_method):\n        N = 10**4\n        if dtype == \""datetime64\"":\n            data = np.array([Timestamp(\""2011/01/01\"")] * N, dtype=dtype)\n        else:\n            data = np.array([1] * N, dtype=dtype)\n        self.df = DataFrame({\""values\"": data, \""key\"": [\""foo\""] * N})""}",groupby.RankWithTies,time,0.22688253365862798
24389,class-level,"terminus-2,claude",0.983835109845662,0.9839495425121074,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.GetItemSingleColumn,indexing,"{""indexing.GetItemSingleColumn.time_frame_getitem_single_column_int"": ""class GetItemSingleColumn:\n    def time_frame_getitem_single_column_int(self):\n        self.df_int_col[0]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GetItemSingleColumn:\n    def setup(self):\n        self.df_string_col = DataFrame(np.random.randn(3000, 1), columns=[\""A\""])\n        self.df_int_col = DataFrame(np.random.randn(3000, 1))""}",indexing.GetItemSingleColumn,time,-8.092833553422667e-05
24384,class-level,"terminus-2,claude",1.0971801607389584,1.0227767447743357,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,hash_functions.NumericSeriesIndexing,hash_functions,"{""hash_functions.NumericSeriesIndexing.time_loc_slice"": ""class NumericSeriesIndexing:\n    def time_loc_slice(self, index, N):\n        # trigger building of mapping\n        self.data.loc[:800]\n\n    def setup(self, dtype, N):\n        vals = np.array(list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype)\n        indices = pd.Index(vals)\n        self.data = pd.Series(np.arange(N), index=indices)""}",hash_functions.NumericSeriesIndexing,time,0.0526191060570175
24391,class-level,"terminus-2,claude",0.9710537629499162,0.9748419732721316,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.MethodLookup,indexing,"{""indexing.MethodLookup.time_lookup_iloc"": ""class MethodLookup:\n    def time_lookup_iloc(self, s):\n        s.iloc\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MethodLookup:\n    def setup_cache(self):\n        s = Series()\n        return s"", ""indexing.MethodLookup.time_lookup_loc"": ""class MethodLookup:\n    def time_lookup_loc(self, s):\n        s.loc\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MethodLookup:\n    def setup_cache(self):\n        s = Series()\n        return s""}",indexing.MethodLookup,time,-0.0026790737780872952
24386,class-level,"terminus-2,claude",1.2428012749308386,1.0570132078439995,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_cached_properties.IndexCache,index_cached_properties,"{""index_cached_properties.IndexCache.time_engine"": ""class IndexCache:\n    def time_engine(self, index_type):\n        self.idx._engine\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}"", ""index_cached_properties.IndexCache.time_inferred_type"": ""class IndexCache:\n    def time_inferred_type(self, index_type):\n        self.idx.inferred_type\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}"", ""index_cached_properties.IndexCache.time_is_monotonic_decreasing"": ""class IndexCache:\n    def time_is_monotonic_decreasing(self, index_type):\n        self.idx.is_monotonic_decreasing\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}"", ""index_cached_properties.IndexCache.time_is_monotonic_increasing"": ""class IndexCache:\n    def time_is_monotonic_increasing(self, index_type):\n        self.idx.is_monotonic_increasing\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}"", ""index_cached_properties.IndexCache.time_shape"": ""class IndexCache:\n    def time_shape(self, index_type):\n        self.idx.shape\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}"", ""index_cached_properties.IndexCache.time_values"": ""class IndexCache:\n    def time_values(self, index_type):\n        self.idx._values\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}""}",index_cached_properties.IndexCache,time,0.1313918437672129
24396,class-level,"terminus-2,claude",0.9976982308082416,1.0389840824597911,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing_engines.MaskedNumericEngineIndexing,indexing_engines,"{""indexing_engines.MaskedNumericEngineIndexing.time_get_loc"": ""class MaskedNumericEngineIndexing:\n    def time_get_loc(self, engine_and_dtype, index_type, unique, N):\n        self.data.get_loc(self.key_early)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())[::-1]\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.zeros(N * 3, dtype=dtype.lower())\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype.lower())\n                arr[N:] = np.arange(N * 2, dtype=dtype.lower())\n    \n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n            mask[-1] = True\n    \n        self.data = engine(BaseMaskedArray(arr, mask))\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]"", ""indexing_engines.MaskedNumericEngineIndexing.time_get_loc_near_middle"": ""class MaskedNumericEngineIndexing:\n    def time_get_loc_near_middle(self, engine_and_dtype, index_type, unique, N):\n        # searchsorted performance may be different near the middle of a range\n        #  vs near an endpoint\n        self.data.get_loc(self.key_middle)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())[::-1]\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.zeros(N * 3, dtype=dtype.lower())\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype.lower())\n                arr[N:] = np.arange(N * 2, dtype=dtype.lower())\n    \n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n            mask[-1] = True\n    \n        self.data = engine(BaseMaskedArray(arr, mask))\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]""}",indexing_engines.MaskedNumericEngineIndexing,time,-0.02919791488794167
24398,class-level,"terminus-2,claude",1.0232202472077538,1.0287380341736552,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,inference.ToNumericDowncast,inference,"{""inference.ToNumericDowncast.time_downcast"": ""class ToNumericDowncast:\n    def time_downcast(self, dtype, downcast):\n        to_numeric(self.data, downcast=downcast)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToNumericDowncast:\n    def setup(self, dtype, downcast):\n        self.data = self.data_dict[dtype]""}",inference.ToNumericDowncast,time,-0.003902253865559638
24390,class-level,"terminus-2,claude",1.0230200839080663,1.038504163552764,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.IndexSingleRow,indexing,"{""indexing.IndexSingleRow.time_loc_row"": ""class IndexSingleRow:\n    def time_loc_row(self, unique_cols):\n        self.df.loc[10000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IndexSingleRow:\n    def setup(self, unique_cols):\n        arr = np.arange(10**7).reshape(-1, 10)\n        df = DataFrame(arr)\n        dtypes = [\""u1\"", \""u2\"", \""u4\"", \""u8\"", \""i1\"", \""i2\"", \""i4\"", \""i8\"", \""f8\"", \""f4\""]\n        for i, d in enumerate(dtypes):\n            df[i] = df[i].astype(d)\n    \n        if not unique_cols:\n            # GH#33032 single-row lookups with non-unique columns were\n            #  15x slower than with unique columns\n            df.columns = [\""A\"", \""A\""] + list(df.columns[2:])\n    \n        self.df = df""}",indexing.IndexSingleRow,time,-0.010950551375316676
24401,class-level,"terminus-2,claude",1.035572857603586,1.065536048258366,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.json.NormalizeJSON,io.json,"{""io.json.NormalizeJSON.time_normalize_json"": ""class NormalizeJSON:\n    def time_normalize_json(self, orient, frame):\n        json_normalize(self.data)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NormalizeJSON:\n    def setup(self, orient, frame):\n        data = {\n            \""hello\"": [\""thisisatest\"", 999898, \""mixed types\""],\n            \""nest1\"": {\""nest2\"": {\""nest3\"": \""nest3_value\"", \""nest3_int\"": 3445}},\n            \""nest1_list\"": {\""nest2\"": [\""blah\"", 32423, 546456.876, 92030234]},\n            \""hello2\"": \""string\"",\n        }\n        self.data = [data for i in range(10000)]""}",io.json.NormalizeJSON,time,-0.02119037528626586
24392,class-level,"terminus-2,claude",1.1974388419001345,1.052174150069658,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.MultiIndexing,indexing,"{""indexing.MultiIndexing.time_loc_all_scalars"": ""class MultiIndexing:\n    def time_loc_all_scalars(self, unique_levels):\n        target = tuple([self.tgt_scalar] * self.nlevels)\n        self.df.loc[target, :]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MultiIndexing:\n    def setup(self, unique_levels):\n        self.nlevels = 2\n        if unique_levels:\n            mi = MultiIndex.from_arrays([range(1000000)] * self.nlevels)\n        else:\n            mi = MultiIndex.from_product([range(1000)] * self.nlevels)\n        self.df = DataFrame(np.random.randn(len(mi)), index=mi)\n    \n        self.tgt_slice = slice(200, 800)\n        self.tgt_null_slice = slice(None)\n        self.tgt_list = list(range(0, 1000, 10))\n        self.tgt_scalar = 500\n    \n        bool_indexer = np.zeros(len(mi), dtype=np.bool_)\n        bool_indexer[slice(0, len(mi), 100)] = True\n        self.tgt_bool_indexer = bool_indexer""}",indexing.MultiIndexing,time,0.10273316253923376
24383,class-level,"terminus-2,claude",1.053843992194949,1.042586536069814,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.Transform,groupby,"{""groupby.Transform.time_transform_lambda_max"": ""class Transform:\n    def time_transform_lambda_max(self):\n        self.df.groupby(level=\""lev1\"").transform(lambda x: max(x))\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]"", ""groupby.Transform.time_transform_lambda_max_wide"": ""class Transform:\n    def time_transform_lambda_max_wide(self):\n        self.df_wide.groupby(level=0).transform(lambda x: np.max(x, axis=0))\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]"", ""groupby.Transform.time_transform_multi_key1"": ""class Transform:\n    def time_transform_multi_key1(self):\n        self.df1.groupby([\""jim\"", \""joe\""])[\""jolie\""].transform(\""max\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]"", ""groupby.Transform.time_transform_multi_key2"": ""class Transform:\n    def time_transform_multi_key2(self):\n        self.df2.groupby([\""jim\"", \""joe\""])[\""jolie\""].transform(\""max\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]"", ""groupby.Transform.time_transform_multi_key3"": ""class Transform:\n    def time_transform_multi_key3(self):\n        self.df3.groupby([\""jim\"", \""joe\""])[\""jolie\""].transform(\""max\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]"", ""groupby.Transform.time_transform_ufunc_max"": ""class Transform:\n    def time_transform_ufunc_max(self):\n        self.df.groupby(level=\""lev1\"").transform(np.max)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]""}",groupby.Transform,time,0.00796142583107132
24394,class-level,"terminus-2,claude",1.27131367101806,1.2314549995620576,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericMaskedIndexing,indexing,"{""indexing.NumericMaskedIndexing.time_get_indexer"": ""class NumericMaskedIndexing:\n    def time_get_indexer(self, dtype, monotonic):\n        self.data.get_indexer(self.indexer)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericMaskedIndexing:\n    def setup(self, dtype, monotonic):\n        indices = {\n            True: Index(self.monotonic_list, dtype=dtype),\n            False: Index(self.non_monotonic_list, dtype=dtype).append(\n                Index([NA], dtype=dtype)\n            ),\n        }\n        self.data = indices[monotonic]\n        self.indexer = np.arange(300, 1_000)\n        self.data_dups = self.data.append(self.data)"", ""indexing.NumericMaskedIndexing.time_get_indexer_dups"": ""class NumericMaskedIndexing:\n    def time_get_indexer_dups(self, dtype, monotonic):\n        self.data.get_indexer_for(self.indexer)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericMaskedIndexing:\n    def setup(self, dtype, monotonic):\n        indices = {\n            True: Index(self.monotonic_list, dtype=dtype),\n            False: Index(self.non_monotonic_list, dtype=dtype).append(\n                Index([NA], dtype=dtype)\n            ),\n        }\n        self.data = indices[monotonic]\n        self.indexer = np.arange(300, 1_000)\n        self.data_dups = self.data.append(self.data)""}",indexing.NumericMaskedIndexing,time,0.028188593674683383
24407,class-level,"terminus-2,claude",0.9920469880372312,1.0221675206886116,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.Explode,reshape,"{""reshape.Explode.time_explode"": ""class Explode:\n    def time_explode(self, n_rows, max_list_length):\n        self.series.explode()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Explode:\n    def setup(self, n_rows, max_list_length):\n        data = [np.arange(np.random.randint(max_list_length)) for _ in range(n_rows)]\n        self.series = pd.Series(data)""}",reshape.Explode,time,-0.021301649682730107
24397,class-level,"terminus-2,claude",0.976440471749312,0.9812345114538252,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing_engines.NumericEngineIndexing,indexing_engines,"{""indexing_engines.NumericEngineIndexing.time_get_loc"": ""class NumericEngineIndexing:\n    def time_get_loc(self, engine_and_dtype, index_type, unique, N):\n        self.data.get_loc(self.key_early)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)[::-1]\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.empty(N * 3, dtype=dtype)\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype)\n                arr[N:] = np.arange(N * 2, dtype=dtype)\n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype)\n    \n        self.data = engine(arr)\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]"", ""indexing_engines.NumericEngineIndexing.time_get_loc_near_middle"": ""class NumericEngineIndexing:\n    def time_get_loc_near_middle(self, engine_and_dtype, index_type, unique, N):\n        # searchsorted performance may be different near the middle of a range\n        #  vs near an endpoint\n        self.data.get_loc(self.key_middle)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)[::-1]\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.empty(N * 3, dtype=dtype)\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype)\n                arr[N:] = np.arange(N * 2, dtype=dtype)\n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype)\n    \n        self.data = engine(arr)\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]""}",indexing_engines.NumericEngineIndexing,time,-0.003390409974903252
24405,class-level,"terminus-2,claude",0.9558450517631384,0.9662950819021658,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,multiindex_object.Integer,multiindex_object,"{""multiindex_object.Integer.time_is_monotonic"": ""class Integer:\n    def time_is_monotonic(self):\n        self.mi_int.is_monotonic_increasing\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Integer:\n    def setup(self):\n        self.mi_int = MultiIndex.from_product(\n            [np.arange(1000), np.arange(1000)], names=[\""one\"", \""two\""]\n        )\n        self.obj_index = np.array(\n            [\n                (0, 10),\n                (0, 11),\n                (0, 12),\n                (0, 13),\n                (0, 14),\n                (0, 15),\n                (0, 16),\n                (0, 17),\n                (0, 18),\n                (0, 19),\n            ],\n            dtype=object,\n        )\n        self.other_mi_many_mismatches = MultiIndex.from_tuples(\n            [\n                (-7, 41),\n                (-2, 3),\n                (-0.7, 5),\n                (0, 0),\n                (0, 1.5),\n                (0, 340),\n                (0, 1001),\n                (1, -4),\n                (1, 20),\n                (1, 1040),\n                (432, -5),\n                (432, 17),\n                (439, 165.5),\n                (998, -4),\n                (998, 24065),\n                (999, 865.2),\n                (999, 1000),\n                (1045, -843),\n            ]\n        )""}",multiindex_object.Integer,time,-0.00739040321006185
24402,class-level,"terminus-2,claude",0.9375712081637044,0.97232287374828,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.style.Render,io.style,"{""io.style.Render.time_apply_format_hide_render"": ""class Render:\n    def time_apply_format_hide_render(self, cols, rows):\n        self._style_apply_format_hide()\n        self.st._render_html(True, True)\n\n    def setup(self, cols, rows):\n        self.df = DataFrame(\n            np.random.randn(rows, cols),\n            columns=[f\""float_{i+1}\"" for i in range(cols)],\n            index=[f\""row_{i+1}\"" for i in range(rows)],\n        )"", ""io.style.Render.time_apply_render"": ""class Render:\n    def time_apply_render(self, cols, rows):\n        self._style_apply()\n        self.st._render_html(True, True)\n\n    def setup(self, cols, rows):\n        self.df = DataFrame(\n            np.random.randn(rows, cols),\n            columns=[f\""float_{i+1}\"" for i in range(cols)],\n            index=[f\""row_{i+1}\"" for i in range(rows)],\n        )"", ""io.style.Render.time_format_render"": ""class Render:\n    def time_format_render(self, cols, rows):\n        self._style_format()\n        self.st._render_html(True, True)\n\n    def setup(self, cols, rows):\n        self.df = DataFrame(\n            np.random.randn(rows, cols),\n            columns=[f\""float_{i+1}\"" for i in range(cols)],\n            index=[f\""row_{i+1}\"" for i in range(rows)],\n        )"", ""io.style.Render.time_tooltips_render"": ""class Render:\n    def time_tooltips_render(self, cols, rows):\n        self._style_tooltips()\n        self.st._render_html(True, True)\n\n    def setup(self, cols, rows):\n        self.df = DataFrame(\n            np.random.randn(rows, cols),\n            columns=[f\""float_{i+1}\"" for i in range(cols)],\n            index=[f\""row_{i+1}\"" for i in range(rows)],\n        )""}",io.style.Render,time,-0.02457684977692754
24403,class-level,"terminus-2,claude",1.026893575991134,1.0409424730696608,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,join_merge.MergeAsof,join_merge,"{""join_merge.MergeAsof.time_by_int"": ""class MergeAsof:\n    def time_by_int(self, direction, tolerance):\n        merge_asof(\n            self.df1c,\n            self.df2c,\n            on=\""time\"",\n            by=\""key2\"",\n            direction=direction,\n            tolerance=tolerance,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MergeAsof:\n    def setup(self, direction, tolerance):\n        one_count = 200000\n        two_count = 1000000\n    \n        df1 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, one_count / 20, one_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), one_count),\n                \""key2\"": np.random.randint(0, 25, one_count),\n                \""value1\"": np.random.randn(one_count),\n            }\n        )\n        df2 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, two_count / 20, two_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), two_count),\n                \""key2\"": np.random.randint(0, 25, two_count),\n                \""value2\"": np.random.randn(two_count),\n            }\n        )\n    \n        df1 = df1.sort_values(\""time\"")\n        df2 = df2.sort_values(\""time\"")\n    \n        df1[\""time32\""] = np.int32(df1.time)\n        df2[\""time32\""] = np.int32(df2.time)\n    \n        df1[\""timeu64\""] = np.uint64(df1.time)\n        df2[\""timeu64\""] = np.uint64(df2.time)\n    \n        self.df1a = df1[[\""time\"", \""value1\""]]\n        self.df2a = df2[[\""time\"", \""value2\""]]\n        self.df1b = df1[[\""time\"", \""key\"", \""value1\""]]\n        self.df2b = df2[[\""time\"", \""key\"", \""value2\""]]\n        self.df1c = df1[[\""time\"", \""key2\"", \""value1\""]]\n        self.df2c = df2[[\""time\"", \""key2\"", \""value2\""]]\n        self.df1d = df1[[\""time32\"", \""value1\""]]\n        self.df2d = df2[[\""time32\"", \""value2\""]]\n        self.df1e = df1[[\""time\"", \""key\"", \""key2\"", \""value1\""]]\n        self.df2e = df2[[\""time\"", \""key\"", \""key2\"", \""value2\""]]\n        self.df1f = df1[[\""timeu64\"", \""value1\""]]\n        self.df2f = df2[[\""timeu64\"", \""value2\""]]"", ""join_merge.MergeAsof.time_by_object"": ""class MergeAsof:\n    def time_by_object(self, direction, tolerance):\n        merge_asof(\n            self.df1b,\n            self.df2b,\n            on=\""time\"",\n            by=\""key\"",\n            direction=direction,\n            tolerance=tolerance,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MergeAsof:\n    def setup(self, direction, tolerance):\n        one_count = 200000\n        two_count = 1000000\n    \n        df1 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, one_count / 20, one_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), one_count),\n                \""key2\"": np.random.randint(0, 25, one_count),\n                \""value1\"": np.random.randn(one_count),\n            }\n        )\n        df2 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, two_count / 20, two_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), two_count),\n                \""key2\"": np.random.randint(0, 25, two_count),\n                \""value2\"": np.random.randn(two_count),\n            }\n        )\n    \n        df1 = df1.sort_values(\""time\"")\n        df2 = df2.sort_values(\""time\"")\n    \n        df1[\""time32\""] = np.int32(df1.time)\n        df2[\""time32\""] = np.int32(df2.time)\n    \n        df1[\""timeu64\""] = np.uint64(df1.time)\n        df2[\""timeu64\""] = np.uint64(df2.time)\n    \n        self.df1a = df1[[\""time\"", \""value1\""]]\n        self.df2a = df2[[\""time\"", \""value2\""]]\n        self.df1b = df1[[\""time\"", \""key\"", \""value1\""]]\n        self.df2b = df2[[\""time\"", \""key\"", \""value2\""]]\n        self.df1c = df1[[\""time\"", \""key2\"", \""value1\""]]\n        self.df2c = df2[[\""time\"", \""key2\"", \""value2\""]]\n        self.df1d = df1[[\""time32\"", \""value1\""]]\n        self.df2d = df2[[\""time32\"", \""value2\""]]\n        self.df1e = df1[[\""time\"", \""key\"", \""key2\"", \""value1\""]]\n        self.df2e = df2[[\""time\"", \""key\"", \""key2\"", \""value2\""]]\n        self.df1f = df1[[\""timeu64\"", \""value1\""]]\n        self.df2f = df2[[\""timeu64\"", \""value2\""]]"", ""join_merge.MergeAsof.time_on_int32"": ""class MergeAsof:\n    def time_on_int32(self, direction, tolerance):\n        merge_asof(\n            self.df1d, self.df2d, on=\""time32\"", direction=direction, tolerance=tolerance\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MergeAsof:\n    def setup(self, direction, tolerance):\n        one_count = 200000\n        two_count = 1000000\n    \n        df1 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, one_count / 20, one_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), one_count),\n                \""key2\"": np.random.randint(0, 25, one_count),\n                \""value1\"": np.random.randn(one_count),\n            }\n        )\n        df2 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, two_count / 20, two_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), two_count),\n                \""key2\"": np.random.randint(0, 25, two_count),\n                \""value2\"": np.random.randn(two_count),\n            }\n        )\n    \n        df1 = df1.sort_values(\""time\"")\n        df2 = df2.sort_values(\""time\"")\n    \n        df1[\""time32\""] = np.int32(df1.time)\n        df2[\""time32\""] = np.int32(df2.time)\n    \n        df1[\""timeu64\""] = np.uint64(df1.time)\n        df2[\""timeu64\""] = np.uint64(df2.time)\n    \n        self.df1a = df1[[\""time\"", \""value1\""]]\n        self.df2a = df2[[\""time\"", \""value2\""]]\n        self.df1b = df1[[\""time\"", \""key\"", \""value1\""]]\n        self.df2b = df2[[\""time\"", \""key\"", \""value2\""]]\n        self.df1c = df1[[\""time\"", \""key2\"", \""value1\""]]\n        self.df2c = df2[[\""time\"", \""key2\"", \""value2\""]]\n        self.df1d = df1[[\""time32\"", \""value1\""]]\n        self.df2d = df2[[\""time32\"", \""value2\""]]\n        self.df1e = df1[[\""time\"", \""key\"", \""key2\"", \""value1\""]]\n        self.df2e = df2[[\""time\"", \""key\"", \""key2\"", \""value2\""]]\n        self.df1f = df1[[\""timeu64\"", \""value1\""]]\n        self.df2f = df2[[\""timeu64\"", \""value2\""]]"", ""join_merge.MergeAsof.time_on_uint64"": ""class MergeAsof:\n    def time_on_uint64(self, direction, tolerance):\n        merge_asof(\n            self.df1f, self.df2f, on=\""timeu64\"", direction=direction, tolerance=tolerance\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MergeAsof:\n    def setup(self, direction, tolerance):\n        one_count = 200000\n        two_count = 1000000\n    \n        df1 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, one_count / 20, one_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), one_count),\n                \""key2\"": np.random.randint(0, 25, one_count),\n                \""value1\"": np.random.randn(one_count),\n            }\n        )\n        df2 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, two_count / 20, two_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), two_count),\n                \""key2\"": np.random.randint(0, 25, two_count),\n                \""value2\"": np.random.randn(two_count),\n            }\n        )\n    \n        df1 = df1.sort_values(\""time\"")\n        df2 = df2.sort_values(\""time\"")\n    \n        df1[\""time32\""] = np.int32(df1.time)\n        df2[\""time32\""] = np.int32(df2.time)\n    \n        df1[\""timeu64\""] = np.uint64(df1.time)\n        df2[\""timeu64\""] = np.uint64(df2.time)\n    \n        self.df1a = df1[[\""time\"", \""value1\""]]\n        self.df2a = df2[[\""time\"", \""value2\""]]\n        self.df1b = df1[[\""time\"", \""key\"", \""value1\""]]\n        self.df2b = df2[[\""time\"", \""key\"", \""value2\""]]\n        self.df1c = df1[[\""time\"", \""key2\"", \""value1\""]]\n        self.df2c = df2[[\""time\"", \""key2\"", \""value2\""]]\n        self.df1d = df1[[\""time32\"", \""value1\""]]\n        self.df2d = df2[[\""time32\"", \""value2\""]]\n        self.df1e = df1[[\""time\"", \""key\"", \""key2\"", \""value1\""]]\n        self.df2e = df2[[\""time\"", \""key\"", \""key2\"", \""value2\""]]\n        self.df1f = df1[[\""timeu64\"", \""value1\""]]\n        self.df2f = df2[[\""timeu64\"", \""value2\""]]""}",join_merge.MergeAsof,time,-0.00993557077689311
24399,class-level,"terminus-2,claude",1.0874673740712428,1.121376298596207,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.csv.ReadCSVCategorical,io.csv,"{""io.csv.ReadCSVCategorical.time_convert_post"": ""class ReadCSVCategorical:\n    def time_convert_post(self, engine):\n        read_csv(self.fname, engine=engine).apply(Categorical)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVCategorical:\n    def setup(self, engine):\n        N = 100000\n        group1 = [\""aaaaaaaa\"", \""bbbbbbb\"", \""cccccccc\"", \""dddddddd\"", \""eeeeeeee\""]\n        df = DataFrame(np.random.choice(group1, (N, 3)), columns=list(\""abc\""))\n        df.to_csv(self.fname, index=False)""}",io.csv.ReadCSVCategorical,time,-0.02398085185641032
24413,class-level,"terminus-2,claude",0.9298952524513872,1.0348813857642023,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Methods,rolling,"{""rolling.Methods.time_method"": ""class Methods:\n    def time_method(self, constructor, window_kwargs, dtype, method):\n        getattr(self.window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Methods:\n    def setup(self, constructor, window_kwargs, dtype, method):\n        N = 10**5\n        window, kwargs = window_kwargs\n        arr = (100 * np.random.random(N)).astype(dtype)\n        obj = getattr(pd, constructor)(arr)\n        self.window = getattr(obj, window)(**kwargs)""}",rolling.Methods,time,-0.07424761903310831
24406,class-level,"terminus-2,claude",1.0634776716049124,1.0770458031434804,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.Cut,reshape,"{""reshape.Cut.time_cut_datetime"": ""class Cut:\n    def time_cut_datetime(self, bins):\n        pd.cut(self.datetime_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Cut.time_cut_int"": ""class Cut:\n    def time_cut_int(self, bins):\n        pd.cut(self.int_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Cut.time_cut_interval"": ""class Cut:\n    def time_cut_interval(self, bins):\n        # GH 27668\n        pd.cut(self.int_series, self.interval_bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Cut.time_cut_timedelta"": ""class Cut:\n    def time_cut_timedelta(self, bins):\n        pd.cut(self.timedelta_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Cut.time_qcut_datetime"": ""class Cut:\n    def time_qcut_datetime(self, bins):\n        pd.qcut(self.datetime_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))""}",reshape.Cut,time,-0.009595566858959027
24411,class-level,"terminus-2,claude",1.0343581803854074,1.0459585804573994,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.ForwardWindowMethods,rolling,"{""rolling.ForwardWindowMethods.time_rolling"": ""class ForwardWindowMethods:\n    def time_rolling(self, constructor, window_size, dtype, method):\n        getattr(self.roll, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ForwardWindowMethods:\n    def setup(self, constructor, window_size, dtype, method):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=window_size)\n        self.roll = getattr(pd, constructor)(arr).rolling(window=indexer)""}",rolling.ForwardWindowMethods,time,-0.008203960446953349
24404,class-level,"terminus-2,claude",1.0138664535078776,1.0420834684650648,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,libs.ScalarListLike,libs,"{""libs.ScalarListLike.time_is_list_like"": ""class ScalarListLike:\n    def time_is_list_like(self, param):\n        is_list_like(param)"", ""libs.ScalarListLike.time_is_scalar"": ""class ScalarListLike:\n    def time_is_scalar(self, param):\n        is_scalar(param)""}",libs.ScalarListLike,time,-0.01995545612248034
24409,class-level,"terminus-2,claude",1.023523215951254,1.0680562094800006,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Apply,rolling,"{""rolling.Apply.time_rolling"": ""class Apply:\n    def time_rolling(self, constructor, window, dtype, function, raw):\n        self.roll.apply(function, raw=raw)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, constructor, window, dtype, function, raw):\n        N = 10**3\n        arr = (100 * np.random.random(N)).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)""}",rolling.Apply,time,-0.03149433771481374
24400,class-level,"terminus-2,claude",1.0255708562611685,1.0503761610002444,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.csv.ReadCSVEngine,io.csv,"{""io.csv.ReadCSVEngine.time_read_bytescsv"": ""class ReadCSVEngine:\n    def time_read_bytescsv(self, engine):\n        read_csv(self.data(self.BytesIO_input), engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVEngine:\n    def setup(self, engine):\n        data = [\""A,B,C,D,E\""] + ([\""1,2,3,4,5\""] * 100000)\n        self.StringIO_input = StringIO(\""\\n\"".join(data))\n        # simulate reading from file\n        self.BytesIO_input = BytesIO(self.StringIO_input.read().encode(\""utf-8\""))""}",io.csv.ReadCSVEngine,time,-0.017542648330322447
24408,class-level,"terminus-2,claude",1.0201766673229071,1.0345564193262995,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.WideToLong,reshape,"{""reshape.WideToLong.time_wide_to_long_big"": ""class WideToLong:\n    def time_wide_to_long_big(self):\n        wide_to_long(self.df, self.letters, i=\""id\"", j=\""year\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass WideToLong:\n    def setup(self):\n        nyrs = 20\n        nidvars = 20\n        N = 5000\n        self.letters = list(\""ABCD\"")\n        yrvars = [\n            letter + str(num)\n            for letter, num in product(self.letters, range(1, nyrs + 1))\n        ]\n        columns = [str(i) for i in range(nidvars)] + yrvars\n        self.df = DataFrame(np.random.randn(N, nidvars + len(yrvars)), columns=columns)\n        self.df[\""id\""] = self.df.index""}",reshape.WideToLong,time,-0.01016955587227183
24414,class-level,"terminus-2,claude",1.01023687017864,1.056450567090326,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Pairwise,rolling,"{""rolling.Pairwise.time_groupby"": ""class Pairwise:\n    def time_groupby(self, kwargs_window, method, pairwise):\n        getattr(self.window_group, method)(self.df, pairwise=pairwise)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Pairwise:\n    def setup(self, kwargs_window, method, pairwise):\n        N = 10**4\n        n_groups = 20\n        kwargs, window = kwargs_window\n        groups = [i for _ in range(N // n_groups) for i in range(n_groups)]\n        arr = np.random.random(N)\n        self.df = pd.DataFrame(arr)\n        self.window = getattr(self.df, window)(**kwargs)\n        self.window_group = getattr(\n            pd.DataFrame({\""A\"": groups, \""B\"": arr}).groupby(\""A\""), window\n        )(**kwargs)"", ""rolling.Pairwise.time_pairwise"": ""class Pairwise:\n    def time_pairwise(self, kwargs_window, method, pairwise):\n        getattr(self.window, method)(self.df, pairwise=pairwise)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Pairwise:\n    def setup(self, kwargs_window, method, pairwise):\n        N = 10**4\n        n_groups = 20\n        kwargs, window = kwargs_window\n        groups = [i for _ in range(N // n_groups) for i in range(n_groups)]\n        arr = np.random.random(N)\n        self.df = pd.DataFrame(arr)\n        self.window = getattr(self.df, window)(**kwargs)\n        self.window_group = getattr(\n            pd.DataFrame({\""A\"": groups, \""B\"": arr}).groupby(\""A\""), window\n        )(**kwargs)""}",rolling.Pairwise,time,-0.03268295396866052
24416,class-level,"terminus-2,claude",1.0135964294916813,1.024648637049011,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Rank,rolling,"{""rolling.Rank.time_rank"": ""class Rank:\n    def time_rank(self, constructor, window, dtype, percentile, ascending, method):\n        self.roll.rank(pct=percentile, ascending=ascending, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Rank:\n    def setup(self, constructor, window, dtype, percentile, ascending, method):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)""}",rolling.Rank,time,-0.00781627125695179
24415,class-level,"terminus-2,claude",1.0336586160589174,1.0647197439195957,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Quantile,rolling,"{""rolling.Quantile.time_quantile"": ""class Quantile:\n    def time_quantile(self, constructor, window, dtype, percentile, interpolation):\n        self.roll.quantile(percentile, interpolation=interpolation)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, constructor, window, dtype, percentile, interpolation):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)""}",rolling.Quantile,time,-0.0219668513866183
24410,class-level,"terminus-2,claude",1.0720491588329937,1.0919853885459143,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.EWMMethods,rolling,"{""rolling.EWMMethods.time_ewm"": ""class EWMMethods:\n    def time_ewm(self, constructor, kwargs_method, dtype):\n        getattr(self.ewm, self.method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass EWMMethods:\n    def setup(self, constructor, kwargs_method, dtype):\n        N = 10**5\n        kwargs, method = kwargs_method\n        arr = (100 * np.random.random(N)).astype(dtype)\n        self.method = method\n        self.ewm = getattr(pd, constructor)(arr).ewm(**kwargs)""}",rolling.EWMMethods,time,-0.014099172357086734
24418,class-level,"terminus-2,claude",1.032162152737261,1.069441903454608,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.NanOps,series_methods,"{""series_methods.NanOps.time_func"": ""class NanOps:\n    def time_func(self, func, N, dtype):\n        self.func()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NanOps:\n    def setup(self, func, N, dtype):\n        if func == \""argmax\"" and dtype in {\""Int64\"", \""boolean\""}:\n            # Skip argmax for nullable int since this doesn't work yet (GH-24382)\n            raise NotImplementedError\n        self.s = Series([1] * N, dtype=dtype)\n        self.func = getattr(self.s, func)""}",series_methods.NanOps,time,-0.02636474591042929
24412,class-level,"terminus-2,claude",1.0722471169548256,1.089453826648073,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Groupby,rolling,"{""rolling.Groupby.time_method"": ""class Groupby:\n    def time_method(self, method, window_kwargs):\n        getattr(self.groupby_window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Groupby:\n    def setup(self, method, window_kwargs):\n        N = 1000\n        window, kwargs = window_kwargs\n        df = pd.DataFrame(\n            {\n                \""A\"": [str(i) for i in range(N)] * 10,\n                \""B\"": list(range(N)) * 10,\n            }\n        )\n        if isinstance(kwargs.get(\""window\"", None), str):\n            df.index = pd.date_range(start=\""1900-01-01\"", freq=\""1min\"", periods=N * 10)\n        self.groupby_window = getattr(df.groupby(\""A\""), window)(**kwargs)""}",rolling.Groupby,time,-0.012168818736384411
24420,class-level,"terminus-2,claude",0.9053921944453632,0.9150649934375992,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.Arithmetic,sparse,"{""sparse.Arithmetic.time_add"": ""class Arithmetic:\n    def time_add(self, dense_proportion, fill_value):\n        self.array1 + self.array2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)"", ""sparse.Arithmetic.time_divide"": ""class Arithmetic:\n    def time_divide(self, dense_proportion, fill_value):\n        self.array1 / self.array2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)"", ""sparse.Arithmetic.time_make_union"": ""class Arithmetic:\n    def time_make_union(self, dense_proportion, fill_value):\n        self.array1.sp_index.make_union(self.array2.sp_index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)""}",sparse.Arithmetic,time,-0.006840734789417216
24425,class-level,"terminus-2,claude",0.962042834112159,0.9594607562437548,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.DatetimeAccessor,timeseries,"{""timeseries.DatetimeAccessor.time_dt_accessor"": ""class DatetimeAccessor:\n    def time_dt_accessor(self, tz):\n        self.series.dt\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeAccessor:\n    def setup(self, tz):\n        N = 100000\n        self.series = Series(date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"", tz=tz))"", ""timeseries.DatetimeAccessor.time_dt_accessor_month_name"": ""class DatetimeAccessor:\n    def time_dt_accessor_month_name(self, tz):\n        self.series.dt.month_name()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeAccessor:\n    def setup(self, tz):\n        N = 100000\n        self.series = Series(date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"", tz=tz))"", ""timeseries.DatetimeAccessor.time_dt_accessor_year"": ""class DatetimeAccessor:\n    def time_dt_accessor_year(self, tz):\n        self.series.dt.year\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeAccessor:\n    def setup(self, tz):\n        N = 100000\n        self.series = Series(date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"", tz=tz))""}",timeseries.DatetimeAccessor,time,0.0018260805292815718
24422,class-level,"terminus-2,claude",0.9810111873552728,1.0204791059132996,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strftime.BusinessHourStrftime,strftime,"{""strftime.BusinessHourStrftime.time_frame_offset_repr"": ""class BusinessHourStrftime:\n    def time_frame_offset_repr(self, obs):\n        self.data[\""off\""].apply(repr)\n\n    def setup(self, obs):\n        self.data = pd.DataFrame(\n            {\n                \""off\"": [offsets.BusinessHour()] * obs,\n            }\n        )""}",strftime.BusinessHourStrftime,time,-0.027912247919396595
24423,class-level,"terminus-2,claude",1.008567272346436,1.0198460204179998,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Dummies,strings,"{""strings.Dummies.time_get_dummies"": ""class Dummies:\n    def time_get_dummies(self, dtype):\n        self.s.str.get_dummies(\""|\"")\n\n    def setup(self, dtype):\n        super().setup(dtype)\n        self.s = self.s.str.join(\""|\"")""}",strings.Dummies,time,-0.00797648378469852
24421,class-level,"terminus-2,claude",0.8981767738817392,0.904440968977416,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.ArithmeticBlock,sparse,"{""sparse.ArithmeticBlock.time_addition"": ""class ArithmeticBlock:\n    def time_addition(self, fill_value):\n        self.arr1 + self.arr2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )"", ""sparse.ArithmeticBlock.time_division"": ""class ArithmeticBlock:\n    def time_division(self, fill_value):\n        self.arr1 / self.arr2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )"", ""sparse.ArithmeticBlock.time_make_union"": ""class ArithmeticBlock:\n    def time_make_union(self, fill_value):\n        self.arr1.sp_index.make_union(self.arr2.sp_index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )""}",sparse.ArithmeticBlock,time,-0.0044301238300401505
24426,class-level,"terminus-2,claude",0.9814183275177512,0.9882406520537336,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.DatetimeIndex,timeseries,"{""timeseries.DatetimeIndex.time_get"": ""class DatetimeIndex:\n    def time_get(self, index_type):\n        self.index[0]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]"", ""timeseries.DatetimeIndex.time_normalize"": ""class DatetimeIndex:\n    def time_normalize(self, index_type):\n        self.index.normalize()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]"", ""timeseries.DatetimeIndex.time_timeseries_is_month_start"": ""class DatetimeIndex:\n    def time_timeseries_is_month_start(self, index_type):\n        self.index.is_month_start\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]"", ""timeseries.DatetimeIndex.time_to_date"": ""class DatetimeIndex:\n    def time_to_date(self, index_type):\n        self.index.date\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]"", ""timeseries.DatetimeIndex.time_unique"": ""class DatetimeIndex:\n    def time_unique(self, index_type):\n        self.index.unique()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]""}",timeseries.DatetimeIndex,time,-0.0048248405487853014
24419,class-level,"terminus-2,claude",1.065313482244743,1.1004792458334434,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.SearchSorted,series_methods,"{""series_methods.SearchSorted.time_searchsorted"": ""class SearchSorted:\n    def time_searchsorted(self, dtype):\n        key = \""2\"" if dtype == \""str\"" else 2\n        self.s.searchsorted(key)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SearchSorted:\n    def setup(self, dtype):\n        N = 10**5\n        data = np.array([1] * N + [2] * N + [3] * N).astype(dtype)\n        self.s = Series(data)""}",series_methods.SearchSorted,time,-0.02486970550827467
24439,class-level,"terminus-2,claude",0.9888283744010244,1.020841996522358,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampOps,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_ceil"": ""class TimestampOps:\n    def time_ceil(self, tz):\n        self.ts.ceil(\""5T\"")\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_floor"": ""class TimestampOps:\n    def time_floor(self, tz):\n        self.ts.floor(\""5T\"")\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_normalize"": ""class TimestampOps:\n    def time_normalize(self, tz):\n        self.ts.normalize()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_replace_tz"": ""class TimestampOps:\n    def time_replace_tz(self, tz):\n        self.ts.replace(tzinfo=pytz.timezone(\""US/Eastern\""))\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_to_julian_date"": ""class TimestampOps:\n    def time_to_julian_date(self, tz):\n        self.ts.to_julian_date()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_tz_convert"": ""class TimestampOps:\n    def time_tz_convert(self, tz):\n        if self.ts.tz is not None:\n            self.ts.tz_convert(tz)\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_tz_localize"": ""class TimestampOps:\n    def time_tz_localize(self, tz):\n        if self.ts.tz is None:\n            self.ts.tz_localize(tz)\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.TimestampOps,time,-0.022640468261197758
24437,class-level,"terminus-2,claude",1.005809507856748,1.0156490510826353,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timedelta.TimedeltaConstructor,tslibs.timedelta,"{""tslibs.timedelta.TimedeltaConstructor.time_from_datetime_timedelta"": ""class TimedeltaConstructor:\n    def time_from_datetime_timedelta(self):\n        Timedelta(self.dttimedelta)\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")"", ""tslibs.timedelta.TimedeltaConstructor.time_from_unit"": ""class TimedeltaConstructor:\n    def time_from_unit(self):\n        Timedelta(1, unit=\""d\"")\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")""}",tslibs.timedelta.TimedeltaConstructor,time,-0.006958658575592161
24436,class-level,"terminus-2,claude",0.9740867278162968,0.9958619199845872,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.resolution.TimeResolution,tslibs.resolution,"{""tslibs.resolution.TimeResolution.time_get_resolution"": ""class TimeResolution:\n    def time_get_resolution(self, unit, size, tz):\n        get_resolution(self.i8data, tz)\n\n    def setup(self, unit, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        arr = arr.view(f\""M8[{unit}]\"").astype(\""M8[ns]\"").view(\""i8\"")\n        self.i8data = arr""}",tslibs.resolution.TimeResolution,time,-0.015399711575877214
24424,class-level,"terminus-2,claude",1.0671928049767203,1.1247370842666709,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Methods,strings,"{""strings.Methods.time_count"": ""class Methods:\n    def time_count(self, dtype):\n        self.s.str.count(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_endswith"": ""class Methods:\n    def time_endswith(self, dtype):\n        self.s.str.endswith(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_isdecimal"": ""class Methods:\n    def time_isdecimal(self, dtype):\n        self.s.str.isdecimal()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_join"": ""class Methods:\n    def time_join(self, dtype):\n        self.s.str.join(\"" \"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_lower"": ""class Methods:\n    def time_lower(self, dtype):\n        self.s.str.lower()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_replace"": ""class Methods:\n    def time_replace(self, dtype):\n        self.s.str.replace(\""A\"", \""\\x01\\x01\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_rpartition"": ""class Methods:\n    def time_rpartition(self, dtype):\n        self.s.str.rpartition(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_upper"": ""class Methods:\n    def time_upper(self, dtype):\n        self.s.str.upper()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_wrap"": ""class Methods:\n    def time_wrap(self, dtype):\n        self.s.str.wrap(10)\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.Methods,time,-0.04069609567889006
24432,class-level,"terminus-2,claude",0.9846621274957932,0.9637683780969988,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.OnOffset,tslibs.offsets,"{""tslibs.offsets.OnOffset.time_on_offset"": ""class OnOffset:\n    def time_on_offset(self, offset):\n        for date in self.dates:\n            offset.is_on_offset(date)\n\n    def setup(self, offset):\n        self.dates = [\n            datetime(2016, m, d)\n            for m in [10, 11, 12]\n            for d in [1, 2, 3, 28, 29, 30, 31]\n            if not (m == 11 and d == 31)\n        ]""}",tslibs.offsets.OnOffset,time,0.01477634328061839
24427,class-level,"terminus-2,claude",1.0789464905606996,1.0714083894064304,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.TzLocalize,timeseries,"{""timeseries.TzLocalize.time_infer_dst"": ""class TzLocalize:\n    def time_infer_dst(self, tz):\n        self.index.tz_localize(tz, ambiguous=\""infer\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TzLocalize:\n    def setup(self, tz):\n        dst_rng = date_range(\n            start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n        )\n        self.index = date_range(start=\""10/29/2000\"", end=\""10/29/2000 00:59:59\"", freq=\""S\"")\n        self.index = self.index.append(dst_rng)\n        self.index = self.index.append(dst_rng)\n        self.index = self.index.append(\n            date_range(start=\""10/29/2000 2:00:00\"", end=\""10/29/2000 3:00:00\"", freq=\""S\"")\n        )""}",timeseries.TzLocalize,time,0.005331047492411052
24434,class-level,"terminus-2,claude",0.9770122721104428,0.9966201154783566,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.TimeDT64ArrToPeriodArr,tslibs.period,"{""tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr"": ""class TimeDT64ArrToPeriodArr:\n    def time_dt64arr_to_periodarr(self, size, freq, tz):\n        dt64arr_to_periodarr(self.i8values, freq, tz)\n\n    def setup(self, size, freq, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.arange(10, dtype=\""i8\"").repeat(size // 10)\n        self.i8values = arr""}",tslibs.period.TimeDT64ArrToPeriodArr,time,-0.013866933074903656
24417,class-level,"terminus-2,claude",1.0367516467908529,1.0685224038975427,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.VariableWindowMethods,rolling,"{""rolling.VariableWindowMethods.time_method"": ""class Methods:\n    def time_method(self, constructor, window_kwargs, dtype, method):\n        getattr(self.window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass VariableWindowMethods:\n    def setup(self, constructor, window, dtype, method):\n        N = 10**5\n        arr = (100 * np.random.random(N)).astype(dtype)\n        index = pd.date_range(\""2017-01-01\"", periods=N, freq=\""5s\"")\n        self.window = getattr(pd, constructor)(arr, index=index).rolling(window)""}",rolling.VariableWindowMethods,time,-0.022468710825098912
24429,class-level,"terminus-2,claude",1.0175278213569792,1.0255275966642323,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.fields.TimeGetStartEndField,tslibs.fields,"{""tslibs.fields.TimeGetStartEndField.time_get_start_end_field"": ""class TimeGetStartEndField:\n    def time_get_start_end_field(self, size, side, period, freqstr, month_kw):\n        get_start_end_field(self.i8data, self.attrname, freqstr, month_kw=month_kw)\n\n    def setup(self, size, side, period, freqstr, month_kw):\n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr\n    \n        self.attrname = f\""is_{period}_{side}\""""}",tslibs.fields.TimeGetStartEndField,time,-0.005657549722244098
24428,class-level,"terminus-2,claude",0.9939602532093392,0.9749632137875638,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.fields.TimeGetDateField,tslibs.fields,"{""tslibs.fields.TimeGetDateField.time_get_date_field"": ""class TimeGetDateField:\n    def time_get_date_field(self, size, field):\n        get_date_field(self.i8data, field)\n\n    def setup(self, size, field):\n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.fields.TimeGetDateField,time,0.013434964230392817
24431,class-level,"terminus-2,claude",1.0423312615693856,1.0556626112451606,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.OffestDatetimeArithmetic,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_add"": ""class OffestDatetimeArithmetic:\n    def time_add(self, offset):\n        self.date + offset\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")"", ""tslibs.offsets.OffestDatetimeArithmetic.time_add_10"": ""class OffestDatetimeArithmetic:\n    def time_add_10(self, offset):\n        self.date + (10 * offset)\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")"", ""tslibs.offsets.OffestDatetimeArithmetic.time_add_np_dt64"": ""class OffestDatetimeArithmetic:\n    def time_add_np_dt64(self, offset):\n        offset + self.dt64\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")"", ""tslibs.offsets.OffestDatetimeArithmetic.time_subtract"": ""class OffestDatetimeArithmetic:\n    def time_subtract(self, offset):\n        self.date - offset\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")"", ""tslibs.offsets.OffestDatetimeArithmetic.time_subtract_10"": ""class OffestDatetimeArithmetic:\n    def time_subtract_10(self, offset):\n        self.date - (10 * offset)\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.OffestDatetimeArithmetic,time,-0.00942811151044907
24430,class-level,"terminus-2,claude",0.9710297826026658,0.964714718516657,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.normalize.Normalize,tslibs.normalize,"{""tslibs.normalize.Normalize.time_is_date_array_normalized"": ""class Normalize:\n    def time_is_date_array_normalized(self, size, tz):\n        # TODO: cases with different levels of short-circuiting\n        # 10 i.e. NPY_FR_ns\n        is_date_array_normalized(self.i8data, tz, 10)\n\n    def setup(self, size, tz):\n        # use an array that will have is_date_array_normalized give True,\n        #  so we do not short-circuit early.\n        dti = pd.date_range(\""2016-01-01\"", periods=10, tz=tz).repeat(size // 10)\n        self.i8data = dti.asi8\n    \n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError"", ""tslibs.normalize.Normalize.time_normalize_i8_timestamps"": ""class Normalize:\n    def time_normalize_i8_timestamps(self, size, tz):\n        # 10 i.e. NPY_FR_ns\n        normalize_i8_timestamps(self.i8data, tz, 10)\n\n    def setup(self, size, tz):\n        # use an array that will have is_date_array_normalized give True,\n        #  so we do not short-circuit early.\n        dti = pd.date_range(\""2016-01-01\"", periods=10, tz=tz).repeat(size // 10)\n        self.i8data = dti.asi8\n    \n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError""}",tslibs.normalize.Normalize,time,0.004466099070727576
24440,class-level,"terminus-2,claude",0.988336333281245,0.9965368412123108,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampProperties,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_dayofweek"": ""class TimestampProperties:\n    def time_dayofweek(self, tz):\n        self.ts.dayofweek\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_dayofyear"": ""class TimestampProperties:\n    def time_dayofyear(self, tz):\n        self.ts.dayofyear\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_days_in_month"": ""class TimestampProperties:\n    def time_days_in_month(self, tz):\n        self.ts.days_in_month\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_month_end"": ""class TimestampProperties:\n    def time_is_month_end(self, tz):\n        self.ts.is_month_end\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_month_start"": ""class TimestampProperties:\n    def time_is_month_start(self, tz):\n        self.ts.is_month_start\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_quarter_end"": ""class TimestampProperties:\n    def time_is_quarter_end(self, tz):\n        self.ts.is_quarter_end\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_quarter_start"": ""class TimestampProperties:\n    def time_is_quarter_start(self, tz):\n        self.ts.is_quarter_start\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_year_start"": ""class TimestampProperties:\n    def time_is_year_start(self, tz):\n        self.ts.is_year_start\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_microsecond"": ""class TimestampProperties:\n    def time_microsecond(self, tz):\n        self.ts.microsecond\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_month_name"": ""class TimestampProperties:\n    def time_month_name(self, tz):\n        self.ts.month_name()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_tz"": ""class TimestampProperties:\n    def time_tz(self, tz):\n        self.ts.tz\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_week"": ""class TimestampProperties:\n    def time_week(self, tz):\n        self.ts.week\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_weekday_name"": ""class TimestampProperties:\n    def time_weekday_name(self, tz):\n        self.ts.day_name()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.TimestampProperties,time,-0.005799510559452439
24433,class-level,"terminus-2,claude",0.976999345527098,1.0187289916107118,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.PeriodUnaryMethods,tslibs.period,"{""tslibs.period.PeriodUnaryMethods.time_asfreq"": ""class PeriodUnaryMethods:\n    def time_asfreq(self, freq):\n        self.per.asfreq(\""A\"")\n\n    def setup(self, freq):\n        self.per = Period(\""2012-06-01\"", freq=freq)""}",tslibs.period.PeriodUnaryMethods,time,-0.029511772336360573
24438,class-level,"terminus-2,claude",1.036955504496203,1.0281391962917308,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampConstruction,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_from_npdatetime64"": ""class TimestampConstruction:\n    def time_from_npdatetime64(self):\n        Timestamp(self.npdatetime64)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampConstruction.time_parse_iso8601_no_tz"": ""class TimestampConstruction:\n    def time_parse_iso8601_no_tz(self):\n        Timestamp(\""2017-08-25 08:16:14\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampConstruction.time_parse_now"": ""class TimestampConstruction:\n    def time_parse_now(self):\n        Timestamp(\""now\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.TimestampConstruction,time,0.006235012874450017
24435,class-level,"terminus-2,claude",0.9681999031708676,0.9754416429948614,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.TimePeriodArrToDT64Arr,tslibs.period,"{""tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr"": ""class TimePeriodArrToDT64Arr:\n    def time_periodarray_to_dt64arr(self, size, freq):\n        periodarr_to_dt64arr(self.i8values, freq)\n\n    def setup(self, size, freq):\n        arr = np.arange(10, dtype=\""i8\"").repeat(size // 10)\n        self.i8values = arr""}",tslibs.period.TimePeriodArrToDT64Arr,time,-0.005121456735497728
24441,class-level,"terminus-2,claude",0.98887136318111,1.018084972226594,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.tslib.TimeIntsToPydatetime,tslibs.tslib,"{""tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime"": ""class TimeIntsToPydatetime:\n    def time_ints_to_pydatetime(self, box, size, tz):\n        ints_to_pydatetime(self.i8data, tz, box=box)\n\n    def setup(self, box, size, tz):\n        if box == \""date\"" and tz is not None:\n            # tz is ignored, so avoid running redundant benchmarks\n            raise NotImplementedError  # skip benchmark\n        if size == 10**6 and tz is _tzs[-1]:\n            # This is cumbersomely-slow, so skip to trim runtime\n            raise NotImplementedError  # skip benchmark\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.tslib.TimeIntsToPydatetime,time,-0.020660260993977435
24442,class-level,"terminus-2,claude",0.9803890644371034,0.992241436005673,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.tz_convert.TimeTZConvert,tslibs.tz_convert,"{""tslibs.tz_convert.TimeTZConvert.time_tz_convert_from_utc"": ""class TimeTZConvert:\n    def time_tz_convert_from_utc(self, size, tz):\n        # effectively:\n        #  dti = DatetimeIndex(self.i8data, tz=tz)\n        #  dti.tz_localize(None)\n        if old_sig:\n            tz_convert_from_utc(self.i8data, UTC, tz)\n        else:\n            tz_convert_from_utc(self.i8data, tz)\n\n    def setup(self, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr"", ""tslibs.tz_convert.TimeTZConvert.time_tz_localize_to_utc"": ""class TimeTZConvert:\n    def time_tz_localize_to_utc(self, size, tz):\n        # effectively:\n        #  dti = DatetimeIndex(self.i8data)\n        #  dti.tz_localize(tz, ambiguous=\""NaT\"", nonexistent=\""NaT\"")\n        tz_localize_to_utc(self.i8data, tz, ambiguous=\""NaT\"", nonexistent=\""NaT\"")\n\n    def setup(self, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.tz_convert.TimeTZConvert,time,-0.008382158110728193
24449,class-level,"terminus-2,gpt-5",1.1364390900666588,0.7970819164535795,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.NumericInferOps,arithmetic,"{""arithmetic.NumericInferOps.time_add"": ""class NumericInferOps:\n    def time_add(self, dtype):\n        self.df[\""A\""] + self.df[\""B\""]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericInferOps:\n    def setup(self, dtype):\n        N = 5 * 10**5\n        self.df = DataFrame(\n            {\""A\"": np.arange(N).astype(dtype), \""B\"": np.arange(N).astype(dtype)}\n        )"", ""arithmetic.NumericInferOps.time_divide"": ""class NumericInferOps:\n    def time_divide(self, dtype):\n        self.df[\""A\""] / self.df[\""B\""]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericInferOps:\n    def setup(self, dtype):\n        N = 5 * 10**5\n        self.df = DataFrame(\n            {\""A\"": np.arange(N).astype(dtype), \""B\"": np.arange(N).astype(dtype)}\n        )"", ""arithmetic.NumericInferOps.time_modulo"": ""class NumericInferOps:\n    def time_modulo(self, dtype):\n        self.df[\""A\""] % self.df[\""B\""]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericInferOps:\n    def setup(self, dtype):\n        N = 5 * 10**5\n        self.df = DataFrame(\n            {\""A\"": np.arange(N).astype(dtype), \""B\"": np.arange(N).astype(dtype)}\n        )"", ""arithmetic.NumericInferOps.time_multiply"": ""class NumericInferOps:\n    def time_multiply(self, dtype):\n        self.df[\""A\""] * self.df[\""B\""]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericInferOps:\n    def setup(self, dtype):\n        N = 5 * 10**5\n        self.df = DataFrame(\n            {\""A\"": np.arange(N).astype(dtype), \""B\"": np.arange(N).astype(dtype)}\n        )"", ""arithmetic.NumericInferOps.time_subtract"": ""class NumericInferOps:\n    def time_subtract(self, dtype):\n        self.df[\""A\""] - self.df[\""B\""]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericInferOps:\n    def setup(self, dtype):\n        N = 5 * 10**5\n        self.df = DataFrame(\n            {\""A\"": np.arange(N).astype(dtype), \""B\"": np.arange(N).astype(dtype)}\n        )""}",arithmetic.NumericInferOps,time,0.23999800114079156
24454,class-level,"terminus-2,gpt-5",0.9751943239358802,0.9825157394271008,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.CategoricalSlicing,categoricals,"{""categoricals.CategoricalSlicing.time_getitem_scalar"": ""class CategoricalSlicing:\n    def time_getitem_scalar(self, index):\n        self.data[self.scalar]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalSlicing:\n    def setup(self, index):\n        N = 10**6\n        categories = [\""a\"", \""b\"", \""c\""]\n        values = [0] * N + [1] * N + [2] * N\n        if index == \""monotonic_incr\"":\n            self.data = pd.Categorical.from_codes(values, categories=categories)\n        elif index == \""monotonic_decr\"":\n            self.data = pd.Categorical.from_codes(\n                list(reversed(values)), categories=categories\n            )\n        elif index == \""non_monotonic\"":\n            self.data = pd.Categorical.from_codes([0, 1, 2] * N, categories=categories)\n        else:\n            raise ValueError(f\""Invalid index param: {index}\"")\n    \n        self.scalar = 10000\n        self.list = list(range(10000))\n        self.cat_scalar = \""b\""""}",categoricals.CategoricalSlicing,time,-0.005177804449236607
24443,class-level,"terminus-2,gpt-5",1.320380371101082,1.0265993124974522,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,algorithms.Duplicated,algorithms,"{""algorithms.Duplicated.time_duplicated"": ""class Duplicated:\n    def time_duplicated(self, unique, keep, dtype):\n        self.idx.duplicated(keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Duplicated:\n    def setup(self, unique, keep, dtype):\n        N = 10**5\n        data = {\n            \""int\"": pd.Index(np.arange(N), dtype=\""int64\""),\n            \""uint\"": pd.Index(np.arange(N), dtype=\""uint64\""),\n            \""float\"": pd.Index(np.random.randn(N), dtype=\""float64\""),\n            \""string\"": tm.makeStringIndex(N),\n            \""datetime64[ns]\"": pd.date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n            \""datetime64[ns, tz]\"": pd.date_range(\n                \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n            ),\n        }[dtype]\n        if not unique:\n            data = data.repeat(5)\n        self.idx = data\n        # cache is_unique\n        self.idx.is_unique""}",algorithms.Duplicated,time,0.207765953750799
24448,class-level,"terminus-2,gpt-5",1.0634911940872729,0.9570005702528782,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.IntFrameWithScalar,arithmetic,"{""arithmetic.IntFrameWithScalar.time_frame_op_with_scalar"": ""class IntFrameWithScalar:\n    def time_frame_op_with_scalar(self, dtype, scalar, op):\n        op(self.df, scalar)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IntFrameWithScalar:\n    def setup(self, dtype, scalar, op):\n        arr = np.random.randn(20000, 100)\n        self.df = DataFrame(arr.astype(dtype))""}",arithmetic.IntFrameWithScalar,time,0.07531161515869496
24451,class-level,"terminus-2,gpt-5",0.980583029376074,0.948302164804101,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,array.ArrowExtensionArray,array,"{""array.ArrowExtensionArray.time_to_numpy"": ""class ArrowExtensionArray:\n    def time_to_numpy(self, dtype, hasna):\n        self.arr.to_numpy()\n\n    def setup(self, dtype, hasna):\n        N = 100_000\n        if dtype == \""boolean[pyarrow]\"":\n            data = np.random.choice([True, False], N, replace=True)\n        elif dtype == \""float64[pyarrow]\"":\n            data = np.random.randn(N)\n        elif dtype == \""int64[pyarrow]\"":\n            data = np.arange(N)\n        elif dtype == \""string[pyarrow]\"":\n            data = tm.rands_array(10, N)\n        elif dtype == \""timestamp[ns][pyarrow]\"":\n            data = pd.date_range(\""2000-01-01\"", freq=\""s\"", periods=N)\n        else:\n            raise NotImplementedError\n    \n        arr = pd.array(data, dtype=dtype)\n        if hasna:\n            arr[::2] = pd.NA\n        self.arr = arr""}",array.ArrowExtensionArray,time,0.02282946575104172
24445,class-level,"terminus-2,gpt-5",1.1345145036953457,1.123615183139807,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,algorithms.Quantile,algorithms,"{""algorithms.Quantile.time_quantile"": ""class Quantile:\n    def time_quantile(self, quantile, interpolation, dtype):\n        self.idx.quantile(quantile, interpolation=interpolation)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, quantile, interpolation, dtype):\n        N = 10**5\n        data = {\n            \""int\"": np.arange(N),\n            \""uint\"": np.arange(N).astype(np.uint64),\n            \""float\"": np.random.randn(N),\n        }\n        self.idx = pd.Series(data[dtype].repeat(5))""}",algorithms.Quantile,time,0.007708147493308891
24453,class-level,"terminus-2,gpt-5",0.9994774685975464,0.9690018695661616,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,boolean.TimeLogicalOps,boolean,"{""boolean.TimeLogicalOps.time_and_array"": ""class TimeLogicalOps:\n    def time_and_array(self):\n        self.left & self.right\n\n    def setup(self):\n        N = 10_000\n        left, right, lmask, rmask = np.random.randint(0, 2, size=(4, N)).astype(\""bool\"")\n        self.left = pd.arrays.BooleanArray(left, lmask)\n        self.right = pd.arrays.BooleanArray(right, rmask)"", ""boolean.TimeLogicalOps.time_and_scalar"": ""class TimeLogicalOps:\n    def time_and_scalar(self):\n        self.left & True\n        self.left & False\n\n    def setup(self):\n        N = 10_000\n        left, right, lmask, rmask = np.random.randint(0, 2, size=(4, N)).astype(\""bool\"")\n        self.left = pd.arrays.BooleanArray(left, lmask)\n        self.right = pd.arrays.BooleanArray(right, rmask)""}",boolean.TimeLogicalOps,time,0.021552757447938296
24455,class-level,"terminus-2,gpt-5",1.0997558588050889,0.901924432099614,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,ctors.SeriesConstructors,ctors,"{""ctors.SeriesConstructors.time_series_constructor"": ""class SeriesConstructors:\n    def time_series_constructor(self, data_fmt, with_index, dtype):\n        Series(self.data, index=self.index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesConstructors:\n    def setup(self, data_fmt, with_index, dtype):\n        if data_fmt in (gen_of_str, gen_of_tuples) and with_index:\n            raise NotImplementedError(\n                \""Series constructors do not support using generators with indexes\""\n            )\n        N = 10**4\n        if dtype == \""float\"":\n            arr = np.random.randn(N)\n        else:\n            arr = np.arange(N)\n        self.data = data_fmt(arr)\n        self.index = np.arange(N) if with_index else None""}",ctors.SeriesConstructors,time,0.13990907122027924
24444,class-level,"terminus-2,gpt-5",1.2250480322656268,1.1702881536723302,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,algorithms.Factorize,algorithms,"{""algorithms.Factorize.time_factorize"": ""class Factorize:\n    def time_factorize(self, unique, sort, dtype):\n        pd.factorize(self.data, sort=sort)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Factorize:\n    def setup(self, unique, sort, dtype):\n        N = 10**5\n        string_index = tm.makeStringIndex(N)\n        string_arrow = None\n        if dtype == \""string[pyarrow]\"":\n            try:\n                string_arrow = pd.array(string_index, dtype=\""string[pyarrow]\"")\n            except ImportError:\n                raise NotImplementedError\n    \n        data = {\n            \""int\"": pd.Index(np.arange(N), dtype=\""int64\""),\n            \""uint\"": pd.Index(np.arange(N), dtype=\""uint64\""),\n            \""float\"": pd.Index(np.random.randn(N), dtype=\""float64\""),\n            \""object\"": string_index,\n            \""datetime64[ns]\"": pd.date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n            \""datetime64[ns, tz]\"": pd.date_range(\n                \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n            ),\n            \""Int64\"": pd.array(np.arange(N), dtype=\""Int64\""),\n            \""boolean\"": pd.array(np.random.randint(0, 2, N), dtype=\""boolean\""),\n            \""string[pyarrow]\"": string_arrow,\n        }[dtype]\n        if not unique:\n            data = data.repeat(5)\n        self.data = data""}",algorithms.Factorize,time,0.03872692969822954
24446,class-level,"terminus-2,gpt-5",1.070761322454298,1.0686585616269475,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.IsIn,algos.isin,"{""algos.isin.IsIn.time_isin"": ""class IsIn:\n    def time_isin(self, dtype):\n        self.series.isin(self.values)\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)"", ""algos.isin.IsIn.time_isin_categorical"": ""class IsIn:\n    def time_isin_categorical(self, dtype):\n        self.series.isin(self.cat_values)\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)"", ""algos.isin.IsIn.time_isin_empty"": ""class IsIn:\n    def time_isin_empty(self, dtype):\n        self.series.isin([])\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)"", ""algos.isin.IsIn.time_isin_mismatched_dtype"": ""class IsIn:\n    def time_isin_mismatched_dtype(self, dtype):\n        self.series.isin(self.mismatched)\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)""}",algos.isin.IsIn,time,0.0014871010094416569
24450,class-level,"terminus-2,gpt-5",0.99927294112674,0.969296484273031,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.OffsetArrayArithmetic,arithmetic,"{""arithmetic.OffsetArrayArithmetic.time_add_dti_offset"": ""class OffsetArrayArithmetic:\n    def time_add_dti_offset(self, offset):\n        with warnings.catch_warnings(record=True):\n            self.rng + offset\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass OffsetArrayArithmetic:\n    def setup(self, offset):\n        N = 10000\n        rng = date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"")\n        self.rng = rng\n        self.ser = Series(rng)""}",arithmetic.OffsetArrayArithmetic,time,0.021199757322283658
24447,class-level,"terminus-2,gpt-5",1.122841416979358,1.173784775878192,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.IsInLongSeriesLookUpDominates,algos.isin,"{""algos.isin.IsInLongSeriesLookUpDominates.time_isin"": ""class IsInLongSeriesLookUpDominates:\n    def time_isin(self, dtypes, MaxNumber, series_type):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, MaxNumber, series_type):\n        N = 10**7\n    \n        if series_type == \""random_hits\"":\n            array = np.random.randint(0, MaxNumber, N)\n        if series_type == \""random_misses\"":\n            array = np.random.randint(0, MaxNumber, N) + MaxNumber\n        if series_type == \""monotone_hits\"":\n            array = np.repeat(np.arange(MaxNumber), N // MaxNumber)\n        if series_type == \""monotone_misses\"":\n            array = np.arange(N) + MaxNumber\n    \n        self.series = Series(array).astype(dtype)\n    \n        self.values = np.arange(MaxNumber).astype(dtype.lower())""}",algos.isin.IsInLongSeriesLookUpDominates,time,-0.03602783514769036
24458,class-level,"terminus-2,gpt-5",1.236797875597159,1.21933316247237,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.Eval,eval,"{""eval.Eval.time_add"": ""class Eval:\n    def time_add(self, engine, threads):\n        pd.eval(\""self.df + self.df2 + self.df3 + self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)"", ""eval.Eval.time_and"": ""class Eval:\n    def time_and(self, engine, threads):\n        pd.eval(\n            \""(self.df > 0) & (self.df2 > 0) & (self.df3 > 0) & (self.df4 > 0)\"",\n            engine=engine,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)"", ""eval.Eval.time_chained_cmp"": ""class Eval:\n    def time_chained_cmp(self, engine, threads):\n        pd.eval(\""self.df < self.df2 < self.df3 < self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)"", ""eval.Eval.time_mult"": ""class Eval:\n    def time_mult(self, engine, threads):\n        pd.eval(\""self.df * self.df2 * self.df3 * self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)""}",eval.Eval,time,0.012351282266470243
24452,class-level,"terminus-2,gpt-5",0.9979521419972516,0.981116408258753,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,array.BooleanArray,array,"{""array.BooleanArray.time_from_bool_array"": ""class BooleanArray:\n    def time_from_bool_array(self):\n        pd.array(self.values_bool, dtype=\""boolean\"")\n\n    def setup(self):\n        self.values_bool = np.array([True, False, True, False])\n        self.values_float = np.array([1.0, 0.0, 1.0, 0.0])\n        self.values_integer = np.array([1, 0, 1, 0])\n        self.values_integer_like = [1, 0, 1, 0]\n        self.data = np.array([True, False, True, False])\n        self.mask = np.array([False, False, True, False])"", ""array.BooleanArray.time_from_float_array"": ""class BooleanArray:\n    def time_from_float_array(self):\n        pd.array(self.values_float, dtype=\""boolean\"")\n\n    def setup(self):\n        self.values_bool = np.array([True, False, True, False])\n        self.values_float = np.array([1.0, 0.0, 1.0, 0.0])\n        self.values_integer = np.array([1, 0, 1, 0])\n        self.values_integer_like = [1, 0, 1, 0]\n        self.data = np.array([True, False, True, False])\n        self.mask = np.array([False, False, True, False])""}",array.BooleanArray,time,0.011906459503888656
24457,class-level,"terminus-2,gpt-5",1.0670961406083044,1.1074222477456166,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.SelectDtypes,dtypes,"{""dtypes.SelectDtypes.time_select_dtype_bool_exclude"": ""class SelectDtypes:\n    def time_select_dtype_bool_exclude(self, dtype):\n        self.df_bool.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_bool_include"": ""class SelectDtypes:\n    def time_select_dtype_bool_include(self, dtype):\n        self.df_bool.select_dtypes(include=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_float_exclude"": ""class SelectDtypes:\n    def time_select_dtype_float_exclude(self, dtype):\n        self.df_float.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_float_include"": ""class SelectDtypes:\n    def time_select_dtype_float_include(self, dtype):\n        self.df_float.select_dtypes(include=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_int_exclude"": ""class SelectDtypes:\n    def time_select_dtype_int_exclude(self, dtype):\n        self.df_int.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_int_include"": ""class SelectDtypes:\n    def time_select_dtype_int_include(self, dtype):\n        self.df_int.select_dtypes(include=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_string_exclude"": ""class SelectDtypes:\n    def time_select_dtype_string_exclude(self, dtype):\n        self.df_string.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_string_include"": ""class SelectDtypes:\n    def time_select_dtype_string_include(self, dtype):\n        self.df_string.select_dtypes(include=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )""}",dtypes.SelectDtypes,time,-0.028519170535581485
24456,class-level,"terminus-2,gpt-5",1.0155700319684131,0.9843871123238844,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.DtypesInvalid,dtypes,"{""dtypes.DtypesInvalid.time_pandas_dtype_invalid"": ""class DtypesInvalid:\n    def time_pandas_dtype_invalid(self, dtype):\n        try:\n            pandas_dtype(self.data_dict[dtype])\n        except TypeError:\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)""}",dtypes.DtypesInvalid,time,0.022052984189907133
24469,class-level,"terminus-2,gpt-5",1.1691773958960416,1.0972896468407467,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Fillna,frame_methods,"{""frame_methods.Fillna.time_frame_fillna"": ""class Fillna:\n    def time_frame_fillna(self, inplace, method, dtype):\n        self.df.fillna(inplace=inplace, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Fillna:\n    def setup(self, inplace, method, dtype):\n        N, M = 10000, 100\n        if dtype in (\""datetime64[ns]\"", \""datetime64[ns, tz]\"", \""timedelta64[ns]\""):\n            data = {\n                \""datetime64[ns]\"": date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n                \""datetime64[ns, tz]\"": date_range(\n                    \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n                ),\n                \""timedelta64[ns]\"": timedelta_range(start=\""1 day\"", periods=N, freq=\""1D\""),\n            }\n            self.df = DataFrame({f\""col_{i}\"": data[dtype] for i in range(M)})\n            self.df[::2] = None\n        else:\n            values = np.random.randn(N, M)\n            values[::2] = np.nan\n            if dtype == \""Int64\"":\n                values = values.round()\n            self.df = DataFrame(values, dtype=dtype)""}",frame_methods.Fillna,time,0.05083999225975597
24466,class-level,"terminus-2,gpt-5",1.2064066631978931,1.159104083931885,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Dropna,frame_methods,"{""frame_methods.Dropna.time_dropna"": ""class Dropna:\n    def time_dropna(self, how, axis):\n        self.df.dropna(how=how, axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Dropna:\n    def setup(self, how, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\"""", ""frame_methods.Dropna.time_dropna_axis_mixed_dtypes"": ""class Dropna:\n    def time_dropna_axis_mixed_dtypes(self, how, axis):\n        self.df_mixed.dropna(how=how, axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Dropna:\n    def setup(self, how, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""""}",frame_methods.Dropna,time,0.03345302635502704
24470,class-level,"terminus-2,gpt-5",1.0245262266507311,1.0703447516666793,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Iteration,frame_methods,"{""frame_methods.Iteration.time_items_cached"": ""class Iteration:\n    def time_items_cached(self):\n        for name, col in self.df.items():\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))"", ""frame_methods.Iteration.time_iterrows"": ""class Iteration:\n    def time_iterrows(self):\n        for row in self.df.iterrows():\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))""}",frame_methods.Iteration,time,-0.032403483038152894
24460,class-level,"terminus-2,gpt-5",0.9836834185219744,0.994961338034834,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,finalize.Finalize,finalize,"{""finalize.Finalize.time_finalize_micro"": ""class Finalize:\n    def time_finalize_micro(self, param):\n        self.obj.__finalize__(self.obj, method=\""__finalize__\"")\n\n    def setup(self, param):\n        N = 1000\n        obj = param(dtype=float)\n        for i in range(N):\n            obj.attrs[i] = i\n        self.obj = obj""}",finalize.Finalize,time,-0.0079758978167324
24462,class-level,"terminus-2,gpt-5",1.2988358280316128,1.0644373832418372,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromDicts,frame_ctor,"{""frame_ctor.FromDicts.time_list_of_dict"": ""class FromDicts:\n    def time_list_of_dict(self):\n        DataFrame(self.dict_list)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDicts:\n    def setup(self):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n        frame = DataFrame(np.random.randn(N, K), index=self.index, columns=self.columns)\n        self.data = frame.to_dict()\n        self.dict_list = frame.to_dict(orient=\""records\"")\n        self.data2 = {i: {j: float(j) for j in range(100)} for i in range(2000)}\n    \n        # arrays which we won't consolidate\n        self.dict_of_categoricals = {i: Categorical(np.arange(N)) for i in range(K)}""}",frame_ctor.FromDicts,time,0.16576976293477763
24459,class-level,"terminus-2,gpt-5",1.1771260988920358,1.143118171288414,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.Query,eval,"{""eval.Query.time_query_datetime_column"": ""class Query:\n    def time_query_datetime_column(self):\n        self.df.query(\""dates < @self.ts\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Query:\n    def setup(self):\n        N = 10**6\n        halfway = (N // 2) - 1\n        index = pd.date_range(\""20010101\"", periods=N, freq=\""T\"")\n        s = pd.Series(index)\n        self.ts = s.iloc[halfway]\n        self.df = pd.DataFrame({\""a\"": np.random.randn(N), \""dates\"": index}, index=index)\n        data = np.random.randn(N)\n        self.min_val = data.min()\n        self.max_val = data.max()"", ""eval.Query.time_query_with_boolean_selection"": ""class Query:\n    def time_query_with_boolean_selection(self):\n        self.df.query(\""(a >= @self.min_val) & (a <= @self.max_val)\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Query:\n    def setup(self):\n        N = 10**6\n        halfway = (N // 2) - 1\n        index = pd.date_range(\""20010101\"", periods=N, freq=\""T\"")\n        s = pd.Series(index)\n        self.ts = s.iloc[halfway]\n        self.df = pd.DataFrame({\""a\"": np.random.randn(N), \""dates\"": index}, index=index)\n        data = np.random.randn(N)\n        self.min_val = data.min()\n        self.max_val = data.max()""}",eval.Query,time,0.024050868177950362
24467,class-level,"terminus-2,gpt-5",1.0356180912954065,1.0415164232546907,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Duplicated,frame_methods,"{""frame_methods.Duplicated.time_frame_duplicated_wide"": ""class Duplicated:\n    def time_frame_duplicated_wide(self):\n        self.df2.duplicated()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Duplicated:\n    def setup(self):\n        n = 1 << 20\n        t = date_range(\""2015-01-01\"", freq=\""S\"", periods=(n // 64))\n        xs = np.random.randn(n // 64).round(2)\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(-1 << 8, 1 << 8, n),\n                \""b\"": np.random.choice(t, n),\n                \""c\"": np.random.choice(xs, n),\n            }\n        )\n        self.df2 = DataFrame(np.random.randn(1000, 100).astype(str)).T""}",frame_methods.Duplicated,time,-0.004171380452110496
24464,class-level,"terminus-2,gpt-5",1.205200898897349,1.1014807750057138,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Count,frame_methods,"{""frame_methods.Count.time_count_level_mixed_dtypes_multi"": ""class Count:\n    def time_count_level_mixed_dtypes_multi(self, axis):\n        self.df_mixed.count(axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Count:\n    def setup(self, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""\n    \n        self.df.index = MultiIndex.from_arrays([self.df.index, self.df.index])\n        self.df.columns = MultiIndex.from_arrays([self.df.columns, self.df.columns])\n        self.df_mixed.index = MultiIndex.from_arrays(\n            [self.df_mixed.index, self.df_mixed.index]\n        )\n        self.df_mixed.columns = MultiIndex.from_arrays(\n            [self.df_mixed.columns, self.df_mixed.columns]\n        )"", ""frame_methods.Count.time_count_level_multi"": ""class Count:\n    def time_count_level_multi(self, axis):\n        self.df.count(axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Count:\n    def setup(self, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""\n    \n        self.df.index = MultiIndex.from_arrays([self.df.index, self.df.index])\n        self.df.columns = MultiIndex.from_arrays([self.df.columns, self.df.columns])\n        self.df_mixed.index = MultiIndex.from_arrays(\n            [self.df_mixed.index, self.df_mixed.index]\n        )\n        self.df_mixed.columns = MultiIndex.from_arrays(\n            [self.df_mixed.columns, self.df_mixed.columns]\n        )""}",frame_methods.Count,time,0.07335227997994004
24471,class-level,"terminus-2,gpt-5",1.09159313350511,1.274450093142372,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.ToDict,frame_methods,"{""frame_methods.ToDict.time_to_dict_datetimelike"": ""class ToDict:\n    def time_to_dict_datetimelike(self, orient):\n        self.datetimelike_df.to_dict(orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToDict:\n    def setup(self, orient):\n        data = np.random.randint(0, 1000, size=(10000, 4))\n        self.int_df = DataFrame(data)\n        self.datetimelike_df = self.int_df.astype(\""timedelta64[ns]\"")"", ""frame_methods.ToDict.time_to_dict_ints"": ""class ToDict:\n    def time_to_dict_ints(self, orient):\n        self.int_df.to_dict(orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToDict:\n    def setup(self, orient):\n        data = np.random.randint(0, 1000, size=(10000, 4))\n        self.int_df = DataFrame(data)\n        self.datetimelike_df = self.int_df.astype(\""timedelta64[ns]\"")""}",frame_methods.ToDict,time,-0.12931892477882745
24468,class-level,"terminus-2,gpt-5",1.2468229681117633,1.5831426169002387,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Equals,frame_methods,"{""frame_methods.Equals.time_frame_float_equal"": ""class Equals:\n    def time_frame_float_equal(self):\n        self.float_df.equals(self.float_df)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Equals:\n    def setup(self):\n        N = 10**3\n        self.float_df = DataFrame(np.random.randn(N, N))\n        self.float_df_nan = self.float_df.copy()\n        self.float_df_nan.iloc[-1, -1] = np.nan\n    \n        self.object_df = DataFrame(\""foo\"", index=range(N), columns=range(N))\n        self.object_df_nan = self.object_df.copy()\n        self.object_df_nan.iloc[-1, -1] = np.nan\n    \n        self.nonunique_cols = self.object_df.copy()\n        self.nonunique_cols.columns = [\""A\""] * len(self.nonunique_cols.columns)\n        self.nonunique_cols_nan = self.nonunique_cols.copy()\n        self.nonunique_cols_nan.iloc[-1, -1] = np.nan"", ""frame_methods.Equals.time_frame_float_unequal"": ""class Equals:\n    def time_frame_float_unequal(self):\n        self.float_df.equals(self.float_df_nan)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Equals:\n    def setup(self):\n        N = 10**3\n        self.float_df = DataFrame(np.random.randn(N, N))\n        self.float_df_nan = self.float_df.copy()\n        self.float_df_nan.iloc[-1, -1] = np.nan\n    \n        self.object_df = DataFrame(\""foo\"", index=range(N), columns=range(N))\n        self.object_df_nan = self.object_df.copy()\n        self.object_df_nan.iloc[-1, -1] = np.nan\n    \n        self.nonunique_cols = self.object_df.copy()\n        self.nonunique_cols.columns = [\""A\""] * len(self.nonunique_cols.columns)\n        self.nonunique_cols_nan = self.nonunique_cols.copy()\n        self.nonunique_cols_nan.iloc[-1, -1] = np.nan"", ""frame_methods.Equals.time_frame_nonunique_equal"": ""class Equals:\n    def time_frame_nonunique_equal(self):\n        self.nonunique_cols.equals(self.nonunique_cols)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Equals:\n    def setup(self):\n        N = 10**3\n        self.float_df = DataFrame(np.random.randn(N, N))\n        self.float_df_nan = self.float_df.copy()\n        self.float_df_nan.iloc[-1, -1] = np.nan\n    \n        self.object_df = DataFrame(\""foo\"", index=range(N), columns=range(N))\n        self.object_df_nan = self.object_df.copy()\n        self.object_df_nan.iloc[-1, -1] = np.nan\n    \n        self.nonunique_cols = self.object_df.copy()\n        self.nonunique_cols.columns = [\""A\""] * len(self.nonunique_cols.columns)\n        self.nonunique_cols_nan = self.nonunique_cols.copy()\n        self.nonunique_cols_nan.iloc[-1, -1] = np.nan""}",frame_methods.Equals,time,-0.23784982233979876
24465,class-level,"terminus-2,gpt-5",1.108747063883863,1.0483986784350785,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Describe,frame_methods,"{""frame_methods.Describe.time_dataframe_describe"": ""class Describe:\n    def time_dataframe_describe(self):\n        self.df.describe()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Describe:\n    def setup(self):\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(0, 100, 10**6),\n                \""b\"": np.random.randint(0, 100, 10**6),\n                \""c\"": np.random.randint(0, 100, 10**6),\n            }\n        )"", ""frame_methods.Describe.time_series_describe"": ""class Describe:\n    def time_series_describe(self):\n        self.df[\""a\""].describe()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Describe:\n    def setup(self):\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(0, 100, 10**6),\n                \""b\"": np.random.randint(0, 100, 10**6),\n                \""c\"": np.random.randint(0, 100, 10**6),\n            }\n        )""}",frame_methods.Describe,time,0.04267919762997491
24461,class-level,"terminus-2,gpt-5",1.1568948063018856,1.109764724574163,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromArrays,frame_ctor,"{""frame_ctor.FromArrays.time_frame_from_arrays_sparse"": ""class FromArrays:\n    def time_frame_from_arrays_sparse(self):\n        self.df = DataFrame._from_arrays(\n            self.sparse_arrays,\n            index=self.index,\n            columns=self.columns,\n            verify_integrity=False,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromArrays:\n    def setup(self):\n        N_rows = 1000\n        N_cols = 1000\n        self.float_arrays = [np.random.randn(N_rows) for _ in range(N_cols)]\n        self.sparse_arrays = [\n            pd.arrays.SparseArray(np.random.randint(0, 2, N_rows), dtype=\""float64\"")\n            for _ in range(N_cols)\n        ]\n        self.int_arrays = [\n            pd.array(np.random.randint(1000, size=N_rows), dtype=\""Int64\"")\n            for _ in range(N_cols)\n        ]\n        self.index = pd.Index(range(N_rows))\n        self.columns = pd.Index(range(N_cols))""}",frame_ctor.FromArrays,time,0.033331033753693416
24473,class-level,"terminus-2,gpt-5",1.1248654485317862,1.0230055870235066,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.Apply,groupby,"{""groupby.Apply.time_copy_function_multi_col"": ""class Apply:\n    def time_copy_function_multi_col(self, factor):\n        self.df.groupby([\""key\"", \""key2\""]).apply(self.df_copy_function)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, factor):\n        N = 10**factor\n        # two cases:\n        # - small groups: small data (N**4) + many labels (2000) -> average group\n        #   size of 5 (-> larger overhead of slicing method)\n        # - larger groups: larger data (N**5) + fewer labels (20) -> average group\n        #   size of 5000\n        labels = np.random.randint(0, 2000 if factor == 4 else 20, size=N)\n        labels2 = np.random.randint(0, 3, size=N)\n        df = DataFrame(\n            {\n                \""key\"": labels,\n                \""key2\"": labels2,\n                \""value1\"": np.random.randn(N),\n                \""value2\"": [\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (N // 4),\n            }\n        )\n        self.df = df"", ""groupby.Apply.time_copy_overhead_single_col"": ""class Apply:\n    def time_copy_overhead_single_col(self, factor):\n        self.df.groupby(\""key\"").apply(self.df_copy_function)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, factor):\n        N = 10**factor\n        # two cases:\n        # - small groups: small data (N**4) + many labels (2000) -> average group\n        #   size of 5 (-> larger overhead of slicing method)\n        # - larger groups: larger data (N**5) + fewer labels (20) -> average group\n        #   size of 5000\n        labels = np.random.randint(0, 2000 if factor == 4 else 20, size=N)\n        labels2 = np.random.randint(0, 3, size=N)\n        df = DataFrame(\n            {\n                \""key\"": labels,\n                \""key2\"": labels2,\n                \""value1\"": np.random.randn(N),\n                \""value2\"": [\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (N // 4),\n            }\n        )\n        self.df = df"", ""groupby.Apply.time_scalar_function_multi_col"": ""class Apply:\n    def time_scalar_function_multi_col(self, factor):\n        self.df.groupby([\""key\"", \""key2\""]).apply(lambda x: 1)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, factor):\n        N = 10**factor\n        # two cases:\n        # - small groups: small data (N**4) + many labels (2000) -> average group\n        #   size of 5 (-> larger overhead of slicing method)\n        # - larger groups: larger data (N**5) + fewer labels (20) -> average group\n        #   size of 5000\n        labels = np.random.randint(0, 2000 if factor == 4 else 20, size=N)\n        labels2 = np.random.randint(0, 3, size=N)\n        df = DataFrame(\n            {\n                \""key\"": labels,\n                \""key2\"": labels2,\n                \""value1\"": np.random.randn(N),\n                \""value2\"": [\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (N // 4),\n            }\n        )\n        self.df = df"", ""groupby.Apply.time_scalar_function_single_col"": ""class Apply:\n    def time_scalar_function_single_col(self, factor):\n        self.df.groupby(\""key\"").apply(lambda x: 1)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, factor):\n        N = 10**factor\n        # two cases:\n        # - small groups: small data (N**4) + many labels (2000) -> average group\n        #   size of 5 (-> larger overhead of slicing method)\n        # - larger groups: larger data (N**5) + fewer labels (20) -> average group\n        #   size of 5000\n        labels = np.random.randint(0, 2000 if factor == 4 else 20, size=N)\n        labels2 = np.random.randint(0, 3, size=N)\n        df = DataFrame(\n            {\n                \""key\"": labels,\n                \""key2\"": labels2,\n                \""value1\"": np.random.randn(N),\n                \""value2\"": [\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (N // 4),\n            }\n        )\n        self.df = df""}",groupby.Apply,time,0.0720366771628568
24463,class-level,"terminus-2,gpt-5",1.0535484176124674,1.0407155721351138,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromScalar,frame_ctor,"{""frame_ctor.FromScalar.time_frame_from_scalar_ea_float64_na"": ""class FromScalar:\n    def time_frame_from_scalar_ea_float64_na(self):\n        DataFrame(\n            NA,\n            index=range(self.nrows),\n            columns=list(\""abc\""),\n            dtype=Float64Dtype(),\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromScalar:\n    def setup(self):\n        self.nrows = 100_000""}",frame_ctor.FromScalar,time,0.009075562572385884
24472,class-level,"terminus-2,gpt-5",1.0748106205367165,1.139579860075716,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,gil.ParallelGroupbyMethods,gil,"{""gil.ParallelGroupbyMethods.time_parallel"": ""class ParallelGroupbyMethods:\n    def time_parallel(self, threads, method):\n        self.parallel()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ParallelGroupbyMethods:\n    def setup(self, threads, method):\n        N = 10**6\n        ngroups = 10**3\n        df = DataFrame(\n            {\""key\"": np.random.randint(0, ngroups, size=N), \""data\"": np.random.randn(N)}\n        )\n    \n        @test_parallel(num_threads=threads)\n        def parallel():\n            getattr(df.groupby(\""key\"")[\""data\""], method)()\n    \n        self.parallel = parallel\n    \n        def loop():\n            getattr(df.groupby(\""key\"")[\""data\""], method)()\n    \n        self.loop = loop""}",gil.ParallelGroupbyMethods,time,-0.04580568567114541
24478,class-level,"terminus-2,gpt-5",1.1111333736972115,0.7678419434164367,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.String,groupby,"{""groupby.String.time_str_func"": ""class String:\n    def time_str_func(self, dtype, method):\n        self.df.groupby(\""a\"")[self.df.columns[1:]].agg(method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass String:\n    def setup(self, dtype, method):\n        cols = list(\""abcdefghjkl\"")\n        self.df = DataFrame(\n            np.random.randint(0, 100, size=(10_000, len(cols))),\n            columns=cols,\n            dtype=dtype,\n        )""}",groupby.String,time,0.2427803608774928
24474,class-level,"terminus-2,gpt-5",1.177381398721525,1.0188571417553025,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.ApplyNonUniqueUnsortedIndex,groupby,"{""groupby.ApplyNonUniqueUnsortedIndex.time_groupby_apply_non_unique_unsorted_index"": ""class ApplyNonUniqueUnsortedIndex:\n    def time_groupby_apply_non_unique_unsorted_index(self):\n        self.df.groupby(\""key\"", group_keys=False).apply(lambda x: x)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ApplyNonUniqueUnsortedIndex:\n    def setup(self):\n        # GH 46527\n        # unsorted and non-unique index\n        idx = np.arange(100)[::-1]\n        idx = Index(np.repeat(idx, 200), name=\""key\"")\n        self.df = DataFrame(np.random.randn(len(idx), 10), index=idx)""}",groupby.ApplyNonUniqueUnsortedIndex,time,0.11211050704824782
24479,class-level,"terminus-2,gpt-5",1.062726464149392,1.042586536069814,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.Transform,groupby,"{""groupby.Transform.time_transform_lambda_max"": ""class Transform:\n    def time_transform_lambda_max(self):\n        self.df.groupby(level=\""lev1\"").transform(lambda x: max(x))\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]"", ""groupby.Transform.time_transform_lambda_max_tall"": ""class Transform:\n    def time_transform_lambda_max_tall(self):\n        self.df_tall.groupby(level=0).transform(lambda x: np.max(x, axis=0))\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]"", ""groupby.Transform.time_transform_lambda_max_wide"": ""class Transform:\n    def time_transform_lambda_max_wide(self):\n        self.df_wide.groupby(level=0).transform(lambda x: np.max(x, axis=0))\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]"", ""groupby.Transform.time_transform_multi_key1"": ""class Transform:\n    def time_transform_multi_key1(self):\n        self.df1.groupby([\""jim\"", \""joe\""])[\""jolie\""].transform(\""max\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]"", ""groupby.Transform.time_transform_multi_key2"": ""class Transform:\n    def time_transform_multi_key2(self):\n        self.df2.groupby([\""jim\"", \""joe\""])[\""jolie\""].transform(\""max\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]"", ""groupby.Transform.time_transform_ufunc_max"": ""class Transform:\n    def time_transform_ufunc_max(self):\n        self.df.groupby(level=\""lev1\"").transform(np.max)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]""}",groupby.Transform,time,0.014243230607905224
24476,class-level,"terminus-2,gpt-5",1.068289823762479,1.0710216498449578,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.GroupByMethods,groupby,"{""groupby.GroupByMethods.time_dtype_as_field"": ""class GroupByMethods:\n    def time_dtype_as_field(self, dtype, method, application, ncols):\n        self.as_field_method()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupByMethods:\n    def setup(self, dtype, method, application, ncols):\n        if method in method_blocklist.get(dtype, {}):\n            raise NotImplementedError  # skip benchmark\n    \n        if ncols != 1 and method in [\""value_counts\"", \""unique\""]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if application == \""transformation\"" and method in [\n            \""describe\"",\n            \""head\"",\n            \""tail\"",\n            \""unique\"",\n            \""value_counts\"",\n            \""size\"",\n        ]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if method == \""describe\"":\n            ngroups = 20\n        elif method == \""skew\"":\n            ngroups = 100\n        else:\n            ngroups = 1000\n        size = ngroups * 2\n        rng = np.arange(ngroups).reshape(-1, 1)\n        rng = np.broadcast_to(rng, (len(rng), ncols))\n        taker = np.random.randint(0, ngroups, size=size)\n        values = rng.take(taker, axis=0)\n        if dtype == \""int\"":\n            key = np.random.randint(0, size, size=size)\n        elif dtype in (\""int16\"", \""uint\""):\n            key = np.random.randint(0, size, size=size, dtype=dtype)\n        elif dtype == \""float\"":\n            key = np.concatenate(\n                [np.random.random(ngroups) * 0.1, np.random.random(ngroups) * 10.0]\n            )\n        elif dtype == \""object\"":\n            key = [\""foo\""] * size\n        elif dtype == \""datetime\"":\n            key = date_range(\""1/1/2011\"", periods=size, freq=\""s\"")\n    \n        cols = [f\""values{n}\"" for n in range(ncols)]\n        df = DataFrame(values, columns=cols)\n        df[\""key\""] = key\n    \n        if len(cols) == 1:\n            cols = cols[0]\n    \n        if application == \""transformation\"":\n            self.as_group_method = lambda: df.groupby(\""key\"")[cols].transform(method)\n            self.as_field_method = lambda: df.groupby(cols)[\""key\""].transform(method)\n        else:\n            self.as_group_method = getattr(df.groupby(\""key\"")[cols], method)\n            self.as_field_method = getattr(df.groupby(cols)[\""key\""], method)"", ""groupby.GroupByMethods.time_dtype_as_group"": ""class GroupByMethods:\n    def time_dtype_as_group(self, dtype, method, application, ncols):\n        self.as_group_method()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupByMethods:\n    def setup(self, dtype, method, application, ncols):\n        if method in method_blocklist.get(dtype, {}):\n            raise NotImplementedError  # skip benchmark\n    \n        if ncols != 1 and method in [\""value_counts\"", \""unique\""]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if application == \""transformation\"" and method in [\n            \""describe\"",\n            \""head\"",\n            \""tail\"",\n            \""unique\"",\n            \""value_counts\"",\n            \""size\"",\n        ]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if method == \""describe\"":\n            ngroups = 20\n        elif method == \""skew\"":\n            ngroups = 100\n        else:\n            ngroups = 1000\n        size = ngroups * 2\n        rng = np.arange(ngroups).reshape(-1, 1)\n        rng = np.broadcast_to(rng, (len(rng), ncols))\n        taker = np.random.randint(0, ngroups, size=size)\n        values = rng.take(taker, axis=0)\n        if dtype == \""int\"":\n            key = np.random.randint(0, size, size=size)\n        elif dtype in (\""int16\"", \""uint\""):\n            key = np.random.randint(0, size, size=size, dtype=dtype)\n        elif dtype == \""float\"":\n            key = np.concatenate(\n                [np.random.random(ngroups) * 0.1, np.random.random(ngroups) * 10.0]\n            )\n        elif dtype == \""object\"":\n            key = [\""foo\""] * size\n        elif dtype == \""datetime\"":\n            key = date_range(\""1/1/2011\"", periods=size, freq=\""s\"")\n    \n        cols = [f\""values{n}\"" for n in range(ncols)]\n        df = DataFrame(values, columns=cols)\n        df[\""key\""] = key\n    \n        if len(cols) == 1:\n            cols = cols[0]\n    \n        if application == \""transformation\"":\n            self.as_group_method = lambda: df.groupby(\""key\"")[cols].transform(method)\n            self.as_field_method = lambda: df.groupby(cols)[\""key\""].transform(method)\n        else:\n            self.as_group_method = getattr(df.groupby(\""key\"")[cols], method)\n            self.as_field_method = getattr(df.groupby(cols)[\""key\""], method)""}",groupby.GroupByMethods,time,-0.001931984499631475
24484,class-level,"terminus-2,gpt-5",1.273250890439918,1.16908494765572,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_object.SetOperations,index_object,"{""index_object.SetOperations.time_operation"": ""class SetOperations:\n    def time_operation(self, index_structure, dtype, method):\n        getattr(self.left, method)(self.right)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SetOperations:\n    def setup(self, index_structure, dtype, method):\n        N = 10**5\n        dates_left = date_range(\""1/1/2000\"", periods=N, freq=\""T\"")\n        fmt = \""%Y-%m-%d %H:%M:%S\""\n        date_str_left = Index(dates_left.strftime(fmt))\n        int_left = Index(np.arange(N))\n        ea_int_left = Index(np.arange(N), dtype=\""Int64\"")\n        str_left = tm.makeStringIndex(N)\n    \n        data = {\n            \""datetime\"": dates_left,\n            \""date_string\"": date_str_left,\n            \""int\"": int_left,\n            \""strings\"": str_left,\n            \""ea_int\"": ea_int_left,\n        }\n    \n        if index_structure == \""non_monotonic\"":\n            data = {k: mi[::-1] for k, mi in data.items()}\n    \n        data = {k: {\""left\"": idx, \""right\"": idx[:-1]} for k, idx in data.items()}\n    \n        self.left = data[dtype][\""left\""]\n        self.right = data[dtype][\""right\""]""}",index_object.SetOperations,time,0.07366756915431266
24482,class-level,"terminus-2,gpt-5",1.318807753788909,1.0570132078439995,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_cached_properties.IndexCache,index_cached_properties,"{""index_cached_properties.IndexCache.time_engine"": ""class IndexCache:\n    def time_engine(self, index_type):\n        self.idx._engine\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}"", ""index_cached_properties.IndexCache.time_inferred_type"": ""class IndexCache:\n    def time_inferred_type(self, index_type):\n        self.idx.inferred_type\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}"", ""index_cached_properties.IndexCache.time_is_monotonic_decreasing"": ""class IndexCache:\n    def time_is_monotonic_decreasing(self, index_type):\n        self.idx.is_monotonic_decreasing\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}"", ""index_cached_properties.IndexCache.time_is_monotonic_increasing"": ""class IndexCache:\n    def time_is_monotonic_increasing(self, index_type):\n        self.idx.is_monotonic_increasing\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}"", ""index_cached_properties.IndexCache.time_shape"": ""class IndexCache:\n    def time_shape(self, index_type):\n        self.idx.shape\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}""}",index_cached_properties.IndexCache,time,0.18514465766966717
24483,class-level,"terminus-2,gpt-5",1.1059881917140686,1.0263895534781546,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_object.Indexing,index_object,"{""index_object.Indexing.time_boolean_array"": ""class Indexing:\n    def time_boolean_array(self, dtype):\n        self.idx[self.array_mask]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self, dtype):\n        N = 10**6\n        self.idx = getattr(tm, f\""make{dtype}Index\"")(N)\n        self.array_mask = (np.arange(N) % 3) == 0\n        self.series_mask = Series(self.array_mask)\n        self.sorted = self.idx.sort_values()\n        half = N // 2\n        self.non_unique = self.idx[:half].append(self.idx[:half])\n        self.non_unique_sorted = (\n            self.sorted[:half].append(self.sorted[:half]).sort_values()\n        )\n        self.key = self.sorted[N // 4]"", ""index_object.Indexing.time_get_loc"": ""class Indexing:\n    def time_get_loc(self, dtype):\n        self.idx.get_loc(self.key)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self, dtype):\n        N = 10**6\n        self.idx = getattr(tm, f\""make{dtype}Index\"")(N)\n        self.array_mask = (np.arange(N) % 3) == 0\n        self.series_mask = Series(self.array_mask)\n        self.sorted = self.idx.sort_values()\n        half = N // 2\n        self.non_unique = self.idx[:half].append(self.idx[:half])\n        self.non_unique_sorted = (\n            self.sorted[:half].append(self.sorted[:half]).sort_values()\n        )\n        self.key = self.sorted[N // 4]"", ""index_object.Indexing.time_get_loc_non_unique"": ""class Indexing:\n    def time_get_loc_non_unique(self, dtype):\n        self.non_unique.get_loc(self.key)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self, dtype):\n        N = 10**6\n        self.idx = getattr(tm, f\""make{dtype}Index\"")(N)\n        self.array_mask = (np.arange(N) % 3) == 0\n        self.series_mask = Series(self.array_mask)\n        self.sorted = self.idx.sort_values()\n        half = N // 2\n        self.non_unique = self.idx[:half].append(self.idx[:half])\n        self.non_unique_sorted = (\n            self.sorted[:half].append(self.sorted[:half]).sort_values()\n        )\n        self.key = self.sorted[N // 4]"", ""index_object.Indexing.time_get_loc_non_unique_sorted"": ""class Indexing:\n    def time_get_loc_non_unique_sorted(self, dtype):\n        self.non_unique_sorted.get_loc(self.key)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self, dtype):\n        N = 10**6\n        self.idx = getattr(tm, f\""make{dtype}Index\"")(N)\n        self.array_mask = (np.arange(N) % 3) == 0\n        self.series_mask = Series(self.array_mask)\n        self.sorted = self.idx.sort_values()\n        half = N // 2\n        self.non_unique = self.idx[:half].append(self.idx[:half])\n        self.non_unique_sorted = (\n            self.sorted[:half].append(self.sorted[:half]).sort_values()\n        )\n        self.key = self.sorted[N // 4]"", ""index_object.Indexing.time_get_loc_sorted"": ""class Indexing:\n    def time_get_loc_sorted(self, dtype):\n        self.sorted.get_loc(self.key)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self, dtype):\n        N = 10**6\n        self.idx = getattr(tm, f\""make{dtype}Index\"")(N)\n        self.array_mask = (np.arange(N) % 3) == 0\n        self.series_mask = Series(self.array_mask)\n        self.sorted = self.idx.sort_values()\n        half = N // 2\n        self.non_unique = self.idx[:half].append(self.idx[:half])\n        self.non_unique_sorted = (\n            self.sorted[:half].append(self.sorted[:half]).sort_values()\n        )\n        self.key = self.sorted[N // 4]"", ""index_object.Indexing.time_slice"": ""class Indexing:\n    def time_slice(self, dtype):\n        self.idx[:-1]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self, dtype):\n        N = 10**6\n        self.idx = getattr(tm, f\""make{dtype}Index\"")(N)\n        self.array_mask = (np.arange(N) % 3) == 0\n        self.series_mask = Series(self.array_mask)\n        self.sorted = self.idx.sort_values()\n        half = N // 2\n        self.non_unique = self.idx[:half].append(self.idx[:half])\n        self.non_unique_sorted = (\n            self.sorted[:half].append(self.sorted[:half]).sort_values()\n        )\n        self.key = self.sorted[N // 4]""}",index_object.Indexing,time,0.05629323779060393
24477,class-level,"terminus-2,gpt-5",1.0521344820479284,0.7453157545798069,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.RankWithTies,groupby,"{""groupby.RankWithTies.time_rank_ties"": ""class RankWithTies:\n    def time_rank_ties(self, dtype, tie_method):\n        self.df.groupby(\""key\"").rank(method=tie_method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass RankWithTies:\n    def setup(self, dtype, tie_method):\n        N = 10**4\n        if dtype == \""datetime64\"":\n            data = np.array([Timestamp(\""2011/01/01\"")] * N, dtype=dtype)\n        else:\n            data = np.array([1] * N, dtype=dtype)\n        self.df = DataFrame({\""values\"": data, \""key\"": [\""foo\""] * N})""}",groupby.RankWithTies,time,0.2169863702037634
24481,class-level,"terminus-2,gpt-5",1.1057859716008107,1.1141947051174346,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,hash_functions.UniqueAndFactorizeArange,hash_functions,"{""hash_functions.UniqueAndFactorizeArange.time_factorize"": ""class UniqueAndFactorizeArange:\n    def time_factorize(self, exponent):\n        pd.factorize(self.a2)\n\n    def setup(self, exponent):\n        a = np.arange(10**4, dtype=\""float64\"")\n        self.a2 = (a + 10**exponent).repeat(100)""}",hash_functions.UniqueAndFactorizeArange,time,-0.005946770520950394
24475,class-level,"terminus-2,gpt-5",1.0701570395163813,1.070948001002442,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.Cumulative,groupby,"{""groupby.Cumulative.time_frame_transform"": ""class Cumulative:\n    def time_frame_transform(self, dtype, method, with_nans):\n        self.df.groupby(\""key\"").transform(method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cumulative:\n    def setup(self, dtype, method, with_nans):\n        if with_nans and dtype == \""int64\"":\n            raise NotImplementedError(\""Construction of df would raise\"")\n    \n        N = 500_000\n        keys = np.random.randint(0, 100, size=N)\n        vals = np.random.randint(-10, 10, (N, 5))\n    \n        if with_nans:\n            null_vals = vals.astype(float, copy=True)\n            null_vals[::2, :] = np.nan\n            null_vals[::3, :] = np.nan\n            df = DataFrame(null_vals, columns=list(\""abcde\""), dtype=dtype)\n            df[\""key\""] = keys\n            self.df = df\n        else:\n            df = DataFrame(vals, columns=list(\""abcde\"")).astype(dtype, copy=False)\n            df[\""key\""] = keys\n            self.df = df""}",groupby.Cumulative,time,-0.0005593787030131648
24480,class-level,"terminus-2,gpt-5",1.0902066194515143,1.0353260541593894,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,hash_functions.NumericSeriesIndexing,hash_functions,"{""hash_functions.NumericSeriesIndexing.time_loc_slice"": ""class NumericSeriesIndexing:\n    def time_loc_slice(self, index, N):\n        # trigger building of mapping\n        self.data.loc[:800]\n\n    def setup(self, dtype, N):\n        vals = np.array(list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype)\n        indices = pd.Index(vals)\n        self.data = pd.Series(np.arange(N), index=indices)""}",hash_functions.NumericSeriesIndexing,time,0.03881228097038538
24486,class-level,"terminus-2,gpt-5",1.0684170052273432,1.0333263660754053,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.DatetimeIndexIndexing,indexing,"{""indexing.DatetimeIndexIndexing.time_get_indexer_mismatched_tz"": ""class DatetimeIndexIndexing:\n    def time_get_indexer_mismatched_tz(self):\n        # reached via e.g.\n        #  ser = Series(range(len(dti)), index=dti)\n        #  ser[dti2]\n        self.dti.get_indexer(self.dti2)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndexIndexing:\n    def setup(self):\n        dti = date_range(\""2016-01-01\"", periods=10000, tz=\""US/Pacific\"")\n        dti2 = dti.tz_convert(\""UTC\"")\n        self.dti = dti\n        self.dti2 = dti2""}",indexing.DatetimeIndexIndexing,time,0.024816576486519016
24488,class-level,"terminus-2,gpt-5",1.1218583187860265,1.0568560442684949,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.MultiIndexing,indexing,"{""indexing.MultiIndexing.time_loc_all_null_slices"": ""class MultiIndexing:\n    def time_loc_all_null_slices(self, unique_levels):\n        target = tuple([self.tgt_null_slice] * self.nlevels)\n        self.df.loc[target, :]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MultiIndexing:\n    def setup(self, unique_levels):\n        self.nlevels = 2\n        if unique_levels:\n            mi = MultiIndex.from_arrays([range(1000000)] * self.nlevels)\n        else:\n            mi = MultiIndex.from_product([range(1000)] * self.nlevels)\n        self.df = DataFrame(np.random.randn(len(mi)), index=mi)\n    \n        self.tgt_slice = slice(200, 800)\n        self.tgt_null_slice = slice(None)\n        self.tgt_list = list(range(0, 1000, 10))\n        self.tgt_scalar = 500\n    \n        bool_indexer = np.zeros(len(mi), dtype=np.bool_)\n        bool_indexer[slice(0, len(mi), 100)] = True\n        self.tgt_bool_indexer = bool_indexer"", ""indexing.MultiIndexing.time_loc_all_scalars"": ""class MultiIndexing:\n    def time_loc_all_scalars(self, unique_levels):\n        target = tuple([self.tgt_scalar] * self.nlevels)\n        self.df.loc[target, :]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MultiIndexing:\n    def setup(self, unique_levels):\n        self.nlevels = 2\n        if unique_levels:\n            mi = MultiIndex.from_arrays([range(1000000)] * self.nlevels)\n        else:\n            mi = MultiIndex.from_product([range(1000)] * self.nlevels)\n        self.df = DataFrame(np.random.randn(len(mi)), index=mi)\n    \n        self.tgt_slice = slice(200, 800)\n        self.tgt_null_slice = slice(None)\n        self.tgt_list = list(range(0, 1000, 10))\n        self.tgt_scalar = 500\n    \n        bool_indexer = np.zeros(len(mi), dtype=np.bool_)\n        bool_indexer[slice(0, len(mi), 100)] = True\n        self.tgt_bool_indexer = bool_indexer"", ""indexing.MultiIndexing.time_loc_null_slice_plus_slice"": ""class MultiIndexing:\n    def time_loc_null_slice_plus_slice(self, unique_levels):\n        target = (self.tgt_null_slice, self.tgt_slice)\n        self.df.loc[target, :]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MultiIndexing:\n    def setup(self, unique_levels):\n        self.nlevels = 2\n        if unique_levels:\n            mi = MultiIndex.from_arrays([range(1000000)] * self.nlevels)\n        else:\n            mi = MultiIndex.from_product([range(1000)] * self.nlevels)\n        self.df = DataFrame(np.random.randn(len(mi)), index=mi)\n    \n        self.tgt_slice = slice(200, 800)\n        self.tgt_null_slice = slice(None)\n        self.tgt_list = list(range(0, 1000, 10))\n        self.tgt_scalar = 500\n    \n        bool_indexer = np.zeros(len(mi), dtype=np.bool_)\n        bool_indexer[slice(0, len(mi), 100)] = True\n        self.tgt_bool_indexer = bool_indexer"", ""indexing.MultiIndexing.time_xs_full_key"": ""class MultiIndexing:\n    def time_xs_full_key(self, unique_levels):\n        target = tuple([self.tgt_scalar] * self.nlevels)\n        self.df.xs(target)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MultiIndexing:\n    def setup(self, unique_levels):\n        self.nlevels = 2\n        if unique_levels:\n            mi = MultiIndex.from_arrays([range(1000000)] * self.nlevels)\n        else:\n            mi = MultiIndex.from_product([range(1000)] * self.nlevels)\n        self.df = DataFrame(np.random.randn(len(mi)), index=mi)\n    \n        self.tgt_slice = slice(200, 800)\n        self.tgt_null_slice = slice(None)\n        self.tgt_list = list(range(0, 1000, 10))\n        self.tgt_scalar = 500\n    \n        bool_indexer = np.zeros(len(mi), dtype=np.bool_)\n        bool_indexer[slice(0, len(mi), 100)] = True\n        self.tgt_bool_indexer = bool_indexer"", ""indexing.MultiIndexing.time_xs_level_1"": ""class MultiIndexing:\n    def time_xs_level_1(self, unique_levels):\n        target = self.tgt_scalar\n        self.df.xs(target, level=1)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MultiIndexing:\n    def setup(self, unique_levels):\n        self.nlevels = 2\n        if unique_levels:\n            mi = MultiIndex.from_arrays([range(1000000)] * self.nlevels)\n        else:\n            mi = MultiIndex.from_product([range(1000)] * self.nlevels)\n        self.df = DataFrame(np.random.randn(len(mi)), index=mi)\n    \n        self.tgt_slice = slice(200, 800)\n        self.tgt_null_slice = slice(None)\n        self.tgt_list = list(range(0, 1000, 10))\n        self.tgt_scalar = 500\n    \n        bool_indexer = np.zeros(len(mi), dtype=np.bool_)\n        bool_indexer[slice(0, len(mi), 100)] = True\n        self.tgt_bool_indexer = bool_indexer""}",indexing.MultiIndexing,time,0.04597049117222886
24489,class-level,"terminus-2,gpt-5",1.0445544469464385,1.0410766696393976,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NonNumericSeriesIndexing,indexing,"{""indexing.NonNumericSeriesIndexing.time_getitem_label_slice"": ""class NonNumericSeriesIndexing:\n    def time_getitem_label_slice(self, index, index_structure):\n        self.s[: self.lbl]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]"", ""indexing.NonNumericSeriesIndexing.time_getitem_list_like"": ""class NonNumericSeriesIndexing:\n    def time_getitem_list_like(self, index, index_structure):\n        self.s[[self.lbl]]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]"", ""indexing.NonNumericSeriesIndexing.time_getitem_pos_slice"": ""class NonNumericSeriesIndexing:\n    def time_getitem_pos_slice(self, index, index_structure):\n        self.s[:80000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]"", ""indexing.NonNumericSeriesIndexing.time_getitem_scalar"": ""class NonNumericSeriesIndexing:\n    def time_getitem_scalar(self, index, index_structure):\n        self.s[self.lbl]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]""}",indexing.NonNumericSeriesIndexing,time,0.0024595313345409536
24485,class-level,"terminus-2,gpt-5",1.0043937794419442,0.9875688486727868,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.CategoricalIndexIndexing,indexing,"{""indexing.CategoricalIndexIndexing.time_get_indexer_list"": ""class CategoricalIndexIndexing:\n    def time_get_indexer_list(self, index):\n        self.data_unique.get_indexer(self.cat_list)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalIndexIndexing:\n    def setup(self, index):\n        N = 10**5\n        values = list(\""a\"" * N + \""b\"" * N + \""c\"" * N)\n        indices = {\n            \""monotonic_incr\"": CategoricalIndex(values),\n            \""monotonic_decr\"": CategoricalIndex(reversed(values)),\n            \""non_monotonic\"": CategoricalIndex(list(\""abc\"" * N)),\n        }\n        self.data = indices[index]\n        self.data_unique = CategoricalIndex([str(i) for i in range(N * 3)])\n    \n        self.int_scalar = 10000\n        self.int_list = list(range(10000))\n    \n        self.cat_scalar = \""b\""\n        self.cat_list = [\""1\"", \""3\""]"", ""indexing.CategoricalIndexIndexing.time_getitem_list"": ""class CategoricalIndexIndexing:\n    def time_getitem_list(self, index):\n        self.data[self.int_list]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalIndexIndexing:\n    def setup(self, index):\n        N = 10**5\n        values = list(\""a\"" * N + \""b\"" * N + \""c\"" * N)\n        indices = {\n            \""monotonic_incr\"": CategoricalIndex(values),\n            \""monotonic_decr\"": CategoricalIndex(reversed(values)),\n            \""non_monotonic\"": CategoricalIndex(list(\""abc\"" * N)),\n        }\n        self.data = indices[index]\n        self.data_unique = CategoricalIndex([str(i) for i in range(N * 3)])\n    \n        self.int_scalar = 10000\n        self.int_list = list(range(10000))\n    \n        self.cat_scalar = \""b\""\n        self.cat_list = [\""1\"", \""3\""]"", ""indexing.CategoricalIndexIndexing.time_getitem_scalar"": ""class CategoricalIndexIndexing:\n    def time_getitem_scalar(self, index):\n        self.data[self.int_scalar]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalIndexIndexing:\n    def setup(self, index):\n        N = 10**5\n        values = list(\""a\"" * N + \""b\"" * N + \""c\"" * N)\n        indices = {\n            \""monotonic_incr\"": CategoricalIndex(values),\n            \""monotonic_decr\"": CategoricalIndex(reversed(values)),\n            \""non_monotonic\"": CategoricalIndex(list(\""abc\"" * N)),\n        }\n        self.data = indices[index]\n        self.data_unique = CategoricalIndex([str(i) for i in range(N * 3)])\n    \n        self.int_scalar = 10000\n        self.int_list = list(range(10000))\n    \n        self.cat_scalar = \""b\""\n        self.cat_list = [\""1\"", \""3\""]""}",indexing.CategoricalIndexIndexing,time,0.011898819497282437
24490,class-level,"terminus-2,gpt-5",1.2467143292758482,1.2314549995620576,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericMaskedIndexing,indexing,"{""indexing.NumericMaskedIndexing.time_get_indexer"": ""class NumericMaskedIndexing:\n    def time_get_indexer(self, dtype, monotonic):\n        self.data.get_indexer(self.indexer)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericMaskedIndexing:\n    def setup(self, dtype, monotonic):\n        indices = {\n            True: Index(self.monotonic_list, dtype=dtype),\n            False: Index(self.non_monotonic_list, dtype=dtype).append(\n                Index([NA], dtype=dtype)\n            ),\n        }\n        self.data = indices[monotonic]\n        self.indexer = np.arange(300, 1_000)\n        self.data_dups = self.data.append(self.data)"", ""indexing.NumericMaskedIndexing.time_get_indexer_dups"": ""class NumericMaskedIndexing:\n    def time_get_indexer_dups(self, dtype, monotonic):\n        self.data.get_indexer_for(self.indexer)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericMaskedIndexing:\n    def setup(self, dtype, monotonic):\n        indices = {\n            True: Index(self.monotonic_list, dtype=dtype),\n            False: Index(self.non_monotonic_list, dtype=dtype).append(\n                Index([NA], dtype=dtype)\n            ),\n        }\n        self.data = indices[monotonic]\n        self.indexer = np.arange(300, 1_000)\n        self.data_dups = self.data.append(self.data)""}",indexing.NumericMaskedIndexing,time,0.010791605172412042
24491,class-level,"terminus-2,gpt-5",1.1619154360619786,1.192686485776998,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericSeriesIndexing,indexing,"{""indexing.NumericSeriesIndexing.time_getitem_array"": ""class NumericSeriesIndexing:\n    def time_getitem_array(self, index, index_structure):\n        self.data[self.array]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_getitem_list_like"": ""class NumericSeriesIndexing:\n    def time_getitem_list_like(self, index, index_structure):\n        self.data[[800000]]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_getitem_lists"": ""class NumericSeriesIndexing:\n    def time_getitem_lists(self, index, index_structure):\n        self.data[self.array_list]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_getitem_scalar"": ""class NumericSeriesIndexing:\n    def time_getitem_scalar(self, index, index_structure):\n        self.data[800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_getitem_slice"": ""class NumericSeriesIndexing:\n    def time_getitem_slice(self, index, index_structure):\n        self.data[:800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_iloc_slice"": ""class NumericSeriesIndexing:\n    def time_iloc_slice(self, index, index_structure):\n        self.data.iloc[:800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_loc_array"": ""class NumericSeriesIndexing:\n    def time_loc_array(self, index, index_structure):\n        self.data.loc[self.array]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_loc_slice"": ""class NumericSeriesIndexing:\n    def time_loc_slice(self, index, index_structure):\n        self.data.loc[:800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.NumericSeriesIndexing,time,-0.021761704183182136
24487,class-level,"terminus-2,gpt-5",1.0189707786432889,1.038504163552764,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.IndexSingleRow,indexing,"{""indexing.IndexSingleRow.time_loc_row"": ""class IndexSingleRow:\n    def time_loc_row(self, unique_cols):\n        self.df.loc[10000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IndexSingleRow:\n    def setup(self, unique_cols):\n        arr = np.arange(10**7).reshape(-1, 10)\n        df = DataFrame(arr)\n        dtypes = [\""u1\"", \""u2\"", \""u4\"", \""u8\"", \""i1\"", \""i2\"", \""i4\"", \""i8\"", \""f8\"", \""f4\""]\n        for i, d in enumerate(dtypes):\n            df[i] = df[i].astype(d)\n    \n        if not unique_cols:\n            # GH#33032 single-row lookups with non-unique columns were\n            #  15x slower than with unique columns\n            df.columns = [\""A\"", \""A\""] + list(df.columns[2:])\n    \n        self.df = df""}",indexing.IndexSingleRow,time,-0.013814275042061673
24493,class-level,"terminus-2,gpt-5",1.0369473765631911,1.0426701947055392,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing_engines.NumericEngineIndexing,indexing_engines,"{""indexing_engines.NumericEngineIndexing.time_get_loc"": ""class NumericEngineIndexing:\n    def time_get_loc(self, engine_and_dtype, index_type, unique, N):\n        self.data.get_loc(self.key_early)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)[::-1]\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.empty(N * 3, dtype=dtype)\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype)\n                arr[N:] = np.arange(N * 2, dtype=dtype)\n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype)\n    \n        self.data = engine(arr)\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]"", ""indexing_engines.NumericEngineIndexing.time_get_loc_near_middle"": ""class NumericEngineIndexing:\n    def time_get_loc_near_middle(self, engine_and_dtype, index_type, unique, N):\n        # searchsorted performance may be different near the middle of a range\n        #  vs near an endpoint\n        self.data.get_loc(self.key_middle)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)[::-1]\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.empty(N * 3, dtype=dtype)\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype)\n                arr[N:] = np.arange(N * 2, dtype=dtype)\n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype)\n    \n        self.data = engine(arr)\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]""}",indexing_engines.NumericEngineIndexing,time,-0.004047254697558767
24492,class-level,"terminus-2,gpt-5",1.0913883914544866,1.0715801419827728,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing_engines.MaskedNumericEngineIndexing,indexing_engines,"{""indexing_engines.MaskedNumericEngineIndexing.time_get_loc"": ""class MaskedNumericEngineIndexing:\n    def time_get_loc(self, engine_and_dtype, index_type, unique, N):\n        self.data.get_loc(self.key_early)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())[::-1]\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.zeros(N * 3, dtype=dtype.lower())\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype.lower())\n                arr[N:] = np.arange(N * 2, dtype=dtype.lower())\n    \n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n            mask[-1] = True\n    \n        self.data = engine(BaseMaskedArray(arr, mask))\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]"", ""indexing_engines.MaskedNumericEngineIndexing.time_get_loc_near_middle"": ""class MaskedNumericEngineIndexing:\n    def time_get_loc_near_middle(self, engine_and_dtype, index_type, unique, N):\n        # searchsorted performance may be different near the middle of a range\n        #  vs near an endpoint\n        self.data.get_loc(self.key_middle)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())[::-1]\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.zeros(N * 3, dtype=dtype.lower())\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype.lower())\n                arr[N:] = np.arange(N * 2, dtype=dtype.lower())\n    \n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n            mask[-1] = True\n    \n        self.data = engine(BaseMaskedArray(arr, mask))\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]""}",indexing_engines.MaskedNumericEngineIndexing,time,0.014008662992725445
24494,class-level,"terminus-2,gpt-5",1.1446884577534786,1.121376298596207,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.csv.ReadCSVCategorical,io.csv,"{""io.csv.ReadCSVCategorical.time_convert_post"": ""class ReadCSVCategorical:\n    def time_convert_post(self, engine):\n        read_csv(self.fname, engine=engine).apply(Categorical)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVCategorical:\n    def setup(self, engine):\n        N = 100000\n        group1 = [\""aaaaaaaa\"", \""bbbbbbb\"", \""cccccccc\"", \""dddddddd\"", \""eeeeeeee\""]\n        df = DataFrame(np.random.choice(group1, (N, 3)), columns=list(\""abc\""))\n        df.to_csv(self.fname, index=False)""}",io.csv.ReadCSVCategorical,time,0.016486675500192068
24498,class-level,"terminus-2,gpt-5",1.0319564503661036,1.019150027730217,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.json.ToJSON,io.json,"{""io.json.ToJSON.time_to_json"": ""class ToJSON:\n    def time_to_json(self, orient, frame):\n        getattr(self, frame).to_json(self.fname, orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSON:\n    def setup(self, orient, frame):\n        N = 10**5\n        ncols = 5\n        index = date_range(\""20000101\"", periods=N, freq=\""H\"")\n        timedeltas = timedelta_range(start=1, periods=N, freq=\""s\"")\n        datetimes = date_range(start=1, periods=N, freq=\""s\"")\n        ints = np.random.randint(100000000, size=N)\n        longints = sys.maxsize * np.random.randint(100000000, size=N)\n        floats = np.random.randn(N)\n        strings = tm.makeStringIndex(N)\n        self.df = DataFrame(np.random.randn(N, ncols), index=np.arange(N))\n        self.df_date_idx = DataFrame(np.random.randn(N, ncols), index=index)\n        self.df_td_int_ts = DataFrame(\n            {\n                \""td_1\"": timedeltas,\n                \""td_2\"": timedeltas,\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""ts_1\"": datetimes,\n                \""ts_2\"": datetimes,\n            },\n            index=index,\n        )\n        self.df_int_floats = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""int_3\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""float_3\"": floats,\n            },\n            index=index,\n        )\n        self.df_int_float_str = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )\n    \n        self.df_longint_float_str = DataFrame(\n            {\n                \""longint_1\"": longints,\n                \""longint_2\"": longints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )""}",io.json.ToJSON,time,0.009056875980117826
24496,class-level,"terminus-2,gpt-5",1.0643244568159138,1.1630325669190456,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.hdf.HDFStoreDataFrame,io.hdf,"{""io.hdf.HDFStoreDataFrame.time_query_store_table"": ""class HDFStoreDataFrame:\n    def time_query_store_table(self):\n        self.store.select(\""table\"", where=\""index > self.start and index < self.stop\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass HDFStoreDataFrame:\n    def setup(self):\n        N = 25000\n        index = tm.makeStringIndex(N)\n        self.df = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)}, index=index\n        )\n        self.df_mixed = DataFrame(\n            {\n                \""float1\"": np.random.randn(N),\n                \""float2\"": np.random.randn(N),\n                \""string1\"": [\""foo\""] * N,\n                \""bool1\"": [True] * N,\n                \""int1\"": np.random.randint(0, N, size=N),\n            },\n            index=index,\n        )\n        self.df_wide = DataFrame(np.random.randn(N, 100))\n        self.start_wide = self.df_wide.index[10000]\n        self.stop_wide = self.df_wide.index[15000]\n        self.df2 = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)},\n            index=date_range(\""1/1/2000\"", periods=N),\n        )\n        self.start = self.df2.index[10000]\n        self.stop = self.df2.index[15000]\n        self.df_wide2 = DataFrame(\n            np.random.randn(N, 100), index=date_range(\""1/1/2000\"", periods=N)\n        )\n        self.df_dc = DataFrame(\n            np.random.randn(N, 10), columns=[f\""C{i:03d}\"" for i in range(10)]\n        )\n    \n        self.fname = \""__test__.h5\""\n    \n        self.store = HDFStore(self.fname)\n        self.store.put(\""fixed\"", self.df)\n        self.store.put(\""fixed_mixed\"", self.df_mixed)\n        self.store.append(\""table\"", self.df2)\n        self.store.append(\""table_mixed\"", self.df_mixed)\n        self.store.append(\""table_wide\"", self.df_wide)\n        self.store.append(\""table_wide2\"", self.df_wide2)"", ""io.hdf.HDFStoreDataFrame.time_query_store_table_wide"": ""class HDFStoreDataFrame:\n    def time_query_store_table_wide(self):\n        self.store.select(\n            \""table_wide\"", where=\""index > self.start_wide and index < self.stop_wide\""\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass HDFStoreDataFrame:\n    def setup(self):\n        N = 25000\n        index = tm.makeStringIndex(N)\n        self.df = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)}, index=index\n        )\n        self.df_mixed = DataFrame(\n            {\n                \""float1\"": np.random.randn(N),\n                \""float2\"": np.random.randn(N),\n                \""string1\"": [\""foo\""] * N,\n                \""bool1\"": [True] * N,\n                \""int1\"": np.random.randint(0, N, size=N),\n            },\n            index=index,\n        )\n        self.df_wide = DataFrame(np.random.randn(N, 100))\n        self.start_wide = self.df_wide.index[10000]\n        self.stop_wide = self.df_wide.index[15000]\n        self.df2 = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)},\n            index=date_range(\""1/1/2000\"", periods=N),\n        )\n        self.start = self.df2.index[10000]\n        self.stop = self.df2.index[15000]\n        self.df_wide2 = DataFrame(\n            np.random.randn(N, 100), index=date_range(\""1/1/2000\"", periods=N)\n        )\n        self.df_dc = DataFrame(\n            np.random.randn(N, 10), columns=[f\""C{i:03d}\"" for i in range(10)]\n        )\n    \n        self.fname = \""__test__.h5\""\n    \n        self.store = HDFStore(self.fname)\n        self.store.put(\""fixed\"", self.df)\n        self.store.put(\""fixed_mixed\"", self.df_mixed)\n        self.store.append(\""table\"", self.df2)\n        self.store.append(\""table_mixed\"", self.df_mixed)\n        self.store.append(\""table_wide\"", self.df_wide)\n        self.store.append(\""table_wide2\"", self.df_wide2)"", ""io.hdf.HDFStoreDataFrame.time_read_store"": ""class HDFStoreDataFrame:\n    def time_read_store(self):\n        self.store.get(\""fixed\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass HDFStoreDataFrame:\n    def setup(self):\n        N = 25000\n        index = tm.makeStringIndex(N)\n        self.df = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)}, index=index\n        )\n        self.df_mixed = DataFrame(\n            {\n                \""float1\"": np.random.randn(N),\n                \""float2\"": np.random.randn(N),\n                \""string1\"": [\""foo\""] * N,\n                \""bool1\"": [True] * N,\n                \""int1\"": np.random.randint(0, N, size=N),\n            },\n            index=index,\n        )\n        self.df_wide = DataFrame(np.random.randn(N, 100))\n        self.start_wide = self.df_wide.index[10000]\n        self.stop_wide = self.df_wide.index[15000]\n        self.df2 = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)},\n            index=date_range(\""1/1/2000\"", periods=N),\n        )\n        self.start = self.df2.index[10000]\n        self.stop = self.df2.index[15000]\n        self.df_wide2 = DataFrame(\n            np.random.randn(N, 100), index=date_range(\""1/1/2000\"", periods=N)\n        )\n        self.df_dc = DataFrame(\n            np.random.randn(N, 10), columns=[f\""C{i:03d}\"" for i in range(10)]\n        )\n    \n        self.fname = \""__test__.h5\""\n    \n        self.store = HDFStore(self.fname)\n        self.store.put(\""fixed\"", self.df)\n        self.store.put(\""fixed_mixed\"", self.df_mixed)\n        self.store.append(\""table\"", self.df2)\n        self.store.append(\""table_mixed\"", self.df_mixed)\n        self.store.append(\""table_wide\"", self.df_wide)\n        self.store.append(\""table_wide2\"", self.df_wide2)"", ""io.hdf.HDFStoreDataFrame.time_read_store_table"": ""class HDFStoreDataFrame:\n    def time_read_store_table(self):\n        self.store.select(\""table\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass HDFStoreDataFrame:\n    def setup(self):\n        N = 25000\n        index = tm.makeStringIndex(N)\n        self.df = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)}, index=index\n        )\n        self.df_mixed = DataFrame(\n            {\n                \""float1\"": np.random.randn(N),\n                \""float2\"": np.random.randn(N),\n                \""string1\"": [\""foo\""] * N,\n                \""bool1\"": [True] * N,\n                \""int1\"": np.random.randint(0, N, size=N),\n            },\n            index=index,\n        )\n        self.df_wide = DataFrame(np.random.randn(N, 100))\n        self.start_wide = self.df_wide.index[10000]\n        self.stop_wide = self.df_wide.index[15000]\n        self.df2 = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)},\n            index=date_range(\""1/1/2000\"", periods=N),\n        )\n        self.start = self.df2.index[10000]\n        self.stop = self.df2.index[15000]\n        self.df_wide2 = DataFrame(\n            np.random.randn(N, 100), index=date_range(\""1/1/2000\"", periods=N)\n        )\n        self.df_dc = DataFrame(\n            np.random.randn(N, 10), columns=[f\""C{i:03d}\"" for i in range(10)]\n        )\n    \n        self.fname = \""__test__.h5\""\n    \n        self.store = HDFStore(self.fname)\n        self.store.put(\""fixed\"", self.df)\n        self.store.put(\""fixed_mixed\"", self.df_mixed)\n        self.store.append(\""table\"", self.df2)\n        self.store.append(\""table_mixed\"", self.df_mixed)\n        self.store.append(\""table_wide\"", self.df_wide)\n        self.store.append(\""table_wide2\"", self.df_wide2)"", ""io.hdf.HDFStoreDataFrame.time_read_store_table_mixed"": ""class HDFStoreDataFrame:\n    def time_read_store_table_mixed(self):\n        self.store.select(\""table_mixed\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass HDFStoreDataFrame:\n    def setup(self):\n        N = 25000\n        index = tm.makeStringIndex(N)\n        self.df = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)}, index=index\n        )\n        self.df_mixed = DataFrame(\n            {\n                \""float1\"": np.random.randn(N),\n                \""float2\"": np.random.randn(N),\n                \""string1\"": [\""foo\""] * N,\n                \""bool1\"": [True] * N,\n                \""int1\"": np.random.randint(0, N, size=N),\n            },\n            index=index,\n        )\n        self.df_wide = DataFrame(np.random.randn(N, 100))\n        self.start_wide = self.df_wide.index[10000]\n        self.stop_wide = self.df_wide.index[15000]\n        self.df2 = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)},\n            index=date_range(\""1/1/2000\"", periods=N),\n        )\n        self.start = self.df2.index[10000]\n        self.stop = self.df2.index[15000]\n        self.df_wide2 = DataFrame(\n            np.random.randn(N, 100), index=date_range(\""1/1/2000\"", periods=N)\n        )\n        self.df_dc = DataFrame(\n            np.random.randn(N, 10), columns=[f\""C{i:03d}\"" for i in range(10)]\n        )\n    \n        self.fname = \""__test__.h5\""\n    \n        self.store = HDFStore(self.fname)\n        self.store.put(\""fixed\"", self.df)\n        self.store.put(\""fixed_mixed\"", self.df_mixed)\n        self.store.append(\""table\"", self.df2)\n        self.store.append(\""table_mixed\"", self.df_mixed)\n        self.store.append(\""table_wide\"", self.df_wide)\n        self.store.append(\""table_wide2\"", self.df_wide2)"", ""io.hdf.HDFStoreDataFrame.time_read_store_table_wide"": ""class HDFStoreDataFrame:\n    def time_read_store_table_wide(self):\n        self.store.select(\""table_wide\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass HDFStoreDataFrame:\n    def setup(self):\n        N = 25000\n        index = tm.makeStringIndex(N)\n        self.df = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)}, index=index\n        )\n        self.df_mixed = DataFrame(\n            {\n                \""float1\"": np.random.randn(N),\n                \""float2\"": np.random.randn(N),\n                \""string1\"": [\""foo\""] * N,\n                \""bool1\"": [True] * N,\n                \""int1\"": np.random.randint(0, N, size=N),\n            },\n            index=index,\n        )\n        self.df_wide = DataFrame(np.random.randn(N, 100))\n        self.start_wide = self.df_wide.index[10000]\n        self.stop_wide = self.df_wide.index[15000]\n        self.df2 = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)},\n            index=date_range(\""1/1/2000\"", periods=N),\n        )\n        self.start = self.df2.index[10000]\n        self.stop = self.df2.index[15000]\n        self.df_wide2 = DataFrame(\n            np.random.randn(N, 100), index=date_range(\""1/1/2000\"", periods=N)\n        )\n        self.df_dc = DataFrame(\n            np.random.randn(N, 10), columns=[f\""C{i:03d}\"" for i in range(10)]\n        )\n    \n        self.fname = \""__test__.h5\""\n    \n        self.store = HDFStore(self.fname)\n        self.store.put(\""fixed\"", self.df)\n        self.store.put(\""fixed_mixed\"", self.df_mixed)\n        self.store.append(\""table\"", self.df2)\n        self.store.append(\""table_mixed\"", self.df_mixed)\n        self.store.append(\""table_wide\"", self.df_wide)\n        self.store.append(\""table_wide2\"", self.df_wide2)"", ""io.hdf.HDFStoreDataFrame.time_store_info"": ""class HDFStoreDataFrame:\n    def time_store_info(self):\n        self.store.info()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass HDFStoreDataFrame:\n    def setup(self):\n        N = 25000\n        index = tm.makeStringIndex(N)\n        self.df = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)}, index=index\n        )\n        self.df_mixed = DataFrame(\n            {\n                \""float1\"": np.random.randn(N),\n                \""float2\"": np.random.randn(N),\n                \""string1\"": [\""foo\""] * N,\n                \""bool1\"": [True] * N,\n                \""int1\"": np.random.randint(0, N, size=N),\n            },\n            index=index,\n        )\n        self.df_wide = DataFrame(np.random.randn(N, 100))\n        self.start_wide = self.df_wide.index[10000]\n        self.stop_wide = self.df_wide.index[15000]\n        self.df2 = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)},\n            index=date_range(\""1/1/2000\"", periods=N),\n        )\n        self.start = self.df2.index[10000]\n        self.stop = self.df2.index[15000]\n        self.df_wide2 = DataFrame(\n            np.random.randn(N, 100), index=date_range(\""1/1/2000\"", periods=N)\n        )\n        self.df_dc = DataFrame(\n            np.random.randn(N, 10), columns=[f\""C{i:03d}\"" for i in range(10)]\n        )\n    \n        self.fname = \""__test__.h5\""\n    \n        self.store = HDFStore(self.fname)\n        self.store.put(\""fixed\"", self.df)\n        self.store.put(\""fixed_mixed\"", self.df_mixed)\n        self.store.append(\""table\"", self.df2)\n        self.store.append(\""table_mixed\"", self.df_mixed)\n        self.store.append(\""table_wide\"", self.df_wide)\n        self.store.append(\""table_wide2\"", self.df_wide2)"", ""io.hdf.HDFStoreDataFrame.time_store_str"": ""class HDFStoreDataFrame:\n    def time_store_str(self):\n        str(self.store)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass HDFStoreDataFrame:\n    def setup(self):\n        N = 25000\n        index = tm.makeStringIndex(N)\n        self.df = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)}, index=index\n        )\n        self.df_mixed = DataFrame(\n            {\n                \""float1\"": np.random.randn(N),\n                \""float2\"": np.random.randn(N),\n                \""string1\"": [\""foo\""] * N,\n                \""bool1\"": [True] * N,\n                \""int1\"": np.random.randint(0, N, size=N),\n            },\n            index=index,\n        )\n        self.df_wide = DataFrame(np.random.randn(N, 100))\n        self.start_wide = self.df_wide.index[10000]\n        self.stop_wide = self.df_wide.index[15000]\n        self.df2 = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)},\n            index=date_range(\""1/1/2000\"", periods=N),\n        )\n        self.start = self.df2.index[10000]\n        self.stop = self.df2.index[15000]\n        self.df_wide2 = DataFrame(\n            np.random.randn(N, 100), index=date_range(\""1/1/2000\"", periods=N)\n        )\n        self.df_dc = DataFrame(\n            np.random.randn(N, 10), columns=[f\""C{i:03d}\"" for i in range(10)]\n        )\n    \n        self.fname = \""__test__.h5\""\n    \n        self.store = HDFStore(self.fname)\n        self.store.put(\""fixed\"", self.df)\n        self.store.put(\""fixed_mixed\"", self.df_mixed)\n        self.store.append(\""table\"", self.df2)\n        self.store.append(\""table_mixed\"", self.df_mixed)\n        self.store.append(\""table_wide\"", self.df_wide)\n        self.store.append(\""table_wide2\"", self.df_wide2)"", ""io.hdf.HDFStoreDataFrame.time_write_store"": ""class HDFStoreDataFrame:\n    def time_write_store(self):\n        self.store.put(\""fixed_write\"", self.df)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass HDFStoreDataFrame:\n    def setup(self):\n        N = 25000\n        index = tm.makeStringIndex(N)\n        self.df = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)}, index=index\n        )\n        self.df_mixed = DataFrame(\n            {\n                \""float1\"": np.random.randn(N),\n                \""float2\"": np.random.randn(N),\n                \""string1\"": [\""foo\""] * N,\n                \""bool1\"": [True] * N,\n                \""int1\"": np.random.randint(0, N, size=N),\n            },\n            index=index,\n        )\n        self.df_wide = DataFrame(np.random.randn(N, 100))\n        self.start_wide = self.df_wide.index[10000]\n        self.stop_wide = self.df_wide.index[15000]\n        self.df2 = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)},\n            index=date_range(\""1/1/2000\"", periods=N),\n        )\n        self.start = self.df2.index[10000]\n        self.stop = self.df2.index[15000]\n        self.df_wide2 = DataFrame(\n            np.random.randn(N, 100), index=date_range(\""1/1/2000\"", periods=N)\n        )\n        self.df_dc = DataFrame(\n            np.random.randn(N, 10), columns=[f\""C{i:03d}\"" for i in range(10)]\n        )\n    \n        self.fname = \""__test__.h5\""\n    \n        self.store = HDFStore(self.fname)\n        self.store.put(\""fixed\"", self.df)\n        self.store.put(\""fixed_mixed\"", self.df_mixed)\n        self.store.append(\""table\"", self.df2)\n        self.store.append(\""table_mixed\"", self.df_mixed)\n        self.store.append(\""table_wide\"", self.df_wide)\n        self.store.append(\""table_wide2\"", self.df_wide2)""}",io.hdf.HDFStoreDataFrame,time,-0.06980771577307761
24503,class-level,"terminus-2,gpt-5",1.048828451552811,1.0651493160553005,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,join_merge.MergeAsof,join_merge,"{""join_merge.MergeAsof.time_by_int"": ""class MergeAsof:\n    def time_by_int(self, direction, tolerance):\n        merge_asof(\n            self.df1c,\n            self.df2c,\n            on=\""time\"",\n            by=\""key2\"",\n            direction=direction,\n            tolerance=tolerance,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MergeAsof:\n    def setup(self, direction, tolerance):\n        one_count = 200000\n        two_count = 1000000\n    \n        df1 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, one_count / 20, one_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), one_count),\n                \""key2\"": np.random.randint(0, 25, one_count),\n                \""value1\"": np.random.randn(one_count),\n            }\n        )\n        df2 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, two_count / 20, two_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), two_count),\n                \""key2\"": np.random.randint(0, 25, two_count),\n                \""value2\"": np.random.randn(two_count),\n            }\n        )\n    \n        df1 = df1.sort_values(\""time\"")\n        df2 = df2.sort_values(\""time\"")\n    \n        df1[\""time32\""] = np.int32(df1.time)\n        df2[\""time32\""] = np.int32(df2.time)\n    \n        df1[\""timeu64\""] = np.uint64(df1.time)\n        df2[\""timeu64\""] = np.uint64(df2.time)\n    \n        self.df1a = df1[[\""time\"", \""value1\""]]\n        self.df2a = df2[[\""time\"", \""value2\""]]\n        self.df1b = df1[[\""time\"", \""key\"", \""value1\""]]\n        self.df2b = df2[[\""time\"", \""key\"", \""value2\""]]\n        self.df1c = df1[[\""time\"", \""key2\"", \""value1\""]]\n        self.df2c = df2[[\""time\"", \""key2\"", \""value2\""]]\n        self.df1d = df1[[\""time32\"", \""value1\""]]\n        self.df2d = df2[[\""time32\"", \""value2\""]]\n        self.df1e = df1[[\""time\"", \""key\"", \""key2\"", \""value1\""]]\n        self.df2e = df2[[\""time\"", \""key\"", \""key2\"", \""value2\""]]\n        self.df1f = df1[[\""timeu64\"", \""value1\""]]\n        self.df2f = df2[[\""timeu64\"", \""value2\""]]"", ""join_merge.MergeAsof.time_on_int32"": ""class MergeAsof:\n    def time_on_int32(self, direction, tolerance):\n        merge_asof(\n            self.df1d, self.df2d, on=\""time32\"", direction=direction, tolerance=tolerance\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MergeAsof:\n    def setup(self, direction, tolerance):\n        one_count = 200000\n        two_count = 1000000\n    \n        df1 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, one_count / 20, one_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), one_count),\n                \""key2\"": np.random.randint(0, 25, one_count),\n                \""value1\"": np.random.randn(one_count),\n            }\n        )\n        df2 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, two_count / 20, two_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), two_count),\n                \""key2\"": np.random.randint(0, 25, two_count),\n                \""value2\"": np.random.randn(two_count),\n            }\n        )\n    \n        df1 = df1.sort_values(\""time\"")\n        df2 = df2.sort_values(\""time\"")\n    \n        df1[\""time32\""] = np.int32(df1.time)\n        df2[\""time32\""] = np.int32(df2.time)\n    \n        df1[\""timeu64\""] = np.uint64(df1.time)\n        df2[\""timeu64\""] = np.uint64(df2.time)\n    \n        self.df1a = df1[[\""time\"", \""value1\""]]\n        self.df2a = df2[[\""time\"", \""value2\""]]\n        self.df1b = df1[[\""time\"", \""key\"", \""value1\""]]\n        self.df2b = df2[[\""time\"", \""key\"", \""value2\""]]\n        self.df1c = df1[[\""time\"", \""key2\"", \""value1\""]]\n        self.df2c = df2[[\""time\"", \""key2\"", \""value2\""]]\n        self.df1d = df1[[\""time32\"", \""value1\""]]\n        self.df2d = df2[[\""time32\"", \""value2\""]]\n        self.df1e = df1[[\""time\"", \""key\"", \""key2\"", \""value1\""]]\n        self.df2e = df2[[\""time\"", \""key\"", \""key2\"", \""value2\""]]\n        self.df1f = df1[[\""timeu64\"", \""value1\""]]\n        self.df2f = df2[[\""timeu64\"", \""value2\""]]""}",join_merge.MergeAsof,time,-0.011542336989030753
24495,class-level,"terminus-2,gpt-5",1.0613987074178208,1.044008692136258,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.csv.ReadCSVEngine,io.csv,"{""io.csv.ReadCSVEngine.time_read_bytescsv"": ""class ReadCSVEngine:\n    def time_read_bytescsv(self, engine):\n        read_csv(self.data(self.BytesIO_input), engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVEngine:\n    def setup(self, engine):\n        data = [\""A,B,C,D,E\""] + ([\""1,2,3,4,5\""] * 100000)\n        self.StringIO_input = StringIO(\""\\n\"".join(data))\n        # simulate reading from file\n        self.BytesIO_input = BytesIO(self.StringIO_input.read().encode(\""utf-8\""))"", ""io.csv.ReadCSVEngine.time_read_stringcsv"": ""class ReadCSVEngine:\n    def time_read_stringcsv(self, engine):\n        read_csv(self.data(self.StringIO_input), engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVEngine:\n    def setup(self, engine):\n        data = [\""A,B,C,D,E\""] + ([\""1,2,3,4,5\""] * 100000)\n        self.StringIO_input = StringIO(\""\\n\"".join(data))\n        # simulate reading from file\n        self.BytesIO_input = BytesIO(self.StringIO_input.read().encode(\""utf-8\""))""}",io.csv.ReadCSVEngine,time,0.012298454937456097
24502,class-level,"terminus-2,gpt-5",1.0545492694538523,1.0472118667839048,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.sql.WriteSQLDtypes,io.sql,"{""io.sql.WriteSQLDtypes.time_read_sql_query_select_column"": ""class WriteSQLDtypes:\n    def time_read_sql_query_select_column(self, connection, dtype):\n        read_sql_query(self.query_col, self.con)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass WriteSQLDtypes:\n    def setup(self, connection, dtype):\n        N = 10000\n        con = {\n            \""sqlalchemy\"": create_engine(\""sqlite:///:memory:\""),\n            \""sqlite\"": sqlite3.connect(\"":memory:\""),\n        }\n        self.table_name = \""test_type\""\n        self.query_col = f\""SELECT {dtype} FROM {self.table_name}\""\n        self.con = con[connection]\n        self.df = DataFrame(\n            {\n                \""float\"": np.random.randn(N),\n                \""float_with_nan\"": np.random.randn(N),\n                \""string\"": [\""foo\""] * N,\n                \""bool\"": [True] * N,\n                \""int\"": np.random.randint(0, N, size=N),\n                \""datetime\"": date_range(\""2000-01-01\"", periods=N, freq=\""s\""),\n            },\n            index=tm.makeStringIndex(N),\n        )\n        self.df.iloc[1000:3000, 1] = np.nan\n        self.df[\""date\""] = self.df[\""datetime\""].dt.date\n        self.df[\""time\""] = self.df[\""datetime\""].dt.time\n        self.df[\""datetime_string\""] = self.df[\""datetime\""].astype(str)\n        self.df.to_sql(self.table_name, self.con, if_exists=\""replace\"")""}",io.sql.WriteSQLDtypes,time,0.0051891107991142394
24497,class-level,"terminus-2,gpt-5",1.0318322167913276,1.0532229372599846,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.json.NormalizeJSON,io.json,"{""io.json.NormalizeJSON.time_normalize_json"": ""class NormalizeJSON:\n    def time_normalize_json(self, orient, frame):\n        json_normalize(self.data)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NormalizeJSON:\n    def setup(self, orient, frame):\n        data = {\n            \""hello\"": [\""thisisatest\"", 999898, \""mixed types\""],\n            \""nest1\"": {\""nest2\"": {\""nest3\"": \""nest3_value\"", \""nest3_int\"": 3445}},\n            \""nest1_list\"": {\""nest2\"": [\""blah\"", 32423, 546456.876, 92030234]},\n            \""hello2\"": \""string\"",\n        }\n        self.data = [data for i in range(10000)]""}",io.json.NormalizeJSON,time,-0.015127807969347226
24499,class-level,"terminus-2,gpt-5",1.0254605781770814,1.0151594714925496,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.json.ToJSONLines,io.json,"{""io.json.ToJSONLines.time_delta_int_tstamp_lines"": ""class ToJSONLines:\n    def time_delta_int_tstamp_lines(self):\n        self.df_td_int_ts.to_json(self.fname, orient=\""records\"", lines=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSONLines:\n    def setup(self):\n        N = 10**5\n        ncols = 5\n        index = date_range(\""20000101\"", periods=N, freq=\""H\"")\n        timedeltas = timedelta_range(start=1, periods=N, freq=\""s\"")\n        datetimes = date_range(start=1, periods=N, freq=\""s\"")\n        ints = np.random.randint(100000000, size=N)\n        longints = sys.maxsize * np.random.randint(100000000, size=N)\n        floats = np.random.randn(N)\n        strings = tm.makeStringIndex(N)\n        self.df = DataFrame(np.random.randn(N, ncols), index=np.arange(N))\n        self.df_date_idx = DataFrame(np.random.randn(N, ncols), index=index)\n        self.df_td_int_ts = DataFrame(\n            {\n                \""td_1\"": timedeltas,\n                \""td_2\"": timedeltas,\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""ts_1\"": datetimes,\n                \""ts_2\"": datetimes,\n            },\n            index=index,\n        )\n        self.df_int_floats = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""int_3\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""float_3\"": floats,\n            },\n            index=index,\n        )\n        self.df_int_float_str = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )\n        self.df_longint_float_str = DataFrame(\n            {\n                \""longint_1\"": longints,\n                \""longint_2\"": longints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )"", ""io.json.ToJSONLines.time_float_int_lines"": ""class ToJSONLines:\n    def time_float_int_lines(self):\n        self.df_int_floats.to_json(self.fname, orient=\""records\"", lines=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSONLines:\n    def setup(self):\n        N = 10**5\n        ncols = 5\n        index = date_range(\""20000101\"", periods=N, freq=\""H\"")\n        timedeltas = timedelta_range(start=1, periods=N, freq=\""s\"")\n        datetimes = date_range(start=1, periods=N, freq=\""s\"")\n        ints = np.random.randint(100000000, size=N)\n        longints = sys.maxsize * np.random.randint(100000000, size=N)\n        floats = np.random.randn(N)\n        strings = tm.makeStringIndex(N)\n        self.df = DataFrame(np.random.randn(N, ncols), index=np.arange(N))\n        self.df_date_idx = DataFrame(np.random.randn(N, ncols), index=index)\n        self.df_td_int_ts = DataFrame(\n            {\n                \""td_1\"": timedeltas,\n                \""td_2\"": timedeltas,\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""ts_1\"": datetimes,\n                \""ts_2\"": datetimes,\n            },\n            index=index,\n        )\n        self.df_int_floats = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""int_3\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""float_3\"": floats,\n            },\n            index=index,\n        )\n        self.df_int_float_str = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )\n        self.df_longint_float_str = DataFrame(\n            {\n                \""longint_1\"": longints,\n                \""longint_2\"": longints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )"", ""io.json.ToJSONLines.time_float_longint_str_lines"": ""class ToJSONLines:\n    def time_float_longint_str_lines(self):\n        self.df_longint_float_str.to_json(self.fname, orient=\""records\"", lines=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSONLines:\n    def setup(self):\n        N = 10**5\n        ncols = 5\n        index = date_range(\""20000101\"", periods=N, freq=\""H\"")\n        timedeltas = timedelta_range(start=1, periods=N, freq=\""s\"")\n        datetimes = date_range(start=1, periods=N, freq=\""s\"")\n        ints = np.random.randint(100000000, size=N)\n        longints = sys.maxsize * np.random.randint(100000000, size=N)\n        floats = np.random.randn(N)\n        strings = tm.makeStringIndex(N)\n        self.df = DataFrame(np.random.randn(N, ncols), index=np.arange(N))\n        self.df_date_idx = DataFrame(np.random.randn(N, ncols), index=index)\n        self.df_td_int_ts = DataFrame(\n            {\n                \""td_1\"": timedeltas,\n                \""td_2\"": timedeltas,\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""ts_1\"": datetimes,\n                \""ts_2\"": datetimes,\n            },\n            index=index,\n        )\n        self.df_int_floats = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""int_3\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""float_3\"": floats,\n            },\n            index=index,\n        )\n        self.df_int_float_str = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )\n        self.df_longint_float_str = DataFrame(\n            {\n                \""longint_1\"": longints,\n                \""longint_2\"": longints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )""}",io.json.ToJSONLines,time,0.007285082520885268
24508,class-level,"terminus-2,gpt-5",1.1141466394257598,1.1113814191673432,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.Melt,reshape,"{""reshape.Melt.time_melt_dataframe"": ""class Melt:\n    def time_melt_dataframe(self, dtype):\n        melt(self.df, id_vars=[\""id1\"", \""id2\""])\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Melt:\n    def setup(self, dtype):\n        self.df = DataFrame(\n            np.random.randn(100_000, 3), columns=[\""A\"", \""B\"", \""C\""], dtype=dtype\n        )\n        self.df[\""id1\""] = pd.Series(np.random.randint(0, 10, 10000))\n        self.df[\""id2\""] = pd.Series(np.random.randint(100, 1000, 10000))""}",reshape.Melt,time,0.0019556013142973997
24504,class-level,"terminus-2,gpt-5",0.9971517230074468,1.0420834684650648,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,libs.ScalarListLike,libs,"{""libs.ScalarListLike.time_is_list_like"": ""class ScalarListLike:\n    def time_is_list_like(self, param):\n        is_list_like(param)"", ""libs.ScalarListLike.time_is_scalar"": ""class ScalarListLike:\n    def time_is_scalar(self, param):\n        is_scalar(param)""}",libs.ScalarListLike,time,-0.03177634049336494
24500,class-level,"terminus-2,gpt-5",1.0519878667321685,1.0277600230717796,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.json.ToJSONWide,io.json,"{""io.json.ToJSONWide.time_to_json"": ""class ToJSON:\n    def time_to_json(self, orient, frame):\n        getattr(self, frame).to_json(self.fname, orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSONWide:\n    def setup(self, orient, frame):\n        super().setup(orient, frame)\n        base_df = getattr(self, frame).copy()\n        df_wide = concat([base_df.iloc[:100]] * 1000, ignore_index=True, axis=1)\n        self.df_wide = df_wide"", ""io.json.ToJSONWide.time_to_json_wide"": ""class ToJSONWide:\n    def time_to_json_wide(self, orient, frame):\n        self.df_wide.to_json(self.fname, orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSONWide:\n    def setup(self, orient, frame):\n        super().setup(orient, frame)\n        base_df = getattr(self, frame).copy()\n        df_wide = concat([base_df.iloc[:100]] * 1000, ignore_index=True, axis=1)\n        self.df_wide = df_wide""}",io.json.ToJSONWide,time,0.017134260014419294
24501,class-level,"terminus-2,gpt-5",0.9564595661905344,0.9525199004060194,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.parsers.DoesStringLookLikeDatetime,io.parsers,"{""io.parsers.DoesStringLookLikeDatetime.time_check_datetimes"": ""class DoesStringLookLikeDatetime:\n    def time_check_datetimes(self, value):\n        for obj in self.objects:\n            _does_string_look_like_datetime(obj)\n\n    def setup(self, value):\n        self.objects = [value] * 1000000""}",io.parsers.DoesStringLookLikeDatetime,time,0.002786185137563642
24512,class-level,"terminus-2,gpt-5",1.0391200920219597,1.0605630731393874,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Apply,rolling,"{""rolling.Apply.time_rolling"": ""class Apply:\n    def time_rolling(self, constructor, window, dtype, function, raw):\n        self.roll.apply(function, raw=raw)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, constructor, window, dtype, function, raw):\n        N = 10**3\n        arr = (100 * np.random.random(N)).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)""}",rolling.Apply,time,-0.015164767409779142
24506,class-level,"terminus-2,gpt-5",1.081682532076072,1.096807895039081,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.Cut,reshape,"{""reshape.Cut.time_cut_datetime"": ""class Cut:\n    def time_cut_datetime(self, bins):\n        pd.cut(self.datetime_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Cut.time_cut_float"": ""class Cut:\n    def time_cut_float(self, bins):\n        pd.cut(self.float_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Cut.time_cut_int"": ""class Cut:\n    def time_cut_int(self, bins):\n        pd.cut(self.int_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Cut.time_cut_timedelta"": ""class Cut:\n    def time_cut_timedelta(self, bins):\n        pd.cut(self.timedelta_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Cut.time_qcut_datetime"": ""class Cut:\n    def time_qcut_datetime(self, bins):\n        pd.qcut(self.datetime_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Cut.time_qcut_float"": ""class Cut:\n    def time_qcut_float(self, bins):\n        pd.qcut(self.float_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Cut.time_qcut_timedelta"": ""class Cut:\n    def time_qcut_timedelta(self, bins):\n        pd.qcut(self.timedelta_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))""}",reshape.Cut,time,-0.01069686206719168
24505,class-level,"terminus-2,gpt-5",0.922564721858806,0.9662950819021658,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,multiindex_object.Integer,multiindex_object,"{""multiindex_object.Integer.time_is_monotonic"": ""class Integer:\n    def time_is_monotonic(self):\n        self.mi_int.is_monotonic_increasing\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Integer:\n    def setup(self):\n        self.mi_int = MultiIndex.from_product(\n            [np.arange(1000), np.arange(1000)], names=[\""one\"", \""two\""]\n        )\n        self.obj_index = np.array(\n            [\n                (0, 10),\n                (0, 11),\n                (0, 12),\n                (0, 13),\n                (0, 14),\n                (0, 15),\n                (0, 16),\n                (0, 17),\n                (0, 18),\n                (0, 19),\n            ],\n            dtype=object,\n        )\n        self.other_mi_many_mismatches = MultiIndex.from_tuples(\n            [\n                (-7, 41),\n                (-2, 3),\n                (-0.7, 5),\n                (0, 0),\n                (0, 1.5),\n                (0, 340),\n                (0, 1001),\n                (1, -4),\n                (1, 20),\n                (1, 1040),\n                (432, -5),\n                (432, 17),\n                (439, 165.5),\n                (998, -4),\n                (998, 24065),\n                (999, 865.2),\n                (999, 1000),\n                (1045, -843),\n            ]\n        )""}",multiindex_object.Integer,time,-0.030926704415388855
24507,class-level,"terminus-2,gpt-5",1.0433120478808708,1.0285263308926698,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.Explode,reshape,"{""reshape.Explode.time_explode"": ""class Explode:\n    def time_explode(self, n_rows, max_list_length):\n        self.series.explode()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Explode:\n    def setup(self, n_rows, max_list_length):\n        data = [np.arange(np.random.randint(max_list_length)) for _ in range(n_rows)]\n        self.series = pd.Series(data)""}",reshape.Explode,time,0.010456659821924386
24510,class-level,"terminus-2,gpt-5",1.2660805683617875,1.4175765820288715,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.ReshapeExtensionDtype,reshape,"{""reshape.ReshapeExtensionDtype.time_stack"": ""class ReshapeExtensionDtype:\n    def time_stack(self, dtype):\n        self.df.stack()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReshapeExtensionDtype:\n    def setup(self, dtype):\n        lev = pd.Index(list(\""ABCDEFGHIJ\""))\n        ri = pd.Index(range(1000))\n        mi = MultiIndex.from_product([lev, ri], names=[\""foo\"", \""bar\""])\n    \n        index = date_range(\""2016-01-01\"", periods=10000, freq=\""s\"", tz=\""US/Pacific\"")\n        if dtype == \""Period[s]\"":\n            index = index.tz_localize(None).to_period(\""s\"")\n    \n        ser = pd.Series(index, index=mi)\n        df = ser.unstack(\""bar\"")\n        # roundtrips -> df.stack().equals(ser)\n    \n        self.ser = ser\n        self.df = df"", ""reshape.ReshapeExtensionDtype.time_transpose"": ""class ReshapeExtensionDtype:\n    def time_transpose(self, dtype):\n        self.df.T\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReshapeExtensionDtype:\n    def setup(self, dtype):\n        lev = pd.Index(list(\""ABCDEFGHIJ\""))\n        ri = pd.Index(range(1000))\n        mi = MultiIndex.from_product([lev, ri], names=[\""foo\"", \""bar\""])\n    \n        index = date_range(\""2016-01-01\"", periods=10000, freq=\""s\"", tz=\""US/Pacific\"")\n        if dtype == \""Period[s]\"":\n            index = index.tz_localize(None).to_period(\""s\"")\n    \n        ser = pd.Series(index, index=mi)\n        df = ser.unstack(\""bar\"")\n        # roundtrips -> df.stack().equals(ser)\n    \n        self.ser = ser\n        self.df = df""}",reshape.ReshapeExtensionDtype,time,-0.10714003795409052
24509,class-level,"terminus-2,gpt-5",1.1149141243957632,1.127530469102413,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.PivotTable,reshape,"{""reshape.PivotTable.time_pivot_table"": ""class PivotTable:\n    def time_pivot_table(self):\n        self.df.pivot_table(index=\""key1\"", columns=[\""key2\"", \""key3\""])\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")"", ""reshape.PivotTable.time_pivot_table_agg"": ""class PivotTable:\n    def time_pivot_table_agg(self):\n        self.df.pivot_table(\n            index=\""key1\"", columns=[\""key2\"", \""key3\""], aggfunc=[\""sum\"", \""mean\""]\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")"", ""reshape.PivotTable.time_pivot_table_categorical"": ""class PivotTable:\n    def time_pivot_table_categorical(self):\n        self.df2.pivot_table(\n            index=\""col1\"", values=\""col3\"", columns=\""col2\"", aggfunc=np.sum, fill_value=0\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")"", ""reshape.PivotTable.time_pivot_table_categorical_observed"": ""class PivotTable:\n    def time_pivot_table_categorical_observed(self):\n        self.df2.pivot_table(\n            index=\""col1\"",\n            values=\""col3\"",\n            columns=\""col2\"",\n            aggfunc=np.sum,\n            fill_value=0,\n            observed=True,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")"", ""reshape.PivotTable.time_pivot_table_margins"": ""class PivotTable:\n    def time_pivot_table_margins(self):\n        self.df.pivot_table(index=\""key1\"", columns=[\""key2\"", \""key3\""], margins=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")"", ""reshape.PivotTable.time_pivot_table_margins_only_column"": ""class PivotTable:\n    def time_pivot_table_margins_only_column(self):\n        self.df.pivot_table(columns=[\""key1\"", \""key2\"", \""key3\""], margins=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")""}",reshape.PivotTable,time,-0.008922450287588297
24514,class-level,"terminus-2,gpt-5",1.0399870245389802,1.0462885251255507,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.ForwardWindowMethods,rolling,"{""rolling.ForwardWindowMethods.time_rolling"": ""class ForwardWindowMethods:\n    def time_rolling(self, constructor, window_size, dtype, method):\n        getattr(self.roll, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ForwardWindowMethods:\n    def setup(self, constructor, window_size, dtype, method):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=window_size)\n        self.roll = getattr(pd, constructor)(arr).rolling(window=indexer)""}",rolling.ForwardWindowMethods,time,-0.00445650677975285
24513,class-level,"terminus-2,gpt-5",1.057727120900387,1.0570098215679355,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.EWMMethods,rolling,"{""rolling.EWMMethods.time_ewm"": ""class EWMMethods:\n    def time_ewm(self, constructor, kwargs_method, dtype):\n        getattr(self.ewm, self.method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass EWMMethods:\n    def setup(self, constructor, kwargs_method, dtype):\n        N = 10**5\n        kwargs, method = kwargs_method\n        arr = (100 * np.random.random(N)).astype(dtype)\n        self.method = method\n        self.ewm = getattr(pd, constructor)(arr).ewm(**kwargs)""}",rolling.EWMMethods,time,0.0005072838277590868
24511,class-level,"terminus-2,gpt-5",1.064465515071442,1.0345564193262995,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.WideToLong,reshape,"{""reshape.WideToLong.time_wide_to_long_big"": ""class WideToLong:\n    def time_wide_to_long_big(self):\n        wide_to_long(self.df, self.letters, i=\""id\"", j=\""year\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass WideToLong:\n    def setup(self):\n        nyrs = 20\n        nidvars = 20\n        N = 5000\n        self.letters = list(\""ABCD\"")\n        yrvars = [\n            letter + str(num)\n            for letter, num in product(self.letters, range(1, nyrs + 1))\n        ]\n        columns = [str(i) for i in range(nidvars)] + yrvars\n        self.df = DataFrame(np.random.randn(N, nidvars + len(yrvars)), columns=columns)\n        self.df[\""id\""] = self.df.index""}",reshape.WideToLong,time,0.021152118631642465
24515,class-level,"terminus-2,gpt-5",1.0872302734586787,1.0914052068359117,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Groupby,rolling,"{""rolling.Groupby.time_method"": ""class Groupby:\n    def time_method(self, method, window_kwargs):\n        getattr(self.groupby_window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Groupby:\n    def setup(self, method, window_kwargs):\n        N = 1000\n        window, kwargs = window_kwargs\n        df = pd.DataFrame(\n            {\n                \""A\"": [str(i) for i in range(N)] * 10,\n                \""B\"": list(range(N)) * 10,\n            }\n        )\n        if isinstance(kwargs.get(\""window\"", None), str):\n            df.index = pd.date_range(start=\""1900-01-01\"", freq=\""1min\"", periods=N * 10)\n        self.groupby_window = getattr(df.groupby(\""A\""), window)(**kwargs)""}",rolling.Groupby,time,-0.002952569573714964
24518,class-level,"terminus-2,gpt-5",1.100478784705483,1.0835962245212107,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.GroupbyLargeGroups,rolling,"{""rolling.GroupbyLargeGroups.time_rolling_multiindex_creation"": ""class GroupbyLargeGroups:\n    def time_rolling_multiindex_creation(self):\n        self.df.groupby(\""A\"").rolling(3).mean()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupbyLargeGroups:\n    def setup(self):\n        N = 100000\n        self.df = pd.DataFrame({\""A\"": [1, 2] * (N // 2), \""B\"": np.random.randn(N)})""}",rolling.GroupbyLargeGroups,time,0.01193957580217282
24521,class-level,"terminus-2,gpt-5",1.035932610315794,1.0647197439195957,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Quantile,rolling,"{""rolling.Quantile.time_quantile"": ""class Quantile:\n    def time_quantile(self, constructor, window, dtype, percentile, interpolation):\n        self.roll.quantile(percentile, interpolation=interpolation)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, constructor, window, dtype, percentile, interpolation):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)""}",rolling.Quantile,time,-0.020358651770722557
24522,class-level,"terminus-2,gpt-5",1.0158117231477402,1.0235342956125444,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Rank,rolling,"{""rolling.Rank.time_rank"": ""class Rank:\n    def time_rank(self, constructor, window, dtype, percentile, ascending, method):\n        self.roll.rank(pct=percentile, ascending=ascending, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Rank:\n    def setup(self, constructor, window, dtype, percentile, ascending, method):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)""}",rolling.Rank,time,-0.005461508108065229
24520,class-level,"terminus-2,gpt-5",1.056549763013718,1.0529958245106887,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Pairwise,rolling,"{""rolling.Pairwise.time_groupby"": ""class Pairwise:\n    def time_groupby(self, kwargs_window, method, pairwise):\n        getattr(self.window_group, method)(self.df, pairwise=pairwise)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Pairwise:\n    def setup(self, kwargs_window, method, pairwise):\n        N = 10**4\n        n_groups = 20\n        kwargs, window = kwargs_window\n        groups = [i for _ in range(N // n_groups) for i in range(n_groups)]\n        arr = np.random.random(N)\n        self.df = pd.DataFrame(arr)\n        self.window = getattr(self.df, window)(**kwargs)\n        self.window_group = getattr(\n            pd.DataFrame({\""A\"": groups, \""B\"": arr}).groupby(\""A\""), window\n        )(**kwargs)"", ""rolling.Pairwise.time_pairwise"": ""class Pairwise:\n    def time_pairwise(self, kwargs_window, method, pairwise):\n        getattr(self.window, method)(self.df, pairwise=pairwise)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Pairwise:\n    def setup(self, kwargs_window, method, pairwise):\n        N = 10**4\n        n_groups = 20\n        kwargs, window = kwargs_window\n        groups = [i for _ in range(N // n_groups) for i in range(n_groups)]\n        arr = np.random.random(N)\n        self.df = pd.DataFrame(arr)\n        self.window = getattr(self.df, window)(**kwargs)\n        self.window_group = getattr(\n            pd.DataFrame({\""A\"": groups, \""B\"": arr}).groupby(\""A\""), window\n        )(**kwargs)""}",rolling.Pairwise,time,0.002513393566498733
24517,class-level,"terminus-2,gpt-5",1.0528150478903315,1.0461862350524451,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.GroupbyEWMEngine,rolling,"{""rolling.GroupbyEWMEngine.time_groupby_mean"": ""class GroupbyEWMEngine:\n    def time_groupby_mean(self, engine):\n        self.gb_ewm.mean(engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupbyEWMEngine:\n    def setup(self, engine):\n        df = pd.DataFrame({\""A\"": range(50), \""B\"": range(50)})\n        self.gb_ewm = df.groupby(\""A\"").ewm(com=1.0)""}",rolling.GroupbyEWMEngine,time,0.004687986448293077
24525,class-level,"terminus-2,gpt-5",1.1084518137171673,1.1976886276828178,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.Fillna,series_methods,"{""series_methods.Fillna.time_fillna"": ""class Fillna:\n    def time_fillna(self, dtype, method):\n        value = self.fill_value if method is None else None\n        self.ser.fillna(value=value, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Fillna:\n    def setup(self, dtype, method):\n        N = 10**6\n        if dtype == \""datetime64[ns]\"":\n            data = date_range(\""2000-01-01\"", freq=\""S\"", periods=N)\n            na_value = NaT\n        elif dtype in (\""float64\"", \""Float64\""):\n            data = np.random.randn(N)\n            na_value = np.nan\n        elif dtype in (\""Int64\"", \""int64[pyarrow]\""):\n            data = np.arange(N)\n            na_value = NA\n        elif dtype in (\""string\"", \""string[pyarrow]\""):\n            data = tm.rands_array(5, N)\n            na_value = NA\n        else:\n            raise NotImplementedError\n        fill_value = data[0]\n        ser = Series(data, dtype=dtype)\n        ser[::2] = na_value\n        self.ser = ser\n        self.fill_value = fill_value""}",series_methods.Fillna,time,-0.0631094865386496
24519,class-level,"terminus-2,gpt-5",1.0569603847743732,1.0509165780775958,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Methods,rolling,"{""rolling.Methods.time_method"": ""class Methods:\n    def time_method(self, constructor, window_kwargs, dtype, method):\n        getattr(self.window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Methods:\n    def setup(self, constructor, window_kwargs, dtype, method):\n        N = 10**5\n        window, kwargs = window_kwargs\n        arr = (100 * np.random.random(N)).astype(dtype)\n        obj = getattr(pd, constructor)(arr)\n        self.window = getattr(obj, window)(**kwargs)""}",rolling.Methods,time,0.004274262161794483
24530,class-level,"terminus-2,gpt-5",0.9200853613775578,0.904440968977416,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.ArithmeticBlock,sparse,"{""sparse.ArithmeticBlock.time_addition"": ""class ArithmeticBlock:\n    def time_addition(self, fill_value):\n        self.arr1 + self.arr2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )"", ""sparse.ArithmeticBlock.time_division"": ""class ArithmeticBlock:\n    def time_division(self, fill_value):\n        self.arr1 / self.arr2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )"", ""sparse.ArithmeticBlock.time_intersect"": ""class ArithmeticBlock:\n    def time_intersect(self, fill_value):\n        self.arr2.sp_index.intersect(self.arr2.sp_index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )"", ""sparse.ArithmeticBlock.time_make_union"": ""class ArithmeticBlock:\n    def time_make_union(self, fill_value):\n        self.arr1.sp_index.make_union(self.arr2.sp_index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )""}",sparse.ArithmeticBlock,time,0.011063926732773562
24531,class-level,"terminus-2,gpt-5",1.0250425473131854,1.0425458957734757,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,stat_ops.Correlation,stat_ops,"{""stat_ops.Correlation.time_corr_series"": ""class Correlation:\n    def time_corr_series(self, method):\n        self.s.corr(self.s2, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Correlation:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(500, 15))\n        self.df2 = pd.DataFrame(np.random.randn(500, 15))\n        self.df_wide = pd.DataFrame(np.random.randn(500, 100))\n        self.df_wide_nans = self.df_wide.where(np.random.random((500, 100)) < 0.9)\n        self.s = pd.Series(np.random.randn(500))\n        self.s2 = pd.Series(np.random.randn(500))""}",stat_ops.Correlation,time,-0.012378605700346695
24516,class-level,"terminus-2,gpt-5",1.0827575908690534,1.1082312294361656,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.GroupbyEWM,rolling,"{""rolling.GroupbyEWM.time_groupby_method"": ""class GroupbyEWM:\n    def time_groupby_method(self, method):\n        getattr(self.gb_ewm, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupbyEWM:\n    def setup(self, method):\n        df = pd.DataFrame({\""A\"": range(50), \""B\"": range(50)})\n        self.gb_ewm = df.groupby(\""A\"").ewm(com=1.0)""}",rolling.GroupbyEWM,time,-0.018015303088481018
24529,class-level,"terminus-2,gpt-5",0.9164190198109669,0.9150649934375992,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.Arithmetic,sparse,"{""sparse.Arithmetic.time_add"": ""class Arithmetic:\n    def time_add(self, dense_proportion, fill_value):\n        self.array1 + self.array2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)"", ""sparse.Arithmetic.time_divide"": ""class Arithmetic:\n    def time_divide(self, dense_proportion, fill_value):\n        self.array1 / self.array2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)"", ""sparse.Arithmetic.time_make_union"": ""class Arithmetic:\n    def time_make_union(self, dense_proportion, fill_value):\n        self.array1.sp_index.make_union(self.array2.sp_index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)""}",sparse.Arithmetic,time,0.0009575858368936905
24526,class-level,"terminus-2,gpt-5",1.0584044800057109,1.0597253376016218,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.NanOps,series_methods,"{""series_methods.NanOps.time_func"": ""class NanOps:\n    def time_func(self, func, N, dtype):\n        self.func()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NanOps:\n    def setup(self, func, N, dtype):\n        if func == \""argmax\"" and dtype in {\""Int64\"", \""boolean\""}:\n            # Skip argmax for nullable int since this doesn't work yet (GH-24382)\n            raise NotImplementedError\n        self.s = Series([1] * N, dtype=dtype)\n        self.func = getattr(self.s, func)""}",series_methods.NanOps,time,-0.0009341284270940173
24524,class-level,"terminus-2,gpt-5",1.0423492258524891,1.044855253893347,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.VariableWindowMethods,rolling,"{""rolling.VariableWindowMethods.time_method"": ""class Methods:\n    def time_method(self, constructor, window_kwargs, dtype, method):\n        getattr(self.window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass VariableWindowMethods:\n    def setup(self, constructor, window, dtype, method):\n        N = 10**5\n        arr = (100 * np.random.random(N)).astype(dtype)\n        index = pd.date_range(\""2017-01-01\"", periods=N, freq=\""5s\"")\n        self.window = getattr(pd, constructor)(arr, index=index).rolling(window)""}",rolling.VariableWindowMethods,time,-0.0017722970585982865
24528,class-level,"terminus-2,gpt-5",1.0252508663975362,1.0118678655169218,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.SeriesConstructor,series_methods,"{""series_methods.SeriesConstructor.time_constructor_dict"": ""class SeriesConstructor:\n    def time_constructor_dict(self):\n        Series(data=self.data, index=self.idx)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesConstructor:\n    def setup(self):\n        self.idx = date_range(\n            start=datetime(2015, 10, 26), end=datetime(2016, 1, 1), freq=\""50s\""\n        )\n        self.data = dict(zip(self.idx, range(len(self.idx))))\n        self.array = np.array([1, 2, 3])\n        self.idx2 = Index([\""a\"", \""b\"", \""c\""])""}",series_methods.SeriesConstructor,time,0.009464639943857421
24527,class-level,"terminus-2,gpt-5",1.0410190222734892,1.0623573846427314,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.SearchSorted,series_methods,"{""series_methods.SearchSorted.time_searchsorted"": ""class SearchSorted:\n    def time_searchsorted(self, dtype):\n        key = \""2\"" if dtype == \""str\"" else 2\n        self.s.searchsorted(key)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SearchSorted:\n    def setup(self, dtype):\n        N = 10**5\n        data = np.array([1] * N + [2] * N + [3] * N).astype(dtype)\n        self.s = Series(data)""}",series_methods.SearchSorted,time,-0.01509077961049664
24523,class-level,"terminus-2,gpt-5",1.059443156912705,1.060642811266571,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.TableMethod,rolling,"{""rolling.TableMethod.time_apply"": ""class TableMethod:\n    def time_apply(self, method):\n        self.df.rolling(2, method=method).apply(\n            table_method_func, raw=True, engine=\""numba\""\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TableMethod:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(10, 1000))""}",rolling.TableMethod,time,-0.0008484118485614789
24533,class-level,"terminus-2,gpt-5",1.0592058407757647,1.0413555377244976,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,stat_ops.FrameOps,stat_ops,"{""stat_ops.FrameOps.time_op"": ""class FrameOps:\n    def time_op(self, op, dtype, axis):\n        self.df_func(axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FrameOps:\n    def setup(self, op, dtype, axis):\n        values = np.random.randn(100000, 4)\n        if dtype == \""Int64\"":\n            values = values.astype(int)\n        df = pd.DataFrame(values).astype(dtype)\n        self.df_func = getattr(df, op)""}",stat_ops.FrameOps,time,0.012623976698208708
24535,class-level,"terminus-2,gpt-5",1.0245411181968536,1.0304621123323296,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,strftime.BusinessHourStrftime,strftime,"{""strftime.BusinessHourStrftime.time_frame_offset_repr"": ""class BusinessHourStrftime:\n    def time_frame_offset_repr(self, obs):\n        self.data[\""off\""].apply(repr)\n\n    def setup(self, obs):\n        self.data = pd.DataFrame(\n            {\n                \""off\"": [offsets.BusinessHour()] * obs,\n            }\n        )"", ""strftime.BusinessHourStrftime.time_frame_offset_str"": ""class BusinessHourStrftime:\n    def time_frame_offset_str(self, obs):\n        self.data[\""off\""].apply(str)\n\n    def setup(self, obs):\n        self.data = pd.DataFrame(\n            {\n                \""off\"": [offsets.BusinessHour()] * obs,\n            }\n        )""}",strftime.BusinessHourStrftime,time,-0.004187407450831745
24536,class-level,"terminus-2,gpt-5",1.025119076680235,1.0221869382018522,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Contains,strings,"{""strings.Contains.time_contains"": ""class Contains:\n    def time_contains(self, dtype, regex):\n        self.s.str.contains(\""A\"", regex=regex)\n\n    def setup(self, dtype, regex):\n        super().setup(dtype)""}",strings.Contains,time,0.0020736481459566448
24545,class-level,"terminus-2,gpt-5",1.018202571806409,1.0292640988421993,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.PeriodProperties,tslibs.period,"{""tslibs.period.PeriodProperties.time_property"": ""class PeriodProperties:\n    def time_property(self, freq, attr):\n        getattr(self.per, attr)\n\n    def setup(self, freq, attr):\n        self.per = Period(\""2012-06-01\"", freq=freq)""}",tslibs.period.PeriodProperties,time,-0.007822862118663601
24534,class-level,"terminus-2,gpt-5",1.0271422162117505,1.025052916954476,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,stat_ops.SeriesOps,stat_ops,"{""stat_ops.SeriesOps.time_op"": ""class SeriesOps:\n    def time_op(self, op, dtype):\n        self.s_func()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesOps:\n    def setup(self, op, dtype):\n        s = pd.Series(np.random.randn(100000)).astype(dtype)\n        self.s_func = getattr(s, op)""}",stat_ops.SeriesOps,time,0.001477580804295986
24539,class-level,"terminus-2,gpt-5",0.9762346475653858,0.966058356250507,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.fields.TimeGetDateField,tslibs.fields,"{""tslibs.fields.TimeGetDateField.time_get_date_field"": ""class TimeGetDateField:\n    def time_get_date_field(self, size, field):\n        get_date_field(self.i8data, field)\n\n    def setup(self, size, field):\n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.fields.TimeGetDateField,time,0.007196811396661139
24542,class-level,"terminus-2,gpt-5",1.0501403480943403,1.0502801713536254,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.OffestDatetimeArithmetic,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_add"": ""class OffestDatetimeArithmetic:\n    def time_add(self, offset):\n        self.date + offset\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")"", ""tslibs.offsets.OffestDatetimeArithmetic.time_add_10"": ""class OffestDatetimeArithmetic:\n    def time_add_10(self, offset):\n        self.date + (10 * offset)\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")"", ""tslibs.offsets.OffestDatetimeArithmetic.time_add_np_dt64"": ""class OffestDatetimeArithmetic:\n    def time_add_np_dt64(self, offset):\n        offset + self.dt64\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")"", ""tslibs.offsets.OffestDatetimeArithmetic.time_subtract"": ""class OffestDatetimeArithmetic:\n    def time_subtract(self, offset):\n        self.date - offset\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")"", ""tslibs.offsets.OffestDatetimeArithmetic.time_subtract_10"": ""class OffestDatetimeArithmetic:\n    def time_subtract_10(self, offset):\n        self.date - (10 * offset)\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.OffestDatetimeArithmetic,time,-9.888490755663139e-05
24538,class-level,"terminus-2,gpt-5",1.0163353045195904,1.0009263835748243,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.DatetimeIndex,timeseries,"{""timeseries.DatetimeIndex.time_get"": ""class DatetimeIndex:\n    def time_get(self, index_type):\n        self.index[0]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]"", ""timeseries.DatetimeIndex.time_is_dates_only"": ""class DatetimeIndex:\n    def time_is_dates_only(self, index_type):\n        self.index._is_dates_only\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]"", ""timeseries.DatetimeIndex.time_timeseries_is_month_start"": ""class DatetimeIndex:\n    def time_timeseries_is_month_start(self, index_type):\n        self.index.is_month_start\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]"", ""timeseries.DatetimeIndex.time_to_date"": ""class DatetimeIndex:\n    def time_to_date(self, index_type):\n        self.index.date\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]"", ""timeseries.DatetimeIndex.time_to_pydatetime"": ""class DatetimeIndex:\n    def time_to_pydatetime(self, index_type):\n        self.index.to_pydatetime()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]"", ""timeseries.DatetimeIndex.time_to_time"": ""class DatetimeIndex:\n    def time_to_time(self, index_type):\n        self.index.time\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]"", ""timeseries.DatetimeIndex.time_unique"": ""class DatetimeIndex:\n    def time_unique(self, index_type):\n        self.index.unique()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]""}",timeseries.DatetimeIndex,time,0.010897398122182527
24544,class-level,"terminus-2,gpt-5",1.0687924408221696,1.0892287783290096,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.PeriodConstructor,tslibs.period,"{""tslibs.period.PeriodConstructor.time_period_constructor"": ""class PeriodConstructor:\n    def time_period_constructor(self, freq, is_offset):\n        Period(\""2012-06-01\"", freq=freq)\n\n    def setup(self, freq, is_offset):\n        if is_offset:\n            self.freq = to_offset(freq)\n        else:\n            self.freq = freq""}",tslibs.period.PeriodConstructor,time,-0.01445285537966049
24541,class-level,"terminus-2,gpt-5",1.0027272859251488,0.9623454744132488,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.normalize.Normalize,tslibs.normalize,"{""tslibs.normalize.Normalize.time_is_date_array_normalized"": ""class Normalize:\n    def time_is_date_array_normalized(self, size, tz):\n        # TODO: cases with different levels of short-circuiting\n        # 10 i.e. NPY_FR_ns\n        is_date_array_normalized(self.i8data, tz, 10)\n\n    def setup(self, size, tz):\n        # use an array that will have is_date_array_normalized give True,\n        #  so we do not short-circuit early.\n        dti = pd.date_range(\""2016-01-01\"", periods=10, tz=tz).repeat(size // 10)\n        self.i8data = dti.asi8\n    \n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError""}",tslibs.normalize.Normalize,time,0.028558565425671863
24532,class-level,"terminus-2,gpt-5",1.0710537985610438,1.0475539470866344,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,stat_ops.FrameMultiIndexOps,stat_ops,"{""stat_ops.FrameMultiIndexOps.time_op"": ""class FrameMultiIndexOps:\n    def time_op(self, op):\n        self.df_func()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FrameMultiIndexOps:\n    def setup(self, op):\n        levels = [np.arange(10), np.arange(100), np.arange(100)]\n        codes = [\n            np.arange(10).repeat(10000),\n            np.tile(np.arange(100).repeat(100), 10),\n            np.tile(np.tile(np.arange(100), 100), 10),\n        ]\n        index = pd.MultiIndex(levels=levels, codes=codes)\n        df = pd.DataFrame(np.random.randn(len(index), 4), index=index)\n        self.df_func = getattr(df, op)""}",stat_ops.FrameMultiIndexOps,time,0.016619414055452166
24540,class-level,"terminus-2,gpt-5",1.012088734909618,1.0300157447443716,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.fields.TimeGetStartEndField,tslibs.fields,"{""tslibs.fields.TimeGetStartEndField.time_get_start_end_field"": ""class TimeGetStartEndField:\n    def time_get_start_end_field(self, size, side, period, freqstr, month_kw):\n        get_start_end_field(self.i8data, self.attrname, freqstr, month_kw=month_kw)\n\n    def setup(self, size, side, period, freqstr, month_kw):\n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr\n    \n        self.attrname = f\""is_{period}_{side}\""""}",tslibs.fields.TimeGetStartEndField,time,-0.01267822477705354
24537,class-level,"terminus-2,gpt-5",1.0557051421466708,1.1006754439047826,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Methods,strings,"{""strings.Methods.time_endswith"": ""class Methods:\n    def time_endswith(self, dtype):\n        self.s.str.endswith(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_find"": ""class Methods:\n    def time_find(self, dtype):\n        self.s.str.find(\""[A-Z]+\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_fullmatch"": ""class Methods:\n    def time_fullmatch(self, dtype):\n        self.s.str.fullmatch(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_get"": ""class Methods:\n    def time_get(self, dtype):\n        self.s.str.get(0)\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_isdecimal"": ""class Methods:\n    def time_isdecimal(self, dtype):\n        self.s.str.isdecimal()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_isdigit"": ""class Methods:\n    def time_isdigit(self, dtype):\n        self.s.str.isdigit()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_join"": ""class Methods:\n    def time_join(self, dtype):\n        self.s.str.join(\"" \"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_lower"": ""class Methods:\n    def time_lower(self, dtype):\n        self.s.str.lower()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_partition"": ""class Methods:\n    def time_partition(self, dtype):\n        self.s.str.partition(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_rfind"": ""class Methods:\n    def time_rfind(self, dtype):\n        self.s.str.rfind(\""[A-Z]+\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_slice"": ""class Methods:\n    def time_slice(self, dtype):\n        self.s.str.slice(5, 15, 2)\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_startswith"": ""class Methods:\n    def time_startswith(self, dtype):\n        self.s.str.startswith(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_upper"": ""class Methods:\n    def time_upper(self, dtype):\n        self.s.str.upper()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_wrap"": ""class Methods:\n    def time_wrap(self, dtype):\n        self.s.str.wrap(10)\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_zfill"": ""class Methods:\n    def time_zfill(self, dtype):\n        self.s.str.zfill(10)\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.Methods,time,-0.03180360803261088
24546,class-level,"terminus-2,gpt-5",1.0344168320952527,1.043156661729329,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.PeriodUnaryMethods,tslibs.period,"{""tslibs.period.PeriodUnaryMethods.time_asfreq"": ""class PeriodUnaryMethods:\n    def time_asfreq(self, freq):\n        self.per.asfreq(\""A\"")\n\n    def setup(self, freq):\n        self.per = Period(\""2012-06-01\"", freq=freq)"", ""tslibs.period.PeriodUnaryMethods.time_now"": ""class PeriodUnaryMethods:\n    def time_now(self, freq):\n        self.per.now(freq)\n\n    def setup(self, freq):\n        self.per = Period(\""2012-06-01\"", freq=freq)"", ""tslibs.period.PeriodUnaryMethods.time_to_timestamp"": ""class PeriodUnaryMethods:\n    def time_to_timestamp(self, freq):\n        self.per.to_timestamp()\n\n    def setup(self, freq):\n        self.per = Period(\""2012-06-01\"", freq=freq)""}",tslibs.period.PeriodUnaryMethods,time,-0.006180926191001683
24549,class-level,"terminus-2,gpt-5",1.0075868599150823,1.0124123880355616,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.resolution.TimeResolution,tslibs.resolution,"{""tslibs.resolution.TimeResolution.time_get_resolution"": ""class TimeResolution:\n    def time_get_resolution(self, unit, size, tz):\n        get_resolution(self.i8data, tz)\n\n    def setup(self, unit, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        arr = arr.view(f\""M8[{unit}]\"").astype(\""M8[ns]\"").view(\""i8\"")\n        self.i8data = arr""}",tslibs.resolution.TimeResolution,time,-0.003412679010239891
24543,class-level,"terminus-2,gpt-5",0.981357723208407,0.9876597220558692,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.OnOffset,tslibs.offsets,"{""tslibs.offsets.OnOffset.time_on_offset"": ""class OnOffset:\n    def time_on_offset(self, offset):\n        for date in self.dates:\n            offset.is_on_offset(date)\n\n    def setup(self, offset):\n        self.dates = [\n            datetime(2016, m, d)\n            for m in [10, 11, 12]\n            for d in [1, 2, 3, 28, 29, 30, 31]\n            if not (m == 11 and d == 31)\n        ]""}",tslibs.offsets.OnOffset,time,-0.004456859156621066
24557,class-level,"terminus-2,oracle",1.0219970745309055,1.0219970745309055,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algorithms.Duplicated,algorithms,"{""algorithms.Duplicated.time_duplicated"": ""class Duplicated:\n    def time_duplicated(self, unique, keep, dtype):\n        self.idx.duplicated(keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Duplicated:\n    def setup(self, unique, keep, dtype):\n        N = 10**5\n        data = {\n            \""int\"": pd.Index(np.arange(N), dtype=\""int64\""),\n            \""uint\"": pd.Index(np.arange(N), dtype=\""uint64\""),\n            \""float\"": pd.Index(np.random.randn(N), dtype=\""float64\""),\n            \""string\"": tm.makeStringIndex(N),\n            \""datetime64[ns]\"": pd.date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n            \""datetime64[ns, tz]\"": pd.date_range(\n                \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n            ),\n        }[dtype]\n        if not unique:\n            data = data.repeat(5)\n        self.idx = data\n        # cache is_unique\n        self.idx.is_unique""}",algorithms.Duplicated,time,0.0
24554,class-level,"terminus-2,gpt-5",1.013320197686521,1.012050066604665,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampProperties,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_dayofweek"": ""class TimestampProperties:\n    def time_dayofweek(self, tz):\n        self.ts.dayofweek\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_days_in_month"": ""class TimestampProperties:\n    def time_days_in_month(self, tz):\n        self.ts.days_in_month\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_leap_year"": ""class TimestampProperties:\n    def time_is_leap_year(self, tz):\n        self.ts.is_leap_year\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_month_start"": ""class TimestampProperties:\n    def time_is_month_start(self, tz):\n        self.ts.is_month_start\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_quarter_end"": ""class TimestampProperties:\n    def time_is_quarter_end(self, tz):\n        self.ts.is_quarter_end\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_quarter_start"": ""class TimestampProperties:\n    def time_is_quarter_start(self, tz):\n        self.ts.is_quarter_start\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_year_end"": ""class TimestampProperties:\n    def time_is_year_end(self, tz):\n        self.ts.is_year_end\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_year_start"": ""class TimestampProperties:\n    def time_is_year_start(self, tz):\n        self.ts.is_year_start\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_microsecond"": ""class TimestampProperties:\n    def time_microsecond(self, tz):\n        self.ts.microsecond\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_month_name"": ""class TimestampProperties:\n    def time_month_name(self, tz):\n        self.ts.month_name()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_quarter"": ""class TimestampProperties:\n    def time_quarter(self, tz):\n        self.ts.quarter\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_tz"": ""class TimestampProperties:\n    def time_tz(self, tz):\n        self.ts.tz\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_week"": ""class TimestampProperties:\n    def time_week(self, tz):\n        self.ts.week\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.TimestampProperties,time,0.000898253947564299
24558,class-level,"terminus-2,oracle",1.088845865478887,1.088845865478887,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algorithms.Factorize,algorithms,"{""algorithms.Factorize.time_factorize"": ""class Factorize:\n    def time_factorize(self, unique, sort, dtype):\n        pd.factorize(self.data, sort=sort)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Factorize:\n    def setup(self, unique, sort, dtype):\n        N = 10**5\n        string_index = tm.makeStringIndex(N)\n        string_arrow = None\n        if dtype == \""string[pyarrow]\"":\n            try:\n                string_arrow = pd.array(string_index, dtype=\""string[pyarrow]\"")\n            except ImportError:\n                raise NotImplementedError\n    \n        data = {\n            \""int\"": pd.Index(np.arange(N), dtype=\""int64\""),\n            \""uint\"": pd.Index(np.arange(N), dtype=\""uint64\""),\n            \""float\"": pd.Index(np.random.randn(N), dtype=\""float64\""),\n            \""object\"": string_index,\n            \""datetime64[ns]\"": pd.date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n            \""datetime64[ns, tz]\"": pd.date_range(\n                \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n            ),\n            \""Int64\"": pd.array(np.arange(N), dtype=\""Int64\""),\n            \""boolean\"": pd.array(np.random.randint(0, 2, N), dtype=\""boolean\""),\n            \""string[pyarrow]\"": string_arrow,\n        }[dtype]\n        if not unique:\n            data = data.repeat(5)\n        self.data = data""}",algorithms.Factorize,time,0.0
24552,class-level,"terminus-2,gpt-5",1.0294410734176045,1.0304691815416918,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampConstruction,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_from_datetime_aware"": ""class TimestampConstruction:\n    def time_from_datetime_aware(self):\n        Timestamp(self.dttime_aware)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampConstruction.time_from_datetime_unaware"": ""class TimestampConstruction:\n    def time_from_datetime_unaware(self):\n        Timestamp(self.dttime_unaware)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampConstruction.time_from_npdatetime64"": ""class TimestampConstruction:\n    def time_from_npdatetime64(self):\n        Timestamp(self.npdatetime64)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampConstruction.time_fromordinal"": ""class TimestampConstruction:\n    def time_fromordinal(self):\n        Timestamp.fromordinal(730120)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampConstruction.time_fromtimestamp"": ""class TimestampConstruction:\n    def time_fromtimestamp(self):\n        Timestamp.fromtimestamp(1515448538)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampConstruction.time_parse_dateutil"": ""class TimestampConstruction:\n    def time_parse_dateutil(self):\n        Timestamp(\""2017/08/25 08:16:14 AM\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampConstruction.time_parse_iso8601_no_tz"": ""class TimestampConstruction:\n    def time_parse_iso8601_no_tz(self):\n        Timestamp(\""2017-08-25 08:16:14\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampConstruction.time_parse_iso8601_tz"": ""class TimestampConstruction:\n    def time_parse_iso8601_tz(self):\n        Timestamp(\""2017-08-25 08:16:14-0500\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampConstruction.time_parse_now"": ""class TimestampConstruction:\n    def time_parse_now(self):\n        Timestamp(\""now\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.TimestampConstruction,time,-0.000727092025521406
24548,class-level,"terminus-2,gpt-5",1.0002631015527346,0.9703121427233437,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.TimePeriodArrToDT64Arr,tslibs.period,"{""tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr"": ""class TimePeriodArrToDT64Arr:\n    def time_periodarray_to_dt64arr(self, size, freq):\n        periodarr_to_dt64arr(self.i8values, freq)\n\n    def setup(self, size, freq):\n        arr = np.arange(10, dtype=\""i8\"").repeat(size // 10)\n        self.i8values = arr""}",tslibs.period.TimePeriodArrToDT64Arr,time,0.021181724773260906
24556,class-level,"terminus-2,gpt-5",1.020449862197934,0.9598299359561444,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.tz_convert.TimeTZConvert,tslibs.tz_convert,"{""tslibs.tz_convert.TimeTZConvert.time_tz_convert_from_utc"": ""class TimeTZConvert:\n    def time_tz_convert_from_utc(self, size, tz):\n        # effectively:\n        #  dti = DatetimeIndex(self.i8data, tz=tz)\n        #  dti.tz_localize(None)\n        if old_sig:\n            tz_convert_from_utc(self.i8data, UTC, tz)\n        else:\n            tz_convert_from_utc(self.i8data, tz)\n\n    def setup(self, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr"", ""tslibs.tz_convert.TimeTZConvert.time_tz_localize_to_utc"": ""class TimeTZConvert:\n    def time_tz_localize_to_utc(self, size, tz):\n        # effectively:\n        #  dti = DatetimeIndex(self.i8data)\n        #  dti.tz_localize(tz, ambiguous=\""NaT\"", nonexistent=\""NaT\"")\n        tz_localize_to_utc(self.i8data, tz, ambiguous=\""NaT\"", nonexistent=\""NaT\"")\n\n    def setup(self, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.tz_convert.TimeTZConvert,time,0.04287123496590502
24550,class-level,"terminus-2,gpt-5",1.027340473221818,1.023426105382882,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timedelta.TimedeltaConstructor,tslibs.timedelta,"{""tslibs.timedelta.TimedeltaConstructor.time_from_components"": ""class TimedeltaConstructor:\n    def time_from_components(self):\n        Timedelta(\n            days=1,\n            hours=2,\n            minutes=3,\n            seconds=4,\n            milliseconds=5,\n            microseconds=6,\n            nanoseconds=7,\n        )\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")"", ""tslibs.timedelta.TimedeltaConstructor.time_from_datetime_timedelta"": ""class TimedeltaConstructor:\n    def time_from_datetime_timedelta(self):\n        Timedelta(self.dttimedelta)\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")"", ""tslibs.timedelta.TimedeltaConstructor.time_from_int"": ""class TimedeltaConstructor:\n    def time_from_int(self):\n        Timedelta(123456789)\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")"", ""tslibs.timedelta.TimedeltaConstructor.time_from_iso_format"": ""class TimedeltaConstructor:\n    def time_from_iso_format(self):\n        Timedelta(\""P4DT12H30M5S\"")\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")"", ""tslibs.timedelta.TimedeltaConstructor.time_from_missing"": ""class TimedeltaConstructor:\n    def time_from_missing(self):\n        Timedelta(\""nat\"")\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")"", ""tslibs.timedelta.TimedeltaConstructor.time_from_np_timedelta"": ""class TimedeltaConstructor:\n    def time_from_np_timedelta(self):\n        Timedelta(self.nptimedelta64)\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")"", ""tslibs.timedelta.TimedeltaConstructor.time_from_string"": ""class TimedeltaConstructor:\n    def time_from_string(self):\n        Timedelta(\""1 days\"")\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")""}",tslibs.timedelta.TimedeltaConstructor,time,0.0027682940869421307
24553,class-level,"terminus-2,gpt-5",1.0233243005240569,1.024246450892696,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampOps,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_ceil"": ""class TimestampOps:\n    def time_ceil(self, tz):\n        self.ts.ceil(\""5T\"")\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_floor"": ""class TimestampOps:\n    def time_floor(self, tz):\n        self.ts.floor(\""5T\"")\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_normalize"": ""class TimestampOps:\n    def time_normalize(self, tz):\n        self.ts.normalize()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_replace_None"": ""class TimestampOps:\n    def time_replace_None(self, tz):\n        self.ts.replace(tzinfo=None)\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_replace_tz"": ""class TimestampOps:\n    def time_replace_tz(self, tz):\n        self.ts.replace(tzinfo=pytz.timezone(\""US/Eastern\""))\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_to_julian_date"": ""class TimestampOps:\n    def time_to_julian_date(self, tz):\n        self.ts.to_julian_date()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_to_pydatetime"": ""class TimestampOps:\n    def time_to_pydatetime(self, tz):\n        self.ts.to_pydatetime()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_tz_convert"": ""class TimestampOps:\n    def time_tz_convert(self, tz):\n        if self.ts.tz is not None:\n            self.ts.tz_convert(tz)\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_tz_localize"": ""class TimestampOps:\n    def time_tz_localize(self, tz):\n        if self.ts.tz is None:\n            self.ts.tz_localize(tz)\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.TimestampOps,time,-0.0006521572621210645
24547,class-level,"terminus-2,gpt-5",1.010929828348516,1.0135595023440018,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.TimeDT64ArrToPeriodArr,tslibs.period,"{""tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr"": ""class TimeDT64ArrToPeriodArr:\n    def time_dt64arr_to_periodarr(self, size, freq, tz):\n        dt64arr_to_periodarr(self.i8values, freq, tz)\n\n    def setup(self, size, freq, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.arange(10, dtype=\""i8\"").repeat(size // 10)\n        self.i8values = arr""}",tslibs.period.TimeDT64ArrToPeriodArr,time,-0.0018597411566378074
24562,class-level,"terminus-2,oracle",1.0296213811038797,1.0296213811038797,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.IsinAlmostFullWithRandomInt,algos.isin,"{""algos.isin.IsinAlmostFullWithRandomInt.time_isin"": ""class IsinAlmostFullWithRandomInt:\n    def time_isin(self, dtype, exponent, title):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, exponent, title):\n        M = 3 * 2 ** (exponent - 2)\n        # 0.77-the maximal share of occupied buckets\n        self.series = Series(np.random.randint(0, M, M)).astype(dtype)\n    \n        values = np.random.randint(0, M, M).astype(dtype)\n        if title == \""inside\"":\n            self.values = values\n        elif title == \""outside\"":\n            self.values = values + M\n        else:\n            raise ValueError(title)""}",algos.isin.IsinAlmostFullWithRandomInt,time,0.0
24555,class-level,"terminus-2,gpt-5",1.024648415455555,1.0298420080843886,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.tslib.TimeIntsToPydatetime,tslibs.tslib,"{""tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime"": ""class TimeIntsToPydatetime:\n    def time_ints_to_pydatetime(self, box, size, tz):\n        ints_to_pydatetime(self.i8data, tz, box=box)\n\n    def setup(self, box, size, tz):\n        if box == \""date\"" and tz is not None:\n            # tz is ignored, so avoid running redundant benchmarks\n            raise NotImplementedError  # skip benchmark\n        if size == 10**6 and tz is _tzs[-1]:\n            # This is cumbersomely-slow, so skip to trim runtime\n            raise NotImplementedError  # skip benchmark\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.tslib.TimeIntsToPydatetime,time,-0.0036729792283122812
24559,class-level,"terminus-2,oracle",1.123615183139807,1.123615183139807,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algorithms.Quantile,algorithms,"{""algorithms.Quantile.time_quantile"": ""class Quantile:\n    def time_quantile(self, quantile, interpolation, dtype):\n        self.idx.quantile(quantile, interpolation=interpolation)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, quantile, interpolation, dtype):\n        N = 10**5\n        data = {\n            \""int\"": np.arange(N),\n            \""uint\"": np.arange(N).astype(np.uint64),\n            \""float\"": np.random.randn(N),\n        }\n        self.idx = pd.Series(data[dtype].repeat(5))""}",algorithms.Quantile,time,0.0
24561,class-level,"terminus-2,oracle",1.173784775878192,1.173784775878192,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.IsInLongSeriesLookUpDominates,algos.isin,"{""algos.isin.IsInLongSeriesLookUpDominates.time_isin"": ""class IsInLongSeriesLookUpDominates:\n    def time_isin(self, dtypes, MaxNumber, series_type):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, MaxNumber, series_type):\n        N = 10**7\n    \n        if series_type == \""random_hits\"":\n            array = np.random.randint(0, MaxNumber, N)\n        if series_type == \""random_misses\"":\n            array = np.random.randint(0, MaxNumber, N) + MaxNumber\n        if series_type == \""monotone_hits\"":\n            array = np.repeat(np.arange(MaxNumber), N // MaxNumber)\n        if series_type == \""monotone_misses\"":\n            array = np.arange(N) + MaxNumber\n    \n        self.series = Series(array).astype(dtype)\n    \n        self.values = np.arange(MaxNumber).astype(dtype.lower())""}",algos.isin.IsInLongSeriesLookUpDominates,time,0.0
24551,class-level,"terminus-2,gpt-5",1.0135610916926148,1.0385202816031789,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampAcrossDst,tslibs.timestamp,"{""tslibs.timestamp.TimestampAcrossDst.time_replace_across_dst"": ""class TimestampAcrossDst:\n    def time_replace_across_dst(self):\n        self.ts2.replace(tzinfo=self.tzinfo)\n\n    def setup(self):\n        dt = datetime(2016, 3, 27, 1)\n        self.tzinfo = pytz.timezone(\""CET\"").localize(dt, is_dst=False).tzinfo\n        self.ts2 = Timestamp(dt)""}",tslibs.timestamp.TimestampAcrossDst,time,-0.017651478013128795
24560,class-level,"terminus-2,oracle",1.0657782307658317,1.0657782307658317,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin.IsIn,algos.isin,"{""algos.isin.IsIn.time_isin_categorical"": ""class IsIn:\n    def time_isin_categorical(self, dtype):\n        self.series.isin(self.cat_values)\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)"", ""algos.isin.IsIn.time_isin_empty"": ""class IsIn:\n    def time_isin_empty(self, dtype):\n        self.series.isin([])\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)"", ""algos.isin.IsIn.time_isin_mismatched_dtype"": ""class IsIn:\n    def time_isin_mismatched_dtype(self, dtype):\n        self.series.isin(self.mismatched)\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)""}",algos.isin.IsIn,time,0.0
24564,class-level,"terminus-2,oracle",0.9842875183363564,0.9842875183363564,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.IndexArithmetic,arithmetic,"{""arithmetic.IndexArithmetic.time_modulo"": ""class IndexArithmetic:\n    def time_modulo(self, dtype):\n        self.index % 2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IndexArithmetic:\n    def setup(self, dtype):\n        N = 10**6\n        indexes = {\""int\"": \""makeIntIndex\"", \""float\"": \""makeFloatIndex\""}\n        self.index = getattr(tm, indexes[dtype])(N)""}",arithmetic.IndexArithmetic,time,0.0
24565,class-level,"terminus-2,oracle",0.9813729259103554,0.9813729259103554,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.IntFrameWithScalar,arithmetic,"{""arithmetic.IntFrameWithScalar.time_frame_op_with_scalar"": ""class IntFrameWithScalar:\n    def time_frame_op_with_scalar(self, dtype, scalar, op):\n        op(self.df, scalar)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IntFrameWithScalar:\n    def setup(self, dtype, scalar, op):\n        arr = np.random.randn(20000, 100)\n        self.df = DataFrame(arr.astype(dtype))""}",arithmetic.IntFrameWithScalar,time,0.0
24577,class-level,"terminus-2,oracle",0.8975931204235437,0.8975931204235437,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,array.StringArray,array,"{""array.StringArray.time_from_list"": ""class StringArray:\n    def time_from_list(self):\n        pd.array(self.values_list, dtype=\""string\"")\n\n    def setup(self):\n        N = 100_000\n        values = tm.rands_array(3, N)\n        self.values_obj = np.array(values, dtype=\""object\"")\n        self.values_str = np.array(values, dtype=\""U\"")\n        self.values_list = values.tolist()""}",array.StringArray,time,0.0
24568,class-level,"terminus-2,oracle",0.9482229870823196,0.9482229870823196,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.OffsetArrayArithmetic,arithmetic,"{""arithmetic.OffsetArrayArithmetic.time_add_dti_offset"": ""class OffsetArrayArithmetic:\n    def time_add_dti_offset(self, offset):\n        with warnings.catch_warnings(record=True):\n            self.rng + offset\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass OffsetArrayArithmetic:\n    def setup(self, offset):\n        N = 10000\n        rng = date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"")\n        self.rng = rng\n        self.ser = Series(rng)"", ""arithmetic.OffsetArrayArithmetic.time_add_series_offset"": ""class OffsetArrayArithmetic:\n    def time_add_series_offset(self, offset):\n        with warnings.catch_warnings(record=True):\n            self.ser + offset\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass OffsetArrayArithmetic:\n    def setup(self, offset):\n        N = 10000\n        rng = date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"")\n        self.rng = rng\n        self.ser = Series(rng)""}",arithmetic.OffsetArrayArithmetic,time,0.0
24570,class-level,"terminus-2,oracle",0.8968622270302342,0.8968622270302342,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.TimedeltaOps,arithmetic,"{""arithmetic.TimedeltaOps.time_add_td_ts"": ""class TimedeltaOps:\n    def time_add_td_ts(self):\n        self.td + self.ts\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TimedeltaOps:\n    def setup(self):\n        self.td = to_timedelta(np.arange(1000000))\n        self.ts = Timestamp(\""2000\"")""}",arithmetic.TimedeltaOps,time,0.0
24574,class-level,"terminus-2,oracle",0.981116408258753,0.981116408258753,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,array.BooleanArray,array,"{""array.BooleanArray.time_from_bool_array"": ""class BooleanArray:\n    def time_from_bool_array(self):\n        pd.array(self.values_bool, dtype=\""boolean\"")\n\n    def setup(self):\n        self.values_bool = np.array([True, False, True, False])\n        self.values_float = np.array([1.0, 0.0, 1.0, 0.0])\n        self.values_integer = np.array([1, 0, 1, 0])\n        self.values_integer_like = [1, 0, 1, 0]\n        self.data = np.array([True, False, True, False])\n        self.mask = np.array([False, False, True, False])""}",array.BooleanArray,time,0.0
24569,class-level,"terminus-2,oracle",0.8904017553975511,0.8904017553975511,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.Ops2,arithmetic,"{""arithmetic.Ops2.time_frame_dot"": ""class Ops2:\n    def time_frame_dot(self):\n        self.df.dot(self.df2)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Ops2:\n    def setup(self):\n        N = 10**3\n        self.df = DataFrame(np.random.randn(N, N))\n        self.df2 = DataFrame(np.random.randn(N, N))\n    \n        self.df_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n        self.df2_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n    \n        self.s = Series(np.random.randn(N))"", ""arithmetic.Ops2.time_frame_float_div_by_zero"": ""class Ops2:\n    def time_frame_float_div_by_zero(self):\n        self.df / 0\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Ops2:\n    def setup(self):\n        N = 10**3\n        self.df = DataFrame(np.random.randn(N, N))\n        self.df2 = DataFrame(np.random.randn(N, N))\n    \n        self.df_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n        self.df2_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n    \n        self.s = Series(np.random.randn(N))"", ""arithmetic.Ops2.time_frame_float_mod"": ""class Ops2:\n    def time_frame_float_mod(self):\n        self.df % self.df2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Ops2:\n    def setup(self):\n        N = 10**3\n        self.df = DataFrame(np.random.randn(N, N))\n        self.df2 = DataFrame(np.random.randn(N, N))\n    \n        self.df_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n        self.df2_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n    \n        self.s = Series(np.random.randn(N))"", ""arithmetic.Ops2.time_frame_int_div_by_zero"": ""class Ops2:\n    def time_frame_int_div_by_zero(self):\n        self.df_int / 0\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Ops2:\n    def setup(self):\n        N = 10**3\n        self.df = DataFrame(np.random.randn(N, N))\n        self.df2 = DataFrame(np.random.randn(N, N))\n    \n        self.df_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n        self.df2_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n    \n        self.s = Series(np.random.randn(N))"", ""arithmetic.Ops2.time_frame_series_dot"": ""class Ops2:\n    def time_frame_series_dot(self):\n        self.df.dot(self.s)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Ops2:\n    def setup(self):\n        N = 10**3\n        self.df = DataFrame(np.random.randn(N, N))\n        self.df2 = DataFrame(np.random.randn(N, N))\n    \n        self.df_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n        self.df2_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n    \n        self.s = Series(np.random.randn(N))""}",arithmetic.Ops2,time,0.0
24567,class-level,"terminus-2,oracle",0.7970819164535795,0.7970819164535795,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.NumericInferOps,arithmetic,"{""arithmetic.NumericInferOps.time_divide"": ""class NumericInferOps:\n    def time_divide(self, dtype):\n        self.df[\""A\""] / self.df[\""B\""]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericInferOps:\n    def setup(self, dtype):\n        N = 5 * 10**5\n        self.df = DataFrame(\n            {\""A\"": np.arange(N).astype(dtype), \""B\"": np.arange(N).astype(dtype)}\n        )""}",arithmetic.NumericInferOps,time,0.0
24571,class-level,"terminus-2,oracle",0.8139632572966733,0.8139632572966733,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.Timeseries,arithmetic,"{""arithmetic.Timeseries.time_series_timestamp_compare"": ""class Timeseries:\n    def time_series_timestamp_compare(self, tz):\n        self.s <= self.ts\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Timeseries:\n    def setup(self, tz):\n        N = 10**6\n        halfway = (N // 2) - 1\n        self.s = Series(date_range(\""20010101\"", periods=N, freq=\""T\"", tz=tz))\n        self.ts = self.s[halfway]\n    \n        self.s2 = Series(date_range(\""20010101\"", periods=N, freq=\""s\"", tz=tz))"", ""arithmetic.Timeseries.time_timestamp_ops_diff"": ""class Timeseries:\n    def time_timestamp_ops_diff(self, tz):\n        self.s2.diff()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Timeseries:\n    def setup(self, tz):\n        N = 10**6\n        halfway = (N // 2) - 1\n        self.s = Series(date_range(\""20010101\"", periods=N, freq=\""T\"", tz=tz))\n        self.ts = self.s[halfway]\n    \n        self.s2 = Series(date_range(\""20010101\"", periods=N, freq=\""s\"", tz=tz))"", ""arithmetic.Timeseries.time_timestamp_ops_diff_with_shift"": ""class Timeseries:\n    def time_timestamp_ops_diff_with_shift(self, tz):\n        self.s - self.s.shift()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Timeseries:\n    def setup(self, tz):\n        N = 10**6\n        halfway = (N // 2) - 1\n        self.s = Series(date_range(\""20010101\"", periods=N, freq=\""T\"", tz=tz))\n        self.ts = self.s[halfway]\n    \n        self.s2 = Series(date_range(\""20010101\"", periods=N, freq=\""s\"", tz=tz))"", ""arithmetic.Timeseries.time_timestamp_series_compare"": ""class Timeseries:\n    def time_timestamp_series_compare(self, tz):\n        self.ts >= self.s\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Timeseries:\n    def setup(self, tz):\n        N = 10**6\n        halfway = (N // 2) - 1\n        self.s = Series(date_range(\""20010101\"", periods=N, freq=\""T\"", tz=tz))\n        self.ts = self.s[halfway]\n    \n        self.s2 = Series(date_range(\""20010101\"", periods=N, freq=\""s\"", tz=tz))""}",arithmetic.Timeseries,time,0.0
24563,class-level,"terminus-2,oracle",0.930493242852044,0.930493242852044,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.FrameWithFrameWide,arithmetic,"{""arithmetic.FrameWithFrameWide.time_op_different_blocks"": ""class FrameWithFrameWide:\n    def time_op_different_blocks(self, op, shape):\n        # blocks (and dtypes) are not aligned\n        op(self.left, self.right)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FrameWithFrameWide:\n    def setup(self, op, shape):\n        # we choose dtypes so as to make the blocks\n        #  a) not perfectly match between right and left\n        #  b) appreciably bigger than single columns\n        n_rows, n_cols = shape\n    \n        if op is operator.floordiv:\n            # floordiv is much slower than the other operations -> use less data\n            n_rows = n_rows // 10\n    \n        # construct dataframe with 2 blocks\n        arr1 = np.random.randn(n_rows, n_cols // 2).astype(\""f8\"")\n        arr2 = np.random.randn(n_rows, n_cols // 2).astype(\""f4\"")\n        df = pd.concat([DataFrame(arr1), DataFrame(arr2)], axis=1, ignore_index=True)\n        # should already be the case, but just to be sure\n        df._consolidate_inplace()\n    \n        # TODO: GH#33198 the setting here shouldn't need two steps\n        arr1 = np.random.randn(n_rows, max(n_cols // 4, 3)).astype(\""f8\"")\n        arr2 = np.random.randn(n_rows, n_cols // 2).astype(\""i8\"")\n        arr3 = np.random.randn(n_rows, n_cols // 4).astype(\""f8\"")\n        df2 = pd.concat(\n            [DataFrame(arr1), DataFrame(arr2), DataFrame(arr3)],\n            axis=1,\n            ignore_index=True,\n        )\n        # should already be the case, but just to be sure\n        df2._consolidate_inplace()\n    \n        self.left = df\n        self.right = df2"", ""arithmetic.FrameWithFrameWide.time_op_same_blocks"": ""class FrameWithFrameWide:\n    def time_op_same_blocks(self, op, shape):\n        # blocks (and dtypes) are aligned\n        op(self.left, self.left)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FrameWithFrameWide:\n    def setup(self, op, shape):\n        # we choose dtypes so as to make the blocks\n        #  a) not perfectly match between right and left\n        #  b) appreciably bigger than single columns\n        n_rows, n_cols = shape\n    \n        if op is operator.floordiv:\n            # floordiv is much slower than the other operations -> use less data\n            n_rows = n_rows // 10\n    \n        # construct dataframe with 2 blocks\n        arr1 = np.random.randn(n_rows, n_cols // 2).astype(\""f8\"")\n        arr2 = np.random.randn(n_rows, n_cols // 2).astype(\""f4\"")\n        df = pd.concat([DataFrame(arr1), DataFrame(arr2)], axis=1, ignore_index=True)\n        # should already be the case, but just to be sure\n        df._consolidate_inplace()\n    \n        # TODO: GH#33198 the setting here shouldn't need two steps\n        arr1 = np.random.randn(n_rows, max(n_cols // 4, 3)).astype(\""f8\"")\n        arr2 = np.random.randn(n_rows, n_cols // 2).astype(\""i8\"")\n        arr3 = np.random.randn(n_rows, n_cols // 4).astype(\""f8\"")\n        df2 = pd.concat(\n            [DataFrame(arr1), DataFrame(arr2), DataFrame(arr3)],\n            axis=1,\n            ignore_index=True,\n        )\n        # should already be the case, but just to be sure\n        df2._consolidate_inplace()\n    \n        self.left = df\n        self.right = df2""}",arithmetic.FrameWithFrameWide,time,0.0
24580,class-level,"terminus-2,oracle",0.9545993186647964,0.9545993186647964,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.CategoricalSlicing,categoricals,"{""categoricals.CategoricalSlicing.time_getitem_bool_array"": ""class CategoricalSlicing:\n    def time_getitem_bool_array(self, index):\n        self.data[self.data == self.cat_scalar]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalSlicing:\n    def setup(self, index):\n        N = 10**6\n        categories = [\""a\"", \""b\"", \""c\""]\n        values = [0] * N + [1] * N + [2] * N\n        if index == \""monotonic_incr\"":\n            self.data = pd.Categorical.from_codes(values, categories=categories)\n        elif index == \""monotonic_decr\"":\n            self.data = pd.Categorical.from_codes(\n                list(reversed(values)), categories=categories\n            )\n        elif index == \""non_monotonic\"":\n            self.data = pd.Categorical.from_codes([0, 1, 2] * N, categories=categories)\n        else:\n            raise ValueError(f\""Invalid index param: {index}\"")\n    \n        self.scalar = 10000\n        self.list = list(range(10000))\n        self.cat_scalar = \""b\"""", ""categoricals.CategoricalSlicing.time_getitem_list"": ""class CategoricalSlicing:\n    def time_getitem_list(self, index):\n        self.data[self.list]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalSlicing:\n    def setup(self, index):\n        N = 10**6\n        categories = [\""a\"", \""b\"", \""c\""]\n        values = [0] * N + [1] * N + [2] * N\n        if index == \""monotonic_incr\"":\n            self.data = pd.Categorical.from_codes(values, categories=categories)\n        elif index == \""monotonic_decr\"":\n            self.data = pd.Categorical.from_codes(\n                list(reversed(values)), categories=categories\n            )\n        elif index == \""non_monotonic\"":\n            self.data = pd.Categorical.from_codes([0, 1, 2] * N, categories=categories)\n        else:\n            raise ValueError(f\""Invalid index param: {index}\"")\n    \n        self.scalar = 10000\n        self.list = list(range(10000))\n        self.cat_scalar = \""b\"""", ""categoricals.CategoricalSlicing.time_getitem_scalar"": ""class CategoricalSlicing:\n    def time_getitem_scalar(self, index):\n        self.data[self.scalar]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalSlicing:\n    def setup(self, index):\n        N = 10**6\n        categories = [\""a\"", \""b\"", \""c\""]\n        values = [0] * N + [1] * N + [2] * N\n        if index == \""monotonic_incr\"":\n            self.data = pd.Categorical.from_codes(values, categories=categories)\n        elif index == \""monotonic_decr\"":\n            self.data = pd.Categorical.from_codes(\n                list(reversed(values)), categories=categories\n            )\n        elif index == \""non_monotonic\"":\n            self.data = pd.Categorical.from_codes([0, 1, 2] * N, categories=categories)\n        else:\n            raise ValueError(f\""Invalid index param: {index}\"")\n    \n        self.scalar = 10000\n        self.list = list(range(10000))\n        self.cat_scalar = \""b\""""}",categoricals.CategoricalSlicing,time,0.0
24566,class-level,"terminus-2,oracle",0.9681320119640044,0.9681320119640044,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic.MixedFrameWithSeriesAxis,arithmetic,"{""arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0"": ""class MixedFrameWithSeriesAxis:\n    def time_frame_op_with_series_axis0(self, opname):\n        getattr(self.df, opname)(self.ser, axis=0)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MixedFrameWithSeriesAxis:\n    def setup(self, opname):\n        arr = np.arange(10**6).reshape(1000, -1)\n        df = DataFrame(arr)\n        df[\""C\""] = 1.0\n        self.df = df\n        self.ser = df[0]\n        self.row = df.iloc[0]""}",arithmetic.MixedFrameWithSeriesAxis,time,0.0
24572,class-level,"terminus-2,oracle",0.9620389002167072,0.9620389002167072,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,array.ArrowExtensionArray,array,"{""array.ArrowExtensionArray.time_to_numpy"": ""class ArrowExtensionArray:\n    def time_to_numpy(self, dtype, hasna):\n        self.arr.to_numpy()\n\n    def setup(self, dtype, hasna):\n        N = 100_000\n        if dtype == \""boolean[pyarrow]\"":\n            data = np.random.choice([True, False], N, replace=True)\n        elif dtype == \""float64[pyarrow]\"":\n            data = np.random.randn(N)\n        elif dtype == \""int64[pyarrow]\"":\n            data = np.arange(N)\n        elif dtype == \""string[pyarrow]\"":\n            data = tm.rands_array(10, N)\n        elif dtype == \""timestamp[ns][pyarrow]\"":\n            data = pd.date_range(\""2000-01-01\"", freq=\""s\"", periods=N)\n        else:\n            raise NotImplementedError\n    \n        arr = pd.array(data, dtype=dtype)\n        if hasna:\n            arr[::2] = pd.NA\n        self.arr = arr""}",array.ArrowExtensionArray,time,0.0
24575,class-level,"terminus-2,oracle",0.963676491180532,0.963676491180532,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,array.IntegerArray,array,"{""array.IntegerArray.time_constructor"": ""class IntegerArray:\n    def time_constructor(self):\n        pd.arrays.IntegerArray(self.data, self.mask)\n\n    def setup(self):\n        N = 250_000\n        self.values_integer = np.array([1, 0, 1, 0] * N)\n        self.data = np.array([1, 2, 3, 4] * N, dtype=\""int64\"")\n        self.mask = np.array([False, False, True, False] * N)""}",array.IntegerArray,time,0.0
24579,class-level,"terminus-2,oracle",0.9711150307596093,0.9711150307596093,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,boolean.TimeLogicalOps,boolean,"{""boolean.TimeLogicalOps.time_and_scalar"": ""class TimeLogicalOps:\n    def time_and_scalar(self):\n        self.left & True\n        self.left & False\n\n    def setup(self):\n        N = 10_000\n        left, right, lmask, rmask = np.random.randint(0, 2, size=(4, N)).astype(\""bool\"")\n        self.left = pd.arrays.BooleanArray(left, lmask)\n        self.right = pd.arrays.BooleanArray(right, rmask)"", ""boolean.TimeLogicalOps.time_xor_array"": ""class TimeLogicalOps:\n    def time_xor_array(self):\n        self.left ^ self.right\n\n    def setup(self):\n        N = 10_000\n        left, right, lmask, rmask = np.random.randint(0, 2, size=(4, N)).astype(\""bool\"")\n        self.left = pd.arrays.BooleanArray(left, lmask)\n        self.right = pd.arrays.BooleanArray(right, rmask)""}",boolean.TimeLogicalOps,time,0.0
24590,class-level,"terminus-2,oracle",1.0235979911278916,1.0235979911278916,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.Dtypes,dtypes,"{""dtypes.Dtypes.time_pandas_dtype"": ""class Dtypes:\n    def time_pandas_dtype(self, dtype):\n        pandas_dtype(dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)""}",dtypes.Dtypes,time,0.0
24584,class-level,"terminus-2,oracle",0.8932835108444206,0.8932835108444206,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.Rank,categoricals,"{""categoricals.Rank.time_rank_int"": ""class Rank:\n    def time_rank_int(self):\n        self.s_int.rank()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Rank:\n    def setup(self):\n        N = 10**5\n        ncats = 15\n    \n        self.s_str = pd.Series(tm.makeCategoricalIndex(N, ncats)).astype(str)\n        self.s_str_cat = pd.Series(self.s_str, dtype=\""category\"")\n        with warnings.catch_warnings(record=True):\n            str_cat_type = pd.CategoricalDtype(set(self.s_str), ordered=True)\n            self.s_str_cat_ordered = self.s_str.astype(str_cat_type)\n    \n        self.s_int = pd.Series(np.random.randint(0, ncats, size=N))\n        self.s_int_cat = pd.Series(self.s_int, dtype=\""category\"")\n        with warnings.catch_warnings(record=True):\n            int_cat_type = pd.CategoricalDtype(set(self.s_int), ordered=True)\n            self.s_int_cat_ordered = self.s_int.astype(int_cat_type)"", ""categoricals.Rank.time_rank_string_cat"": ""class Rank:\n    def time_rank_string_cat(self):\n        self.s_str_cat.rank()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Rank:\n    def setup(self):\n        N = 10**5\n        ncats = 15\n    \n        self.s_str = pd.Series(tm.makeCategoricalIndex(N, ncats)).astype(str)\n        self.s_str_cat = pd.Series(self.s_str, dtype=\""category\"")\n        with warnings.catch_warnings(record=True):\n            str_cat_type = pd.CategoricalDtype(set(self.s_str), ordered=True)\n            self.s_str_cat_ordered = self.s_str.astype(str_cat_type)\n    \n        self.s_int = pd.Series(np.random.randint(0, ncats, size=N))\n        self.s_int_cat = pd.Series(self.s_int, dtype=\""category\"")\n        with warnings.catch_warnings(record=True):\n            int_cat_type = pd.CategoricalDtype(set(self.s_int), ordered=True)\n            self.s_int_cat_ordered = self.s_int.astype(int_cat_type)""}",categoricals.Rank,time,0.0
24576,class-level,"terminus-2,oracle",0.904333635973276,0.904333635973276,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,array.IntervalArray,array,"{""array.IntervalArray.time_from_tuples"": ""class IntervalArray:\n    def time_from_tuples(self):\n        pd.arrays.IntervalArray.from_tuples(self.tuples)\n\n    def setup(self):\n        N = 10_000\n        self.tuples = [(i, i + 1) for i in range(N)]""}",array.IntervalArray,time,0.0
24581,class-level,"terminus-2,oracle",0.9820553921372688,0.9820553921372688,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.Concat,categoricals,"{""categoricals.Concat.time_append_overlapping_index"": ""class Concat:\n    def time_append_overlapping_index(self):\n        self.idx_a.append(self.idx_a)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Concat:\n    def setup(self):\n        N = 10**5\n        self.s = pd.Series(list(\""aabbcd\"") * N).astype(\""category\"")\n    \n        self.a = pd.Categorical(list(\""aabbcd\"") * N)\n        self.b = pd.Categorical(list(\""bbcdjk\"") * N)\n    \n        self.idx_a = pd.CategoricalIndex(range(N), range(N))\n        self.idx_b = pd.CategoricalIndex(range(N + 1), range(N + 1))\n        self.df_a = pd.DataFrame(range(N), columns=[\""a\""], index=self.idx_a)\n        self.df_b = pd.DataFrame(range(N + 1), columns=[\""a\""], index=self.idx_b)""}",categoricals.Concat,time,0.0
24578,class-level,"terminus-2,oracle",0.971147038533247,0.971147038533247,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,attrs_caching.SeriesArrayAttribute,attrs_caching,"{""attrs_caching.SeriesArrayAttribute.time_array"": ""class SeriesArrayAttribute:\n    def time_array(self, dtype):\n        self.series.array\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesArrayAttribute:\n    def setup(self, dtype):\n        if dtype == \""numeric\"":\n            self.series = pd.Series([1, 2, 3])\n        elif dtype == \""object\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=object)\n        elif dtype == \""category\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=\""category\"")\n        elif dtype == \""datetime64\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3))\n        elif dtype == \""datetime64tz\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3, tz=\""UTC\""))"", ""attrs_caching.SeriesArrayAttribute.time_extract_array"": ""class SeriesArrayAttribute:\n    def time_extract_array(self, dtype):\n        extract_array(self.series)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesArrayAttribute:\n    def setup(self, dtype):\n        if dtype == \""numeric\"":\n            self.series = pd.Series([1, 2, 3])\n        elif dtype == \""object\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=object)\n        elif dtype == \""category\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=\""category\"")\n        elif dtype == \""datetime64\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3))\n        elif dtype == \""datetime64tz\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3, tz=\""UTC\""))"", ""attrs_caching.SeriesArrayAttribute.time_extract_array_numpy"": ""class SeriesArrayAttribute:\n    def time_extract_array_numpy(self, dtype):\n        extract_array(self.series, extract_numpy=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesArrayAttribute:\n    def setup(self, dtype):\n        if dtype == \""numeric\"":\n            self.series = pd.Series([1, 2, 3])\n        elif dtype == \""object\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=object)\n        elif dtype == \""category\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=\""category\"")\n        elif dtype == \""datetime64\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3))\n        elif dtype == \""datetime64tz\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3, tz=\""UTC\""))""}",attrs_caching.SeriesArrayAttribute,time,0.0
24585,class-level,"terminus-2,oracle",0.7523904029259222,0.7523904029259222,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.RemoveCategories,categoricals,"{""categoricals.RemoveCategories.time_remove_categories"": ""class RemoveCategories:\n    def time_remove_categories(self):\n        self.ts.cat.remove_categories(self.ts.cat.categories[::2])\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass RemoveCategories:\n    def setup(self):\n        n = 5 * 10**5\n        arr = [f\""s{i:04d}\"" for i in np.random.randint(0, n // 10, size=n)]\n        self.ts = pd.Series(arr).astype(\""category\"")""}",categoricals.RemoveCategories,time,0.0
24586,class-level,"terminus-2,oracle",0.9760363826929792,0.9760363826929792,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.SearchSorted,categoricals,"{""categoricals.SearchSorted.time_categorical_contains"": ""class SearchSorted:\n    def time_categorical_contains(self):\n        self.c.searchsorted(self.key)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SearchSorted:\n    def setup(self):\n        N = 10**5\n        self.ci = tm.makeCategoricalIndex(N).sort_values()\n        self.c = self.ci.values\n        self.key = self.ci.categories[1]""}",categoricals.SearchSorted,time,0.0
24573,class-level,"terminus-2,oracle",0.978535045084665,0.978535045084665,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,array.ArrowStringArray,array,"{""array.ArrowStringArray.time_setitem_list"": ""class ArrowStringArray:\n    def time_setitem_list(self, multiple_chunks):\n        indexer = list(range(0, 50)) + list(range(-1000, 0, 50))\n        self.array[indexer] = [\""foo\""] * len(indexer)\n\n    def setup(self, multiple_chunks):\n        try:\n            import pyarrow as pa\n        except ImportError:\n            raise NotImplementedError\n        strings = tm.rands_array(3, 10_000)\n        if multiple_chunks:\n            chunks = [strings[i : i + 100] for i in range(0, len(strings), 100)]\n            self.array = pd.arrays.ArrowStringArray(pa.chunked_array(chunks))\n        else:\n            self.array = pd.arrays.ArrowStringArray(pa.array(strings))"", ""array.ArrowStringArray.time_tolist"": ""class ArrowStringArray:\n    def time_tolist(self, multiple_chunks):\n        self.array.tolist()\n\n    def setup(self, multiple_chunks):\n        try:\n            import pyarrow as pa\n        except ImportError:\n            raise NotImplementedError\n        strings = tm.rands_array(3, 10_000)\n        if multiple_chunks:\n            chunks = [strings[i : i + 100] for i in range(0, len(strings), 100)]\n            self.array = pd.arrays.ArrowStringArray(pa.chunked_array(chunks))\n        else:\n            self.array = pd.arrays.ArrowStringArray(pa.array(strings))""}",array.ArrowStringArray,time,0.0
24589,class-level,"terminus-2,oracle",0.9075099787978216,0.9075099787978216,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,ctors.SeriesConstructors,ctors,"{""ctors.SeriesConstructors.time_series_constructor"": ""class SeriesConstructors:\n    def time_series_constructor(self, data_fmt, with_index, dtype):\n        Series(self.data, index=self.index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesConstructors:\n    def setup(self, data_fmt, with_index, dtype):\n        if data_fmt in (gen_of_str, gen_of_tuples) and with_index:\n            raise NotImplementedError(\n                \""Series constructors do not support using generators with indexes\""\n            )\n        N = 10**4\n        if dtype == \""float\"":\n            arr = np.random.randn(N)\n        else:\n            arr = np.arange(N)\n        self.data = data_fmt(arr)\n        self.index = np.arange(N) if with_index else None""}",ctors.SeriesConstructors,time,0.0
24593,class-level,"terminus-2,oracle",1.21933316247237,1.21933316247237,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.Eval,eval,"{""eval.Eval.time_add"": ""class Eval:\n    def time_add(self, engine, threads):\n        pd.eval(\""self.df + self.df2 + self.df3 + self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)"", ""eval.Eval.time_and"": ""class Eval:\n    def time_and(self, engine, threads):\n        pd.eval(\n            \""(self.df > 0) & (self.df2 > 0) & (self.df3 > 0) & (self.df4 > 0)\"",\n            engine=engine,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)"", ""eval.Eval.time_chained_cmp"": ""class Eval:\n    def time_chained_cmp(self, engine, threads):\n        pd.eval(\""self.df < self.df2 < self.df3 < self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)"", ""eval.Eval.time_mult"": ""class Eval:\n    def time_mult(self, engine, threads):\n        pd.eval(\""self.df * self.df2 * self.df3 * self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)""}",eval.Eval,time,0.0
24587,class-level,"terminus-2,oracle",0.696597972308575,0.696597972308575,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.SetCategories,categoricals,"{""categoricals.SetCategories.time_set_categories"": ""class SetCategories:\n    def time_set_categories(self):\n        self.ts.cat.set_categories(self.ts.cat.categories[::2])\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SetCategories:\n    def setup(self):\n        n = 5 * 10**5\n        arr = [f\""s{i:04d}\"" for i in np.random.randint(0, n // 10, size=n)]\n        self.ts = pd.Series(arr).astype(\""category\"")""}",categoricals.SetCategories,time,0.0
24582,class-level,"terminus-2,oracle",0.9558183004769014,0.9558183004769014,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.Constructor,categoricals,"{""categoricals.Constructor.time_all_nan"": ""class Constructor:\n    def time_all_nan(self):\n        pd.Categorical(self.values_all_nan)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Constructor:\n    def setup(self):\n        N = 10**5\n        self.categories = list(\""abcde\"")\n        self.cat_idx = pd.Index(self.categories)\n        self.values = np.tile(self.categories, N)\n        self.codes = np.tile(range(len(self.categories)), N)\n    \n        self.datetimes = pd.Series(\n            pd.date_range(\""1995-01-01 00:00:00\"", periods=N / 10, freq=\""s\"")\n        )\n        self.datetimes_with_nat = self.datetimes.copy()\n        self.datetimes_with_nat.iloc[-1] = pd.NaT\n    \n        self.values_some_nan = list(np.tile(self.categories + [np.nan], N))\n        self.values_all_nan = [np.nan] * len(self.values)\n        self.values_all_int8 = np.ones(N, \""int8\"")\n        self.categorical = pd.Categorical(self.values, self.categories)\n        self.series = pd.Series(self.categorical)\n        self.intervals = pd.interval_range(0, 1, periods=N // 10)"", ""categoricals.Constructor.time_datetimes"": ""class Constructor:\n    def time_datetimes(self):\n        pd.Categorical(self.datetimes)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Constructor:\n    def setup(self):\n        N = 10**5\n        self.categories = list(\""abcde\"")\n        self.cat_idx = pd.Index(self.categories)\n        self.values = np.tile(self.categories, N)\n        self.codes = np.tile(range(len(self.categories)), N)\n    \n        self.datetimes = pd.Series(\n            pd.date_range(\""1995-01-01 00:00:00\"", periods=N / 10, freq=\""s\"")\n        )\n        self.datetimes_with_nat = self.datetimes.copy()\n        self.datetimes_with_nat.iloc[-1] = pd.NaT\n    \n        self.values_some_nan = list(np.tile(self.categories + [np.nan], N))\n        self.values_all_nan = [np.nan] * len(self.values)\n        self.values_all_int8 = np.ones(N, \""int8\"")\n        self.categorical = pd.Categorical(self.values, self.categories)\n        self.series = pd.Series(self.categorical)\n        self.intervals = pd.interval_range(0, 1, periods=N // 10)"", ""categoricals.Constructor.time_existing_series"": ""class Constructor:\n    def time_existing_series(self):\n        pd.Categorical(self.series)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Constructor:\n    def setup(self):\n        N = 10**5\n        self.categories = list(\""abcde\"")\n        self.cat_idx = pd.Index(self.categories)\n        self.values = np.tile(self.categories, N)\n        self.codes = np.tile(range(len(self.categories)), N)\n    \n        self.datetimes = pd.Series(\n            pd.date_range(\""1995-01-01 00:00:00\"", periods=N / 10, freq=\""s\"")\n        )\n        self.datetimes_with_nat = self.datetimes.copy()\n        self.datetimes_with_nat.iloc[-1] = pd.NaT\n    \n        self.values_some_nan = list(np.tile(self.categories + [np.nan], N))\n        self.values_all_nan = [np.nan] * len(self.values)\n        self.values_all_int8 = np.ones(N, \""int8\"")\n        self.categorical = pd.Categorical(self.values, self.categories)\n        self.series = pd.Series(self.categorical)\n        self.intervals = pd.interval_range(0, 1, periods=N // 10)"", ""categoricals.Constructor.time_fastpath"": ""class Constructor:\n    def time_fastpath(self):\n        pd.Categorical(self.codes, self.cat_idx, fastpath=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Constructor:\n    def setup(self):\n        N = 10**5\n        self.categories = list(\""abcde\"")\n        self.cat_idx = pd.Index(self.categories)\n        self.values = np.tile(self.categories, N)\n        self.codes = np.tile(range(len(self.categories)), N)\n    \n        self.datetimes = pd.Series(\n            pd.date_range(\""1995-01-01 00:00:00\"", periods=N / 10, freq=\""s\"")\n        )\n        self.datetimes_with_nat = self.datetimes.copy()\n        self.datetimes_with_nat.iloc[-1] = pd.NaT\n    \n        self.values_some_nan = list(np.tile(self.categories + [np.nan], N))\n        self.values_all_nan = [np.nan] * len(self.values)\n        self.values_all_int8 = np.ones(N, \""int8\"")\n        self.categorical = pd.Categorical(self.values, self.categories)\n        self.series = pd.Series(self.categorical)\n        self.intervals = pd.interval_range(0, 1, periods=N // 10)"", ""categoricals.Constructor.time_from_codes_all_int8"": ""class Constructor:\n    def time_from_codes_all_int8(self):\n        pd.Categorical.from_codes(self.values_all_int8, self.categories)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Constructor:\n    def setup(self):\n        N = 10**5\n        self.categories = list(\""abcde\"")\n        self.cat_idx = pd.Index(self.categories)\n        self.values = np.tile(self.categories, N)\n        self.codes = np.tile(range(len(self.categories)), N)\n    \n        self.datetimes = pd.Series(\n            pd.date_range(\""1995-01-01 00:00:00\"", periods=N / 10, freq=\""s\"")\n        )\n        self.datetimes_with_nat = self.datetimes.copy()\n        self.datetimes_with_nat.iloc[-1] = pd.NaT\n    \n        self.values_some_nan = list(np.tile(self.categories + [np.nan], N))\n        self.values_all_nan = [np.nan] * len(self.values)\n        self.values_all_int8 = np.ones(N, \""int8\"")\n        self.categorical = pd.Categorical(self.values, self.categories)\n        self.series = pd.Series(self.categorical)\n        self.intervals = pd.interval_range(0, 1, periods=N // 10)"", ""categoricals.Constructor.time_interval"": ""class Constructor:\n    def time_interval(self):\n        pd.Categorical(self.datetimes, categories=self.datetimes)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Constructor:\n    def setup(self):\n        N = 10**5\n        self.categories = list(\""abcde\"")\n        self.cat_idx = pd.Index(self.categories)\n        self.values = np.tile(self.categories, N)\n        self.codes = np.tile(range(len(self.categories)), N)\n    \n        self.datetimes = pd.Series(\n            pd.date_range(\""1995-01-01 00:00:00\"", periods=N / 10, freq=\""s\"")\n        )\n        self.datetimes_with_nat = self.datetimes.copy()\n        self.datetimes_with_nat.iloc[-1] = pd.NaT\n    \n        self.values_some_nan = list(np.tile(self.categories + [np.nan], N))\n        self.values_all_nan = [np.nan] * len(self.values)\n        self.values_all_int8 = np.ones(N, \""int8\"")\n        self.categorical = pd.Categorical(self.values, self.categories)\n        self.series = pd.Series(self.categorical)\n        self.intervals = pd.interval_range(0, 1, periods=N // 10)"", ""categoricals.Constructor.time_with_nan"": ""class Constructor:\n    def time_with_nan(self):\n        pd.Categorical(self.values_some_nan)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Constructor:\n    def setup(self):\n        N = 10**5\n        self.categories = list(\""abcde\"")\n        self.cat_idx = pd.Index(self.categories)\n        self.values = np.tile(self.categories, N)\n        self.codes = np.tile(range(len(self.categories)), N)\n    \n        self.datetimes = pd.Series(\n            pd.date_range(\""1995-01-01 00:00:00\"", periods=N / 10, freq=\""s\"")\n        )\n        self.datetimes_with_nat = self.datetimes.copy()\n        self.datetimes_with_nat.iloc[-1] = pd.NaT\n    \n        self.values_some_nan = list(np.tile(self.categories + [np.nan], N))\n        self.values_all_nan = [np.nan] * len(self.values)\n        self.values_all_int8 = np.ones(N, \""int8\"")\n        self.categorical = pd.Categorical(self.values, self.categories)\n        self.series = pd.Series(self.categorical)\n        self.intervals = pd.interval_range(0, 1, periods=N // 10)""}",categoricals.Constructor,time,0.0
24588,class-level,"terminus-2,oracle",0.9767446348584446,0.9767446348584446,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,ctors.DatetimeIndexConstructor,ctors,"{""ctors.DatetimeIndexConstructor.time_from_list_of_dates"": ""class DatetimeIndexConstructor:\n    def time_from_list_of_dates(self):\n        DatetimeIndex(self.list_of_dates)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndexConstructor:\n    def setup(self):\n        N = 20_000\n        dti = date_range(\""1900-01-01\"", periods=N)\n    \n        self.list_of_timestamps = dti.tolist()\n        self.list_of_dates = dti.date.tolist()\n        self.list_of_datetimes = dti.to_pydatetime().tolist()\n        self.list_of_str = dti.strftime(\""%Y-%m-%d\"").tolist()""}",ctors.DatetimeIndexConstructor,time,0.0
24596,class-level,"terminus-2,oracle",1.109764724574163,1.109764724574163,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromArrays,frame_ctor,"{""frame_ctor.FromArrays.time_frame_from_arrays_sparse"": ""class FromArrays:\n    def time_frame_from_arrays_sparse(self):\n        self.df = DataFrame._from_arrays(\n            self.sparse_arrays,\n            index=self.index,\n            columns=self.columns,\n            verify_integrity=False,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromArrays:\n    def setup(self):\n        N_rows = 1000\n        N_cols = 1000\n        self.float_arrays = [np.random.randn(N_rows) for _ in range(N_cols)]\n        self.sparse_arrays = [\n            pd.arrays.SparseArray(np.random.randint(0, 2, N_rows), dtype=\""float64\"")\n            for _ in range(N_cols)\n        ]\n        self.int_arrays = [\n            pd.array(np.random.randint(1000, size=N_rows), dtype=\""Int64\"")\n            for _ in range(N_cols)\n        ]\n        self.index = pd.Index(range(N_rows))\n        self.columns = pd.Index(range(N_cols))""}",frame_ctor.FromArrays,time,0.0
24591,class-level,"terminus-2,oracle",0.9843871123238844,0.9843871123238844,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.DtypesInvalid,dtypes,"{""dtypes.DtypesInvalid.time_pandas_dtype_invalid"": ""class DtypesInvalid:\n    def time_pandas_dtype_invalid(self, dtype):\n        try:\n            pandas_dtype(self.data_dict[dtype])\n        except TypeError:\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)""}",dtypes.DtypesInvalid,time,0.0
24592,class-level,"terminus-2,oracle",1.002221545307869,1.002221545307869,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes.SelectDtypes,dtypes,"{""dtypes.SelectDtypes.time_select_dtype_bool_include"": ""class SelectDtypes:\n    def time_select_dtype_bool_include(self, dtype):\n        self.df_bool.select_dtypes(include=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_float_include"": ""class SelectDtypes:\n    def time_select_dtype_float_include(self, dtype):\n        self.df_float.select_dtypes(include=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_int_exclude"": ""class SelectDtypes:\n    def time_select_dtype_int_exclude(self, dtype):\n        self.df_int.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_string_exclude"": ""class SelectDtypes:\n    def time_select_dtype_string_exclude(self, dtype):\n        self.df_string.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_string_include"": ""class SelectDtypes:\n    def time_select_dtype_string_include(self, dtype):\n        self.df_string.select_dtypes(include=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )""}",dtypes.SelectDtypes,time,0.0
24583,class-level,"terminus-2,oracle",0.9459350616406256,0.9459350616406256,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals.Indexing,categoricals,"{""categoricals.Indexing.time_align"": ""class Indexing:\n    def time_align(self):\n        pd.DataFrame({\""a\"": self.series, \""b\"": self.series[:500]})\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self):\n        N = 10**5\n        self.index = pd.CategoricalIndex(range(N), range(N))\n        self.series = pd.Series(range(N), index=self.index).sort_index()\n        self.category = self.index[500]"", ""categoricals.Indexing.time_reindex_missing"": ""class Indexing:\n    def time_reindex_missing(self):\n        self.index.reindex([\""a\"", \""b\"", \""c\"", \""d\""])\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self):\n        N = 10**5\n        self.index = pd.CategoricalIndex(range(N), range(N))\n        self.series = pd.Series(range(N), index=self.index).sort_index()\n        self.category = self.index[500]"", ""categoricals.Indexing.time_shallow_copy"": ""class Indexing:\n    def time_shallow_copy(self):\n        self.index._view()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self):\n        N = 10**5\n        self.index = pd.CategoricalIndex(range(N), range(N))\n        self.series = pd.Series(range(N), index=self.index).sort_index()\n        self.category = self.index[500]"", ""categoricals.Indexing.time_sort_values"": ""class Indexing:\n    def time_sort_values(self):\n        self.index.sort_values(ascending=False)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self):\n        N = 10**5\n        self.index = pd.CategoricalIndex(range(N), range(N))\n        self.series = pd.Series(range(N), index=self.index).sort_index()\n        self.category = self.index[500]"", ""categoricals.Indexing.time_unique"": ""class Indexing:\n    def time_unique(self):\n        self.index.unique()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self):\n        N = 10**5\n        self.index = pd.CategoricalIndex(range(N), range(N))\n        self.series = pd.Series(range(N), index=self.index).sort_index()\n        self.category = self.index[500]""}",categoricals.Indexing,time,0.0
24603,class-level,"terminus-2,oracle",1.1014807750057138,1.1014807750057138,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Count,frame_methods,"{""frame_methods.Count.time_count_level_mixed_dtypes_multi"": ""class Count:\n    def time_count_level_mixed_dtypes_multi(self, axis):\n        self.df_mixed.count(axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Count:\n    def setup(self, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""\n    \n        self.df.index = MultiIndex.from_arrays([self.df.index, self.df.index])\n        self.df.columns = MultiIndex.from_arrays([self.df.columns, self.df.columns])\n        self.df_mixed.index = MultiIndex.from_arrays(\n            [self.df_mixed.index, self.df_mixed.index]\n        )\n        self.df_mixed.columns = MultiIndex.from_arrays(\n            [self.df_mixed.columns, self.df_mixed.columns]\n        )""}",frame_methods.Count,time,0.0
24601,class-level,"terminus-2,oracle",1.043884568644845,1.043884568644845,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromScalar,frame_ctor,"{""frame_ctor.FromScalar.time_frame_from_scalar_ea_float64"": ""class FromScalar:\n    def time_frame_from_scalar_ea_float64(self):\n        DataFrame(\n            1.0,\n            index=range(self.nrows),\n            columns=list(\""abc\""),\n            dtype=Float64Dtype(),\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromScalar:\n    def setup(self):\n        self.nrows = 100_000"", ""frame_ctor.FromScalar.time_frame_from_scalar_ea_float64_na"": ""class FromScalar:\n    def time_frame_from_scalar_ea_float64_na(self):\n        DataFrame(\n            NA,\n            index=range(self.nrows),\n            columns=list(\""abc\""),\n            dtype=Float64Dtype(),\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromScalar:\n    def setup(self):\n        self.nrows = 100_000""}",frame_ctor.FromScalar,time,0.0
24594,class-level,"terminus-2,oracle",1.143118171288414,1.143118171288414,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval.Query,eval,"{""eval.Query.time_query_datetime_column"": ""class Query:\n    def time_query_datetime_column(self):\n        self.df.query(\""dates < @self.ts\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Query:\n    def setup(self):\n        N = 10**6\n        halfway = (N // 2) - 1\n        index = pd.date_range(\""20010101\"", periods=N, freq=\""T\"")\n        s = pd.Series(index)\n        self.ts = s.iloc[halfway]\n        self.df = pd.DataFrame({\""a\"": np.random.randn(N), \""dates\"": index}, index=index)\n        data = np.random.randn(N)\n        self.min_val = data.min()\n        self.max_val = data.max()"", ""eval.Query.time_query_with_boolean_selection"": ""class Query:\n    def time_query_with_boolean_selection(self):\n        self.df.query(\""(a >= @self.min_val) & (a <= @self.max_val)\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Query:\n    def setup(self):\n        N = 10**6\n        halfway = (N // 2) - 1\n        index = pd.date_range(\""20010101\"", periods=N, freq=\""T\"")\n        s = pd.Series(index)\n        self.ts = s.iloc[halfway]\n        self.df = pd.DataFrame({\""a\"": np.random.randn(N), \""dates\"": index}, index=index)\n        data = np.random.randn(N)\n        self.min_val = data.min()\n        self.max_val = data.max()""}",eval.Query,time,0.0
24599,class-level,"terminus-2,oracle",1.0154414722650773,1.0154414722650773,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromLists,frame_ctor,"{""frame_ctor.FromLists.time_frame_from_lists"": ""class FromLists:\n    def time_frame_from_lists(self):\n        self.df = DataFrame(self.data)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromLists:\n    def setup(self):\n        N = 1000\n        M = 100\n        self.data = [list(range(M)) for i in range(N)]""}",frame_ctor.FromLists,time,0.0
24598,class-level,"terminus-2,oracle",1.0145578631642584,1.0145578631642584,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromDictwithTimestamp,frame_ctor,"{""frame_ctor.FromDictwithTimestamp.time_dict_with_timestamp_offsets"": ""class FromDictwithTimestamp:\n    def time_dict_with_timestamp_offsets(self, offset):\n        DataFrame(self.d)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDictwithTimestamp:\n    def setup(self, offset):\n        N = 10**3\n        idx = date_range(Timestamp(\""1/1/1900\""), freq=offset, periods=N)\n        df = DataFrame(np.random.randn(N, 10), index=idx)\n        self.d = df.to_dict()""}",frame_ctor.FromDictwithTimestamp,time,0.0
24607,class-level,"terminus-2,oracle",1.5831426169002387,1.5831426169002387,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Equals,frame_methods,"{""frame_methods.Equals.time_frame_float_equal"": ""class Equals:\n    def time_frame_float_equal(self):\n        self.float_df.equals(self.float_df)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Equals:\n    def setup(self):\n        N = 10**3\n        self.float_df = DataFrame(np.random.randn(N, N))\n        self.float_df_nan = self.float_df.copy()\n        self.float_df_nan.iloc[-1, -1] = np.nan\n    \n        self.object_df = DataFrame(\""foo\"", index=range(N), columns=range(N))\n        self.object_df_nan = self.object_df.copy()\n        self.object_df_nan.iloc[-1, -1] = np.nan\n    \n        self.nonunique_cols = self.object_df.copy()\n        self.nonunique_cols.columns = [\""A\""] * len(self.nonunique_cols.columns)\n        self.nonunique_cols_nan = self.nonunique_cols.copy()\n        self.nonunique_cols_nan.iloc[-1, -1] = np.nan""}",frame_methods.Equals,time,0.0
24597,class-level,"terminus-2,oracle",1.0638199520587115,1.0638199520587115,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromDicts,frame_ctor,"{""frame_ctor.FromDicts.time_dict_of_categoricals"": ""class FromDicts:\n    def time_dict_of_categoricals(self):\n        # dict of arrays that we won't consolidate\n        DataFrame(self.dict_of_categoricals)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDicts:\n    def setup(self):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n        frame = DataFrame(np.random.randn(N, K), index=self.index, columns=self.columns)\n        self.data = frame.to_dict()\n        self.dict_list = frame.to_dict(orient=\""records\"")\n        self.data2 = {i: {j: float(j) for j in range(100)} for i in range(2000)}\n    \n        # arrays which we won't consolidate\n        self.dict_of_categoricals = {i: Categorical(np.arange(N)) for i in range(K)}"", ""frame_ctor.FromDicts.time_list_of_dict"": ""class FromDicts:\n    def time_list_of_dict(self):\n        DataFrame(self.dict_list)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDicts:\n    def setup(self):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n        frame = DataFrame(np.random.randn(N, K), index=self.index, columns=self.columns)\n        self.data = frame.to_dict()\n        self.dict_list = frame.to_dict(orient=\""records\"")\n        self.data2 = {i: {j: float(j) for j in range(100)} for i in range(2000)}\n    \n        # arrays which we won't consolidate\n        self.dict_of_categoricals = {i: Categorical(np.arange(N)) for i in range(K)}"", ""frame_ctor.FromDicts.time_nested_dict_int64"": ""class FromDicts:\n    def time_nested_dict_int64(self):\n        # nested dict, integer indexes, regression described in #621\n        DataFrame(self.data2)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDicts:\n    def setup(self):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n        frame = DataFrame(np.random.randn(N, K), index=self.index, columns=self.columns)\n        self.data = frame.to_dict()\n        self.dict_list = frame.to_dict(orient=\""records\"")\n        self.data2 = {i: {j: float(j) for j in range(100)} for i in range(2000)}\n    \n        # arrays which we won't consolidate\n        self.dict_of_categoricals = {i: Categorical(np.arange(N)) for i in range(K)}""}",frame_ctor.FromDicts,time,0.0
24604,class-level,"terminus-2,oracle",1.0483986784350785,1.0483986784350785,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Describe,frame_methods,"{""frame_methods.Describe.time_dataframe_describe"": ""class Describe:\n    def time_dataframe_describe(self):\n        self.df.describe()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Describe:\n    def setup(self):\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(0, 100, 10**6),\n                \""b\"": np.random.randint(0, 100, 10**6),\n                \""c\"": np.random.randint(0, 100, 10**6),\n            }\n        )"", ""frame_methods.Describe.time_series_describe"": ""class Describe:\n    def time_series_describe(self):\n        self.df[\""a\""].describe()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Describe:\n    def setup(self):\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(0, 100, 10**6),\n                \""b\"": np.random.randint(0, 100, 10**6),\n                \""c\"": np.random.randint(0, 100, 10**6),\n            }\n        )""}",frame_methods.Describe,time,0.0
24602,class-level,"terminus-2,oracle",1.0622858336671763,1.0622858336671763,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Apply,frame_methods,"{""frame_methods.Apply.time_apply_axis_1"": ""class Apply:\n    def time_apply_axis_1(self):\n        self.df.apply(lambda x: x + 1, axis=1)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self):\n        self.df = DataFrame(np.random.randn(1000, 100))\n    \n        self.s = Series(np.arange(1028.0))\n        self.df2 = DataFrame({i: self.s for i in range(1028)})\n        self.df3 = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))"", ""frame_methods.Apply.time_apply_lambda_mean"": ""class Apply:\n    def time_apply_lambda_mean(self):\n        self.df.apply(lambda x: x.mean())\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self):\n        self.df = DataFrame(np.random.randn(1000, 100))\n    \n        self.s = Series(np.arange(1028.0))\n        self.df2 = DataFrame({i: self.s for i in range(1028)})\n        self.df3 = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))"", ""frame_methods.Apply.time_apply_np_mean"": ""class Apply:\n    def time_apply_np_mean(self):\n        self.df.apply(np.mean)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self):\n        self.df = DataFrame(np.random.randn(1000, 100))\n    \n        self.s = Series(np.arange(1028.0))\n        self.df2 = DataFrame({i: self.s for i in range(1028)})\n        self.df3 = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))"", ""frame_methods.Apply.time_apply_pass_thru"": ""class Apply:\n    def time_apply_pass_thru(self):\n        self.df.apply(lambda x: x)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self):\n        self.df = DataFrame(np.random.randn(1000, 100))\n    \n        self.s = Series(np.arange(1028.0))\n        self.df2 = DataFrame({i: self.s for i in range(1028)})\n        self.df3 = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))""}",frame_methods.Apply,time,0.0
24608,class-level,"terminus-2,oracle",1.0901762619178108,1.0901762619178108,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Fillna,frame_methods,"{""frame_methods.Fillna.time_frame_fillna"": ""class Fillna:\n    def time_frame_fillna(self, inplace, method, dtype):\n        self.df.fillna(inplace=inplace, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Fillna:\n    def setup(self, inplace, method, dtype):\n        N, M = 10000, 100\n        if dtype in (\""datetime64[ns]\"", \""datetime64[ns, tz]\"", \""timedelta64[ns]\""):\n            data = {\n                \""datetime64[ns]\"": date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n                \""datetime64[ns, tz]\"": date_range(\n                    \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n                ),\n                \""timedelta64[ns]\"": timedelta_range(start=\""1 day\"", periods=N, freq=\""1D\""),\n            }\n            self.df = DataFrame({f\""col_{i}\"": data[dtype] for i in range(M)})\n            self.df[::2] = None\n        else:\n            values = np.random.randn(N, M)\n            values[::2] = np.nan\n            if dtype == \""Int64\"":\n                values = values.round()\n            self.df = DataFrame(values, dtype=dtype)""}",frame_methods.Fillna,time,0.0
24609,class-level,"terminus-2,oracle",1.026015236530584,1.026015236530584,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Interpolate,frame_methods,"{""frame_methods.Interpolate.time_interpolate_some_good"": ""class Interpolate:\n    def time_interpolate_some_good(self, downcast):\n        self.df2.interpolate(downcast=downcast)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Interpolate:\n    def setup(self, downcast):\n        N = 10000\n        # this is the worst case, where every column has NaNs.\n        arr = np.random.randn(N, 100)\n        # NB: we need to set values in array, not in df.values, otherwise\n        #  the benchmark will be misleading for ArrayManager\n        arr[::2] = np.nan\n    \n        self.df = DataFrame(arr)\n    \n        self.df2 = DataFrame(\n            {\n                \""A\"": np.arange(0, N),\n                \""B\"": np.random.randint(0, 100, N),\n                \""C\"": np.random.randn(N),\n                \""D\"": np.random.randn(N),\n            }\n        )\n        self.df2.loc[1::5, \""A\""] = np.nan\n        self.df2.loc[1::5, \""C\""] = np.nan""}",frame_methods.Interpolate,time,0.0
24605,class-level,"terminus-2,oracle",1.1476588961940375,1.1476588961940375,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Dropna,frame_methods,"{""frame_methods.Dropna.time_dropna"": ""class Dropna:\n    def time_dropna(self, how, axis):\n        self.df.dropna(how=how, axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Dropna:\n    def setup(self, how, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\"""", ""frame_methods.Dropna.time_dropna_axis_mixed_dtypes"": ""class Dropna:\n    def time_dropna_axis_mixed_dtypes(self, how, axis):\n        self.df_mixed.dropna(how=how, axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Dropna:\n    def setup(self, how, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""""}",frame_methods.Dropna,time,0.0
24606,class-level,"terminus-2,oracle",1.0415164232546907,1.0415164232546907,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Duplicated,frame_methods,"{""frame_methods.Duplicated.time_frame_duplicated_wide"": ""class Duplicated:\n    def time_frame_duplicated_wide(self):\n        self.df2.duplicated()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Duplicated:\n    def setup(self):\n        n = 1 << 20\n        t = date_range(\""2015-01-01\"", freq=\""S\"", periods=(n // 64))\n        xs = np.random.randn(n // 64).round(2)\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(-1 << 8, 1 << 8, n),\n                \""b\"": np.random.choice(t, n),\n                \""c\"": np.random.choice(xs, n),\n            }\n        )\n        self.df2 = DataFrame(np.random.randn(1000, 100).astype(str)).T""}",frame_methods.Duplicated,time,0.0
24600,class-level,"terminus-2,oracle",1.048155438963408,1.048155438963408,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor.FromRecords,frame_ctor,"{""frame_ctor.FromRecords.time_frame_from_records_generator"": ""class FromRecords:\n    def time_frame_from_records_generator(self, nrows):\n        # issue-6700\n        self.df = DataFrame.from_records(self.gen, nrows=nrows)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromRecords:\n    def setup(self, nrows):\n        N = 100000\n        self.gen = ((x, (x * 20), (x * 100)) for x in range(N))""}",frame_ctor.FromRecords,time,0.0
24620,class-level,"terminus-2,oracle",1.070948001002442,1.070948001002442,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.Cumulative,groupby,"{""groupby.Cumulative.time_frame_transform"": ""class Cumulative:\n    def time_frame_transform(self, dtype, method, with_nans):\n        self.df.groupby(\""key\"").transform(method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cumulative:\n    def setup(self, dtype, method, with_nans):\n        if with_nans and dtype == \""int64\"":\n            raise NotImplementedError(\""Construction of df would raise\"")\n    \n        N = 500_000\n        keys = np.random.randint(0, 100, size=N)\n        vals = np.random.randint(-10, 10, (N, 5))\n    \n        if with_nans:\n            null_vals = vals.astype(float, copy=True)\n            null_vals[::2, :] = np.nan\n            null_vals[::3, :] = np.nan\n            df = DataFrame(null_vals, columns=list(\""abcde\""), dtype=dtype)\n            df[\""key\""] = keys\n            self.df = df\n        else:\n            df = DataFrame(vals, columns=list(\""abcde\"")).astype(dtype, copy=False)\n            df[\""key\""] = keys\n            self.df = df""}",groupby.Cumulative,time,0.0
24616,class-level,"terminus-2,oracle",0.9677818173268288,0.9677818173268288,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,gil.ParallelRolling,gil,"{""gil.ParallelRolling.time_rolling"": ""class ParallelRolling:\n    def time_rolling(self, method):\n        self.parallel_rolling()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ParallelRolling:\n    def setup(self, method):\n        win = 100\n        arr = np.random.rand(100000)\n        if hasattr(DataFrame, \""rolling\""):\n            df = DataFrame(arr).rolling(win)\n    \n            @test_parallel(num_threads=2)\n            def parallel_rolling():\n                getattr(df, method)()\n    \n            self.parallel_rolling = parallel_rolling\n        elif have_rolling_methods:\n            rolling = {\n                \""median\"": rolling_median,\n                \""mean\"": rolling_mean,\n                \""min\"": rolling_min,\n                \""max\"": rolling_max,\n                \""var\"": rolling_var,\n                \""skew\"": rolling_skew,\n                \""kurt\"": rolling_kurt,\n                \""std\"": rolling_std,\n            }\n    \n            @test_parallel(num_threads=2)\n            def parallel_rolling():\n                rolling[method](arr, win)\n    \n            self.parallel_rolling = parallel_rolling\n        else:\n            raise NotImplementedError""}",gil.ParallelRolling,time,0.0
24611,class-level,"terminus-2,oracle",1.1196789103956328,1.1196789103956328,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.NSort,frame_methods,"{""frame_methods.NSort.time_nlargest_two_columns"": ""class NSort:\n    def time_nlargest_two_columns(self, keep):\n        self.df.nlargest(100, [\""A\"", \""B\""], keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NSort:\n    def setup(self, keep):\n        self.df = DataFrame(np.random.randn(100000, 3), columns=list(\""ABC\""))"", ""frame_methods.NSort.time_nsmallest_one_column"": ""class NSort:\n    def time_nsmallest_one_column(self, keep):\n        self.df.nsmallest(100, \""A\"", keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NSort:\n    def setup(self, keep):\n        self.df = DataFrame(np.random.randn(100000, 3), columns=list(\""ABC\""))"", ""frame_methods.NSort.time_nsmallest_two_columns"": ""class NSort:\n    def time_nsmallest_two_columns(self, keep):\n        self.df.nsmallest(100, [\""A\"", \""B\""], keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NSort:\n    def setup(self, keep):\n        self.df = DataFrame(np.random.randn(100000, 3), columns=list(\""ABC\""))""}",frame_methods.NSort,time,0.0
24614,class-level,"terminus-2,oracle",1.0496179777334738,1.0496179777334738,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.ToNumpy,frame_methods,"{""frame_methods.ToNumpy.time_to_numpy_tall"": ""class ToNumpy:\n    def time_to_numpy_tall(self):\n        self.df_tall.to_numpy()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToNumpy:\n    def setup(self):\n        N = 10000\n        M = 10\n        self.df_tall = DataFrame(np.random.randn(N, M))\n        self.df_wide = DataFrame(np.random.randn(M, N))\n        self.df_mixed_tall = self.df_tall.copy()\n        self.df_mixed_tall[\""foo\""] = \""bar\""\n        self.df_mixed_tall[0] = period_range(\""2000\"", periods=N)\n        self.df_mixed_tall[1] = range(N)\n        self.df_mixed_wide = self.df_wide.copy()\n        self.df_mixed_wide[\""foo\""] = \""bar\""\n        self.df_mixed_wide[0] = period_range(\""2000\"", periods=M)\n        self.df_mixed_wide[1] = range(M)"", ""frame_methods.ToNumpy.time_to_numpy_wide"": ""class ToNumpy:\n    def time_to_numpy_wide(self):\n        self.df_wide.to_numpy()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToNumpy:\n    def setup(self):\n        N = 10000\n        M = 10\n        self.df_tall = DataFrame(np.random.randn(N, M))\n        self.df_wide = DataFrame(np.random.randn(M, N))\n        self.df_mixed_tall = self.df_tall.copy()\n        self.df_mixed_tall[\""foo\""] = \""bar\""\n        self.df_mixed_tall[0] = period_range(\""2000\"", periods=N)\n        self.df_mixed_tall[1] = range(N)\n        self.df_mixed_wide = self.df_wide.copy()\n        self.df_mixed_wide[\""foo\""] = \""bar\""\n        self.df_mixed_wide[0] = period_range(\""2000\"", periods=M)\n        self.df_mixed_wide[1] = range(M)"", ""frame_methods.ToNumpy.time_values_tall"": ""class ToNumpy:\n    def time_values_tall(self):\n        self.df_tall.values\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToNumpy:\n    def setup(self):\n        N = 10000\n        M = 10\n        self.df_tall = DataFrame(np.random.randn(N, M))\n        self.df_wide = DataFrame(np.random.randn(M, N))\n        self.df_mixed_tall = self.df_tall.copy()\n        self.df_mixed_tall[\""foo\""] = \""bar\""\n        self.df_mixed_tall[0] = period_range(\""2000\"", periods=N)\n        self.df_mixed_tall[1] = range(N)\n        self.df_mixed_wide = self.df_wide.copy()\n        self.df_mixed_wide[\""foo\""] = \""bar\""\n        self.df_mixed_wide[0] = period_range(\""2000\"", periods=M)\n        self.df_mixed_wide[1] = range(M)"", ""frame_methods.ToNumpy.time_values_wide"": ""class ToNumpy:\n    def time_values_wide(self):\n        self.df_wide.values\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToNumpy:\n    def setup(self):\n        N = 10000\n        M = 10\n        self.df_tall = DataFrame(np.random.randn(N, M))\n        self.df_wide = DataFrame(np.random.randn(M, N))\n        self.df_mixed_tall = self.df_tall.copy()\n        self.df_mixed_tall[\""foo\""] = \""bar\""\n        self.df_mixed_tall[0] = period_range(\""2000\"", periods=N)\n        self.df_mixed_tall[1] = range(N)\n        self.df_mixed_wide = self.df_wide.copy()\n        self.df_mixed_wide[\""foo\""] = \""bar\""\n        self.df_mixed_wide[0] = period_range(\""2000\"", periods=M)\n        self.df_mixed_wide[1] = range(M)""}",frame_methods.ToNumpy,time,0.0
24610,class-level,"terminus-2,oracle",1.0537508533091864,1.0537508533091864,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Iteration,frame_methods,"{""frame_methods.Iteration.time_items"": ""class Iteration:\n    def time_items(self):\n        # (monitor no-copying behaviour)\n        if hasattr(self.df, \""_item_cache\""):\n            self.df._item_cache.clear()\n        for name, col in self.df.items():\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))"", ""frame_methods.Iteration.time_items_cached"": ""class Iteration:\n    def time_items_cached(self):\n        for name, col in self.df.items():\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))"", ""frame_methods.Iteration.time_iteritems_indexing"": ""class Iteration:\n    def time_iteritems_indexing(self):\n        for col in self.df3:\n            self.df3[col]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))"", ""frame_methods.Iteration.time_iterrows"": ""class Iteration:\n    def time_iterrows(self):\n        for row in self.df.iterrows():\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))"", ""frame_methods.Iteration.time_itertuples_raw_start"": ""class Iteration:\n    def time_itertuples_raw_start(self):\n        self.df4.itertuples(index=False, name=None)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))""}",frame_methods.Iteration,time,0.0
24617,class-level,"terminus-2,oracle",0.960157811866578,0.960157811866578,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,gil.ParallelTake1D,gil,"{""gil.ParallelTake1D.time_take1d"": ""class ParallelTake1D:\n    def time_take1d(self, dtype):\n        self.parallel_take1d()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ParallelTake1D:\n    def setup(self, dtype):\n        N = 10**6\n        df = DataFrame({\""col\"": np.arange(N, dtype=dtype)})\n        indexer = np.arange(100, len(df) - 100)\n    \n        @test_parallel(num_threads=2)\n        def parallel_take1d():\n            take_nd(df[\""col\""].values, indexer)\n    \n        self.parallel_take1d = parallel_take1d""}",gil.ParallelTake1D,time,0.0
24615,class-level,"terminus-2,oracle",1.1079542764811172,1.1079542764811172,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,gil.ParallelGroupbyMethods,gil,"{""gil.ParallelGroupbyMethods.time_parallel"": ""class ParallelGroupbyMethods:\n    def time_parallel(self, threads, method):\n        self.parallel()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ParallelGroupbyMethods:\n    def setup(self, threads, method):\n        N = 10**6\n        ngroups = 10**3\n        df = DataFrame(\n            {\""key\"": np.random.randint(0, ngroups, size=N), \""data\"": np.random.randn(N)}\n        )\n    \n        @test_parallel(num_threads=threads)\n        def parallel():\n            getattr(df.groupby(\""key\"")[\""data\""], method)()\n    \n        self.parallel = parallel\n    \n        def loop():\n            getattr(df.groupby(\""key\"")[\""data\""], method)()\n    \n        self.loop = loop""}",gil.ParallelGroupbyMethods,time,0.0
24618,class-level,"terminus-2,oracle",1.0230055870235066,1.0230055870235066,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.Apply,groupby,"{""groupby.Apply.time_scalar_function_multi_col"": ""class Apply:\n    def time_scalar_function_multi_col(self, factor):\n        self.df.groupby([\""key\"", \""key2\""]).apply(lambda x: 1)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, factor):\n        N = 10**factor\n        # two cases:\n        # - small groups: small data (N**4) + many labels (2000) -> average group\n        #   size of 5 (-> larger overhead of slicing method)\n        # - larger groups: larger data (N**5) + fewer labels (20) -> average group\n        #   size of 5000\n        labels = np.random.randint(0, 2000 if factor == 4 else 20, size=N)\n        labels2 = np.random.randint(0, 3, size=N)\n        df = DataFrame(\n            {\n                \""key\"": labels,\n                \""key2\"": labels2,\n                \""value1\"": np.random.randn(N),\n                \""value2\"": [\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (N // 4),\n            }\n        )\n        self.df = df""}",groupby.Apply,time,0.0
24595,class-level,"terminus-2,oracle",0.994961338034834,0.994961338034834,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,finalize.Finalize,finalize,"{""finalize.Finalize.time_finalize_micro"": ""class Finalize:\n    def time_finalize_micro(self, param):\n        self.obj.__finalize__(self.obj, method=\""__finalize__\"")\n\n    def setup(self, param):\n        N = 1000\n        obj = param(dtype=float)\n        for i in range(N):\n            obj.attrs[i] = i\n        self.obj = obj""}",finalize.Finalize,time,0.0
24619,class-level,"terminus-2,oracle",1.0188571417553025,1.0188571417553025,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.ApplyNonUniqueUnsortedIndex,groupby,"{""groupby.ApplyNonUniqueUnsortedIndex.time_groupby_apply_non_unique_unsorted_index"": ""class ApplyNonUniqueUnsortedIndex:\n    def time_groupby_apply_non_unique_unsorted_index(self):\n        self.df.groupby(\""key\"", group_keys=False).apply(lambda x: x)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ApplyNonUniqueUnsortedIndex:\n    def setup(self):\n        # GH 46527\n        # unsorted and non-unique index\n        idx = np.arange(100)[::-1]\n        idx = Index(np.repeat(idx, 200), name=\""key\"")\n        self.df = DataFrame(np.random.randn(len(idx), 10), index=idx)""}",groupby.ApplyNonUniqueUnsortedIndex,time,0.0
24612,class-level,"terminus-2,oracle",1.0248017468505206,1.0248017468505206,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.Quantile,frame_methods,"{""frame_methods.Quantile.time_frame_quantile"": ""class Quantile:\n    def time_frame_quantile(self, axis):\n        self.df.quantile([0.1, 0.5], axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, axis):\n        self.df = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))""}",frame_methods.Quantile,time,0.0
24613,class-level,"terminus-2,oracle",1.274450093142372,1.274450093142372,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods.ToDict,frame_methods,"{""frame_methods.ToDict.time_to_dict_ints"": ""class ToDict:\n    def time_to_dict_ints(self, orient):\n        self.int_df.to_dict(orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToDict:\n    def setup(self, orient):\n        data = np.random.randint(0, 1000, size=(10000, 4))\n        self.int_df = DataFrame(data)\n        self.datetimelike_df = self.int_df.astype(\""timedelta64[ns]\"")""}",frame_methods.ToDict,time,0.0
24622,class-level,"terminus-2,oracle",1.0355765116122002,1.0355765116122002,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.GroupManyLabels,groupby,"{""groupby.GroupManyLabels.time_sum"": ""class GroupManyLabels:\n    def time_sum(self, ncols):\n        self.df.groupby(self.labels).sum()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupManyLabels:\n    def setup(self, ncols):\n        N = 1000\n        data = np.random.randn(N, ncols)\n        self.labels = np.random.randint(0, 100, size=N)\n        self.df = DataFrame(data)""}",groupby.GroupManyLabels,time,0.0
24629,class-level,"terminus-2,oracle",1.1141947051174346,1.1141947051174346,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,hash_functions.UniqueAndFactorizeArange,hash_functions,"{""hash_functions.UniqueAndFactorizeArange.time_factorize"": ""class UniqueAndFactorizeArange:\n    def time_factorize(self, exponent):\n        pd.factorize(self.a2)\n\n    def setup(self, exponent):\n        a = np.arange(10**4, dtype=\""float64\"")\n        self.a2 = (a + 10**exponent).repeat(100)""}",hash_functions.UniqueAndFactorizeArange,time,0.0
24625,class-level,"terminus-2,oracle",0.9242780314831636,0.9242780314831636,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.Size,groupby,"{""groupby.Size.time_category_size"": ""class Size:\n    def time_category_size(self):\n        self.draws.groupby(self.cats).size()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Size:\n    def setup(self):\n        n = 10**5\n        offsets = np.random.randint(n, size=n).astype(\""timedelta64[ns]\"")\n        dates = np.datetime64(\""now\"") + offsets\n        self.df = DataFrame(\n            {\n                \""key1\"": np.random.randint(0, 500, size=n),\n                \""key2\"": np.random.randint(0, 100, size=n),\n                \""value1\"": np.random.randn(n),\n                \""value2\"": np.random.randn(n),\n                \""value3\"": np.random.randn(n),\n                \""dates\"": dates,\n            }\n        )\n        self.draws = Series(np.random.randn(n))\n        labels = Series([\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (n // 4))\n        self.cats = labels.astype(\""category\"")""}",groupby.Size,time,0.0
24627,class-level,"terminus-2,oracle",1.042586536069814,1.042586536069814,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.Transform,groupby,"{""groupby.Transform.time_transform_lambda_max_wide"": ""class Transform:\n    def time_transform_lambda_max_wide(self):\n        self.df_wide.groupby(level=0).transform(lambda x: np.max(x, axis=0))\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]""}",groupby.Transform,time,0.0
24626,class-level,"terminus-2,oracle",0.8147115270091495,0.8147115270091495,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.String,groupby,"{""groupby.String.time_str_func"": ""class String:\n    def time_str_func(self, dtype, method):\n        self.df.groupby(\""a\"")[self.df.columns[1:]].agg(method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass String:\n    def setup(self, dtype, method):\n        cols = list(\""abcdefghjkl\"")\n        self.df = DataFrame(\n            np.random.randint(0, 100, size=(10_000, len(cols))),\n            columns=cols,\n            dtype=dtype,\n        )""}",groupby.String,time,0.0
24637,class-level,"terminus-2,oracle",1.038504163552764,1.038504163552764,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.IndexSingleRow,indexing,"{""indexing.IndexSingleRow.time_loc_row"": ""class IndexSingleRow:\n    def time_loc_row(self, unique_cols):\n        self.df.loc[10000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IndexSingleRow:\n    def setup(self, unique_cols):\n        arr = np.arange(10**7).reshape(-1, 10)\n        df = DataFrame(arr)\n        dtypes = [\""u1\"", \""u2\"", \""u4\"", \""u8\"", \""i1\"", \""i2\"", \""i4\"", \""i8\"", \""f8\"", \""f4\""]\n        for i, d in enumerate(dtypes):\n            df[i] = df[i].astype(d)\n    \n        if not unique_cols:\n            # GH#33032 single-row lookups with non-unique columns were\n            #  15x slower than with unique columns\n            df.columns = [\""A\"", \""A\""] + list(df.columns[2:])\n    \n        self.df = df""}",indexing.IndexSingleRow,time,0.0
24635,class-level,"terminus-2,oracle",1.0333263660754053,1.0333263660754053,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.DatetimeIndexIndexing,indexing,"{""indexing.DatetimeIndexIndexing.time_get_indexer_mismatched_tz"": ""class DatetimeIndexIndexing:\n    def time_get_indexer_mismatched_tz(self):\n        # reached via e.g.\n        #  ser = Series(range(len(dti)), index=dti)\n        #  ser[dti2]\n        self.dti.get_indexer(self.dti2)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndexIndexing:\n    def setup(self):\n        dti = date_range(\""2016-01-01\"", periods=10000, tz=\""US/Pacific\"")\n        dti2 = dti.tz_convert(\""UTC\"")\n        self.dti = dti\n        self.dti2 = dti2""}",indexing.DatetimeIndexIndexing,time,0.0
24628,class-level,"terminus-2,oracle",1.0353260541593894,1.0353260541593894,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,hash_functions.NumericSeriesIndexing,hash_functions,"{""hash_functions.NumericSeriesIndexing.time_loc_slice"": ""class NumericSeriesIndexing:\n    def time_loc_slice(self, index, N):\n        # trigger building of mapping\n        self.data.loc[:800]\n\n    def setup(self, dtype, N):\n        vals = np.array(list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype)\n        indices = pd.Index(vals)\n        self.data = pd.Series(np.arange(N), index=indices)""}",hash_functions.NumericSeriesIndexing,time,0.0
24632,class-level,"terminus-2,oracle",1.1140343749271642,1.1140343749271642,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_object.SetOperations,index_object,"{""index_object.SetOperations.time_operation"": ""class SetOperations:\n    def time_operation(self, index_structure, dtype, method):\n        getattr(self.left, method)(self.right)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SetOperations:\n    def setup(self, index_structure, dtype, method):\n        N = 10**5\n        dates_left = date_range(\""1/1/2000\"", periods=N, freq=\""T\"")\n        fmt = \""%Y-%m-%d %H:%M:%S\""\n        date_str_left = Index(dates_left.strftime(fmt))\n        int_left = Index(np.arange(N))\n        ea_int_left = Index(np.arange(N), dtype=\""Int64\"")\n        str_left = tm.makeStringIndex(N)\n    \n        data = {\n            \""datetime\"": dates_left,\n            \""date_string\"": date_str_left,\n            \""int\"": int_left,\n            \""strings\"": str_left,\n            \""ea_int\"": ea_int_left,\n        }\n    \n        if index_structure == \""non_monotonic\"":\n            data = {k: mi[::-1] for k, mi in data.items()}\n    \n        data = {k: {\""left\"": idx, \""right\"": idx[:-1]} for k, idx in data.items()}\n    \n        self.left = data[dtype][\""left\""]\n        self.right = data[dtype][\""right\""]""}",index_object.SetOperations,time,0.0
24634,class-level,"terminus-2,oracle",1.123397506290995,1.123397506290995,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.DataFrameStringIndexing,indexing,"{""indexing.DataFrameStringIndexing.time_at_setitem"": ""class DataFrameStringIndexing:\n    def time_at_setitem(self):\n        self.df.at[self.idx_scalar, self.col_scalar] = 0.0\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DataFrameStringIndexing:\n    def setup(self):\n        index = tm.makeStringIndex(1000)\n        columns = tm.makeStringIndex(30)\n        with warnings.catch_warnings(record=True):\n            self.df = DataFrame(np.random.randn(1000, 30), index=index, columns=columns)\n        self.idx_scalar = index[100]\n        self.col_scalar = columns[10]\n        self.bool_indexer = self.df[self.col_scalar] > 0\n        self.bool_obj_indexer = self.bool_indexer.astype(object)\n        self.boolean_indexer = (self.df[self.col_scalar] > 0).astype(\""boolean\"")""}",indexing.DataFrameStringIndexing,time,0.0
24621,class-level,"terminus-2,oracle",1.0681337584941253,1.0681337584941253,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.GroupByMethods,groupby,"{""groupby.GroupByMethods.time_dtype_as_field"": ""class GroupByMethods:\n    def time_dtype_as_field(self, dtype, method, application, ncols):\n        self.as_field_method()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupByMethods:\n    def setup(self, dtype, method, application, ncols):\n        if method in method_blocklist.get(dtype, {}):\n            raise NotImplementedError  # skip benchmark\n    \n        if ncols != 1 and method in [\""value_counts\"", \""unique\""]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if application == \""transformation\"" and method in [\n            \""describe\"",\n            \""head\"",\n            \""tail\"",\n            \""unique\"",\n            \""value_counts\"",\n            \""size\"",\n        ]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if method == \""describe\"":\n            ngroups = 20\n        elif method == \""skew\"":\n            ngroups = 100\n        else:\n            ngroups = 1000\n        size = ngroups * 2\n        rng = np.arange(ngroups).reshape(-1, 1)\n        rng = np.broadcast_to(rng, (len(rng), ncols))\n        taker = np.random.randint(0, ngroups, size=size)\n        values = rng.take(taker, axis=0)\n        if dtype == \""int\"":\n            key = np.random.randint(0, size, size=size)\n        elif dtype in (\""int16\"", \""uint\""):\n            key = np.random.randint(0, size, size=size, dtype=dtype)\n        elif dtype == \""float\"":\n            key = np.concatenate(\n                [np.random.random(ngroups) * 0.1, np.random.random(ngroups) * 10.0]\n            )\n        elif dtype == \""object\"":\n            key = [\""foo\""] * size\n        elif dtype == \""datetime\"":\n            key = date_range(\""1/1/2011\"", periods=size, freq=\""s\"")\n    \n        cols = [f\""values{n}\"" for n in range(ncols)]\n        df = DataFrame(values, columns=cols)\n        df[\""key\""] = key\n    \n        if len(cols) == 1:\n            cols = cols[0]\n    \n        if application == \""transformation\"":\n            self.as_group_method = lambda: df.groupby(\""key\"")[cols].transform(method)\n            self.as_field_method = lambda: df.groupby(cols)[\""key\""].transform(method)\n        else:\n            self.as_group_method = getattr(df.groupby(\""key\"")[cols], method)\n            self.as_field_method = getattr(df.groupby(cols)[\""key\""], method)"", ""groupby.GroupByMethods.time_dtype_as_group"": ""class GroupByMethods:\n    def time_dtype_as_group(self, dtype, method, application, ncols):\n        self.as_group_method()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupByMethods:\n    def setup(self, dtype, method, application, ncols):\n        if method in method_blocklist.get(dtype, {}):\n            raise NotImplementedError  # skip benchmark\n    \n        if ncols != 1 and method in [\""value_counts\"", \""unique\""]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if application == \""transformation\"" and method in [\n            \""describe\"",\n            \""head\"",\n            \""tail\"",\n            \""unique\"",\n            \""value_counts\"",\n            \""size\"",\n        ]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if method == \""describe\"":\n            ngroups = 20\n        elif method == \""skew\"":\n            ngroups = 100\n        else:\n            ngroups = 1000\n        size = ngroups * 2\n        rng = np.arange(ngroups).reshape(-1, 1)\n        rng = np.broadcast_to(rng, (len(rng), ncols))\n        taker = np.random.randint(0, ngroups, size=size)\n        values = rng.take(taker, axis=0)\n        if dtype == \""int\"":\n            key = np.random.randint(0, size, size=size)\n        elif dtype in (\""int16\"", \""uint\""):\n            key = np.random.randint(0, size, size=size, dtype=dtype)\n        elif dtype == \""float\"":\n            key = np.concatenate(\n                [np.random.random(ngroups) * 0.1, np.random.random(ngroups) * 10.0]\n            )\n        elif dtype == \""object\"":\n            key = [\""foo\""] * size\n        elif dtype == \""datetime\"":\n            key = date_range(\""1/1/2011\"", periods=size, freq=\""s\"")\n    \n        cols = [f\""values{n}\"" for n in range(ncols)]\n        df = DataFrame(values, columns=cols)\n        df[\""key\""] = key\n    \n        if len(cols) == 1:\n            cols = cols[0]\n    \n        if application == \""transformation\"":\n            self.as_group_method = lambda: df.groupby(\""key\"")[cols].transform(method)\n            self.as_field_method = lambda: df.groupby(cols)[\""key\""].transform(method)\n        else:\n            self.as_group_method = getattr(df.groupby(\""key\"")[cols], method)\n            self.as_field_method = getattr(df.groupby(cols)[\""key\""], method)""}",groupby.GroupByMethods,time,0.0
24623,class-level,"terminus-2,oracle",1.1043999320046491,1.1043999320046491,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.Groups,groupby,"{""groupby.Groups.time_series_groups"": ""class Groups:\n    def time_series_groups(self, data, key):\n        self.ser.groupby(self.ser).groups\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Groups:\n    def setup(self, data, key):\n        self.ser = data[key]\n\n    def setup_cache(self):\n        size = 10**6\n        data = {\n            \""int64_small\"": Series(np.random.randint(0, 100, size=size)),\n            \""int64_large\"": Series(np.random.randint(0, 10000, size=size)),\n            \""object_small\"": Series(\n                tm.makeStringIndex(100).take(np.random.randint(0, 100, size=size))\n            ),\n            \""object_large\"": Series(\n                tm.makeStringIndex(10000).take(np.random.randint(0, 10000, size=size))\n            ),\n        }\n        return data""}",groupby.Groups,time,0.0
24631,class-level,"terminus-2,oracle",1.0245446190102028,1.0245446190102028,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_object.Indexing,index_object,"{""index_object.Indexing.time_get"": ""class Indexing:\n    def time_get(self, dtype):\n        self.idx[1]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self, dtype):\n        N = 10**6\n        self.idx = getattr(tm, f\""make{dtype}Index\"")(N)\n        self.array_mask = (np.arange(N) % 3) == 0\n        self.series_mask = Series(self.array_mask)\n        self.sorted = self.idx.sort_values()\n        half = N // 2\n        self.non_unique = self.idx[:half].append(self.idx[:half])\n        self.non_unique_sorted = (\n            self.sorted[:half].append(self.sorted[:half]).sort_values()\n        )\n        self.key = self.sorted[N // 4]"", ""index_object.Indexing.time_get_loc_sorted"": ""class Indexing:\n    def time_get_loc_sorted(self, dtype):\n        self.sorted.get_loc(self.key)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self, dtype):\n        N = 10**6\n        self.idx = getattr(tm, f\""make{dtype}Index\"")(N)\n        self.array_mask = (np.arange(N) % 3) == 0\n        self.series_mask = Series(self.array_mask)\n        self.sorted = self.idx.sort_values()\n        half = N // 2\n        self.non_unique = self.idx[:half].append(self.idx[:half])\n        self.non_unique_sorted = (\n            self.sorted[:half].append(self.sorted[:half]).sort_values()\n        )\n        self.key = self.sorted[N // 4]""}",index_object.Indexing,time,0.0
24624,class-level,"terminus-2,oracle",0.7453157545798069,0.7453157545798069,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby.RankWithTies,groupby,"{""groupby.RankWithTies.time_rank_ties"": ""class RankWithTies:\n    def time_rank_ties(self, dtype, tie_method):\n        self.df.groupby(\""key\"").rank(method=tie_method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass RankWithTies:\n    def setup(self, dtype, tie_method):\n        N = 10**4\n        if dtype == \""datetime64\"":\n            data = np.array([Timestamp(\""2011/01/01\"")] * N, dtype=dtype)\n        else:\n            data = np.array([1] * N, dtype=dtype)\n        self.df = DataFrame({\""values\"": data, \""key\"": [\""foo\""] * N})""}",groupby.RankWithTies,time,0.0
24640,class-level,"terminus-2,oracle",1.0568560442684949,1.0568560442684949,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.MultiIndexing,indexing,"{""indexing.MultiIndexing.time_loc_all_null_slices"": ""class MultiIndexing:\n    def time_loc_all_null_slices(self, unique_levels):\n        target = tuple([self.tgt_null_slice] * self.nlevels)\n        self.df.loc[target, :]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MultiIndexing:\n    def setup(self, unique_levels):\n        self.nlevels = 2\n        if unique_levels:\n            mi = MultiIndex.from_arrays([range(1000000)] * self.nlevels)\n        else:\n            mi = MultiIndex.from_product([range(1000)] * self.nlevels)\n        self.df = DataFrame(np.random.randn(len(mi)), index=mi)\n    \n        self.tgt_slice = slice(200, 800)\n        self.tgt_null_slice = slice(None)\n        self.tgt_list = list(range(0, 1000, 10))\n        self.tgt_scalar = 500\n    \n        bool_indexer = np.zeros(len(mi), dtype=np.bool_)\n        bool_indexer[slice(0, len(mi), 100)] = True\n        self.tgt_bool_indexer = bool_indexer"", ""indexing.MultiIndexing.time_loc_all_scalars"": ""class MultiIndexing:\n    def time_loc_all_scalars(self, unique_levels):\n        target = tuple([self.tgt_scalar] * self.nlevels)\n        self.df.loc[target, :]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MultiIndexing:\n    def setup(self, unique_levels):\n        self.nlevels = 2\n        if unique_levels:\n            mi = MultiIndex.from_arrays([range(1000000)] * self.nlevels)\n        else:\n            mi = MultiIndex.from_product([range(1000)] * self.nlevels)\n        self.df = DataFrame(np.random.randn(len(mi)), index=mi)\n    \n        self.tgt_slice = slice(200, 800)\n        self.tgt_null_slice = slice(None)\n        self.tgt_list = list(range(0, 1000, 10))\n        self.tgt_scalar = 500\n    \n        bool_indexer = np.zeros(len(mi), dtype=np.bool_)\n        bool_indexer[slice(0, len(mi), 100)] = True\n        self.tgt_bool_indexer = bool_indexer""}",indexing.MultiIndexing,time,0.0
24636,class-level,"terminus-2,oracle",0.9839495425121074,0.9839495425121074,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.GetItemSingleColumn,indexing,"{""indexing.GetItemSingleColumn.time_frame_getitem_single_column_int"": ""class GetItemSingleColumn:\n    def time_frame_getitem_single_column_int(self):\n        self.df_int_col[0]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GetItemSingleColumn:\n    def setup(self):\n        self.df_string_col = DataFrame(np.random.randn(3000, 1), columns=[\""A\""])\n        self.df_int_col = DataFrame(np.random.randn(3000, 1))""}",indexing.GetItemSingleColumn,time,0.0
24642,class-level,"terminus-2,oracle",1.2314549995620576,1.2314549995620576,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericMaskedIndexing,indexing,"{""indexing.NumericMaskedIndexing.time_get_indexer"": ""class NumericMaskedIndexing:\n    def time_get_indexer(self, dtype, monotonic):\n        self.data.get_indexer(self.indexer)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericMaskedIndexing:\n    def setup(self, dtype, monotonic):\n        indices = {\n            True: Index(self.monotonic_list, dtype=dtype),\n            False: Index(self.non_monotonic_list, dtype=dtype).append(\n                Index([NA], dtype=dtype)\n            ),\n        }\n        self.data = indices[monotonic]\n        self.indexer = np.arange(300, 1_000)\n        self.data_dups = self.data.append(self.data)"", ""indexing.NumericMaskedIndexing.time_get_indexer_dups"": ""class NumericMaskedIndexing:\n    def time_get_indexer_dups(self, dtype, monotonic):\n        self.data.get_indexer_for(self.indexer)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericMaskedIndexing:\n    def setup(self, dtype, monotonic):\n        indices = {\n            True: Index(self.monotonic_list, dtype=dtype),\n            False: Index(self.non_monotonic_list, dtype=dtype).append(\n                Index([NA], dtype=dtype)\n            ),\n        }\n        self.data = indices[monotonic]\n        self.indexer = np.arange(300, 1_000)\n        self.data_dups = self.data.append(self.data)""}",indexing.NumericMaskedIndexing,time,0.0
24639,class-level,"terminus-2,oracle",0.9748419732721316,0.9748419732721316,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.MethodLookup,indexing,"{""indexing.MethodLookup.time_lookup_iloc"": ""class MethodLookup:\n    def time_lookup_iloc(self, s):\n        s.iloc\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MethodLookup:\n    def setup_cache(self):\n        s = Series()\n        return s"", ""indexing.MethodLookup.time_lookup_loc"": ""class MethodLookup:\n    def time_lookup_loc(self, s):\n        s.loc\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MethodLookup:\n    def setup_cache(self):\n        s = Series()\n        return s""}",indexing.MethodLookup,time,0.0
24641,class-level,"terminus-2,oracle",1.033147036806328,1.033147036806328,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NonNumericSeriesIndexing,indexing,"{""indexing.NonNumericSeriesIndexing.time_getitem_label_slice"": ""class NonNumericSeriesIndexing:\n    def time_getitem_label_slice(self, index, index_structure):\n        self.s[: self.lbl]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]"", ""indexing.NonNumericSeriesIndexing.time_getitem_list_like"": ""class NonNumericSeriesIndexing:\n    def time_getitem_list_like(self, index, index_structure):\n        self.s[[self.lbl]]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]"", ""indexing.NonNumericSeriesIndexing.time_getitem_scalar"": ""class NonNumericSeriesIndexing:\n    def time_getitem_scalar(self, index, index_structure):\n        self.s[self.lbl]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]""}",indexing.NonNumericSeriesIndexing,time,0.0
24649,class-level,"terminus-2,oracle",1.091054454873935,1.091054454873935,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.csv.ReadCSVCategorical,io.csv,"{""io.csv.ReadCSVCategorical.time_convert_direct"": ""class ReadCSVCategorical:\n    def time_convert_direct(self, engine):\n        read_csv(self.fname, engine=engine, dtype=\""category\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVCategorical:\n    def setup(self, engine):\n        N = 100000\n        group1 = [\""aaaaaaaa\"", \""bbbbbbb\"", \""cccccccc\"", \""dddddddd\"", \""eeeeeeee\""]\n        df = DataFrame(np.random.choice(group1, (N, 3)), columns=list(\""abc\""))\n        df.to_csv(self.fname, index=False)"", ""io.csv.ReadCSVCategorical.time_convert_post"": ""class ReadCSVCategorical:\n    def time_convert_post(self, engine):\n        read_csv(self.fname, engine=engine).apply(Categorical)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVCategorical:\n    def setup(self, engine):\n        N = 100000\n        group1 = [\""aaaaaaaa\"", \""bbbbbbb\"", \""cccccccc\"", \""dddddddd\"", \""eeeeeeee\""]\n        df = DataFrame(np.random.choice(group1, (N, 3)), columns=list(\""abc\""))\n        df.to_csv(self.fname, index=False)""}",io.csv.ReadCSVCategorical,time,0.0
24646,class-level,"terminus-2,oracle",0.9926262937725512,0.9926262937725512,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing_engines.NumericEngineIndexing,indexing_engines,"{""indexing_engines.NumericEngineIndexing.time_get_loc"": ""class NumericEngineIndexing:\n    def time_get_loc(self, engine_and_dtype, index_type, unique, N):\n        self.data.get_loc(self.key_early)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)[::-1]\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.empty(N * 3, dtype=dtype)\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype)\n                arr[N:] = np.arange(N * 2, dtype=dtype)\n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype)\n    \n        self.data = engine(arr)\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]"", ""indexing_engines.NumericEngineIndexing.time_get_loc_near_middle"": ""class NumericEngineIndexing:\n    def time_get_loc_near_middle(self, engine_and_dtype, index_type, unique, N):\n        # searchsorted performance may be different near the middle of a range\n        #  vs near an endpoint\n        self.data.get_loc(self.key_middle)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)[::-1]\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.empty(N * 3, dtype=dtype)\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype)\n                arr[N:] = np.arange(N * 2, dtype=dtype)\n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype)\n    \n        self.data = engine(arr)\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]""}",indexing_engines.NumericEngineIndexing,time,0.0
24633,class-level,"terminus-2,oracle",0.9864520167007408,0.9864520167007408,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.CategoricalIndexIndexing,indexing,"{""indexing.CategoricalIndexIndexing.time_getitem_scalar"": ""class CategoricalIndexIndexing:\n    def time_getitem_scalar(self, index):\n        self.data[self.int_scalar]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalIndexIndexing:\n    def setup(self, index):\n        N = 10**5\n        values = list(\""a\"" * N + \""b\"" * N + \""c\"" * N)\n        indices = {\n            \""monotonic_incr\"": CategoricalIndex(values),\n            \""monotonic_decr\"": CategoricalIndex(reversed(values)),\n            \""non_monotonic\"": CategoricalIndex(list(\""abc\"" * N)),\n        }\n        self.data = indices[index]\n        self.data_unique = CategoricalIndex([str(i) for i in range(N * 3)])\n    \n        self.int_scalar = 10000\n        self.int_list = list(range(10000))\n    \n        self.cat_scalar = \""b\""\n        self.cat_list = [\""1\"", \""3\""]""}",indexing.CategoricalIndexIndexing,time,0.0
24643,class-level,"terminus-2,oracle",1.1629266854856486,1.1629266854856486,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.NumericSeriesIndexing,indexing,"{""indexing.NumericSeriesIndexing.time_getitem_array"": ""class NumericSeriesIndexing:\n    def time_getitem_array(self, index, index_structure):\n        self.data[self.array]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_getitem_list_like"": ""class NumericSeriesIndexing:\n    def time_getitem_list_like(self, index, index_structure):\n        self.data[[800000]]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_getitem_lists"": ""class NumericSeriesIndexing:\n    def time_getitem_lists(self, index, index_structure):\n        self.data[self.array_list]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_getitem_scalar"": ""class NumericSeriesIndexing:\n    def time_getitem_scalar(self, index, index_structure):\n        self.data[800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_getitem_slice"": ""class NumericSeriesIndexing:\n    def time_getitem_slice(self, index, index_structure):\n        self.data[:800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_iloc_slice"": ""class NumericSeriesIndexing:\n    def time_iloc_slice(self, index, index_structure):\n        self.data.iloc[:800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_loc_array"": ""class NumericSeriesIndexing:\n    def time_loc_array(self, index, index_structure):\n        self.data.loc[self.array]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_loc_scalar"": ""class NumericSeriesIndexing:\n    def time_loc_scalar(self, index, index_structure):\n        self.data.loc[800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_loc_slice"": ""class NumericSeriesIndexing:\n    def time_loc_slice(self, index, index_structure):\n        self.data.loc[:800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing.NumericSeriesIndexing,time,0.0
24645,class-level,"terminus-2,oracle",1.036883949799185,1.036883949799185,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing_engines.MaskedNumericEngineIndexing,indexing_engines,"{""indexing_engines.MaskedNumericEngineIndexing.time_get_loc"": ""class MaskedNumericEngineIndexing:\n    def time_get_loc(self, engine_and_dtype, index_type, unique, N):\n        self.data.get_loc(self.key_early)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())[::-1]\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.zeros(N * 3, dtype=dtype.lower())\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype.lower())\n                arr[N:] = np.arange(N * 2, dtype=dtype.lower())\n    \n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n            mask[-1] = True\n    \n        self.data = engine(BaseMaskedArray(arr, mask))\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]"", ""indexing_engines.MaskedNumericEngineIndexing.time_get_loc_near_middle"": ""class MaskedNumericEngineIndexing:\n    def time_get_loc_near_middle(self, engine_and_dtype, index_type, unique, N):\n        # searchsorted performance may be different near the middle of a range\n        #  vs near an endpoint\n        self.data.get_loc(self.key_middle)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())[::-1]\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.zeros(N * 3, dtype=dtype.lower())\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype.lower())\n                arr[N:] = np.arange(N * 2, dtype=dtype.lower())\n    \n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n            mask[-1] = True\n    \n        self.data = engine(BaseMaskedArray(arr, mask))\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]""}",indexing_engines.MaskedNumericEngineIndexing,time,0.0
24638,class-level,"terminus-2,oracle",1.3004972820780516,1.3004972820780516,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.IntervalIndexing,indexing,"{""indexing.IntervalIndexing.time_loc_scalar"": ""class IntervalIndexing:\n    def time_loc_scalar(self, monotonic):\n        monotonic.loc[80000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IntervalIndexing:\n    def setup_cache(self):\n        idx = IntervalIndex.from_breaks(np.arange(1000001))\n        monotonic = Series(np.arange(1000000), index=idx)\n        return monotonic""}",indexing.IntervalIndexing,time,0.0
24651,class-level,"terminus-2,oracle",1.0231803528110297,1.0231803528110297,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.csv.ToCSVDatetimeIndex,io.csv,"{""io.csv.ToCSVDatetimeIndex.time_frame_date_formatting_index"": ""class ToCSVDatetimeIndex:\n    def time_frame_date_formatting_index(self):\n        self.data.to_csv(self.fname, date_format=\""%Y-%m-%d %H:%M:%S\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToCSVDatetimeIndex:\n    def setup(self):\n        rng = date_range(\""2000\"", periods=100_000, freq=\""S\"")\n        self.data = DataFrame({\""a\"": 1}, index=rng)""}",io.csv.ToCSVDatetimeIndex,time,0.0
24644,class-level,"terminus-2,oracle",1.0429647275222034,1.0429647275222034,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing.Take,indexing,"{""indexing.Take.time_take"": ""class Take:\n    def time_take(self, index):\n        self.s.take(self.indexer)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Take:\n    def setup(self, index):\n        N = 100000\n        indexes = {\n            \""int\"": Index(np.arange(N), dtype=np.int64),\n            \""datetime\"": date_range(\""2011-01-01\"", freq=\""S\"", periods=N),\n        }\n        index = indexes[index]\n        self.s = Series(np.random.rand(N), index=index)\n        self.indexer = np.random.randint(0, N, size=N)""}",indexing.Take,time,0.0
24630,class-level,"terminus-2,oracle",1.0570132078439995,1.0570132078439995,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_cached_properties.IndexCache,index_cached_properties,"{""index_cached_properties.IndexCache.time_engine"": ""class IndexCache:\n    def time_engine(self, index_type):\n        self.idx._engine\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}"", ""index_cached_properties.IndexCache.time_is_monotonic_decreasing"": ""class IndexCache:\n    def time_is_monotonic_decreasing(self, index_type):\n        self.idx.is_monotonic_decreasing\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}"", ""index_cached_properties.IndexCache.time_is_monotonic_increasing"": ""class IndexCache:\n    def time_is_monotonic_increasing(self, index_type):\n        self.idx.is_monotonic_increasing\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}""}",index_cached_properties.IndexCache,time,0.0
24650,class-level,"terminus-2,oracle",1.044008692136258,1.044008692136258,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.csv.ReadCSVEngine,io.csv,"{""io.csv.ReadCSVEngine.time_read_bytescsv"": ""class ReadCSVEngine:\n    def time_read_bytescsv(self, engine):\n        read_csv(self.data(self.BytesIO_input), engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVEngine:\n    def setup(self, engine):\n        data = [\""A,B,C,D,E\""] + ([\""1,2,3,4,5\""] * 100000)\n        self.StringIO_input = StringIO(\""\\n\"".join(data))\n        # simulate reading from file\n        self.BytesIO_input = BytesIO(self.StringIO_input.read().encode(\""utf-8\""))"", ""io.csv.ReadCSVEngine.time_read_stringcsv"": ""class ReadCSVEngine:\n    def time_read_stringcsv(self, engine):\n        read_csv(self.data(self.StringIO_input), engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVEngine:\n    def setup(self, engine):\n        data = [\""A,B,C,D,E\""] + ([\""1,2,3,4,5\""] * 100000)\n        self.StringIO_input = StringIO(\""\\n\"".join(data))\n        # simulate reading from file\n        self.BytesIO_input = BytesIO(self.StringIO_input.read().encode(\""utf-8\""))""}",io.csv.ReadCSVEngine,time,0.0
24653,class-level,"terminus-2,oracle",1.1630325669190456,1.1630325669190456,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.hdf.HDFStoreDataFrame,io.hdf,"{""io.hdf.HDFStoreDataFrame.time_query_store_table"": ""class HDFStoreDataFrame:\n    def time_query_store_table(self):\n        self.store.select(\""table\"", where=\""index > self.start and index < self.stop\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass HDFStoreDataFrame:\n    def setup(self):\n        N = 25000\n        index = tm.makeStringIndex(N)\n        self.df = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)}, index=index\n        )\n        self.df_mixed = DataFrame(\n            {\n                \""float1\"": np.random.randn(N),\n                \""float2\"": np.random.randn(N),\n                \""string1\"": [\""foo\""] * N,\n                \""bool1\"": [True] * N,\n                \""int1\"": np.random.randint(0, N, size=N),\n            },\n            index=index,\n        )\n        self.df_wide = DataFrame(np.random.randn(N, 100))\n        self.start_wide = self.df_wide.index[10000]\n        self.stop_wide = self.df_wide.index[15000]\n        self.df2 = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)},\n            index=date_range(\""1/1/2000\"", periods=N),\n        )\n        self.start = self.df2.index[10000]\n        self.stop = self.df2.index[15000]\n        self.df_wide2 = DataFrame(\n            np.random.randn(N, 100), index=date_range(\""1/1/2000\"", periods=N)\n        )\n        self.df_dc = DataFrame(\n            np.random.randn(N, 10), columns=[f\""C{i:03d}\"" for i in range(10)]\n        )\n    \n        self.fname = \""__test__.h5\""\n    \n        self.store = HDFStore(self.fname)\n        self.store.put(\""fixed\"", self.df)\n        self.store.put(\""fixed_mixed\"", self.df_mixed)\n        self.store.append(\""table\"", self.df2)\n        self.store.append(\""table_mixed\"", self.df_mixed)\n        self.store.append(\""table_wide\"", self.df_wide)\n        self.store.append(\""table_wide2\"", self.df_wide2)""}",io.hdf.HDFStoreDataFrame,time,0.0
24648,class-level,"terminus-2,oracle",1.024725834815643,1.024725834815643,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,inference.ToNumericDowncast,inference,"{""inference.ToNumericDowncast.time_downcast"": ""class ToNumericDowncast:\n    def time_downcast(self, dtype, downcast):\n        to_numeric(self.data, downcast=downcast)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToNumericDowncast:\n    def setup(self, dtype, downcast):\n        self.data = self.data_dict[dtype]""}",inference.ToNumericDowncast,time,0.0
24652,class-level,"terminus-2,oracle",1.027590940682821,1.027590940682821,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.excel.ReadExcel,io.excel,"{""io.excel.ReadExcel.time_read_excel"": ""class ReadExcel:\n    def time_read_excel(self, engine):\n        if engine == \""odf\"":\n            fname = self.fname_odf\n        else:\n            fname = self.fname_excel\n        read_excel(fname, engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadExcel:\n    def setup_cache(self):\n        self.df = _generate_dataframe()\n    \n        self.df.to_excel(self.fname_excel, sheet_name=\""Sheet1\"")\n        self._create_odf()""}",io.excel.ReadExcel,time,0.0
24657,class-level,"terminus-2,oracle",1.0388470107889347,1.0388470107889347,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.json.ToJSONWide,io.json,"{""io.json.ToJSONWide.time_to_json"": ""class ToJSON:\n    def time_to_json(self, orient, frame):\n        getattr(self, frame).to_json(self.fname, orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSONWide:\n    def setup(self, orient, frame):\n        super().setup(orient, frame)\n        base_df = getattr(self, frame).copy()\n        df_wide = concat([base_df.iloc[:100]] * 1000, ignore_index=True, axis=1)\n        self.df_wide = df_wide"", ""io.json.ToJSONWide.time_to_json_wide"": ""class ToJSONWide:\n    def time_to_json_wide(self, orient, frame):\n        self.df_wide.to_json(self.fname, orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSONWide:\n    def setup(self, orient, frame):\n        super().setup(orient, frame)\n        base_df = getattr(self, frame).copy()\n        df_wide = concat([base_df.iloc[:100]] * 1000, ignore_index=True, axis=1)\n        self.df_wide = df_wide""}",io.json.ToJSONWide,time,0.0
24647,class-level,"terminus-2,oracle",0.9586934385698772,0.9586934385698772,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,inference.ToDatetimeCache,inference,"{""inference.ToDatetimeCache.time_dup_string_dates_and_format"": ""class ToDatetimeCache:\n    def time_dup_string_dates_and_format(self, cache):\n        to_datetime(self.dup_string_dates, format=\""%Y-%m-%d\"", cache=cache)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToDatetimeCache:\n    def setup(self, cache):\n        N = 10000\n        self.unique_numeric_seconds = list(range(N))\n        self.dup_numeric_seconds = [1000] * N\n        self.dup_string_dates = [\""2000-02-11\""] * N\n        self.dup_string_with_tz = [\""2000-02-11 15:00:00-0800\""] * N""}",inference.ToDatetimeCache,time,0.0
24659,class-level,"terminus-2,oracle",0.9588987691396428,0.9588987691396428,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.parsers.DoesStringLookLikeDatetime,io.parsers,"{""io.parsers.DoesStringLookLikeDatetime.time_check_datetimes"": ""class DoesStringLookLikeDatetime:\n    def time_check_datetimes(self, value):\n        for obj in self.objects:\n            _does_string_look_like_datetime(obj)\n\n    def setup(self, value):\n        self.objects = [value] * 1000000""}",io.parsers.DoesStringLookLikeDatetime,time,0.0
24654,class-level,"terminus-2,oracle",1.0502907382323543,1.0502907382323543,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.json.NormalizeJSON,io.json,"{""io.json.NormalizeJSON.time_normalize_json"": ""class NormalizeJSON:\n    def time_normalize_json(self, orient, frame):\n        json_normalize(self.data)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NormalizeJSON:\n    def setup(self, orient, frame):\n        data = {\n            \""hello\"": [\""thisisatest\"", 999898, \""mixed types\""],\n            \""nest1\"": {\""nest2\"": {\""nest3\"": \""nest3_value\"", \""nest3_int\"": 3445}},\n            \""nest1_list\"": {\""nest2\"": [\""blah\"", 32423, 546456.876, 92030234]},\n            \""hello2\"": \""string\"",\n        }\n        self.data = [data for i in range(10000)]""}",io.json.NormalizeJSON,time,0.0
24666,class-level,"terminus-2,oracle",1.0361836437598764,1.0361836437598764,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,libs.InferDtype,libs,"{""libs.InferDtype.time_infer_dtype"": ""class InferDtype:\n    def time_infer_dtype(self, dtype):\n        infer_dtype(self.data_dict[dtype], skipna=False)""}",libs.InferDtype,time,0.0
24662,class-level,"terminus-2,oracle",0.6705412141744088,0.6705412141744088,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,join_merge.Align,join_merge,"{""join_merge.Align.time_series_align_left_monotonic"": ""class Align:\n    def time_series_align_left_monotonic(self):\n        self.ts1.align(self.ts2, join=\""left\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Align:\n    def setup(self):\n        size = 5 * 10**5\n        rng = np.arange(0, 10**13, 10**7)\n        stamps = np.datetime64(\""now\"").view(\""i8\"") + rng\n        idx1 = np.sort(np.random.choice(stamps, size, replace=False))\n        idx2 = np.sort(np.random.choice(stamps, size, replace=False))\n        self.ts1 = Series(np.random.randn(size), idx1)\n        self.ts2 = Series(np.random.randn(size), idx2)""}",join_merge.Align,time,0.0
24658,class-level,"terminus-2,oracle",1.0156846993935509,1.0156846993935509,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.parsers.ConcatDateCols,io.parsers,"{""io.parsers.ConcatDateCols.time_check_concat"": ""class ConcatDateCols:\n    def time_check_concat(self, value, dim):\n        concat_date_cols(self.object)\n\n    def setup(self, value, dim):\n        count_elem = 10000\n        if dim == 1:\n            self.object = (np.array([value] * count_elem),)\n        if dim == 2:\n            self.object = (\n                np.array([value] * count_elem),\n                np.array([value] * count_elem),\n            )""}",io.parsers.ConcatDateCols,time,0.0
24663,class-level,"terminus-2,oracle",1.0272913623753237,1.0272913623753237,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,join_merge.Concat,join_merge,"{""join_merge.Concat.time_concat_empty_left"": ""class Concat:\n    def time_concat_empty_left(self, axis):\n        concat(self.empty_left, axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Concat:\n    def setup(self, axis):\n        N = 1000\n        s = Series(N, index=tm.makeStringIndex(N))\n        self.series = [s[i:-i] for i in range(1, 10)] * 50\n        self.small_frames = [DataFrame(np.random.randn(5, 4))] * 1000\n        df = DataFrame(\n            {\""A\"": range(N)}, index=date_range(\""20130101\"", periods=N, freq=\""s\"")\n        )\n        self.empty_left = [DataFrame(), df]\n        self.empty_right = [df, DataFrame()]\n        self.mixed_ndims = [df, df.head(N // 2)]""}",join_merge.Concat,time,0.0
24665,class-level,"terminus-2,oracle",1.0362373142798862,1.0362373142798862,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,join_merge.MergeAsof,join_merge,"{""join_merge.MergeAsof.time_by_object"": ""class MergeAsof:\n    def time_by_object(self, direction, tolerance):\n        merge_asof(\n            self.df1b,\n            self.df2b,\n            on=\""time\"",\n            by=\""key\"",\n            direction=direction,\n            tolerance=tolerance,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MergeAsof:\n    def setup(self, direction, tolerance):\n        one_count = 200000\n        two_count = 1000000\n    \n        df1 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, one_count / 20, one_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), one_count),\n                \""key2\"": np.random.randint(0, 25, one_count),\n                \""value1\"": np.random.randn(one_count),\n            }\n        )\n        df2 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, two_count / 20, two_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), two_count),\n                \""key2\"": np.random.randint(0, 25, two_count),\n                \""value2\"": np.random.randn(two_count),\n            }\n        )\n    \n        df1 = df1.sort_values(\""time\"")\n        df2 = df2.sort_values(\""time\"")\n    \n        df1[\""time32\""] = np.int32(df1.time)\n        df2[\""time32\""] = np.int32(df2.time)\n    \n        df1[\""timeu64\""] = np.uint64(df1.time)\n        df2[\""timeu64\""] = np.uint64(df2.time)\n    \n        self.df1a = df1[[\""time\"", \""value1\""]]\n        self.df2a = df2[[\""time\"", \""value2\""]]\n        self.df1b = df1[[\""time\"", \""key\"", \""value1\""]]\n        self.df2b = df2[[\""time\"", \""key\"", \""value2\""]]\n        self.df1c = df1[[\""time\"", \""key2\"", \""value1\""]]\n        self.df2c = df2[[\""time\"", \""key2\"", \""value2\""]]\n        self.df1d = df1[[\""time32\"", \""value1\""]]\n        self.df2d = df2[[\""time32\"", \""value2\""]]\n        self.df1e = df1[[\""time\"", \""key\"", \""key2\"", \""value1\""]]\n        self.df2e = df2[[\""time\"", \""key\"", \""key2\"", \""value2\""]]\n        self.df1f = df1[[\""timeu64\"", \""value1\""]]\n        self.df2f = df2[[\""timeu64\"", \""value2\""]]"", ""join_merge.MergeAsof.time_on_int32"": ""class MergeAsof:\n    def time_on_int32(self, direction, tolerance):\n        merge_asof(\n            self.df1d, self.df2d, on=\""time32\"", direction=direction, tolerance=tolerance\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MergeAsof:\n    def setup(self, direction, tolerance):\n        one_count = 200000\n        two_count = 1000000\n    \n        df1 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, one_count / 20, one_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), one_count),\n                \""key2\"": np.random.randint(0, 25, one_count),\n                \""value1\"": np.random.randn(one_count),\n            }\n        )\n        df2 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, two_count / 20, two_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), two_count),\n                \""key2\"": np.random.randint(0, 25, two_count),\n                \""value2\"": np.random.randn(two_count),\n            }\n        )\n    \n        df1 = df1.sort_values(\""time\"")\n        df2 = df2.sort_values(\""time\"")\n    \n        df1[\""time32\""] = np.int32(df1.time)\n        df2[\""time32\""] = np.int32(df2.time)\n    \n        df1[\""timeu64\""] = np.uint64(df1.time)\n        df2[\""timeu64\""] = np.uint64(df2.time)\n    \n        self.df1a = df1[[\""time\"", \""value1\""]]\n        self.df2a = df2[[\""time\"", \""value2\""]]\n        self.df1b = df1[[\""time\"", \""key\"", \""value1\""]]\n        self.df2b = df2[[\""time\"", \""key\"", \""value2\""]]\n        self.df1c = df1[[\""time\"", \""key2\"", \""value1\""]]\n        self.df2c = df2[[\""time\"", \""key2\"", \""value2\""]]\n        self.df1d = df1[[\""time32\"", \""value1\""]]\n        self.df2d = df2[[\""time32\"", \""value2\""]]\n        self.df1e = df1[[\""time\"", \""key\"", \""key2\"", \""value1\""]]\n        self.df2e = df2[[\""time\"", \""key\"", \""key2\"", \""value2\""]]\n        self.df1f = df1[[\""timeu64\"", \""value1\""]]\n        self.df2f = df2[[\""timeu64\"", \""value2\""]]""}",join_merge.MergeAsof,time,0.0
24656,class-level,"terminus-2,oracle",1.0151594714925496,1.0151594714925496,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.json.ToJSONLines,io.json,"{""io.json.ToJSONLines.time_delta_int_tstamp_lines"": ""class ToJSONLines:\n    def time_delta_int_tstamp_lines(self):\n        self.df_td_int_ts.to_json(self.fname, orient=\""records\"", lines=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSONLines:\n    def setup(self):\n        N = 10**5\n        ncols = 5\n        index = date_range(\""20000101\"", periods=N, freq=\""H\"")\n        timedeltas = timedelta_range(start=1, periods=N, freq=\""s\"")\n        datetimes = date_range(start=1, periods=N, freq=\""s\"")\n        ints = np.random.randint(100000000, size=N)\n        longints = sys.maxsize * np.random.randint(100000000, size=N)\n        floats = np.random.randn(N)\n        strings = tm.makeStringIndex(N)\n        self.df = DataFrame(np.random.randn(N, ncols), index=np.arange(N))\n        self.df_date_idx = DataFrame(np.random.randn(N, ncols), index=index)\n        self.df_td_int_ts = DataFrame(\n            {\n                \""td_1\"": timedeltas,\n                \""td_2\"": timedeltas,\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""ts_1\"": datetimes,\n                \""ts_2\"": datetimes,\n            },\n            index=index,\n        )\n        self.df_int_floats = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""int_3\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""float_3\"": floats,\n            },\n            index=index,\n        )\n        self.df_int_float_str = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )\n        self.df_longint_float_str = DataFrame(\n            {\n                \""longint_1\"": longints,\n                \""longint_2\"": longints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )""}",io.json.ToJSONLines,time,0.0
24655,class-level,"terminus-2,oracle",1.019150027730217,1.019150027730217,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.json.ToJSON,io.json,"{""io.json.ToJSON.time_to_json"": ""class ToJSON:\n    def time_to_json(self, orient, frame):\n        getattr(self, frame).to_json(self.fname, orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSON:\n    def setup(self, orient, frame):\n        N = 10**5\n        ncols = 5\n        index = date_range(\""20000101\"", periods=N, freq=\""H\"")\n        timedeltas = timedelta_range(start=1, periods=N, freq=\""s\"")\n        datetimes = date_range(start=1, periods=N, freq=\""s\"")\n        ints = np.random.randint(100000000, size=N)\n        longints = sys.maxsize * np.random.randint(100000000, size=N)\n        floats = np.random.randn(N)\n        strings = tm.makeStringIndex(N)\n        self.df = DataFrame(np.random.randn(N, ncols), index=np.arange(N))\n        self.df_date_idx = DataFrame(np.random.randn(N, ncols), index=index)\n        self.df_td_int_ts = DataFrame(\n            {\n                \""td_1\"": timedeltas,\n                \""td_2\"": timedeltas,\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""ts_1\"": datetimes,\n                \""ts_2\"": datetimes,\n            },\n            index=index,\n        )\n        self.df_int_floats = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""int_3\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""float_3\"": floats,\n            },\n            index=index,\n        )\n        self.df_int_float_str = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )\n    \n        self.df_longint_float_str = DataFrame(\n            {\n                \""longint_1\"": longints,\n                \""longint_2\"": longints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )""}",io.json.ToJSON,time,0.0
24668,class-level,"terminus-2,oracle",1.2854618192567615,1.2854618192567615,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,multiindex_object.GetLoc,multiindex_object,"{""multiindex_object.GetLoc.time_large_get_loc_warm"": ""class GetLoc:\n    def time_large_get_loc_warm(self):\n        for _ in range(1000):\n            self.mi_large.get_loc((999, 19, \""Z\""))\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GetLoc:\n    def setup(self):\n        self.mi_large = MultiIndex.from_product(\n            [np.arange(1000), np.arange(20), list(string.ascii_letters)],\n            names=[\""one\"", \""two\"", \""three\""],\n        )\n        self.mi_med = MultiIndex.from_product(\n            [np.arange(1000), np.arange(10), list(\""A\"")], names=[\""one\"", \""two\"", \""three\""]\n        )\n        self.mi_small = MultiIndex.from_product(\n            [np.arange(100), list(\""A\""), list(\""A\"")], names=[\""one\"", \""two\"", \""three\""]\n        )"", ""multiindex_object.GetLoc.time_med_get_loc"": ""class GetLoc:\n    def time_med_get_loc(self):\n        self.mi_med.get_loc((999, 9, \""A\""))\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GetLoc:\n    def setup(self):\n        self.mi_large = MultiIndex.from_product(\n            [np.arange(1000), np.arange(20), list(string.ascii_letters)],\n            names=[\""one\"", \""two\"", \""three\""],\n        )\n        self.mi_med = MultiIndex.from_product(\n            [np.arange(1000), np.arange(10), list(\""A\"")], names=[\""one\"", \""two\"", \""three\""]\n        )\n        self.mi_small = MultiIndex.from_product(\n            [np.arange(100), list(\""A\""), list(\""A\"")], names=[\""one\"", \""two\"", \""three\""]\n        )""}",multiindex_object.GetLoc,time,0.0
24660,class-level,"terminus-2,oracle",1.0472118667839048,1.0472118667839048,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.sql.WriteSQLDtypes,io.sql,"{""io.sql.WriteSQLDtypes.time_read_sql_query_select_column"": ""class WriteSQLDtypes:\n    def time_read_sql_query_select_column(self, connection, dtype):\n        read_sql_query(self.query_col, self.con)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass WriteSQLDtypes:\n    def setup(self, connection, dtype):\n        N = 10000\n        con = {\n            \""sqlalchemy\"": create_engine(\""sqlite:///:memory:\""),\n            \""sqlite\"": sqlite3.connect(\"":memory:\""),\n        }\n        self.table_name = \""test_type\""\n        self.query_col = f\""SELECT {dtype} FROM {self.table_name}\""\n        self.con = con[connection]\n        self.df = DataFrame(\n            {\n                \""float\"": np.random.randn(N),\n                \""float_with_nan\"": np.random.randn(N),\n                \""string\"": [\""foo\""] * N,\n                \""bool\"": [True] * N,\n                \""int\"": np.random.randint(0, N, size=N),\n                \""datetime\"": date_range(\""2000-01-01\"", periods=N, freq=\""s\""),\n            },\n            index=tm.makeStringIndex(N),\n        )\n        self.df.iloc[1000:3000, 1] = np.nan\n        self.df[\""date\""] = self.df[\""datetime\""].dt.date\n        self.df[\""time\""] = self.df[\""datetime\""].dt.time\n        self.df[\""datetime_string\""] = self.df[\""datetime\""].astype(str)\n        self.df.to_sql(self.table_name, self.con, if_exists=\""replace\"")""}",io.sql.WriteSQLDtypes,time,0.0
24677,class-level,"terminus-2,oracle",1.0285263308926698,1.0285263308926698,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.Explode,reshape,"{""reshape.Explode.time_explode"": ""class Explode:\n    def time_explode(self, n_rows, max_list_length):\n        self.series.explode()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Explode:\n    def setup(self, n_rows, max_list_length):\n        data = [np.arange(np.random.randint(max_list_length)) for _ in range(n_rows)]\n        self.series = pd.Series(data)""}",reshape.Explode,time,0.0
24664,class-level,"terminus-2,oracle",0.9154205285523832,0.9154205285523832,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,join_merge.ConcatIndexDtype,join_merge,"{""join_merge.ConcatIndexDtype.time_concat_series"": ""class ConcatIndexDtype:\n    def time_concat_series(self, dtype, structure, axis, sort):\n        concat(self.series, axis=axis, sort=sort)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ConcatIndexDtype:\n    def setup(self, dtype, structure, axis, sort):\n        N = 10_000\n        if dtype == \""datetime64[ns]\"":\n            vals = date_range(\""1970-01-01\"", periods=N)\n        elif dtype in (\""int64\"", \""Int64\""):\n            vals = np.arange(N, dtype=np.int64)\n        elif dtype in (\""string[python]\"", \""string[pyarrow]\""):\n            vals = tm.makeStringIndex(N)\n        else:\n            raise NotImplementedError\n    \n        idx = Index(vals, dtype=dtype)\n    \n        if structure == \""monotonic\"":\n            idx = idx.sort_values()\n        elif structure == \""non_monotonic\"":\n            idx = idx[::-1]\n        elif structure == \""has_na\"":\n            if not idx._can_hold_na:\n                raise NotImplementedError\n            idx = Index([None], dtype=dtype).append(idx)\n        else:\n            raise NotImplementedError\n    \n        self.series = [Series(i, idx[:-i]) for i in range(1, 6)]""}",join_merge.ConcatIndexDtype,time,0.0
24669,class-level,"terminus-2,oracle",0.9662950819021658,0.9662950819021658,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,multiindex_object.Integer,multiindex_object,"{""multiindex_object.Integer.time_is_monotonic"": ""class Integer:\n    def time_is_monotonic(self):\n        self.mi_int.is_monotonic_increasing\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Integer:\n    def setup(self):\n        self.mi_int = MultiIndex.from_product(\n            [np.arange(1000), np.arange(1000)], names=[\""one\"", \""two\""]\n        )\n        self.obj_index = np.array(\n            [\n                (0, 10),\n                (0, 11),\n                (0, 12),\n                (0, 13),\n                (0, 14),\n                (0, 15),\n                (0, 16),\n                (0, 17),\n                (0, 18),\n                (0, 19),\n            ],\n            dtype=object,\n        )\n        self.other_mi_many_mismatches = MultiIndex.from_tuples(\n            [\n                (-7, 41),\n                (-2, 3),\n                (-0.7, 5),\n                (0, 0),\n                (0, 1.5),\n                (0, 340),\n                (0, 1001),\n                (1, -4),\n                (1, 20),\n                (1, 1040),\n                (432, -5),\n                (432, 17),\n                (439, 165.5),\n                (998, -4),\n                (998, 24065),\n                (999, 865.2),\n                (999, 1000),\n                (1045, -843),\n            ]\n        )""}",multiindex_object.Integer,time,0.0
24675,class-level,"terminus-2,oracle",0.9481537929203344,0.9481537929203344,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,replace.ReplaceList,replace,"{""replace.ReplaceList.time_replace_list"": ""class ReplaceList:\n    def time_replace_list(self, inplace):\n        self.df.replace([np.inf, -np.inf], np.nan, inplace=inplace)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReplaceList:\n    def setup(self, inplace):\n        self.df = pd.DataFrame({\""A\"": 0, \""B\"": 0}, index=range(4 * 10**7))""}",replace.ReplaceList,time,0.0
24667,class-level,"terminus-2,oracle",1.0241087926012522,1.0241087926012522,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,libs.ScalarListLike,libs,"{""libs.ScalarListLike.time_is_list_like"": ""class ScalarListLike:\n    def time_is_list_like(self, param):\n        is_list_like(param)"", ""libs.ScalarListLike.time_is_scalar"": ""class ScalarListLike:\n    def time_is_scalar(self, param):\n        is_scalar(param)""}",libs.ScalarListLike,time,0.0
24661,class-level,"terminus-2,oracle",0.97232287374828,0.97232287374828,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.style.Render,io.style,"{""io.style.Render.time_format_render"": ""class Render:\n    def time_format_render(self, cols, rows):\n        self._style_format()\n        self.st._render_html(True, True)\n\n    def setup(self, cols, rows):\n        self.df = DataFrame(\n            np.random.randn(rows, cols),\n            columns=[f\""float_{i+1}\"" for i in range(cols)],\n            index=[f\""row_{i+1}\"" for i in range(rows)],\n        )""}",io.style.Render,time,0.0
24673,class-level,"terminus-2,oracle",0.97254858342969,0.97254858342969,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,plotting.SeriesPlotting,plotting,"{""plotting.SeriesPlotting.time_series_plot"": ""class SeriesPlotting:\n    def time_series_plot(self, kind):\n        self.s.plot(kind=kind)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesPlotting:\n    def setup(self, kind):\n        if kind in [\""bar\"", \""barh\"", \""pie\""]:\n            n = 100\n        elif kind in [\""kde\""]:\n            n = 10000\n        else:\n            n = 1000000\n    \n        self.s = Series(np.random.randn(n))\n        if kind in [\""area\"", \""pie\""]:\n            self.s = self.s.abs()""}",plotting.SeriesPlotting,time,0.0
24678,class-level,"terminus-2,oracle",1.1113814191673432,1.1113814191673432,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.Melt,reshape,"{""reshape.Melt.time_melt_dataframe"": ""class Melt:\n    def time_melt_dataframe(self, dtype):\n        melt(self.df, id_vars=[\""id1\"", \""id2\""])\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Melt:\n    def setup(self, dtype):\n        self.df = DataFrame(\n            np.random.randn(100_000, 3), columns=[\""A\"", \""B\"", \""C\""], dtype=dtype\n        )\n        self.df[\""id1\""] = pd.Series(np.random.randint(0, 10, 10000))\n        self.df[\""id2\""] = pd.Series(np.random.randint(100, 1000, 10000))""}",reshape.Melt,time,0.0
24671,class-level,"terminus-2,oracle",1.036520085499017,1.036520085499017,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,period.Indexing,period,"{""period.Indexing.time_align"": ""class Indexing:\n    def time_align(self):\n        DataFrame({\""a\"": self.series, \""b\"": self.series[:500]})\n\n    def setup(self):\n        self.index = period_range(start=\""1985\"", periods=1000, freq=\""D\"")\n        self.series = Series(range(1000), index=self.index)\n        self.period = self.index[500]"", ""period.Indexing.time_intersection"": ""class Indexing:\n    def time_intersection(self):\n        self.index[:750].intersection(self.index[250:])\n\n    def setup(self):\n        self.index = period_range(start=\""1985\"", periods=1000, freq=\""D\"")\n        self.series = Series(range(1000), index=self.index)\n        self.period = self.index[500]"", ""period.Indexing.time_series_loc"": ""class Indexing:\n    def time_series_loc(self):\n        self.series.loc[self.period]\n\n    def setup(self):\n        self.index = period_range(start=\""1985\"", periods=1000, freq=\""D\"")\n        self.series = Series(range(1000), index=self.index)\n        self.period = self.index[500]""}",period.Indexing,time,0.0
24679,class-level,"terminus-2,oracle",1.127530469102413,1.127530469102413,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.PivotTable,reshape,"{""reshape.PivotTable.time_pivot_table_categorical"": ""class PivotTable:\n    def time_pivot_table_categorical(self):\n        self.df2.pivot_table(\n            index=\""col1\"", values=\""col3\"", columns=\""col2\"", aggfunc=np.sum, fill_value=0\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")"", ""reshape.PivotTable.time_pivot_table_categorical_observed"": ""class PivotTable:\n    def time_pivot_table_categorical_observed(self):\n        self.df2.pivot_table(\n            index=\""col1\"",\n            values=\""col3\"",\n            columns=\""col2\"",\n            aggfunc=np.sum,\n            fill_value=0,\n            observed=True,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")"", ""reshape.PivotTable.time_pivot_table_margins"": ""class PivotTable:\n    def time_pivot_table_margins(self):\n        self.df.pivot_table(index=\""key1\"", columns=[\""key2\"", \""key3\""], margins=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")"", ""reshape.PivotTable.time_pivot_table_margins_only_column"": ""class PivotTable:\n    def time_pivot_table_margins_only_column(self):\n        self.df.pivot_table(columns=[\""key1\"", \""key2\"", \""key3\""], margins=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")""}",reshape.PivotTable,time,0.0
24670,class-level,"terminus-2,oracle",0.9111576927899844,0.9111576927899844,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,multiindex_object.SetOperations,multiindex_object,"{""multiindex_object.SetOperations.time_operation"": ""class SetOperations:\n    def time_operation(self, index_structure, dtype, method, sort):\n        getattr(self.left, method)(self.right, sort=sort)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SetOperations:\n    def setup(self, index_structure, dtype, method, sort):\n        N = 10**5\n        level1 = range(1000)\n    \n        level2 = date_range(start=\""1/1/2000\"", periods=N // 1000)\n        dates_left = MultiIndex.from_product([level1, level2])\n    \n        level2 = range(N // 1000)\n        int_left = MultiIndex.from_product([level1, level2])\n    \n        level2 = tm.makeStringIndex(N // 1000).values\n        str_left = MultiIndex.from_product([level1, level2])\n    \n        level2 = range(N // 1000)\n        ea_int_left = MultiIndex.from_product([level1, Series(level2, dtype=\""Int64\"")])\n    \n        data = {\n            \""datetime\"": dates_left,\n            \""int\"": int_left,\n            \""string\"": str_left,\n            \""ea_int\"": ea_int_left,\n        }\n    \n        if index_structure == \""non_monotonic\"":\n            data = {k: mi[::-1] for k, mi in data.items()}\n    \n        data = {k: {\""left\"": mi, \""right\"": mi[:-1]} for k, mi in data.items()}\n        self.left = data[dtype][\""left\""]\n        self.right = data[dtype][\""right\""]""}",multiindex_object.SetOperations,time,0.0
24672,class-level,"terminus-2,oracle",1.0396601642827117,1.0396601642827117,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,period.PeriodIndexConstructor,period,"{""period.PeriodIndexConstructor.time_from_date_range"": ""class PeriodIndexConstructor:\n    def time_from_date_range(self, freq, is_offset):\n        PeriodIndex(self.rng, freq=freq)\n\n    def setup(self, freq, is_offset):\n        self.rng = date_range(\""1985\"", periods=1000)\n        self.rng2 = date_range(\""1985\"", periods=1000).to_pydatetime()\n        self.ints = list(range(2000, 3000))\n        self.daily_ints = (\n            date_range(\""1/1/2000\"", periods=1000, freq=freq).strftime(\""%Y%m%d\"").map(int)\n        )\n        if is_offset:\n            self.freq = to_offset(freq)\n        else:\n            self.freq = freq""}",period.PeriodIndexConstructor,time,0.0
24680,class-level,"terminus-2,oracle",1.4175765820288715,1.4175765820288715,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.ReshapeExtensionDtype,reshape,"{""reshape.ReshapeExtensionDtype.time_stack"": ""class ReshapeExtensionDtype:\n    def time_stack(self, dtype):\n        self.df.stack()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReshapeExtensionDtype:\n    def setup(self, dtype):\n        lev = pd.Index(list(\""ABCDEFGHIJ\""))\n        ri = pd.Index(range(1000))\n        mi = MultiIndex.from_product([lev, ri], names=[\""foo\"", \""bar\""])\n    \n        index = date_range(\""2016-01-01\"", periods=10000, freq=\""s\"", tz=\""US/Pacific\"")\n        if dtype == \""Period[s]\"":\n            index = index.tz_localize(None).to_period(\""s\"")\n    \n        ser = pd.Series(index, index=mi)\n        df = ser.unstack(\""bar\"")\n        # roundtrips -> df.stack().equals(ser)\n    \n        self.ser = ser\n        self.df = df""}",reshape.ReshapeExtensionDtype,time,0.0
24681,class-level,"terminus-2,oracle",1.0345564193262995,1.0345564193262995,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.WideToLong,reshape,"{""reshape.WideToLong.time_wide_to_long_big"": ""class WideToLong:\n    def time_wide_to_long_big(self):\n        wide_to_long(self.df, self.letters, i=\""id\"", j=\""year\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass WideToLong:\n    def setup(self):\n        nyrs = 20\n        nidvars = 20\n        N = 5000\n        self.letters = list(\""ABCD\"")\n        yrvars = [\n            letter + str(num)\n            for letter, num in product(self.letters, range(1, nyrs + 1))\n        ]\n        columns = [str(i) for i in range(nidvars)] + yrvars\n        self.df = DataFrame(np.random.randn(N, nidvars + len(yrvars)), columns=columns)\n        self.df[\""id\""] = self.df.index""}",reshape.WideToLong,time,0.0
24676,class-level,"terminus-2,oracle",1.0635353869264876,1.0635353869264876,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape.Cut,reshape,"{""reshape.Cut.time_cut_datetime"": ""class Cut:\n    def time_cut_datetime(self, bins):\n        pd.cut(self.datetime_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Cut.time_cut_interval"": ""class Cut:\n    def time_cut_interval(self, bins):\n        # GH 27668\n        pd.cut(self.int_series, self.interval_bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Cut.time_cut_timedelta"": ""class Cut:\n    def time_cut_timedelta(self, bins):\n        pd.cut(self.timedelta_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Cut.time_qcut_datetime"": ""class Cut:\n    def time_qcut_datetime(self, bins):\n        pd.qcut(self.datetime_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Cut.time_qcut_float"": ""class Cut:\n    def time_qcut_float(self, bins):\n        pd.qcut(self.float_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))""}",reshape.Cut,time,0.0
24682,class-level,"terminus-2,oracle",1.0542741512301976,1.0542741512301976,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Apply,rolling,"{""rolling.Apply.time_rolling"": ""class Apply:\n    def time_rolling(self, constructor, window, dtype, function, raw):\n        self.roll.apply(function, raw=raw)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, constructor, window, dtype, function, raw):\n        N = 10**3\n        arr = (100 * np.random.random(N)).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)""}",rolling.Apply,time,0.0
24692,class-level,"terminus-2,oracle",1.024073566008088,1.024073566008088,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Rank,rolling,"{""rolling.Rank.time_rank"": ""class Rank:\n    def time_rank(self, constructor, window, dtype, percentile, ascending, method):\n        self.roll.rank(pct=percentile, ascending=ascending, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Rank:\n    def setup(self, constructor, window, dtype, percentile, ascending, method):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)""}",rolling.Rank,time,0.0
24683,class-level,"terminus-2,oracle",1.0570098215679355,1.0570098215679355,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.EWMMethods,rolling,"{""rolling.EWMMethods.time_ewm"": ""class EWMMethods:\n    def time_ewm(self, constructor, kwargs_method, dtype):\n        getattr(self.ewm, self.method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass EWMMethods:\n    def setup(self, constructor, kwargs_method, dtype):\n        N = 10**5\n        kwargs, method = kwargs_method\n        arr = (100 * np.random.random(N)).astype(dtype)\n        self.method = method\n        self.ewm = getattr(pd, constructor)(arr).ewm(**kwargs)""}",rolling.EWMMethods,time,0.0
24674,class-level,"terminus-2,oracle",0.9804572743060842,0.9804572743060842,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reindex.Fillna,reindex,"{""reindex.Fillna.time_reindexed"": ""class Fillna:\n    def time_reindexed(self, method):\n        self.ts_reindexed.fillna(method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Fillna:\n    def setup(self, method):\n        N = 100000\n        self.idx = date_range(\""1/1/2000\"", periods=N, freq=\""1min\"")\n        ts = Series(np.random.randn(N), index=self.idx)[::2]\n        self.ts_reindexed = ts.reindex(self.idx)\n        self.ts_float32 = self.ts_reindexed.astype(\""float32\"")""}",reindex.Fillna,time,0.0
24684,class-level,"terminus-2,oracle",1.0448612492390277,1.0448612492390277,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.ForwardWindowMethods,rolling,"{""rolling.ForwardWindowMethods.time_rolling"": ""class ForwardWindowMethods:\n    def time_rolling(self, constructor, window_size, dtype, method):\n        getattr(self.roll, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ForwardWindowMethods:\n    def setup(self, constructor, window_size, dtype, method):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=window_size)\n        self.roll = getattr(pd, constructor)(arr).rolling(window=indexer)""}",rolling.ForwardWindowMethods,time,0.0
24694,class-level,"terminus-2,oracle",1.046121270484629,1.046121270484629,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.VariableWindowMethods,rolling,"{""rolling.VariableWindowMethods.time_method"": ""class Methods:\n    def time_method(self, constructor, window_kwargs, dtype, method):\n        getattr(self.window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass VariableWindowMethods:\n    def setup(self, constructor, window, dtype, method):\n        N = 10**5\n        arr = (100 * np.random.random(N)).astype(dtype)\n        index = pd.date_range(\""2017-01-01\"", periods=N, freq=\""5s\"")\n        self.window = getattr(pd, constructor)(arr, index=index).rolling(window)""}",rolling.VariableWindowMethods,time,0.0
24688,class-level,"terminus-2,oracle",1.0835962245212107,1.0835962245212107,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.GroupbyLargeGroups,rolling,"{""rolling.GroupbyLargeGroups.time_rolling_multiindex_creation"": ""class GroupbyLargeGroups:\n    def time_rolling_multiindex_creation(self):\n        self.df.groupby(\""A\"").rolling(3).mean()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupbyLargeGroups:\n    def setup(self):\n        N = 100000\n        self.df = pd.DataFrame({\""A\"": [1, 2] * (N // 2), \""B\"": np.random.randn(N)})""}",rolling.GroupbyLargeGroups,time,0.0
24689,class-level,"terminus-2,oracle",1.0461441520445638,1.0461441520445638,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Methods,rolling,"{""rolling.Methods.time_method"": ""class Methods:\n    def time_method(self, constructor, window_kwargs, dtype, method):\n        getattr(self.window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Methods:\n    def setup(self, constructor, window_kwargs, dtype, method):\n        N = 10**5\n        window, kwargs = window_kwargs\n        arr = (100 * np.random.random(N)).astype(dtype)\n        obj = getattr(pd, constructor)(arr)\n        self.window = getattr(obj, window)(**kwargs)""}",rolling.Methods,time,0.0
24698,class-level,"terminus-2,oracle",1.0549105958071132,1.0549105958071132,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.SearchSorted,series_methods,"{""series_methods.SearchSorted.time_searchsorted"": ""class SearchSorted:\n    def time_searchsorted(self, dtype):\n        key = \""2\"" if dtype == \""str\"" else 2\n        self.s.searchsorted(key)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SearchSorted:\n    def setup(self, dtype):\n        N = 10**5\n        data = np.array([1] * N + [2] * N + [3] * N).astype(dtype)\n        self.s = Series(data)""}",series_methods.SearchSorted,time,0.0
24690,class-level,"terminus-2,oracle",1.0560469467073563,1.0560469467073563,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Pairwise,rolling,"{""rolling.Pairwise.time_groupby"": ""class Pairwise:\n    def time_groupby(self, kwargs_window, method, pairwise):\n        getattr(self.window_group, method)(self.df, pairwise=pairwise)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Pairwise:\n    def setup(self, kwargs_window, method, pairwise):\n        N = 10**4\n        n_groups = 20\n        kwargs, window = kwargs_window\n        groups = [i for _ in range(N // n_groups) for i in range(n_groups)]\n        arr = np.random.random(N)\n        self.df = pd.DataFrame(arr)\n        self.window = getattr(self.df, window)(**kwargs)\n        self.window_group = getattr(\n            pd.DataFrame({\""A\"": groups, \""B\"": arr}).groupby(\""A\""), window\n        )(**kwargs)"", ""rolling.Pairwise.time_pairwise"": ""class Pairwise:\n    def time_pairwise(self, kwargs_window, method, pairwise):\n        getattr(self.window, method)(self.df, pairwise=pairwise)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Pairwise:\n    def setup(self, kwargs_window, method, pairwise):\n        N = 10**4\n        n_groups = 20\n        kwargs, window = kwargs_window\n        groups = [i for _ in range(N // n_groups) for i in range(n_groups)]\n        arr = np.random.random(N)\n        self.df = pd.DataFrame(arr)\n        self.window = getattr(self.df, window)(**kwargs)\n        self.window_group = getattr(\n            pd.DataFrame({\""A\"": groups, \""B\"": arr}).groupby(\""A\""), window\n        )(**kwargs)""}",rolling.Pairwise,time,0.0
24685,class-level,"terminus-2,oracle",1.0888663070669695,1.0888663070669695,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Groupby,rolling,"{""rolling.Groupby.time_method"": ""class Groupby:\n    def time_method(self, method, window_kwargs):\n        getattr(self.groupby_window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Groupby:\n    def setup(self, method, window_kwargs):\n        N = 1000\n        window, kwargs = window_kwargs\n        df = pd.DataFrame(\n            {\n                \""A\"": [str(i) for i in range(N)] * 10,\n                \""B\"": list(range(N)) * 10,\n            }\n        )\n        if isinstance(kwargs.get(\""window\"", None), str):\n            df.index = pd.date_range(start=\""1900-01-01\"", freq=\""1min\"", periods=N * 10)\n        self.groupby_window = getattr(df.groupby(\""A\""), window)(**kwargs)""}",rolling.Groupby,time,0.0
24696,class-level,"terminus-2,oracle",1.1976886276828178,1.1976886276828178,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.Fillna,series_methods,"{""series_methods.Fillna.time_fillna"": ""class Fillna:\n    def time_fillna(self, dtype, method):\n        value = self.fill_value if method is None else None\n        self.ser.fillna(value=value, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Fillna:\n    def setup(self, dtype, method):\n        N = 10**6\n        if dtype == \""datetime64[ns]\"":\n            data = date_range(\""2000-01-01\"", freq=\""S\"", periods=N)\n            na_value = NaT\n        elif dtype in (\""float64\"", \""Float64\""):\n            data = np.random.randn(N)\n            na_value = np.nan\n        elif dtype in (\""Int64\"", \""int64[pyarrow]\""):\n            data = np.arange(N)\n            na_value = NA\n        elif dtype in (\""string\"", \""string[pyarrow]\""):\n            data = tm.rands_array(5, N)\n            na_value = NA\n        else:\n            raise NotImplementedError\n        fill_value = data[0]\n        ser = Series(data, dtype=dtype)\n        ser[::2] = na_value\n        self.ser = ser\n        self.fill_value = fill_value""}",series_methods.Fillna,time,0.0
24693,class-level,"terminus-2,oracle",1.060642811266571,1.060642811266571,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.TableMethod,rolling,"{""rolling.TableMethod.time_apply"": ""class TableMethod:\n    def time_apply(self, method):\n        self.df.rolling(2, method=method).apply(\n            table_method_func, raw=True, engine=\""numba\""\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TableMethod:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(10, 1000))""}",rolling.TableMethod,time,0.0
24703,class-level,"terminus-2,oracle",1.0153968750429954,1.0153968750429954,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.Take,sparse,"{""sparse.Take.time_take"": ""class Take:\n    def time_take(self, indices, allow_fill):\n        self.sp_arr.take(indices, allow_fill=allow_fill)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Take:\n    def setup(self, indices, allow_fill):\n        N = 1_000_000\n        fill_value = 0.0\n        arr = make_array(N, 1e-5, fill_value, np.float64)\n        self.sp_arr = SparseArray(arr, fill_value=fill_value)""}",sparse.Take,time,0.0
24695,class-level,"terminus-2,oracle",1.0243815660476452,1.0243815660476452,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.Any,series_methods,"{""series_methods.Any.time_any"": ""class Any:\n    def time_any(self, N, case, dtype):\n        self.s.any()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Any:\n    def setup(self, N, case, dtype):\n        val = case == \""fast\""\n        self.s = Series([val] * N, dtype=dtype)""}",series_methods.Any,time,0.0
24699,class-level,"terminus-2,oracle",1.0118678655169218,1.0118678655169218,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.SeriesConstructor,series_methods,"{""series_methods.SeriesConstructor.time_constructor_dict"": ""class SeriesConstructor:\n    def time_constructor_dict(self):\n        Series(data=self.data, index=self.idx)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesConstructor:\n    def setup(self):\n        self.idx = date_range(\n            start=datetime(2015, 10, 26), end=datetime(2016, 1, 1), freq=\""50s\""\n        )\n        self.data = dict(zip(self.idx, range(len(self.idx))))\n        self.array = np.array([1, 2, 3])\n        self.idx2 = Index([\""a\"", \""b\"", \""c\""])""}",series_methods.SeriesConstructor,time,0.0
24686,class-level,"terminus-2,oracle",1.1082312294361656,1.1082312294361656,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.GroupbyEWM,rolling,"{""rolling.GroupbyEWM.time_groupby_method"": ""class GroupbyEWM:\n    def time_groupby_method(self, method):\n        getattr(self.gb_ewm, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupbyEWM:\n    def setup(self, method):\n        df = pd.DataFrame({\""A\"": range(50), \""B\"": range(50)})\n        self.gb_ewm = df.groupby(\""A\"").ewm(com=1.0)""}",rolling.GroupbyEWM,time,0.0
24687,class-level,"terminus-2,oracle",1.0461862350524451,1.0461862350524451,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.GroupbyEWMEngine,rolling,"{""rolling.GroupbyEWMEngine.time_groupby_mean"": ""class GroupbyEWMEngine:\n    def time_groupby_mean(self, engine):\n        self.gb_ewm.mean(engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupbyEWMEngine:\n    def setup(self, engine):\n        df = pd.DataFrame({\""A\"": range(50), \""B\"": range(50)})\n        self.gb_ewm = df.groupby(\""A\"").ewm(com=1.0)""}",rolling.GroupbyEWMEngine,time,0.0
24701,class-level,"terminus-2,oracle",0.9150649934375992,0.9150649934375992,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.Arithmetic,sparse,"{""sparse.Arithmetic.time_add"": ""class Arithmetic:\n    def time_add(self, dense_proportion, fill_value):\n        self.array1 + self.array2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)"", ""sparse.Arithmetic.time_divide"": ""class Arithmetic:\n    def time_divide(self, dense_proportion, fill_value):\n        self.array1 / self.array2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)"", ""sparse.Arithmetic.time_make_union"": ""class Arithmetic:\n    def time_make_union(self, dense_proportion, fill_value):\n        self.array1.sp_index.make_union(self.array2.sp_index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)""}",sparse.Arithmetic,time,0.0
24691,class-level,"terminus-2,oracle",1.0647197439195957,1.0647197439195957,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling.Quantile,rolling,"{""rolling.Quantile.time_quantile"": ""class Quantile:\n    def time_quantile(self, constructor, window, dtype, percentile, interpolation):\n        self.roll.quantile(percentile, interpolation=interpolation)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, constructor, window, dtype, percentile, interpolation):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)""}",rolling.Quantile,time,0.0
24704,class-level,"terminus-2,oracle",1.0281049793776291,1.0281049793776291,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,stat_ops.Correlation,stat_ops,"{""stat_ops.Correlation.time_corr"": ""class Correlation:\n    def time_corr(self, method):\n        self.df.corr(method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Correlation:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(500, 15))\n        self.df2 = pd.DataFrame(np.random.randn(500, 15))\n        self.df_wide = pd.DataFrame(np.random.randn(500, 100))\n        self.df_wide_nans = self.df_wide.where(np.random.random((500, 100)) < 0.9)\n        self.s = pd.Series(np.random.randn(500))\n        self.s2 = pd.Series(np.random.randn(500))"", ""stat_ops.Correlation.time_corr_series"": ""class Correlation:\n    def time_corr_series(self, method):\n        self.s.corr(self.s2, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Correlation:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(500, 15))\n        self.df2 = pd.DataFrame(np.random.randn(500, 15))\n        self.df_wide = pd.DataFrame(np.random.randn(500, 100))\n        self.df_wide_nans = self.df_wide.where(np.random.random((500, 100)) < 0.9)\n        self.s = pd.Series(np.random.randn(500))\n        self.s2 = pd.Series(np.random.randn(500))"", ""stat_ops.Correlation.time_corr_wide_nans"": ""class Correlation:\n    def time_corr_wide_nans(self, method):\n        self.df_wide_nans.corr(method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Correlation:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(500, 15))\n        self.df2 = pd.DataFrame(np.random.randn(500, 15))\n        self.df_wide = pd.DataFrame(np.random.randn(500, 100))\n        self.df_wide_nans = self.df_wide.where(np.random.random((500, 100)) < 0.9)\n        self.s = pd.Series(np.random.randn(500))\n        self.s2 = pd.Series(np.random.randn(500))"", ""stat_ops.Correlation.time_corrwith_cols"": ""class Correlation:\n    def time_corrwith_cols(self, method):\n        self.df.corrwith(self.df2, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Correlation:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(500, 15))\n        self.df2 = pd.DataFrame(np.random.randn(500, 15))\n        self.df_wide = pd.DataFrame(np.random.randn(500, 100))\n        self.df_wide_nans = self.df_wide.where(np.random.random((500, 100)) < 0.9)\n        self.s = pd.Series(np.random.randn(500))\n        self.s2 = pd.Series(np.random.randn(500))"", ""stat_ops.Correlation.time_corrwith_rows"": ""class Correlation:\n    def time_corrwith_rows(self, method):\n        self.df.corrwith(self.df2, axis=1, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Correlation:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(500, 15))\n        self.df2 = pd.DataFrame(np.random.randn(500, 15))\n        self.df_wide = pd.DataFrame(np.random.randn(500, 100))\n        self.df_wide_nans = self.df_wide.where(np.random.random((500, 100)) < 0.9)\n        self.s = pd.Series(np.random.randn(500))\n        self.s2 = pd.Series(np.random.randn(500))""}",stat_ops.Correlation,time,0.0
24697,class-level,"terminus-2,oracle",1.0573150212261662,1.0573150212261662,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.NanOps,series_methods,"{""series_methods.NanOps.time_func"": ""class NanOps:\n    def time_func(self, func, N, dtype):\n        self.func()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NanOps:\n    def setup(self, func, N, dtype):\n        if func == \""argmax\"" and dtype in {\""Int64\"", \""boolean\""}:\n            # Skip argmax for nullable int since this doesn't work yet (GH-24382)\n            raise NotImplementedError\n        self.s = Series([1] * N, dtype=dtype)\n        self.func = getattr(self.s, func)""}",series_methods.NanOps,time,0.0
24700,class-level,"terminus-2,oracle",0.9192684582296644,0.9192684582296644,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods.ToFrame,series_methods,"{""series_methods.ToFrame.time_to_frame"": ""class ToFrame:\n    def time_to_frame(self, dtype, name):\n        self.ser.to_frame(name)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToFrame:\n    def setup(self, dtype, name):\n        arr = np.arange(10**5)\n        ser = Series(arr, dtype=dtype)\n        self.ser = ser""}",series_methods.ToFrame,time,0.0
24711,class-level,"terminus-2,oracle",1.018177527973979,1.018177527973979,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Dummies,strings,"{""strings.Dummies.time_get_dummies"": ""class Dummies:\n    def time_get_dummies(self, dtype):\n        self.s.str.get_dummies(\""|\"")\n\n    def setup(self, dtype):\n        super().setup(dtype)\n        self.s = self.s.str.join(\""|\"")""}",strings.Dummies,time,0.0
24705,class-level,"terminus-2,oracle",1.0475539470866344,1.0475539470866344,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,stat_ops.FrameMultiIndexOps,stat_ops,"{""stat_ops.FrameMultiIndexOps.time_op"": ""class FrameMultiIndexOps:\n    def time_op(self, op):\n        self.df_func()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FrameMultiIndexOps:\n    def setup(self, op):\n        levels = [np.arange(10), np.arange(100), np.arange(100)]\n        codes = [\n            np.arange(10).repeat(10000),\n            np.tile(np.arange(100).repeat(100), 10),\n            np.tile(np.tile(np.arange(100), 100), 10),\n        ]\n        index = pd.MultiIndex(levels=levels, codes=codes)\n        df = pd.DataFrame(np.random.randn(len(index), 4), index=index)\n        self.df_func = getattr(df, op)""}",stat_ops.FrameMultiIndexOps,time,0.0
24702,class-level,"terminus-2,oracle",0.904440968977416,0.904440968977416,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse.ArithmeticBlock,sparse,"{""sparse.ArithmeticBlock.time_addition"": ""class ArithmeticBlock:\n    def time_addition(self, fill_value):\n        self.arr1 + self.arr2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )"", ""sparse.ArithmeticBlock.time_division"": ""class ArithmeticBlock:\n    def time_division(self, fill_value):\n        self.arr1 / self.arr2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )"", ""sparse.ArithmeticBlock.time_make_union"": ""class ArithmeticBlock:\n    def time_make_union(self, fill_value):\n        self.arr1.sp_index.make_union(self.arr2.sp_index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )""}",sparse.ArithmeticBlock,time,0.0
24710,class-level,"terminus-2,oracle",1.0221869382018522,1.0221869382018522,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Contains,strings,"{""strings.Contains.time_contains"": ""class Contains:\n    def time_contains(self, dtype, regex):\n        self.s.str.contains(\""A\"", regex=regex)\n\n    def setup(self, dtype, regex):\n        super().setup(dtype)""}",strings.Contains,time,0.0
24707,class-level,"terminus-2,oracle",1.0235986125470364,1.0235986125470364,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,stat_ops.SeriesOps,stat_ops,"{""stat_ops.SeriesOps.time_op"": ""class SeriesOps:\n    def time_op(self, op, dtype):\n        self.s_func()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesOps:\n    def setup(self, op, dtype):\n        s = pd.Series(np.random.randn(100000)).astype(dtype)\n        self.s_func = getattr(s, op)""}",stat_ops.SeriesOps,time,0.0
24712,class-level,"terminus-2,oracle",1.0736758717032655,1.0736758717032655,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings.Methods,strings,"{""strings.Methods.time_endswith"": ""class Methods:\n    def time_endswith(self, dtype):\n        self.s.str.endswith(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_findall"": ""class Methods:\n    def time_findall(self, dtype):\n        self.s.str.findall(\""[A-Z]+\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_fullmatch"": ""class Methods:\n    def time_fullmatch(self, dtype):\n        self.s.str.fullmatch(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_get"": ""class Methods:\n    def time_get(self, dtype):\n        self.s.str.get(0)\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_isdigit"": ""class Methods:\n    def time_isdigit(self, dtype):\n        self.s.str.isdigit()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_istitle"": ""class Methods:\n    def time_istitle(self, dtype):\n        self.s.str.istitle()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_lower"": ""class Methods:\n    def time_lower(self, dtype):\n        self.s.str.lower()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_match"": ""class Methods:\n    def time_match(self, dtype):\n        self.s.str.match(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_upper"": ""class Methods:\n    def time_upper(self, dtype):\n        self.s.str.upper()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_wrap"": ""class Methods:\n    def time_wrap(self, dtype):\n        self.s.str.wrap(10)\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings.Methods,time,0.0
24706,class-level,"terminus-2,oracle",1.0357025052036626,1.0357025052036626,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,stat_ops.FrameOps,stat_ops,"{""stat_ops.FrameOps.time_op"": ""class FrameOps:\n    def time_op(self, op, dtype, axis):\n        self.df_func(axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FrameOps:\n    def setup(self, op, dtype, axis):\n        values = np.random.randn(100000, 4)\n        if dtype == \""Int64\"":\n            values = values.astype(int)\n        df = pd.DataFrame(values).astype(dtype)\n        self.df_func = getattr(df, op)""}",stat_ops.FrameOps,time,0.0
24717,class-level,"terminus-2,oracle",1.0555519096179355,1.0555519096179355,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.ResetIndex,timeseries,"{""timeseries.ResetIndex.time_reset_datetimeindex"": ""class ResetIndex:\n    def time_reset_datetimeindex(self, tz):\n        self.df.reset_index()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ResetIndex:\n    def setup(self, tz):\n        idx = date_range(start=\""1/1/2000\"", periods=1000, freq=\""H\"", tz=tz)\n        self.df = DataFrame(np.random.randn(1000, 2), index=idx)""}",timeseries.ResetIndex,time,0.0
24715,class-level,"terminus-2,oracle",0.9467504414755472,0.9467504414755472,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.DatetimeAccessor,timeseries,"{""timeseries.DatetimeAccessor.time_dt_accessor_day_name"": ""class DatetimeAccessor:\n    def time_dt_accessor_day_name(self, tz):\n        self.series.dt.day_name()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeAccessor:\n    def setup(self, tz):\n        N = 100000\n        self.series = Series(date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"", tz=tz))"", ""timeseries.DatetimeAccessor.time_dt_accessor_month_name"": ""class DatetimeAccessor:\n    def time_dt_accessor_month_name(self, tz):\n        self.series.dt.month_name()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeAccessor:\n    def setup(self, tz):\n        N = 100000\n        self.series = Series(date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"", tz=tz))""}",timeseries.DatetimeAccessor,time,0.0
24709,class-level,"terminus-2,oracle",1.0107349830512775,1.0107349830512775,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strftime.DatetimeStrftime,strftime,"{""strftime.DatetimeStrftime.time_frame_datetime_to_str"": ""class DatetimeStrftime:\n    def time_frame_datetime_to_str(self, obs):\n        self.data[\""dt\""].astype(str)\n\n    def setup(self, obs):\n        d = \""2018-11-29\""\n        dt = \""2018-11-26 11:18:27.0\""\n        self.data = pd.DataFrame(\n            {\n                \""dt\"": [np.datetime64(dt)] * obs,\n                \""d\"": [np.datetime64(d)] * obs,\n                \""r\"": [np.random.uniform()] * obs,\n            }\n        )""}",strftime.DatetimeStrftime,time,0.0
24720,class-level,"terminus-2,oracle",1.026500169451337,1.026500169451337,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.fields.TimeGetStartEndField,tslibs.fields,"{""tslibs.fields.TimeGetStartEndField.time_get_start_end_field"": ""class TimeGetStartEndField:\n    def time_get_start_end_field(self, size, side, period, freqstr, month_kw):\n        get_start_end_field(self.i8data, self.attrname, freqstr, month_kw=month_kw)\n\n    def setup(self, size, side, period, freqstr, month_kw):\n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr\n    \n        self.attrname = f\""is_{period}_{side}\""""}",tslibs.fields.TimeGetStartEndField,time,0.0
24714,class-level,"terminus-2,oracle",1.0595535554422533,1.0595535554422533,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timedelta.TimedeltaIndexing,timedelta,"{""timedelta.TimedeltaIndexing.time_intersection"": ""class TimedeltaIndexing:\n    def time_intersection(self):\n        self.index.intersection(self.index2)\n\n    def setup(self):\n        self.index = timedelta_range(start=\""1985\"", periods=1000, freq=\""D\"")\n        self.index2 = timedelta_range(start=\""1986\"", periods=1000, freq=\""D\"")\n        self.series = Series(range(1000), index=self.index)\n        self.timedelta = self.index[500]"", ""timedelta.TimedeltaIndexing.time_series_loc"": ""class TimedeltaIndexing:\n    def time_series_loc(self):\n        self.series.loc[self.timedelta]\n\n    def setup(self):\n        self.index = timedelta_range(start=\""1985\"", periods=1000, freq=\""D\"")\n        self.index2 = timedelta_range(start=\""1986\"", periods=1000, freq=\""D\"")\n        self.series = Series(range(1000), index=self.index)\n        self.timedelta = self.index[500]""}",timedelta.TimedeltaIndexing,time,0.0
24726,class-level,"terminus-2,oracle",1.0137567719325948,1.0137567719325948,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.PeriodProperties,tslibs.period,"{""tslibs.period.PeriodProperties.time_property"": ""class PeriodProperties:\n    def time_property(self, freq, attr):\n        getattr(self.per, attr)\n\n    def setup(self, freq, attr):\n        self.per = Period(\""2012-06-01\"", freq=freq)""}",tslibs.period.PeriodProperties,time,0.0
24728,class-level,"terminus-2,oracle",1.0052824550428674,1.0052824550428674,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.TimeDT64ArrToPeriodArr,tslibs.period,"{""tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr"": ""class TimeDT64ArrToPeriodArr:\n    def time_dt64arr_to_periodarr(self, size, freq, tz):\n        dt64arr_to_periodarr(self.i8values, freq, tz)\n\n    def setup(self, size, freq, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.arange(10, dtype=\""i8\"").repeat(size // 10)\n        self.i8values = arr""}",tslibs.period.TimeDT64ArrToPeriodArr,time,0.0
24708,class-level,"terminus-2,oracle",1.027957242178722,1.027957242178722,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strftime.BusinessHourStrftime,strftime,"{""strftime.BusinessHourStrftime.time_frame_offset_repr"": ""class BusinessHourStrftime:\n    def time_frame_offset_repr(self, obs):\n        self.data[\""off\""].apply(repr)\n\n    def setup(self, obs):\n        self.data = pd.DataFrame(\n            {\n                \""off\"": [offsets.BusinessHour()] * obs,\n            }\n        )"", ""strftime.BusinessHourStrftime.time_frame_offset_str"": ""class BusinessHourStrftime:\n    def time_frame_offset_str(self, obs):\n        self.data[\""off\""].apply(str)\n\n    def setup(self, obs):\n        self.data = pd.DataFrame(\n            {\n                \""off\"": [offsets.BusinessHour()] * obs,\n            }\n        )""}",strftime.BusinessHourStrftime,time,0.0
24719,class-level,"terminus-2,oracle",0.9810336757022337,0.9810336757022337,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.fields.TimeGetDateField,tslibs.fields,"{""tslibs.fields.TimeGetDateField.time_get_date_field"": ""class TimeGetDateField:\n    def time_get_date_field(self, size, field):\n        get_date_field(self.i8data, field)\n\n    def setup(self, size, field):\n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.fields.TimeGetDateField,time,0.0
24721,class-level,"terminus-2,oracle",0.9843482318148696,0.9843482318148696,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.fields.TimeGetTimedeltaField,tslibs.fields,"{""tslibs.fields.TimeGetTimedeltaField.time_get_timedelta_field"": ""class TimeGetTimedeltaField:\n    def time_get_timedelta_field(self, size, field):\n        get_timedelta_field(self.i8data, field)\n\n    def setup(self, size, field):\n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.fields.TimeGetTimedeltaField,time,0.0
24716,class-level,"terminus-2,oracle",0.9872643739945262,0.9872643739945262,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.DatetimeIndex,timeseries,"{""timeseries.DatetimeIndex.time_get"": ""class DatetimeIndex:\n    def time_get(self, index_type):\n        self.index[0]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]"", ""timeseries.DatetimeIndex.time_timeseries_is_month_start"": ""class DatetimeIndex:\n    def time_timeseries_is_month_start(self, index_type):\n        self.index.is_month_start\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]"", ""timeseries.DatetimeIndex.time_to_date"": ""class DatetimeIndex:\n    def time_to_date(self, index_type):\n        self.index.date\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]"", ""timeseries.DatetimeIndex.time_to_time"": ""class DatetimeIndex:\n    def time_to_time(self, index_type):\n        self.index.time\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]""}",timeseries.DatetimeIndex,time,0.0
24718,class-level,"terminus-2,oracle",1.0714083894064304,1.0714083894064304,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries.TzLocalize,timeseries,"{""timeseries.TzLocalize.time_infer_dst"": ""class TzLocalize:\n    def time_infer_dst(self, tz):\n        self.index.tz_localize(tz, ambiguous=\""infer\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TzLocalize:\n    def setup(self, tz):\n        dst_rng = date_range(\n            start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n        )\n        self.index = date_range(start=\""10/29/2000\"", end=\""10/29/2000 00:59:59\"", freq=\""S\"")\n        self.index = self.index.append(dst_rng)\n        self.index = self.index.append(dst_rng)\n        self.index = self.index.append(\n            date_range(start=\""10/29/2000 2:00:00\"", end=\""10/29/2000 3:00:00\"", freq=\""S\"")\n        )""}",timeseries.TzLocalize,time,0.0
24725,class-level,"terminus-2,oracle",1.0892287783290096,1.0892287783290096,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.PeriodConstructor,tslibs.period,"{""tslibs.period.PeriodConstructor.time_period_constructor"": ""class PeriodConstructor:\n    def time_period_constructor(self, freq, is_offset):\n        Period(\""2012-06-01\"", freq=freq)\n\n    def setup(self, freq, is_offset):\n        if is_offset:\n            self.freq = to_offset(freq)\n        else:\n            self.freq = freq""}",tslibs.period.PeriodConstructor,time,0.0
24723,class-level,"terminus-2,oracle",1.047814140927383,1.047814140927383,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.OffestDatetimeArithmetic,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_add"": ""class OffestDatetimeArithmetic:\n    def time_add(self, offset):\n        self.date + offset\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")"", ""tslibs.offsets.OffestDatetimeArithmetic.time_add_10"": ""class OffestDatetimeArithmetic:\n    def time_add_10(self, offset):\n        self.date + (10 * offset)\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")"", ""tslibs.offsets.OffestDatetimeArithmetic.time_add_np_dt64"": ""class OffestDatetimeArithmetic:\n    def time_add_np_dt64(self, offset):\n        offset + self.dt64\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")"", ""tslibs.offsets.OffestDatetimeArithmetic.time_subtract"": ""class OffestDatetimeArithmetic:\n    def time_subtract(self, offset):\n        self.date - offset\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")"", ""tslibs.offsets.OffestDatetimeArithmetic.time_subtract_10"": ""class OffestDatetimeArithmetic:\n    def time_subtract_10(self, offset):\n        self.date - (10 * offset)\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")""}",tslibs.offsets.OffestDatetimeArithmetic,time,0.0
24730,class-level,"terminus-2,oracle",1.0065466865962085,1.0065466865962085,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.resolution.TimeResolution,tslibs.resolution,"{""tslibs.resolution.TimeResolution.time_get_resolution"": ""class TimeResolution:\n    def time_get_resolution(self, unit, size, tz):\n        get_resolution(self.i8data, tz)\n\n    def setup(self, unit, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        arr = arr.view(f\""M8[{unit}]\"").astype(\""M8[ns]\"").view(\""i8\"")\n        self.i8data = arr""}",tslibs.resolution.TimeResolution,time,0.0
24722,class-level,"terminus-2,oracle",0.9708070656940136,0.9708070656940136,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.normalize.Normalize,tslibs.normalize,"{""tslibs.normalize.Normalize.time_is_date_array_normalized"": ""class Normalize:\n    def time_is_date_array_normalized(self, size, tz):\n        # TODO: cases with different levels of short-circuiting\n        # 10 i.e. NPY_FR_ns\n        is_date_array_normalized(self.i8data, tz, 10)\n\n    def setup(self, size, tz):\n        # use an array that will have is_date_array_normalized give True,\n        #  so we do not short-circuit early.\n        dti = pd.date_range(\""2016-01-01\"", periods=10, tz=tz).repeat(size // 10)\n        self.i8data = dti.asi8\n    \n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError"", ""tslibs.normalize.Normalize.time_normalize_i8_timestamps"": ""class Normalize:\n    def time_normalize_i8_timestamps(self, size, tz):\n        # 10 i.e. NPY_FR_ns\n        normalize_i8_timestamps(self.i8data, tz, 10)\n\n    def setup(self, size, tz):\n        # use an array that will have is_date_array_normalized give True,\n        #  so we do not short-circuit early.\n        dti = pd.date_range(\""2016-01-01\"", periods=10, tz=tz).repeat(size // 10)\n        self.i8data = dti.asi8\n    \n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError""}",tslibs.normalize.Normalize,time,0.0
24724,class-level,"terminus-2,oracle",1.001471372421829,1.001471372421829,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets.OnOffset,tslibs.offsets,"{""tslibs.offsets.OnOffset.time_on_offset"": ""class OnOffset:\n    def time_on_offset(self, offset):\n        for date in self.dates:\n            offset.is_on_offset(date)\n\n    def setup(self, offset):\n        self.dates = [\n            datetime(2016, m, d)\n            for m in [10, 11, 12]\n            for d in [1, 2, 3, 28, 29, 30, 31]\n            if not (m == 11 and d == 31)\n        ]""}",tslibs.offsets.OnOffset,time,0.0
24727,class-level,"terminus-2,oracle",1.043156661729329,1.043156661729329,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.PeriodUnaryMethods,tslibs.period,"{""tslibs.period.PeriodUnaryMethods.time_asfreq"": ""class PeriodUnaryMethods:\n    def time_asfreq(self, freq):\n        self.per.asfreq(\""A\"")\n\n    def setup(self, freq):\n        self.per = Period(\""2012-06-01\"", freq=freq)"", ""tslibs.period.PeriodUnaryMethods.time_now"": ""class PeriodUnaryMethods:\n    def time_now(self, freq):\n        self.per.now(freq)\n\n    def setup(self, freq):\n        self.per = Period(\""2012-06-01\"", freq=freq)"", ""tslibs.period.PeriodUnaryMethods.time_to_timestamp"": ""class PeriodUnaryMethods:\n    def time_to_timestamp(self, freq):\n        self.per.to_timestamp()\n\n    def setup(self, freq):\n        self.per = Period(\""2012-06-01\"", freq=freq)""}",tslibs.period.PeriodUnaryMethods,time,0.0
24713,class-level,"terminus-2,oracle",1.005475445309784,1.005475445309784,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timedelta.DatetimeAccessor,timedelta,"{""timedelta.DatetimeAccessor.time_timedelta_seconds"": ""class DatetimeAccessor:\n    def time_timedelta_seconds(self, series):\n        series.dt.seconds\n\n    def setup_cache(self):\n        N = 100000\n        series = Series(timedelta_range(\""1 days\"", periods=N, freq=\""h\""))\n        return series""}",timedelta.DatetimeAccessor,time,0.0
24734,class-level,"terminus-2,oracle",1.0249411433252356,1.0249411433252356,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampOps,tslibs.timestamp,"{""tslibs.timestamp.TimestampOps.time_ceil"": ""class TimestampOps:\n    def time_ceil(self, tz):\n        self.ts.ceil(\""5T\"")\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_floor"": ""class TimestampOps:\n    def time_floor(self, tz):\n        self.ts.floor(\""5T\"")\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_normalize"": ""class TimestampOps:\n    def time_normalize(self, tz):\n        self.ts.normalize()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_replace_None"": ""class TimestampOps:\n    def time_replace_None(self, tz):\n        self.ts.replace(tzinfo=None)\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_replace_tz"": ""class TimestampOps:\n    def time_replace_tz(self, tz):\n        self.ts.replace(tzinfo=pytz.timezone(\""US/Eastern\""))\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_to_julian_date"": ""class TimestampOps:\n    def time_to_julian_date(self, tz):\n        self.ts.to_julian_date()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_tz_convert"": ""class TimestampOps:\n    def time_tz_convert(self, tz):\n        if self.ts.tz is not None:\n            self.ts.tz_convert(tz)\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_tz_localize"": ""class TimestampOps:\n    def time_tz_localize(self, tz):\n        if self.ts.tz is None:\n            self.ts.tz_localize(tz)\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)""}",tslibs.timestamp.TimestampOps,time,0.0
24729,class-level,"terminus-2,oracle",0.9737964783501856,0.9737964783501856,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period.TimePeriodArrToDT64Arr,tslibs.period,"{""tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr"": ""class TimePeriodArrToDT64Arr:\n    def time_periodarray_to_dt64arr(self, size, freq):\n        periodarr_to_dt64arr(self.i8values, freq)\n\n    def setup(self, size, freq):\n        arr = np.arange(10, dtype=\""i8\"").repeat(size // 10)\n        self.i8values = arr""}",tslibs.period.TimePeriodArrToDT64Arr,time,0.0
24737,class-level,"terminus-2,oracle",1.0302587692888638,1.0302587692888638,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.tz_convert.TimeTZConvert,tslibs.tz_convert,"{""tslibs.tz_convert.TimeTZConvert.time_tz_convert_from_utc"": ""class TimeTZConvert:\n    def time_tz_convert_from_utc(self, size, tz):\n        # effectively:\n        #  dti = DatetimeIndex(self.i8data, tz=tz)\n        #  dti.tz_localize(None)\n        if old_sig:\n            tz_convert_from_utc(self.i8data, UTC, tz)\n        else:\n            tz_convert_from_utc(self.i8data, tz)\n\n    def setup(self, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr"", ""tslibs.tz_convert.TimeTZConvert.time_tz_localize_to_utc"": ""class TimeTZConvert:\n    def time_tz_localize_to_utc(self, size, tz):\n        # effectively:\n        #  dti = DatetimeIndex(self.i8data)\n        #  dti.tz_localize(tz, ambiguous=\""NaT\"", nonexistent=\""NaT\"")\n        tz_localize_to_utc(self.i8data, tz, ambiguous=\""NaT\"", nonexistent=\""NaT\"")\n\n    def setup(self, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.tz_convert.TimeTZConvert,time,0.0
24735,class-level,"terminus-2,oracle",1.008663482979988,1.008663482979988,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampProperties,tslibs.timestamp,"{""tslibs.timestamp.TimestampProperties.time_dayofyear"": ""class TimestampProperties:\n    def time_dayofyear(self, tz):\n        self.ts.dayofyear\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_days_in_month"": ""class TimestampProperties:\n    def time_days_in_month(self, tz):\n        self.ts.days_in_month\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_leap_year"": ""class TimestampProperties:\n    def time_is_leap_year(self, tz):\n        self.ts.is_leap_year\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_quarter_end"": ""class TimestampProperties:\n    def time_is_quarter_end(self, tz):\n        self.ts.is_quarter_end\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_quarter_start"": ""class TimestampProperties:\n    def time_is_quarter_start(self, tz):\n        self.ts.is_quarter_start\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_year_end"": ""class TimestampProperties:\n    def time_is_year_end(self, tz):\n        self.ts.is_year_end\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_year_start"": ""class TimestampProperties:\n    def time_is_year_start(self, tz):\n        self.ts.is_year_start\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_microsecond"": ""class TimestampProperties:\n    def time_microsecond(self, tz):\n        self.ts.microsecond\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_month_name"": ""class TimestampProperties:\n    def time_month_name(self, tz):\n        self.ts.month_name()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_quarter"": ""class TimestampProperties:\n    def time_quarter(self, tz):\n        self.ts.quarter\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_week"": ""class TimestampProperties:\n    def time_week(self, tz):\n        self.ts.week\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_weekday_name"": ""class TimestampProperties:\n    def time_weekday_name(self, tz):\n        self.ts.day_name()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp.TimestampProperties,time,0.0
24736,class-level,"terminus-2,oracle",1.0124058949042414,1.0124058949042414,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.tslib.TimeIntsToPydatetime,tslibs.tslib,"{""tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime"": ""class TimeIntsToPydatetime:\n    def time_ints_to_pydatetime(self, box, size, tz):\n        ints_to_pydatetime(self.i8data, tz, box=box)\n\n    def setup(self, box, size, tz):\n        if box == \""date\"" and tz is not None:\n            # tz is ignored, so avoid running redundant benchmarks\n            raise NotImplementedError  # skip benchmark\n        if size == 10**6 and tz is _tzs[-1]:\n            # This is cumbersomely-slow, so skip to trim runtime\n            raise NotImplementedError  # skip benchmark\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.tslib.TimeIntsToPydatetime,time,0.0
24731,class-level,"terminus-2,oracle",1.023426105382882,1.023426105382882,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timedelta.TimedeltaConstructor,tslibs.timedelta,"{""tslibs.timedelta.TimedeltaConstructor.time_from_datetime_timedelta"": ""class TimedeltaConstructor:\n    def time_from_datetime_timedelta(self):\n        Timedelta(self.dttimedelta)\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")"", ""tslibs.timedelta.TimedeltaConstructor.time_from_iso_format"": ""class TimedeltaConstructor:\n    def time_from_iso_format(self):\n        Timedelta(\""P4DT12H30M5S\"")\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")"", ""tslibs.timedelta.TimedeltaConstructor.time_from_np_timedelta"": ""class TimedeltaConstructor:\n    def time_from_np_timedelta(self):\n        Timedelta(self.nptimedelta64)\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")"", ""tslibs.timedelta.TimedeltaConstructor.time_from_string"": ""class TimedeltaConstructor:\n    def time_from_string(self):\n        Timedelta(\""1 days\"")\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")""}",tslibs.timedelta.TimedeltaConstructor,time,0.0
24732,class-level,"terminus-2,oracle",1.0385202816031789,1.0385202816031789,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampAcrossDst,tslibs.timestamp,"{""tslibs.timestamp.TimestampAcrossDst.time_replace_across_dst"": ""class TimestampAcrossDst:\n    def time_replace_across_dst(self):\n        self.ts2.replace(tzinfo=self.tzinfo)\n\n    def setup(self):\n        dt = datetime(2016, 3, 27, 1)\n        self.tzinfo = pytz.timezone(\""CET\"").localize(dt, is_dst=False).tzinfo\n        self.ts2 = Timestamp(dt)""}",tslibs.timestamp.TimestampAcrossDst,time,0.0
24733,class-level,"terminus-2,oracle",1.0304691815416918,1.0304691815416918,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp.TimestampConstruction,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_from_datetime_aware"": ""class TimestampConstruction:\n    def time_from_datetime_aware(self):\n        Timestamp(self.dttime_aware)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampConstruction.time_from_datetime_unaware"": ""class TimestampConstruction:\n    def time_from_datetime_unaware(self):\n        Timestamp(self.dttime_unaware)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampConstruction.time_from_npdatetime64"": ""class TimestampConstruction:\n    def time_from_npdatetime64(self):\n        Timestamp(self.npdatetime64)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampConstruction.time_parse_dateutil"": ""class TimestampConstruction:\n    def time_parse_dateutil(self):\n        Timestamp(\""2017/08/25 08:16:14 AM\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampConstruction.time_parse_iso8601_no_tz"": ""class TimestampConstruction:\n    def time_parse_iso8601_no_tz(self):\n        Timestamp(\""2017-08-25 08:16:14\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampConstruction.time_parse_iso8601_tz"": ""class TimestampConstruction:\n    def time_parse_iso8601_tz(self):\n        Timestamp(\""2017-08-25 08:16:14-0500\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")""}",tslibs.timestamp.TimestampConstruction,time,0.0
29130,class-level,"terminus-2,claude",1.4547160527174117,3.093394649383032,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,ensemble.HistGradientBoostingClassifierBenchmark,ensemble,"{""ensemble.HistGradientBoostingClassifierBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass HistGradientBoostingClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",ensemble.HistGradientBoostingClassifierBenchmark,time,-1.1588957543604106
29131,class-level,"terminus-2,claude",0.9878758042534672,1.024858002625557,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,ensemble.RandomForestClassifierBenchmark,ensemble,"{""ensemble.RandomForestClassifierBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RandomForestClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""ensemble.RandomForestClassifierBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RandomForestClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",ensemble.RandomForestClassifierBenchmark,time,-0.026154312851548598
29128,class-level,"terminus-2,claude",1.0742448146448818,1.03428784582855,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,cluster.KMeansBenchmark,cluster,"{""cluster.KMeansBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",cluster.KMeansBenchmark,time,0.028258110902639116
29129,class-level,"terminus-2,claude",0.8335698226637654,0.8450338227894318,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,cluster.MiniBatchKMeansBenchmark,cluster,"{""cluster.MiniBatchKMeansBenchmark.time_transform"": ""class Transformer:\n    def time_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchKMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",cluster.MiniBatchKMeansBenchmark,time,-0.008107496552805089
29138,class-level,"terminus-2,oracle",1.3083516604062315,1.3083516604062315,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,cluster.KMeansBenchmark,cluster,"{""cluster.KMeansBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""cluster.KMeansBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""cluster.KMeansBenchmark.time_transform"": ""class Transformer:\n    def time_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",cluster.KMeansBenchmark,time,0.0
29143,class-level,"terminus-2,oracle",1.020202162067072,1.020202162067072,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,linear_model.LassoBenchmark,linear_model,"{""linear_model.LassoBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LassoBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",linear_model.LassoBenchmark,time,0.0
29140,class-level,"terminus-2,oracle",1.851809249895161,1.851809249895161,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,ensemble.HistGradientBoostingClassifierBenchmark,ensemble,"{""ensemble.HistGradientBoostingClassifierBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass HistGradientBoostingClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""ensemble.HistGradientBoostingClassifierBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass HistGradientBoostingClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",ensemble.HistGradientBoostingClassifierBenchmark,time,0.0
29146,class-level,"terminus-2,oracle",0.9384013064633068,0.9384013064633068,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,metrics.PairwiseDistancesBenchmark,metrics,"{""metrics.PairwiseDistancesBenchmark.time_pairwise_distances"": ""class PairwiseDistancesBenchmark:\n    def time_pairwise_distances(self, *args):\n        pairwise_distances(self.X, **self.pdist_params)\n\n    def setup(self, *params):\n        representation, metric, n_jobs = params\n    \n        if representation == \""sparse\"" and metric == \""correlation\"":\n            raise NotImplementedError\n    \n        if Benchmark.data_size == \""large\"":\n            if metric in (\""manhattan\"", \""correlation\""):\n                n_samples = 8000\n            else:\n                n_samples = 24000\n        else:\n            if metric in (\""manhattan\"", \""correlation\""):\n                n_samples = 4000\n            else:\n                n_samples = 12000\n    \n        data = _random_dataset(n_samples=n_samples, representation=representation)\n        self.X, self.X_val, self.y, self.y_val = data\n    \n        self.pdist_params = {\""metric\"": metric, \""n_jobs\"": n_jobs}""}",metrics.PairwiseDistancesBenchmark,time,0.0
29141,class-level,"terminus-2,oracle",1.024858002625557,1.024858002625557,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,ensemble.RandomForestClassifierBenchmark,ensemble,"{""ensemble.RandomForestClassifierBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RandomForestClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",ensemble.RandomForestClassifierBenchmark,time,0.0
29132,class-level,"terminus-2,claude",0.9002117666510273,0.9384013064633068,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,metrics.PairwiseDistancesBenchmark,metrics,"{""metrics.PairwiseDistancesBenchmark.time_pairwise_distances"": ""class PairwiseDistancesBenchmark:\n    def time_pairwise_distances(self, *args):\n        pairwise_distances(self.X, **self.pdist_params)\n\n    def setup(self, *params):\n        representation, metric, n_jobs = params\n    \n        if representation == \""sparse\"" and metric == \""correlation\"":\n            raise NotImplementedError\n    \n        if Benchmark.data_size == \""large\"":\n            if metric in (\""manhattan\"", \""correlation\""):\n                n_samples = 8000\n            else:\n                n_samples = 24000\n        else:\n            if metric in (\""manhattan\"", \""correlation\""):\n                n_samples = 4000\n            else:\n                n_samples = 12000\n    \n        data = _random_dataset(n_samples=n_samples, representation=representation)\n        self.X, self.X_val, self.y, self.y_val = data\n    \n        self.pdist_params = {\""metric\"": metric, \""n_jobs\"": n_jobs}""}",metrics.PairwiseDistancesBenchmark,time,-0.027008161111937386
29145,class-level,"terminus-2,oracle",0.7054739058722969,0.7054739058722969,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,linear_model.SGDRegressorBenchmark,linear_model,"{""linear_model.SGDRegressorBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SGDRegressorBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""linear_model.SGDRegressorBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SGDRegressorBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",linear_model.SGDRegressorBenchmark,time,0.0
29142,class-level,"terminus-2,oracle",1.01322469325088,1.01322469325088,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,linear_model.ElasticNetBenchmark,linear_model,"{""linear_model.ElasticNetBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass ElasticNetBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",linear_model.ElasticNetBenchmark,time,0.0
29139,class-level,"terminus-2,oracle",1.186205131359353,1.186205131359353,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,cluster.MiniBatchKMeansBenchmark,cluster,"{""cluster.MiniBatchKMeansBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchKMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""cluster.MiniBatchKMeansBenchmark.time_transform"": ""class Transformer:\n    def time_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchKMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",cluster.MiniBatchKMeansBenchmark,time,0.0
29147,class-level,"terminus-2,oracle",0.9564125578038288,0.9564125578038288,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,neighbors.KNeighborsClassifierBenchmark,neighbors,"{""neighbors.KNeighborsClassifierBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KNeighborsClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""neighbors.KNeighborsClassifierBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KNeighborsClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",neighbors.KNeighborsClassifierBenchmark,time,0.0
29144,class-level,"terminus-2,oracle",0.9787176714587016,0.9787176714587016,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,linear_model.LogisticRegressionBenchmark,linear_model,"{""linear_model.LogisticRegressionBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LogisticRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",linear_model.LogisticRegressionBenchmark,time,0.0
29150,module-level,"terminus-2,claude",1.0236377642108565,1.040693761082678,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.core,io_ascii.core,"{""io_ascii.core.CoreSuite.time_continuation_inputter"": ""class CoreSuite:\n    def time_continuation_inputter(self):\n        core.ContinuationLinesInputter().process_lines(self.lines)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]"", ""io_ascii.core.CoreSuite.time_default_splitter_call"": ""class CoreSuite:\n    def time_default_splitter_call(self):\n        core.DefaultSplitter()(self.csv_line)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]""}",io_ascii.core,time,-0.012062232582617806
29148,module-level,"terminus-2,claude",0.9738645742485048,0.9589327323279948,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,convolve,convolve,"{""convolve.Convolve.time_convolve"": ""class Convolve:\n    def time_convolve(self, ndim, size, boundary, nan_treatment):\n        convolve(\n            self.array, self.kernel, boundary=boundary, nan_treatment=nan_treatment\n        )\n\n    def setup(self, ndim, size, boundary, nan_treatment):\n        np.random.seed(12345)\n    \n        self.kernel = np.random.random(kernel_shapes[ndim][size])\n        self.array = np.random.random(array_shapes[ndim][size])"", ""convolve.ConvolveFFT.time_convolve_fft"": ""class ConvolveFFT:\n    def time_convolve_fft(self, ndim, size, boundary, nan_treatment):\n        convolve_fft(\n            self.array, self.kernel, boundary=boundary, nan_treatment=nan_treatment\n        )\n\n    def setup(self, ndim, size, boundary, nan_treatment):\n        np.random.seed(12345)\n    \n        self.kernel = np.random.random(kernel_shapes[ndim][size])\n        self.array = np.random.random(array_shapes[ndim][size])""}",convolve,time,0.010560001358210752
29149,module-level,"terminus-2,claude",0.9674152093259594,0.9464337069256658,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports,imports,"{""imports.timeraw_import_astropy_table"": ""def timeraw_import_astropy_table():\n    return \""\""\""\n    from astropy import table\n    \""\""\"""", ""imports.timeraw_import_astropy_units"": ""def timeraw_import_astropy_units():\n    return \""\""\""\n    from astropy import units\n    \""\""\"""", ""imports.timeraw_import_astropy_utils"": ""def timeraw_import_astropy_utils():\n    return \""\""\""\n    from astropy import utils\n    \""\""\"""", ""imports.timeraw_import_astropy_utils_iers"": ""def timeraw_import_astropy_utils_iers():\n    return \""\""\""\n    from astropy.utils import iers\n    \""\""\"""", ""imports.timeraw_import_astropy_visualization"": ""def timeraw_import_astropy_visualization():\n    return \""\""\""\n    from astropy import visualization\n    \""\""\"""", ""imports.timeraw_import_astropy_wcs"": ""def timeraw_import_astropy_wcs():\n    return \""\""\""\n    from astropy import wcs\n    \""\""\""""}",imports,time,0.014838403394832828
29151,module-level,"terminus-2,claude",1.0127378598692578,0.9971303976901288,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main,io_ascii.main,"{""io_ascii.main.AastexFloat.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.BasicString.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.CommentedHeaderString.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.IpacString.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.LatexString.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.NoHeaderString.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.SextractorFloat.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.SextractorInt.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.SextractorString.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main,time,0.011037809179016271
29155,module-level,"terminus-2,claude",1.4975765696098011,1.0314625110350797,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,units,units,"{""units.TimeQuantityOpLargeArray.time_quantity_np_square"": ""class TimeQuantityOpSmallArray:\n    def time_quantity_np_square(self):\n        np.power(self.data, 2)\n\nclass TimeQuantityOpLargeArray:\n    def setup(self):\n        data = np.arange(1e6) + 1\n        self.data = data * u.g\n        self.out_sq = data * u.g**2\n        self.out_sqrt = data * u.g**0.5"", ""units.TimeQuantityOpLargeArray.time_quantity_np_square_out"": ""class TimeQuantityOpSmallArray:\n    def time_quantity_np_square_out(self):\n        np.power(self.data, 2, out=self.out_sq)\n\nclass TimeQuantityOpLargeArray:\n    def setup(self):\n        data = np.arange(1e6) + 1\n        self.data = data * u.g\n        self.out_sq = data * u.g**2\n        self.out_sqrt = data * u.g**0.5"", ""units.TimeQuantityOpLargeArrayDiffUnit.time_quantity_equal"": ""class TimeQuantityOpSmallArrayDiffUnit:\n    def time_quantity_equal(self):\n        # Same as operator.eq\n        self.data == self.data2\n\nclass TimeQuantityOpLargeArrayDiffUnit:\n    def setup(self):\n        data = np.arange(1e6) + 1\n        self.data = data * u.g\n    \n        # A different but dimensionally compatible unit\n        self.data2 = 0.001 * data * u.kg"", ""units.TimeQuantityOpLargeArrayDiffUnit.time_quantity_np_equal"": ""class TimeQuantityOpSmallArrayDiffUnit:\n    def time_quantity_np_equal(self):\n        np.equal(self.data, self.data2)\n\nclass TimeQuantityOpLargeArrayDiffUnit:\n    def setup(self):\n        data = np.arange(1e6) + 1\n        self.data = data * u.g\n    \n        # A different but dimensionally compatible unit\n        self.data2 = 0.001 * data * u.kg"", ""units.TimeQuantityOpLargeArrayDiffUnit.time_quantity_truediv"": ""class TimeQuantityOpSmallArrayDiffUnit:\n    def time_quantity_truediv(self):\n        # Since benchmark is PY3 only, this is always true divide.\n        # Same as operator.truediv\n        self.data / self.data2\n\nclass TimeQuantityOpLargeArrayDiffUnit:\n    def setup(self):\n        data = np.arange(1e6) + 1\n        self.data = data * u.g\n    \n        # A different but dimensionally compatible unit\n        self.data2 = 0.001 * data * u.kg"", ""units.TimeQuantityOpSmallArray.time_quantity_np_square"": ""class TimeQuantityOpSmallArray:\n    def time_quantity_np_square(self):\n        np.power(self.data, 2)\n\n    def setup(self):\n        data = np.array([1.0, 2.0, 3.0])\n        self.data = data * u.g\n        self.out_sq = data * u.g**2\n        self.out_sqrt = data * u.g**0.5"", ""units.TimeQuantityOpSmallArray.time_quantity_np_square_out"": ""class TimeQuantityOpSmallArray:\n    def time_quantity_np_square_out(self):\n        np.power(self.data, 2, out=self.out_sq)\n\n    def setup(self):\n        data = np.array([1.0, 2.0, 3.0])\n        self.data = data * u.g\n        self.out_sq = data * u.g**2\n        self.out_sqrt = data * u.g**0.5"", ""units.TimeQuantityOpSmallArray.time_quantity_square"": ""class TimeQuantityOpSmallArray:\n    def time_quantity_square(self):\n        self.data**2\n\n    def setup(self):\n        data = np.array([1.0, 2.0, 3.0])\n        self.data = data * u.g\n        self.out_sq = data * u.g**2\n        self.out_sqrt = data * u.g**0.5"", ""units.TimeQuantityOpSmallArrayDiffUnit.time_quantity_add"": ""class TimeQuantityOpSmallArrayDiffUnit:\n    def time_quantity_add(self):\n        # Same as operator.add\n        self.data + self.data2\n\n    def setup(self):\n        data = np.array([1.0, 2.0, 3.0])\n        self.data = data * u.g\n    \n        # A different but dimensionally compatible unit\n        self.data2 = 0.001 * data * u.kg"", ""units.TimeQuantityOpSmallArrayDiffUnit.time_quantity_mul"": ""class TimeQuantityOpSmallArrayDiffUnit:\n    def time_quantity_mul(self):\n        # Same as operator.mul\n        self.data * self.data2\n\n    def setup(self):\n        data = np.array([1.0, 2.0, 3.0])\n        self.data = data * u.g\n    \n        # A different but dimensionally compatible unit\n        self.data2 = 0.001 * data * u.kg"", ""units.TimeQuantityOpSmallArrayDiffUnit.time_quantity_np_multiply"": ""class TimeQuantityOpSmallArrayDiffUnit:\n    def time_quantity_np_multiply(self):\n        np.multiply(self.data, self.data2)\n\n    def setup(self):\n        data = np.array([1.0, 2.0, 3.0])\n        self.data = data * u.g\n    \n        # A different but dimensionally compatible unit\n        self.data2 = 0.001 * data * u.kg"", ""units.TimeQuantityOpSmallArrayDiffUnit.time_quantity_np_truediv"": ""class TimeQuantityOpSmallArrayDiffUnit:\n    def time_quantity_np_truediv(self):\n        np.true_divide(self.data, self.data2)\n\n    def setup(self):\n        data = np.array([1.0, 2.0, 3.0])\n        self.data = data * u.g\n    \n        # A different but dimensionally compatible unit\n        self.data2 = 0.001 * data * u.kg"", ""units.TimeQuantityOpSmallArraySameUnit.time_quantity_mul"": ""class TimeQuantityOpSmallArrayDiffUnit:\n    def time_quantity_mul(self):\n        # Same as operator.mul\n        self.data * self.data2\n\nclass TimeQuantityOpSmallArraySameUnit:\n    def setup(self):\n        data = np.array([1.0, 2.0, 3.0])\n        self.data = data * u.g\n        self.data2 = self.data.copy()"", ""units.TimeQuantityOpSmallArraySameUnit.time_quantity_np_add"": ""class TimeQuantityOpSmallArrayDiffUnit:\n    def time_quantity_np_add(self):\n        np.add(self.data, self.data2)\n\nclass TimeQuantityOpSmallArraySameUnit:\n    def setup(self):\n        data = np.array([1.0, 2.0, 3.0])\n        self.data = data * u.g\n        self.data2 = self.data.copy()"", ""units.TimeQuantityOpSmallArraySameUnit.time_quantity_np_equal"": ""class TimeQuantityOpSmallArrayDiffUnit:\n    def time_quantity_np_equal(self):\n        np.equal(self.data, self.data2)\n\nclass TimeQuantityOpSmallArraySameUnit:\n    def setup(self):\n        data = np.array([1.0, 2.0, 3.0])\n        self.data = data * u.g\n        self.data2 = self.data.copy()"", ""units.TimeQuantityOpSmallArraySameUnit.time_quantity_np_multiply"": ""class TimeQuantityOpSmallArrayDiffUnit:\n    def time_quantity_np_multiply(self):\n        np.multiply(self.data, self.data2)\n\nclass TimeQuantityOpSmallArraySameUnit:\n    def setup(self):\n        data = np.array([1.0, 2.0, 3.0])\n        self.data = data * u.g\n        self.data2 = self.data.copy()"", ""units.TimeQuantityOpSmallArraySameUnit.time_quantity_np_subtract"": ""class TimeQuantityOpSmallArrayDiffUnit:\n    def time_quantity_np_subtract(self):\n        np.subtract(self.data, self.data2)\n\nclass TimeQuantityOpSmallArraySameUnit:\n    def setup(self):\n        data = np.array([1.0, 2.0, 3.0])\n        self.data = data * u.g\n        self.data2 = self.data.copy()"", ""units.TimeQuantityOpSmallArraySameUnit.time_quantity_np_truediv"": ""class TimeQuantityOpSmallArrayDiffUnit:\n    def time_quantity_np_truediv(self):\n        np.true_divide(self.data, self.data2)\n\nclass TimeQuantityOpSmallArraySameUnit:\n    def setup(self):\n        data = np.array([1.0, 2.0, 3.0])\n        self.data = data * u.g\n        self.data2 = self.data.copy()"", ""units.TimeQuantityOpSmallArraySameUnit.time_quantity_sub"": ""class TimeQuantityOpSmallArrayDiffUnit:\n    def time_quantity_sub(self):\n        # Same as operator.sub\n        self.data - self.data2\n\nclass TimeQuantityOpSmallArraySameUnit:\n    def setup(self):\n        data = np.array([1.0, 2.0, 3.0])\n        self.data = data * u.g\n        self.data2 = self.data.copy()"", ""units.TimeQuantityOpSmallArraySameUnit.time_quantity_truediv"": ""class TimeQuantityOpSmallArrayDiffUnit:\n    def time_quantity_truediv(self):\n        # Since benchmark is PY3 only, this is always true divide.\n        # Same as operator.truediv\n        self.data / self.data2\n\nclass TimeQuantityOpSmallArraySameUnit:\n    def setup(self):\n        data = np.array([1.0, 2.0, 3.0])\n        self.data = data * u.g\n        self.data2 = self.data.copy()"", ""units.time_compose_complex"": ""def time_compose_complex():\n    # Composing a complex unit can be very inefficient\n    (u.kg / u.s**3 * u.au**2.5 / u.yr**0.5 / u.sr**2).compose()"", ""units.time_quantity_view"": ""def time_quantity_view():\n    q1.view(u.Quantity)""}",units,time,0.32964219135411704
29156,module-level,"terminus-2,claude",0.9798645361369982,0.9773346618089332,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,wcs,wcs,"{""wcs.WCSTransformations.time_ape14_world_to_pixel"": ""class WCSTransformations:\n    def time_ape14_world_to_pixel(self, size):\n        wcs.world_to_pixel(self.coord)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)"", ""wcs.WCSTransformations.time_pix2world_x_y_0"": ""class WCSTransformations:\n    def time_pix2world_x_y_0(self, size):\n        wcs.wcs_pix2world(self.px, self.py, 0)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)"", ""wcs.WCSTransformations.time_pix2world_x_y_1"": ""class WCSTransformations:\n    def time_pix2world_x_y_1(self, size):\n        wcs.wcs_pix2world(self.px, self.py, 1)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)"", ""wcs.WCSTransformations.time_world2pix_x_y_0"": ""class WCSTransformations:\n    def time_world2pix_x_y_0(self, size):\n        wcs.wcs_world2pix(self.wx, self.wy, 0)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)"", ""wcs.WCSTransformations.time_world2pix_x_y_1"": ""class WCSTransformations:\n    def time_world2pix_x_y_1(self, size):\n        wcs.wcs_world2pix(self.wx, self.wy, 1)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)""}",wcs,time,0.0017891614767078868
29154,module-level,"terminus-2,claude",1.030126969352848,1.0444499492813466,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table,table,"{""table.TimeMaskedTable.time_column_set"": ""class TimeTable:\n    def time_column_set(self):\n        self.table[\""a\""] = 0.0\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6"", ""table.TimeMaskedTable.time_group"": ""class TimeTable:\n    def time_group(self):\n        self.table.group_by(\""d\"")\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6"", ""table.TimeMaskedTable.time_init_from_np_array_no_copy"": ""class TimeTable:\n    def time_init_from_np_array_no_copy(self):\n        Table(self.np_table, copy=False)\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6"", ""table.TimeTable.time_copy_table"": ""class TimeTable:\n    def time_copy_table(self):\n        self.table.copy()\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6"", ""table.TimeTable.time_read_rows"": ""class TimeTable:\n    def time_read_rows(self):\n        for row in self.table:\n            tuple(row)\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6"", ""table.TimeTable.time_sort"": ""class TimeTable:\n    def time_sort(self):\n        self.table.sort(\""a\"")\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6"", ""table.TimeTable.time_table_slice_bool"": ""class TimeTable:\n    def time_table_slice_bool(self):\n        self.table[self.bool_mask]\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6"", ""table.TimeTableInitWithLists.time_init_lists"": ""class TimeTableInitWithLists:\n    def time_init_lists(self):\n        Table([self.dat, self.dat, self.dat], names=[\""time\"", \""rate\"", \""error\""])\n\n    def setup(self):\n        self.dat = list(range(100_000))"", ""table.TimeTableInitWithMultiDimLists.time_init_int_1d"": ""class TimeTableInitWithMultiDimLists:\n    def time_init_int_1d(self):\n        Table([self.data_int_1d])\n\n    def setup(self):\n        np_data_int = np.arange(1_000_000, dtype=np.int64)\n        np_data_float = np_data_int.astype(np.float64)\n        np_data_str = np_data_int.astype(\""U\"")\n    \n        self.data_int_1d = np_data_int.tolist()\n    \n        self.data_int_3d = np_data_int.reshape(1000, 100, 10).tolist()\n    \n        self.data_int_masked_1d = self.data_int_1d.copy()\n        self.data_int_masked_1d[-1] = np.ma.masked\n    \n        self.data_int_masked_3d = self.data_int_3d.copy()\n        self.data_int_masked_3d[-1][-1][-1] = np.ma.masked\n    \n        self.data_float_1d = np_data_float.tolist()\n    \n        self.data_str_1d = np_data_str.tolist()\n    \n        self.data_str_masked_1d = self.data_str_1d.copy()\n        self.data_str_masked_1d[-1] = np.ma.masked"", ""table.TimeTableInitWithMultiDimLists.time_init_int_3d"": ""class TimeTableInitWithMultiDimLists:\n    def time_init_int_3d(self):\n        Table([self.data_int_3d])\n\n    def setup(self):\n        np_data_int = np.arange(1_000_000, dtype=np.int64)\n        np_data_float = np_data_int.astype(np.float64)\n        np_data_str = np_data_int.astype(\""U\"")\n    \n        self.data_int_1d = np_data_int.tolist()\n    \n        self.data_int_3d = np_data_int.reshape(1000, 100, 10).tolist()\n    \n        self.data_int_masked_1d = self.data_int_1d.copy()\n        self.data_int_masked_1d[-1] = np.ma.masked\n    \n        self.data_int_masked_3d = self.data_int_3d.copy()\n        self.data_int_masked_3d[-1][-1][-1] = np.ma.masked\n    \n        self.data_float_1d = np_data_float.tolist()\n    \n        self.data_str_1d = np_data_str.tolist()\n    \n        self.data_str_masked_1d = self.data_str_1d.copy()\n        self.data_str_masked_1d[-1] = np.ma.masked"", ""table.TimeTableInitWithMultiDimLists.time_init_int_masked_1d"": ""class TimeTableInitWithMultiDimLists:\n    def time_init_int_masked_1d(self):\n        Table([self.data_int_masked_1d])\n\n    def setup(self):\n        np_data_int = np.arange(1_000_000, dtype=np.int64)\n        np_data_float = np_data_int.astype(np.float64)\n        np_data_str = np_data_int.astype(\""U\"")\n    \n        self.data_int_1d = np_data_int.tolist()\n    \n        self.data_int_3d = np_data_int.reshape(1000, 100, 10).tolist()\n    \n        self.data_int_masked_1d = self.data_int_1d.copy()\n        self.data_int_masked_1d[-1] = np.ma.masked\n    \n        self.data_int_masked_3d = self.data_int_3d.copy()\n        self.data_int_masked_3d[-1][-1][-1] = np.ma.masked\n    \n        self.data_float_1d = np_data_float.tolist()\n    \n        self.data_str_1d = np_data_str.tolist()\n    \n        self.data_str_masked_1d = self.data_str_1d.copy()\n        self.data_str_masked_1d[-1] = np.ma.masked"", ""table.TimeTableInitWithMultiDimLists.time_init_int_masked_3d"": ""class TimeTableInitWithMultiDimLists:\n    def time_init_int_masked_3d(self):\n        Table([self.data_int_masked_3d])\n\n    def setup(self):\n        np_data_int = np.arange(1_000_000, dtype=np.int64)\n        np_data_float = np_data_int.astype(np.float64)\n        np_data_str = np_data_int.astype(\""U\"")\n    \n        self.data_int_1d = np_data_int.tolist()\n    \n        self.data_int_3d = np_data_int.reshape(1000, 100, 10).tolist()\n    \n        self.data_int_masked_1d = self.data_int_1d.copy()\n        self.data_int_masked_1d[-1] = np.ma.masked\n    \n        self.data_int_masked_3d = self.data_int_3d.copy()\n        self.data_int_masked_3d[-1][-1][-1] = np.ma.masked\n    \n        self.data_float_1d = np_data_float.tolist()\n    \n        self.data_str_1d = np_data_str.tolist()\n    \n        self.data_str_masked_1d = self.data_str_1d.copy()\n        self.data_str_masked_1d[-1] = np.ma.masked""}",table,time,-0.010129405890027239
29153,module-level,"terminus-2,claude",1.0502744755235731,1.0187313338888078,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,stats.sigma_clipping,stats.sigma_clipping,"{""stats.sigma_clipping.SigmaClipBenchmarks.time_2d_array"": ""class SigmaClipBenchmarks:\n    def time_2d_array(self):\n        self.sigclip(self.data[0])\n\n    def setup(self):\n        # Avoid top-level module import to make sure that the benchmarks are\n        # compatible with versions of astropy that did not have this functionality.\n        from astropy.stats import SigmaClip\n    \n        size = (4, 2048, 2048)\n    \n        with NumpyRNGContext(12345):\n            self.data = np.random.normal(size=size)\n    \n            # add outliers\n            nbad = 100000\n            zbad = np.random.randint(low=0, high=size[0] - 1, size=nbad)\n            ybad = np.random.randint(low=0, high=size[1] - 1, size=nbad)\n            xbad = np.random.randint(low=0, high=size[2] - 1, size=nbad)\n            self.data[zbad, ybad, xbad] = np.random.choice([-1, 1], size=nbad) * (\n                10 + np.random.rand(nbad)\n            )\n    \n            # The defaults use median as the cenfunc and standard\n            # deviation as the stdfunc.  The default iters is 5.\n            self.sigclip = SigmaClip(sigma=3)"", ""stats.sigma_clipping.SigmaClipBenchmarks.time_2d_array_axis"": ""class SigmaClipBenchmarks:\n    def time_2d_array_axis(self):\n        self.sigclip(self.data[0], axis=0)\n\n    def setup(self):\n        # Avoid top-level module import to make sure that the benchmarks are\n        # compatible with versions of astropy that did not have this functionality.\n        from astropy.stats import SigmaClip\n    \n        size = (4, 2048, 2048)\n    \n        with NumpyRNGContext(12345):\n            self.data = np.random.normal(size=size)\n    \n            # add outliers\n            nbad = 100000\n            zbad = np.random.randint(low=0, high=size[0] - 1, size=nbad)\n            ybad = np.random.randint(low=0, high=size[1] - 1, size=nbad)\n            xbad = np.random.randint(low=0, high=size[2] - 1, size=nbad)\n            self.data[zbad, ybad, xbad] = np.random.choice([-1, 1], size=nbad) * (\n                10 + np.random.rand(nbad)\n            )\n    \n            # The defaults use median as the cenfunc and standard\n            # deviation as the stdfunc.  The default iters is 5.\n            self.sigclip = SigmaClip(sigma=3)"", ""stats.sigma_clipping.SigmaClipBenchmarks.time_3d_array"": ""class SigmaClipBenchmarks:\n    def time_3d_array(self):\n        self.sigclip(self.data[:, :1024, :1024])\n\n    def setup(self):\n        # Avoid top-level module import to make sure that the benchmarks are\n        # compatible with versions of astropy that did not have this functionality.\n        from astropy.stats import SigmaClip\n    \n        size = (4, 2048, 2048)\n    \n        with NumpyRNGContext(12345):\n            self.data = np.random.normal(size=size)\n    \n            # add outliers\n            nbad = 100000\n            zbad = np.random.randint(low=0, high=size[0] - 1, size=nbad)\n            ybad = np.random.randint(low=0, high=size[1] - 1, size=nbad)\n            xbad = np.random.randint(low=0, high=size[2] - 1, size=nbad)\n            self.data[zbad, ybad, xbad] = np.random.choice([-1, 1], size=nbad) * (\n                10 + np.random.rand(nbad)\n            )\n    \n            # The defaults use median as the cenfunc and standard\n            # deviation as the stdfunc.  The default iters is 5.\n            self.sigclip = SigmaClip(sigma=3)"", ""stats.sigma_clipping.SigmaClipBenchmarks.time_3d_array_axis"": ""class SigmaClipBenchmarks:\n    def time_3d_array_axis(self):\n        self.sigclip(self.data, axis=0)\n\n    def setup(self):\n        # Avoid top-level module import to make sure that the benchmarks are\n        # compatible with versions of astropy that did not have this functionality.\n        from astropy.stats import SigmaClip\n    \n        size = (4, 2048, 2048)\n    \n        with NumpyRNGContext(12345):\n            self.data = np.random.normal(size=size)\n    \n            # add outliers\n            nbad = 100000\n            zbad = np.random.randint(low=0, high=size[0] - 1, size=nbad)\n            ybad = np.random.randint(low=0, high=size[1] - 1, size=nbad)\n            xbad = np.random.randint(low=0, high=size[2] - 1, size=nbad)\n            self.data[zbad, ybad, xbad] = np.random.choice([-1, 1], size=nbad) * (\n                10 + np.random.rand(nbad)\n            )\n    \n            # The defaults use median as the cenfunc and standard\n            # deviation as the stdfunc.  The default iters is 5.\n            self.sigclip = SigmaClip(sigma=3)"", ""stats.sigma_clipping.SigmaClipBenchmarks.time_3d_array_axis2"": ""class SigmaClipBenchmarks:\n    def time_3d_array_axis2(self):\n        self.sigclip(self.data, axis=(0, 1))\n\n    def setup(self):\n        # Avoid top-level module import to make sure that the benchmarks are\n        # compatible with versions of astropy that did not have this functionality.\n        from astropy.stats import SigmaClip\n    \n        size = (4, 2048, 2048)\n    \n        with NumpyRNGContext(12345):\n            self.data = np.random.normal(size=size)\n    \n            # add outliers\n            nbad = 100000\n            zbad = np.random.randint(low=0, high=size[0] - 1, size=nbad)\n            ybad = np.random.randint(low=0, high=size[1] - 1, size=nbad)\n            xbad = np.random.randint(low=0, high=size[2] - 1, size=nbad)\n            self.data[zbad, ybad, xbad] = np.random.choice([-1, 1], size=nbad) * (\n                10 + np.random.rand(nbad)\n            )\n    \n            # The defaults use median as the cenfunc and standard\n            # deviation as the stdfunc.  The default iters is 5.\n            self.sigclip = SigmaClip(sigma=3)""}",stats.sigma_clipping,time,0.022307738072677052
29152,module-level,"terminus-2,claude",1.055341850348677,1.0075690507073685,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,modeling.fitting,modeling.fitting,"{""modeling.fitting.time_Chebyshev2D_LinearLSQFitter"": ""def time_Chebyshev2D_LinearLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        z = z_base + np.random.normal(0.0, 0.2, z_base.shape)\n        fit_LinearLSQFitter(Chebyshev2D, x_grid, y_grid, z)\n    except Warning:\n        pass"", ""modeling.fitting.time_Gaussian2D_LevMarLSQFitter"": ""def time_Gaussian2D_LevMarLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        z = z_base + np.random.normal(0.0, 0.2, z_base.shape)\n        fit_LevMarLSQFitter(Gaussian2D, x_grid, y_grid, z)\n    except Warning:\n        pass"", ""modeling.fitting.time_Gaussian2D_SimplexLSQFitter"": ""def time_Gaussian2D_SimplexLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        z = z_base + np.random.normal(0.0, 0.2, z_base.shape)\n        fit_SimplexLSQFitter(Gaussian2D, x_grid, y_grid, z)\n    except Warning:\n        pass"", ""modeling.fitting.time_Polynomial2D_LinearLSQFitter"": ""def time_Polynomial2D_LinearLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        z = z_base + np.random.normal(0.0, 0.2, z_base.shape)\n        fit_LinearLSQFitter(Polynomial2D, x_grid, y_grid, z)\n    except Warning:\n        pass"", ""modeling.fitting.time_init_LinearLSQFitter"": ""def time_init_LinearLSQFitter():\n    fitting.LinearLSQFitter()"", ""modeling.fitting.time_init_SimplexLSQFitter"": ""def time_init_SimplexLSQFitter():\n    fitting.SimplexLSQFitter()"", ""modeling.fitting.time_large_gauss_combined_2d_LevMarLSQFitter"": ""def time_large_gauss_combined_2d_LevMarLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        z = z_base + np.random.normal(0.0, 0.2, z_base.shape)\n        fit_LevMarLSQFitter(large_gauss_combined_2d, x_grid, y_grid, z)\n    except Warning:\n        pass"", ""modeling.fitting.time_physical_model_with_units_LevMarLSQFitter"": ""def time_physical_model_with_units_LevMarLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        fit_LevMarLSQFitter(black_body, wav, fnu)\n    except Warning:\n        pass"", ""modeling.fitting.time_uncertanty_Linear1D_LinearLSQFitter"": ""def time_uncertanty_Linear1D_LinearLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        fit_LinearLSQFitter(\n            line_init, x_points, y_points, weights=1.0 / y_unc\n        )\n    except Warning:\n        pass""}",modeling.fitting,time,0.03378557258932704
29167,module-level,"terminus-2,oracle",0.9761447922304972,0.9761447922304972,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,imports,imports,"{""imports.timeraw_import_astropy_io"": ""def timeraw_import_astropy_io():\n    return \""\""\""\n    from astropy import io\n    \""\""\"""", ""imports.timeraw_import_astropy_io_fits"": ""def timeraw_import_astropy_io_fits():\n    return \""\""\""\n    from astropy.io import fits\n    \""\""\"""", ""imports.timeraw_import_astropy_io_misc"": ""def timeraw_import_astropy_io_misc():\n    return \""\""\""\n    from astropy.io import misc\n    \""\""\"""", ""imports.timeraw_import_astropy_io_misc_hdf5"": ""def timeraw_import_astropy_io_misc_hdf5():\n    return \""\""\""\n    from astropy.io.misc import hdf5\n    \""\""\"""", ""imports.timeraw_import_astropy_table"": ""def timeraw_import_astropy_table():\n    return \""\""\""\n    from astropy import table\n    \""\""\"""", ""imports.timeraw_import_astropy_time"": ""def timeraw_import_astropy_time():\n    return \""\""\""\n    from astropy import time\n    \""\""\"""", ""imports.timeraw_import_astropy_timeseries"": ""def timeraw_import_astropy_timeseries():\n    return \""\""\""\n    from astropy import timeseries\n    \""\""\"""", ""imports.timeraw_import_astropy_timeseries_io"": ""def timeraw_import_astropy_timeseries_io():\n    return \""\""\""\n    from astropy.timeseries import io\n    \""\""\"""", ""imports.timeraw_import_astropy_timeseries_periodograms"": ""def timeraw_import_astropy_timeseries_periodograms():\n    return \""\""\""\n    from astropy.timeseries import periodograms\n    \""\""\"""", ""imports.timeraw_import_astropy_units"": ""def timeraw_import_astropy_units():\n    return \""\""\""\n    from astropy import units\n    \""\""\"""", ""imports.timeraw_import_astropy_units_quantity"": ""def timeraw_import_astropy_units_quantity():\n    return \""\""\""\n    from astropy.units import quantity\n    \""\""\"""", ""imports.timeraw_import_astropy_utils"": ""def timeraw_import_astropy_utils():\n    return \""\""\""\n    from astropy import utils\n    \""\""\"""", ""imports.timeraw_import_astropy_utils_iers"": ""def timeraw_import_astropy_utils_iers():\n    return \""\""\""\n    from astropy.utils import iers\n    \""\""\"""", ""imports.timeraw_import_astropy_visualization"": ""def timeraw_import_astropy_visualization():\n    return \""\""\""\n    from astropy import visualization\n    \""\""\"""", ""imports.timeraw_import_astropy_visualization_wcsaxes"": ""def timeraw_import_astropy_visualization_wcsaxes():\n    return \""\""\""\n    from astropy.visualization import wcsaxes\n    \""\""\"""", ""imports.timeraw_import_astropy_wcs"": ""def timeraw_import_astropy_wcs():\n    return \""\""\""\n    from astropy import wcs\n    \""\""\"""", ""imports.timeraw_import_astropy_wcs_wcsapi"": ""def timeraw_import_astropy_wcs_wcsapi():\n    return \""\""\""\n    from astropy.wcs import wcsapi\n    \""\""\""""}",imports,time,0.0
29165,module-level,"terminus-2,oracle",0.9981291131628012,0.9981291131628012,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,convolve,convolve,"{""convolve.Convolve.time_convolve"": ""class Convolve:\n    def time_convolve(self, ndim, size, boundary, nan_treatment):\n        convolve(\n            self.array, self.kernel, boundary=boundary, nan_treatment=nan_treatment\n        )\n\n    def setup(self, ndim, size, boundary, nan_treatment):\n        np.random.seed(12345)\n    \n        self.kernel = np.random.random(kernel_shapes[ndim][size])\n        self.array = np.random.random(array_shapes[ndim][size])"", ""convolve.ConvolveFFT.time_convolve_fft"": ""class ConvolveFFT:\n    def time_convolve_fft(self, ndim, size, boundary, nan_treatment):\n        convolve_fft(\n            self.array, self.kernel, boundary=boundary, nan_treatment=nan_treatment\n        )\n\n    def setup(self, ndim, size, boundary, nan_treatment):\n        np.random.seed(12345)\n    \n        self.kernel = np.random.random(kernel_shapes[ndim][size])\n        self.array = np.random.random(array_shapes[ndim][size])""}",convolve,time,0.0
29166,module-level,"terminus-2,oracle",1.0158010361832068,1.0158010361832068,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,coordinates,coordinates,"{""coordinates.FrameBenchmarks.time_concatenate_array"": ""class FrameBenchmarks:\n    def time_concatenate_array(self):\n        concatenate((self.icrs_array, self.icrs_array))\n\n    def setup(self):\n        self.scalar_ra = 3.2 * u.deg\n        self.scalar_dec = 2.2 * u.deg\n    \n        self.scalar_pmra = 3.2 * u.mas / u.yr\n        self.scalar_pmdec = 2.2 * u.mas / u.yr\n    \n        self.array_ra = np.linspace(0.0, 360.0, 1000) * u.deg\n        self.array_dec = np.linspace(-90.0, 90.0, 1000) * u.deg\n    \n        np.random.seed(12345)\n        self.icrs_scalar = ICRS(ra=1 * u.deg, dec=2 * u.deg)\n        self.icrs_array = ICRS(\n            ra=np.random.random(10000) * u.deg, dec=np.random.random(10000) * u.deg\n        )\n    \n        # Some points to use for benchmarking coordinate matching.\n        # These were motivated by some tests done in astropy/astropy#7324:\n        # https://github.com/astropy/astropy/pull/7324#issuecomment-392382719\n        xyz_uniform1 = rnd.uniform(size=(3, 10000)) * u.kpc\n        xyz_uniform2 = rnd.uniform(size=(3, 10000)) * u.kpc\n        self.icrs_uniform1 = ICRS(\n            xyz_uniform1, representation_type=CartesianRepresentation\n        )\n        self.icrs_uniform2 = ICRS(\n            xyz_uniform2, representation_type=CartesianRepresentation\n        )\n    \n        phi = rnd.uniform(0, 2 * np.pi, size=10000)\n        theta = np.arccos(2 * rnd.uniform(size=10000) - 1)\n        xyz_uniform_sph1 = (\n            np.vstack(\n                (\n                    np.cos(phi) * np.sin(theta),\n                    np.sin(phi) * np.sin(theta),\n                    np.cos(theta),\n                )\n            )\n            * u.kpc\n        )\n    \n        phi = rnd.uniform(0, 2 * np.pi, size=10000)\n        theta = np.arccos(2 * rnd.uniform(size=10000) - 1)\n        xyz_uniform_sph2 = (\n            np.vstack(\n                (\n                    np.cos(phi) * np.sin(theta),\n                    np.sin(phi) * np.sin(theta),\n                    np.cos(theta),\n                )\n            )\n            * u.kpc\n        )\n        self.icrs_uniform_sph1 = ICRS(\n            xyz_uniform_sph1, representation_type=CartesianRepresentation\n        )\n        self.icrs_uniform_sph2 = ICRS(\n            xyz_uniform_sph2, representation_type=CartesianRepresentation\n        )\n    \n        xyz0 = rnd.uniform(-100, 100, size=(8, 3))\n        xyz_clustered1 = np.vstack(rnd.normal(xyz0, size=(10000, 8, 3))).T * u.kpc\n        xyz_clustered2 = np.vstack(rnd.normal(xyz0, size=(10000, 8, 3))).T * u.kpc\n        self.icrs_clustered1 = ICRS(\n            xyz_clustered1, representation_type=CartesianRepresentation\n        )\n        self.icrs_clustered2 = ICRS(\n            xyz_clustered2, representation_type=CartesianRepresentation\n        )"", ""coordinates.SkyCoordBenchmarks.time_iter_array"": ""class SkyCoordBenchmarks:\n    def time_iter_array(self):\n        for c in self.coord_array_1e3:\n            pass\n\n    def setup(self):\n        self.coord_scalar = SkyCoord(1, 2, unit=\""deg\"", frame=\""icrs\"")\n    \n        lon, lat = np.ones((2, 1000))\n        self.coord_array_1e3 = SkyCoord(lon, lat, unit=\""deg\"", frame=\""icrs\"")\n    \n        self.lon_1e6, self.lat_1e6 = np.ones((2, int(1e6)))\n        self.coord_array_1e6 = SkyCoord(\n            self.lon_1e6, self.lat_1e6, unit=\""deg\"", frame=\""icrs\""\n        )\n    \n        self.scalar_q_ra = 1 * u.deg\n        self.scalar_q_dec = 2 * u.deg\n    \n        np.random.seed(12345)\n        self.array_q_ra = np.random.rand(int(1e6)) * 360 * u.deg\n        self.array_q_dec = (np.random.rand(int(1e6)) * 180 - 90) * u.deg\n    \n        self.scalar_repr = UnitSphericalRepresentation(\n            lat=self.scalar_q_dec, lon=self.scalar_q_ra\n        )\n        self.array_repr = UnitSphericalRepresentation(\n            lat=self.array_q_dec, lon=self.array_q_ra\n        )""}",coordinates,time,0.0
29170,module-level,"terminus-2,oracle",1.0145693146798205,1.0145693146798205,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.table,io_ascii.table,"{""io_ascii.table.TableSuite.time_str_vals_str"": ""class TableSuite:\n    def time_str_vals_str(self):\n        self.table_cols[2].iter_str_vals()\n\n    def setup(self):\n        self.lst = []\n        self.lst.append([random.randint(-500, 500) for i in range(1000)])\n        self.lst.append([random.random() * 500 - 500 for i in range(1000)])\n        self.lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, self.lst):\n            col.data = x\n        self.table_cols = [table.Column(x) for x in self.lst]\n        self.outputter = core.TableOutputter()\n        self.table = table.Table()""}",io_ascii.table,time,0.0
29169,module-level,"terminus-2,oracle",1.032880322076006,1.032880322076006,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main,io_ascii.main,"{""io_ascii.main.AastexFloat.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.CommentedHeaderFloat.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.CommentedHeaderFloat.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.CommentedHeaderInt.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.NoHeaderInt.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.NoHeaderInt.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.RdbInt.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.SextractorString.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.TabInt.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main,time,0.0
29168,module-level,"terminus-2,oracle",1.0273108897764325,1.0273108897764325,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.core,io_ascii.core,"{""io_ascii.core.CoreSuite.time_continuation_inputter"": ""class CoreSuite:\n    def time_continuation_inputter(self):\n        core.ContinuationLinesInputter().process_lines(self.lines)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]"", ""io_ascii.core.CoreSuite.time_convert_vals"": ""class CoreSuite:\n    def time_convert_vals(self):\n        core.TableOutputter()._convert_vals(self.cols)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]"", ""io_ascii.core.CoreSuite.time_default_splitter_call"": ""class CoreSuite:\n    def time_default_splitter_call(self):\n        core.DefaultSplitter()(self.csv_line)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]"", ""io_ascii.core.CoreSuite.time_whitespace_splitter"": ""class CoreSuite:\n    def time_whitespace_splitter(self):\n        core.WhitespaceSplitter().process_line(self.line)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]""}",io_ascii.core,time,0.0
29162,module-level,"terminus-2,gpt-5",0.9850053604045756,1.0166635229568928,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,modeling.model,modeling.model,"{""modeling.model.time_eval_gaussian_no_units_big"": ""def time_eval_gaussian_no_units_big():\n    gauss1d_no_units(x_no_units_big)"", ""modeling.model.time_eval_gaussian_with_units_small"": ""def time_eval_gaussian_with_units_small():\n    gauss1d_with_units(x_with_units_small)"", ""modeling.model.time_init_gaussian_with_units"": ""def time_init_gaussian_with_units():\n    models.Gaussian1D(amplitude=10 * u.Hz, mean=5 * u.m, stddev=1.2 * u.cm)""}",modeling.model,time,-0.022389082427381277
29159,module-level,"terminus-2,gpt-5",1.0235888675616638,1.027145428415974,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.core,io_ascii.core,"{""io_ascii.core.CoreSuite.time_convert_vals"": ""class CoreSuite:\n    def time_convert_vals(self):\n        core.TableOutputter()._convert_vals(self.cols)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]"", ""io_ascii.core.CoreSuite.time_default_splitter_call"": ""class CoreSuite:\n    def time_default_splitter_call(self):\n        core.DefaultSplitter()(self.csv_line)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]"", ""io_ascii.core.CoreSuite.time_whitespace_splitter"": ""class CoreSuite:\n    def time_whitespace_splitter(self):\n        core.WhitespaceSplitter().process_line(self.line)\n\n    def setup(self):\n        self.lines = []\n        options = [\n            [\""a b c d\""],\n            [\""a b c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c \\\\\"", \""d\""],\n            [\""a b \\\\\"", \""c d\""],\n            [\""a \\\\\"", \""b c \\\\\"", \""d\""],\n        ]\n        for i in range(1000):\n            self.lines.extend(options[i % 5])\n        options = ['\""a\\tbc\\t\\td\""', \""ab cd\"", \""\\tab\\t\\tc\\td\"", \""a \\tb \\tcd\""]\n        self.line = \""\"".join([options[i % 4] for i in range(1000)])\n        self.vals = [randword() for i in range(1000)]\n        self.csv_line = \"",\"".join([str(x) for x in self.vals])\n        lst = []\n        lst.append([random.randint(-500, 500) for i in range(1000)])\n        lst.append([random.random() * 500 - 500 for i in range(1000)])\n        lst.append(\n            [\""\"".join([random.choice(uppercase) for j in range(6)]) for i in range(1000)]\n        )\n        self.cols = [core.Column(str(i + 1)) for i in range(3)]\n        for col, x in izip(self.cols, lst):\n            col.str_vals = [str(s) for s in x]""}",io_ascii.core,time,-0.002515248128932263
29158,module-level,"terminus-2,gpt-5",1.0804701481035563,1.0188527292566272,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,imports,imports,"{""imports.timeraw_import_astropy_constants"": ""def timeraw_import_astropy_constants():\n    return \""\""\""\n    from astropy import constants\n    \""\""\"""", ""imports.timeraw_import_astropy_coordinates"": ""def timeraw_import_astropy_coordinates():\n    return \""\""\""\n    from astropy import coordinates\n    \""\""\"""", ""imports.timeraw_import_astropy_cosmology"": ""def timeraw_import_astropy_cosmology():\n    return \""\""\""\n    from astropy import cosmology\n    \""\""\"""", ""imports.timeraw_import_astropy_io"": ""def timeraw_import_astropy_io():\n    return \""\""\""\n    from astropy import io\n    \""\""\"""", ""imports.timeraw_import_astropy_io_ascii"": ""def timeraw_import_astropy_io_ascii():\n    return \""\""\""\n    from astropy.io import ascii\n    \""\""\"""", ""imports.timeraw_import_astropy_io_fits"": ""def timeraw_import_astropy_io_fits():\n    return \""\""\""\n    from astropy.io import fits\n    \""\""\"""", ""imports.timeraw_import_astropy_io_misc"": ""def timeraw_import_astropy_io_misc():\n    return \""\""\""\n    from astropy.io import misc\n    \""\""\"""", ""imports.timeraw_import_astropy_io_misc_hdf5"": ""def timeraw_import_astropy_io_misc_hdf5():\n    return \""\""\""\n    from astropy.io.misc import hdf5\n    \""\""\"""", ""imports.timeraw_import_astropy_io_misc_pandas"": ""def timeraw_import_astropy_io_misc_pandas():\n    return \""\""\""\n    from astropy.io.misc import pandas\n    \""\""\"""", ""imports.timeraw_import_astropy_io_misc_yaml"": ""def timeraw_import_astropy_io_misc_yaml():\n    return \""\""\""\n    from astropy.io.misc import yaml\n    \""\""\"""", ""imports.timeraw_import_astropy_io_votable"": ""def timeraw_import_astropy_io_votable():\n    return \""\""\""\n    from astropy.io import votable\n    \""\""\"""", ""imports.timeraw_import_astropy_modeling"": ""def timeraw_import_astropy_modeling():\n    return \""\""\""\n    from astropy import modeling\n    \""\""\"""", ""imports.timeraw_import_astropy_nddata"": ""def timeraw_import_astropy_nddata():\n    return \""\""\""\n    from astropy import nddata\n    \""\""\"""", ""imports.timeraw_import_astropy_time"": ""def timeraw_import_astropy_time():\n    return \""\""\""\n    from astropy import time\n    \""\""\"""", ""imports.timeraw_import_astropy_timeseries"": ""def timeraw_import_astropy_timeseries():\n    return \""\""\""\n    from astropy import timeseries\n    \""\""\"""", ""imports.timeraw_import_astropy_timeseries_io"": ""def timeraw_import_astropy_timeseries_io():\n    return \""\""\""\n    from astropy.timeseries import io\n    \""\""\""""}",imports,time,0.043576675280713704
29164,module-level,"terminus-2,gpt-5",1.0082041248679634,0.991306196805949,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,wcs,wcs,"{""wcs.WCSTransformations.time_ape14_world_to_pixel"": ""class WCSTransformations:\n    def time_ape14_world_to_pixel(self, size):\n        wcs.world_to_pixel(self.coord)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)"", ""wcs.WCSTransformations.time_pix2world_x_y_0"": ""class WCSTransformations:\n    def time_pix2world_x_y_0(self, size):\n        wcs.wcs_pix2world(self.px, self.py, 0)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)"", ""wcs.WCSTransformations.time_world2pix_x_y_0"": ""class WCSTransformations:\n    def time_world2pix_x_y_0(self, size):\n        wcs.wcs_world2pix(self.wx, self.wy, 0)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)""}",wcs,time,0.011950444173984725
29172,module-level,"terminus-2,oracle",1.008718951331412,1.008718951331412,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,modeling.fitting,modeling.fitting,"{""modeling.fitting.time_Chebyshev1D_LevMarLSQFitter"": ""def time_Chebyshev1D_LevMarLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        y = y_base + np.random.normal(0.0, 0.2, y_base.shape)\n        fit_LevMarLSQFitter(Chebyshev1D, x, y)\n    except Warning:\n        pass"", ""modeling.fitting.time_init_LevMarLSQFitter"": ""def time_init_LevMarLSQFitter():\n    fitting.LevMarLSQFitter()"", ""modeling.fitting.time_init_LinearLSQFitter"": ""def time_init_LinearLSQFitter():\n    fitting.LinearLSQFitter()""}",modeling.fitting,time,0.0
29161,module-level,"terminus-2,gpt-5",1.036180212091713,1.009294393676725,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,modeling.fitting,modeling.fitting,"{""modeling.fitting.time_Chebyshev1D_LevMarLSQFitter"": ""def time_Chebyshev1D_LevMarLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        y = y_base + np.random.normal(0.0, 0.2, y_base.shape)\n        fit_LevMarLSQFitter(Chebyshev1D, x, y)\n    except Warning:\n        pass"", ""modeling.fitting.time_Chebyshev2D_LinearLSQFitter"": ""def time_Chebyshev2D_LinearLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        z = z_base + np.random.normal(0.0, 0.2, z_base.shape)\n        fit_LinearLSQFitter(Chebyshev2D, x_grid, y_grid, z)\n    except Warning:\n        pass"", ""modeling.fitting.time_Gaussian2D_LevMarLSQFitter"": ""def time_Gaussian2D_LevMarLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        z = z_base + np.random.normal(0.0, 0.2, z_base.shape)\n        fit_LevMarLSQFitter(Gaussian2D, x_grid, y_grid, z)\n    except Warning:\n        pass"", ""modeling.fitting.time_Gaussian2D_SimplexLSQFitter"": ""def time_Gaussian2D_SimplexLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        z = z_base + np.random.normal(0.0, 0.2, z_base.shape)\n        fit_SimplexLSQFitter(Gaussian2D, x_grid, y_grid, z)\n    except Warning:\n        pass"", ""modeling.fitting.time_Polynomial2D_LinearLSQFitter"": ""def time_Polynomial2D_LinearLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        z = z_base + np.random.normal(0.0, 0.2, z_base.shape)\n        fit_LinearLSQFitter(Polynomial2D, x_grid, y_grid, z)\n    except Warning:\n        pass"", ""modeling.fitting.time_init_LevMarLSQFitter"": ""def time_init_LevMarLSQFitter():\n    fitting.LevMarLSQFitter()"", ""modeling.fitting.time_init_SimplexLSQFitter"": ""def time_init_SimplexLSQFitter():\n    fitting.SimplexLSQFitter()"", ""modeling.fitting.time_large_gauss_combined_1d_LevMarLSQFitter"": ""def time_large_gauss_combined_1d_LevMarLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        y = y_base + np.random.normal(0.0, 0.2, y_base.shape)\n        fit_LevMarLSQFitter(large_gauss_combined_1d, x, y)\n    except Warning:\n        pass"", ""modeling.fitting.time_large_gauss_combined_2d_LevMarLSQFitter"": ""def time_large_gauss_combined_2d_LevMarLSQFitter():\n    warnings.filterwarnings(\""error\"")\n    try:\n        z = z_base + np.random.normal(0.0, 0.2, z_base.shape)\n        fit_LevMarLSQFitter(large_gauss_combined_2d, x_grid, y_grid, z)\n    except Warning:\n        pass""}",modeling.fitting,time,0.019014015852183912
29171,module-level,"terminus-2,oracle",1.0322761218682588,1.0322761218682588,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,io_fits,io_fits,"{""io_fits.FITSHeader.time_get_float"": ""class FITSHeader:\n    def time_get_float(self):\n        self.hdr.get(\""FLT999\"")\n\n    def setup(self):\n        self.hdr = make_header()\n        self.hdr_string = self.hdr.tostring()"", ""io_fits.FITSHeader.time_get_hierarch"": ""class FITSHeader:\n    def time_get_hierarch(self):\n        self.hdr.get(\""HIERARCH FOO BAR 999\"")\n\n    def setup(self):\n        self.hdr = make_header()\n        self.hdr_string = self.hdr.tostring()"", ""io_fits.FITSHeader.time_get_int"": ""class FITSHeader:\n    def time_get_int(self):\n        self.hdr.get(\""INT999\"")\n\n    def setup(self):\n        self.hdr = make_header()\n        self.hdr_string = self.hdr.tostring()""}",io_fits,time,0.0
29157,module-level,"terminus-2,gpt-5",0.9835777340871984,0.9591490792274736,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,convolve,convolve,"{""convolve.Convolve.time_convolve"": ""class Convolve:\n    def time_convolve(self, ndim, size, boundary, nan_treatment):\n        convolve(\n            self.array, self.kernel, boundary=boundary, nan_treatment=nan_treatment\n        )\n\n    def setup(self, ndim, size, boundary, nan_treatment):\n        np.random.seed(12345)\n    \n        self.kernel = np.random.random(kernel_shapes[ndim][size])\n        self.array = np.random.random(array_shapes[ndim][size])"", ""convolve.ConvolveFFT.time_convolve_fft"": ""class ConvolveFFT:\n    def time_convolve_fft(self, ndim, size, boundary, nan_treatment):\n        convolve_fft(\n            self.array, self.kernel, boundary=boundary, nan_treatment=nan_treatment\n        )\n\n    def setup(self, ndim, size, boundary, nan_treatment):\n        np.random.seed(12345)\n    \n        self.kernel = np.random.random(kernel_shapes[ndim][size])\n        self.array = np.random.random(array_shapes[ndim][size])""}",convolve,time,0.017276276421304658
29173,module-level,"terminus-2,oracle",1.0166635229568928,1.0166635229568928,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,modeling.model,modeling.model,"{""modeling.model.time_init_gaussian_with_units"": ""def time_init_gaussian_with_units():\n    models.Gaussian1D(amplitude=10 * u.Hz, mean=5 * u.m, stddev=1.2 * u.cm)""}",modeling.model,time,0.0
29174,module-level,"terminus-2,oracle",1.0187313338888078,1.0187313338888078,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,stats.sigma_clipping,stats.sigma_clipping,"{""stats.sigma_clipping.SigmaClipBenchmarks.time_3d_array_axis2"": ""class SigmaClipBenchmarks:\n    def time_3d_array_axis2(self):\n        self.sigclip(self.data, axis=(0, 1))\n\n    def setup(self):\n        # Avoid top-level module import to make sure that the benchmarks are\n        # compatible with versions of astropy that did not have this functionality.\n        from astropy.stats import SigmaClip\n    \n        size = (4, 2048, 2048)\n    \n        with NumpyRNGContext(12345):\n            self.data = np.random.normal(size=size)\n    \n            # add outliers\n            nbad = 100000\n            zbad = np.random.randint(low=0, high=size[0] - 1, size=nbad)\n            ybad = np.random.randint(low=0, high=size[1] - 1, size=nbad)\n            xbad = np.random.randint(low=0, high=size[2] - 1, size=nbad)\n            self.data[zbad, ybad, xbad] = np.random.choice([-1, 1], size=nbad) * (\n                10 + np.random.rand(nbad)\n            )\n    \n            # The defaults use median as the cenfunc and standard\n            # deviation as the stdfunc.  The default iters is 5.\n            self.sigclip = SigmaClip(sigma=3)""}",stats.sigma_clipping,time,0.0
29176,module-level,"terminus-2,oracle",1.0251313478009412,1.0251313478009412,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,units,units,"{""units.TimeQuantityOpLargeArray.time_quantity_np_square"": ""class TimeQuantityOpSmallArray:\n    def time_quantity_np_square(self):\n        np.power(self.data, 2)\n\nclass TimeQuantityOpLargeArray:\n    def setup(self):\n        data = np.arange(1e6) + 1\n        self.data = data * u.g\n        self.out_sq = data * u.g**2\n        self.out_sqrt = data * u.g**0.5"", ""units.TimeQuantityOpSmallArrayDiffUnit.time_quantity_add"": ""class TimeQuantityOpSmallArrayDiffUnit:\n    def time_quantity_add(self):\n        # Same as operator.add\n        self.data + self.data2\n\n    def setup(self):\n        data = np.array([1.0, 2.0, 3.0])\n        self.data = data * u.g\n    \n        # A different but dimensionally compatible unit\n        self.data2 = 0.001 * data * u.kg"", ""units.TimeQuantityOpSmallArrayDiffUnit.time_quantity_np_add"": ""class TimeQuantityOpSmallArrayDiffUnit:\n    def time_quantity_np_add(self):\n        np.add(self.data, self.data2)\n\n    def setup(self):\n        data = np.array([1.0, 2.0, 3.0])\n        self.data = data * u.g\n    \n        # A different but dimensionally compatible unit\n        self.data2 = 0.001 * data * u.kg"", ""units.time_quantity_view"": ""def time_quantity_view():\n    q1.view(u.Quantity)"", ""units.time_unit_to"": ""def time_unit_to():\n    u.m.to(u.pc)""}",units,time,0.0
29175,module-level,"terminus-2,oracle",0.9940032465471444,0.9940032465471444,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,table,table,"{""table.TimeMaskedTable.time_join_inner"": ""class TimeTable:\n    def time_join_inner(self):\n        join(self.table, self.other_table, keys=\""i\"", join_type=\""inner\"")\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6"", ""table.TimeTable.time_add_column"": ""class TimeTable:\n    def time_add_column(self):\n        self.table[\""e\""] = self.extra_column\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6"", ""table.TimeTable.time_column_slice_bool"": ""class TimeTable:\n    def time_column_slice_bool(self):\n        self.table[\""a\""][self.bool_mask]\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6"", ""table.TimeTable.time_hstack"": ""class TimeTable:\n    def time_hstack(self):\n        hstack([self.table, self.other_table_2])\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6"", ""table.TimeTable.time_init_from_np_array_copy"": ""class TimeTable:\n    def time_init_from_np_array_copy(self):\n        Table(self.np_table, copy=True)\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6"", ""table.TimeTable.time_item_get_rowfirst"": ""class TimeTable:\n    def time_item_get_rowfirst(self):\n        self.table[300][\""b\""]\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6"", ""table.TimeTable.time_read_rows"": ""class TimeTable:\n    def time_read_rows(self):\n        for row in self.table:\n            tuple(row)\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6"", ""table.TimeTableInitWithLists.time_init_lists"": ""class TimeTableInitWithLists:\n    def time_init_lists(self):\n        Table([self.dat, self.dat, self.dat], names=[\""time\"", \""rate\"", \""error\""])\n\n    def setup(self):\n        self.dat = list(range(100_000))"", ""table.TimeTableInitWithMultiDimLists.time_init_int_1d"": ""class TimeTableInitWithMultiDimLists:\n    def time_init_int_1d(self):\n        Table([self.data_int_1d])\n\n    def setup(self):\n        np_data_int = np.arange(1_000_000, dtype=np.int64)\n        np_data_float = np_data_int.astype(np.float64)\n        np_data_str = np_data_int.astype(\""U\"")\n    \n        self.data_int_1d = np_data_int.tolist()\n    \n        self.data_int_3d = np_data_int.reshape(1000, 100, 10).tolist()\n    \n        self.data_int_masked_1d = self.data_int_1d.copy()\n        self.data_int_masked_1d[-1] = np.ma.masked\n    \n        self.data_int_masked_3d = self.data_int_3d.copy()\n        self.data_int_masked_3d[-1][-1][-1] = np.ma.masked\n    \n        self.data_float_1d = np_data_float.tolist()\n    \n        self.data_str_1d = np_data_str.tolist()\n    \n        self.data_str_masked_1d = self.data_str_1d.copy()\n        self.data_str_masked_1d[-1] = np.ma.masked"", ""table.TimeTableInitWithMultiDimLists.time_init_int_masked_3d"": ""class TimeTableInitWithMultiDimLists:\n    def time_init_int_masked_3d(self):\n        Table([self.data_int_masked_3d])\n\n    def setup(self):\n        np_data_int = np.arange(1_000_000, dtype=np.int64)\n        np_data_float = np_data_int.astype(np.float64)\n        np_data_str = np_data_int.astype(\""U\"")\n    \n        self.data_int_1d = np_data_int.tolist()\n    \n        self.data_int_3d = np_data_int.reshape(1000, 100, 10).tolist()\n    \n        self.data_int_masked_1d = self.data_int_1d.copy()\n        self.data_int_masked_1d[-1] = np.ma.masked\n    \n        self.data_int_masked_3d = self.data_int_3d.copy()\n        self.data_int_masked_3d[-1][-1][-1] = np.ma.masked\n    \n        self.data_float_1d = np_data_float.tolist()\n    \n        self.data_str_1d = np_data_str.tolist()\n    \n        self.data_str_masked_1d = self.data_str_1d.copy()\n        self.data_str_masked_1d[-1] = np.ma.masked""}",table,time,0.0
29177,module-level,"terminus-2,oracle",1.0136094389155583,1.0136094389155583,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,votable,votable,"{""votable.TimeVOTableSmallOverhead.time_small_binary2"": ""class TimeVOTableSmallOverhead:\n    def time_small_binary2(self):\n        parse(io.BytesIO(self.binary2_data))\n\n    def setup(self):\n        table = Table(\n            [\n                ra_data[:SMALL_SIZE],\n                dec_data[:SMALL_SIZE],\n                mag_data[:SMALL_SIZE]\n            ],\n            names=['ra', 'dec', 'mag']\n        )\n    \n        self.binary_data = create_votable_bytes(table, 'binary')\n        self.binary2_data = create_votable_bytes(table, 'binary2')""}",votable,time,0.0
29178,module-level,"terminus-2,oracle",0.9543181972536148,0.9543181972536148,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,astropy_astropy,wcs,wcs,"{""wcs.WCSTransformations.time_ape14_world_to_pixel"": ""class WCSTransformations:\n    def time_ape14_world_to_pixel(self, size):\n        wcs.world_to_pixel(self.coord)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)"", ""wcs.WCSTransformations.time_pix2world_x_y_0"": ""class WCSTransformations:\n    def time_pix2world_x_y_0(self, size):\n        wcs.wcs_pix2world(self.px, self.py, 0)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)"", ""wcs.WCSTransformations.time_pix2world_x_y_1"": ""class WCSTransformations:\n    def time_pix2world_x_y_1(self, size):\n        wcs.wcs_pix2world(self.px, self.py, 1)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)"", ""wcs.WCSTransformations.time_world2pix_x_y_0"": ""class WCSTransformations:\n    def time_world2pix_x_y_0(self, size):\n        wcs.wcs_world2pix(self.wx, self.wy, 0)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)"", ""wcs.WCSTransformations.time_world2pix_x_y_1"": ""class WCSTransformations:\n    def time_world2pix_x_y_1(self, size):\n        wcs.wcs_world2pix(self.wx, self.wy, 1)\n\n    def setup(self, size):\n        np.random.seed(12345)\n        gen = Generator(PCG64())\n        self.px = gen.uniform(0, 20, size)\n        self.py = gen.uniform(0, 20, size)\n        self.pxy = np.vstack([self.px, self.py])\n        self.wx, self.wy = wcs.wcs_pix2world(self.px, self.py, 0)\n        self.wxy = np.vstack([self.wx, self.wy])\n        self.coord = wcs.pixel_to_world(self.px, self.py)""}",wcs,time,0.0
29160,module-level,"terminus-2,gpt-5",1.0398649861719789,1.0472848900207263,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,io_ascii.main,io_ascii.main,"{""io_ascii.main.CommentedHeaderFloat.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.CommentedHeaderInt.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.FixedWidthNoHeaderFloat.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.FixedWidthNoHeaderFloat.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.FixedWidthNoHeaderString.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.FixedWidthNoHeaderString.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.FixedWidthString.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.FixedWidthTwoLineString.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.IpacFloat.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.IpacInt.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.IpacString.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.LatexFloat.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.LatexInt.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.LatexInt.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.LatexString.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.NoHeaderInt.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.NoHeaderString.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.RdbInt.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.RdbInt.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.SextractorFloat.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.SextractorInt.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.SextractorString.time_read"": ""class _ASCIISuite:\n    def read(self):\n        return ascii.read(BytesIO(self.data), format=self.file_format, guess=False)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()"", ""io_ascii.main.TabInt.time_write"": ""class _ASCIISuite:\n    def write(self):\n        ascii.write(self.table, self.output, format=self.file_format)\n\n    def setup(self):\n        self.tables = {}\n        self.data = {}\n        self.output = StringIO()\n        self.writers = {\n            \""csv\"": ascii.Csv,\n            \""rdb\"": ascii.Rdb,\n            \""fixed_width\"": ascii.FixedWidth,\n            \""fixed_width_no_header\"": ascii.FixedWidthNoHeader,\n            \""fixed_width_two_line\"": ascii.FixedWidthTwoLine,\n            \""tab\"": ascii.Tab,\n            \""no_header\"": ascii.NoHeader,\n            \""commented_header\"": ascii.CommentedHeader,\n            \""basic\"": ascii.Basic,\n            \""ipac\"": ascii.Ipac,\n            \""latex\"": ascii.Latex,\n            \""aastex\"": ascii.AASTex,\n        }\n        with io.open(\n            os.path.join(\n                HERE, \""files\"", self.file_format, \""{0}.txt\"".format(self.data_type)\n            ),\n            \""rb\"",\n        ) as f:\n            self.data = f.read()\n        if self.file_format != \""sextractor\"":\n            self.table = self.read()""}",io_ascii.main,time,-0.005247456753003866
29163,module-level,"terminus-2,gpt-5",1.003804077337397,1.0693566471636309,astropy_astropy_21,/recordings/2025-10-01__09-26-33/astropy_astropy_21/astropy_astropy_21.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,astropy_astropy,table,table,"{""table.TimeMaskedTable.time_column_get"": ""class TimeTable:\n    def time_column_get(self):\n        self.table[\""c\""]\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6"", ""table.TimeTable.time_column_make_bool_mask"": ""class TimeTable:\n    def time_column_make_bool_mask(self):\n        self.table[\""a\""] > 0.6\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6"", ""table.TimeTable.time_multi_column_get"": ""class TimeTable:\n    def time_multi_column_get(self):\n        self.table[(\""a\"", \""c\"")]\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6"", ""table.TimeTable.time_row_get"": ""class TimeTable:\n    def time_row_get(self):\n        self.table[300]\n\n    def setup(self):\n        # Initialize table\n        self.table = Table(masked=self.masked)\n    \n        # Create column with mixed types\n        np.random.seed(12345)\n        self.table[\""i\""] = np.arange(1000)\n        self.table[\""a\""] = np.random.random(1000)  # float\n        self.table[\""b\""] = np.random.random(1000) > 0.5  # bool\n        self.table[\""c\""] = np.random.random((1000, 10))  # 2d column\n        self.table[\""d\""] = np.random.choice(np.array(list(string.ascii_letters)), 1000)\n    \n        self.np_table = np.array(self.table)\n    \n        self.extra_row = {\""a\"": 1.2, \""b\"": True, \""c\"": np.repeat(1, 10), \""d\"": \""Z\""}\n    \n        self.extra_column = np.random.randint(0, 100, 1000)\n    \n        self.row_indices = np.where(self.table[\""a\""] > 0.9)[0]\n    \n        self.table_grouped = self.table.group_by(\""d\"")\n    \n        # Another table for testing joining\n        self.other_table = Table(masked=self.masked)\n        self.other_table[\""i\""] = np.arange(1, 1000, 3)\n        self.other_table[\""f\""] = np.random.random()\n        self.other_table.sort(\""f\"")\n    \n        # Another table for testing hstack\n        self.other_table_2 = Table(masked=self.masked)\n        self.other_table_2[\""g\""] = np.random.random(1000)\n        self.other_table_2[\""h\""] = np.random.random((1000, 10))\n    \n        self.bool_mask = self.table[\""a\""] > 0.6"", ""table.TimeTableInitWithLists.time_init_lists"": ""class TimeTableInitWithLists:\n    def time_init_lists(self):\n        Table([self.dat, self.dat, self.dat], names=[\""time\"", \""rate\"", \""error\""])\n\n    def setup(self):\n        self.dat = list(range(100_000))"", ""table.TimeTableInitWithMultiDimLists.time_init_float_1d"": ""class TimeTableInitWithMultiDimLists:\n    def time_init_float_1d(self):\n        Table([self.data_float_1d])\n\n    def setup(self):\n        np_data_int = np.arange(1_000_000, dtype=np.int64)\n        np_data_float = np_data_int.astype(np.float64)\n        np_data_str = np_data_int.astype(\""U\"")\n    \n        self.data_int_1d = np_data_int.tolist()\n    \n        self.data_int_3d = np_data_int.reshape(1000, 100, 10).tolist()\n    \n        self.data_int_masked_1d = self.data_int_1d.copy()\n        self.data_int_masked_1d[-1] = np.ma.masked\n    \n        self.data_int_masked_3d = self.data_int_3d.copy()\n        self.data_int_masked_3d[-1][-1][-1] = np.ma.masked\n    \n        self.data_float_1d = np_data_float.tolist()\n    \n        self.data_str_1d = np_data_str.tolist()\n    \n        self.data_str_masked_1d = self.data_str_1d.copy()\n        self.data_str_masked_1d[-1] = np.ma.masked"", ""table.TimeTableInitWithMultiDimLists.time_init_int_1d"": ""class TimeTableInitWithMultiDimLists:\n    def time_init_int_1d(self):\n        Table([self.data_int_1d])\n\n    def setup(self):\n        np_data_int = np.arange(1_000_000, dtype=np.int64)\n        np_data_float = np_data_int.astype(np.float64)\n        np_data_str = np_data_int.astype(\""U\"")\n    \n        self.data_int_1d = np_data_int.tolist()\n    \n        self.data_int_3d = np_data_int.reshape(1000, 100, 10).tolist()\n    \n        self.data_int_masked_1d = self.data_int_1d.copy()\n        self.data_int_masked_1d[-1] = np.ma.masked\n    \n        self.data_int_masked_3d = self.data_int_3d.copy()\n        self.data_int_masked_3d[-1][-1][-1] = np.ma.masked\n    \n        self.data_float_1d = np_data_float.tolist()\n    \n        self.data_str_1d = np_data_str.tolist()\n    \n        self.data_str_masked_1d = self.data_str_1d.copy()\n        self.data_str_masked_1d[-1] = np.ma.masked"", ""table.TimeTableInitWithMultiDimLists.time_init_int_masked_1d"": ""class TimeTableInitWithMultiDimLists:\n    def time_init_int_masked_1d(self):\n        Table([self.data_int_masked_1d])\n\n    def setup(self):\n        np_data_int = np.arange(1_000_000, dtype=np.int64)\n        np_data_float = np_data_int.astype(np.float64)\n        np_data_str = np_data_int.astype(\""U\"")\n    \n        self.data_int_1d = np_data_int.tolist()\n    \n        self.data_int_3d = np_data_int.reshape(1000, 100, 10).tolist()\n    \n        self.data_int_masked_1d = self.data_int_1d.copy()\n        self.data_int_masked_1d[-1] = np.ma.masked\n    \n        self.data_int_masked_3d = self.data_int_3d.copy()\n        self.data_int_masked_3d[-1][-1][-1] = np.ma.masked\n    \n        self.data_float_1d = np_data_float.tolist()\n    \n        self.data_str_1d = np_data_str.tolist()\n    \n        self.data_str_masked_1d = self.data_str_1d.copy()\n        self.data_str_masked_1d[-1] = np.ma.masked"", ""table.TimeTableInitWithMultiDimLists.time_init_str_masked_1d"": ""class TimeTableInitWithMultiDimLists:\n    def time_init_str_masked_1d(self):\n        Table([self.data_str_masked_1d])\n\n    def setup(self):\n        np_data_int = np.arange(1_000_000, dtype=np.int64)\n        np_data_float = np_data_int.astype(np.float64)\n        np_data_str = np_data_int.astype(\""U\"")\n    \n        self.data_int_1d = np_data_int.tolist()\n    \n        self.data_int_3d = np_data_int.reshape(1000, 100, 10).tolist()\n    \n        self.data_int_masked_1d = self.data_int_1d.copy()\n        self.data_int_masked_1d[-1] = np.ma.masked\n    \n        self.data_int_masked_3d = self.data_int_3d.copy()\n        self.data_int_masked_3d[-1][-1][-1] = np.ma.masked\n    \n        self.data_float_1d = np_data_float.tolist()\n    \n        self.data_str_1d = np_data_str.tolist()\n    \n        self.data_str_masked_1d = self.data_str_1d.copy()\n        self.data_str_masked_1d[-1] = np.ma.masked""}",table,time,-0.0463596674867283
29133,class-level,"terminus-2,gpt-5",1.0349332922636336,1.513804722063677,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,cluster.KMeansBenchmark,cluster,"{""cluster.KMeansBenchmark.time_transform"": ""class Transformer:\n    def time_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",cluster.KMeansBenchmark,time,-0.3386643775106389
29135,class-level,"terminus-2,gpt-5",1.026560358036901,1.024858002625557,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,ensemble.RandomForestClassifierBenchmark,ensemble,"{""ensemble.RandomForestClassifierBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RandomForestClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",ensemble.RandomForestClassifierBenchmark,time,0.0012039288623366359
29137,class-level,"terminus-2,gpt-5",0.9701632232173348,0.9771958841499164,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,neighbors.KNeighborsClassifierBenchmark,neighbors,"{""neighbors.KNeighborsClassifierBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KNeighborsClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""neighbors.KNeighborsClassifierBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KNeighborsClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",neighbors.KNeighborsClassifierBenchmark,time,-0.004973593304513148
29134,class-level,"terminus-2,gpt-5",2.336977261529984,3.093394649383032,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,ensemble.HistGradientBoostingClassifierBenchmark,ensemble,"{""ensemble.HistGradientBoostingClassifierBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass HistGradientBoostingClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",ensemble.HistGradientBoostingClassifierBenchmark,time,-0.5349486477037116
29136,class-level,"terminus-2,gpt-5",1.1394064492358398,1.010540831267532,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,linear_model.LogisticRegressionBenchmark,linear_model,"{""linear_model.LogisticRegressionBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LogisticRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""linear_model.LogisticRegressionBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LogisticRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",linear_model.LogisticRegressionBenchmark,time,0.09113551482907205
29960,module-level,"terminus-2,claude",1.1806303293192026,1.1165010169656513,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algorithms,algorithms,"{""algorithms.Duplicated.time_duplicated"": ""class Duplicated:\n    def time_duplicated(self, unique, keep, dtype):\n        self.idx.duplicated(keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Duplicated:\n    def setup(self, unique, keep, dtype):\n        N = 10**5\n        data = {\n            \""int\"": pd.Index(np.arange(N), dtype=\""int64\""),\n            \""uint\"": pd.Index(np.arange(N), dtype=\""uint64\""),\n            \""float\"": pd.Index(np.random.randn(N), dtype=\""float64\""),\n            \""string\"": tm.makeStringIndex(N),\n            \""datetime64[ns]\"": pd.date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n            \""datetime64[ns, tz]\"": pd.date_range(\n                \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n            ),\n        }[dtype]\n        if not unique:\n            data = data.repeat(5)\n        self.idx = data\n        # cache is_unique\n        self.idx.is_unique"", ""algorithms.DuplicatedMaskedArray.time_duplicated"": ""class DuplicatedMaskedArray:\n    def time_duplicated(self, unique, keep, dtype):\n        self.ser.duplicated(keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DuplicatedMaskedArray:\n    def setup(self, unique, keep, dtype):\n        N = 10**5\n        data = pd.Series(np.arange(N), dtype=dtype)\n        data[list(range(1, N, 100))] = pd.NA\n        if not unique:\n            data = data.repeat(5)\n        self.ser = data\n        # cache is_unique\n        self.ser.is_unique"", ""algorithms.Factorize.time_factorize"": ""class Factorize:\n    def time_factorize(self, unique, sort, dtype):\n        pd.factorize(self.data, sort=sort)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Factorize:\n    def setup(self, unique, sort, dtype):\n        N = 10**5\n        string_index = tm.makeStringIndex(N)\n        string_arrow = None\n        if dtype == \""string[pyarrow]\"":\n            try:\n                string_arrow = pd.array(string_index, dtype=\""string[pyarrow]\"")\n            except ImportError:\n                raise NotImplementedError\n    \n        data = {\n            \""int\"": pd.Index(np.arange(N), dtype=\""int64\""),\n            \""uint\"": pd.Index(np.arange(N), dtype=\""uint64\""),\n            \""float\"": pd.Index(np.random.randn(N), dtype=\""float64\""),\n            \""object\"": string_index,\n            \""datetime64[ns]\"": pd.date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n            \""datetime64[ns, tz]\"": pd.date_range(\n                \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n            ),\n            \""Int64\"": pd.array(np.arange(N), dtype=\""Int64\""),\n            \""boolean\"": pd.array(np.random.randint(0, 2, N), dtype=\""boolean\""),\n            \""string[pyarrow]\"": string_arrow,\n        }[dtype]\n        if not unique:\n            data = data.repeat(5)\n        self.data = data"", ""algorithms.Quantile.time_quantile"": ""class Quantile:\n    def time_quantile(self, quantile, interpolation, dtype):\n        self.idx.quantile(quantile, interpolation=interpolation)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, quantile, interpolation, dtype):\n        N = 10**5\n        data = {\n            \""int\"": np.arange(N),\n            \""uint\"": np.arange(N).astype(np.uint64),\n            \""float\"": np.random.randn(N),\n        }\n        self.idx = pd.Series(data[dtype].repeat(5))""}",algorithms,time,0.04535312047634466
29962,module-level,"terminus-2,claude",1.0893206618062383,0.9223674844574956,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic,arithmetic,"{""arithmetic.AddOverflowScalar.time_add_overflow_scalar"": ""class AddOverflowScalar:\n    def time_add_overflow_scalar(self, scalar):\n        checked_add_with_arr(self.arr, scalar)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass AddOverflowScalar:\n    def setup(self, scalar):\n        N = 10**6\n        self.arr = np.arange(N)"", ""arithmetic.ApplyIndex.time_apply_index"": ""class ApplyIndex:\n    def time_apply_index(self, offset):\n        self.rng + offset\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ApplyIndex:\n    def setup(self, offset):\n        N = 10000\n        rng = date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"")\n        self.rng = rng"", ""arithmetic.BinaryOpsMultiIndex.time_binary_op_multiindex"": ""class BinaryOpsMultiIndex:\n    def time_binary_op_multiindex(self, func):\n        getattr(self.df, func)(self.arg_df, level=0)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass BinaryOpsMultiIndex:\n    def setup(self, func):\n        array = date_range(\""20200101 00:00\"", \""20200102 0:00\"", freq=\""S\"")\n        level_0_names = [str(i) for i in range(30)]\n    \n        index = pd.MultiIndex.from_product([level_0_names, array])\n        column_names = [\""col_1\"", \""col_2\""]\n    \n        self.df = DataFrame(\n            np.random.rand(len(index), 2), index=index, columns=column_names\n        )\n    \n        self.arg_df = DataFrame(\n            np.random.randint(1, 10, (len(level_0_names), 2)),\n            index=level_0_names,\n            columns=column_names,\n        )"", ""arithmetic.DateInferOps.time_add_timedeltas"": ""class DateInferOps:\n    def time_add_timedeltas(self, df):\n        df[\""timedelta\""] + df[\""timedelta\""]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DateInferOps:\n    def setup_cache(self):\n        N = 5 * 10**5\n        df = DataFrame({\""datetime64\"": np.arange(N).astype(\""datetime64[ms]\"")})\n        df[\""timedelta\""] = df[\""datetime64\""] - df[\""datetime64\""]\n        return df"", ""arithmetic.FrameWithFrameWide.time_op_different_blocks"": ""class FrameWithFrameWide:\n    def time_op_different_blocks(self, op, shape):\n        # blocks (and dtypes) are not aligned\n        op(self.left, self.right)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FrameWithFrameWide:\n    def setup(self, op, shape):\n        # we choose dtypes so as to make the blocks\n        #  a) not perfectly match between right and left\n        #  b) appreciably bigger than single columns\n        n_rows, n_cols = shape\n    \n        if op is operator.floordiv:\n            # floordiv is much slower than the other operations -> use less data\n            n_rows = n_rows // 10\n    \n        # construct dataframe with 2 blocks\n        arr1 = np.random.randn(n_rows, n_cols // 2).astype(\""f8\"")\n        arr2 = np.random.randn(n_rows, n_cols // 2).astype(\""f4\"")\n        df = pd.concat([DataFrame(arr1), DataFrame(arr2)], axis=1, ignore_index=True)\n        # should already be the case, but just to be sure\n        df._consolidate_inplace()\n    \n        # TODO: GH#33198 the setting here shouldn't need two steps\n        arr1 = np.random.randn(n_rows, max(n_cols // 4, 3)).astype(\""f8\"")\n        arr2 = np.random.randn(n_rows, n_cols // 2).astype(\""i8\"")\n        arr3 = np.random.randn(n_rows, n_cols // 4).astype(\""f8\"")\n        df2 = pd.concat(\n            [DataFrame(arr1), DataFrame(arr2), DataFrame(arr3)],\n            axis=1,\n            ignore_index=True,\n        )\n        # should already be the case, but just to be sure\n        df2._consolidate_inplace()\n    \n        self.left = df\n        self.right = df2"", ""arithmetic.IntFrameWithScalar.time_frame_op_with_scalar"": ""class IntFrameWithScalar:\n    def time_frame_op_with_scalar(self, dtype, scalar, op):\n        op(self.df, scalar)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IntFrameWithScalar:\n    def setup(self, dtype, scalar, op):\n        arr = np.random.randn(20000, 100)\n        self.df = DataFrame(arr.astype(dtype))"", ""arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0"": ""class MixedFrameWithSeriesAxis:\n    def time_frame_op_with_series_axis0(self, opname):\n        getattr(self.df, opname)(self.ser, axis=0)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MixedFrameWithSeriesAxis:\n    def setup(self, opname):\n        arr = np.arange(10**6).reshape(1000, -1)\n        df = DataFrame(arr)\n        df[\""C\""] = 1.0\n        self.df = df\n        self.ser = df[0]\n        self.row = df.iloc[0]"", ""arithmetic.NumericInferOps.time_divide"": ""class NumericInferOps:\n    def time_divide(self, dtype):\n        self.df[\""A\""] / self.df[\""B\""]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericInferOps:\n    def setup(self, dtype):\n        N = 5 * 10**5\n        self.df = DataFrame(\n            {\""A\"": np.arange(N).astype(dtype), \""B\"": np.arange(N).astype(dtype)}\n        )"", ""arithmetic.NumericInferOps.time_modulo"": ""class NumericInferOps:\n    def time_modulo(self, dtype):\n        self.df[\""A\""] % self.df[\""B\""]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericInferOps:\n    def setup(self, dtype):\n        N = 5 * 10**5\n        self.df = DataFrame(\n            {\""A\"": np.arange(N).astype(dtype), \""B\"": np.arange(N).astype(dtype)}\n        )"", ""arithmetic.NumericInferOps.time_subtract"": ""class NumericInferOps:\n    def time_subtract(self, dtype):\n        self.df[\""A\""] - self.df[\""B\""]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericInferOps:\n    def setup(self, dtype):\n        N = 5 * 10**5\n        self.df = DataFrame(\n            {\""A\"": np.arange(N).astype(dtype), \""B\"": np.arange(N).astype(dtype)}\n        )"", ""arithmetic.OffsetArrayArithmetic.time_add_dti_offset"": ""class OffsetArrayArithmetic:\n    def time_add_dti_offset(self, offset):\n        with warnings.catch_warnings(record=True):\n            self.rng + offset\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass OffsetArrayArithmetic:\n    def setup(self, offset):\n        N = 10000\n        rng = date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"")\n        self.rng = rng\n        self.ser = Series(rng)"", ""arithmetic.OffsetArrayArithmetic.time_add_series_offset"": ""class OffsetArrayArithmetic:\n    def time_add_series_offset(self, offset):\n        with warnings.catch_warnings(record=True):\n            self.ser + offset\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass OffsetArrayArithmetic:\n    def setup(self, offset):\n        N = 10000\n        rng = date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"")\n        self.rng = rng\n        self.ser = Series(rng)"", ""arithmetic.Ops2.time_frame_float_mod"": ""class Ops2:\n    def time_frame_float_mod(self):\n        self.df % self.df2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Ops2:\n    def setup(self):\n        N = 10**3\n        self.df = DataFrame(np.random.randn(N, N))\n        self.df2 = DataFrame(np.random.randn(N, N))\n    \n        self.df_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n        self.df2_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n    \n        self.s = Series(np.random.randn(N))""}",arithmetic,time,0.11807155399486755
29969,module-level,"terminus-2,claude",1.1422100858782998,1.0604397121487763,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor,frame_ctor,"{""frame_ctor.FromArrays.time_frame_from_arrays_float"": ""class FromArrays:\n    def time_frame_from_arrays_float(self):\n        self.df = DataFrame._from_arrays(\n            self.float_arrays,\n            index=self.index,\n            columns=self.columns,\n            verify_integrity=False,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromArrays:\n    def setup(self):\n        N_rows = 1000\n        N_cols = 1000\n        self.float_arrays = [np.random.randn(N_rows) for _ in range(N_cols)]\n        self.sparse_arrays = [\n            pd.arrays.SparseArray(np.random.randint(0, 2, N_rows), dtype=\""float64\"")\n            for _ in range(N_cols)\n        ]\n        self.int_arrays = [\n            pd.array(np.random.randint(1000, size=N_rows), dtype=\""Int64\"")\n            for _ in range(N_cols)\n        ]\n        self.index = pd.Index(range(N_rows))\n        self.columns = pd.Index(range(N_cols))"", ""frame_ctor.FromArrays.time_frame_from_arrays_int"": ""class FromArrays:\n    def time_frame_from_arrays_int(self):\n        self.df = DataFrame._from_arrays(\n            self.int_arrays,\n            index=self.index,\n            columns=self.columns,\n            verify_integrity=False,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromArrays:\n    def setup(self):\n        N_rows = 1000\n        N_cols = 1000\n        self.float_arrays = [np.random.randn(N_rows) for _ in range(N_cols)]\n        self.sparse_arrays = [\n            pd.arrays.SparseArray(np.random.randint(0, 2, N_rows), dtype=\""float64\"")\n            for _ in range(N_cols)\n        ]\n        self.int_arrays = [\n            pd.array(np.random.randint(1000, size=N_rows), dtype=\""Int64\"")\n            for _ in range(N_cols)\n        ]\n        self.index = pd.Index(range(N_rows))\n        self.columns = pd.Index(range(N_cols))"", ""frame_ctor.FromArrays.time_frame_from_arrays_sparse"": ""class FromArrays:\n    def time_frame_from_arrays_sparse(self):\n        self.df = DataFrame._from_arrays(\n            self.sparse_arrays,\n            index=self.index,\n            columns=self.columns,\n            verify_integrity=False,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromArrays:\n    def setup(self):\n        N_rows = 1000\n        N_cols = 1000\n        self.float_arrays = [np.random.randn(N_rows) for _ in range(N_cols)]\n        self.sparse_arrays = [\n            pd.arrays.SparseArray(np.random.randint(0, 2, N_rows), dtype=\""float64\"")\n            for _ in range(N_cols)\n        ]\n        self.int_arrays = [\n            pd.array(np.random.randint(1000, size=N_rows), dtype=\""Int64\"")\n            for _ in range(N_cols)\n        ]\n        self.index = pd.Index(range(N_rows))\n        self.columns = pd.Index(range(N_cols))"", ""frame_ctor.FromDicts.time_dict_of_categoricals"": ""class FromDicts:\n    def time_dict_of_categoricals(self):\n        # dict of arrays that we won't consolidate\n        DataFrame(self.dict_of_categoricals)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDicts:\n    def setup(self):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n        frame = DataFrame(np.random.randn(N, K), index=self.index, columns=self.columns)\n        self.data = frame.to_dict()\n        self.dict_list = frame.to_dict(orient=\""records\"")\n        self.data2 = {i: {j: float(j) for j in range(100)} for i in range(2000)}\n    \n        # arrays which we won't consolidate\n        self.dict_of_categoricals = {i: Categorical(np.arange(N)) for i in range(K)}"", ""frame_ctor.FromDicts.time_list_of_dict"": ""class FromDicts:\n    def time_list_of_dict(self):\n        DataFrame(self.dict_list)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDicts:\n    def setup(self):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n        frame = DataFrame(np.random.randn(N, K), index=self.index, columns=self.columns)\n        self.data = frame.to_dict()\n        self.dict_list = frame.to_dict(orient=\""records\"")\n        self.data2 = {i: {j: float(j) for j in range(100)} for i in range(2000)}\n    \n        # arrays which we won't consolidate\n        self.dict_of_categoricals = {i: Categorical(np.arange(N)) for i in range(K)}"", ""frame_ctor.FromDicts.time_nested_dict_int64"": ""class FromDicts:\n    def time_nested_dict_int64(self):\n        # nested dict, integer indexes, regression described in #621\n        DataFrame(self.data2)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDicts:\n    def setup(self):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n        frame = DataFrame(np.random.randn(N, K), index=self.index, columns=self.columns)\n        self.data = frame.to_dict()\n        self.dict_list = frame.to_dict(orient=\""records\"")\n        self.data2 = {i: {j: float(j) for j in range(100)} for i in range(2000)}\n    \n        # arrays which we won't consolidate\n        self.dict_of_categoricals = {i: Categorical(np.arange(N)) for i in range(K)}"", ""frame_ctor.FromLists.time_frame_from_lists"": ""class FromLists:\n    def time_frame_from_lists(self):\n        self.df = DataFrame(self.data)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromLists:\n    def setup(self):\n        N = 1000\n        M = 100\n        self.data = [list(range(M)) for i in range(N)]"", ""frame_ctor.FromRecords.time_frame_from_records_generator"": ""class FromRecords:\n    def time_frame_from_records_generator(self, nrows):\n        # issue-6700\n        self.df = DataFrame.from_records(self.gen, nrows=nrows)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromRecords:\n    def setup(self, nrows):\n        N = 100000\n        self.gen = ((x, (x * 20), (x * 100)) for x in range(N))""}",frame_ctor,time,0.057829118620596504
29964,module-level,"terminus-2,claude",0.9898216294453116,0.9878651791606782,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,attrs_caching,attrs_caching,"{""attrs_caching.SeriesArrayAttribute.time_extract_array"": ""class SeriesArrayAttribute:\n    def time_extract_array(self, dtype):\n        extract_array(self.series)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesArrayAttribute:\n    def setup(self, dtype):\n        if dtype == \""numeric\"":\n            self.series = pd.Series([1, 2, 3])\n        elif dtype == \""object\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=object)\n        elif dtype == \""category\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=\""category\"")\n        elif dtype == \""datetime64\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3))\n        elif dtype == \""datetime64tz\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3, tz=\""UTC\""))""}",attrs_caching,time,0.0013836282069543328
29973,module-level,"terminus-2,claude",1.1464855640805325,1.0675075800877531,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,hash_functions,hash_functions,"{""hash_functions.NumericSeriesIndexing.time_loc_slice"": ""class NumericSeriesIndexing:\n    def time_loc_slice(self, index, N):\n        # trigger building of mapping\n        self.data.loc[:800]\n\n    def setup(self, dtype, N):\n        vals = np.array(list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype)\n        indices = pd.Index(vals)\n        self.data = pd.Series(np.arange(N), index=indices)"", ""hash_functions.NumericSeriesIndexingShuffled.time_loc_slice"": ""class NumericSeriesIndexingShuffled:\n    def time_loc_slice(self, index, N):\n        # trigger building of mapping\n        self.data.loc[:800]\n\n    def setup(self, dtype, N):\n        vals = np.array(list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype)\n        np.random.shuffle(vals)\n        indices = pd.Index(vals)\n        self.data = pd.Series(np.arange(N), index=indices)"", ""hash_functions.Unique.time_unique"": ""class Unique:\n    def time_unique(self, exponent):\n        pd.unique(self.ser_unique)\n\n    def setup(self, dtype):\n        self.ser = pd.Series(([1, pd.NA, 2] + list(range(100_000))) * 3, dtype=dtype)\n        self.ser_unique = pd.Series(list(range(300_000)) + [pd.NA], dtype=dtype)"", ""hash_functions.Unique.time_unique_with_duplicates"": ""class Unique:\n    def time_unique_with_duplicates(self, exponent):\n        pd.unique(self.ser)\n\n    def setup(self, dtype):\n        self.ser = pd.Series(([1, pd.NA, 2] + list(range(100_000))) * 3, dtype=dtype)\n        self.ser_unique = pd.Series(list(range(300_000)) + [pd.NA], dtype=dtype)"", ""hash_functions.UniqueAndFactorizeArange.time_factorize"": ""class UniqueAndFactorizeArange:\n    def time_factorize(self, exponent):\n        pd.factorize(self.a2)\n\n    def setup(self, exponent):\n        a = np.arange(10**4, dtype=\""float64\"")\n        self.a2 = (a + 10**exponent).repeat(100)"", ""hash_functions.UniqueForLargePyObjectInts.time_unique"": ""class UniqueForLargePyObjectInts:\n    def time_unique(self):\n        pd.unique(self.arr)\n\n    def setup(self):\n        lst = [x << 32 for x in range(5000)]\n        self.arr = np.array(lst, dtype=np.object_)""}",hash_functions,time,0.05585430268230505
29972,module-level,"terminus-2,claude",1.0772061892854623,1.0599352035224512,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby,groupby,"{""groupby.Apply.time_copy_function_multi_col"": ""class Apply:\n    def time_copy_function_multi_col(self, factor):\n        self.df.groupby([\""key\"", \""key2\""]).apply(self.df_copy_function)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, factor):\n        N = 10**factor\n        # two cases:\n        # - small groups: small data (N**4) + many labels (2000) -> average group\n        #   size of 5 (-> larger overhead of slicing method)\n        # - larger groups: larger data (N**5) + fewer labels (20) -> average group\n        #   size of 5000\n        labels = np.random.randint(0, 2000 if factor == 4 else 20, size=N)\n        labels2 = np.random.randint(0, 3, size=N)\n        df = DataFrame(\n            {\n                \""key\"": labels,\n                \""key2\"": labels2,\n                \""value1\"": np.random.randn(N),\n                \""value2\"": [\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (N // 4),\n            }\n        )\n        self.df = df"", ""groupby.Apply.time_copy_overhead_single_col"": ""class Apply:\n    def time_copy_overhead_single_col(self, factor):\n        self.df.groupby(\""key\"").apply(self.df_copy_function)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, factor):\n        N = 10**factor\n        # two cases:\n        # - small groups: small data (N**4) + many labels (2000) -> average group\n        #   size of 5 (-> larger overhead of slicing method)\n        # - larger groups: larger data (N**5) + fewer labels (20) -> average group\n        #   size of 5000\n        labels = np.random.randint(0, 2000 if factor == 4 else 20, size=N)\n        labels2 = np.random.randint(0, 3, size=N)\n        df = DataFrame(\n            {\n                \""key\"": labels,\n                \""key2\"": labels2,\n                \""value1\"": np.random.randn(N),\n                \""value2\"": [\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (N // 4),\n            }\n        )\n        self.df = df"", ""groupby.Apply.time_scalar_function_multi_col"": ""class Apply:\n    def time_scalar_function_multi_col(self, factor):\n        self.df.groupby([\""key\"", \""key2\""]).apply(lambda x: 1)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, factor):\n        N = 10**factor\n        # two cases:\n        # - small groups: small data (N**4) + many labels (2000) -> average group\n        #   size of 5 (-> larger overhead of slicing method)\n        # - larger groups: larger data (N**5) + fewer labels (20) -> average group\n        #   size of 5000\n        labels = np.random.randint(0, 2000 if factor == 4 else 20, size=N)\n        labels2 = np.random.randint(0, 3, size=N)\n        df = DataFrame(\n            {\n                \""key\"": labels,\n                \""key2\"": labels2,\n                \""value1\"": np.random.randn(N),\n                \""value2\"": [\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (N // 4),\n            }\n        )\n        self.df = df"", ""groupby.Apply.time_scalar_function_single_col"": ""class Apply:\n    def time_scalar_function_single_col(self, factor):\n        self.df.groupby(\""key\"").apply(lambda x: 1)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, factor):\n        N = 10**factor\n        # two cases:\n        # - small groups: small data (N**4) + many labels (2000) -> average group\n        #   size of 5 (-> larger overhead of slicing method)\n        # - larger groups: larger data (N**5) + fewer labels (20) -> average group\n        #   size of 5000\n        labels = np.random.randint(0, 2000 if factor == 4 else 20, size=N)\n        labels2 = np.random.randint(0, 3, size=N)\n        df = DataFrame(\n            {\n                \""key\"": labels,\n                \""key2\"": labels2,\n                \""value1\"": np.random.randn(N),\n                \""value2\"": [\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (N // 4),\n            }\n        )\n        self.df = df"", ""groupby.ApplyNonUniqueUnsortedIndex.time_groupby_apply_non_unique_unsorted_index"": ""class ApplyNonUniqueUnsortedIndex:\n    def time_groupby_apply_non_unique_unsorted_index(self):\n        self.df.groupby(\""key\"", group_keys=False).apply(lambda x: x)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ApplyNonUniqueUnsortedIndex:\n    def setup(self):\n        # GH 46527\n        # unsorted and non-unique index\n        idx = np.arange(100)[::-1]\n        idx = Index(np.repeat(idx, 200), name=\""key\"")\n        self.df = DataFrame(np.random.randn(len(idx), 10), index=idx)"", ""groupby.Categories.time_groupby_extra_cat_nosort"": ""class Categories:\n    def time_groupby_extra_cat_nosort(self):\n        self.df_extra_cat.groupby(\""a\"", sort=False)[\""b\""].count()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Categories:\n    def setup(self):\n        N = 10**5\n        arr = np.random.random(N)\n        data = {\""a\"": Categorical(np.random.randint(10000, size=N)), \""b\"": arr}\n        self.df = DataFrame(data)\n        data = {\n            \""a\"": Categorical(np.random.randint(10000, size=N), ordered=True),\n            \""b\"": arr,\n        }\n        self.df_ordered = DataFrame(data)\n        data = {\n            \""a\"": Categorical(\n                np.random.randint(100, size=N), categories=np.arange(10000)\n            ),\n            \""b\"": arr,\n        }\n        self.df_extra_cat = DataFrame(data)"", ""groupby.Categories.time_groupby_extra_cat_sort"": ""class Categories:\n    def time_groupby_extra_cat_sort(self):\n        self.df_extra_cat.groupby(\""a\"")[\""b\""].count()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Categories:\n    def setup(self):\n        N = 10**5\n        arr = np.random.random(N)\n        data = {\""a\"": Categorical(np.random.randint(10000, size=N)), \""b\"": arr}\n        self.df = DataFrame(data)\n        data = {\n            \""a\"": Categorical(np.random.randint(10000, size=N), ordered=True),\n            \""b\"": arr,\n        }\n        self.df_ordered = DataFrame(data)\n        data = {\n            \""a\"": Categorical(\n                np.random.randint(100, size=N), categories=np.arange(10000)\n            ),\n            \""b\"": arr,\n        }\n        self.df_extra_cat = DataFrame(data)"", ""groupby.Categories.time_groupby_nosort"": ""class Categories:\n    def time_groupby_nosort(self):\n        self.df.groupby(\""a\"", sort=False)[\""b\""].count()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Categories:\n    def setup(self):\n        N = 10**5\n        arr = np.random.random(N)\n        data = {\""a\"": Categorical(np.random.randint(10000, size=N)), \""b\"": arr}\n        self.df = DataFrame(data)\n        data = {\n            \""a\"": Categorical(np.random.randint(10000, size=N), ordered=True),\n            \""b\"": arr,\n        }\n        self.df_ordered = DataFrame(data)\n        data = {\n            \""a\"": Categorical(\n                np.random.randint(100, size=N), categories=np.arange(10000)\n            ),\n            \""b\"": arr,\n        }\n        self.df_extra_cat = DataFrame(data)"", ""groupby.Categories.time_groupby_ordered_sort"": ""class Categories:\n    def time_groupby_ordered_sort(self):\n        self.df_ordered.groupby(\""a\"")[\""b\""].count()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Categories:\n    def setup(self):\n        N = 10**5\n        arr = np.random.random(N)\n        data = {\""a\"": Categorical(np.random.randint(10000, size=N)), \""b\"": arr}\n        self.df = DataFrame(data)\n        data = {\n            \""a\"": Categorical(np.random.randint(10000, size=N), ordered=True),\n            \""b\"": arr,\n        }\n        self.df_ordered = DataFrame(data)\n        data = {\n            \""a\"": Categorical(\n                np.random.randint(100, size=N), categories=np.arange(10000)\n            ),\n            \""b\"": arr,\n        }\n        self.df_extra_cat = DataFrame(data)"", ""groupby.Cumulative.time_frame_transform"": ""class Cumulative:\n    def time_frame_transform(self, dtype, method, with_nans):\n        self.df.groupby(\""key\"").transform(method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cumulative:\n    def setup(self, dtype, method, with_nans):\n        if with_nans and dtype == \""int64\"":\n            raise NotImplementedError(\""Construction of df would raise\"")\n    \n        N = 500_000\n        keys = np.random.randint(0, 100, size=N)\n        vals = np.random.randint(-10, 10, (N, 5))\n    \n        if with_nans:\n            null_vals = vals.astype(float, copy=True)\n            null_vals[::2, :] = np.nan\n            null_vals[::3, :] = np.nan\n            df = DataFrame(null_vals, columns=list(\""abcde\""), dtype=dtype)\n            df[\""key\""] = keys\n            self.df = df\n        else:\n            df = DataFrame(vals, columns=list(\""abcde\"")).astype(dtype, copy=False)\n            df[\""key\""] = keys\n            self.df = df"", ""groupby.Datelike.time_sum"": ""class Datelike:\n    def time_sum(self, grouper):\n        self.df.groupby(self.grouper).sum()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Datelike:\n    def setup(self, grouper):\n        N = 10**4\n        rng_map = {\n            \""period_range\"": period_range,\n            \""date_range\"": date_range,\n            \""date_range_tz\"": partial(date_range, tz=\""US/Central\""),\n        }\n        self.grouper = rng_map[grouper](\""1900-01-01\"", freq=\""D\"", periods=N)\n        self.df = DataFrame(np.random.randn(10**4, 2))"", ""groupby.FillNA.time_df_bfill"": ""class FillNA:\n    def time_df_bfill(self):\n        self.df.groupby(\""group\"").fillna(method=\""bfill\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FillNA:\n    def setup(self):\n        N = 100\n        self.df = DataFrame(\n            {\""group\"": [1] * N + [2] * N, \""value\"": [np.nan, 1.0] * N}\n        ).set_index(\""group\"")"", ""groupby.FillNA.time_df_ffill"": ""class FillNA:\n    def time_df_ffill(self):\n        self.df.groupby(\""group\"").fillna(method=\""ffill\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FillNA:\n    def setup(self):\n        N = 100\n        self.df = DataFrame(\n            {\""group\"": [1] * N + [2] * N, \""value\"": [np.nan, 1.0] * N}\n        ).set_index(\""group\"")"", ""groupby.GroupByCythonAggEaDtypes.time_frame_agg"": ""class GroupByCythonAggEaDtypes:\n    def time_frame_agg(self, dtype, method):\n        self.df.groupby(\""key\"").agg(method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupByCythonAggEaDtypes:\n    def setup(self, dtype, method):\n        N = 1_000_000\n        df = DataFrame(\n            np.random.randint(0, high=100, size=(N, 10)),\n            columns=list(\""abcdefghij\""),\n            dtype=dtype,\n        )\n        df.loc[list(range(1, N, 5)), list(\""abcdefghij\"")] = NA\n        df[\""key\""] = np.random.randint(0, 100, size=N)\n        self.df = df"", ""groupby.GroupByMethods.time_dtype_as_field"": ""class GroupByMethods:\n    def time_dtype_as_field(self, dtype, method, application, ncols):\n        self.as_field_method()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupByMethods:\n    def setup(self, dtype, method, application, ncols):\n        if method in method_blocklist.get(dtype, {}):\n            raise NotImplementedError  # skip benchmark\n    \n        if ncols != 1 and method in [\""value_counts\"", \""unique\""]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if application == \""transformation\"" and method in [\n            \""describe\"",\n            \""head\"",\n            \""tail\"",\n            \""unique\"",\n            \""value_counts\"",\n            \""size\"",\n        ]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if method == \""describe\"":\n            ngroups = 20\n        elif method == \""skew\"":\n            ngroups = 100\n        else:\n            ngroups = 1000\n        size = ngroups * 2\n        rng = np.arange(ngroups).reshape(-1, 1)\n        rng = np.broadcast_to(rng, (len(rng), ncols))\n        taker = np.random.randint(0, ngroups, size=size)\n        values = rng.take(taker, axis=0)\n        if dtype == \""int\"":\n            key = np.random.randint(0, size, size=size)\n        elif dtype in (\""int16\"", \""uint\""):\n            key = np.random.randint(0, size, size=size, dtype=dtype)\n        elif dtype == \""float\"":\n            key = np.concatenate(\n                [np.random.random(ngroups) * 0.1, np.random.random(ngroups) * 10.0]\n            )\n        elif dtype == \""object\"":\n            key = [\""foo\""] * size\n        elif dtype == \""datetime\"":\n            key = date_range(\""1/1/2011\"", periods=size, freq=\""s\"")\n    \n        cols = [f\""values{n}\"" for n in range(ncols)]\n        df = DataFrame(values, columns=cols)\n        df[\""key\""] = key\n    \n        if len(cols) == 1:\n            cols = cols[0]\n    \n        if application == \""transformation\"":\n            self.as_group_method = lambda: df.groupby(\""key\"")[cols].transform(method)\n            self.as_field_method = lambda: df.groupby(cols)[\""key\""].transform(method)\n        else:\n            self.as_group_method = getattr(df.groupby(\""key\"")[cols], method)\n            self.as_field_method = getattr(df.groupby(cols)[\""key\""], method)"", ""groupby.GroupByMethods.time_dtype_as_group"": ""class GroupByMethods:\n    def time_dtype_as_group(self, dtype, method, application, ncols):\n        self.as_group_method()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupByMethods:\n    def setup(self, dtype, method, application, ncols):\n        if method in method_blocklist.get(dtype, {}):\n            raise NotImplementedError  # skip benchmark\n    \n        if ncols != 1 and method in [\""value_counts\"", \""unique\""]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if application == \""transformation\"" and method in [\n            \""describe\"",\n            \""head\"",\n            \""tail\"",\n            \""unique\"",\n            \""value_counts\"",\n            \""size\"",\n        ]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if method == \""describe\"":\n            ngroups = 20\n        elif method == \""skew\"":\n            ngroups = 100\n        else:\n            ngroups = 1000\n        size = ngroups * 2\n        rng = np.arange(ngroups).reshape(-1, 1)\n        rng = np.broadcast_to(rng, (len(rng), ncols))\n        taker = np.random.randint(0, ngroups, size=size)\n        values = rng.take(taker, axis=0)\n        if dtype == \""int\"":\n            key = np.random.randint(0, size, size=size)\n        elif dtype in (\""int16\"", \""uint\""):\n            key = np.random.randint(0, size, size=size, dtype=dtype)\n        elif dtype == \""float\"":\n            key = np.concatenate(\n                [np.random.random(ngroups) * 0.1, np.random.random(ngroups) * 10.0]\n            )\n        elif dtype == \""object\"":\n            key = [\""foo\""] * size\n        elif dtype == \""datetime\"":\n            key = date_range(\""1/1/2011\"", periods=size, freq=\""s\"")\n    \n        cols = [f\""values{n}\"" for n in range(ncols)]\n        df = DataFrame(values, columns=cols)\n        df[\""key\""] = key\n    \n        if len(cols) == 1:\n            cols = cols[0]\n    \n        if application == \""transformation\"":\n            self.as_group_method = lambda: df.groupby(\""key\"")[cols].transform(method)\n            self.as_field_method = lambda: df.groupby(cols)[\""key\""].transform(method)\n        else:\n            self.as_group_method = getattr(df.groupby(\""key\"")[cols], method)\n            self.as_field_method = getattr(df.groupby(cols)[\""key\""], method)"", ""groupby.GroupManyLabels.time_sum"": ""class GroupManyLabels:\n    def time_sum(self, ncols):\n        self.df.groupby(self.labels).sum()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupManyLabels:\n    def setup(self, ncols):\n        N = 1000\n        data = np.random.randn(N, ncols)\n        self.labels = np.random.randint(0, 100, size=N)\n        self.df = DataFrame(data)"", ""groupby.GroupStrings.time_multi_columns"": ""class GroupStrings:\n    def time_multi_columns(self):\n        self.df.groupby(list(\""abcd\"")).max()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupStrings:\n    def setup(self):\n        n = 2 * 10**5\n        alpha = list(map(\""\"".join, product(ascii_letters, repeat=4)))\n        data = np.random.choice(alpha, (n // 5, 4), replace=False)\n        data = np.repeat(data, 5, axis=0)\n        self.df = DataFrame(data, columns=list(\""abcd\""))\n        self.df[\""joe\""] = (np.random.randn(len(self.df)) * 10).round(3)\n        self.df = self.df.sample(frac=1).reset_index(drop=True)"", ""groupby.Int64.time_overflow"": ""class Int64:\n    def time_overflow(self):\n        self.df.groupby(self.cols).max()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Int64:\n    def setup(self):\n        arr = np.random.randint(-1 << 12, 1 << 12, (1 << 17, 5))\n        i = np.random.choice(len(arr), len(arr) * 5)\n        arr = np.vstack((arr, arr[i]))\n        i = np.random.permutation(len(arr))\n        arr = arr[i]\n        self.cols = list(\""abcde\"")\n        self.df = DataFrame(arr, columns=self.cols)\n        self.df[\""jim\""], self.df[\""joe\""] = np.random.randn(2, len(self.df)) * 10"", ""groupby.MultiColumn.time_lambda_sum"": ""class MultiColumn:\n    def time_lambda_sum(self, df):\n        df.groupby([\""key1\"", \""key2\""]).agg(lambda x: x.values.sum())\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MultiColumn:\n    def setup_cache(self):\n        N = 10**5\n        key1 = np.tile(np.arange(100, dtype=object), 1000)\n        key2 = key1.copy()\n        np.random.shuffle(key1)\n        np.random.shuffle(key2)\n        df = DataFrame(\n            {\n                \""key1\"": key1,\n                \""key2\"": key2,\n                \""data1\"": np.random.randn(N),\n                \""data2\"": np.random.randn(N),\n            }\n        )\n        return df"", ""groupby.Nth.time_groupby_nth_all"": ""class Nth:\n    def time_groupby_nth_all(self, dtype):\n        self.df.groupby(\""key\"").nth(0, dropna=\""all\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Nth:\n    def setup(self, dtype):\n        N = 10**5\n        # with datetimes (GH7555)\n        if dtype == \""datetime\"":\n            values = date_range(\""1/1/2011\"", periods=N, freq=\""s\"")\n        elif dtype == \""object\"":\n            values = [\""foo\""] * N\n        else:\n            values = np.arange(N).astype(dtype)\n    \n        key = np.arange(N)\n        self.df = DataFrame({\""key\"": key, \""values\"": values})\n        self.df.iloc[1, 1] = np.nan  # insert missing data"", ""groupby.Nth.time_series_nth"": ""class Nth:\n    def time_series_nth(self, dtype):\n        self.df[\""values\""].groupby(self.df[\""key\""]).nth(0)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Nth:\n    def setup(self, dtype):\n        N = 10**5\n        # with datetimes (GH7555)\n        if dtype == \""datetime\"":\n            values = date_range(\""1/1/2011\"", periods=N, freq=\""s\"")\n        elif dtype == \""object\"":\n            values = [\""foo\""] * N\n        else:\n            values = np.arange(N).astype(dtype)\n    \n        key = np.arange(N)\n        self.df = DataFrame({\""key\"": key, \""values\"": values})\n        self.df.iloc[1, 1] = np.nan  # insert missing data"", ""groupby.Nth.time_series_nth_all"": ""class Nth:\n    def time_series_nth_all(self, dtype):\n        self.df[\""values\""].groupby(self.df[\""key\""]).nth(0, dropna=\""all\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Nth:\n    def setup(self, dtype):\n        N = 10**5\n        # with datetimes (GH7555)\n        if dtype == \""datetime\"":\n            values = date_range(\""1/1/2011\"", periods=N, freq=\""s\"")\n        elif dtype == \""object\"":\n            values = [\""foo\""] * N\n        else:\n            values = np.arange(N).astype(dtype)\n    \n        key = np.arange(N)\n        self.df = DataFrame({\""key\"": key, \""values\"": values})\n        self.df.iloc[1, 1] = np.nan  # insert missing data"", ""groupby.Nth.time_series_nth_any"": ""class Nth:\n    def time_series_nth_any(self, dtype):\n        self.df[\""values\""].groupby(self.df[\""key\""]).nth(0, dropna=\""any\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Nth:\n    def setup(self, dtype):\n        N = 10**5\n        # with datetimes (GH7555)\n        if dtype == \""datetime\"":\n            values = date_range(\""1/1/2011\"", periods=N, freq=\""s\"")\n        elif dtype == \""object\"":\n            values = [\""foo\""] * N\n        else:\n            values = np.arange(N).astype(dtype)\n    \n        key = np.arange(N)\n        self.df = DataFrame({\""key\"": key, \""values\"": values})\n        self.df.iloc[1, 1] = np.nan  # insert missing data"", ""groupby.RankWithTies.time_rank_ties"": ""class RankWithTies:\n    def time_rank_ties(self, dtype, tie_method):\n        self.df.groupby(\""key\"").rank(method=tie_method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass RankWithTies:\n    def setup(self, dtype, tie_method):\n        N = 10**4\n        if dtype == \""datetime64\"":\n            data = np.array([Timestamp(\""2011/01/01\"")] * N, dtype=dtype)\n        else:\n            data = np.array([1] * N, dtype=dtype)\n        self.df = DataFrame({\""values\"": data, \""key\"": [\""foo\""] * N})"", ""groupby.Resample.time_resample"": ""class Resample:\n    def time_resample(self):\n        self.df.groupby(level=\""groups\"").resample(\""10s\"", on=\""timedeltas\"").mean()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Resample:\n    def setup(self):\n        num_timedeltas = 20_000\n        num_groups = 3\n    \n        index = MultiIndex.from_product(\n            [\n                np.arange(num_groups),\n                to_timedelta(np.arange(num_timedeltas), unit=\""s\""),\n            ],\n            names=[\""groups\"", \""timedeltas\""],\n        )\n        data = np.random.randint(0, 1000, size=(len(index)))\n    \n        self.df = DataFrame(data, index=index).reset_index(\""timedeltas\"")\n        self.df_multiindex = DataFrame(data, index=index)"", ""groupby.Resample.time_resample_multiindex"": ""class Resample:\n    def time_resample_multiindex(self):\n        self.df_multiindex.groupby(level=\""groups\"").resample(\n            \""10s\"", level=\""timedeltas\""\n        ).mean()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Resample:\n    def setup(self):\n        num_timedeltas = 20_000\n        num_groups = 3\n    \n        index = MultiIndex.from_product(\n            [\n                np.arange(num_groups),\n                to_timedelta(np.arange(num_timedeltas), unit=\""s\""),\n            ],\n            names=[\""groups\"", \""timedeltas\""],\n        )\n        data = np.random.randint(0, 1000, size=(len(index)))\n    \n        self.df = DataFrame(data, index=index).reset_index(\""timedeltas\"")\n        self.df_multiindex = DataFrame(data, index=index)"", ""groupby.Shift.time_defaults"": ""class Shift:\n    def time_defaults(self):\n        self.df.groupby(\""g\"").shift()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Shift:\n    def setup(self):\n        N = 18\n        self.df = DataFrame({\""g\"": [\""a\"", \""b\""] * 9, \""v\"": list(range(N))})"", ""groupby.Shift.time_fill_value"": ""class Shift:\n    def time_fill_value(self):\n        self.df.groupby(\""g\"").shift(fill_value=99)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Shift:\n    def setup(self):\n        N = 18\n        self.df = DataFrame({\""g\"": [\""a\"", \""b\""] * 9, \""v\"": list(range(N))})"", ""groupby.Size.time_category_size"": ""class Size:\n    def time_category_size(self):\n        self.draws.groupby(self.cats).size()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Size:\n    def setup(self):\n        n = 10**5\n        offsets = np.random.randint(n, size=n).astype(\""timedelta64[ns]\"")\n        dates = np.datetime64(\""now\"") + offsets\n        self.df = DataFrame(\n            {\n                \""key1\"": np.random.randint(0, 500, size=n),\n                \""key2\"": np.random.randint(0, 100, size=n),\n                \""value1\"": np.random.randn(n),\n                \""value2\"": np.random.randn(n),\n                \""value3\"": np.random.randn(n),\n                \""dates\"": dates,\n            }\n        )\n        self.draws = Series(np.random.randn(n))\n        labels = Series([\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (n // 4))\n        self.cats = labels.astype(\""category\"")"", ""groupby.Size.time_multi_size"": ""class Size:\n    def time_multi_size(self):\n        self.df.groupby([\""key1\"", \""key2\""]).size()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Size:\n    def setup(self):\n        n = 10**5\n        offsets = np.random.randint(n, size=n).astype(\""timedelta64[ns]\"")\n        dates = np.datetime64(\""now\"") + offsets\n        self.df = DataFrame(\n            {\n                \""key1\"": np.random.randint(0, 500, size=n),\n                \""key2\"": np.random.randint(0, 100, size=n),\n                \""value1\"": np.random.randn(n),\n                \""value2\"": np.random.randn(n),\n                \""value3\"": np.random.randn(n),\n                \""dates\"": dates,\n            }\n        )\n        self.draws = Series(np.random.randn(n))\n        labels = Series([\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (n // 4))\n        self.cats = labels.astype(\""category\"")"", ""groupby.String.time_str_func"": ""class String:\n    def time_str_func(self, dtype, method):\n        self.df.groupby(\""a\"")[self.df.columns[1:]].agg(method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass String:\n    def setup(self, dtype, method):\n        cols = list(\""abcdefghjkl\"")\n        self.df = DataFrame(\n            np.random.randint(0, 100, size=(10_000, len(cols))),\n            columns=cols,\n            dtype=dtype,\n        )"", ""groupby.SumBools.time_groupby_sum_booleans"": ""class SumBools:\n    def time_groupby_sum_booleans(self):\n        self.df.groupby(\""ii\"").sum()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SumBools:\n    def setup(self):\n        N = 500\n        self.df = DataFrame({\""ii\"": range(N), \""bb\"": [True] * N})"", ""groupby.Transform.time_transform_lambda_max"": ""class Transform:\n    def time_transform_lambda_max(self):\n        self.df.groupby(level=\""lev1\"").transform(lambda x: max(x))\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]"", ""groupby.Transform.time_transform_lambda_max_wide"": ""class Transform:\n    def time_transform_lambda_max_wide(self):\n        self.df_wide.groupby(level=0).transform(lambda x: np.max(x, axis=0))\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]"", ""groupby.Transform.time_transform_multi_key1"": ""class Transform:\n    def time_transform_multi_key1(self):\n        self.df1.groupby([\""jim\"", \""joe\""])[\""jolie\""].transform(\""max\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]"", ""groupby.Transform.time_transform_multi_key2"": ""class Transform:\n    def time_transform_multi_key2(self):\n        self.df2.groupby([\""jim\"", \""joe\""])[\""jolie\""].transform(\""max\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]"", ""groupby.Transform.time_transform_multi_key3"": ""class Transform:\n    def time_transform_multi_key3(self):\n        self.df3.groupby([\""jim\"", \""joe\""])[\""jolie\""].transform(\""max\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]"", ""groupby.Transform.time_transform_ufunc_max"": ""class Transform:\n    def time_transform_ufunc_max(self):\n        self.df.groupby(level=\""lev1\"").transform(np.max)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]"", ""groupby.TransformBools.time_transform_mean"": ""class TransformBools:\n    def time_transform_mean(self):\n        self.df[\""signal\""].groupby(self.g).transform(np.mean)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TransformBools:\n    def setup(self):\n        N = 120000\n        transition_points = np.sort(np.random.choice(np.arange(N), 1400))\n        transitions = np.zeros(N, dtype=np.bool_)\n        transitions[transition_points] = True\n        self.g = transitions.cumsum()\n        self.df = DataFrame({\""signal\"": np.random.rand(N)})"", ""groupby.TransformEngine.time_dataframe_cython"": ""class TransformEngine:\n    def time_dataframe_cython(self, parallel):\n        def function(values):\n            return values * 5\n    \n        self.grouper.transform(function, engine=\""cython\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TransformEngine:\n    def setup(self, parallel):\n        N = 10**3\n        data = DataFrame(\n            {0: [str(i) for i in range(100)] * N, 1: list(range(100)) * N},\n            columns=[0, 1],\n        )\n        self.parallel = parallel\n        self.grouper = data.groupby(0)"", ""groupby.TransformEngine.time_dataframe_numba"": ""class TransformEngine:\n    def time_dataframe_numba(self, parallel):\n        def function(values, index):\n            return values * 5\n    \n        self.grouper.transform(\n            function, engine=\""numba\"", engine_kwargs={\""parallel\"": self.parallel}\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TransformEngine:\n    def setup(self, parallel):\n        N = 10**3\n        data = DataFrame(\n            {0: [str(i) for i in range(100)] * N, 1: list(range(100)) * N},\n            columns=[0, 1],\n        )\n        self.parallel = parallel\n        self.grouper = data.groupby(0)"", ""groupby.TransformEngine.time_series_numba"": ""class TransformEngine:\n    def time_series_numba(self, parallel):\n        def function(values, index):\n            return values * 5\n    \n        self.grouper[1].transform(\n            function, engine=\""numba\"", engine_kwargs={\""parallel\"": self.parallel}\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TransformEngine:\n    def setup(self, parallel):\n        N = 10**3\n        data = DataFrame(\n            {0: [str(i) for i in range(100)] * N, 1: list(range(100)) * N},\n            columns=[0, 1],\n        )\n        self.parallel = parallel\n        self.grouper = data.groupby(0)""}",groupby,time,0.012214275645693858
29970,module-level,"terminus-2,claude",1.164766242987184,1.0966129989238598,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods,frame_methods,"{""frame_methods.Apply.time_apply_axis_1"": ""class Apply:\n    def time_apply_axis_1(self):\n        self.df.apply(lambda x: x + 1, axis=1)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self):\n        self.df = DataFrame(np.random.randn(1000, 100))\n    \n        self.s = Series(np.arange(1028.0))\n        self.df2 = DataFrame({i: self.s for i in range(1028)})\n        self.df3 = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))"", ""frame_methods.Apply.time_apply_lambda_mean"": ""class Apply:\n    def time_apply_lambda_mean(self):\n        self.df.apply(lambda x: x.mean())\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self):\n        self.df = DataFrame(np.random.randn(1000, 100))\n    \n        self.s = Series(np.arange(1028.0))\n        self.df2 = DataFrame({i: self.s for i in range(1028)})\n        self.df3 = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))"", ""frame_methods.Count.time_count_level_mixed_dtypes_multi"": ""class Count:\n    def time_count_level_mixed_dtypes_multi(self, axis):\n        self.df_mixed.count(axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Count:\n    def setup(self, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""\n    \n        self.df.index = MultiIndex.from_arrays([self.df.index, self.df.index])\n        self.df.columns = MultiIndex.from_arrays([self.df.columns, self.df.columns])\n        self.df_mixed.index = MultiIndex.from_arrays(\n            [self.df_mixed.index, self.df_mixed.index]\n        )\n        self.df_mixed.columns = MultiIndex.from_arrays(\n            [self.df_mixed.columns, self.df_mixed.columns]\n        )"", ""frame_methods.Count.time_count_level_multi"": ""class Count:\n    def time_count_level_multi(self, axis):\n        self.df.count(axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Count:\n    def setup(self, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""\n    \n        self.df.index = MultiIndex.from_arrays([self.df.index, self.df.index])\n        self.df.columns = MultiIndex.from_arrays([self.df.columns, self.df.columns])\n        self.df_mixed.index = MultiIndex.from_arrays(\n            [self.df_mixed.index, self.df_mixed.index]\n        )\n        self.df_mixed.columns = MultiIndex.from_arrays(\n            [self.df_mixed.columns, self.df_mixed.columns]\n        )"", ""frame_methods.Describe.time_dataframe_describe"": ""class Describe:\n    def time_dataframe_describe(self):\n        self.df.describe()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Describe:\n    def setup(self):\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(0, 100, 10**6),\n                \""b\"": np.random.randint(0, 100, 10**6),\n                \""c\"": np.random.randint(0, 100, 10**6),\n            }\n        )"", ""frame_methods.Describe.time_series_describe"": ""class Describe:\n    def time_series_describe(self):\n        self.df[\""a\""].describe()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Describe:\n    def setup(self):\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(0, 100, 10**6),\n                \""b\"": np.random.randint(0, 100, 10**6),\n                \""c\"": np.random.randint(0, 100, 10**6),\n            }\n        )"", ""frame_methods.Dropna.time_dropna"": ""class Dropna:\n    def time_dropna(self, how, axis):\n        self.df.dropna(how=how, axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Dropna:\n    def setup(self, how, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\"""", ""frame_methods.Dropna.time_dropna_axis_mixed_dtypes"": ""class Dropna:\n    def time_dropna_axis_mixed_dtypes(self, how, axis):\n        self.df_mixed.dropna(how=how, axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Dropna:\n    def setup(self, how, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\"""", ""frame_methods.Dtypes.time_frame_dtypes"": ""class Dtypes:\n    def time_frame_dtypes(self):\n        self.df.dtypes\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Dtypes:\n    def setup(self):\n        self.df = DataFrame(np.random.randn(1000, 1000))"", ""frame_methods.Duplicated.time_frame_duplicated_wide"": ""class Duplicated:\n    def time_frame_duplicated_wide(self):\n        self.df2.duplicated()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Duplicated:\n    def setup(self):\n        n = 1 << 20\n        t = date_range(\""2015-01-01\"", freq=\""S\"", periods=(n // 64))\n        xs = np.random.randn(n // 64).round(2)\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(-1 << 8, 1 << 8, n),\n                \""b\"": np.random.choice(t, n),\n                \""c\"": np.random.choice(xs, n),\n            }\n        )\n        self.df2 = DataFrame(np.random.randn(1000, 100).astype(str)).T"", ""frame_methods.Equals.time_frame_float_unequal"": ""class Equals:\n    def time_frame_float_unequal(self):\n        self.float_df.equals(self.float_df_nan)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Equals:\n    def setup(self):\n        N = 10**3\n        self.float_df = DataFrame(np.random.randn(N, N))\n        self.float_df_nan = self.float_df.copy()\n        self.float_df_nan.iloc[-1, -1] = np.nan\n    \n        self.object_df = DataFrame(\""foo\"", index=range(N), columns=range(N))\n        self.object_df_nan = self.object_df.copy()\n        self.object_df_nan.iloc[-1, -1] = np.nan\n    \n        self.nonunique_cols = self.object_df.copy()\n        self.nonunique_cols.columns = [\""A\""] * len(self.nonunique_cols.columns)\n        self.nonunique_cols_nan = self.nonunique_cols.copy()\n        self.nonunique_cols_nan.iloc[-1, -1] = np.nan"", ""frame_methods.Fillna.time_frame_fillna"": ""class Fillna:\n    def time_frame_fillna(self, inplace, method, dtype):\n        self.df.fillna(inplace=inplace, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Fillna:\n    def setup(self, inplace, method, dtype):\n        N, M = 10000, 100\n        if dtype in (\""datetime64[ns]\"", \""datetime64[ns, tz]\"", \""timedelta64[ns]\""):\n            data = {\n                \""datetime64[ns]\"": date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n                \""datetime64[ns, tz]\"": date_range(\n                    \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n                ),\n                \""timedelta64[ns]\"": timedelta_range(start=\""1 day\"", periods=N, freq=\""1D\""),\n            }\n            self.df = DataFrame({f\""col_{i}\"": data[dtype] for i in range(M)})\n            self.df[::2] = None\n        else:\n            values = np.random.randn(N, M)\n            values[::2] = np.nan\n            if dtype == \""Int64\"":\n                values = values.round()\n            self.df = DataFrame(values, dtype=dtype)"", ""frame_methods.GetNumericData.time_frame_get_numeric_data"": ""class GetNumericData:\n    def time_frame_get_numeric_data(self):\n        self.df._get_numeric_data()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GetNumericData:\n    def setup(self):\n        self.df = DataFrame(np.random.randn(10000, 25))\n        self.df[\""foo\""] = \""bar\""\n        self.df[\""bar\""] = \""baz\""\n        self.df = self.df._consolidate()"", ""frame_methods.Isnull.time_isnull"": ""class Isnull:\n    def time_isnull(self):\n        isnull(self.df)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Isnull:\n    def setup(self):\n        N = 10**3\n        self.df_no_null = DataFrame(np.random.randn(N, N))\n    \n        sample = np.array([np.nan, 1.0])\n        data = np.random.choice(sample, (N, N))\n        self.df = DataFrame(data)\n    \n        sample = np.array(list(string.ascii_letters + string.whitespace))\n        data = np.random.choice(sample, (N, N))\n        self.df_strings = DataFrame(data)\n    \n        sample = np.array(\n            [\n                NaT,\n                np.nan,\n                None,\n                np.datetime64(\""NaT\""),\n                np.timedelta64(\""NaT\""),\n                0,\n                1,\n                2.0,\n                \""\"",\n                \""abcd\"",\n            ]\n        )\n        data = np.random.choice(sample, (N, N))\n        self.df_obj = DataFrame(data)"", ""frame_methods.Isnull.time_isnull_floats_no_null"": ""class Isnull:\n    def time_isnull_floats_no_null(self):\n        isnull(self.df_no_null)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Isnull:\n    def setup(self):\n        N = 10**3\n        self.df_no_null = DataFrame(np.random.randn(N, N))\n    \n        sample = np.array([np.nan, 1.0])\n        data = np.random.choice(sample, (N, N))\n        self.df = DataFrame(data)\n    \n        sample = np.array(list(string.ascii_letters + string.whitespace))\n        data = np.random.choice(sample, (N, N))\n        self.df_strings = DataFrame(data)\n    \n        sample = np.array(\n            [\n                NaT,\n                np.nan,\n                None,\n                np.datetime64(\""NaT\""),\n                np.timedelta64(\""NaT\""),\n                0,\n                1,\n                2.0,\n                \""\"",\n                \""abcd\"",\n            ]\n        )\n        data = np.random.choice(sample, (N, N))\n        self.df_obj = DataFrame(data)"", ""frame_methods.Iteration.time_itertuples_raw_start"": ""class Iteration:\n    def time_itertuples_raw_start(self):\n        self.df4.itertuples(index=False, name=None)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))"", ""frame_methods.Iteration.time_itertuples_read_first"": ""class Iteration:\n    def time_itertuples_read_first(self):\n        next(self.df4.itertuples())\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))"", ""frame_methods.Iteration.time_itertuples_start"": ""class Iteration:\n    def time_itertuples_start(self):\n        self.df4.itertuples()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))"", ""frame_methods.MaskBool.time_frame_mask_floats"": ""class MaskBool:\n    def time_frame_mask_floats(self):\n        self.bools.astype(float).mask(self.mask)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MaskBool:\n    def setup(self):\n        data = np.random.randn(1000, 500)\n        df = DataFrame(data)\n        df = df.where(df > 0)\n        self.bools = df > 0\n        self.mask = isnull(df)"", ""frame_methods.MemoryUsage.time_memory_usage_object_dtype"": ""class MemoryUsage:\n    def time_memory_usage_object_dtype(self):\n        self.df2.memory_usage(deep=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MemoryUsage:\n    def setup(self):\n        self.df = DataFrame(np.random.randn(100000, 2), columns=list(\""AB\""))\n        self.df2 = self.df.copy()\n        self.df2[\""A\""] = self.df2[\""A\""].astype(\""object\"")"", ""frame_methods.NSort.time_nlargest_one_column"": ""class NSort:\n    def time_nlargest_one_column(self, keep):\n        self.df.nlargest(100, \""A\"", keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NSort:\n    def setup(self, keep):\n        self.df = DataFrame(np.random.randn(100000, 3), columns=list(\""ABC\""))"", ""frame_methods.NSort.time_nlargest_two_columns"": ""class NSort:\n    def time_nlargest_two_columns(self, keep):\n        self.df.nlargest(100, [\""A\"", \""B\""], keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NSort:\n    def setup(self, keep):\n        self.df = DataFrame(np.random.randn(100000, 3), columns=list(\""ABC\""))"", ""frame_methods.NSort.time_nsmallest_one_column"": ""class NSort:\n    def time_nsmallest_one_column(self, keep):\n        self.df.nsmallest(100, \""A\"", keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NSort:\n    def setup(self, keep):\n        self.df = DataFrame(np.random.randn(100000, 3), columns=list(\""ABC\""))"", ""frame_methods.NSort.time_nsmallest_two_columns"": ""class NSort:\n    def time_nsmallest_two_columns(self, keep):\n        self.df.nsmallest(100, [\""A\"", \""B\""], keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NSort:\n    def setup(self, keep):\n        self.df = DataFrame(np.random.randn(100000, 3), columns=list(\""ABC\""))"", ""frame_methods.Nunique.time_frame_nunique"": ""class Nunique:\n    def time_frame_nunique(self):\n        self.df.nunique()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Nunique:\n    def setup(self):\n        self.df = DataFrame(np.random.randn(10000, 1000))"", ""frame_methods.Quantile.time_frame_quantile"": ""class Quantile:\n    def time_frame_quantile(self, axis):\n        self.df.quantile([0.1, 0.5], axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, axis):\n        self.df = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))"", ""frame_methods.Rank.time_rank"": ""class Rank:\n    def time_rank(self, dtype):\n        self.df.rank()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Rank:\n    def setup(self, dtype):\n        self.df = DataFrame(\n            np.random.randn(10000, 10).astype(dtype), columns=range(10), dtype=dtype\n        )"", ""frame_methods.Rename.time_rename_single"": ""class Rename:\n    def time_rename_single(self):\n        self.df.rename({0: 0})\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Rename:\n    def setup(self):\n        N = 10**3\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.idx = np.arange(4 * N, 7 * N)\n        self.dict_idx = {k: k for k in self.idx}\n        self.df2 = DataFrame(\n            {\n                c: {\n                    0: np.random.randint(0, 2, N).astype(np.bool_),\n                    1: np.random.randint(0, N, N).astype(np.int16),\n                    2: np.random.randint(0, N, N).astype(np.int32),\n                    3: np.random.randint(0, N, N).astype(np.int64),\n                }[np.random.randint(0, 4)]\n                for c in range(N)\n            }\n        )"", ""frame_methods.Repr.time_html_repr_trunc_si"": ""class Repr:\n    def time_html_repr_trunc_si(self):\n        self.df4._repr_html_()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Repr:\n    def setup(self):\n        nrows = 10000\n        data = np.random.randn(nrows, 10)\n        arrays = np.tile(np.random.randn(3, nrows // 100), 100)\n        idx = MultiIndex.from_arrays(arrays)\n        self.df3 = DataFrame(data, index=idx)\n        self.df4 = DataFrame(data, index=np.random.randn(nrows))\n        self.df_tall = DataFrame(np.random.randn(nrows, 10))\n        self.df_wide = DataFrame(np.random.randn(10, nrows))"", ""frame_methods.Repr.time_repr_tall"": ""class Repr:\n    def time_repr_tall(self):\n        repr(self.df_tall)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Repr:\n    def setup(self):\n        nrows = 10000\n        data = np.random.randn(nrows, 10)\n        arrays = np.tile(np.random.randn(3, nrows // 100), 100)\n        idx = MultiIndex.from_arrays(arrays)\n        self.df3 = DataFrame(data, index=idx)\n        self.df4 = DataFrame(data, index=np.random.randn(nrows))\n        self.df_tall = DataFrame(np.random.randn(nrows, 10))\n        self.df_wide = DataFrame(np.random.randn(10, nrows))"", ""frame_methods.ToDict.time_to_dict_ints"": ""class ToDict:\n    def time_to_dict_ints(self, orient):\n        self.int_df.to_dict(orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToDict:\n    def setup(self, orient):\n        data = np.random.randint(0, 1000, size=(10000, 4))\n        self.int_df = DataFrame(data)\n        self.datetimelike_df = self.int_df.astype(\""timedelta64[ns]\"")""}",frame_methods,time,0.04819889962045554
29966,module-level,"terminus-2,claude",1.056802365350365,0.9399366890806246,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,ctors,ctors,"{""ctors.DatetimeIndexConstructor.time_from_list_of_dates"": ""class DatetimeIndexConstructor:\n    def time_from_list_of_dates(self):\n        DatetimeIndex(self.list_of_dates)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndexConstructor:\n    def setup(self):\n        N = 20_000\n        dti = date_range(\""1900-01-01\"", periods=N)\n    \n        self.list_of_timestamps = dti.tolist()\n        self.list_of_dates = dti.date.tolist()\n        self.list_of_datetimes = dti.to_pydatetime().tolist()\n        self.list_of_str = dti.strftime(\""%Y-%m-%d\"").tolist()"", ""ctors.MultiIndexConstructor.time_multiindex_from_iterables"": ""class MultiIndexConstructor:\n    def time_multiindex_from_iterables(self):\n        MultiIndex.from_product(self.iterables)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MultiIndexConstructor:\n    def setup(self):\n        N = 10**4\n        self.iterables = [tm.makeStringIndex(N), range(20)]"", ""ctors.SeriesConstructors.time_series_constructor"": ""class SeriesConstructors:\n    def time_series_constructor(self, data_fmt, with_index, dtype):\n        Series(self.data, index=self.index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesConstructors:\n    def setup(self, data_fmt, with_index, dtype):\n        if data_fmt in (gen_of_str, gen_of_tuples) and with_index:\n            raise NotImplementedError(\n                \""Series constructors do not support using generators with indexes\""\n            )\n        N = 10**4\n        if dtype == \""float\"":\n            arr = np.random.randn(N)\n        else:\n            arr = np.arange(N)\n        self.data = data_fmt(arr)\n        self.index = np.arange(N) if with_index else None"", ""ctors.SeriesDtypesConstructors.time_dtindex_from_series"": ""class SeriesDtypesConstructors:\n    def time_dtindex_from_series(self):\n        DatetimeIndex(self.s)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesDtypesConstructors:\n    def setup(self):\n        N = 10**4\n        self.arr = np.random.randn(N)\n        self.arr_str = np.array([\""foo\"", \""bar\"", \""baz\""], dtype=object)\n        self.s = Series(\n            [Timestamp(\""20110101\""), Timestamp(\""20120101\""), Timestamp(\""20130101\"")]\n            * N\n            * 10\n        )"", ""ctors.SeriesDtypesConstructors.time_index_from_array_string"": ""class SeriesDtypesConstructors:\n    def time_index_from_array_string(self):\n        Index(self.arr_str)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesDtypesConstructors:\n    def setup(self):\n        N = 10**4\n        self.arr = np.random.randn(N)\n        self.arr_str = np.array([\""foo\"", \""bar\"", \""baz\""], dtype=object)\n        self.s = Series(\n            [Timestamp(\""20110101\""), Timestamp(\""20120101\""), Timestamp(\""20130101\"")]\n            * N\n            * 10\n        )""}",ctors,time,0.08264899311862839
29971,module-level,"terminus-2,claude",1.0691904866780633,1.1079542764811172,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,gil,gil,"{""gil.ParallelGroupbyMethods.time_parallel"": ""class ParallelGroupbyMethods:\n    def time_parallel(self, threads, method):\n        self.parallel()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ParallelGroupbyMethods:\n    def setup(self, threads, method):\n        N = 10**6\n        ngroups = 10**3\n        df = DataFrame(\n            {\""key\"": np.random.randint(0, ngroups, size=N), \""data\"": np.random.randn(N)}\n        )\n    \n        @test_parallel(num_threads=threads)\n        def parallel():\n            getattr(df.groupby(\""key\"")[\""data\""], method)()\n    \n        self.parallel = parallel\n    \n        def loop():\n            getattr(df.groupby(\""key\"")[\""data\""], method)()\n    \n        self.loop = loop"", ""gil.ParallelGroups.time_get_groups"": ""class ParallelGroups:\n    def time_get_groups(self, threads):\n        self.get_groups()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ParallelGroups:\n    def setup(self, threads):\n        size = 2**22\n        ngroups = 10**3\n        data = Series(np.random.randint(0, ngroups, size=size))\n    \n        @test_parallel(num_threads=threads)\n        def get_groups():\n            data.groupby(data).groups\n    \n        self.get_groups = get_groups""}",gil,time,-0.02741427850286701
29961,module-level,"terminus-2,claude",1.108857234481463,1.064921425988438,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin,algos.isin,"{""algos.isin.IsIn.time_isin_categorical"": ""class IsIn:\n    def time_isin_categorical(self, dtype):\n        self.series.isin(self.cat_values)\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)"", ""algos.isin.IsIn.time_isin_empty"": ""class IsIn:\n    def time_isin_empty(self, dtype):\n        self.series.isin([])\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)"", ""algos.isin.IsIn.time_isin_mismatched_dtype"": ""class IsIn:\n    def time_isin_mismatched_dtype(self, dtype):\n        self.series.isin(self.mismatched)\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)"", ""algos.isin.IsInLongSeriesLookUpDominates.time_isin"": ""class IsInLongSeriesLookUpDominates:\n    def time_isin(self, dtypes, MaxNumber, series_type):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, MaxNumber, series_type):\n        N = 10**7\n    \n        if series_type == \""random_hits\"":\n            array = np.random.randint(0, MaxNumber, N)\n        if series_type == \""random_misses\"":\n            array = np.random.randint(0, MaxNumber, N) + MaxNumber\n        if series_type == \""monotone_hits\"":\n            array = np.repeat(np.arange(MaxNumber), N // MaxNumber)\n        if series_type == \""monotone_misses\"":\n            array = np.arange(N) + MaxNumber\n    \n        self.series = Series(array).astype(dtype)\n    \n        self.values = np.arange(MaxNumber).astype(dtype.lower())"", ""algos.isin.IsInLongSeriesValuesDominate.time_isin"": ""class IsInLongSeriesValuesDominate:\n    def time_isin(self, dtypes, series_type):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, series_type):\n        N = 10**7\n    \n        if series_type == \""random\"":\n            vals = np.random.randint(0, 10 * N, N)\n        if series_type == \""monotone\"":\n            vals = np.arange(N)\n    \n        self.values = vals.astype(dtype.lower())\n        M = 10**6 + 1\n        self.series = Series(np.arange(M)).astype(dtype)"", ""algos.isin.IsinAlmostFullWithRandomInt.time_isin"": ""class IsinAlmostFullWithRandomInt:\n    def time_isin(self, dtype, exponent, title):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, exponent, title):\n        M = 3 * 2 ** (exponent - 2)\n        # 0.77-the maximal share of occupied buckets\n        self.series = Series(np.random.randint(0, M, M)).astype(dtype)\n    \n        values = np.random.randint(0, M, M).astype(dtype)\n        if title == \""inside\"":\n            self.values = values\n        elif title == \""outside\"":\n            self.values = values + M\n        else:\n            raise ValueError(title)"", ""algos.isin.IsinWithArange.time_isin"": ""class IsinWithArange:\n    def time_isin(self, dtype, M, offset_factor):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, M, offset_factor):\n        offset = int(M * offset_factor)\n        tmp = Series(np.random.randint(offset, M + offset, 10**6))\n        self.series = tmp.astype(dtype)\n        self.values = np.arange(M).astype(dtype)"", ""algos.isin.IsinWithRandomFloat.time_isin"": ""class IsinWithRandomFloat:\n    def time_isin(self, dtype, size, title):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, size, title):\n        self.values = np.random.rand(size)\n        self.series = Series(self.values).astype(dtype)\n        np.random.shuffle(self.values)\n    \n        if title == \""outside\"":\n            self.values = self.values + 0.1""}",algos.isin,time,0.031072000348673842
29974,module-level,"terminus-2,claude",1.2428012749308386,1.0570132078439995,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_cached_properties,index_cached_properties,"{""index_cached_properties.IndexCache.time_engine"": ""class IndexCache:\n    def time_engine(self, index_type):\n        self.idx._engine\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}"", ""index_cached_properties.IndexCache.time_inferred_type"": ""class IndexCache:\n    def time_inferred_type(self, index_type):\n        self.idx.inferred_type\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}"", ""index_cached_properties.IndexCache.time_is_monotonic_decreasing"": ""class IndexCache:\n    def time_is_monotonic_decreasing(self, index_type):\n        self.idx.is_monotonic_decreasing\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}"", ""index_cached_properties.IndexCache.time_is_monotonic_increasing"": ""class IndexCache:\n    def time_is_monotonic_increasing(self, index_type):\n        self.idx.is_monotonic_increasing\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}"", ""index_cached_properties.IndexCache.time_shape"": ""class IndexCache:\n    def time_shape(self, index_type):\n        self.idx.shape\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}"", ""index_cached_properties.IndexCache.time_values"": ""class IndexCache:\n    def time_values(self, index_type):\n        self.idx._values\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}""}",index_cached_properties,time,0.1313918437672129
29967,module-level,"terminus-2,claude",1.0626723732881027,1.07617088471239,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes,dtypes,"{""dtypes.Dtypes.time_pandas_dtype"": ""class Dtypes:\n    def time_pandas_dtype(self, dtype):\n        pandas_dtype(dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)"", ""dtypes.DtypesInvalid.time_pandas_dtype_invalid"": ""class DtypesInvalid:\n    def time_pandas_dtype_invalid(self, dtype):\n        try:\n            pandas_dtype(self.data_dict[dtype])\n        except TypeError:\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)"", ""dtypes.SelectDtypes.time_select_dtype_bool_exclude"": ""class SelectDtypes:\n    def time_select_dtype_bool_exclude(self, dtype):\n        self.df_bool.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_bool_include"": ""class SelectDtypes:\n    def time_select_dtype_bool_include(self, dtype):\n        self.df_bool.select_dtypes(include=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_float_exclude"": ""class SelectDtypes:\n    def time_select_dtype_float_exclude(self, dtype):\n        self.df_float.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_float_include"": ""class SelectDtypes:\n    def time_select_dtype_float_include(self, dtype):\n        self.df_float.select_dtypes(include=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_int_exclude"": ""class SelectDtypes:\n    def time_select_dtype_int_exclude(self, dtype):\n        self.df_int.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_int_include"": ""class SelectDtypes:\n    def time_select_dtype_int_include(self, dtype):\n        self.df_int.select_dtypes(include=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_string_exclude"": ""class SelectDtypes:\n    def time_select_dtype_string_exclude(self, dtype):\n        self.df_string.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_string_include"": ""class SelectDtypes:\n    def time_select_dtype_string_include(self, dtype):\n        self.df_string.select_dtypes(include=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )""}",dtypes,time,-0.009546330568802953
29980,module-level,"terminus-2,claude",0.9375712081637044,0.97232287374828,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.style,io.style,"{""io.style.Render.time_apply_format_hide_render"": ""class Render:\n    def time_apply_format_hide_render(self, cols, rows):\n        self._style_apply_format_hide()\n        self.st._render_html(True, True)\n\n    def setup(self, cols, rows):\n        self.df = DataFrame(\n            np.random.randn(rows, cols),\n            columns=[f\""float_{i+1}\"" for i in range(cols)],\n            index=[f\""row_{i+1}\"" for i in range(rows)],\n        )"", ""io.style.Render.time_apply_render"": ""class Render:\n    def time_apply_render(self, cols, rows):\n        self._style_apply()\n        self.st._render_html(True, True)\n\n    def setup(self, cols, rows):\n        self.df = DataFrame(\n            np.random.randn(rows, cols),\n            columns=[f\""float_{i+1}\"" for i in range(cols)],\n            index=[f\""row_{i+1}\"" for i in range(rows)],\n        )"", ""io.style.Render.time_format_render"": ""class Render:\n    def time_format_render(self, cols, rows):\n        self._style_format()\n        self.st._render_html(True, True)\n\n    def setup(self, cols, rows):\n        self.df = DataFrame(\n            np.random.randn(rows, cols),\n            columns=[f\""float_{i+1}\"" for i in range(cols)],\n            index=[f\""row_{i+1}\"" for i in range(rows)],\n        )"", ""io.style.Render.time_tooltips_render"": ""class Render:\n    def time_tooltips_render(self, cols, rows):\n        self._style_tooltips()\n        self.st._render_html(True, True)\n\n    def setup(self, cols, rows):\n        self.df = DataFrame(\n            np.random.randn(rows, cols),\n            columns=[f\""float_{i+1}\"" for i in range(cols)],\n            index=[f\""row_{i+1}\"" for i in range(rows)],\n        )""}",io.style,time,-0.02457684977692754
29984,module-level,"terminus-2,claude",1.042455905499289,1.0629000949196494,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape,reshape,"{""reshape.Crosstab.time_crosstab_normalize"": ""class Crosstab:\n    def time_crosstab_normalize(self):\n        pd.crosstab(self.vec1, self.vec2, normalize=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Crosstab:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        self.ind1 = np.random.randint(0, 3, size=N)\n        self.ind2 = np.random.randint(0, 2, size=N)\n        self.vec1 = fac1.take(self.ind1)\n        self.vec2 = fac2.take(self.ind2)"", ""reshape.Cut.time_cut_datetime"": ""class Cut:\n    def time_cut_datetime(self, bins):\n        pd.cut(self.datetime_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Cut.time_cut_int"": ""class Cut:\n    def time_cut_int(self, bins):\n        pd.cut(self.int_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Cut.time_cut_interval"": ""class Cut:\n    def time_cut_interval(self, bins):\n        # GH 27668\n        pd.cut(self.int_series, self.interval_bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Cut.time_cut_timedelta"": ""class Cut:\n    def time_cut_timedelta(self, bins):\n        pd.cut(self.timedelta_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Cut.time_qcut_datetime"": ""class Cut:\n    def time_qcut_datetime(self, bins):\n        pd.qcut(self.datetime_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Explode.time_explode"": ""class Explode:\n    def time_explode(self, n_rows, max_list_length):\n        self.series.explode()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Explode:\n    def setup(self, n_rows, max_list_length):\n        data = [np.arange(np.random.randint(max_list_length)) for _ in range(n_rows)]\n        self.series = pd.Series(data)"", ""reshape.ReshapeExtensionDtype.time_unstack_fast"": ""class ReshapeExtensionDtype:\n    def time_unstack_fast(self, dtype):\n        # last level -> doesn't have to make copies\n        self.ser.unstack(\""bar\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReshapeExtensionDtype:\n    def setup(self, dtype):\n        lev = pd.Index(list(\""ABCDEFGHIJ\""))\n        ri = pd.Index(range(1000))\n        mi = MultiIndex.from_product([lev, ri], names=[\""foo\"", \""bar\""])\n    \n        index = date_range(\""2016-01-01\"", periods=10000, freq=\""s\"", tz=\""US/Pacific\"")\n        if dtype == \""Period[s]\"":\n            index = index.tz_localize(None).to_period(\""s\"")\n    \n        ser = pd.Series(index, index=mi)\n        df = ser.unstack(\""bar\"")\n        # roundtrips -> df.stack().equals(ser)\n    \n        self.ser = ser\n        self.df = df"", ""reshape.WideToLong.time_wide_to_long_big"": ""class WideToLong:\n    def time_wide_to_long_big(self):\n        wide_to_long(self.df, self.letters, i=\""id\"", j=\""year\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass WideToLong:\n    def setup(self):\n        nyrs = 20\n        nidvars = 20\n        N = 5000\n        self.letters = list(\""ABCD\"")\n        yrvars = [\n            letter + str(num)\n            for letter, num in product(self.letters, range(1, nyrs + 1))\n        ]\n        columns = [str(i) for i in range(nidvars)] + yrvars\n        self.df = DataFrame(np.random.randn(N, nidvars + len(yrvars)), columns=columns)\n        self.df[\""id\""] = self.df.index""}",reshape,time,-0.014458408359519389
29983,module-level,"terminus-2,claude",1.0465174007231333,0.9662950819021658,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,multiindex_object,multiindex_object,"{""multiindex_object.Integer.time_is_monotonic"": ""class Integer:\n    def time_is_monotonic(self):\n        self.mi_int.is_monotonic_increasing\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Integer:\n    def setup(self):\n        self.mi_int = MultiIndex.from_product(\n            [np.arange(1000), np.arange(1000)], names=[\""one\"", \""two\""]\n        )\n        self.obj_index = np.array(\n            [\n                (0, 10),\n                (0, 11),\n                (0, 12),\n                (0, 13),\n                (0, 14),\n                (0, 15),\n                (0, 16),\n                (0, 17),\n                (0, 18),\n                (0, 19),\n            ],\n            dtype=object,\n        )\n        self.other_mi_many_mismatches = MultiIndex.from_tuples(\n            [\n                (-7, 41),\n                (-2, 3),\n                (-0.7, 5),\n                (0, 0),\n                (0, 1.5),\n                (0, 340),\n                (0, 1001),\n                (1, -4),\n                (1, 20),\n                (1, 1040),\n                (432, -5),\n                (432, 17),\n                (439, 165.5),\n                (998, -4),\n                (998, 24065),\n                (999, 865.2),\n                (999, 1000),\n                (1045, -843),\n            ]\n        )"", ""multiindex_object.SetOperations.time_operation"": ""class SetOperations:\n    def time_operation(self, index_structure, dtype, method, sort):\n        getattr(self.left, method)(self.right, sort=sort)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SetOperations:\n    def setup(self, index_structure, dtype, method, sort):\n        N = 10**5\n        level1 = range(1000)\n    \n        level2 = date_range(start=\""1/1/2000\"", periods=N // 1000)\n        dates_left = MultiIndex.from_product([level1, level2])\n    \n        level2 = range(N // 1000)\n        int_left = MultiIndex.from_product([level1, level2])\n    \n        level2 = tm.makeStringIndex(N // 1000).values\n        str_left = MultiIndex.from_product([level1, level2])\n    \n        level2 = range(N // 1000)\n        ea_int_left = MultiIndex.from_product([level1, Series(level2, dtype=\""Int64\"")])\n    \n        data = {\n            \""datetime\"": dates_left,\n            \""int\"": int_left,\n            \""string\"": str_left,\n            \""ea_int\"": ea_int_left,\n        }\n    \n        if index_structure == \""non_monotonic\"":\n            data = {k: mi[::-1] for k, mi in data.items()}\n    \n        data = {k: {\""left\"": mi, \""right\"": mi[:-1]} for k, mi in data.items()}\n        self.left = data[dtype][\""left\""]\n        self.right = data[dtype][\""right\""]"", ""multiindex_object.SortValues.time_sort_values"": ""class SortValues:\n    def time_sort_values(self, dtype):\n        self.mi.sort_values()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SortValues:\n    def setup(self, dtype):\n        a = array(np.tile(np.arange(100), 1000), dtype=dtype)\n        b = array(np.tile(np.arange(1000), 100), dtype=dtype)\n        self.mi = MultiIndex.from_arrays([a, b])""}",multiindex_object,time,0.05673431316900101
29986,module-level,"terminus-2,claude",1.035941147591004,1.0766651260948683,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods,series_methods,"{""series_methods.Map.time_map"": ""class Map:\n    def time_map(self, mapper, *args, **kwargs):\n        self.s.map(self.map_data)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Map:\n    def setup(self, mapper, dtype):\n        map_size = 1000\n        map_data = Series(map_size - np.arange(map_size), dtype=dtype)\n    \n        # construct mapper\n        if mapper == \""Series\"":\n            self.map_data = map_data\n        elif mapper == \""dict\"":\n            self.map_data = map_data.to_dict()\n        elif mapper == \""lambda\"":\n            map_dict = map_data.to_dict()\n            self.map_data = lambda x: map_dict[x]\n        else:\n            raise NotImplementedError\n    \n        self.s = Series(np.random.randint(0, map_size, 10000), dtype=dtype)"", ""series_methods.Mode.time_mode"": ""class Mode:\n    def time_mode(self, N, dtype):\n        self.s.mode()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Mode:\n    def setup(self, N, dtype):\n        self.s = Series(np.random.randint(0, N, size=10 * N)).astype(dtype)"", ""series_methods.NanOps.time_func"": ""class NanOps:\n    def time_func(self, func, N, dtype):\n        self.func()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NanOps:\n    def setup(self, func, N, dtype):\n        if func == \""argmax\"" and dtype in {\""Int64\"", \""boolean\""}:\n            # Skip argmax for nullable int since this doesn't work yet (GH-24382)\n            raise NotImplementedError\n        self.s = Series([1] * N, dtype=dtype)\n        self.func = getattr(self.s, func)"", ""series_methods.SearchSorted.time_searchsorted"": ""class SearchSorted:\n    def time_searchsorted(self, dtype):\n        key = \""2\"" if dtype == \""str\"" else 2\n        self.s.searchsorted(key)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SearchSorted:\n    def setup(self, dtype):\n        N = 10**5\n        data = np.array([1] * N + [2] * N + [3] * N).astype(dtype)\n        self.s = Series(data)""}",series_methods,time,-0.028800550568503693
29965,module-level,"terminus-2,claude",1.0148120085912389,0.9825157394271008,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals,categoricals,"{""categoricals.CategoricalSlicing.time_getitem_list_like"": ""class CategoricalSlicing:\n    def time_getitem_list_like(self, index):\n        self.data[[self.scalar]]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalSlicing:\n    def setup(self, index):\n        N = 10**6\n        categories = [\""a\"", \""b\"", \""c\""]\n        values = [0] * N + [1] * N + [2] * N\n        if index == \""monotonic_incr\"":\n            self.data = pd.Categorical.from_codes(values, categories=categories)\n        elif index == \""monotonic_decr\"":\n            self.data = pd.Categorical.from_codes(\n                list(reversed(values)), categories=categories\n            )\n        elif index == \""non_monotonic\"":\n            self.data = pd.Categorical.from_codes([0, 1, 2] * N, categories=categories)\n        else:\n            raise ValueError(f\""Invalid index param: {index}\"")\n    \n        self.scalar = 10000\n        self.list = list(range(10000))\n        self.cat_scalar = \""b\"""", ""categoricals.CategoricalSlicing.time_getitem_scalar"": ""class CategoricalSlicing:\n    def time_getitem_scalar(self, index):\n        self.data[self.scalar]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalSlicing:\n    def setup(self, index):\n        N = 10**6\n        categories = [\""a\"", \""b\"", \""c\""]\n        values = [0] * N + [1] * N + [2] * N\n        if index == \""monotonic_incr\"":\n            self.data = pd.Categorical.from_codes(values, categories=categories)\n        elif index == \""monotonic_decr\"":\n            self.data = pd.Categorical.from_codes(\n                list(reversed(values)), categories=categories\n            )\n        elif index == \""non_monotonic\"":\n            self.data = pd.Categorical.from_codes([0, 1, 2] * N, categories=categories)\n        else:\n            raise ValueError(f\""Invalid index param: {index}\"")\n    \n        self.scalar = 10000\n        self.list = list(range(10000))\n        self.cat_scalar = \""b\"""", ""categoricals.CategoricalSlicing.time_getitem_slice"": ""class CategoricalSlicing:\n    def time_getitem_slice(self, index):\n        self.data[: self.scalar]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalSlicing:\n    def setup(self, index):\n        N = 10**6\n        categories = [\""a\"", \""b\"", \""c\""]\n        values = [0] * N + [1] * N + [2] * N\n        if index == \""monotonic_incr\"":\n            self.data = pd.Categorical.from_codes(values, categories=categories)\n        elif index == \""monotonic_decr\"":\n            self.data = pd.Categorical.from_codes(\n                list(reversed(values)), categories=categories\n            )\n        elif index == \""non_monotonic\"":\n            self.data = pd.Categorical.from_codes([0, 1, 2] * N, categories=categories)\n        else:\n            raise ValueError(f\""Invalid index param: {index}\"")\n    \n        self.scalar = 10000\n        self.list = list(range(10000))\n        self.cat_scalar = \""b\"""", ""categoricals.Contains.time_categorical_contains"": ""class Contains:\n    def time_categorical_contains(self):\n        self.key in self.c\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Contains:\n    def setup(self):\n        N = 10**5\n        self.ci = tm.makeCategoricalIndex(N)\n        self.c = self.ci.values\n        self.key = self.ci.categories[0]"", ""categoricals.IsMonotonic.time_categorical_index_is_monotonic_increasing"": ""class IsMonotonic:\n    def time_categorical_index_is_monotonic_increasing(self):\n        self.c.is_monotonic_increasing\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IsMonotonic:\n    def setup(self):\n        N = 1000\n        self.c = pd.CategoricalIndex(list(\""a\"" * N + \""b\"" * N + \""c\"" * N))\n        self.s = pd.Series(self.c)"", ""categoricals.Rank.time_rank_string"": ""class Rank:\n    def time_rank_string(self):\n        self.s_str.rank()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Rank:\n    def setup(self):\n        N = 10**5\n        ncats = 15\n    \n        self.s_str = pd.Series(tm.makeCategoricalIndex(N, ncats)).astype(str)\n        self.s_str_cat = pd.Series(self.s_str, dtype=\""category\"")\n        with warnings.catch_warnings(record=True):\n            str_cat_type = pd.CategoricalDtype(set(self.s_str), ordered=True)\n            self.s_str_cat_ordered = self.s_str.astype(str_cat_type)\n    \n        self.s_int = pd.Series(np.random.randint(0, ncats, size=N))\n        self.s_int_cat = pd.Series(self.s_int, dtype=\""category\"")\n        with warnings.catch_warnings(record=True):\n            int_cat_type = pd.CategoricalDtype(set(self.s_int), ordered=True)\n            self.s_int_cat_ordered = self.s_int.astype(int_cat_type)"", ""categoricals.Repr.time_rendering"": ""class Repr:\n    def time_rendering(self):\n        str(self.sel)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Repr:\n    def setup(self):\n        self.sel = pd.Series([\""s1234\""]).astype(\""category\"")""}",categoricals,time,0.02284036008779214
29989,module-level,"terminus-2,claude",1.0366956990275238,1.109116693497637,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings,strings,"{""strings.Contains.time_contains"": ""class Contains:\n    def time_contains(self, dtype, regex):\n        self.s.str.contains(\""A\"", regex=regex)\n\n    def setup(self, dtype, regex):\n        super().setup(dtype)"", ""strings.Dummies.time_get_dummies"": ""class Dummies:\n    def time_get_dummies(self, dtype):\n        self.s.str.get_dummies(\""|\"")\n\n    def setup(self, dtype):\n        super().setup(dtype)\n        self.s = self.s.str.join(\""|\"")"", ""strings.Methods.time_count"": ""class Methods:\n    def time_count(self, dtype):\n        self.s.str.count(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_endswith"": ""class Methods:\n    def time_endswith(self, dtype):\n        self.s.str.endswith(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_isdecimal"": ""class Methods:\n    def time_isdecimal(self, dtype):\n        self.s.str.isdecimal()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_join"": ""class Methods:\n    def time_join(self, dtype):\n        self.s.str.join(\"" \"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_lower"": ""class Methods:\n    def time_lower(self, dtype):\n        self.s.str.lower()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_replace"": ""class Methods:\n    def time_replace(self, dtype):\n        self.s.str.replace(\""A\"", \""\\x01\\x01\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_rpartition"": ""class Methods:\n    def time_rpartition(self, dtype):\n        self.s.str.rpartition(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_upper"": ""class Methods:\n    def time_upper(self, dtype):\n        self.s.str.upper()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_wrap"": ""class Methods:\n    def time_wrap(self, dtype):\n        self.s.str.wrap(10)\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.StringArrayConstruction.time_string_array_construction"": ""class StringArrayConstruction:\n    def time_string_array_construction(self):\n        StringArray(self.series_arr)\n\n    def setup(self):\n        self.series_arr = tm.rands_array(nchars=10, size=10**5)\n        self.series_arr_nan = np.concatenate([self.series_arr, np.array([NA] * 1000)])"", ""strings.StringArrayConstruction.time_string_array_with_nan_construction"": ""class StringArrayConstruction:\n    def time_string_array_with_nan_construction(self):\n        StringArray(self.series_arr_nan)\n\n    def setup(self):\n        self.series_arr = tm.rands_array(nchars=10, size=10**5)\n        self.series_arr_nan = np.concatenate([self.series_arr, np.array([NA] * 1000)])""}",strings,time,-0.05121711065778876
29976,module-level,"terminus-2,claude",0.9844645347907592,1.0129094817169162,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing_engines,indexing_engines,"{""indexing_engines.MaskedNumericEngineIndexing.time_get_loc"": ""class MaskedNumericEngineIndexing:\n    def time_get_loc(self, engine_and_dtype, index_type, unique, N):\n        self.data.get_loc(self.key_early)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())[::-1]\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.zeros(N * 3, dtype=dtype.lower())\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype.lower())\n                arr[N:] = np.arange(N * 2, dtype=dtype.lower())\n    \n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n            mask[-1] = True\n    \n        self.data = engine(BaseMaskedArray(arr, mask))\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]"", ""indexing_engines.MaskedNumericEngineIndexing.time_get_loc_near_middle"": ""class MaskedNumericEngineIndexing:\n    def time_get_loc_near_middle(self, engine_and_dtype, index_type, unique, N):\n        # searchsorted performance may be different near the middle of a range\n        #  vs near an endpoint\n        self.data.get_loc(self.key_middle)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())[::-1]\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.zeros(N * 3, dtype=dtype.lower())\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype.lower())\n                arr[N:] = np.arange(N * 2, dtype=dtype.lower())\n    \n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n            mask[-1] = True\n    \n        self.data = engine(BaseMaskedArray(arr, mask))\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]"", ""indexing_engines.NumericEngineIndexing.time_get_loc"": ""class NumericEngineIndexing:\n    def time_get_loc(self, engine_and_dtype, index_type, unique, N):\n        self.data.get_loc(self.key_early)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)[::-1]\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.empty(N * 3, dtype=dtype)\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype)\n                arr[N:] = np.arange(N * 2, dtype=dtype)\n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype)\n    \n        self.data = engine(arr)\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]"", ""indexing_engines.NumericEngineIndexing.time_get_loc_near_middle"": ""class NumericEngineIndexing:\n    def time_get_loc_near_middle(self, engine_and_dtype, index_type, unique, N):\n        # searchsorted performance may be different near the middle of a range\n        #  vs near an endpoint\n        self.data.get_loc(self.key_middle)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)[::-1]\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.empty(N * 3, dtype=dtype)\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype)\n                arr[N:] = np.arange(N * 2, dtype=dtype)\n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype)\n    \n        self.data = engine(arr)\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]""}",indexing_engines,time,-0.020116652705910152
29975,module-level,"terminus-2,claude",1.1085330999349416,1.10951166250629,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing,indexing,"{""indexing.CategoricalIndexIndexing.time_get_indexer_list"": ""class CategoricalIndexIndexing:\n    def time_get_indexer_list(self, index):\n        self.data_unique.get_indexer(self.cat_list)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalIndexIndexing:\n    def setup(self, index):\n        N = 10**5\n        values = list(\""a\"" * N + \""b\"" * N + \""c\"" * N)\n        indices = {\n            \""monotonic_incr\"": CategoricalIndex(values),\n            \""monotonic_decr\"": CategoricalIndex(reversed(values)),\n            \""non_monotonic\"": CategoricalIndex(list(\""abc\"" * N)),\n        }\n        self.data = indices[index]\n        self.data_unique = CategoricalIndex([str(i) for i in range(N * 3)])\n    \n        self.int_scalar = 10000\n        self.int_list = list(range(10000))\n    \n        self.cat_scalar = \""b\""\n        self.cat_list = [\""1\"", \""3\""]"", ""indexing.CategoricalIndexIndexing.time_getitem_list"": ""class CategoricalIndexIndexing:\n    def time_getitem_list(self, index):\n        self.data[self.int_list]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalIndexIndexing:\n    def setup(self, index):\n        N = 10**5\n        values = list(\""a\"" * N + \""b\"" * N + \""c\"" * N)\n        indices = {\n            \""monotonic_incr\"": CategoricalIndex(values),\n            \""monotonic_decr\"": CategoricalIndex(reversed(values)),\n            \""non_monotonic\"": CategoricalIndex(list(\""abc\"" * N)),\n        }\n        self.data = indices[index]\n        self.data_unique = CategoricalIndex([str(i) for i in range(N * 3)])\n    \n        self.int_scalar = 10000\n        self.int_list = list(range(10000))\n    \n        self.cat_scalar = \""b\""\n        self.cat_list = [\""1\"", \""3\""]"", ""indexing.CategoricalIndexIndexing.time_getitem_scalar"": ""class CategoricalIndexIndexing:\n    def time_getitem_scalar(self, index):\n        self.data[self.int_scalar]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalIndexIndexing:\n    def setup(self, index):\n        N = 10**5\n        values = list(\""a\"" * N + \""b\"" * N + \""c\"" * N)\n        indices = {\n            \""monotonic_incr\"": CategoricalIndex(values),\n            \""monotonic_decr\"": CategoricalIndex(reversed(values)),\n            \""non_monotonic\"": CategoricalIndex(list(\""abc\"" * N)),\n        }\n        self.data = indices[index]\n        self.data_unique = CategoricalIndex([str(i) for i in range(N * 3)])\n    \n        self.int_scalar = 10000\n        self.int_list = list(range(10000))\n    \n        self.cat_scalar = \""b\""\n        self.cat_list = [\""1\"", \""3\""]"", ""indexing.DataFrameNumericIndexing.time_loc_dups"": ""class DataFrameNumericIndexing:\n    def time_loc_dups(self, index, index_structure):\n        self.df_dup.loc[self.idx_dupe]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DataFrameNumericIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**5\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.idx_dupe = np.array(range(30)) * 99\n        self.df = DataFrame(np.random.randn(N, 5), index=indices[index_structure])\n        self.df_dup = concat([self.df, 2 * self.df, 3 * self.df])\n        self.bool_indexer = [True] * (N // 2) + [False] * (N - N // 2)"", ""indexing.DataFrameStringIndexing.time_at_setitem"": ""class DataFrameStringIndexing:\n    def time_at_setitem(self):\n        self.df.at[self.idx_scalar, self.col_scalar] = 0.0\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DataFrameStringIndexing:\n    def setup(self):\n        index = tm.makeStringIndex(1000)\n        columns = tm.makeStringIndex(30)\n        with warnings.catch_warnings(record=True):\n            self.df = DataFrame(np.random.randn(1000, 30), index=index, columns=columns)\n        self.idx_scalar = index[100]\n        self.col_scalar = columns[10]\n        self.bool_indexer = self.df[self.col_scalar] > 0\n        self.bool_obj_indexer = self.bool_indexer.astype(object)\n        self.boolean_indexer = (self.df[self.col_scalar] > 0).astype(\""boolean\"")"", ""indexing.GetItemSingleColumn.time_frame_getitem_single_column_int"": ""class GetItemSingleColumn:\n    def time_frame_getitem_single_column_int(self):\n        self.df_int_col[0]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GetItemSingleColumn:\n    def setup(self):\n        self.df_string_col = DataFrame(np.random.randn(3000, 1), columns=[\""A\""])\n        self.df_int_col = DataFrame(np.random.randn(3000, 1))"", ""indexing.IndexSingleRow.time_loc_row"": ""class IndexSingleRow:\n    def time_loc_row(self, unique_cols):\n        self.df.loc[10000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IndexSingleRow:\n    def setup(self, unique_cols):\n        arr = np.arange(10**7).reshape(-1, 10)\n        df = DataFrame(arr)\n        dtypes = [\""u1\"", \""u2\"", \""u4\"", \""u8\"", \""i1\"", \""i2\"", \""i4\"", \""i8\"", \""f8\"", \""f4\""]\n        for i, d in enumerate(dtypes):\n            df[i] = df[i].astype(d)\n    \n        if not unique_cols:\n            # GH#33032 single-row lookups with non-unique columns were\n            #  15x slower than with unique columns\n            df.columns = [\""A\"", \""A\""] + list(df.columns[2:])\n    \n        self.df = df"", ""indexing.MethodLookup.time_lookup_iloc"": ""class MethodLookup:\n    def time_lookup_iloc(self, s):\n        s.iloc\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MethodLookup:\n    def setup_cache(self):\n        s = Series()\n        return s"", ""indexing.MethodLookup.time_lookup_loc"": ""class MethodLookup:\n    def time_lookup_loc(self, s):\n        s.loc\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MethodLookup:\n    def setup_cache(self):\n        s = Series()\n        return s"", ""indexing.MultiIndexing.time_loc_all_scalars"": ""class MultiIndexing:\n    def time_loc_all_scalars(self, unique_levels):\n        target = tuple([self.tgt_scalar] * self.nlevels)\n        self.df.loc[target, :]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MultiIndexing:\n    def setup(self, unique_levels):\n        self.nlevels = 2\n        if unique_levels:\n            mi = MultiIndex.from_arrays([range(1000000)] * self.nlevels)\n        else:\n            mi = MultiIndex.from_product([range(1000)] * self.nlevels)\n        self.df = DataFrame(np.random.randn(len(mi)), index=mi)\n    \n        self.tgt_slice = slice(200, 800)\n        self.tgt_null_slice = slice(None)\n        self.tgt_list = list(range(0, 1000, 10))\n        self.tgt_scalar = 500\n    \n        bool_indexer = np.zeros(len(mi), dtype=np.bool_)\n        bool_indexer[slice(0, len(mi), 100)] = True\n        self.tgt_bool_indexer = bool_indexer"", ""indexing.NonNumericSeriesIndexing.time_getitem_label_slice"": ""class NonNumericSeriesIndexing:\n    def time_getitem_label_slice(self, index, index_structure):\n        self.s[: self.lbl]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]"", ""indexing.NonNumericSeriesIndexing.time_getitem_list_like"": ""class NonNumericSeriesIndexing:\n    def time_getitem_list_like(self, index, index_structure):\n        self.s[[self.lbl]]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]"", ""indexing.NonNumericSeriesIndexing.time_getitem_pos_slice"": ""class NonNumericSeriesIndexing:\n    def time_getitem_pos_slice(self, index, index_structure):\n        self.s[:80000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]"", ""indexing.NonNumericSeriesIndexing.time_getitem_scalar"": ""class NonNumericSeriesIndexing:\n    def time_getitem_scalar(self, index, index_structure):\n        self.s[self.lbl]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]"", ""indexing.NumericMaskedIndexing.time_get_indexer"": ""class NumericMaskedIndexing:\n    def time_get_indexer(self, dtype, monotonic):\n        self.data.get_indexer(self.indexer)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericMaskedIndexing:\n    def setup(self, dtype, monotonic):\n        indices = {\n            True: Index(self.monotonic_list, dtype=dtype),\n            False: Index(self.non_monotonic_list, dtype=dtype).append(\n                Index([NA], dtype=dtype)\n            ),\n        }\n        self.data = indices[monotonic]\n        self.indexer = np.arange(300, 1_000)\n        self.data_dups = self.data.append(self.data)"", ""indexing.NumericMaskedIndexing.time_get_indexer_dups"": ""class NumericMaskedIndexing:\n    def time_get_indexer_dups(self, dtype, monotonic):\n        self.data.get_indexer_for(self.indexer)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericMaskedIndexing:\n    def setup(self, dtype, monotonic):\n        indices = {\n            True: Index(self.monotonic_list, dtype=dtype),\n            False: Index(self.non_monotonic_list, dtype=dtype).append(\n                Index([NA], dtype=dtype)\n            ),\n        }\n        self.data = indices[monotonic]\n        self.indexer = np.arange(300, 1_000)\n        self.data_dups = self.data.append(self.data)"", ""indexing.NumericSeriesIndexing.time_getitem_array"": ""class NumericSeriesIndexing:\n    def time_getitem_array(self, index, index_structure):\n        self.data[self.array]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_getitem_list_like"": ""class NumericSeriesIndexing:\n    def time_getitem_list_like(self, index, index_structure):\n        self.data[[800000]]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_getitem_lists"": ""class NumericSeriesIndexing:\n    def time_getitem_lists(self, index, index_structure):\n        self.data[self.array_list]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_getitem_scalar"": ""class NumericSeriesIndexing:\n    def time_getitem_scalar(self, index, index_structure):\n        self.data[800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_getitem_slice"": ""class NumericSeriesIndexing:\n    def time_getitem_slice(self, index, index_structure):\n        self.data[:800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_iloc_slice"": ""class NumericSeriesIndexing:\n    def time_iloc_slice(self, index, index_structure):\n        self.data.iloc[:800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_loc_array"": ""class NumericSeriesIndexing:\n    def time_loc_array(self, index, index_structure):\n        self.data.loc[self.array]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_loc_list_like"": ""class NumericSeriesIndexing:\n    def time_loc_list_like(self, index, index_structure):\n        self.data.loc[[800000]]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_loc_scalar"": ""class NumericSeriesIndexing:\n    def time_loc_scalar(self, index, index_structure):\n        self.data.loc[800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing,time,-0.0006920527378700978
29968,module-level,"terminus-2,claude",1.1985708699042772,1.1970532082915912,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval,eval,"{""eval.Eval.time_add"": ""class Eval:\n    def time_add(self, engine, threads):\n        pd.eval(\""self.df + self.df2 + self.df3 + self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)"", ""eval.Eval.time_and"": ""class Eval:\n    def time_and(self, engine, threads):\n        pd.eval(\n            \""(self.df > 0) & (self.df2 > 0) & (self.df3 > 0) & (self.df4 > 0)\"",\n            engine=engine,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)"", ""eval.Eval.time_chained_cmp"": ""class Eval:\n    def time_chained_cmp(self, engine, threads):\n        pd.eval(\""self.df < self.df2 < self.df3 < self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)"", ""eval.Eval.time_mult"": ""class Eval:\n    def time_mult(self, engine, threads):\n        pd.eval(\""self.df * self.df2 * self.df3 * self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)"", ""eval.Query.time_query_datetime_column"": ""class Query:\n    def time_query_datetime_column(self):\n        self.df.query(\""dates < @self.ts\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Query:\n    def setup(self):\n        N = 10**6\n        halfway = (N // 2) - 1\n        index = pd.date_range(\""20010101\"", periods=N, freq=\""T\"")\n        s = pd.Series(index)\n        self.ts = s.iloc[halfway]\n        self.df = pd.DataFrame({\""a\"": np.random.randn(N), \""dates\"": index}, index=index)\n        data = np.random.randn(N)\n        self.min_val = data.min()\n        self.max_val = data.max()"", ""eval.Query.time_query_datetime_index"": ""class Query:\n    def time_query_datetime_index(self):\n        self.df.query(\""index < @self.ts\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Query:\n    def setup(self):\n        N = 10**6\n        halfway = (N // 2) - 1\n        index = pd.date_range(\""20010101\"", periods=N, freq=\""T\"")\n        s = pd.Series(index)\n        self.ts = s.iloc[halfway]\n        self.df = pd.DataFrame({\""a\"": np.random.randn(N), \""dates\"": index}, index=index)\n        data = np.random.randn(N)\n        self.min_val = data.min()\n        self.max_val = data.max()"", ""eval.Query.time_query_with_boolean_selection"": ""class Query:\n    def time_query_with_boolean_selection(self):\n        self.df.query(\""(a >= @self.min_val) & (a <= @self.max_val)\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Query:\n    def setup(self):\n        N = 10**6\n        halfway = (N // 2) - 1\n        index = pd.date_range(\""20010101\"", periods=N, freq=\""T\"")\n        s = pd.Series(index)\n        self.ts = s.iloc[halfway]\n        self.df = pd.DataFrame({\""a\"": np.random.randn(N), \""dates\"": index}, index=index)\n        data = np.random.randn(N)\n        self.min_val = data.min()\n        self.max_val = data.max()""}",eval,time,0.0010733109000607929
29985,module-level,"terminus-2,claude",1.0257173985057948,1.0636931337086513,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling,rolling,"{""rolling.Apply.time_rolling"": ""class Apply:\n    def time_rolling(self, constructor, window, dtype, function, raw):\n        self.roll.apply(function, raw=raw)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, constructor, window, dtype, function, raw):\n        N = 10**3\n        arr = (100 * np.random.random(N)).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)"", ""rolling.EWMMethods.time_ewm"": ""class EWMMethods:\n    def time_ewm(self, constructor, kwargs_method, dtype):\n        getattr(self.ewm, self.method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass EWMMethods:\n    def setup(self, constructor, kwargs_method, dtype):\n        N = 10**5\n        kwargs, method = kwargs_method\n        arr = (100 * np.random.random(N)).astype(dtype)\n        self.method = method\n        self.ewm = getattr(pd, constructor)(arr).ewm(**kwargs)"", ""rolling.ForwardWindowMethods.time_rolling"": ""class ForwardWindowMethods:\n    def time_rolling(self, constructor, window_size, dtype, method):\n        getattr(self.roll, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ForwardWindowMethods:\n    def setup(self, constructor, window_size, dtype, method):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=window_size)\n        self.roll = getattr(pd, constructor)(arr).rolling(window=indexer)"", ""rolling.Groupby.time_method"": ""class Groupby:\n    def time_method(self, method, window_kwargs):\n        getattr(self.groupby_window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Groupby:\n    def setup(self, method, window_kwargs):\n        N = 1000\n        window, kwargs = window_kwargs\n        df = pd.DataFrame(\n            {\n                \""A\"": [str(i) for i in range(N)] * 10,\n                \""B\"": list(range(N)) * 10,\n            }\n        )\n        if isinstance(kwargs.get(\""window\"", None), str):\n            df.index = pd.date_range(start=\""1900-01-01\"", freq=\""1min\"", periods=N * 10)\n        self.groupby_window = getattr(df.groupby(\""A\""), window)(**kwargs)"", ""rolling.Methods.time_method"": ""class Methods:\n    def time_method(self, constructor, window_kwargs, dtype, method):\n        getattr(self.window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Methods:\n    def setup(self, constructor, window_kwargs, dtype, method):\n        N = 10**5\n        window, kwargs = window_kwargs\n        arr = (100 * np.random.random(N)).astype(dtype)\n        obj = getattr(pd, constructor)(arr)\n        self.window = getattr(obj, window)(**kwargs)"", ""rolling.Pairwise.time_groupby"": ""class Pairwise:\n    def time_groupby(self, kwargs_window, method, pairwise):\n        getattr(self.window_group, method)(self.df, pairwise=pairwise)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Pairwise:\n    def setup(self, kwargs_window, method, pairwise):\n        N = 10**4\n        n_groups = 20\n        kwargs, window = kwargs_window\n        groups = [i for _ in range(N // n_groups) for i in range(n_groups)]\n        arr = np.random.random(N)\n        self.df = pd.DataFrame(arr)\n        self.window = getattr(self.df, window)(**kwargs)\n        self.window_group = getattr(\n            pd.DataFrame({\""A\"": groups, \""B\"": arr}).groupby(\""A\""), window\n        )(**kwargs)"", ""rolling.Pairwise.time_pairwise"": ""class Pairwise:\n    def time_pairwise(self, kwargs_window, method, pairwise):\n        getattr(self.window, method)(self.df, pairwise=pairwise)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Pairwise:\n    def setup(self, kwargs_window, method, pairwise):\n        N = 10**4\n        n_groups = 20\n        kwargs, window = kwargs_window\n        groups = [i for _ in range(N // n_groups) for i in range(n_groups)]\n        arr = np.random.random(N)\n        self.df = pd.DataFrame(arr)\n        self.window = getattr(self.df, window)(**kwargs)\n        self.window_group = getattr(\n            pd.DataFrame({\""A\"": groups, \""B\"": arr}).groupby(\""A\""), window\n        )(**kwargs)"", ""rolling.Quantile.time_quantile"": ""class Quantile:\n    def time_quantile(self, constructor, window, dtype, percentile, interpolation):\n        self.roll.quantile(percentile, interpolation=interpolation)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, constructor, window, dtype, percentile, interpolation):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)"", ""rolling.Rank.time_rank"": ""class Rank:\n    def time_rank(self, constructor, window, dtype, percentile, ascending, method):\n        self.roll.rank(pct=percentile, ascending=ascending, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Rank:\n    def setup(self, constructor, window, dtype, percentile, ascending, method):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)"", ""rolling.VariableWindowMethods.time_method"": ""class Methods:\n    def time_method(self, constructor, window_kwargs, dtype, method):\n        getattr(self.window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass VariableWindowMethods:\n    def setup(self, constructor, window, dtype, method):\n        N = 10**5\n        arr = (100 * np.random.random(N)).astype(dtype)\n        index = pd.date_range(\""2017-01-01\"", periods=N, freq=\""5s\"")\n        self.window = getattr(pd, constructor)(arr, index=index).rolling(window)""}",rolling,time,-0.026856955589007404
29981,module-level,"terminus-2,claude",0.9950973228040104,1.0409424730696608,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,join_merge,join_merge,"{""join_merge.Concat.time_concat_mixed_ndims"": ""class Concat:\n    def time_concat_mixed_ndims(self, axis):\n        concat(self.mixed_ndims, axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Concat:\n    def setup(self, axis):\n        N = 1000\n        s = Series(N, index=tm.makeStringIndex(N))\n        self.series = [s[i:-i] for i in range(1, 10)] * 50\n        self.small_frames = [DataFrame(np.random.randn(5, 4))] * 1000\n        df = DataFrame(\n            {\""A\"": range(N)}, index=date_range(\""20130101\"", periods=N, freq=\""s\"")\n        )\n        self.empty_left = [DataFrame(), df]\n        self.empty_right = [df, DataFrame()]\n        self.mixed_ndims = [df, df.head(N // 2)]"", ""join_merge.ConcatIndexDtype.time_concat_series"": ""class ConcatIndexDtype:\n    def time_concat_series(self, dtype, structure, axis, sort):\n        concat(self.series, axis=axis, sort=sort)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ConcatIndexDtype:\n    def setup(self, dtype, structure, axis, sort):\n        N = 10_000\n        if dtype == \""datetime64[ns]\"":\n            vals = date_range(\""1970-01-01\"", periods=N)\n        elif dtype in (\""int64\"", \""Int64\""):\n            vals = np.arange(N, dtype=np.int64)\n        elif dtype in (\""string[python]\"", \""string[pyarrow]\""):\n            vals = tm.makeStringIndex(N)\n        else:\n            raise NotImplementedError\n    \n        idx = Index(vals, dtype=dtype)\n    \n        if structure == \""monotonic\"":\n            idx = idx.sort_values()\n        elif structure == \""non_monotonic\"":\n            idx = idx[::-1]\n        elif structure == \""has_na\"":\n            if not idx._can_hold_na:\n                raise NotImplementedError\n            idx = Index([None], dtype=dtype).append(idx)\n        else:\n            raise NotImplementedError\n    \n        self.series = [Series(i, idx[:-i]) for i in range(1, 6)]"", ""join_merge.MergeAsof.time_by_int"": ""class MergeAsof:\n    def time_by_int(self, direction, tolerance):\n        merge_asof(\n            self.df1c,\n            self.df2c,\n            on=\""time\"",\n            by=\""key2\"",\n            direction=direction,\n            tolerance=tolerance,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MergeAsof:\n    def setup(self, direction, tolerance):\n        one_count = 200000\n        two_count = 1000000\n    \n        df1 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, one_count / 20, one_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), one_count),\n                \""key2\"": np.random.randint(0, 25, one_count),\n                \""value1\"": np.random.randn(one_count),\n            }\n        )\n        df2 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, two_count / 20, two_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), two_count),\n                \""key2\"": np.random.randint(0, 25, two_count),\n                \""value2\"": np.random.randn(two_count),\n            }\n        )\n    \n        df1 = df1.sort_values(\""time\"")\n        df2 = df2.sort_values(\""time\"")\n    \n        df1[\""time32\""] = np.int32(df1.time)\n        df2[\""time32\""] = np.int32(df2.time)\n    \n        df1[\""timeu64\""] = np.uint64(df1.time)\n        df2[\""timeu64\""] = np.uint64(df2.time)\n    \n        self.df1a = df1[[\""time\"", \""value1\""]]\n        self.df2a = df2[[\""time\"", \""value2\""]]\n        self.df1b = df1[[\""time\"", \""key\"", \""value1\""]]\n        self.df2b = df2[[\""time\"", \""key\"", \""value2\""]]\n        self.df1c = df1[[\""time\"", \""key2\"", \""value1\""]]\n        self.df2c = df2[[\""time\"", \""key2\"", \""value2\""]]\n        self.df1d = df1[[\""time32\"", \""value1\""]]\n        self.df2d = df2[[\""time32\"", \""value2\""]]\n        self.df1e = df1[[\""time\"", \""key\"", \""key2\"", \""value1\""]]\n        self.df2e = df2[[\""time\"", \""key\"", \""key2\"", \""value2\""]]\n        self.df1f = df1[[\""timeu64\"", \""value1\""]]\n        self.df2f = df2[[\""timeu64\"", \""value2\""]]"", ""join_merge.MergeAsof.time_by_object"": ""class MergeAsof:\n    def time_by_object(self, direction, tolerance):\n        merge_asof(\n            self.df1b,\n            self.df2b,\n            on=\""time\"",\n            by=\""key\"",\n            direction=direction,\n            tolerance=tolerance,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MergeAsof:\n    def setup(self, direction, tolerance):\n        one_count = 200000\n        two_count = 1000000\n    \n        df1 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, one_count / 20, one_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), one_count),\n                \""key2\"": np.random.randint(0, 25, one_count),\n                \""value1\"": np.random.randn(one_count),\n            }\n        )\n        df2 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, two_count / 20, two_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), two_count),\n                \""key2\"": np.random.randint(0, 25, two_count),\n                \""value2\"": np.random.randn(two_count),\n            }\n        )\n    \n        df1 = df1.sort_values(\""time\"")\n        df2 = df2.sort_values(\""time\"")\n    \n        df1[\""time32\""] = np.int32(df1.time)\n        df2[\""time32\""] = np.int32(df2.time)\n    \n        df1[\""timeu64\""] = np.uint64(df1.time)\n        df2[\""timeu64\""] = np.uint64(df2.time)\n    \n        self.df1a = df1[[\""time\"", \""value1\""]]\n        self.df2a = df2[[\""time\"", \""value2\""]]\n        self.df1b = df1[[\""time\"", \""key\"", \""value1\""]]\n        self.df2b = df2[[\""time\"", \""key\"", \""value2\""]]\n        self.df1c = df1[[\""time\"", \""key2\"", \""value1\""]]\n        self.df2c = df2[[\""time\"", \""key2\"", \""value2\""]]\n        self.df1d = df1[[\""time32\"", \""value1\""]]\n        self.df2d = df2[[\""time32\"", \""value2\""]]\n        self.df1e = df1[[\""time\"", \""key\"", \""key2\"", \""value1\""]]\n        self.df2e = df2[[\""time\"", \""key\"", \""key2\"", \""value2\""]]\n        self.df1f = df1[[\""timeu64\"", \""value1\""]]\n        self.df2f = df2[[\""timeu64\"", \""value2\""]]"", ""join_merge.MergeAsof.time_on_int32"": ""class MergeAsof:\n    def time_on_int32(self, direction, tolerance):\n        merge_asof(\n            self.df1d, self.df2d, on=\""time32\"", direction=direction, tolerance=tolerance\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MergeAsof:\n    def setup(self, direction, tolerance):\n        one_count = 200000\n        two_count = 1000000\n    \n        df1 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, one_count / 20, one_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), one_count),\n                \""key2\"": np.random.randint(0, 25, one_count),\n                \""value1\"": np.random.randn(one_count),\n            }\n        )\n        df2 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, two_count / 20, two_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), two_count),\n                \""key2\"": np.random.randint(0, 25, two_count),\n                \""value2\"": np.random.randn(two_count),\n            }\n        )\n    \n        df1 = df1.sort_values(\""time\"")\n        df2 = df2.sort_values(\""time\"")\n    \n        df1[\""time32\""] = np.int32(df1.time)\n        df2[\""time32\""] = np.int32(df2.time)\n    \n        df1[\""timeu64\""] = np.uint64(df1.time)\n        df2[\""timeu64\""] = np.uint64(df2.time)\n    \n        self.df1a = df1[[\""time\"", \""value1\""]]\n        self.df2a = df2[[\""time\"", \""value2\""]]\n        self.df1b = df1[[\""time\"", \""key\"", \""value1\""]]\n        self.df2b = df2[[\""time\"", \""key\"", \""value2\""]]\n        self.df1c = df1[[\""time\"", \""key2\"", \""value1\""]]\n        self.df2c = df2[[\""time\"", \""key2\"", \""value2\""]]\n        self.df1d = df1[[\""time32\"", \""value1\""]]\n        self.df2d = df2[[\""time32\"", \""value2\""]]\n        self.df1e = df1[[\""time\"", \""key\"", \""key2\"", \""value1\""]]\n        self.df2e = df2[[\""time\"", \""key\"", \""key2\"", \""value2\""]]\n        self.df1f = df1[[\""timeu64\"", \""value1\""]]\n        self.df2f = df2[[\""timeu64\"", \""value2\""]]"", ""join_merge.MergeAsof.time_on_uint64"": ""class MergeAsof:\n    def time_on_uint64(self, direction, tolerance):\n        merge_asof(\n            self.df1f, self.df2f, on=\""timeu64\"", direction=direction, tolerance=tolerance\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MergeAsof:\n    def setup(self, direction, tolerance):\n        one_count = 200000\n        two_count = 1000000\n    \n        df1 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, one_count / 20, one_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), one_count),\n                \""key2\"": np.random.randint(0, 25, one_count),\n                \""value1\"": np.random.randn(one_count),\n            }\n        )\n        df2 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, two_count / 20, two_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), two_count),\n                \""key2\"": np.random.randint(0, 25, two_count),\n                \""value2\"": np.random.randn(two_count),\n            }\n        )\n    \n        df1 = df1.sort_values(\""time\"")\n        df2 = df2.sort_values(\""time\"")\n    \n        df1[\""time32\""] = np.int32(df1.time)\n        df2[\""time32\""] = np.int32(df2.time)\n    \n        df1[\""timeu64\""] = np.uint64(df1.time)\n        df2[\""timeu64\""] = np.uint64(df2.time)\n    \n        self.df1a = df1[[\""time\"", \""value1\""]]\n        self.df2a = df2[[\""time\"", \""value2\""]]\n        self.df1b = df1[[\""time\"", \""key\"", \""value1\""]]\n        self.df2b = df2[[\""time\"", \""key\"", \""value2\""]]\n        self.df1c = df1[[\""time\"", \""key2\"", \""value1\""]]\n        self.df2c = df2[[\""time\"", \""key2\"", \""value2\""]]\n        self.df1d = df1[[\""time32\"", \""value1\""]]\n        self.df2d = df2[[\""time32\"", \""value2\""]]\n        self.df1e = df1[[\""time\"", \""key\"", \""key2\"", \""value1\""]]\n        self.df2e = df2[[\""time\"", \""key\"", \""key2\"", \""value2\""]]\n        self.df1f = df1[[\""timeu64\"", \""value1\""]]\n        self.df2f = df2[[\""timeu64\"", \""value2\""]]""}",join_merge,time,-0.03242231277627326
29987,module-level,"terminus-2,claude",0.90556049957991,0.9104966439230252,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse,sparse,"{""sparse.Arithmetic.time_add"": ""class Arithmetic:\n    def time_add(self, dense_proportion, fill_value):\n        self.array1 + self.array2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)"", ""sparse.Arithmetic.time_divide"": ""class Arithmetic:\n    def time_divide(self, dense_proportion, fill_value):\n        self.array1 / self.array2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)"", ""sparse.Arithmetic.time_make_union"": ""class Arithmetic:\n    def time_make_union(self, dense_proportion, fill_value):\n        self.array1.sp_index.make_union(self.array2.sp_index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)"", ""sparse.ArithmeticBlock.time_addition"": ""class ArithmeticBlock:\n    def time_addition(self, fill_value):\n        self.arr1 + self.arr2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )"", ""sparse.ArithmeticBlock.time_division"": ""class ArithmeticBlock:\n    def time_division(self, fill_value):\n        self.arr1 / self.arr2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )"", ""sparse.ArithmeticBlock.time_make_union"": ""class ArithmeticBlock:\n    def time_make_union(self, fill_value):\n        self.arr1.sp_index.make_union(self.arr2.sp_index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )"", ""sparse.GetItem.time_slice"": ""class GetItem:\n    def time_slice(self):\n        self.sp_arr[1:]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GetItem:\n    def setup(self):\n        N = 1_000_000\n        d = 1e-5\n        arr = make_array(N, d, np.nan, np.float64)\n        self.sp_arr = SparseArray(arr)""}",sparse,time,-0.0034909083048905083
29979,module-level,"terminus-2,claude",0.8990875217472502,1.065536048258366,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.json,io.json,"{""io.json.NormalizeJSON.time_normalize_json"": ""class NormalizeJSON:\n    def time_normalize_json(self, orient, frame):\n        json_normalize(self.data)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NormalizeJSON:\n    def setup(self, orient, frame):\n        data = {\n            \""hello\"": [\""thisisatest\"", 999898, \""mixed types\""],\n            \""nest1\"": {\""nest2\"": {\""nest3\"": \""nest3_value\"", \""nest3_int\"": 3445}},\n            \""nest1_list\"": {\""nest2\"": [\""blah\"", 32423, 546456.876, 92030234]},\n            \""hello2\"": \""string\"",\n        }\n        self.data = [data for i in range(10000)]"", ""io.json.ToJSONWide.time_to_json_wide"": ""class ToJSONWide:\n    def time_to_json_wide(self, orient, frame):\n        self.df_wide.to_json(self.fname, orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSONWide:\n    def setup(self, orient, frame):\n        super().setup(orient, frame)\n        base_df = getattr(self, frame).copy()\n        df_wide = concat([base_df.iloc[:100]] * 1000, ignore_index=True, axis=1)\n        self.df_wide = df_wide""}",io.json,time,-0.11771465807009608
29993,module-level,"terminus-2,claude",1.0308592747002812,1.0437129734639106,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_add"": ""class OffestDatetimeArithmetic:\n    def time_add(self, offset):\n        self.date + offset\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")"", ""tslibs.offsets.OffestDatetimeArithmetic.time_add_10"": ""class OffestDatetimeArithmetic:\n    def time_add_10(self, offset):\n        self.date + (10 * offset)\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")"", ""tslibs.offsets.OffestDatetimeArithmetic.time_add_np_dt64"": ""class OffestDatetimeArithmetic:\n    def time_add_np_dt64(self, offset):\n        offset + self.dt64\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")"", ""tslibs.offsets.OffestDatetimeArithmetic.time_subtract"": ""class OffestDatetimeArithmetic:\n    def time_subtract(self, offset):\n        self.date - offset\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")"", ""tslibs.offsets.OffestDatetimeArithmetic.time_subtract_10"": ""class OffestDatetimeArithmetic:\n    def time_subtract_10(self, offset):\n        self.date - (10 * offset)\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")"", ""tslibs.offsets.OnOffset.time_on_offset"": ""class OnOffset:\n    def time_on_offset(self, offset):\n        for date in self.dates:\n            offset.is_on_offset(date)\n\n    def setup(self, offset):\n        self.dates = [\n            datetime(2016, m, d)\n            for m in [10, 11, 12]\n            for d in [1, 2, 3, 28, 29, 30, 31]\n            if not (m == 11 and d == 31)\n        ]""}",tslibs.offsets,time,-0.009090310299596468
29982,module-level,"terminus-2,claude",0.9832094273508514,1.0420834684650648,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,libs,libs,"{""libs.InferDtype.time_infer_dtype"": ""class InferDtype:\n    def time_infer_dtype(self, dtype):\n        infer_dtype(self.data_dict[dtype], skipna=False)"", ""libs.InferDtype.time_infer_dtype_skipna"": ""class InferDtype:\n    def time_infer_dtype_skipna(self, dtype):\n        infer_dtype(self.data_dict[dtype], skipna=True)"", ""libs.ScalarListLike.time_is_list_like"": ""class ScalarListLike:\n    def time_is_list_like(self, param):\n        is_list_like(param)"", ""libs.ScalarListLike.time_is_scalar"": ""class ScalarListLike:\n    def time_is_scalar(self, param):\n        is_scalar(param)""}",libs,time,-0.041636521297180665
29963,module-level,"terminus-2,claude",1.026664975818492,0.9813637113697236,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,array,array,"{""array.ArrowExtensionArray.time_to_numpy"": ""class ArrowExtensionArray:\n    def time_to_numpy(self, dtype, hasna):\n        self.arr.to_numpy()\n\n    def setup(self, dtype, hasna):\n        N = 100_000\n        if dtype == \""boolean[pyarrow]\"":\n            data = np.random.choice([True, False], N, replace=True)\n        elif dtype == \""float64[pyarrow]\"":\n            data = np.random.randn(N)\n        elif dtype == \""int64[pyarrow]\"":\n            data = np.arange(N)\n        elif dtype == \""string[pyarrow]\"":\n            data = tm.rands_array(10, N)\n        elif dtype == \""timestamp[ns][pyarrow]\"":\n            data = pd.date_range(\""2000-01-01\"", freq=\""s\"", periods=N)\n        else:\n            raise NotImplementedError\n    \n        arr = pd.array(data, dtype=dtype)\n        if hasna:\n            arr[::2] = pd.NA\n        self.arr = arr"", ""array.ArrowStringArray.time_setitem"": ""class ArrowStringArray:\n    def time_setitem(self, multiple_chunks):\n        for i in range(200):\n            self.array[i] = \""foo\""\n\n    def setup(self, multiple_chunks):\n        try:\n            import pyarrow as pa\n        except ImportError:\n            raise NotImplementedError\n        strings = tm.rands_array(3, 10_000)\n        if multiple_chunks:\n            chunks = [strings[i : i + 100] for i in range(0, len(strings), 100)]\n            self.array = pd.arrays.ArrowStringArray(pa.chunked_array(chunks))\n        else:\n            self.array = pd.arrays.ArrowStringArray(pa.array(strings))"", ""array.ArrowStringArray.time_setitem_slice"": ""class ArrowStringArray:\n    def time_setitem_slice(self, multiple_chunks):\n        self.array[::10] = \""foo\""\n\n    def setup(self, multiple_chunks):\n        try:\n            import pyarrow as pa\n        except ImportError:\n            raise NotImplementedError\n        strings = tm.rands_array(3, 10_000)\n        if multiple_chunks:\n            chunks = [strings[i : i + 100] for i in range(0, len(strings), 100)]\n            self.array = pd.arrays.ArrowStringArray(pa.chunked_array(chunks))\n        else:\n            self.array = pd.arrays.ArrowStringArray(pa.array(strings))"", ""array.BooleanArray.time_from_float_array"": ""class BooleanArray:\n    def time_from_float_array(self):\n        pd.array(self.values_float, dtype=\""boolean\"")\n\n    def setup(self):\n        self.values_bool = np.array([True, False, True, False])\n        self.values_float = np.array([1.0, 0.0, 1.0, 0.0])\n        self.values_integer = np.array([1, 0, 1, 0])\n        self.values_integer_like = [1, 0, 1, 0]\n        self.data = np.array([True, False, True, False])\n        self.mask = np.array([False, False, True, False])""}",array,time,0.032037669341420424
29995,module-level,"terminus-2,claude",0.9740867278162968,0.9958619199845872,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.resolution,tslibs.resolution,"{""tslibs.resolution.TimeResolution.time_get_resolution"": ""class TimeResolution:\n    def time_get_resolution(self, unit, size, tz):\n        get_resolution(self.i8data, tz)\n\n    def setup(self, unit, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        arr = arr.view(f\""M8[{unit}]\"").astype(\""M8[ns]\"").view(\""i8\"")\n        self.i8data = arr""}",tslibs.resolution,time,-0.015399711575877214
29991,module-level,"terminus-2,claude",1.0159245625747002,1.0230613529919172,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.fields,tslibs.fields,"{""tslibs.fields.TimeGetDateField.time_get_date_field"": ""class TimeGetDateField:\n    def time_get_date_field(self, size, field):\n        get_date_field(self.i8data, field)\n\n    def setup(self, size, field):\n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr"", ""tslibs.fields.TimeGetStartEndField.time_get_start_end_field"": ""class TimeGetStartEndField:\n    def time_get_start_end_field(self, size, side, period, freqstr, month_kw):\n        get_start_end_field(self.i8data, self.attrname, freqstr, month_kw=month_kw)\n\n    def setup(self, size, side, period, freqstr, month_kw):\n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr\n    \n        self.attrname = f\""is_{period}_{side}\"""", ""tslibs.fields.TimeGetTimedeltaField.time_get_timedelta_field"": ""class TimeGetTimedeltaField:\n    def time_get_timedelta_field(self, size, field):\n        get_timedelta_field(self.i8data, field)\n\n    def setup(self, size, field):\n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.fields,time,-0.00504723508996962
29977,module-level,"terminus-2,claude",0.9864513273764552,1.0287380341736552,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,inference,inference,"{""inference.ToDatetimeCache.time_unique_seconds_and_unit"": ""class ToDatetimeCache:\n    def time_unique_seconds_and_unit(self, cache):\n        to_datetime(self.unique_numeric_seconds, unit=\""s\"", cache=cache)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToDatetimeCache:\n    def setup(self, cache):\n        N = 10000\n        self.unique_numeric_seconds = list(range(N))\n        self.dup_numeric_seconds = [1000] * N\n        self.dup_string_dates = [\""2000-02-11\""] * N\n        self.dup_string_with_tz = [\""2000-02-11 15:00:00-0800\""] * N"", ""inference.ToNumericDowncast.time_downcast"": ""class ToNumericDowncast:\n    def time_downcast(self, dtype, downcast):\n        to_numeric(self.data, downcast=downcast)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToNumericDowncast:\n    def setup(self, dtype, downcast):\n        self.data = self.data_dict[dtype]""}",inference,time,-0.02990573323705794
29994,module-level,"terminus-2,claude",0.9753004437330034,0.9881397547958164,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period,tslibs.period,"{""tslibs.period.PeriodProperties.time_property"": ""class PeriodProperties:\n    def time_property(self, freq, attr):\n        getattr(self.per, attr)\n\n    def setup(self, freq, attr):\n        self.per = Period(\""2012-06-01\"", freq=freq)"", ""tslibs.period.PeriodUnaryMethods.time_asfreq"": ""class PeriodUnaryMethods:\n    def time_asfreq(self, freq):\n        self.per.asfreq(\""A\"")\n\n    def setup(self, freq):\n        self.per = Period(\""2012-06-01\"", freq=freq)"", ""tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr"": ""class TimeDT64ArrToPeriodArr:\n    def time_dt64arr_to_periodarr(self, size, freq, tz):\n        dt64arr_to_periodarr(self.i8values, freq, tz)\n\n    def setup(self, size, freq, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.arange(10, dtype=\""i8\"").repeat(size // 10)\n        self.i8values = arr"", ""tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr"": ""class TimePeriodArrToDT64Arr:\n    def time_periodarray_to_dt64arr(self, size, freq):\n        periodarr_to_dt64arr(self.i8values, freq)\n\n    def setup(self, size, freq):\n        arr = np.arange(10, dtype=\""i8\"").repeat(size // 10)\n        self.i8values = arr""}",tslibs.period,time,-0.009080135122215699
29996,module-level,"terminus-2,claude",0.9982053398268514,1.0156490510826353,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timedelta,tslibs.timedelta,"{""tslibs.timedelta.TimedeltaConstructor.time_from_datetime_timedelta"": ""class TimedeltaConstructor:\n    def time_from_datetime_timedelta(self):\n        Timedelta(self.dttimedelta)\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")"", ""tslibs.timedelta.TimedeltaConstructor.time_from_unit"": ""class TimedeltaConstructor:\n    def time_from_unit(self):\n        Timedelta(1, unit=\""d\"")\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")"", ""tslibs.timedelta.TimedeltaProperties.time_timedelta_days"": ""class TimedeltaProperties:\n    def time_timedelta_days(self, td):\n        td.days\n\n    def setup_cache(self):\n        td = Timedelta(days=365, minutes=35, seconds=25, milliseconds=35)\n        return td"", ""tslibs.timedelta.TimedeltaProperties.time_timedelta_nanoseconds"": ""class TimedeltaProperties:\n    def time_timedelta_nanoseconds(self, td):\n        td.nanoseconds\n\n    def setup_cache(self):\n        td = Timedelta(days=365, minutes=35, seconds=25, milliseconds=35)\n        return td""}",tslibs.timedelta,time,-0.012336429459535935
29999,module-level,"terminus-2,claude",0.9803890644371034,0.992241436005673,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.tz_convert,tslibs.tz_convert,"{""tslibs.tz_convert.TimeTZConvert.time_tz_convert_from_utc"": ""class TimeTZConvert:\n    def time_tz_convert_from_utc(self, size, tz):\n        # effectively:\n        #  dti = DatetimeIndex(self.i8data, tz=tz)\n        #  dti.tz_localize(None)\n        if old_sig:\n            tz_convert_from_utc(self.i8data, UTC, tz)\n        else:\n            tz_convert_from_utc(self.i8data, tz)\n\n    def setup(self, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr"", ""tslibs.tz_convert.TimeTZConvert.time_tz_localize_to_utc"": ""class TimeTZConvert:\n    def time_tz_localize_to_utc(self, size, tz):\n        # effectively:\n        #  dti = DatetimeIndex(self.i8data)\n        #  dti.tz_localize(tz, ambiguous=\""NaT\"", nonexistent=\""NaT\"")\n        tz_localize_to_utc(self.i8data, tz, ambiguous=\""NaT\"", nonexistent=\""NaT\"")\n\n    def setup(self, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.tz_convert,time,-0.008382158110728193
29978,module-level,"terminus-2,claude",0.9759464486562456,1.085295780677391,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.csv,io.csv,"{""io.csv.ReadCSVCategorical.time_convert_post"": ""class ReadCSVCategorical:\n    def time_convert_post(self, engine):\n        read_csv(self.fname, engine=engine).apply(Categorical)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVCategorical:\n    def setup(self, engine):\n        N = 100000\n        group1 = [\""aaaaaaaa\"", \""bbbbbbb\"", \""cccccccc\"", \""dddddddd\"", \""eeeeeeee\""]\n        df = DataFrame(np.random.choice(group1, (N, 3)), columns=list(\""abc\""))\n        df.to_csv(self.fname, index=False)"", ""io.csv.ReadCSVEngine.time_read_bytescsv"": ""class ReadCSVEngine:\n    def time_read_bytescsv(self, engine):\n        read_csv(self.data(self.BytesIO_input), engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVEngine:\n    def setup(self, engine):\n        data = [\""A,B,C,D,E\""] + ([\""1,2,3,4,5\""] * 100000)\n        self.StringIO_input = StringIO(\""\\n\"".join(data))\n        # simulate reading from file\n        self.BytesIO_input = BytesIO(self.StringIO_input.read().encode(\""utf-8\""))"", ""io.csv.ReadCSVFloatPrecision.time_read_csv"": ""class ReadCSVFloatPrecision:\n    def time_read_csv(self, sep, decimal, float_precision):\n        read_csv(\n            self.data(self.StringIO_input),\n            sep=sep,\n            header=None,\n            names=list(\""abc\""),\n            float_precision=float_precision,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVFloatPrecision:\n    def setup(self, sep, decimal, float_precision):\n        floats = [\n            \""\"".join([random.choice(string.digits) for _ in range(28)])\n            for _ in range(15)\n        ]\n        rows = sep.join([f\""0{decimal}\"" + \""{}\""] * 3) + \""\\n\""\n        data = rows * 5\n        data = data.format(*floats) * 200  # 1000 x 3 strings csv\n        self.StringIO_input = StringIO(data)"", ""io.csv.ReadCSVFloatPrecision.time_read_csv_python_engine"": ""class ReadCSVFloatPrecision:\n    def time_read_csv_python_engine(self, sep, decimal, float_precision):\n        read_csv(\n            self.data(self.StringIO_input),\n            sep=sep,\n            header=None,\n            engine=\""python\"",\n            float_precision=None,\n            names=list(\""abc\""),\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVFloatPrecision:\n    def setup(self, sep, decimal, float_precision):\n        floats = [\n            \""\"".join([random.choice(string.digits) for _ in range(28)])\n            for _ in range(15)\n        ]\n        rows = sep.join([f\""0{decimal}\"" + \""{}\""] * 3) + \""\\n\""\n        data = rows * 5\n        data = data.format(*floats) * 200  # 1000 x 3 strings csv\n        self.StringIO_input = StringIO(data)"", ""io.csv.ReadCSVParseSpecialDate.time_read_special_date"": ""class ReadCSVParseSpecialDate:\n    def time_read_special_date(self, value, engine):\n        read_csv(\n            self.data(self.StringIO_input),\n            engine=engine,\n            sep=\"",\"",\n            header=None,\n            names=[\""Date\""],\n            parse_dates=[\""Date\""],\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVParseSpecialDate:\n    def setup(self, value, engine):\n        count_elem = 10000\n        data = self.objects[value] * count_elem\n        self.StringIO_input = StringIO(data)"", ""io.csv.ToCSVIndexes.time_standard_index"": ""class ToCSVIndexes:\n    def time_standard_index(self):\n        self.df_standard_index.to_csv(self.fname)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToCSVIndexes:\n    def setup(self):\n        ROWS = 100000\n        COLS = 5\n        # For tests using .head(), create an initial dataframe with this many times\n        # more rows\n        HEAD_ROW_MULTIPLIER = 10\n    \n        self.df_standard_index = self._create_df(ROWS, COLS)\n    \n        self.df_custom_index_then_head = (\n            self._create_df(ROWS * HEAD_ROW_MULTIPLIER, COLS)\n            .set_index([\""index1\"", \""index2\"", \""index3\""])\n            .head(ROWS)\n        )\n    \n        self.df_head_then_custom_index = (\n            self._create_df(ROWS * HEAD_ROW_MULTIPLIER, COLS)\n            .head(ROWS)\n            .set_index([\""index1\"", \""index2\"", \""index3\""])\n        )"", ""io.csv.ToCSVMultiIndexUnusedLevels.time_single_index_frame"": ""class ToCSVMultiIndexUnusedLevels:\n    def time_single_index_frame(self):\n        self.df_single_index.to_csv(self.fname)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToCSVMultiIndexUnusedLevels:\n    def setup(self):\n        df = DataFrame({\""a\"": np.random.randn(100_000), \""b\"": 1, \""c\"": 1})\n        self.df = df.set_index([\""a\"", \""b\""])\n        self.df_unused_levels = self.df.iloc[:10_000]\n        self.df_single_index = df.set_index([\""a\""]).iloc[:10_000]"", ""io.csv.ToCSVMultiIndexUnusedLevels.time_sliced_frame"": ""class ToCSVMultiIndexUnusedLevels:\n    def time_sliced_frame(self):\n        self.df_unused_levels.to_csv(self.fname)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToCSVMultiIndexUnusedLevels:\n    def setup(self):\n        df = DataFrame({\""a\"": np.random.randn(100_000), \""b\"": 1, \""c\"": 1})\n        self.df = df.set_index([\""a\"", \""b\""])\n        self.df_unused_levels = self.df.iloc[:10_000]\n        self.df_single_index = df.set_index([\""a\""]).iloc[:10_000]""}",io.csv,time,-0.07733333240533628
29988,module-level,"terminus-2,claude",0.9810111873552728,1.0204791059132996,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strftime,strftime,"{""strftime.BusinessHourStrftime.time_frame_offset_repr"": ""class BusinessHourStrftime:\n    def time_frame_offset_repr(self, obs):\n        self.data[\""off\""].apply(repr)\n\n    def setup(self, obs):\n        self.data = pd.DataFrame(\n            {\n                \""off\"": [offsets.BusinessHour()] * obs,\n            }\n        )""}",strftime,time,-0.027912247919396595
29992,module-level,"terminus-2,claude",0.9710297826026658,0.964714718516657,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.normalize,tslibs.normalize,"{""tslibs.normalize.Normalize.time_is_date_array_normalized"": ""class Normalize:\n    def time_is_date_array_normalized(self, size, tz):\n        # TODO: cases with different levels of short-circuiting\n        # 10 i.e. NPY_FR_ns\n        is_date_array_normalized(self.i8data, tz, 10)\n\n    def setup(self, size, tz):\n        # use an array that will have is_date_array_normalized give True,\n        #  so we do not short-circuit early.\n        dti = pd.date_range(\""2016-01-01\"", periods=10, tz=tz).repeat(size // 10)\n        self.i8data = dti.asi8\n    \n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError"", ""tslibs.normalize.Normalize.time_normalize_i8_timestamps"": ""class Normalize:\n    def time_normalize_i8_timestamps(self, size, tz):\n        # 10 i.e. NPY_FR_ns\n        normalize_i8_timestamps(self.i8data, tz, 10)\n\n    def setup(self, size, tz):\n        # use an array that will have is_date_array_normalized give True,\n        #  so we do not short-circuit early.\n        dti = pd.date_range(\""2016-01-01\"", periods=10, tz=tz).repeat(size // 10)\n        self.i8data = dti.asi8\n    \n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError""}",tslibs.normalize,time,0.004466099070727576
29990,module-level,"terminus-2,claude",0.9687024370942496,1.000983507899129,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries,timeseries,"{""timeseries.AsOf.time_asof_single"": ""class AsOf:\n    def time_asof_single(self, constructor):\n        self.ts.asof(self.date)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass AsOf:\n    def setup(self, constructor):\n        N = 10000\n        M = 10\n        rng = date_range(start=\""1/1/1990\"", periods=N, freq=\""53s\"")\n        data = {\n            \""DataFrame\"": DataFrame(np.random.randn(N, M)),\n            \""Series\"": Series(np.random.randn(N)),\n        }\n        self.ts = data[constructor]\n        self.ts.index = rng\n        self.ts2 = self.ts.copy()\n        self.ts2.iloc[250:5000] = np.nan\n        self.ts3 = self.ts.copy()\n        self.ts3.iloc[-5000:] = np.nan\n        self.dates = date_range(start=\""1/1/1990\"", periods=N * 10, freq=\""5s\"")\n        self.date = self.dates[0]\n        self.date_last = self.dates[-1]\n        self.date_early = self.date - timedelta(10)"", ""timeseries.DatetimeAccessor.time_dt_accessor"": ""class DatetimeAccessor:\n    def time_dt_accessor(self, tz):\n        self.series.dt\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeAccessor:\n    def setup(self, tz):\n        N = 100000\n        self.series = Series(date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"", tz=tz))"", ""timeseries.DatetimeAccessor.time_dt_accessor_month_name"": ""class DatetimeAccessor:\n    def time_dt_accessor_month_name(self, tz):\n        self.series.dt.month_name()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeAccessor:\n    def setup(self, tz):\n        N = 100000\n        self.series = Series(date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"", tz=tz))"", ""timeseries.DatetimeAccessor.time_dt_accessor_year"": ""class DatetimeAccessor:\n    def time_dt_accessor_year(self, tz):\n        self.series.dt.year\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeAccessor:\n    def setup(self, tz):\n        N = 100000\n        self.series = Series(date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"", tz=tz))"", ""timeseries.DatetimeIndex.time_get"": ""class DatetimeIndex:\n    def time_get(self, index_type):\n        self.index[0]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]"", ""timeseries.DatetimeIndex.time_normalize"": ""class DatetimeIndex:\n    def time_normalize(self, index_type):\n        self.index.normalize()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]"", ""timeseries.DatetimeIndex.time_timeseries_is_month_start"": ""class DatetimeIndex:\n    def time_timeseries_is_month_start(self, index_type):\n        self.index.is_month_start\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]"", ""timeseries.DatetimeIndex.time_to_date"": ""class DatetimeIndex:\n    def time_to_date(self, index_type):\n        self.index.date\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]"", ""timeseries.DatetimeIndex.time_unique"": ""class DatetimeIndex:\n    def time_unique(self, index_type):\n        self.index.unique()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]"", ""timeseries.InferFreq.time_infer_freq"": ""class InferFreq:\n    def time_infer_freq(self, freq):\n        infer_freq(self.idx)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass InferFreq:\n    def setup(self, freq):\n        if freq is None:\n            self.idx = date_range(start=\""1/1/1700\"", freq=\""D\"", periods=10000)\n            self.idx._data._freq = None\n        else:\n            self.idx = date_range(start=\""1/1/1700\"", freq=freq, periods=10000)"", ""timeseries.Iteration.time_iter"": ""class Iteration:\n    def time_iter(self, time_index):\n        for _ in self.idx:\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self, time_index):\n        N = 10**6\n        if time_index is timedelta_range:\n            self.idx = time_index(start=0, freq=\""T\"", periods=N)\n        else:\n            self.idx = time_index(start=\""20140101\"", freq=\""T\"", periods=N)\n        self.exit = 10000"", ""timeseries.Iteration.time_iter_preexit"": ""class Iteration:\n    def time_iter_preexit(self, time_index):\n        for i, _ in enumerate(self.idx):\n            if i > self.exit:\n                break\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self, time_index):\n        N = 10**6\n        if time_index is timedelta_range:\n            self.idx = time_index(start=0, freq=\""T\"", periods=N)\n        else:\n            self.idx = time_index(start=\""20140101\"", freq=\""T\"", periods=N)\n        self.exit = 10000"", ""timeseries.Lookup.time_lookup_and_cleanup"": ""class Lookup:\n    def time_lookup_and_cleanup(self):\n        self.ts[self.lookup_val]\n        self.ts.index._cleanup()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Lookup:\n    def setup(self):\n        N = 1500000\n        rng = date_range(start=\""1/1/2000\"", periods=N, freq=\""S\"")\n        self.ts = Series(1, index=rng)\n        self.lookup_val = rng[N // 2]"", ""timeseries.ResampleDataFrame.time_method"": ""class ResampleDataFrame:\n    def time_method(self, method):\n        self.resample()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ResampleDataFrame:\n    def setup(self, method):\n        rng = date_range(start=\""20130101\"", periods=100000, freq=\""50L\"")\n        df = DataFrame(np.random.randn(100000, 2), index=rng)\n        self.resample = getattr(df.resample(\""1s\""), method)"", ""timeseries.ResampleDatetetime64.time_resample"": ""class ResampleDatetetime64:\n    def time_resample(self):\n        self.dt_ts.resample(\""1S\"").last()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ResampleDatetetime64:\n    def setup(self):\n        rng3 = date_range(\n            start=\""2000-01-01 00:00:00\"", end=\""2000-01-01 10:00:00\"", freq=\""555000U\""\n        )\n        self.dt_ts = Series(5, rng3, dtype=\""datetime64[ns]\"")"", ""timeseries.ResampleSeries.time_resample"": ""class ResampleSeries:\n    def time_resample(self, index, freq, method):\n        self.resample()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ResampleSeries:\n    def setup(self, index, freq, method):\n        indexes = {\n            \""period\"": period_range(start=\""1/1/2000\"", end=\""1/1/2001\"", freq=\""T\""),\n            \""datetime\"": date_range(start=\""1/1/2000\"", end=\""1/1/2001\"", freq=\""T\""),\n        }\n        idx = indexes[index]\n        ts = Series(np.random.randn(len(idx)), index=idx)\n        self.resample = getattr(ts.resample(freq), method)"", ""timeseries.SortIndex.time_get_slice"": ""class SortIndex:\n    def time_get_slice(self, monotonic):\n        self.s[:10000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SortIndex:\n    def setup(self, monotonic):\n        N = 10**5\n        idx = date_range(start=\""1/1/2000\"", periods=N, freq=\""s\"")\n        self.s = Series(np.random.randn(N), index=idx)\n        if not monotonic:\n            self.s = self.s.sample(frac=1)"", ""timeseries.TzLocalize.time_infer_dst"": ""class TzLocalize:\n    def time_infer_dst(self, tz):\n        self.index.tz_localize(tz, ambiguous=\""infer\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TzLocalize:\n    def setup(self, tz):\n        dst_rng = date_range(\n            start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n        )\n        self.index = date_range(start=\""10/29/2000\"", end=\""10/29/2000 00:59:59\"", freq=\""S\"")\n        self.index = self.index.append(dst_rng)\n        self.index = self.index.append(dst_rng)\n        self.index = self.index.append(\n            date_range(start=\""10/29/2000 2:00:00\"", end=\""10/29/2000 3:00:00\"", freq=\""S\"")\n        )""}",timeseries,time,-0.022829611601753393
29997,module-level,"terminus-2,claude",0.9916370869645746,1.0116103611834897,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp,tslibs.timestamp,"{""tslibs.timestamp.TimestampConstruction.time_from_npdatetime64"": ""class TimestampConstruction:\n    def time_from_npdatetime64(self):\n        Timestamp(self.npdatetime64)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampConstruction.time_parse_iso8601_no_tz"": ""class TimestampConstruction:\n    def time_parse_iso8601_no_tz(self):\n        Timestamp(\""2017-08-25 08:16:14\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampConstruction.time_parse_now"": ""class TimestampConstruction:\n    def time_parse_now(self):\n        Timestamp(\""now\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampOps.time_ceil"": ""class TimestampOps:\n    def time_ceil(self, tz):\n        self.ts.ceil(\""5T\"")\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_floor"": ""class TimestampOps:\n    def time_floor(self, tz):\n        self.ts.floor(\""5T\"")\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_normalize"": ""class TimestampOps:\n    def time_normalize(self, tz):\n        self.ts.normalize()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_replace_tz"": ""class TimestampOps:\n    def time_replace_tz(self, tz):\n        self.ts.replace(tzinfo=pytz.timezone(\""US/Eastern\""))\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_to_julian_date"": ""class TimestampOps:\n    def time_to_julian_date(self, tz):\n        self.ts.to_julian_date()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_tz_convert"": ""class TimestampOps:\n    def time_tz_convert(self, tz):\n        if self.ts.tz is not None:\n            self.ts.tz_convert(tz)\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_tz_localize"": ""class TimestampOps:\n    def time_tz_localize(self, tz):\n        if self.ts.tz is None:\n            self.ts.tz_localize(tz)\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampProperties.time_dayofweek"": ""class TimestampProperties:\n    def time_dayofweek(self, tz):\n        self.ts.dayofweek\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_dayofyear"": ""class TimestampProperties:\n    def time_dayofyear(self, tz):\n        self.ts.dayofyear\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_days_in_month"": ""class TimestampProperties:\n    def time_days_in_month(self, tz):\n        self.ts.days_in_month\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_month_end"": ""class TimestampProperties:\n    def time_is_month_end(self, tz):\n        self.ts.is_month_end\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_month_start"": ""class TimestampProperties:\n    def time_is_month_start(self, tz):\n        self.ts.is_month_start\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_quarter_end"": ""class TimestampProperties:\n    def time_is_quarter_end(self, tz):\n        self.ts.is_quarter_end\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_quarter_start"": ""class TimestampProperties:\n    def time_is_quarter_start(self, tz):\n        self.ts.is_quarter_start\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_year_start"": ""class TimestampProperties:\n    def time_is_year_start(self, tz):\n        self.ts.is_year_start\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_microsecond"": ""class TimestampProperties:\n    def time_microsecond(self, tz):\n        self.ts.microsecond\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_month_name"": ""class TimestampProperties:\n    def time_month_name(self, tz):\n        self.ts.month_name()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_tz"": ""class TimestampProperties:\n    def time_tz(self, tz):\n        self.ts.tz\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_week"": ""class TimestampProperties:\n    def time_week(self, tz):\n        self.ts.week\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_weekday_name"": ""class TimestampProperties:\n    def time_weekday_name(self, tz):\n        self.ts.day_name()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp,time,-0.014125370734734823
29998,module-level,"terminus-2,claude",0.98887136318111,1.018084972226594,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.tslib,tslibs.tslib,"{""tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime"": ""class TimeIntsToPydatetime:\n    def time_ints_to_pydatetime(self, box, size, tz):\n        ints_to_pydatetime(self.i8data, tz, box=box)\n\n    def setup(self, box, size, tz):\n        if box == \""date\"" and tz is not None:\n            # tz is ignored, so avoid running redundant benchmarks\n            raise NotImplementedError  # skip benchmark\n        if size == 10**6 and tz is _tzs[-1]:\n            # This is cumbersomely-slow, so skip to trim runtime\n            raise NotImplementedError  # skip benchmark\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.tslib,time,-0.020660260993977435
30006,module-level,"terminus-2,gpt-5",1.083036447175486,0.901924432099614,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,ctors,ctors,"{""ctors.DatetimeIndexConstructor.time_from_list_of_timestamps"": ""class DatetimeIndexConstructor:\n    def time_from_list_of_timestamps(self):\n        DatetimeIndex(self.list_of_timestamps)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndexConstructor:\n    def setup(self):\n        N = 20_000\n        dti = date_range(\""1900-01-01\"", periods=N)\n    \n        self.list_of_timestamps = dti.tolist()\n        self.list_of_dates = dti.date.tolist()\n        self.list_of_datetimes = dti.to_pydatetime().tolist()\n        self.list_of_str = dti.strftime(\""%Y-%m-%d\"").tolist()"", ""ctors.MultiIndexConstructor.time_multiindex_from_iterables"": ""class MultiIndexConstructor:\n    def time_multiindex_from_iterables(self):\n        MultiIndex.from_product(self.iterables)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MultiIndexConstructor:\n    def setup(self):\n        N = 10**4\n        self.iterables = [tm.makeStringIndex(N), range(20)]"", ""ctors.SeriesConstructors.time_series_constructor"": ""class SeriesConstructors:\n    def time_series_constructor(self, data_fmt, with_index, dtype):\n        Series(self.data, index=self.index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesConstructors:\n    def setup(self, data_fmt, with_index, dtype):\n        if data_fmt in (gen_of_str, gen_of_tuples) and with_index:\n            raise NotImplementedError(\n                \""Series constructors do not support using generators with indexes\""\n            )\n        N = 10**4\n        if dtype == \""float\"":\n            arr = np.random.randn(N)\n        else:\n            arr = np.arange(N)\n        self.data = data_fmt(arr)\n        self.index = np.arange(N) if with_index else None"", ""ctors.SeriesDtypesConstructors.time_dtindex_from_series"": ""class SeriesDtypesConstructors:\n    def time_dtindex_from_series(self):\n        DatetimeIndex(self.s)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesDtypesConstructors:\n    def setup(self):\n        N = 10**4\n        self.arr = np.random.randn(N)\n        self.arr_str = np.array([\""foo\"", \""bar\"", \""baz\""], dtype=object)\n        self.s = Series(\n            [Timestamp(\""20110101\""), Timestamp(\""20120101\""), Timestamp(\""20130101\"")]\n            * N\n            * 10\n        )"", ""ctors.SeriesDtypesConstructors.time_index_from_array_floats"": ""class SeriesDtypesConstructors:\n    def time_index_from_array_floats(self):\n        Index(self.arr)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesDtypesConstructors:\n    def setup(self):\n        N = 10**4\n        self.arr = np.random.randn(N)\n        self.arr_str = np.array([\""foo\"", \""bar\"", \""baz\""], dtype=object)\n        self.s = Series(\n            [Timestamp(\""20110101\""), Timestamp(\""20120101\""), Timestamp(\""20130101\"")]\n            * N\n            * 10\n        )""}",ctors,time,0.1280848762912815
30001,module-level,"terminus-2,gpt-5",1.1033744092484534,1.0855018133793013,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin,algos.isin,"{""algos.isin.IsIn.time_isin"": ""class IsIn:\n    def time_isin(self, dtype):\n        self.series.isin(self.values)\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)"", ""algos.isin.IsIn.time_isin_categorical"": ""class IsIn:\n    def time_isin_categorical(self, dtype):\n        self.series.isin(self.cat_values)\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)"", ""algos.isin.IsIn.time_isin_empty"": ""class IsIn:\n    def time_isin_empty(self, dtype):\n        self.series.isin([])\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)"", ""algos.isin.IsIn.time_isin_mismatched_dtype"": ""class IsIn:\n    def time_isin_mismatched_dtype(self, dtype):\n        self.series.isin(self.mismatched)\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)"", ""algos.isin.IsInForObjects.time_isin"": ""class IsInForObjects:\n    def time_isin(self, series_type, vals_type):\n        self.series.isin(self.values)\n\n    def setup(self, series_type, vals_type):\n        N_many = 10**5\n    \n        if series_type == \""nans\"":\n            ser_vals = np.full(10**4, np.nan)\n        elif series_type == \""short\"":\n            ser_vals = np.arange(2)\n        elif series_type == \""long\"":\n            ser_vals = np.arange(N_many)\n        elif series_type == \""long_floats\"":\n            ser_vals = np.arange(N_many, dtype=np.float_)\n    \n        self.series = Series(ser_vals).astype(object)\n    \n        if vals_type == \""nans\"":\n            values = np.full(10**4, np.nan)\n        elif vals_type == \""short\"":\n            values = np.arange(2)\n        elif vals_type == \""long\"":\n            values = np.arange(N_many)\n        elif vals_type == \""long_floats\"":\n            values = np.arange(N_many, dtype=np.float_)\n    \n        self.values = values.astype(object)"", ""algos.isin.IsInLongSeriesLookUpDominates.time_isin"": ""class IsInLongSeriesLookUpDominates:\n    def time_isin(self, dtypes, MaxNumber, series_type):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, MaxNumber, series_type):\n        N = 10**7\n    \n        if series_type == \""random_hits\"":\n            array = np.random.randint(0, MaxNumber, N)\n        if series_type == \""random_misses\"":\n            array = np.random.randint(0, MaxNumber, N) + MaxNumber\n        if series_type == \""monotone_hits\"":\n            array = np.repeat(np.arange(MaxNumber), N // MaxNumber)\n        if series_type == \""monotone_misses\"":\n            array = np.arange(N) + MaxNumber\n    \n        self.series = Series(array).astype(dtype)\n    \n        self.values = np.arange(MaxNumber).astype(dtype.lower())"", ""algos.isin.IsInLongSeriesValuesDominate.time_isin"": ""class IsInLongSeriesValuesDominate:\n    def time_isin(self, dtypes, series_type):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, series_type):\n        N = 10**7\n    \n        if series_type == \""random\"":\n            vals = np.random.randint(0, 10 * N, N)\n        if series_type == \""monotone\"":\n            vals = np.arange(N)\n    \n        self.values = vals.astype(dtype.lower())\n        M = 10**6 + 1\n        self.series = Series(np.arange(M)).astype(dtype)"", ""algos.isin.IsinAlmostFullWithRandomInt.time_isin"": ""class IsinAlmostFullWithRandomInt:\n    def time_isin(self, dtype, exponent, title):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, exponent, title):\n        M = 3 * 2 ** (exponent - 2)\n        # 0.77-the maximal share of occupied buckets\n        self.series = Series(np.random.randint(0, M, M)).astype(dtype)\n    \n        values = np.random.randint(0, M, M).astype(dtype)\n        if title == \""inside\"":\n            self.values = values\n        elif title == \""outside\"":\n            self.values = values + M\n        else:\n            raise ValueError(title)"", ""algos.isin.IsinWithArange.time_isin"": ""class IsinWithArange:\n    def time_isin(self, dtype, M, offset_factor):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, M, offset_factor):\n        offset = int(M * offset_factor)\n        tmp = Series(np.random.randint(offset, M + offset, 10**6))\n        self.series = tmp.astype(dtype)\n        self.values = np.arange(M).astype(dtype)""}",algos.isin,time,0.012639742481720036
30003,module-level,"terminus-2,gpt-5",0.9919965312450744,0.959116452735395,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,array,array,"{""array.ArrowExtensionArray.time_to_numpy"": ""class ArrowExtensionArray:\n    def time_to_numpy(self, dtype, hasna):\n        self.arr.to_numpy()\n\n    def setup(self, dtype, hasna):\n        N = 100_000\n        if dtype == \""boolean[pyarrow]\"":\n            data = np.random.choice([True, False], N, replace=True)\n        elif dtype == \""float64[pyarrow]\"":\n            data = np.random.randn(N)\n        elif dtype == \""int64[pyarrow]\"":\n            data = np.arange(N)\n        elif dtype == \""string[pyarrow]\"":\n            data = tm.rands_array(10, N)\n        elif dtype == \""timestamp[ns][pyarrow]\"":\n            data = pd.date_range(\""2000-01-01\"", freq=\""s\"", periods=N)\n        else:\n            raise NotImplementedError\n    \n        arr = pd.array(data, dtype=dtype)\n        if hasna:\n            arr[::2] = pd.NA\n        self.arr = arr"", ""array.ArrowStringArray.time_setitem_slice"": ""class ArrowStringArray:\n    def time_setitem_slice(self, multiple_chunks):\n        self.array[::10] = \""foo\""\n\n    def setup(self, multiple_chunks):\n        try:\n            import pyarrow as pa\n        except ImportError:\n            raise NotImplementedError\n        strings = tm.rands_array(3, 10_000)\n        if multiple_chunks:\n            chunks = [strings[i : i + 100] for i in range(0, len(strings), 100)]\n            self.array = pd.arrays.ArrowStringArray(pa.chunked_array(chunks))\n        else:\n            self.array = pd.arrays.ArrowStringArray(pa.array(strings))"", ""array.ArrowStringArray.time_tolist"": ""class ArrowStringArray:\n    def time_tolist(self, multiple_chunks):\n        self.array.tolist()\n\n    def setup(self, multiple_chunks):\n        try:\n            import pyarrow as pa\n        except ImportError:\n            raise NotImplementedError\n        strings = tm.rands_array(3, 10_000)\n        if multiple_chunks:\n            chunks = [strings[i : i + 100] for i in range(0, len(strings), 100)]\n            self.array = pd.arrays.ArrowStringArray(pa.chunked_array(chunks))\n        else:\n            self.array = pd.arrays.ArrowStringArray(pa.array(strings))"", ""array.BooleanArray.time_from_bool_array"": ""class BooleanArray:\n    def time_from_bool_array(self):\n        pd.array(self.values_bool, dtype=\""boolean\"")\n\n    def setup(self):\n        self.values_bool = np.array([True, False, True, False])\n        self.values_float = np.array([1.0, 0.0, 1.0, 0.0])\n        self.values_integer = np.array([1, 0, 1, 0])\n        self.values_integer_like = [1, 0, 1, 0]\n        self.data = np.array([True, False, True, False])\n        self.mask = np.array([False, False, True, False])"", ""array.BooleanArray.time_from_float_array"": ""class BooleanArray:\n    def time_from_float_array(self):\n        pd.array(self.values_float, dtype=\""boolean\"")\n\n    def setup(self):\n        self.values_bool = np.array([True, False, True, False])\n        self.values_float = np.array([1.0, 0.0, 1.0, 0.0])\n        self.values_integer = np.array([1, 0, 1, 0])\n        self.values_integer_like = [1, 0, 1, 0]\n        self.data = np.array([True, False, True, False])\n        self.mask = np.array([False, False, True, False])""}",array,time,0.023253237984214522
30002,module-level,"terminus-2,gpt-5",1.108180864343897,0.920093805794198,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic,arithmetic,"{""arithmetic.BinaryOpsMultiIndex.time_binary_op_multiindex"": ""class BinaryOpsMultiIndex:\n    def time_binary_op_multiindex(self, func):\n        getattr(self.df, func)(self.arg_df, level=0)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass BinaryOpsMultiIndex:\n    def setup(self, func):\n        array = date_range(\""20200101 00:00\"", \""20200102 0:00\"", freq=\""S\"")\n        level_0_names = [str(i) for i in range(30)]\n    \n        index = pd.MultiIndex.from_product([level_0_names, array])\n        column_names = [\""col_1\"", \""col_2\""]\n    \n        self.df = DataFrame(\n            np.random.rand(len(index), 2), index=index, columns=column_names\n        )\n    \n        self.arg_df = DataFrame(\n            np.random.randint(1, 10, (len(level_0_names), 2)),\n            index=level_0_names,\n            columns=column_names,\n        )"", ""arithmetic.IndexArithmetic.time_multiply"": ""class IndexArithmetic:\n    def time_multiply(self, dtype):\n        self.index * 2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IndexArithmetic:\n    def setup(self, dtype):\n        N = 10**6\n        indexes = {\""int\"": \""makeIntIndex\"", \""float\"": \""makeFloatIndex\""}\n        self.index = getattr(tm, indexes[dtype])(N)"", ""arithmetic.IndexArithmetic.time_subtract"": ""class IndexArithmetic:\n    def time_subtract(self, dtype):\n        self.index - 2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IndexArithmetic:\n    def setup(self, dtype):\n        N = 10**6\n        indexes = {\""int\"": \""makeIntIndex\"", \""float\"": \""makeFloatIndex\""}\n        self.index = getattr(tm, indexes[dtype])(N)"", ""arithmetic.IntFrameWithScalar.time_frame_op_with_scalar"": ""class IntFrameWithScalar:\n    def time_frame_op_with_scalar(self, dtype, scalar, op):\n        op(self.df, scalar)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IntFrameWithScalar:\n    def setup(self, dtype, scalar, op):\n        arr = np.random.randn(20000, 100)\n        self.df = DataFrame(arr.astype(dtype))"", ""arithmetic.IrregularOps.time_add"": ""class IrregularOps:\n    def time_add(self):\n        self.left + self.right\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IrregularOps:\n    def setup(self):\n        N = 10**5\n        idx = date_range(start=\""1/1/2000\"", periods=N, freq=\""s\"")\n        s = Series(np.random.randn(N), index=idx)\n        self.left = s.sample(frac=1)\n        self.right = s.sample(frac=1)"", ""arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0"": ""class MixedFrameWithSeriesAxis:\n    def time_frame_op_with_series_axis0(self, opname):\n        getattr(self.df, opname)(self.ser, axis=0)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MixedFrameWithSeriesAxis:\n    def setup(self, opname):\n        arr = np.arange(10**6).reshape(1000, -1)\n        df = DataFrame(arr)\n        df[\""C\""] = 1.0\n        self.df = df\n        self.ser = df[0]\n        self.row = df.iloc[0]"", ""arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis1"": ""class MixedFrameWithSeriesAxis:\n    def time_frame_op_with_series_axis1(self, opname):\n        getattr(operator, opname)(self.df, self.ser)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MixedFrameWithSeriesAxis:\n    def setup(self, opname):\n        arr = np.arange(10**6).reshape(1000, -1)\n        df = DataFrame(arr)\n        df[\""C\""] = 1.0\n        self.df = df\n        self.ser = df[0]\n        self.row = df.iloc[0]"", ""arithmetic.NumericInferOps.time_add"": ""class NumericInferOps:\n    def time_add(self, dtype):\n        self.df[\""A\""] + self.df[\""B\""]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericInferOps:\n    def setup(self, dtype):\n        N = 5 * 10**5\n        self.df = DataFrame(\n            {\""A\"": np.arange(N).astype(dtype), \""B\"": np.arange(N).astype(dtype)}\n        )"", ""arithmetic.NumericInferOps.time_divide"": ""class NumericInferOps:\n    def time_divide(self, dtype):\n        self.df[\""A\""] / self.df[\""B\""]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericInferOps:\n    def setup(self, dtype):\n        N = 5 * 10**5\n        self.df = DataFrame(\n            {\""A\"": np.arange(N).astype(dtype), \""B\"": np.arange(N).astype(dtype)}\n        )"", ""arithmetic.NumericInferOps.time_modulo"": ""class NumericInferOps:\n    def time_modulo(self, dtype):\n        self.df[\""A\""] % self.df[\""B\""]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericInferOps:\n    def setup(self, dtype):\n        N = 5 * 10**5\n        self.df = DataFrame(\n            {\""A\"": np.arange(N).astype(dtype), \""B\"": np.arange(N).astype(dtype)}\n        )"", ""arithmetic.NumericInferOps.time_multiply"": ""class NumericInferOps:\n    def time_multiply(self, dtype):\n        self.df[\""A\""] * self.df[\""B\""]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericInferOps:\n    def setup(self, dtype):\n        N = 5 * 10**5\n        self.df = DataFrame(\n            {\""A\"": np.arange(N).astype(dtype), \""B\"": np.arange(N).astype(dtype)}\n        )"", ""arithmetic.NumericInferOps.time_subtract"": ""class NumericInferOps:\n    def time_subtract(self, dtype):\n        self.df[\""A\""] - self.df[\""B\""]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericInferOps:\n    def setup(self, dtype):\n        N = 5 * 10**5\n        self.df = DataFrame(\n            {\""A\"": np.arange(N).astype(dtype), \""B\"": np.arange(N).astype(dtype)}\n        )"", ""arithmetic.OffsetArrayArithmetic.time_add_dti_offset"": ""class OffsetArrayArithmetic:\n    def time_add_dti_offset(self, offset):\n        with warnings.catch_warnings(record=True):\n            self.rng + offset\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass OffsetArrayArithmetic:\n    def setup(self, offset):\n        N = 10000\n        rng = date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"")\n        self.rng = rng\n        self.ser = Series(rng)""}",arithmetic,time,0.13301772174660473
30008,module-level,"terminus-2,gpt-5",1.2216021690332857,1.1970532082915912,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval,eval,"{""eval.Eval.time_add"": ""class Eval:\n    def time_add(self, engine, threads):\n        pd.eval(\""self.df + self.df2 + self.df3 + self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)"", ""eval.Eval.time_and"": ""class Eval:\n    def time_and(self, engine, threads):\n        pd.eval(\n            \""(self.df > 0) & (self.df2 > 0) & (self.df3 > 0) & (self.df4 > 0)\"",\n            engine=engine,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)"", ""eval.Eval.time_chained_cmp"": ""class Eval:\n    def time_chained_cmp(self, engine, threads):\n        pd.eval(\""self.df < self.df2 < self.df3 < self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)"", ""eval.Eval.time_mult"": ""class Eval:\n    def time_mult(self, engine, threads):\n        pd.eval(\""self.df * self.df2 * self.df3 * self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)"", ""eval.Query.time_query_datetime_column"": ""class Query:\n    def time_query_datetime_column(self):\n        self.df.query(\""dates < @self.ts\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Query:\n    def setup(self):\n        N = 10**6\n        halfway = (N // 2) - 1\n        index = pd.date_range(\""20010101\"", periods=N, freq=\""T\"")\n        s = pd.Series(index)\n        self.ts = s.iloc[halfway]\n        self.df = pd.DataFrame({\""a\"": np.random.randn(N), \""dates\"": index}, index=index)\n        data = np.random.randn(N)\n        self.min_val = data.min()\n        self.max_val = data.max()"", ""eval.Query.time_query_with_boolean_selection"": ""class Query:\n    def time_query_with_boolean_selection(self):\n        self.df.query(\""(a >= @self.min_val) & (a <= @self.max_val)\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Query:\n    def setup(self):\n        N = 10**6\n        halfway = (N // 2) - 1\n        index = pd.date_range(\""20010101\"", periods=N, freq=\""T\"")\n        s = pd.Series(index)\n        self.ts = s.iloc[halfway]\n        self.df = pd.DataFrame({\""a\"": np.random.randn(N), \""dates\"": index}, index=index)\n        data = np.random.randn(N)\n        self.min_val = data.min()\n        self.max_val = data.max()""}",eval,time,0.0173613583746071
30012,module-level,"terminus-2,gpt-5",1.1149391614714514,1.139579860075716,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,gil,gil,"{""gil.ParallelFactorize.time_loop"": ""class ParallelFactorize:\n    def time_loop(self, threads):\n        for i in range(threads):\n            self.loop()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ParallelFactorize:\n    def setup(self, threads):\n        strings = tm.makeStringIndex(100000)\n    \n        @test_parallel(num_threads=threads)\n        def parallel():\n            factorize(strings)\n    \n        self.parallel = parallel\n    \n        def loop():\n            factorize(strings)\n    \n        self.loop = loop"", ""gil.ParallelFactorize.time_parallel"": ""class ParallelFactorize:\n    def time_parallel(self, threads):\n        self.parallel()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ParallelFactorize:\n    def setup(self, threads):\n        strings = tm.makeStringIndex(100000)\n    \n        @test_parallel(num_threads=threads)\n        def parallel():\n            factorize(strings)\n    \n        self.parallel = parallel\n    \n        def loop():\n            factorize(strings)\n    \n        self.loop = loop"", ""gil.ParallelGroupbyMethods.time_parallel"": ""class ParallelGroupbyMethods:\n    def time_parallel(self, threads, method):\n        self.parallel()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ParallelGroupbyMethods:\n    def setup(self, threads, method):\n        N = 10**6\n        ngroups = 10**3\n        df = DataFrame(\n            {\""key\"": np.random.randint(0, ngroups, size=N), \""data\"": np.random.randn(N)}\n        )\n    \n        @test_parallel(num_threads=threads)\n        def parallel():\n            getattr(df.groupby(\""key\"")[\""data\""], method)()\n    \n        self.parallel = parallel\n    \n        def loop():\n            getattr(df.groupby(\""key\"")[\""data\""], method)()\n    \n        self.loop = loop"", ""gil.ParallelGroups.time_get_groups"": ""class ParallelGroups:\n    def time_get_groups(self, threads):\n        self.get_groups()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ParallelGroups:\n    def setup(self, threads):\n        size = 2**22\n        ngroups = 10**3\n        data = Series(np.random.randint(0, ngroups, size=size))\n    \n        @test_parallel(num_threads=threads)\n        def get_groups():\n            data.groupby(data).groups\n    \n        self.get_groups = get_groups"", ""gil.ParallelReadCSV.time_read_csv"": ""class ParallelReadCSV:\n    def time_read_csv(self, dtype):\n        self.parallel_read_csv()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ParallelReadCSV:\n    def setup(self, dtype):\n        rows = 10000\n        cols = 50\n        data = {\n            \""float\"": DataFrame(np.random.randn(rows, cols)),\n            \""datetime\"": DataFrame(\n                np.random.randn(rows, cols), index=date_range(\""1/1/2000\"", periods=rows)\n            ),\n            \""object\"": DataFrame(\n                \""foo\"", index=range(rows), columns=[\""object%03d\"" for _ in range(5)]\n            ),\n        }\n    \n        self.fname = f\""__test_{dtype}__.csv\""\n        df = data[dtype]\n        df.to_csv(self.fname)\n    \n        @test_parallel(num_threads=2)\n        def parallel_read_csv():\n            read_csv(self.fname)\n    \n        self.parallel_read_csv = parallel_read_csv"", ""gil.ParallelRolling.time_rolling"": ""class ParallelRolling:\n    def time_rolling(self, method):\n        self.parallel_rolling()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ParallelRolling:\n    def setup(self, method):\n        win = 100\n        arr = np.random.rand(100000)\n        if hasattr(DataFrame, \""rolling\""):\n            df = DataFrame(arr).rolling(win)\n    \n            @test_parallel(num_threads=2)\n            def parallel_rolling():\n                getattr(df, method)()\n    \n            self.parallel_rolling = parallel_rolling\n        elif have_rolling_methods:\n            rolling = {\n                \""median\"": rolling_median,\n                \""mean\"": rolling_mean,\n                \""min\"": rolling_min,\n                \""max\"": rolling_max,\n                \""var\"": rolling_var,\n                \""skew\"": rolling_skew,\n                \""kurt\"": rolling_kurt,\n                \""std\"": rolling_std,\n            }\n    \n            @test_parallel(num_threads=2)\n            def parallel_rolling():\n                rolling[method](arr, win)\n    \n            self.parallel_rolling = parallel_rolling\n        else:\n            raise NotImplementedError"", ""gil.ParallelTake1D.time_take1d"": ""class ParallelTake1D:\n    def time_take1d(self, dtype):\n        self.parallel_take1d()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ParallelTake1D:\n    def setup(self, dtype):\n        N = 10**6\n        df = DataFrame({\""col\"": np.arange(N, dtype=dtype)})\n        indexer = np.arange(100, len(df) - 100)\n    \n        @test_parallel(num_threads=2)\n        def parallel_take1d():\n            take_nd(df[\""col\""].values, indexer)\n    \n        self.parallel_take1d = parallel_take1d""}",gil,time,-0.017426236636679387
30010,module-level,"terminus-2,gpt-5",1.1168525998445935,1.0712587013838206,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor,frame_ctor,"{""frame_ctor.FromArrays.time_frame_from_arrays_sparse"": ""class FromArrays:\n    def time_frame_from_arrays_sparse(self):\n        self.df = DataFrame._from_arrays(\n            self.sparse_arrays,\n            index=self.index,\n            columns=self.columns,\n            verify_integrity=False,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromArrays:\n    def setup(self):\n        N_rows = 1000\n        N_cols = 1000\n        self.float_arrays = [np.random.randn(N_rows) for _ in range(N_cols)]\n        self.sparse_arrays = [\n            pd.arrays.SparseArray(np.random.randint(0, 2, N_rows), dtype=\""float64\"")\n            for _ in range(N_cols)\n        ]\n        self.int_arrays = [\n            pd.array(np.random.randint(1000, size=N_rows), dtype=\""Int64\"")\n            for _ in range(N_cols)\n        ]\n        self.index = pd.Index(range(N_rows))\n        self.columns = pd.Index(range(N_cols))"", ""frame_ctor.FromDicts.time_list_of_dict"": ""class FromDicts:\n    def time_list_of_dict(self):\n        DataFrame(self.dict_list)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDicts:\n    def setup(self):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n        frame = DataFrame(np.random.randn(N, K), index=self.index, columns=self.columns)\n        self.data = frame.to_dict()\n        self.dict_list = frame.to_dict(orient=\""records\"")\n        self.data2 = {i: {j: float(j) for j in range(100)} for i in range(2000)}\n    \n        # arrays which we won't consolidate\n        self.dict_of_categoricals = {i: Categorical(np.arange(N)) for i in range(K)}"", ""frame_ctor.FromScalar.time_frame_from_scalar_ea_float64_na"": ""class FromScalar:\n    def time_frame_from_scalar_ea_float64_na(self):\n        DataFrame(\n            NA,\n            index=range(self.nrows),\n            columns=list(\""abc\""),\n            dtype=Float64Dtype(),\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromScalar:\n    def setup(self):\n        self.nrows = 100_000"", ""frame_ctor.FromSeries.time_mi_series"": ""class FromSeries:\n    def time_mi_series(self):\n        DataFrame(self.s)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromSeries:\n    def setup(self):\n        mi = MultiIndex.from_product([range(100), range(100)])\n        self.s = Series(np.random.randn(10000), index=mi)""}",frame_ctor,time,0.03224462408824108
30007,module-level,"terminus-2,gpt-5",1.0640370665115757,1.0912388732766265,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes,dtypes,"{""dtypes.CheckDtypes.time_is_extension_array_dtype_true"": ""class CheckDtypes:\n    def time_is_extension_array_dtype_true(self):\n        is_extension_array_dtype(self.ext_dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CheckDtypes:\n    def setup(self):\n        self.ext_dtype = pd.Int64Dtype()\n        self.np_dtype = np.dtype(\""int64\"")"", ""dtypes.Dtypes.time_pandas_dtype"": ""class Dtypes:\n    def time_pandas_dtype(self, dtype):\n        pandas_dtype(dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)"", ""dtypes.DtypesInvalid.time_pandas_dtype_invalid"": ""class DtypesInvalid:\n    def time_pandas_dtype_invalid(self, dtype):\n        try:\n            pandas_dtype(self.data_dict[dtype])\n        except TypeError:\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)"", ""dtypes.SelectDtypes.time_select_dtype_bool_exclude"": ""class SelectDtypes:\n    def time_select_dtype_bool_exclude(self, dtype):\n        self.df_bool.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_bool_include"": ""class SelectDtypes:\n    def time_select_dtype_bool_include(self, dtype):\n        self.df_bool.select_dtypes(include=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_float_exclude"": ""class SelectDtypes:\n    def time_select_dtype_float_exclude(self, dtype):\n        self.df_float.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_float_include"": ""class SelectDtypes:\n    def time_select_dtype_float_include(self, dtype):\n        self.df_float.select_dtypes(include=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_int_exclude"": ""class SelectDtypes:\n    def time_select_dtype_int_exclude(self, dtype):\n        self.df_int.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_int_include"": ""class SelectDtypes:\n    def time_select_dtype_int_include(self, dtype):\n        self.df_int.select_dtypes(include=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_string_exclude"": ""class SelectDtypes:\n    def time_select_dtype_string_exclude(self, dtype):\n        self.df_string.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_string_include"": ""class SelectDtypes:\n    def time_select_dtype_string_include(self, dtype):\n        self.df_string.select_dtypes(include=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )""}",dtypes,time,-0.01923748710399632
30009,module-level,"terminus-2,gpt-5",0.9836834185219744,0.994961338034834,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,finalize,finalize,"{""finalize.Finalize.time_finalize_micro"": ""class Finalize:\n    def time_finalize_micro(self, param):\n        self.obj.__finalize__(self.obj, method=\""__finalize__\"")\n\n    def setup(self, param):\n        N = 1000\n        obj = param(dtype=float)\n        for i in range(N):\n            obj.attrs[i] = i\n        self.obj = obj""}",finalize,time,-0.0079758978167324
30013,module-level,"terminus-2,gpt-5",1.070075418851028,1.0576740147088748,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby,groupby,"{""groupby.AggEngine.time_dataframe_cython"": ""class AggEngine:\n    def time_dataframe_cython(self, parallel):\n        def function(values):\n            total = 0\n            for i, value in enumerate(values):\n                if i % 2:\n                    total += value + 5\n                else:\n                    total += value * 2\n            return total\n    \n        self.grouper.agg(function, engine=\""cython\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass AggEngine:\n    def setup(self, parallel):\n        N = 10**3\n        data = DataFrame(\n            {0: [str(i) for i in range(100)] * N, 1: list(range(100)) * N},\n            columns=[0, 1],\n        )\n        self.parallel = parallel\n        self.grouper = data.groupby(0)"", ""groupby.AggEngine.time_dataframe_numba"": ""class AggEngine:\n    def time_dataframe_numba(self, parallel):\n        def function(values, index):\n            total = 0\n            for i, value in enumerate(values):\n                if i % 2:\n                    total += value + 5\n                else:\n                    total += value * 2\n            return total\n    \n        self.grouper.agg(\n            function, engine=\""numba\"", engine_kwargs={\""parallel\"": self.parallel}\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass AggEngine:\n    def setup(self, parallel):\n        N = 10**3\n        data = DataFrame(\n            {0: [str(i) for i in range(100)] * N, 1: list(range(100)) * N},\n            columns=[0, 1],\n        )\n        self.parallel = parallel\n        self.grouper = data.groupby(0)"", ""groupby.AggEngine.time_series_cython"": ""class AggEngine:\n    def time_series_cython(self, parallel):\n        def function(values):\n            total = 0\n            for i, value in enumerate(values):\n                if i % 2:\n                    total += value + 5\n                else:\n                    total += value * 2\n            return total\n    \n        self.grouper[1].agg(function, engine=\""cython\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass AggEngine:\n    def setup(self, parallel):\n        N = 10**3\n        data = DataFrame(\n            {0: [str(i) for i in range(100)] * N, 1: list(range(100)) * N},\n            columns=[0, 1],\n        )\n        self.parallel = parallel\n        self.grouper = data.groupby(0)"", ""groupby.AggEngine.time_series_numba"": ""class AggEngine:\n    def time_series_numba(self, parallel):\n        def function(values, index):\n            total = 0\n            for i, value in enumerate(values):\n                if i % 2:\n                    total += value + 5\n                else:\n                    total += value * 2\n            return total\n    \n        self.grouper[1].agg(\n            function, engine=\""numba\"", engine_kwargs={\""parallel\"": self.parallel}\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass AggEngine:\n    def setup(self, parallel):\n        N = 10**3\n        data = DataFrame(\n            {0: [str(i) for i in range(100)] * N, 1: list(range(100)) * N},\n            columns=[0, 1],\n        )\n        self.parallel = parallel\n        self.grouper = data.groupby(0)"", ""groupby.Apply.time_copy_function_multi_col"": ""class Apply:\n    def time_copy_function_multi_col(self, factor):\n        self.df.groupby([\""key\"", \""key2\""]).apply(self.df_copy_function)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, factor):\n        N = 10**factor\n        # two cases:\n        # - small groups: small data (N**4) + many labels (2000) -> average group\n        #   size of 5 (-> larger overhead of slicing method)\n        # - larger groups: larger data (N**5) + fewer labels (20) -> average group\n        #   size of 5000\n        labels = np.random.randint(0, 2000 if factor == 4 else 20, size=N)\n        labels2 = np.random.randint(0, 3, size=N)\n        df = DataFrame(\n            {\n                \""key\"": labels,\n                \""key2\"": labels2,\n                \""value1\"": np.random.randn(N),\n                \""value2\"": [\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (N // 4),\n            }\n        )\n        self.df = df"", ""groupby.Apply.time_copy_overhead_single_col"": ""class Apply:\n    def time_copy_overhead_single_col(self, factor):\n        self.df.groupby(\""key\"").apply(self.df_copy_function)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, factor):\n        N = 10**factor\n        # two cases:\n        # - small groups: small data (N**4) + many labels (2000) -> average group\n        #   size of 5 (-> larger overhead of slicing method)\n        # - larger groups: larger data (N**5) + fewer labels (20) -> average group\n        #   size of 5000\n        labels = np.random.randint(0, 2000 if factor == 4 else 20, size=N)\n        labels2 = np.random.randint(0, 3, size=N)\n        df = DataFrame(\n            {\n                \""key\"": labels,\n                \""key2\"": labels2,\n                \""value1\"": np.random.randn(N),\n                \""value2\"": [\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (N // 4),\n            }\n        )\n        self.df = df"", ""groupby.Apply.time_scalar_function_multi_col"": ""class Apply:\n    def time_scalar_function_multi_col(self, factor):\n        self.df.groupby([\""key\"", \""key2\""]).apply(lambda x: 1)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, factor):\n        N = 10**factor\n        # two cases:\n        # - small groups: small data (N**4) + many labels (2000) -> average group\n        #   size of 5 (-> larger overhead of slicing method)\n        # - larger groups: larger data (N**5) + fewer labels (20) -> average group\n        #   size of 5000\n        labels = np.random.randint(0, 2000 if factor == 4 else 20, size=N)\n        labels2 = np.random.randint(0, 3, size=N)\n        df = DataFrame(\n            {\n                \""key\"": labels,\n                \""key2\"": labels2,\n                \""value1\"": np.random.randn(N),\n                \""value2\"": [\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (N // 4),\n            }\n        )\n        self.df = df"", ""groupby.Apply.time_scalar_function_single_col"": ""class Apply:\n    def time_scalar_function_single_col(self, factor):\n        self.df.groupby(\""key\"").apply(lambda x: 1)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, factor):\n        N = 10**factor\n        # two cases:\n        # - small groups: small data (N**4) + many labels (2000) -> average group\n        #   size of 5 (-> larger overhead of slicing method)\n        # - larger groups: larger data (N**5) + fewer labels (20) -> average group\n        #   size of 5000\n        labels = np.random.randint(0, 2000 if factor == 4 else 20, size=N)\n        labels2 = np.random.randint(0, 3, size=N)\n        df = DataFrame(\n            {\n                \""key\"": labels,\n                \""key2\"": labels2,\n                \""value1\"": np.random.randn(N),\n                \""value2\"": [\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (N // 4),\n            }\n        )\n        self.df = df"", ""groupby.ApplyNonUniqueUnsortedIndex.time_groupby_apply_non_unique_unsorted_index"": ""class ApplyNonUniqueUnsortedIndex:\n    def time_groupby_apply_non_unique_unsorted_index(self):\n        self.df.groupby(\""key\"", group_keys=False).apply(lambda x: x)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ApplyNonUniqueUnsortedIndex:\n    def setup(self):\n        # GH 46527\n        # unsorted and non-unique index\n        idx = np.arange(100)[::-1]\n        idx = Index(np.repeat(idx, 200), name=\""key\"")\n        self.df = DataFrame(np.random.randn(len(idx), 10), index=idx)"", ""groupby.Categories.time_groupby_extra_cat_nosort"": ""class Categories:\n    def time_groupby_extra_cat_nosort(self):\n        self.df_extra_cat.groupby(\""a\"", sort=False)[\""b\""].count()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Categories:\n    def setup(self):\n        N = 10**5\n        arr = np.random.random(N)\n        data = {\""a\"": Categorical(np.random.randint(10000, size=N)), \""b\"": arr}\n        self.df = DataFrame(data)\n        data = {\n            \""a\"": Categorical(np.random.randint(10000, size=N), ordered=True),\n            \""b\"": arr,\n        }\n        self.df_ordered = DataFrame(data)\n        data = {\n            \""a\"": Categorical(\n                np.random.randint(100, size=N), categories=np.arange(10000)\n            ),\n            \""b\"": arr,\n        }\n        self.df_extra_cat = DataFrame(data)"", ""groupby.Categories.time_groupby_extra_cat_sort"": ""class Categories:\n    def time_groupby_extra_cat_sort(self):\n        self.df_extra_cat.groupby(\""a\"")[\""b\""].count()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Categories:\n    def setup(self):\n        N = 10**5\n        arr = np.random.random(N)\n        data = {\""a\"": Categorical(np.random.randint(10000, size=N)), \""b\"": arr}\n        self.df = DataFrame(data)\n        data = {\n            \""a\"": Categorical(np.random.randint(10000, size=N), ordered=True),\n            \""b\"": arr,\n        }\n        self.df_ordered = DataFrame(data)\n        data = {\n            \""a\"": Categorical(\n                np.random.randint(100, size=N), categories=np.arange(10000)\n            ),\n            \""b\"": arr,\n        }\n        self.df_extra_cat = DataFrame(data)"", ""groupby.Categories.time_groupby_nosort"": ""class Categories:\n    def time_groupby_nosort(self):\n        self.df.groupby(\""a\"", sort=False)[\""b\""].count()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Categories:\n    def setup(self):\n        N = 10**5\n        arr = np.random.random(N)\n        data = {\""a\"": Categorical(np.random.randint(10000, size=N)), \""b\"": arr}\n        self.df = DataFrame(data)\n        data = {\n            \""a\"": Categorical(np.random.randint(10000, size=N), ordered=True),\n            \""b\"": arr,\n        }\n        self.df_ordered = DataFrame(data)\n        data = {\n            \""a\"": Categorical(\n                np.random.randint(100, size=N), categories=np.arange(10000)\n            ),\n            \""b\"": arr,\n        }\n        self.df_extra_cat = DataFrame(data)"", ""groupby.Categories.time_groupby_ordered_nosort"": ""class Categories:\n    def time_groupby_ordered_nosort(self):\n        self.df_ordered.groupby(\""a\"", sort=False)[\""b\""].count()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Categories:\n    def setup(self):\n        N = 10**5\n        arr = np.random.random(N)\n        data = {\""a\"": Categorical(np.random.randint(10000, size=N)), \""b\"": arr}\n        self.df = DataFrame(data)\n        data = {\n            \""a\"": Categorical(np.random.randint(10000, size=N), ordered=True),\n            \""b\"": arr,\n        }\n        self.df_ordered = DataFrame(data)\n        data = {\n            \""a\"": Categorical(\n                np.random.randint(100, size=N), categories=np.arange(10000)\n            ),\n            \""b\"": arr,\n        }\n        self.df_extra_cat = DataFrame(data)"", ""groupby.Categories.time_groupby_ordered_sort"": ""class Categories:\n    def time_groupby_ordered_sort(self):\n        self.df_ordered.groupby(\""a\"")[\""b\""].count()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Categories:\n    def setup(self):\n        N = 10**5\n        arr = np.random.random(N)\n        data = {\""a\"": Categorical(np.random.randint(10000, size=N)), \""b\"": arr}\n        self.df = DataFrame(data)\n        data = {\n            \""a\"": Categorical(np.random.randint(10000, size=N), ordered=True),\n            \""b\"": arr,\n        }\n        self.df_ordered = DataFrame(data)\n        data = {\n            \""a\"": Categorical(\n                np.random.randint(100, size=N), categories=np.arange(10000)\n            ),\n            \""b\"": arr,\n        }\n        self.df_extra_cat = DataFrame(data)"", ""groupby.Categories.time_groupby_sort"": ""class Categories:\n    def time_groupby_sort(self):\n        self.df.groupby(\""a\"")[\""b\""].count()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Categories:\n    def setup(self):\n        N = 10**5\n        arr = np.random.random(N)\n        data = {\""a\"": Categorical(np.random.randint(10000, size=N)), \""b\"": arr}\n        self.df = DataFrame(data)\n        data = {\n            \""a\"": Categorical(np.random.randint(10000, size=N), ordered=True),\n            \""b\"": arr,\n        }\n        self.df_ordered = DataFrame(data)\n        data = {\n            \""a\"": Categorical(\n                np.random.randint(100, size=N), categories=np.arange(10000)\n            ),\n            \""b\"": arr,\n        }\n        self.df_extra_cat = DataFrame(data)"", ""groupby.Cumulative.time_frame_transform"": ""class Cumulative:\n    def time_frame_transform(self, dtype, method, with_nans):\n        self.df.groupby(\""key\"").transform(method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cumulative:\n    def setup(self, dtype, method, with_nans):\n        if with_nans and dtype == \""int64\"":\n            raise NotImplementedError(\""Construction of df would raise\"")\n    \n        N = 500_000\n        keys = np.random.randint(0, 100, size=N)\n        vals = np.random.randint(-10, 10, (N, 5))\n    \n        if with_nans:\n            null_vals = vals.astype(float, copy=True)\n            null_vals[::2, :] = np.nan\n            null_vals[::3, :] = np.nan\n            df = DataFrame(null_vals, columns=list(\""abcde\""), dtype=dtype)\n            df[\""key\""] = keys\n            self.df = df\n        else:\n            df = DataFrame(vals, columns=list(\""abcde\"")).astype(dtype, copy=False)\n            df[\""key\""] = keys\n            self.df = df"", ""groupby.DateAttributes.time_len_groupby_object"": ""class DateAttributes:\n    def time_len_groupby_object(self):\n        len(self.ts.groupby([self.year, self.month, self.day]))\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DateAttributes:\n    def setup(self):\n        rng = date_range(\""1/1/2000\"", \""12/31/2005\"", freq=\""H\"")\n        self.year, self.month, self.day = rng.year, rng.month, rng.day\n        self.ts = Series(np.random.randn(len(rng)), index=rng)"", ""groupby.Datelike.time_sum"": ""class Datelike:\n    def time_sum(self, grouper):\n        self.df.groupby(self.grouper).sum()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Datelike:\n    def setup(self, grouper):\n        N = 10**4\n        rng_map = {\n            \""period_range\"": period_range,\n            \""date_range\"": date_range,\n            \""date_range_tz\"": partial(date_range, tz=\""US/Central\""),\n        }\n        self.grouper = rng_map[grouper](\""1900-01-01\"", freq=\""D\"", periods=N)\n        self.df = DataFrame(np.random.randn(10**4, 2))"", ""groupby.FillNA.time_df_bfill"": ""class FillNA:\n    def time_df_bfill(self):\n        self.df.groupby(\""group\"").fillna(method=\""bfill\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FillNA:\n    def setup(self):\n        N = 100\n        self.df = DataFrame(\n            {\""group\"": [1] * N + [2] * N, \""value\"": [np.nan, 1.0] * N}\n        ).set_index(\""group\"")"", ""groupby.FillNA.time_df_ffill"": ""class FillNA:\n    def time_df_ffill(self):\n        self.df.groupby(\""group\"").fillna(method=\""ffill\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FillNA:\n    def setup(self):\n        N = 100\n        self.df = DataFrame(\n            {\""group\"": [1] * N + [2] * N, \""value\"": [np.nan, 1.0] * N}\n        ).set_index(\""group\"")"", ""groupby.FillNA.time_srs_bfill"": ""class FillNA:\n    def time_srs_bfill(self):\n        self.df.groupby(\""group\"")[\""value\""].fillna(method=\""bfill\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FillNA:\n    def setup(self):\n        N = 100\n        self.df = DataFrame(\n            {\""group\"": [1] * N + [2] * N, \""value\"": [np.nan, 1.0] * N}\n        ).set_index(\""group\"")"", ""groupby.FillNA.time_srs_ffill"": ""class FillNA:\n    def time_srs_ffill(self):\n        self.df.groupby(\""group\"")[\""value\""].fillna(method=\""ffill\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FillNA:\n    def setup(self):\n        N = 100\n        self.df = DataFrame(\n            {\""group\"": [1] * N + [2] * N, \""value\"": [np.nan, 1.0] * N}\n        ).set_index(\""group\"")"", ""groupby.Float32.time_sum"": ""class Float32:\n    def time_sum(self):\n        self.df.groupby([\""a\""])[\""b\""].sum()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Float32:\n    def setup(self):\n        tmp1 = (np.random.random(10000) * 0.1).astype(np.float32)\n        tmp2 = (np.random.random(10000) * 10.0).astype(np.float32)\n        tmp = np.concatenate((tmp1, tmp2))\n        arr = np.repeat(tmp, 10)\n        self.df = DataFrame({\""a\"": arr, \""b\"": arr})"", ""groupby.GroupByCythonAgg.time_frame_agg"": ""class GroupByCythonAgg:\n    def time_frame_agg(self, dtype, method):\n        self.df.groupby(\""key\"").agg(method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupByCythonAgg:\n    def setup(self, dtype, method):\n        N = 1_000_000\n        df = DataFrame(np.random.randn(N, 10), columns=list(\""abcdefghij\""))\n        df[\""key\""] = np.random.randint(0, 100, size=N)\n        self.df = df"", ""groupby.GroupByCythonAggEaDtypes.time_frame_agg"": ""class GroupByCythonAggEaDtypes:\n    def time_frame_agg(self, dtype, method):\n        self.df.groupby(\""key\"").agg(method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupByCythonAggEaDtypes:\n    def setup(self, dtype, method):\n        N = 1_000_000\n        df = DataFrame(\n            np.random.randint(0, high=100, size=(N, 10)),\n            columns=list(\""abcdefghij\""),\n            dtype=dtype,\n        )\n        df.loc[list(range(1, N, 5)), list(\""abcdefghij\"")] = NA\n        df[\""key\""] = np.random.randint(0, 100, size=N)\n        self.df = df"", ""groupby.GroupByMethods.time_dtype_as_field"": ""class GroupByMethods:\n    def time_dtype_as_field(self, dtype, method, application, ncols):\n        self.as_field_method()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupByMethods:\n    def setup(self, dtype, method, application, ncols):\n        if method in method_blocklist.get(dtype, {}):\n            raise NotImplementedError  # skip benchmark\n    \n        if ncols != 1 and method in [\""value_counts\"", \""unique\""]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if application == \""transformation\"" and method in [\n            \""describe\"",\n            \""head\"",\n            \""tail\"",\n            \""unique\"",\n            \""value_counts\"",\n            \""size\"",\n        ]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if method == \""describe\"":\n            ngroups = 20\n        elif method == \""skew\"":\n            ngroups = 100\n        else:\n            ngroups = 1000\n        size = ngroups * 2\n        rng = np.arange(ngroups).reshape(-1, 1)\n        rng = np.broadcast_to(rng, (len(rng), ncols))\n        taker = np.random.randint(0, ngroups, size=size)\n        values = rng.take(taker, axis=0)\n        if dtype == \""int\"":\n            key = np.random.randint(0, size, size=size)\n        elif dtype in (\""int16\"", \""uint\""):\n            key = np.random.randint(0, size, size=size, dtype=dtype)\n        elif dtype == \""float\"":\n            key = np.concatenate(\n                [np.random.random(ngroups) * 0.1, np.random.random(ngroups) * 10.0]\n            )\n        elif dtype == \""object\"":\n            key = [\""foo\""] * size\n        elif dtype == \""datetime\"":\n            key = date_range(\""1/1/2011\"", periods=size, freq=\""s\"")\n    \n        cols = [f\""values{n}\"" for n in range(ncols)]\n        df = DataFrame(values, columns=cols)\n        df[\""key\""] = key\n    \n        if len(cols) == 1:\n            cols = cols[0]\n    \n        if application == \""transformation\"":\n            self.as_group_method = lambda: df.groupby(\""key\"")[cols].transform(method)\n            self.as_field_method = lambda: df.groupby(cols)[\""key\""].transform(method)\n        else:\n            self.as_group_method = getattr(df.groupby(\""key\"")[cols], method)\n            self.as_field_method = getattr(df.groupby(cols)[\""key\""], method)"", ""groupby.GroupByMethods.time_dtype_as_group"": ""class GroupByMethods:\n    def time_dtype_as_group(self, dtype, method, application, ncols):\n        self.as_group_method()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupByMethods:\n    def setup(self, dtype, method, application, ncols):\n        if method in method_blocklist.get(dtype, {}):\n            raise NotImplementedError  # skip benchmark\n    \n        if ncols != 1 and method in [\""value_counts\"", \""unique\""]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if application == \""transformation\"" and method in [\n            \""describe\"",\n            \""head\"",\n            \""tail\"",\n            \""unique\"",\n            \""value_counts\"",\n            \""size\"",\n        ]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if method == \""describe\"":\n            ngroups = 20\n        elif method == \""skew\"":\n            ngroups = 100\n        else:\n            ngroups = 1000\n        size = ngroups * 2\n        rng = np.arange(ngroups).reshape(-1, 1)\n        rng = np.broadcast_to(rng, (len(rng), ncols))\n        taker = np.random.randint(0, ngroups, size=size)\n        values = rng.take(taker, axis=0)\n        if dtype == \""int\"":\n            key = np.random.randint(0, size, size=size)\n        elif dtype in (\""int16\"", \""uint\""):\n            key = np.random.randint(0, size, size=size, dtype=dtype)\n        elif dtype == \""float\"":\n            key = np.concatenate(\n                [np.random.random(ngroups) * 0.1, np.random.random(ngroups) * 10.0]\n            )\n        elif dtype == \""object\"":\n            key = [\""foo\""] * size\n        elif dtype == \""datetime\"":\n            key = date_range(\""1/1/2011\"", periods=size, freq=\""s\"")\n    \n        cols = [f\""values{n}\"" for n in range(ncols)]\n        df = DataFrame(values, columns=cols)\n        df[\""key\""] = key\n    \n        if len(cols) == 1:\n            cols = cols[0]\n    \n        if application == \""transformation\"":\n            self.as_group_method = lambda: df.groupby(\""key\"")[cols].transform(method)\n            self.as_field_method = lambda: df.groupby(cols)[\""key\""].transform(method)\n        else:\n            self.as_group_method = getattr(df.groupby(\""key\"")[cols], method)\n            self.as_field_method = getattr(df.groupby(cols)[\""key\""], method)"", ""groupby.GroupStrings.time_multi_columns"": ""class GroupStrings:\n    def time_multi_columns(self):\n        self.df.groupby(list(\""abcd\"")).max()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupStrings:\n    def setup(self):\n        n = 2 * 10**5\n        alpha = list(map(\""\"".join, product(ascii_letters, repeat=4)))\n        data = np.random.choice(alpha, (n // 5, 4), replace=False)\n        data = np.repeat(data, 5, axis=0)\n        self.df = DataFrame(data, columns=list(\""abcd\""))\n        self.df[\""joe\""] = (np.random.randn(len(self.df)) * 10).round(3)\n        self.df = self.df.sample(frac=1).reset_index(drop=True)"", ""groupby.Int64.time_overflow"": ""class Int64:\n    def time_overflow(self):\n        self.df.groupby(self.cols).max()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Int64:\n    def setup(self):\n        arr = np.random.randint(-1 << 12, 1 << 12, (1 << 17, 5))\n        i = np.random.choice(len(arr), len(arr) * 5)\n        arr = np.vstack((arr, arr[i]))\n        i = np.random.permutation(len(arr))\n        arr = arr[i]\n        self.cols = list(\""abcde\"")\n        self.df = DataFrame(arr, columns=self.cols)\n        self.df[\""jim\""], self.df[\""joe\""] = np.random.randn(2, len(self.df)) * 10"", ""groupby.MultiColumn.time_lambda_sum"": ""class MultiColumn:\n    def time_lambda_sum(self, df):\n        df.groupby([\""key1\"", \""key2\""]).agg(lambda x: x.values.sum())\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MultiColumn:\n    def setup_cache(self):\n        N = 10**5\n        key1 = np.tile(np.arange(100, dtype=object), 1000)\n        key2 = key1.copy()\n        np.random.shuffle(key1)\n        np.random.shuffle(key2)\n        df = DataFrame(\n            {\n                \""key1\"": key1,\n                \""key2\"": key2,\n                \""data1\"": np.random.randn(N),\n                \""data2\"": np.random.randn(N),\n            }\n        )\n        return df"", ""groupby.Nth.time_frame_nth"": ""class Nth:\n    def time_frame_nth(self, dtype):\n        self.df.groupby(\""key\"").nth(0)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Nth:\n    def setup(self, dtype):\n        N = 10**5\n        # with datetimes (GH7555)\n        if dtype == \""datetime\"":\n            values = date_range(\""1/1/2011\"", periods=N, freq=\""s\"")\n        elif dtype == \""object\"":\n            values = [\""foo\""] * N\n        else:\n            values = np.arange(N).astype(dtype)\n    \n        key = np.arange(N)\n        self.df = DataFrame({\""key\"": key, \""values\"": values})\n        self.df.iloc[1, 1] = np.nan  # insert missing data"", ""groupby.Nth.time_frame_nth_any"": ""class Nth:\n    def time_frame_nth_any(self, dtype):\n        self.df.groupby(\""key\"").nth(0, dropna=\""any\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Nth:\n    def setup(self, dtype):\n        N = 10**5\n        # with datetimes (GH7555)\n        if dtype == \""datetime\"":\n            values = date_range(\""1/1/2011\"", periods=N, freq=\""s\"")\n        elif dtype == \""object\"":\n            values = [\""foo\""] * N\n        else:\n            values = np.arange(N).astype(dtype)\n    \n        key = np.arange(N)\n        self.df = DataFrame({\""key\"": key, \""values\"": values})\n        self.df.iloc[1, 1] = np.nan  # insert missing data"", ""groupby.Nth.time_groupby_nth_all"": ""class Nth:\n    def time_groupby_nth_all(self, dtype):\n        self.df.groupby(\""key\"").nth(0, dropna=\""all\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Nth:\n    def setup(self, dtype):\n        N = 10**5\n        # with datetimes (GH7555)\n        if dtype == \""datetime\"":\n            values = date_range(\""1/1/2011\"", periods=N, freq=\""s\"")\n        elif dtype == \""object\"":\n            values = [\""foo\""] * N\n        else:\n            values = np.arange(N).astype(dtype)\n    \n        key = np.arange(N)\n        self.df = DataFrame({\""key\"": key, \""values\"": values})\n        self.df.iloc[1, 1] = np.nan  # insert missing data"", ""groupby.Nth.time_series_nth"": ""class Nth:\n    def time_series_nth(self, dtype):\n        self.df[\""values\""].groupby(self.df[\""key\""]).nth(0)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Nth:\n    def setup(self, dtype):\n        N = 10**5\n        # with datetimes (GH7555)\n        if dtype == \""datetime\"":\n            values = date_range(\""1/1/2011\"", periods=N, freq=\""s\"")\n        elif dtype == \""object\"":\n            values = [\""foo\""] * N\n        else:\n            values = np.arange(N).astype(dtype)\n    \n        key = np.arange(N)\n        self.df = DataFrame({\""key\"": key, \""values\"": values})\n        self.df.iloc[1, 1] = np.nan  # insert missing data"", ""groupby.Nth.time_series_nth_all"": ""class Nth:\n    def time_series_nth_all(self, dtype):\n        self.df[\""values\""].groupby(self.df[\""key\""]).nth(0, dropna=\""all\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Nth:\n    def setup(self, dtype):\n        N = 10**5\n        # with datetimes (GH7555)\n        if dtype == \""datetime\"":\n            values = date_range(\""1/1/2011\"", periods=N, freq=\""s\"")\n        elif dtype == \""object\"":\n            values = [\""foo\""] * N\n        else:\n            values = np.arange(N).astype(dtype)\n    \n        key = np.arange(N)\n        self.df = DataFrame({\""key\"": key, \""values\"": values})\n        self.df.iloc[1, 1] = np.nan  # insert missing data"", ""groupby.Nth.time_series_nth_any"": ""class Nth:\n    def time_series_nth_any(self, dtype):\n        self.df[\""values\""].groupby(self.df[\""key\""]).nth(0, dropna=\""any\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Nth:\n    def setup(self, dtype):\n        N = 10**5\n        # with datetimes (GH7555)\n        if dtype == \""datetime\"":\n            values = date_range(\""1/1/2011\"", periods=N, freq=\""s\"")\n        elif dtype == \""object\"":\n            values = [\""foo\""] * N\n        else:\n            values = np.arange(N).astype(dtype)\n    \n        key = np.arange(N)\n        self.df = DataFrame({\""key\"": key, \""values\"": values})\n        self.df.iloc[1, 1] = np.nan  # insert missing data"", ""groupby.RankWithTies.time_rank_ties"": ""class RankWithTies:\n    def time_rank_ties(self, dtype, tie_method):\n        self.df.groupby(\""key\"").rank(method=tie_method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass RankWithTies:\n    def setup(self, dtype, tie_method):\n        N = 10**4\n        if dtype == \""datetime64\"":\n            data = np.array([Timestamp(\""2011/01/01\"")] * N, dtype=dtype)\n        else:\n            data = np.array([1] * N, dtype=dtype)\n        self.df = DataFrame({\""values\"": data, \""key\"": [\""foo\""] * N})"", ""groupby.Shift.time_defaults"": ""class Shift:\n    def time_defaults(self):\n        self.df.groupby(\""g\"").shift()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Shift:\n    def setup(self):\n        N = 18\n        self.df = DataFrame({\""g\"": [\""a\"", \""b\""] * 9, \""v\"": list(range(N))})"", ""groupby.Shift.time_fill_value"": ""class Shift:\n    def time_fill_value(self):\n        self.df.groupby(\""g\"").shift(fill_value=99)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Shift:\n    def setup(self):\n        N = 18\n        self.df = DataFrame({\""g\"": [\""a\"", \""b\""] * 9, \""v\"": list(range(N))})"", ""groupby.Size.time_multi_size"": ""class Size:\n    def time_multi_size(self):\n        self.df.groupby([\""key1\"", \""key2\""]).size()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Size:\n    def setup(self):\n        n = 10**5\n        offsets = np.random.randint(n, size=n).astype(\""timedelta64[ns]\"")\n        dates = np.datetime64(\""now\"") + offsets\n        self.df = DataFrame(\n            {\n                \""key1\"": np.random.randint(0, 500, size=n),\n                \""key2\"": np.random.randint(0, 100, size=n),\n                \""value1\"": np.random.randn(n),\n                \""value2\"": np.random.randn(n),\n                \""value3\"": np.random.randn(n),\n                \""dates\"": dates,\n            }\n        )\n        self.draws = Series(np.random.randn(n))\n        labels = Series([\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (n // 4))\n        self.cats = labels.astype(\""category\"")"", ""groupby.String.time_str_func"": ""class String:\n    def time_str_func(self, dtype, method):\n        self.df.groupby(\""a\"")[self.df.columns[1:]].agg(method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass String:\n    def setup(self, dtype, method):\n        cols = list(\""abcdefghjkl\"")\n        self.df = DataFrame(\n            np.random.randint(0, 100, size=(10_000, len(cols))),\n            columns=cols,\n            dtype=dtype,\n        )"", ""groupby.SumBools.time_groupby_sum_booleans"": ""class SumBools:\n    def time_groupby_sum_booleans(self):\n        self.df.groupby(\""ii\"").sum()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SumBools:\n    def setup(self):\n        N = 500\n        self.df = DataFrame({\""ii\"": range(N), \""bb\"": [True] * N})"", ""groupby.Transform.time_transform_lambda_max"": ""class Transform:\n    def time_transform_lambda_max(self):\n        self.df.groupby(level=\""lev1\"").transform(lambda x: max(x))\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]"", ""groupby.Transform.time_transform_lambda_max_tall"": ""class Transform:\n    def time_transform_lambda_max_tall(self):\n        self.df_tall.groupby(level=0).transform(lambda x: np.max(x, axis=0))\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]"", ""groupby.Transform.time_transform_lambda_max_wide"": ""class Transform:\n    def time_transform_lambda_max_wide(self):\n        self.df_wide.groupby(level=0).transform(lambda x: np.max(x, axis=0))\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]"", ""groupby.Transform.time_transform_multi_key1"": ""class Transform:\n    def time_transform_multi_key1(self):\n        self.df1.groupby([\""jim\"", \""joe\""])[\""jolie\""].transform(\""max\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]"", ""groupby.Transform.time_transform_multi_key2"": ""class Transform:\n    def time_transform_multi_key2(self):\n        self.df2.groupby([\""jim\"", \""joe\""])[\""jolie\""].transform(\""max\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]"", ""groupby.Transform.time_transform_ufunc_max"": ""class Transform:\n    def time_transform_ufunc_max(self):\n        self.df.groupby(level=\""lev1\"").transform(np.max)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]"", ""groupby.TransformBools.time_transform_mean"": ""class TransformBools:\n    def time_transform_mean(self):\n        self.df[\""signal\""].groupby(self.g).transform(np.mean)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TransformBools:\n    def setup(self):\n        N = 120000\n        transition_points = np.sort(np.random.choice(np.arange(N), 1400))\n        transitions = np.zeros(N, dtype=np.bool_)\n        transitions[transition_points] = True\n        self.g = transitions.cumsum()\n        self.df = DataFrame({\""signal\"": np.random.rand(N)})"", ""groupby.TransformEngine.time_dataframe_cython"": ""class TransformEngine:\n    def time_dataframe_cython(self, parallel):\n        def function(values):\n            return values * 5\n    \n        self.grouper.transform(function, engine=\""cython\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TransformEngine:\n    def setup(self, parallel):\n        N = 10**3\n        data = DataFrame(\n            {0: [str(i) for i in range(100)] * N, 1: list(range(100)) * N},\n            columns=[0, 1],\n        )\n        self.parallel = parallel\n        self.grouper = data.groupby(0)"", ""groupby.TransformEngine.time_dataframe_numba"": ""class TransformEngine:\n    def time_dataframe_numba(self, parallel):\n        def function(values, index):\n            return values * 5\n    \n        self.grouper.transform(\n            function, engine=\""numba\"", engine_kwargs={\""parallel\"": self.parallel}\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TransformEngine:\n    def setup(self, parallel):\n        N = 10**3\n        data = DataFrame(\n            {0: [str(i) for i in range(100)] * N, 1: list(range(100)) * N},\n            columns=[0, 1],\n        )\n        self.parallel = parallel\n        self.grouper = data.groupby(0)"", ""groupby.TransformEngine.time_series_cython"": ""class TransformEngine:\n    def time_series_cython(self, parallel):\n        def function(values):\n            return values * 5\n    \n        self.grouper[1].transform(function, engine=\""cython\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TransformEngine:\n    def setup(self, parallel):\n        N = 10**3\n        data = DataFrame(\n            {0: [str(i) for i in range(100)] * N, 1: list(range(100)) * N},\n            columns=[0, 1],\n        )\n        self.parallel = parallel\n        self.grouper = data.groupby(0)"", ""groupby.TransformEngine.time_series_numba"": ""class TransformEngine:\n    def time_series_numba(self, parallel):\n        def function(values, index):\n            return values * 5\n    \n        self.grouper[1].transform(\n            function, engine=\""numba\"", engine_kwargs={\""parallel\"": self.parallel}\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TransformEngine:\n    def setup(self, parallel):\n        N = 10**3\n        data = DataFrame(\n            {0: [str(i) for i in range(100)] * N, 1: list(range(100)) * N},\n            columns=[0, 1],\n        )\n        self.parallel = parallel\n        self.grouper = data.groupby(0)"", ""groupby.TransformNaN.time_first"": ""class TransformNaN:\n    def time_first(self):\n        self.df_nans.groupby(\""key\"").transform(\""first\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TransformNaN:\n    def setup(self):\n        self.df_nans = DataFrame(\n            {\""key\"": np.repeat(np.arange(1000), 10), \""B\"": np.nan, \""C\"": np.nan}\n        )\n        self.df_nans.loc[4::10, \""B\"":\""C\""] = 5""}",groupby,time,0.008770441401805646
30004,module-level,"terminus-2,gpt-5",0.9994774685975464,0.9690018695661616,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,boolean,boolean,"{""boolean.TimeLogicalOps.time_and_array"": ""class TimeLogicalOps:\n    def time_and_array(self):\n        self.left & self.right\n\n    def setup(self):\n        N = 10_000\n        left, right, lmask, rmask = np.random.randint(0, 2, size=(4, N)).astype(\""bool\"")\n        self.left = pd.arrays.BooleanArray(left, lmask)\n        self.right = pd.arrays.BooleanArray(right, rmask)"", ""boolean.TimeLogicalOps.time_and_scalar"": ""class TimeLogicalOps:\n    def time_and_scalar(self):\n        self.left & True\n        self.left & False\n\n    def setup(self):\n        N = 10_000\n        left, right, lmask, rmask = np.random.randint(0, 2, size=(4, N)).astype(\""bool\"")\n        self.left = pd.arrays.BooleanArray(left, lmask)\n        self.right = pd.arrays.BooleanArray(right, rmask)""}",boolean,time,0.021552757447938296
30015,module-level,"terminus-2,gpt-5",1.318807753788909,1.0570132078439995,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_cached_properties,index_cached_properties,"{""index_cached_properties.IndexCache.time_engine"": ""class IndexCache:\n    def time_engine(self, index_type):\n        self.idx._engine\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}"", ""index_cached_properties.IndexCache.time_inferred_type"": ""class IndexCache:\n    def time_inferred_type(self, index_type):\n        self.idx.inferred_type\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}"", ""index_cached_properties.IndexCache.time_is_monotonic_decreasing"": ""class IndexCache:\n    def time_is_monotonic_decreasing(self, index_type):\n        self.idx.is_monotonic_decreasing\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}"", ""index_cached_properties.IndexCache.time_is_monotonic_increasing"": ""class IndexCache:\n    def time_is_monotonic_increasing(self, index_type):\n        self.idx.is_monotonic_increasing\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}"", ""index_cached_properties.IndexCache.time_shape"": ""class IndexCache:\n    def time_shape(self, index_type):\n        self.idx.shape\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}""}",index_cached_properties,time,0.18514465766966717
30005,module-level,"terminus-2,gpt-5",1.0568614181804572,0.9825157394271008,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals,categoricals,"{""categoricals.CategoricalSlicing.time_getitem_scalar"": ""class CategoricalSlicing:\n    def time_getitem_scalar(self, index):\n        self.data[self.scalar]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalSlicing:\n    def setup(self, index):\n        N = 10**6\n        categories = [\""a\"", \""b\"", \""c\""]\n        values = [0] * N + [1] * N + [2] * N\n        if index == \""monotonic_incr\"":\n            self.data = pd.Categorical.from_codes(values, categories=categories)\n        elif index == \""monotonic_decr\"":\n            self.data = pd.Categorical.from_codes(\n                list(reversed(values)), categories=categories\n            )\n        elif index == \""non_monotonic\"":\n            self.data = pd.Categorical.from_codes([0, 1, 2] * N, categories=categories)\n        else:\n            raise ValueError(f\""Invalid index param: {index}\"")\n    \n        self.scalar = 10000\n        self.list = list(range(10000))\n        self.cat_scalar = \""b\"""", ""categoricals.Contains.time_categorical_contains"": ""class Contains:\n    def time_categorical_contains(self):\n        self.key in self.c\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Contains:\n    def setup(self):\n        N = 10**5\n        self.ci = tm.makeCategoricalIndex(N)\n        self.c = self.ci.values\n        self.key = self.ci.categories[0]"", ""categoricals.ValueCounts.time_value_counts"": ""class ValueCounts:\n    def time_value_counts(self, dropna):\n        self.ts.value_counts(dropna=dropna)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ValueCounts:\n    def setup(self, dropna):\n        n = 5 * 10**5\n        arr = [f\""s{i:04d}\"" for i in np.random.randint(0, n // 10, size=n)]\n        self.ts = pd.Series(arr).astype(\""category\"")""}",categoricals,time,0.05257827351722518
30000,module-level,"terminus-2,gpt-5",1.1779769566555465,1.1226153547165727,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,algorithms,algorithms,"{""algorithms.Duplicated.time_duplicated"": ""class Duplicated:\n    def time_duplicated(self, unique, keep, dtype):\n        self.idx.duplicated(keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Duplicated:\n    def setup(self, unique, keep, dtype):\n        N = 10**5\n        data = {\n            \""int\"": pd.Index(np.arange(N), dtype=\""int64\""),\n            \""uint\"": pd.Index(np.arange(N), dtype=\""uint64\""),\n            \""float\"": pd.Index(np.random.randn(N), dtype=\""float64\""),\n            \""string\"": tm.makeStringIndex(N),\n            \""datetime64[ns]\"": pd.date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n            \""datetime64[ns, tz]\"": pd.date_range(\n                \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n            ),\n        }[dtype]\n        if not unique:\n            data = data.repeat(5)\n        self.idx = data\n        # cache is_unique\n        self.idx.is_unique"", ""algorithms.DuplicatedMaskedArray.time_duplicated"": ""class DuplicatedMaskedArray:\n    def time_duplicated(self, unique, keep, dtype):\n        self.ser.duplicated(keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DuplicatedMaskedArray:\n    def setup(self, unique, keep, dtype):\n        N = 10**5\n        data = pd.Series(np.arange(N), dtype=dtype)\n        data[list(range(1, N, 100))] = pd.NA\n        if not unique:\n            data = data.repeat(5)\n        self.ser = data\n        # cache is_unique\n        self.ser.is_unique"", ""algorithms.Factorize.time_factorize"": ""class Factorize:\n    def time_factorize(self, unique, sort, dtype):\n        pd.factorize(self.data, sort=sort)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Factorize:\n    def setup(self, unique, sort, dtype):\n        N = 10**5\n        string_index = tm.makeStringIndex(N)\n        string_arrow = None\n        if dtype == \""string[pyarrow]\"":\n            try:\n                string_arrow = pd.array(string_index, dtype=\""string[pyarrow]\"")\n            except ImportError:\n                raise NotImplementedError\n    \n        data = {\n            \""int\"": pd.Index(np.arange(N), dtype=\""int64\""),\n            \""uint\"": pd.Index(np.arange(N), dtype=\""uint64\""),\n            \""float\"": pd.Index(np.random.randn(N), dtype=\""float64\""),\n            \""object\"": string_index,\n            \""datetime64[ns]\"": pd.date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n            \""datetime64[ns, tz]\"": pd.date_range(\n                \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n            ),\n            \""Int64\"": pd.array(np.arange(N), dtype=\""Int64\""),\n            \""boolean\"": pd.array(np.random.randint(0, 2, N), dtype=\""boolean\""),\n            \""string[pyarrow]\"": string_arrow,\n        }[dtype]\n        if not unique:\n            data = data.repeat(5)\n        self.data = data"", ""algorithms.Quantile.time_quantile"": ""class Quantile:\n    def time_quantile(self, quantile, interpolation, dtype):\n        self.idx.quantile(quantile, interpolation=interpolation)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, quantile, interpolation, dtype):\n        N = 10**5\n        data = {\n            \""int\"": np.arange(N),\n            \""uint\"": np.arange(N).astype(np.uint64),\n            \""float\"": np.random.randn(N),\n        }\n        self.idx = pd.Series(data[dtype].repeat(5))""}",algorithms,time,0.039152476618793335
30011,module-level,"terminus-2,gpt-5",1.137970498644126,1.1262051258054844,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods,frame_methods,"{""frame_methods.Count.time_count_level_mixed_dtypes_multi"": ""class Count:\n    def time_count_level_mixed_dtypes_multi(self, axis):\n        self.df_mixed.count(axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Count:\n    def setup(self, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""\n    \n        self.df.index = MultiIndex.from_arrays([self.df.index, self.df.index])\n        self.df.columns = MultiIndex.from_arrays([self.df.columns, self.df.columns])\n        self.df_mixed.index = MultiIndex.from_arrays(\n            [self.df_mixed.index, self.df_mixed.index]\n        )\n        self.df_mixed.columns = MultiIndex.from_arrays(\n            [self.df_mixed.columns, self.df_mixed.columns]\n        )"", ""frame_methods.Count.time_count_level_multi"": ""class Count:\n    def time_count_level_multi(self, axis):\n        self.df.count(axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Count:\n    def setup(self, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""\n    \n        self.df.index = MultiIndex.from_arrays([self.df.index, self.df.index])\n        self.df.columns = MultiIndex.from_arrays([self.df.columns, self.df.columns])\n        self.df_mixed.index = MultiIndex.from_arrays(\n            [self.df_mixed.index, self.df_mixed.index]\n        )\n        self.df_mixed.columns = MultiIndex.from_arrays(\n            [self.df_mixed.columns, self.df_mixed.columns]\n        )"", ""frame_methods.Describe.time_dataframe_describe"": ""class Describe:\n    def time_dataframe_describe(self):\n        self.df.describe()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Describe:\n    def setup(self):\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(0, 100, 10**6),\n                \""b\"": np.random.randint(0, 100, 10**6),\n                \""c\"": np.random.randint(0, 100, 10**6),\n            }\n        )"", ""frame_methods.Describe.time_series_describe"": ""class Describe:\n    def time_series_describe(self):\n        self.df[\""a\""].describe()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Describe:\n    def setup(self):\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(0, 100, 10**6),\n                \""b\"": np.random.randint(0, 100, 10**6),\n                \""c\"": np.random.randint(0, 100, 10**6),\n            }\n        )"", ""frame_methods.Dropna.time_dropna"": ""class Dropna:\n    def time_dropna(self, how, axis):\n        self.df.dropna(how=how, axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Dropna:\n    def setup(self, how, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\"""", ""frame_methods.Dropna.time_dropna_axis_mixed_dtypes"": ""class Dropna:\n    def time_dropna_axis_mixed_dtypes(self, how, axis):\n        self.df_mixed.dropna(how=how, axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Dropna:\n    def setup(self, how, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\"""", ""frame_methods.Duplicated.time_frame_duplicated_wide"": ""class Duplicated:\n    def time_frame_duplicated_wide(self):\n        self.df2.duplicated()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Duplicated:\n    def setup(self):\n        n = 1 << 20\n        t = date_range(\""2015-01-01\"", freq=\""S\"", periods=(n // 64))\n        xs = np.random.randn(n // 64).round(2)\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(-1 << 8, 1 << 8, n),\n                \""b\"": np.random.choice(t, n),\n                \""c\"": np.random.choice(xs, n),\n            }\n        )\n        self.df2 = DataFrame(np.random.randn(1000, 100).astype(str)).T"", ""frame_methods.Equals.time_frame_float_equal"": ""class Equals:\n    def time_frame_float_equal(self):\n        self.float_df.equals(self.float_df)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Equals:\n    def setup(self):\n        N = 10**3\n        self.float_df = DataFrame(np.random.randn(N, N))\n        self.float_df_nan = self.float_df.copy()\n        self.float_df_nan.iloc[-1, -1] = np.nan\n    \n        self.object_df = DataFrame(\""foo\"", index=range(N), columns=range(N))\n        self.object_df_nan = self.object_df.copy()\n        self.object_df_nan.iloc[-1, -1] = np.nan\n    \n        self.nonunique_cols = self.object_df.copy()\n        self.nonunique_cols.columns = [\""A\""] * len(self.nonunique_cols.columns)\n        self.nonunique_cols_nan = self.nonunique_cols.copy()\n        self.nonunique_cols_nan.iloc[-1, -1] = np.nan"", ""frame_methods.Equals.time_frame_float_unequal"": ""class Equals:\n    def time_frame_float_unequal(self):\n        self.float_df.equals(self.float_df_nan)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Equals:\n    def setup(self):\n        N = 10**3\n        self.float_df = DataFrame(np.random.randn(N, N))\n        self.float_df_nan = self.float_df.copy()\n        self.float_df_nan.iloc[-1, -1] = np.nan\n    \n        self.object_df = DataFrame(\""foo\"", index=range(N), columns=range(N))\n        self.object_df_nan = self.object_df.copy()\n        self.object_df_nan.iloc[-1, -1] = np.nan\n    \n        self.nonunique_cols = self.object_df.copy()\n        self.nonunique_cols.columns = [\""A\""] * len(self.nonunique_cols.columns)\n        self.nonunique_cols_nan = self.nonunique_cols.copy()\n        self.nonunique_cols_nan.iloc[-1, -1] = np.nan"", ""frame_methods.Equals.time_frame_nonunique_equal"": ""class Equals:\n    def time_frame_nonunique_equal(self):\n        self.nonunique_cols.equals(self.nonunique_cols)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Equals:\n    def setup(self):\n        N = 10**3\n        self.float_df = DataFrame(np.random.randn(N, N))\n        self.float_df_nan = self.float_df.copy()\n        self.float_df_nan.iloc[-1, -1] = np.nan\n    \n        self.object_df = DataFrame(\""foo\"", index=range(N), columns=range(N))\n        self.object_df_nan = self.object_df.copy()\n        self.object_df_nan.iloc[-1, -1] = np.nan\n    \n        self.nonunique_cols = self.object_df.copy()\n        self.nonunique_cols.columns = [\""A\""] * len(self.nonunique_cols.columns)\n        self.nonunique_cols_nan = self.nonunique_cols.copy()\n        self.nonunique_cols_nan.iloc[-1, -1] = np.nan"", ""frame_methods.Fillna.time_frame_fillna"": ""class Fillna:\n    def time_frame_fillna(self, inplace, method, dtype):\n        self.df.fillna(inplace=inplace, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Fillna:\n    def setup(self, inplace, method, dtype):\n        N, M = 10000, 100\n        if dtype in (\""datetime64[ns]\"", \""datetime64[ns, tz]\"", \""timedelta64[ns]\""):\n            data = {\n                \""datetime64[ns]\"": date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n                \""datetime64[ns, tz]\"": date_range(\n                    \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n                ),\n                \""timedelta64[ns]\"": timedelta_range(start=\""1 day\"", periods=N, freq=\""1D\""),\n            }\n            self.df = DataFrame({f\""col_{i}\"": data[dtype] for i in range(M)})\n            self.df[::2] = None\n        else:\n            values = np.random.randn(N, M)\n            values[::2] = np.nan\n            if dtype == \""Int64\"":\n                values = values.round()\n            self.df = DataFrame(values, dtype=dtype)"", ""frame_methods.Interpolate.time_interpolate"": ""class Interpolate:\n    def time_interpolate(self, downcast):\n        self.df.interpolate(downcast=downcast)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Interpolate:\n    def setup(self, downcast):\n        N = 10000\n        # this is the worst case, where every column has NaNs.\n        arr = np.random.randn(N, 100)\n        # NB: we need to set values in array, not in df.values, otherwise\n        #  the benchmark will be misleading for ArrayManager\n        arr[::2] = np.nan\n    \n        self.df = DataFrame(arr)\n    \n        self.df2 = DataFrame(\n            {\n                \""A\"": np.arange(0, N),\n                \""B\"": np.random.randint(0, 100, N),\n                \""C\"": np.random.randn(N),\n                \""D\"": np.random.randn(N),\n            }\n        )\n        self.df2.loc[1::5, \""A\""] = np.nan\n        self.df2.loc[1::5, \""C\""] = np.nan"", ""frame_methods.Iteration.time_items_cached"": ""class Iteration:\n    def time_items_cached(self):\n        for name, col in self.df.items():\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))"", ""frame_methods.Iteration.time_iterrows"": ""class Iteration:\n    def time_iterrows(self):\n        for row in self.df.iterrows():\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))"", ""frame_methods.Reindex.time_reindex_axis1"": ""class Reindex:\n    def time_reindex_axis1(self):\n        self.df.reindex(columns=self.idx_cols)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Reindex:\n    def setup(self):\n        N = 10**3\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.idx = np.arange(4 * N, 7 * N)\n        self.idx_cols = np.random.randint(0, N, N)\n        self.df2 = DataFrame(\n            {\n                c: {\n                    0: np.random.randint(0, 2, N).astype(np.bool_),\n                    1: np.random.randint(0, N, N).astype(np.int16),\n                    2: np.random.randint(0, N, N).astype(np.int32),\n                    3: np.random.randint(0, N, N).astype(np.int64),\n                }[np.random.randint(0, 4)]\n                for c in range(N)\n            }\n        )"", ""frame_methods.Reindex.time_reindex_both_axes"": ""class Reindex:\n    def time_reindex_both_axes(self):\n        self.df.reindex(index=self.idx, columns=self.idx_cols)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Reindex:\n    def setup(self):\n        N = 10**3\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.idx = np.arange(4 * N, 7 * N)\n        self.idx_cols = np.random.randint(0, N, N)\n        self.df2 = DataFrame(\n            {\n                c: {\n                    0: np.random.randint(0, 2, N).astype(np.bool_),\n                    1: np.random.randint(0, N, N).astype(np.int16),\n                    2: np.random.randint(0, N, N).astype(np.int32),\n                    3: np.random.randint(0, N, N).astype(np.int64),\n                }[np.random.randint(0, 4)]\n                for c in range(N)\n            }\n        )"", ""frame_methods.Rename.time_rename_axis1"": ""class Rename:\n    def time_rename_axis1(self):\n        self.df.rename(columns=self.dict_idx)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Rename:\n    def setup(self):\n        N = 10**3\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.idx = np.arange(4 * N, 7 * N)\n        self.dict_idx = {k: k for k in self.idx}\n        self.df2 = DataFrame(\n            {\n                c: {\n                    0: np.random.randint(0, 2, N).astype(np.bool_),\n                    1: np.random.randint(0, N, N).astype(np.int16),\n                    2: np.random.randint(0, N, N).astype(np.int32),\n                    3: np.random.randint(0, N, N).astype(np.int64),\n                }[np.random.randint(0, 4)]\n                for c in range(N)\n            }\n        )"", ""frame_methods.SortValues.time_frame_sort_values"": ""class SortValues:\n    def time_frame_sort_values(self, ascending):\n        self.df.sort_values(by=\""A\"", ascending=ascending)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SortValues:\n    def setup(self, ascending):\n        self.df = DataFrame(np.random.randn(1000000, 2), columns=list(\""AB\""))"", ""frame_methods.ToDict.time_to_dict_datetimelike"": ""class ToDict:\n    def time_to_dict_datetimelike(self, orient):\n        self.datetimelike_df.to_dict(orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToDict:\n    def setup(self, orient):\n        data = np.random.randint(0, 1000, size=(10000, 4))\n        self.int_df = DataFrame(data)\n        self.datetimelike_df = self.int_df.astype(\""timedelta64[ns]\"")"", ""frame_methods.ToDict.time_to_dict_ints"": ""class ToDict:\n    def time_to_dict_ints(self, orient):\n        self.int_df.to_dict(orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToDict:\n    def setup(self, orient):\n        data = np.random.randint(0, 1000, size=(10000, 4))\n        self.int_df = DataFrame(data)\n        self.datetimelike_df = self.int_df.astype(\""timedelta64[ns]\"")"", ""frame_methods.ToRecords.time_to_records"": ""class ToRecords:\n    def time_to_records(self):\n        self.df.to_records(index=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToRecords:\n    def setup(self):\n        N = 100_000\n        data = np.random.randn(N, 2)\n        mi = MultiIndex.from_arrays(\n            [\n                np.arange(N),\n                date_range(\""1970-01-01\"", periods=N, freq=\""ms\""),\n            ]\n        )\n        self.df = DataFrame(data)\n        self.df_mi = DataFrame(data, index=mi)""}",frame_methods,time,0.00832063142761071
30016,module-level,"terminus-2,gpt-5",1.2059464036751324,1.1439940383752718,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_object,index_object,"{""index_object.IndexAppend.time_append_int_list"": ""class IndexAppend:\n    def time_append_int_list(self):\n        self.int_idx.append(self.int_idxs)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IndexAppend:\n    def setup(self):\n        N = 10_000\n        self.range_idx = RangeIndex(0, 100)\n        self.int_idx = self.range_idx.astype(int)\n        self.obj_idx = self.int_idx.astype(str)\n        self.range_idxs = []\n        self.int_idxs = []\n        self.object_idxs = []\n        for i in range(1, N):\n            r_idx = RangeIndex(i * 100, (i + 1) * 100)\n            self.range_idxs.append(r_idx)\n            i_idx = r_idx.astype(int)\n            self.int_idxs.append(i_idx)\n            o_idx = i_idx.astype(str)\n            self.object_idxs.append(o_idx)"", ""index_object.IndexAppend.time_append_obj_list"": ""class IndexAppend:\n    def time_append_obj_list(self):\n        self.obj_idx.append(self.object_idxs)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IndexAppend:\n    def setup(self):\n        N = 10_000\n        self.range_idx = RangeIndex(0, 100)\n        self.int_idx = self.range_idx.astype(int)\n        self.obj_idx = self.int_idx.astype(str)\n        self.range_idxs = []\n        self.int_idxs = []\n        self.object_idxs = []\n        for i in range(1, N):\n            r_idx = RangeIndex(i * 100, (i + 1) * 100)\n            self.range_idxs.append(r_idx)\n            i_idx = r_idx.astype(int)\n            self.int_idxs.append(i_idx)\n            o_idx = i_idx.astype(str)\n            self.object_idxs.append(o_idx)"", ""index_object.Indexing.time_boolean_array"": ""class Indexing:\n    def time_boolean_array(self, dtype):\n        self.idx[self.array_mask]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self, dtype):\n        N = 10**6\n        self.idx = getattr(tm, f\""make{dtype}Index\"")(N)\n        self.array_mask = (np.arange(N) % 3) == 0\n        self.series_mask = Series(self.array_mask)\n        self.sorted = self.idx.sort_values()\n        half = N // 2\n        self.non_unique = self.idx[:half].append(self.idx[:half])\n        self.non_unique_sorted = (\n            self.sorted[:half].append(self.sorted[:half]).sort_values()\n        )\n        self.key = self.sorted[N // 4]"", ""index_object.Indexing.time_get_loc"": ""class Indexing:\n    def time_get_loc(self, dtype):\n        self.idx.get_loc(self.key)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self, dtype):\n        N = 10**6\n        self.idx = getattr(tm, f\""make{dtype}Index\"")(N)\n        self.array_mask = (np.arange(N) % 3) == 0\n        self.series_mask = Series(self.array_mask)\n        self.sorted = self.idx.sort_values()\n        half = N // 2\n        self.non_unique = self.idx[:half].append(self.idx[:half])\n        self.non_unique_sorted = (\n            self.sorted[:half].append(self.sorted[:half]).sort_values()\n        )\n        self.key = self.sorted[N // 4]"", ""index_object.Indexing.time_get_loc_non_unique"": ""class Indexing:\n    def time_get_loc_non_unique(self, dtype):\n        self.non_unique.get_loc(self.key)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self, dtype):\n        N = 10**6\n        self.idx = getattr(tm, f\""make{dtype}Index\"")(N)\n        self.array_mask = (np.arange(N) % 3) == 0\n        self.series_mask = Series(self.array_mask)\n        self.sorted = self.idx.sort_values()\n        half = N // 2\n        self.non_unique = self.idx[:half].append(self.idx[:half])\n        self.non_unique_sorted = (\n            self.sorted[:half].append(self.sorted[:half]).sort_values()\n        )\n        self.key = self.sorted[N // 4]"", ""index_object.Indexing.time_get_loc_non_unique_sorted"": ""class Indexing:\n    def time_get_loc_non_unique_sorted(self, dtype):\n        self.non_unique_sorted.get_loc(self.key)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self, dtype):\n        N = 10**6\n        self.idx = getattr(tm, f\""make{dtype}Index\"")(N)\n        self.array_mask = (np.arange(N) % 3) == 0\n        self.series_mask = Series(self.array_mask)\n        self.sorted = self.idx.sort_values()\n        half = N // 2\n        self.non_unique = self.idx[:half].append(self.idx[:half])\n        self.non_unique_sorted = (\n            self.sorted[:half].append(self.sorted[:half]).sort_values()\n        )\n        self.key = self.sorted[N // 4]"", ""index_object.Indexing.time_get_loc_sorted"": ""class Indexing:\n    def time_get_loc_sorted(self, dtype):\n        self.sorted.get_loc(self.key)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self, dtype):\n        N = 10**6\n        self.idx = getattr(tm, f\""make{dtype}Index\"")(N)\n        self.array_mask = (np.arange(N) % 3) == 0\n        self.series_mask = Series(self.array_mask)\n        self.sorted = self.idx.sort_values()\n        half = N // 2\n        self.non_unique = self.idx[:half].append(self.idx[:half])\n        self.non_unique_sorted = (\n            self.sorted[:half].append(self.sorted[:half]).sort_values()\n        )\n        self.key = self.sorted[N // 4]"", ""index_object.Indexing.time_slice"": ""class Indexing:\n    def time_slice(self, dtype):\n        self.idx[:-1]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self, dtype):\n        N = 10**6\n        self.idx = getattr(tm, f\""make{dtype}Index\"")(N)\n        self.array_mask = (np.arange(N) % 3) == 0\n        self.series_mask = Series(self.array_mask)\n        self.sorted = self.idx.sort_values()\n        half = N // 2\n        self.non_unique = self.idx[:half].append(self.idx[:half])\n        self.non_unique_sorted = (\n            self.sorted[:half].append(self.sorted[:half]).sort_values()\n        )\n        self.key = self.sorted[N // 4]"", ""index_object.IntervalIndexMethod.time_intersection"": ""class IntervalIndexMethod:\n    def time_intersection(self, N):\n        self.left.intersection(self.right)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IntervalIndexMethod:\n    def setup(self, N):\n        left = np.append(np.arange(N), np.array(0))\n        right = np.append(np.arange(1, N + 1), np.array(1))\n        self.intv = IntervalIndex.from_arrays(left, right)\n        self.intv._engine\n    \n        self.intv2 = IntervalIndex.from_arrays(left + 1, right + 1)\n        self.intv2._engine\n    \n        self.left = IntervalIndex.from_breaks(np.arange(N))\n        self.right = IntervalIndex.from_breaks(np.arange(N - 3, 2 * N - 3))"", ""index_object.SetOperations.time_operation"": ""class SetOperations:\n    def time_operation(self, index_structure, dtype, method):\n        getattr(self.left, method)(self.right)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SetOperations:\n    def setup(self, index_structure, dtype, method):\n        N = 10**5\n        dates_left = date_range(\""1/1/2000\"", periods=N, freq=\""T\"")\n        fmt = \""%Y-%m-%d %H:%M:%S\""\n        date_str_left = Index(dates_left.strftime(fmt))\n        int_left = Index(np.arange(N))\n        ea_int_left = Index(np.arange(N), dtype=\""Int64\"")\n        str_left = tm.makeStringIndex(N)\n    \n        data = {\n            \""datetime\"": dates_left,\n            \""date_string\"": date_str_left,\n            \""int\"": int_left,\n            \""strings\"": str_left,\n            \""ea_int\"": ea_int_left,\n        }\n    \n        if index_structure == \""non_monotonic\"":\n            data = {k: mi[::-1] for k, mi in data.items()}\n    \n        data = {k: {\""left\"": idx, \""right\"": idx[:-1]} for k, idx in data.items()}\n    \n        self.left = data[dtype][\""left\""]\n        self.right = data[dtype][\""right\""]"", ""index_object.UnionWithDuplicates.time_union_with_duplicates"": ""class UnionWithDuplicates:\n    def time_union_with_duplicates(self):\n        self.left.union(self.right)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass UnionWithDuplicates:\n    def setup(self):\n        self.left = Index(np.repeat(np.arange(1000), 100))\n        self.right = Index(np.tile(np.arange(500, 1500), 50))""}",index_object,time,0.04381355396029745
30014,module-level,"terminus-2,gpt-5",1.1426508338698622,1.060974938794787,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,hash_functions,hash_functions,"{""hash_functions.NumericSeriesIndexing.time_loc_slice"": ""class NumericSeriesIndexing:\n    def time_loc_slice(self, index, N):\n        # trigger building of mapping\n        self.data.loc[:800]\n\n    def setup(self, dtype, N):\n        vals = np.array(list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype)\n        indices = pd.Index(vals)\n        self.data = pd.Series(np.arange(N), index=indices)"", ""hash_functions.NumericSeriesIndexingShuffled.time_loc_slice"": ""class NumericSeriesIndexingShuffled:\n    def time_loc_slice(self, index, N):\n        # trigger building of mapping\n        self.data.loc[:800]\n\n    def setup(self, dtype, N):\n        vals = np.array(list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype)\n        np.random.shuffle(vals)\n        indices = pd.Index(vals)\n        self.data = pd.Series(np.arange(N), index=indices)"", ""hash_functions.Unique.time_unique"": ""class Unique:\n    def time_unique(self, exponent):\n        pd.unique(self.ser_unique)\n\n    def setup(self, dtype):\n        self.ser = pd.Series(([1, pd.NA, 2] + list(range(100_000))) * 3, dtype=dtype)\n        self.ser_unique = pd.Series(list(range(300_000)) + [pd.NA], dtype=dtype)"", ""hash_functions.Unique.time_unique_with_duplicates"": ""class Unique:\n    def time_unique_with_duplicates(self, exponent):\n        pd.unique(self.ser)\n\n    def setup(self, dtype):\n        self.ser = pd.Series(([1, pd.NA, 2] + list(range(100_000))) * 3, dtype=dtype)\n        self.ser_unique = pd.Series(list(range(300_000)) + [pd.NA], dtype=dtype)"", ""hash_functions.UniqueAndFactorizeArange.time_factorize"": ""class UniqueAndFactorizeArange:\n    def time_factorize(self, exponent):\n        pd.factorize(self.a2)\n\n    def setup(self, exponent):\n        a = np.arange(10**4, dtype=\""float64\"")\n        self.a2 = (a + 10**exponent).repeat(100)"", ""hash_functions.UniqueForLargePyObjectInts.time_unique"": ""class UniqueForLargePyObjectInts:\n    def time_unique(self):\n        pd.unique(self.arr)\n\n    def setup(self):\n        lst = [x << 32 for x in range(5000)]\n        self.arr = np.array(lst, dtype=np.object_)""}",hash_functions,time,0.05776230203329216
30017,module-level,"terminus-2,gpt-5",1.0940084645165833,1.1192258607526446,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing,indexing,"{""indexing.CategoricalIndexIndexing.time_get_indexer_list"": ""class CategoricalIndexIndexing:\n    def time_get_indexer_list(self, index):\n        self.data_unique.get_indexer(self.cat_list)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalIndexIndexing:\n    def setup(self, index):\n        N = 10**5\n        values = list(\""a\"" * N + \""b\"" * N + \""c\"" * N)\n        indices = {\n            \""monotonic_incr\"": CategoricalIndex(values),\n            \""monotonic_decr\"": CategoricalIndex(reversed(values)),\n            \""non_monotonic\"": CategoricalIndex(list(\""abc\"" * N)),\n        }\n        self.data = indices[index]\n        self.data_unique = CategoricalIndex([str(i) for i in range(N * 3)])\n    \n        self.int_scalar = 10000\n        self.int_list = list(range(10000))\n    \n        self.cat_scalar = \""b\""\n        self.cat_list = [\""1\"", \""3\""]"", ""indexing.CategoricalIndexIndexing.time_getitem_list"": ""class CategoricalIndexIndexing:\n    def time_getitem_list(self, index):\n        self.data[self.int_list]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalIndexIndexing:\n    def setup(self, index):\n        N = 10**5\n        values = list(\""a\"" * N + \""b\"" * N + \""c\"" * N)\n        indices = {\n            \""monotonic_incr\"": CategoricalIndex(values),\n            \""monotonic_decr\"": CategoricalIndex(reversed(values)),\n            \""non_monotonic\"": CategoricalIndex(list(\""abc\"" * N)),\n        }\n        self.data = indices[index]\n        self.data_unique = CategoricalIndex([str(i) for i in range(N * 3)])\n    \n        self.int_scalar = 10000\n        self.int_list = list(range(10000))\n    \n        self.cat_scalar = \""b\""\n        self.cat_list = [\""1\"", \""3\""]"", ""indexing.CategoricalIndexIndexing.time_getitem_scalar"": ""class CategoricalIndexIndexing:\n    def time_getitem_scalar(self, index):\n        self.data[self.int_scalar]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalIndexIndexing:\n    def setup(self, index):\n        N = 10**5\n        values = list(\""a\"" * N + \""b\"" * N + \""c\"" * N)\n        indices = {\n            \""monotonic_incr\"": CategoricalIndex(values),\n            \""monotonic_decr\"": CategoricalIndex(reversed(values)),\n            \""non_monotonic\"": CategoricalIndex(list(\""abc\"" * N)),\n        }\n        self.data = indices[index]\n        self.data_unique = CategoricalIndex([str(i) for i in range(N * 3)])\n    \n        self.int_scalar = 10000\n        self.int_list = list(range(10000))\n    \n        self.cat_scalar = \""b\""\n        self.cat_list = [\""1\"", \""3\""]"", ""indexing.ChainIndexing.time_chained_indexing"": ""class ChainIndexing:\n    def time_chained_indexing(self, mode):\n        df = self.df\n        N = self.N\n        with warnings.catch_warnings(record=True):\n            with option_context(\""mode.chained_assignment\"", mode):\n                df2 = df[df.A > N // 2]\n                df2[\""C\""] = 1.0\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ChainIndexing:\n    def setup(self, mode):\n        self.N = 1000000\n        self.df = DataFrame({\""A\"": np.arange(self.N), \""B\"": \""foo\""})"", ""indexing.DataFrameNumericIndexing.time_bool_indexer"": ""class DataFrameNumericIndexing:\n    def time_bool_indexer(self, index, index_structure):\n        self.df[self.bool_indexer]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DataFrameNumericIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**5\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.idx_dupe = np.array(range(30)) * 99\n        self.df = DataFrame(np.random.randn(N, 5), index=indices[index_structure])\n        self.df_dup = concat([self.df, 2 * self.df, 3 * self.df])\n        self.bool_indexer = [True] * (N // 2) + [False] * (N - N // 2)"", ""indexing.DataFrameNumericIndexing.time_iloc"": ""class DataFrameNumericIndexing:\n    def time_iloc(self, index, index_structure):\n        self.df.iloc[:100, 0]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DataFrameNumericIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**5\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.idx_dupe = np.array(range(30)) * 99\n        self.df = DataFrame(np.random.randn(N, 5), index=indices[index_structure])\n        self.df_dup = concat([self.df, 2 * self.df, 3 * self.df])\n        self.bool_indexer = [True] * (N // 2) + [False] * (N - N // 2)"", ""indexing.DataFrameNumericIndexing.time_loc_dups"": ""class DataFrameNumericIndexing:\n    def time_loc_dups(self, index, index_structure):\n        self.df_dup.loc[self.idx_dupe]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DataFrameNumericIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**5\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.idx_dupe = np.array(range(30)) * 99\n        self.df = DataFrame(np.random.randn(N, 5), index=indices[index_structure])\n        self.df_dup = concat([self.df, 2 * self.df, 3 * self.df])\n        self.bool_indexer = [True] * (N // 2) + [False] * (N - N // 2)"", ""indexing.DatetimeIndexIndexing.time_get_indexer_mismatched_tz"": ""class DatetimeIndexIndexing:\n    def time_get_indexer_mismatched_tz(self):\n        # reached via e.g.\n        #  ser = Series(range(len(dti)), index=dti)\n        #  ser[dti2]\n        self.dti.get_indexer(self.dti2)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndexIndexing:\n    def setup(self):\n        dti = date_range(\""2016-01-01\"", periods=10000, tz=\""US/Pacific\"")\n        dti2 = dti.tz_convert(\""UTC\"")\n        self.dti = dti\n        self.dti2 = dti2"", ""indexing.IndexSingleRow.time_loc_row"": ""class IndexSingleRow:\n    def time_loc_row(self, unique_cols):\n        self.df.loc[10000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IndexSingleRow:\n    def setup(self, unique_cols):\n        arr = np.arange(10**7).reshape(-1, 10)\n        df = DataFrame(arr)\n        dtypes = [\""u1\"", \""u2\"", \""u4\"", \""u8\"", \""i1\"", \""i2\"", \""i4\"", \""i8\"", \""f8\"", \""f4\""]\n        for i, d in enumerate(dtypes):\n            df[i] = df[i].astype(d)\n    \n        if not unique_cols:\n            # GH#33032 single-row lookups with non-unique columns were\n            #  15x slower than with unique columns\n            df.columns = [\""A\"", \""A\""] + list(df.columns[2:])\n    \n        self.df = df"", ""indexing.InsertColumns.time_assign_list_like_with_setitem"": ""class InsertColumns:\n    def time_assign_list_like_with_setitem(self):\n        self.df[list(range(100))] = np.random.randn(self.N, 100)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass InsertColumns:\n    def setup(self):\n        self.N = 10**3\n        self.df = DataFrame(index=range(self.N))\n        self.df2 = DataFrame(np.random.randn(self.N, 2))"", ""indexing.InsertColumns.time_insert_middle"": ""class InsertColumns:\n    def time_insert_middle(self):\n        # same as time_insert but inserting to a middle column rather than\n        #  front or back (which have fast-paths)\n        for i in range(100):\n            self.df2.insert(\n                1, \""colname\"", np.random.randn(self.N), allow_duplicates=True\n            )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass InsertColumns:\n    def setup(self):\n        self.N = 10**3\n        self.df = DataFrame(index=range(self.N))\n        self.df2 = DataFrame(np.random.randn(self.N, 2))"", ""indexing.MultiIndexing.time_loc_all_null_slices"": ""class MultiIndexing:\n    def time_loc_all_null_slices(self, unique_levels):\n        target = tuple([self.tgt_null_slice] * self.nlevels)\n        self.df.loc[target, :]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MultiIndexing:\n    def setup(self, unique_levels):\n        self.nlevels = 2\n        if unique_levels:\n            mi = MultiIndex.from_arrays([range(1000000)] * self.nlevels)\n        else:\n            mi = MultiIndex.from_product([range(1000)] * self.nlevels)\n        self.df = DataFrame(np.random.randn(len(mi)), index=mi)\n    \n        self.tgt_slice = slice(200, 800)\n        self.tgt_null_slice = slice(None)\n        self.tgt_list = list(range(0, 1000, 10))\n        self.tgt_scalar = 500\n    \n        bool_indexer = np.zeros(len(mi), dtype=np.bool_)\n        bool_indexer[slice(0, len(mi), 100)] = True\n        self.tgt_bool_indexer = bool_indexer"", ""indexing.MultiIndexing.time_loc_all_scalars"": ""class MultiIndexing:\n    def time_loc_all_scalars(self, unique_levels):\n        target = tuple([self.tgt_scalar] * self.nlevels)\n        self.df.loc[target, :]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MultiIndexing:\n    def setup(self, unique_levels):\n        self.nlevels = 2\n        if unique_levels:\n            mi = MultiIndex.from_arrays([range(1000000)] * self.nlevels)\n        else:\n            mi = MultiIndex.from_product([range(1000)] * self.nlevels)\n        self.df = DataFrame(np.random.randn(len(mi)), index=mi)\n    \n        self.tgt_slice = slice(200, 800)\n        self.tgt_null_slice = slice(None)\n        self.tgt_list = list(range(0, 1000, 10))\n        self.tgt_scalar = 500\n    \n        bool_indexer = np.zeros(len(mi), dtype=np.bool_)\n        bool_indexer[slice(0, len(mi), 100)] = True\n        self.tgt_bool_indexer = bool_indexer"", ""indexing.MultiIndexing.time_loc_null_slice_plus_slice"": ""class MultiIndexing:\n    def time_loc_null_slice_plus_slice(self, unique_levels):\n        target = (self.tgt_null_slice, self.tgt_slice)\n        self.df.loc[target, :]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MultiIndexing:\n    def setup(self, unique_levels):\n        self.nlevels = 2\n        if unique_levels:\n            mi = MultiIndex.from_arrays([range(1000000)] * self.nlevels)\n        else:\n            mi = MultiIndex.from_product([range(1000)] * self.nlevels)\n        self.df = DataFrame(np.random.randn(len(mi)), index=mi)\n    \n        self.tgt_slice = slice(200, 800)\n        self.tgt_null_slice = slice(None)\n        self.tgt_list = list(range(0, 1000, 10))\n        self.tgt_scalar = 500\n    \n        bool_indexer = np.zeros(len(mi), dtype=np.bool_)\n        bool_indexer[slice(0, len(mi), 100)] = True\n        self.tgt_bool_indexer = bool_indexer"", ""indexing.MultiIndexing.time_xs_full_key"": ""class MultiIndexing:\n    def time_xs_full_key(self, unique_levels):\n        target = tuple([self.tgt_scalar] * self.nlevels)\n        self.df.xs(target)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MultiIndexing:\n    def setup(self, unique_levels):\n        self.nlevels = 2\n        if unique_levels:\n            mi = MultiIndex.from_arrays([range(1000000)] * self.nlevels)\n        else:\n            mi = MultiIndex.from_product([range(1000)] * self.nlevels)\n        self.df = DataFrame(np.random.randn(len(mi)), index=mi)\n    \n        self.tgt_slice = slice(200, 800)\n        self.tgt_null_slice = slice(None)\n        self.tgt_list = list(range(0, 1000, 10))\n        self.tgt_scalar = 500\n    \n        bool_indexer = np.zeros(len(mi), dtype=np.bool_)\n        bool_indexer[slice(0, len(mi), 100)] = True\n        self.tgt_bool_indexer = bool_indexer"", ""indexing.MultiIndexing.time_xs_level_1"": ""class MultiIndexing:\n    def time_xs_level_1(self, unique_levels):\n        target = self.tgt_scalar\n        self.df.xs(target, level=1)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MultiIndexing:\n    def setup(self, unique_levels):\n        self.nlevels = 2\n        if unique_levels:\n            mi = MultiIndex.from_arrays([range(1000000)] * self.nlevels)\n        else:\n            mi = MultiIndex.from_product([range(1000)] * self.nlevels)\n        self.df = DataFrame(np.random.randn(len(mi)), index=mi)\n    \n        self.tgt_slice = slice(200, 800)\n        self.tgt_null_slice = slice(None)\n        self.tgt_list = list(range(0, 1000, 10))\n        self.tgt_scalar = 500\n    \n        bool_indexer = np.zeros(len(mi), dtype=np.bool_)\n        bool_indexer[slice(0, len(mi), 100)] = True\n        self.tgt_bool_indexer = bool_indexer"", ""indexing.NonNumericSeriesIndexing.time_getitem_label_slice"": ""class NonNumericSeriesIndexing:\n    def time_getitem_label_slice(self, index, index_structure):\n        self.s[: self.lbl]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]"", ""indexing.NonNumericSeriesIndexing.time_getitem_list_like"": ""class NonNumericSeriesIndexing:\n    def time_getitem_list_like(self, index, index_structure):\n        self.s[[self.lbl]]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]"", ""indexing.NonNumericSeriesIndexing.time_getitem_pos_slice"": ""class NonNumericSeriesIndexing:\n    def time_getitem_pos_slice(self, index, index_structure):\n        self.s[:80000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]"", ""indexing.NonNumericSeriesIndexing.time_getitem_scalar"": ""class NonNumericSeriesIndexing:\n    def time_getitem_scalar(self, index, index_structure):\n        self.s[self.lbl]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]"", ""indexing.NumericMaskedIndexing.time_get_indexer"": ""class NumericMaskedIndexing:\n    def time_get_indexer(self, dtype, monotonic):\n        self.data.get_indexer(self.indexer)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericMaskedIndexing:\n    def setup(self, dtype, monotonic):\n        indices = {\n            True: Index(self.monotonic_list, dtype=dtype),\n            False: Index(self.non_monotonic_list, dtype=dtype).append(\n                Index([NA], dtype=dtype)\n            ),\n        }\n        self.data = indices[monotonic]\n        self.indexer = np.arange(300, 1_000)\n        self.data_dups = self.data.append(self.data)"", ""indexing.NumericMaskedIndexing.time_get_indexer_dups"": ""class NumericMaskedIndexing:\n    def time_get_indexer_dups(self, dtype, monotonic):\n        self.data.get_indexer_for(self.indexer)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericMaskedIndexing:\n    def setup(self, dtype, monotonic):\n        indices = {\n            True: Index(self.monotonic_list, dtype=dtype),\n            False: Index(self.non_monotonic_list, dtype=dtype).append(\n                Index([NA], dtype=dtype)\n            ),\n        }\n        self.data = indices[monotonic]\n        self.indexer = np.arange(300, 1_000)\n        self.data_dups = self.data.append(self.data)"", ""indexing.NumericSeriesIndexing.time_getitem_array"": ""class NumericSeriesIndexing:\n    def time_getitem_array(self, index, index_structure):\n        self.data[self.array]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_getitem_list_like"": ""class NumericSeriesIndexing:\n    def time_getitem_list_like(self, index, index_structure):\n        self.data[[800000]]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_getitem_lists"": ""class NumericSeriesIndexing:\n    def time_getitem_lists(self, index, index_structure):\n        self.data[self.array_list]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_getitem_scalar"": ""class NumericSeriesIndexing:\n    def time_getitem_scalar(self, index, index_structure):\n        self.data[800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_getitem_slice"": ""class NumericSeriesIndexing:\n    def time_getitem_slice(self, index, index_structure):\n        self.data[:800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_iloc_slice"": ""class NumericSeriesIndexing:\n    def time_iloc_slice(self, index, index_structure):\n        self.data.iloc[:800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_loc_array"": ""class NumericSeriesIndexing:\n    def time_loc_array(self, index, index_structure):\n        self.data.loc[self.array]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_loc_slice"": ""class NumericSeriesIndexing:\n    def time_loc_slice(self, index, index_structure):\n        self.data.loc[:800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()""}",indexing,time,-0.017834085032575206
30019,module-level,"terminus-2,gpt-5",1.0884658407979235,1.0691858451479093,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.csv,io.csv,"{""io.csv.ReadCSVCategorical.time_convert_post"": ""class ReadCSVCategorical:\n    def time_convert_post(self, engine):\n        read_csv(self.fname, engine=engine).apply(Categorical)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVCategorical:\n    def setup(self, engine):\n        N = 100000\n        group1 = [\""aaaaaaaa\"", \""bbbbbbb\"", \""cccccccc\"", \""dddddddd\"", \""eeeeeeee\""]\n        df = DataFrame(np.random.choice(group1, (N, 3)), columns=list(\""abc\""))\n        df.to_csv(self.fname, index=False)"", ""io.csv.ReadCSVEngine.time_read_bytescsv"": ""class ReadCSVEngine:\n    def time_read_bytescsv(self, engine):\n        read_csv(self.data(self.BytesIO_input), engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVEngine:\n    def setup(self, engine):\n        data = [\""A,B,C,D,E\""] + ([\""1,2,3,4,5\""] * 100000)\n        self.StringIO_input = StringIO(\""\\n\"".join(data))\n        # simulate reading from file\n        self.BytesIO_input = BytesIO(self.StringIO_input.read().encode(\""utf-8\""))"", ""io.csv.ReadCSVEngine.time_read_stringcsv"": ""class ReadCSVEngine:\n    def time_read_stringcsv(self, engine):\n        read_csv(self.data(self.StringIO_input), engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVEngine:\n    def setup(self, engine):\n        data = [\""A,B,C,D,E\""] + ([\""1,2,3,4,5\""] * 100000)\n        self.StringIO_input = StringIO(\""\\n\"".join(data))\n        # simulate reading from file\n        self.BytesIO_input = BytesIO(self.StringIO_input.read().encode(\""utf-8\""))""}",io.csv,time,0.013635074717124654
30018,module-level,"terminus-2,gpt-5",1.069495684056082,1.066962684645545,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing_engines,indexing_engines,"{""indexing_engines.MaskedNumericEngineIndexing.time_get_loc"": ""class MaskedNumericEngineIndexing:\n    def time_get_loc(self, engine_and_dtype, index_type, unique, N):\n        self.data.get_loc(self.key_early)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())[::-1]\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.zeros(N * 3, dtype=dtype.lower())\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype.lower())\n                arr[N:] = np.arange(N * 2, dtype=dtype.lower())\n    \n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n            mask[-1] = True\n    \n        self.data = engine(BaseMaskedArray(arr, mask))\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]"", ""indexing_engines.MaskedNumericEngineIndexing.time_get_loc_near_middle"": ""class MaskedNumericEngineIndexing:\n    def time_get_loc_near_middle(self, engine_and_dtype, index_type, unique, N):\n        # searchsorted performance may be different near the middle of a range\n        #  vs near an endpoint\n        self.data.get_loc(self.key_middle)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())[::-1]\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.zeros(N * 3, dtype=dtype.lower())\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype.lower())\n                arr[N:] = np.arange(N * 2, dtype=dtype.lower())\n    \n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n            mask[-1] = True\n    \n        self.data = engine(BaseMaskedArray(arr, mask))\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]"", ""indexing_engines.NumericEngineIndexing.time_get_loc"": ""class NumericEngineIndexing:\n    def time_get_loc(self, engine_and_dtype, index_type, unique, N):\n        self.data.get_loc(self.key_early)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)[::-1]\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.empty(N * 3, dtype=dtype)\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype)\n                arr[N:] = np.arange(N * 2, dtype=dtype)\n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype)\n    \n        self.data = engine(arr)\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]"", ""indexing_engines.NumericEngineIndexing.time_get_loc_near_middle"": ""class NumericEngineIndexing:\n    def time_get_loc_near_middle(self, engine_and_dtype, index_type, unique, N):\n        # searchsorted performance may be different near the middle of a range\n        #  vs near an endpoint\n        self.data.get_loc(self.key_middle)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)[::-1]\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.empty(N * 3, dtype=dtype)\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype)\n                arr[N:] = np.arange(N * 2, dtype=dtype)\n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype)\n    \n        self.data = engine(arr)\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]"", ""indexing_engines.ObjectEngineIndexing.time_get_loc"": ""class ObjectEngineIndexing:\n    def time_get_loc(self, index_type):\n        self.data.get_loc(\""b\"")\n\n    def setup(self, index_type):\n        N = 10**5\n        values = list(\""a\"" * N + \""b\"" * N + \""c\"" * N)\n        arr = {\n            \""monotonic_incr\"": np.array(values, dtype=object),\n            \""monotonic_decr\"": np.array(list(reversed(values)), dtype=object),\n            \""non_monotonic\"": np.array(list(\""abc\"") * N, dtype=object),\n        }[index_type]\n    \n        self.data = libindex.ObjectEngine(arr)\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(\""b\"")""}",indexing_engines,time,0.0017913715774661922
30022,module-level,"terminus-2,gpt-5",0.9564595661905344,0.9525199004060194,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.parsers,io.parsers,"{""io.parsers.DoesStringLookLikeDatetime.time_check_datetimes"": ""class DoesStringLookLikeDatetime:\n    def time_check_datetimes(self, value):\n        for obj in self.objects:\n            _does_string_look_like_datetime(obj)\n\n    def setup(self, value):\n        self.objects = [value] * 1000000""}",io.parsers,time,0.002786185137563642
30020,module-level,"terminus-2,gpt-5",1.0683914381599744,1.1630325669190456,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.hdf,io.hdf,"{""io.hdf.HDF.time_read_hdf"": ""class HDF:\n    def time_read_hdf(self, format):\n        read_hdf(self.fname, \""df\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass HDF:\n    def setup(self, format):\n        self.fname = \""__test__.h5\""\n        N = 100000\n        C = 5\n        self.df = DataFrame(\n            np.random.randn(N, C),\n            columns=[f\""float{i}\"" for i in range(C)],\n            index=date_range(\""20000101\"", periods=N, freq=\""H\""),\n        )\n        self.df[\""object\""] = tm.makeStringIndex(N)\n        self.df.to_hdf(self.fname, \""df\"", format=format)\n    \n        # Numeric df\n        self.df1 = self.df.copy()\n        self.df1 = self.df1.reset_index()\n        self.df1.to_hdf(self.fname, \""df1\"", format=format)"", ""io.hdf.HDF.time_write_hdf"": ""class HDF:\n    def time_write_hdf(self, format):\n        self.df.to_hdf(self.fname, \""df\"", format=format)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass HDF:\n    def setup(self, format):\n        self.fname = \""__test__.h5\""\n        N = 100000\n        C = 5\n        self.df = DataFrame(\n            np.random.randn(N, C),\n            columns=[f\""float{i}\"" for i in range(C)],\n            index=date_range(\""20000101\"", periods=N, freq=\""H\""),\n        )\n        self.df[\""object\""] = tm.makeStringIndex(N)\n        self.df.to_hdf(self.fname, \""df\"", format=format)\n    \n        # Numeric df\n        self.df1 = self.df.copy()\n        self.df1 = self.df1.reset_index()\n        self.df1.to_hdf(self.fname, \""df1\"", format=format)"", ""io.hdf.HDFStoreDataFrame.time_query_store_table"": ""class HDFStoreDataFrame:\n    def time_query_store_table(self):\n        self.store.select(\""table\"", where=\""index > self.start and index < self.stop\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass HDFStoreDataFrame:\n    def setup(self):\n        N = 25000\n        index = tm.makeStringIndex(N)\n        self.df = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)}, index=index\n        )\n        self.df_mixed = DataFrame(\n            {\n                \""float1\"": np.random.randn(N),\n                \""float2\"": np.random.randn(N),\n                \""string1\"": [\""foo\""] * N,\n                \""bool1\"": [True] * N,\n                \""int1\"": np.random.randint(0, N, size=N),\n            },\n            index=index,\n        )\n        self.df_wide = DataFrame(np.random.randn(N, 100))\n        self.start_wide = self.df_wide.index[10000]\n        self.stop_wide = self.df_wide.index[15000]\n        self.df2 = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)},\n            index=date_range(\""1/1/2000\"", periods=N),\n        )\n        self.start = self.df2.index[10000]\n        self.stop = self.df2.index[15000]\n        self.df_wide2 = DataFrame(\n            np.random.randn(N, 100), index=date_range(\""1/1/2000\"", periods=N)\n        )\n        self.df_dc = DataFrame(\n            np.random.randn(N, 10), columns=[f\""C{i:03d}\"" for i in range(10)]\n        )\n    \n        self.fname = \""__test__.h5\""\n    \n        self.store = HDFStore(self.fname)\n        self.store.put(\""fixed\"", self.df)\n        self.store.put(\""fixed_mixed\"", self.df_mixed)\n        self.store.append(\""table\"", self.df2)\n        self.store.append(\""table_mixed\"", self.df_mixed)\n        self.store.append(\""table_wide\"", self.df_wide)\n        self.store.append(\""table_wide2\"", self.df_wide2)"", ""io.hdf.HDFStoreDataFrame.time_query_store_table_wide"": ""class HDFStoreDataFrame:\n    def time_query_store_table_wide(self):\n        self.store.select(\n            \""table_wide\"", where=\""index > self.start_wide and index < self.stop_wide\""\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass HDFStoreDataFrame:\n    def setup(self):\n        N = 25000\n        index = tm.makeStringIndex(N)\n        self.df = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)}, index=index\n        )\n        self.df_mixed = DataFrame(\n            {\n                \""float1\"": np.random.randn(N),\n                \""float2\"": np.random.randn(N),\n                \""string1\"": [\""foo\""] * N,\n                \""bool1\"": [True] * N,\n                \""int1\"": np.random.randint(0, N, size=N),\n            },\n            index=index,\n        )\n        self.df_wide = DataFrame(np.random.randn(N, 100))\n        self.start_wide = self.df_wide.index[10000]\n        self.stop_wide = self.df_wide.index[15000]\n        self.df2 = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)},\n            index=date_range(\""1/1/2000\"", periods=N),\n        )\n        self.start = self.df2.index[10000]\n        self.stop = self.df2.index[15000]\n        self.df_wide2 = DataFrame(\n            np.random.randn(N, 100), index=date_range(\""1/1/2000\"", periods=N)\n        )\n        self.df_dc = DataFrame(\n            np.random.randn(N, 10), columns=[f\""C{i:03d}\"" for i in range(10)]\n        )\n    \n        self.fname = \""__test__.h5\""\n    \n        self.store = HDFStore(self.fname)\n        self.store.put(\""fixed\"", self.df)\n        self.store.put(\""fixed_mixed\"", self.df_mixed)\n        self.store.append(\""table\"", self.df2)\n        self.store.append(\""table_mixed\"", self.df_mixed)\n        self.store.append(\""table_wide\"", self.df_wide)\n        self.store.append(\""table_wide2\"", self.df_wide2)"", ""io.hdf.HDFStoreDataFrame.time_read_store"": ""class HDFStoreDataFrame:\n    def time_read_store(self):\n        self.store.get(\""fixed\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass HDFStoreDataFrame:\n    def setup(self):\n        N = 25000\n        index = tm.makeStringIndex(N)\n        self.df = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)}, index=index\n        )\n        self.df_mixed = DataFrame(\n            {\n                \""float1\"": np.random.randn(N),\n                \""float2\"": np.random.randn(N),\n                \""string1\"": [\""foo\""] * N,\n                \""bool1\"": [True] * N,\n                \""int1\"": np.random.randint(0, N, size=N),\n            },\n            index=index,\n        )\n        self.df_wide = DataFrame(np.random.randn(N, 100))\n        self.start_wide = self.df_wide.index[10000]\n        self.stop_wide = self.df_wide.index[15000]\n        self.df2 = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)},\n            index=date_range(\""1/1/2000\"", periods=N),\n        )\n        self.start = self.df2.index[10000]\n        self.stop = self.df2.index[15000]\n        self.df_wide2 = DataFrame(\n            np.random.randn(N, 100), index=date_range(\""1/1/2000\"", periods=N)\n        )\n        self.df_dc = DataFrame(\n            np.random.randn(N, 10), columns=[f\""C{i:03d}\"" for i in range(10)]\n        )\n    \n        self.fname = \""__test__.h5\""\n    \n        self.store = HDFStore(self.fname)\n        self.store.put(\""fixed\"", self.df)\n        self.store.put(\""fixed_mixed\"", self.df_mixed)\n        self.store.append(\""table\"", self.df2)\n        self.store.append(\""table_mixed\"", self.df_mixed)\n        self.store.append(\""table_wide\"", self.df_wide)\n        self.store.append(\""table_wide2\"", self.df_wide2)"", ""io.hdf.HDFStoreDataFrame.time_read_store_table"": ""class HDFStoreDataFrame:\n    def time_read_store_table(self):\n        self.store.select(\""table\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass HDFStoreDataFrame:\n    def setup(self):\n        N = 25000\n        index = tm.makeStringIndex(N)\n        self.df = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)}, index=index\n        )\n        self.df_mixed = DataFrame(\n            {\n                \""float1\"": np.random.randn(N),\n                \""float2\"": np.random.randn(N),\n                \""string1\"": [\""foo\""] * N,\n                \""bool1\"": [True] * N,\n                \""int1\"": np.random.randint(0, N, size=N),\n            },\n            index=index,\n        )\n        self.df_wide = DataFrame(np.random.randn(N, 100))\n        self.start_wide = self.df_wide.index[10000]\n        self.stop_wide = self.df_wide.index[15000]\n        self.df2 = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)},\n            index=date_range(\""1/1/2000\"", periods=N),\n        )\n        self.start = self.df2.index[10000]\n        self.stop = self.df2.index[15000]\n        self.df_wide2 = DataFrame(\n            np.random.randn(N, 100), index=date_range(\""1/1/2000\"", periods=N)\n        )\n        self.df_dc = DataFrame(\n            np.random.randn(N, 10), columns=[f\""C{i:03d}\"" for i in range(10)]\n        )\n    \n        self.fname = \""__test__.h5\""\n    \n        self.store = HDFStore(self.fname)\n        self.store.put(\""fixed\"", self.df)\n        self.store.put(\""fixed_mixed\"", self.df_mixed)\n        self.store.append(\""table\"", self.df2)\n        self.store.append(\""table_mixed\"", self.df_mixed)\n        self.store.append(\""table_wide\"", self.df_wide)\n        self.store.append(\""table_wide2\"", self.df_wide2)"", ""io.hdf.HDFStoreDataFrame.time_read_store_table_mixed"": ""class HDFStoreDataFrame:\n    def time_read_store_table_mixed(self):\n        self.store.select(\""table_mixed\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass HDFStoreDataFrame:\n    def setup(self):\n        N = 25000\n        index = tm.makeStringIndex(N)\n        self.df = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)}, index=index\n        )\n        self.df_mixed = DataFrame(\n            {\n                \""float1\"": np.random.randn(N),\n                \""float2\"": np.random.randn(N),\n                \""string1\"": [\""foo\""] * N,\n                \""bool1\"": [True] * N,\n                \""int1\"": np.random.randint(0, N, size=N),\n            },\n            index=index,\n        )\n        self.df_wide = DataFrame(np.random.randn(N, 100))\n        self.start_wide = self.df_wide.index[10000]\n        self.stop_wide = self.df_wide.index[15000]\n        self.df2 = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)},\n            index=date_range(\""1/1/2000\"", periods=N),\n        )\n        self.start = self.df2.index[10000]\n        self.stop = self.df2.index[15000]\n        self.df_wide2 = DataFrame(\n            np.random.randn(N, 100), index=date_range(\""1/1/2000\"", periods=N)\n        )\n        self.df_dc = DataFrame(\n            np.random.randn(N, 10), columns=[f\""C{i:03d}\"" for i in range(10)]\n        )\n    \n        self.fname = \""__test__.h5\""\n    \n        self.store = HDFStore(self.fname)\n        self.store.put(\""fixed\"", self.df)\n        self.store.put(\""fixed_mixed\"", self.df_mixed)\n        self.store.append(\""table\"", self.df2)\n        self.store.append(\""table_mixed\"", self.df_mixed)\n        self.store.append(\""table_wide\"", self.df_wide)\n        self.store.append(\""table_wide2\"", self.df_wide2)"", ""io.hdf.HDFStoreDataFrame.time_read_store_table_wide"": ""class HDFStoreDataFrame:\n    def time_read_store_table_wide(self):\n        self.store.select(\""table_wide\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass HDFStoreDataFrame:\n    def setup(self):\n        N = 25000\n        index = tm.makeStringIndex(N)\n        self.df = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)}, index=index\n        )\n        self.df_mixed = DataFrame(\n            {\n                \""float1\"": np.random.randn(N),\n                \""float2\"": np.random.randn(N),\n                \""string1\"": [\""foo\""] * N,\n                \""bool1\"": [True] * N,\n                \""int1\"": np.random.randint(0, N, size=N),\n            },\n            index=index,\n        )\n        self.df_wide = DataFrame(np.random.randn(N, 100))\n        self.start_wide = self.df_wide.index[10000]\n        self.stop_wide = self.df_wide.index[15000]\n        self.df2 = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)},\n            index=date_range(\""1/1/2000\"", periods=N),\n        )\n        self.start = self.df2.index[10000]\n        self.stop = self.df2.index[15000]\n        self.df_wide2 = DataFrame(\n            np.random.randn(N, 100), index=date_range(\""1/1/2000\"", periods=N)\n        )\n        self.df_dc = DataFrame(\n            np.random.randn(N, 10), columns=[f\""C{i:03d}\"" for i in range(10)]\n        )\n    \n        self.fname = \""__test__.h5\""\n    \n        self.store = HDFStore(self.fname)\n        self.store.put(\""fixed\"", self.df)\n        self.store.put(\""fixed_mixed\"", self.df_mixed)\n        self.store.append(\""table\"", self.df2)\n        self.store.append(\""table_mixed\"", self.df_mixed)\n        self.store.append(\""table_wide\"", self.df_wide)\n        self.store.append(\""table_wide2\"", self.df_wide2)"", ""io.hdf.HDFStoreDataFrame.time_store_info"": ""class HDFStoreDataFrame:\n    def time_store_info(self):\n        self.store.info()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass HDFStoreDataFrame:\n    def setup(self):\n        N = 25000\n        index = tm.makeStringIndex(N)\n        self.df = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)}, index=index\n        )\n        self.df_mixed = DataFrame(\n            {\n                \""float1\"": np.random.randn(N),\n                \""float2\"": np.random.randn(N),\n                \""string1\"": [\""foo\""] * N,\n                \""bool1\"": [True] * N,\n                \""int1\"": np.random.randint(0, N, size=N),\n            },\n            index=index,\n        )\n        self.df_wide = DataFrame(np.random.randn(N, 100))\n        self.start_wide = self.df_wide.index[10000]\n        self.stop_wide = self.df_wide.index[15000]\n        self.df2 = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)},\n            index=date_range(\""1/1/2000\"", periods=N),\n        )\n        self.start = self.df2.index[10000]\n        self.stop = self.df2.index[15000]\n        self.df_wide2 = DataFrame(\n            np.random.randn(N, 100), index=date_range(\""1/1/2000\"", periods=N)\n        )\n        self.df_dc = DataFrame(\n            np.random.randn(N, 10), columns=[f\""C{i:03d}\"" for i in range(10)]\n        )\n    \n        self.fname = \""__test__.h5\""\n    \n        self.store = HDFStore(self.fname)\n        self.store.put(\""fixed\"", self.df)\n        self.store.put(\""fixed_mixed\"", self.df_mixed)\n        self.store.append(\""table\"", self.df2)\n        self.store.append(\""table_mixed\"", self.df_mixed)\n        self.store.append(\""table_wide\"", self.df_wide)\n        self.store.append(\""table_wide2\"", self.df_wide2)"", ""io.hdf.HDFStoreDataFrame.time_store_str"": ""class HDFStoreDataFrame:\n    def time_store_str(self):\n        str(self.store)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass HDFStoreDataFrame:\n    def setup(self):\n        N = 25000\n        index = tm.makeStringIndex(N)\n        self.df = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)}, index=index\n        )\n        self.df_mixed = DataFrame(\n            {\n                \""float1\"": np.random.randn(N),\n                \""float2\"": np.random.randn(N),\n                \""string1\"": [\""foo\""] * N,\n                \""bool1\"": [True] * N,\n                \""int1\"": np.random.randint(0, N, size=N),\n            },\n            index=index,\n        )\n        self.df_wide = DataFrame(np.random.randn(N, 100))\n        self.start_wide = self.df_wide.index[10000]\n        self.stop_wide = self.df_wide.index[15000]\n        self.df2 = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)},\n            index=date_range(\""1/1/2000\"", periods=N),\n        )\n        self.start = self.df2.index[10000]\n        self.stop = self.df2.index[15000]\n        self.df_wide2 = DataFrame(\n            np.random.randn(N, 100), index=date_range(\""1/1/2000\"", periods=N)\n        )\n        self.df_dc = DataFrame(\n            np.random.randn(N, 10), columns=[f\""C{i:03d}\"" for i in range(10)]\n        )\n    \n        self.fname = \""__test__.h5\""\n    \n        self.store = HDFStore(self.fname)\n        self.store.put(\""fixed\"", self.df)\n        self.store.put(\""fixed_mixed\"", self.df_mixed)\n        self.store.append(\""table\"", self.df2)\n        self.store.append(\""table_mixed\"", self.df_mixed)\n        self.store.append(\""table_wide\"", self.df_wide)\n        self.store.append(\""table_wide2\"", self.df_wide2)"", ""io.hdf.HDFStoreDataFrame.time_write_store"": ""class HDFStoreDataFrame:\n    def time_write_store(self):\n        self.store.put(\""fixed_write\"", self.df)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass HDFStoreDataFrame:\n    def setup(self):\n        N = 25000\n        index = tm.makeStringIndex(N)\n        self.df = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)}, index=index\n        )\n        self.df_mixed = DataFrame(\n            {\n                \""float1\"": np.random.randn(N),\n                \""float2\"": np.random.randn(N),\n                \""string1\"": [\""foo\""] * N,\n                \""bool1\"": [True] * N,\n                \""int1\"": np.random.randint(0, N, size=N),\n            },\n            index=index,\n        )\n        self.df_wide = DataFrame(np.random.randn(N, 100))\n        self.start_wide = self.df_wide.index[10000]\n        self.stop_wide = self.df_wide.index[15000]\n        self.df2 = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)},\n            index=date_range(\""1/1/2000\"", periods=N),\n        )\n        self.start = self.df2.index[10000]\n        self.stop = self.df2.index[15000]\n        self.df_wide2 = DataFrame(\n            np.random.randn(N, 100), index=date_range(\""1/1/2000\"", periods=N)\n        )\n        self.df_dc = DataFrame(\n            np.random.randn(N, 10), columns=[f\""C{i:03d}\"" for i in range(10)]\n        )\n    \n        self.fname = \""__test__.h5\""\n    \n        self.store = HDFStore(self.fname)\n        self.store.put(\""fixed\"", self.df)\n        self.store.put(\""fixed_mixed\"", self.df_mixed)\n        self.store.append(\""table\"", self.df2)\n        self.store.append(\""table_mixed\"", self.df_mixed)\n        self.store.append(\""table_wide\"", self.df_wide)\n        self.store.append(\""table_wide2\"", self.df_wide2)""}",io.hdf,time,-0.06693149134304892
30024,module-level,"terminus-2,gpt-5",1.0428598450648927,1.0651493160553005,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,join_merge,join_merge,"{""join_merge.Concat.time_concat_empty_right"": ""class Concat:\n    def time_concat_empty_right(self, axis):\n        concat(self.empty_right, axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Concat:\n    def setup(self, axis):\n        N = 1000\n        s = Series(N, index=tm.makeStringIndex(N))\n        self.series = [s[i:-i] for i in range(1, 10)] * 50\n        self.small_frames = [DataFrame(np.random.randn(5, 4))] * 1000\n        df = DataFrame(\n            {\""A\"": range(N)}, index=date_range(\""20130101\"", periods=N, freq=\""s\"")\n        )\n        self.empty_left = [DataFrame(), df]\n        self.empty_right = [df, DataFrame()]\n        self.mixed_ndims = [df, df.head(N // 2)]"", ""join_merge.ConcatIndexDtype.time_concat_series"": ""class ConcatIndexDtype:\n    def time_concat_series(self, dtype, structure, axis, sort):\n        concat(self.series, axis=axis, sort=sort)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ConcatIndexDtype:\n    def setup(self, dtype, structure, axis, sort):\n        N = 10_000\n        if dtype == \""datetime64[ns]\"":\n            vals = date_range(\""1970-01-01\"", periods=N)\n        elif dtype in (\""int64\"", \""Int64\""):\n            vals = np.arange(N, dtype=np.int64)\n        elif dtype in (\""string[python]\"", \""string[pyarrow]\""):\n            vals = tm.makeStringIndex(N)\n        else:\n            raise NotImplementedError\n    \n        idx = Index(vals, dtype=dtype)\n    \n        if structure == \""monotonic\"":\n            idx = idx.sort_values()\n        elif structure == \""non_monotonic\"":\n            idx = idx[::-1]\n        elif structure == \""has_na\"":\n            if not idx._can_hold_na:\n                raise NotImplementedError\n            idx = Index([None], dtype=dtype).append(idx)\n        else:\n            raise NotImplementedError\n    \n        self.series = [Series(i, idx[:-i]) for i in range(1, 6)]"", ""join_merge.I8Merge.time_i8merge"": ""class I8Merge:\n    def time_i8merge(self, how):\n        merge(self.left, self.right, how=how)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass I8Merge:\n    def setup(self, how):\n        low, high, n = -1000, 1000, 10**6\n        self.left = DataFrame(\n            np.random.randint(low, high, (n, 7)), columns=list(\""ABCDEFG\"")\n        )\n        self.left[\""left\""] = self.left.sum(axis=1)\n        self.right = self.left.sample(frac=1).rename({\""left\"": \""right\""}, axis=1)\n        self.right = self.right.reset_index(drop=True)\n        self.right[\""right\""] *= -1"", ""join_merge.Join.time_join_dataframe_index_single_key_bigger"": ""class Join:\n    def time_join_dataframe_index_single_key_bigger(self, sort):\n        self.df.join(self.df_key2, on=\""key2\"", sort=sort)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Join:\n    def setup(self, sort):\n        level1 = tm.makeStringIndex(10).values\n        level2 = tm.makeStringIndex(1000).values\n        codes1 = np.arange(10).repeat(1000)\n        codes2 = np.tile(np.arange(1000), 10)\n        index2 = MultiIndex(levels=[level1, level2], codes=[codes1, codes2])\n        self.df_multi = DataFrame(\n            np.random.randn(len(index2), 4), index=index2, columns=[\""A\"", \""B\"", \""C\"", \""D\""]\n        )\n    \n        self.key1 = np.tile(level1.take(codes1), 10)\n        self.key2 = np.tile(level2.take(codes2), 10)\n        self.df = DataFrame(\n            {\n                \""data1\"": np.random.randn(100000),\n                \""data2\"": np.random.randn(100000),\n                \""key1\"": self.key1,\n                \""key2\"": self.key2,\n            }\n        )\n    \n        self.df_key1 = DataFrame(\n            np.random.randn(len(level1), 4), index=level1, columns=[\""A\"", \""B\"", \""C\"", \""D\""]\n        )\n        self.df_key2 = DataFrame(\n            np.random.randn(len(level2), 4), index=level2, columns=[\""A\"", \""B\"", \""C\"", \""D\""]\n        )\n    \n        shuf = np.arange(100000)\n        np.random.shuffle(shuf)\n        self.df_shuf = self.df.reindex(self.df.index[shuf])"", ""join_merge.JoinIndex.time_left_outer_join_index"": ""class JoinIndex:\n    def time_left_outer_join_index(self):\n        self.left.join(self.right, on=\""jim\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass JoinIndex:\n    def setup(self):\n        N = 50000\n        self.left = DataFrame(\n            np.random.randint(1, N / 500, (N, 2)), columns=[\""jim\"", \""joe\""]\n        )\n        self.right = DataFrame(\n            np.random.randint(1, N / 500, (N, 2)), columns=[\""jolie\"", \""jolia\""]\n        ).set_index(\""jolie\"")"", ""join_merge.JoinMultiindexSubset.time_join_multiindex_subset"": ""class JoinMultiindexSubset:\n    def time_join_multiindex_subset(self):\n        self.left.join(self.right)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass JoinMultiindexSubset:\n    def setup(self):\n        N = 100_000\n        mi1 = MultiIndex.from_arrays([np.arange(N)] * 4, names=[\""a\"", \""b\"", \""c\"", \""d\""])\n        mi2 = MultiIndex.from_arrays([np.arange(N)] * 2, names=[\""a\"", \""b\""])\n        self.left = DataFrame({\""col1\"": 1}, index=mi1)\n        self.right = DataFrame({\""col2\"": 2}, index=mi2)"", ""join_merge.Merge.time_merge_dataframe_empty_left"": ""class Merge:\n    def time_merge_dataframe_empty_left(self, sort):\n        merge(self.left.iloc[:0], self.right, sort=sort)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Merge:\n    def setup(self, sort):\n        N = 10000\n        indices = tm.makeStringIndex(N).values\n        indices2 = tm.makeStringIndex(N).values\n        key = np.tile(indices[:8000], 10)\n        key2 = np.tile(indices2[:8000], 10)\n        self.left = DataFrame(\n            {\""key\"": key, \""key2\"": key2, \""value\"": np.random.randn(80000)}\n        )\n        self.right = DataFrame(\n            {\n                \""key\"": indices[2000:],\n                \""key2\"": indices2[2000:],\n                \""value2\"": np.random.randn(8000),\n            }\n        )\n    \n        self.df = DataFrame(\n            {\n                \""key1\"": np.tile(np.arange(500).repeat(10), 2),\n                \""key2\"": np.tile(np.arange(250).repeat(10), 4),\n                \""value\"": np.random.randn(10000),\n            }\n        )\n        self.df2 = DataFrame({\""key1\"": np.arange(500), \""value2\"": np.random.randn(500)})\n        self.df3 = self.df[:5000]"", ""join_merge.MergeAsof.time_by_int"": ""class MergeAsof:\n    def time_by_int(self, direction, tolerance):\n        merge_asof(\n            self.df1c,\n            self.df2c,\n            on=\""time\"",\n            by=\""key2\"",\n            direction=direction,\n            tolerance=tolerance,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MergeAsof:\n    def setup(self, direction, tolerance):\n        one_count = 200000\n        two_count = 1000000\n    \n        df1 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, one_count / 20, one_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), one_count),\n                \""key2\"": np.random.randint(0, 25, one_count),\n                \""value1\"": np.random.randn(one_count),\n            }\n        )\n        df2 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, two_count / 20, two_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), two_count),\n                \""key2\"": np.random.randint(0, 25, two_count),\n                \""value2\"": np.random.randn(two_count),\n            }\n        )\n    \n        df1 = df1.sort_values(\""time\"")\n        df2 = df2.sort_values(\""time\"")\n    \n        df1[\""time32\""] = np.int32(df1.time)\n        df2[\""time32\""] = np.int32(df2.time)\n    \n        df1[\""timeu64\""] = np.uint64(df1.time)\n        df2[\""timeu64\""] = np.uint64(df2.time)\n    \n        self.df1a = df1[[\""time\"", \""value1\""]]\n        self.df2a = df2[[\""time\"", \""value2\""]]\n        self.df1b = df1[[\""time\"", \""key\"", \""value1\""]]\n        self.df2b = df2[[\""time\"", \""key\"", \""value2\""]]\n        self.df1c = df1[[\""time\"", \""key2\"", \""value1\""]]\n        self.df2c = df2[[\""time\"", \""key2\"", \""value2\""]]\n        self.df1d = df1[[\""time32\"", \""value1\""]]\n        self.df2d = df2[[\""time32\"", \""value2\""]]\n        self.df1e = df1[[\""time\"", \""key\"", \""key2\"", \""value1\""]]\n        self.df2e = df2[[\""time\"", \""key\"", \""key2\"", \""value2\""]]\n        self.df1f = df1[[\""timeu64\"", \""value1\""]]\n        self.df2f = df2[[\""timeu64\"", \""value2\""]]"", ""join_merge.MergeAsof.time_on_int32"": ""class MergeAsof:\n    def time_on_int32(self, direction, tolerance):\n        merge_asof(\n            self.df1d, self.df2d, on=\""time32\"", direction=direction, tolerance=tolerance\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MergeAsof:\n    def setup(self, direction, tolerance):\n        one_count = 200000\n        two_count = 1000000\n    \n        df1 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, one_count / 20, one_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), one_count),\n                \""key2\"": np.random.randint(0, 25, one_count),\n                \""value1\"": np.random.randn(one_count),\n            }\n        )\n        df2 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, two_count / 20, two_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), two_count),\n                \""key2\"": np.random.randint(0, 25, two_count),\n                \""value2\"": np.random.randn(two_count),\n            }\n        )\n    \n        df1 = df1.sort_values(\""time\"")\n        df2 = df2.sort_values(\""time\"")\n    \n        df1[\""time32\""] = np.int32(df1.time)\n        df2[\""time32\""] = np.int32(df2.time)\n    \n        df1[\""timeu64\""] = np.uint64(df1.time)\n        df2[\""timeu64\""] = np.uint64(df2.time)\n    \n        self.df1a = df1[[\""time\"", \""value1\""]]\n        self.df2a = df2[[\""time\"", \""value2\""]]\n        self.df1b = df1[[\""time\"", \""key\"", \""value1\""]]\n        self.df2b = df2[[\""time\"", \""key\"", \""value2\""]]\n        self.df1c = df1[[\""time\"", \""key2\"", \""value1\""]]\n        self.df2c = df2[[\""time\"", \""key2\"", \""value2\""]]\n        self.df1d = df1[[\""time32\"", \""value1\""]]\n        self.df2d = df2[[\""time32\"", \""value2\""]]\n        self.df1e = df1[[\""time\"", \""key\"", \""key2\"", \""value1\""]]\n        self.df2e = df2[[\""time\"", \""key\"", \""key2\"", \""value2\""]]\n        self.df1f = df1[[\""timeu64\"", \""value1\""]]\n        self.df2f = df2[[\""timeu64\"", \""value2\""]]"", ""join_merge.MergeEA.time_merge"": ""class MergeEA:\n    def time_merge(self, dtype):\n        merge(self.left, self.right)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MergeEA:\n    def setup(self, dtype):\n        N = 10_000\n        indices = np.arange(1, N)\n        key = np.tile(indices[:8000], 10)\n        self.left = DataFrame(\n            {\""key\"": Series(key, dtype=dtype), \""value\"": np.random.randn(80000)}\n        )\n        self.right = DataFrame(\n            {\n                \""key\"": Series(indices[2000:], dtype=dtype),\n                \""value2\"": np.random.randn(7999),\n            }\n        )""}",join_merge,time,-0.015763416542013995
30021,module-level,"terminus-2,gpt-5",1.0336145534111614,1.033572500759336,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.json,io.json,"{""io.json.NormalizeJSON.time_normalize_json"": ""class NormalizeJSON:\n    def time_normalize_json(self, orient, frame):\n        json_normalize(self.data)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NormalizeJSON:\n    def setup(self, orient, frame):\n        data = {\n            \""hello\"": [\""thisisatest\"", 999898, \""mixed types\""],\n            \""nest1\"": {\""nest2\"": {\""nest3\"": \""nest3_value\"", \""nest3_int\"": 3445}},\n            \""nest1_list\"": {\""nest2\"": [\""blah\"", 32423, 546456.876, 92030234]},\n            \""hello2\"": \""string\"",\n        }\n        self.data = [data for i in range(10000)]"", ""io.json.ToJSON.time_to_json"": ""class ToJSON:\n    def time_to_json(self, orient, frame):\n        getattr(self, frame).to_json(self.fname, orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSON:\n    def setup(self, orient, frame):\n        N = 10**5\n        ncols = 5\n        index = date_range(\""20000101\"", periods=N, freq=\""H\"")\n        timedeltas = timedelta_range(start=1, periods=N, freq=\""s\"")\n        datetimes = date_range(start=1, periods=N, freq=\""s\"")\n        ints = np.random.randint(100000000, size=N)\n        longints = sys.maxsize * np.random.randint(100000000, size=N)\n        floats = np.random.randn(N)\n        strings = tm.makeStringIndex(N)\n        self.df = DataFrame(np.random.randn(N, ncols), index=np.arange(N))\n        self.df_date_idx = DataFrame(np.random.randn(N, ncols), index=index)\n        self.df_td_int_ts = DataFrame(\n            {\n                \""td_1\"": timedeltas,\n                \""td_2\"": timedeltas,\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""ts_1\"": datetimes,\n                \""ts_2\"": datetimes,\n            },\n            index=index,\n        )\n        self.df_int_floats = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""int_3\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""float_3\"": floats,\n            },\n            index=index,\n        )\n        self.df_int_float_str = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )\n    \n        self.df_longint_float_str = DataFrame(\n            {\n                \""longint_1\"": longints,\n                \""longint_2\"": longints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )"", ""io.json.ToJSONLines.time_delta_int_tstamp_lines"": ""class ToJSONLines:\n    def time_delta_int_tstamp_lines(self):\n        self.df_td_int_ts.to_json(self.fname, orient=\""records\"", lines=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSONLines:\n    def setup(self):\n        N = 10**5\n        ncols = 5\n        index = date_range(\""20000101\"", periods=N, freq=\""H\"")\n        timedeltas = timedelta_range(start=1, periods=N, freq=\""s\"")\n        datetimes = date_range(start=1, periods=N, freq=\""s\"")\n        ints = np.random.randint(100000000, size=N)\n        longints = sys.maxsize * np.random.randint(100000000, size=N)\n        floats = np.random.randn(N)\n        strings = tm.makeStringIndex(N)\n        self.df = DataFrame(np.random.randn(N, ncols), index=np.arange(N))\n        self.df_date_idx = DataFrame(np.random.randn(N, ncols), index=index)\n        self.df_td_int_ts = DataFrame(\n            {\n                \""td_1\"": timedeltas,\n                \""td_2\"": timedeltas,\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""ts_1\"": datetimes,\n                \""ts_2\"": datetimes,\n            },\n            index=index,\n        )\n        self.df_int_floats = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""int_3\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""float_3\"": floats,\n            },\n            index=index,\n        )\n        self.df_int_float_str = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )\n        self.df_longint_float_str = DataFrame(\n            {\n                \""longint_1\"": longints,\n                \""longint_2\"": longints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )"", ""io.json.ToJSONLines.time_float_int_lines"": ""class ToJSONLines:\n    def time_float_int_lines(self):\n        self.df_int_floats.to_json(self.fname, orient=\""records\"", lines=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSONLines:\n    def setup(self):\n        N = 10**5\n        ncols = 5\n        index = date_range(\""20000101\"", periods=N, freq=\""H\"")\n        timedeltas = timedelta_range(start=1, periods=N, freq=\""s\"")\n        datetimes = date_range(start=1, periods=N, freq=\""s\"")\n        ints = np.random.randint(100000000, size=N)\n        longints = sys.maxsize * np.random.randint(100000000, size=N)\n        floats = np.random.randn(N)\n        strings = tm.makeStringIndex(N)\n        self.df = DataFrame(np.random.randn(N, ncols), index=np.arange(N))\n        self.df_date_idx = DataFrame(np.random.randn(N, ncols), index=index)\n        self.df_td_int_ts = DataFrame(\n            {\n                \""td_1\"": timedeltas,\n                \""td_2\"": timedeltas,\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""ts_1\"": datetimes,\n                \""ts_2\"": datetimes,\n            },\n            index=index,\n        )\n        self.df_int_floats = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""int_3\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""float_3\"": floats,\n            },\n            index=index,\n        )\n        self.df_int_float_str = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )\n        self.df_longint_float_str = DataFrame(\n            {\n                \""longint_1\"": longints,\n                \""longint_2\"": longints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )"", ""io.json.ToJSONLines.time_float_longint_str_lines"": ""class ToJSONLines:\n    def time_float_longint_str_lines(self):\n        self.df_longint_float_str.to_json(self.fname, orient=\""records\"", lines=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSONLines:\n    def setup(self):\n        N = 10**5\n        ncols = 5\n        index = date_range(\""20000101\"", periods=N, freq=\""H\"")\n        timedeltas = timedelta_range(start=1, periods=N, freq=\""s\"")\n        datetimes = date_range(start=1, periods=N, freq=\""s\"")\n        ints = np.random.randint(100000000, size=N)\n        longints = sys.maxsize * np.random.randint(100000000, size=N)\n        floats = np.random.randn(N)\n        strings = tm.makeStringIndex(N)\n        self.df = DataFrame(np.random.randn(N, ncols), index=np.arange(N))\n        self.df_date_idx = DataFrame(np.random.randn(N, ncols), index=index)\n        self.df_td_int_ts = DataFrame(\n            {\n                \""td_1\"": timedeltas,\n                \""td_2\"": timedeltas,\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""ts_1\"": datetimes,\n                \""ts_2\"": datetimes,\n            },\n            index=index,\n        )\n        self.df_int_floats = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""int_3\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""float_3\"": floats,\n            },\n            index=index,\n        )\n        self.df_int_float_str = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )\n        self.df_longint_float_str = DataFrame(\n            {\n                \""longint_1\"": longints,\n                \""longint_2\"": longints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )"", ""io.json.ToJSONWide.time_to_json"": ""class ToJSON:\n    def time_to_json(self, orient, frame):\n        getattr(self, frame).to_json(self.fname, orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSONWide:\n    def setup(self, orient, frame):\n        super().setup(orient, frame)\n        base_df = getattr(self, frame).copy()\n        df_wide = concat([base_df.iloc[:100]] * 1000, ignore_index=True, axis=1)\n        self.df_wide = df_wide"", ""io.json.ToJSONWide.time_to_json_wide"": ""class ToJSONWide:\n    def time_to_json_wide(self, orient, frame):\n        self.df_wide.to_json(self.fname, orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSONWide:\n    def setup(self, orient, frame):\n        super().setup(orient, frame)\n        base_df = getattr(self, frame).copy()\n        df_wide = concat([base_df.iloc[:100]] * 1000, ignore_index=True, axis=1)\n        self.df_wide = df_wide""}",io.json,time,2.9740206382914854e-05
30023,module-level,"terminus-2,gpt-5",1.051607601322199,1.0472118667839048,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.sql,io.sql,"{""io.sql.ReadSQLTableDtypes.time_read_sql_table_column"": ""class ReadSQLTableDtypes:\n    def time_read_sql_table_column(self, dtype):\n        read_sql_table(self.table_name, self.con, columns=[dtype])\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadSQLTableDtypes:\n    def setup(self, dtype):\n        N = 10000\n        self.table_name = \""test\""\n        self.con = create_engine(\""sqlite:///:memory:\"")\n        self.df = DataFrame(\n            {\n                \""float\"": np.random.randn(N),\n                \""float_with_nan\"": np.random.randn(N),\n                \""string\"": [\""foo\""] * N,\n                \""bool\"": [True] * N,\n                \""int\"": np.random.randint(0, N, size=N),\n                \""datetime\"": date_range(\""2000-01-01\"", periods=N, freq=\""s\""),\n            },\n            index=tm.makeStringIndex(N),\n        )\n        self.df.iloc[1000:3000, 1] = np.nan\n        self.df[\""date\""] = self.df[\""datetime\""].dt.date\n        self.df[\""time\""] = self.df[\""datetime\""].dt.time\n        self.df[\""datetime_string\""] = self.df[\""datetime\""].astype(str)\n        self.df.to_sql(self.table_name, self.con, if_exists=\""replace\"")"", ""io.sql.WriteSQLDtypes.time_read_sql_query_select_column"": ""class WriteSQLDtypes:\n    def time_read_sql_query_select_column(self, connection, dtype):\n        read_sql_query(self.query_col, self.con)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass WriteSQLDtypes:\n    def setup(self, connection, dtype):\n        N = 10000\n        con = {\n            \""sqlalchemy\"": create_engine(\""sqlite:///:memory:\""),\n            \""sqlite\"": sqlite3.connect(\"":memory:\""),\n        }\n        self.table_name = \""test_type\""\n        self.query_col = f\""SELECT {dtype} FROM {self.table_name}\""\n        self.con = con[connection]\n        self.df = DataFrame(\n            {\n                \""float\"": np.random.randn(N),\n                \""float_with_nan\"": np.random.randn(N),\n                \""string\"": [\""foo\""] * N,\n                \""bool\"": [True] * N,\n                \""int\"": np.random.randint(0, N, size=N),\n                \""datetime\"": date_range(\""2000-01-01\"", periods=N, freq=\""s\""),\n            },\n            index=tm.makeStringIndex(N),\n        )\n        self.df.iloc[1000:3000, 1] = np.nan\n        self.df[\""date\""] = self.df[\""datetime\""].dt.date\n        self.df[\""time\""] = self.df[\""datetime\""].dt.time\n        self.df[\""datetime_string\""] = self.df[\""datetime\""].astype(str)\n        self.df.to_sql(self.table_name, self.con, if_exists=\""replace\"")""}",io.sql,time,0.003108723152966293
30028,module-level,"terminus-2,gpt-5",1.0476062920110194,1.0574171460879946,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling,rolling,"{""rolling.Apply.time_rolling"": ""class Apply:\n    def time_rolling(self, constructor, window, dtype, function, raw):\n        self.roll.apply(function, raw=raw)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, constructor, window, dtype, function, raw):\n        N = 10**3\n        arr = (100 * np.random.random(N)).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)"", ""rolling.EWMMethods.time_ewm"": ""class EWMMethods:\n    def time_ewm(self, constructor, kwargs_method, dtype):\n        getattr(self.ewm, self.method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass EWMMethods:\n    def setup(self, constructor, kwargs_method, dtype):\n        N = 10**5\n        kwargs, method = kwargs_method\n        arr = (100 * np.random.random(N)).astype(dtype)\n        self.method = method\n        self.ewm = getattr(pd, constructor)(arr).ewm(**kwargs)"", ""rolling.ForwardWindowMethods.time_rolling"": ""class ForwardWindowMethods:\n    def time_rolling(self, constructor, window_size, dtype, method):\n        getattr(self.roll, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ForwardWindowMethods:\n    def setup(self, constructor, window_size, dtype, method):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=window_size)\n        self.roll = getattr(pd, constructor)(arr).rolling(window=indexer)"", ""rolling.Groupby.time_method"": ""class Groupby:\n    def time_method(self, method, window_kwargs):\n        getattr(self.groupby_window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Groupby:\n    def setup(self, method, window_kwargs):\n        N = 1000\n        window, kwargs = window_kwargs\n        df = pd.DataFrame(\n            {\n                \""A\"": [str(i) for i in range(N)] * 10,\n                \""B\"": list(range(N)) * 10,\n            }\n        )\n        if isinstance(kwargs.get(\""window\"", None), str):\n            df.index = pd.date_range(start=\""1900-01-01\"", freq=\""1min\"", periods=N * 10)\n        self.groupby_window = getattr(df.groupby(\""A\""), window)(**kwargs)"", ""rolling.GroupbyEWM.time_groupby_method"": ""class GroupbyEWM:\n    def time_groupby_method(self, method):\n        getattr(self.gb_ewm, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupbyEWM:\n    def setup(self, method):\n        df = pd.DataFrame({\""A\"": range(50), \""B\"": range(50)})\n        self.gb_ewm = df.groupby(\""A\"").ewm(com=1.0)"", ""rolling.GroupbyEWMEngine.time_groupby_mean"": ""class GroupbyEWMEngine:\n    def time_groupby_mean(self, engine):\n        self.gb_ewm.mean(engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupbyEWMEngine:\n    def setup(self, engine):\n        df = pd.DataFrame({\""A\"": range(50), \""B\"": range(50)})\n        self.gb_ewm = df.groupby(\""A\"").ewm(com=1.0)"", ""rolling.GroupbyLargeGroups.time_rolling_multiindex_creation"": ""class GroupbyLargeGroups:\n    def time_rolling_multiindex_creation(self):\n        self.df.groupby(\""A\"").rolling(3).mean()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupbyLargeGroups:\n    def setup(self):\n        N = 100000\n        self.df = pd.DataFrame({\""A\"": [1, 2] * (N // 2), \""B\"": np.random.randn(N)})"", ""rolling.Methods.time_method"": ""class Methods:\n    def time_method(self, constructor, window_kwargs, dtype, method):\n        getattr(self.window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Methods:\n    def setup(self, constructor, window_kwargs, dtype, method):\n        N = 10**5\n        window, kwargs = window_kwargs\n        arr = (100 * np.random.random(N)).astype(dtype)\n        obj = getattr(pd, constructor)(arr)\n        self.window = getattr(obj, window)(**kwargs)"", ""rolling.Pairwise.time_groupby"": ""class Pairwise:\n    def time_groupby(self, kwargs_window, method, pairwise):\n        getattr(self.window_group, method)(self.df, pairwise=pairwise)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Pairwise:\n    def setup(self, kwargs_window, method, pairwise):\n        N = 10**4\n        n_groups = 20\n        kwargs, window = kwargs_window\n        groups = [i for _ in range(N // n_groups) for i in range(n_groups)]\n        arr = np.random.random(N)\n        self.df = pd.DataFrame(arr)\n        self.window = getattr(self.df, window)(**kwargs)\n        self.window_group = getattr(\n            pd.DataFrame({\""A\"": groups, \""B\"": arr}).groupby(\""A\""), window\n        )(**kwargs)"", ""rolling.Pairwise.time_pairwise"": ""class Pairwise:\n    def time_pairwise(self, kwargs_window, method, pairwise):\n        getattr(self.window, method)(self.df, pairwise=pairwise)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Pairwise:\n    def setup(self, kwargs_window, method, pairwise):\n        N = 10**4\n        n_groups = 20\n        kwargs, window = kwargs_window\n        groups = [i for _ in range(N // n_groups) for i in range(n_groups)]\n        arr = np.random.random(N)\n        self.df = pd.DataFrame(arr)\n        self.window = getattr(self.df, window)(**kwargs)\n        self.window_group = getattr(\n            pd.DataFrame({\""A\"": groups, \""B\"": arr}).groupby(\""A\""), window\n        )(**kwargs)"", ""rolling.Quantile.time_quantile"": ""class Quantile:\n    def time_quantile(self, constructor, window, dtype, percentile, interpolation):\n        self.roll.quantile(percentile, interpolation=interpolation)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, constructor, window, dtype, percentile, interpolation):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)"", ""rolling.Rank.time_rank"": ""class Rank:\n    def time_rank(self, constructor, window, dtype, percentile, ascending, method):\n        self.roll.rank(pct=percentile, ascending=ascending, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Rank:\n    def setup(self, constructor, window, dtype, percentile, ascending, method):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)"", ""rolling.TableMethod.time_apply"": ""class TableMethod:\n    def time_apply(self, method):\n        self.df.rolling(2, method=method).apply(\n            table_method_func, raw=True, engine=\""numba\""\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TableMethod:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(10, 1000))"", ""rolling.VariableWindowMethods.time_method"": ""class Methods:\n    def time_method(self, constructor, window_kwargs, dtype, method):\n        getattr(self.window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass VariableWindowMethods:\n    def setup(self, constructor, window, dtype, method):\n        N = 10**5\n        arr = (100 * np.random.random(N)).astype(dtype)\n        index = pd.date_range(\""2017-01-01\"", periods=N, freq=\""5s\"")\n        self.window = getattr(pd, constructor)(arr, index=index).rolling(window)""}",rolling,time,-0.006938369219925912
30025,module-level,"terminus-2,gpt-5",0.9960594293284988,1.0420834684650648,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,libs,libs,"{""libs.CacheReadonly.time_cache_readonly"": ""class CacheReadonly:\n    def time_cache_readonly(self):\n        self.obj.prop\n\n    def setup(self):\n        class Foo:\n            @cache_readonly\n            def prop(self):\n                return 5\n    \n        self.obj = Foo()"", ""libs.ScalarListLike.time_is_list_like"": ""class ScalarListLike:\n    def time_is_list_like(self, param):\n        is_list_like(param)"", ""libs.ScalarListLike.time_is_scalar"": ""class ScalarListLike:\n    def time_is_scalar(self, param):\n        is_scalar(param)""}",libs,time,-0.03254882541482748
30033,module-level,"terminus-2,gpt-5",1.0470243933673409,1.0916650189248993,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings,strings,"{""strings.Cat.time_cat"": ""class Cat:\n    def time_cat(self, other_cols, sep, na_rep, na_frac):\n        # before the concatenation (one caller + other_cols columns), the total\n        # expected fraction of rows containing any NaN is:\n        # reduce(lambda t, _: t + (1 - t) * na_frac, range(other_cols + 1), 0)\n        # for other_cols=3 and na_frac=0.15, this works out to ~48%\n        self.s.str.cat(others=self.others, sep=sep, na_rep=na_rep)\n\n    def setup(self, other_cols, sep, na_rep, na_frac):\n        N = 10**5\n        mask_gen = lambda: np.random.choice([True, False], N, p=[1 - na_frac, na_frac])\n        self.s = Series(tm.makeStringIndex(N)).where(mask_gen())\n        if other_cols == 0:\n            # str.cat self-concatenates only for others=None\n            self.others = None\n        else:\n            self.others = DataFrame(\n                {i: tm.makeStringIndex(N).where(mask_gen()) for i in range(other_cols)}\n            )"", ""strings.Contains.time_contains"": ""class Contains:\n    def time_contains(self, dtype, regex):\n        self.s.str.contains(\""A\"", regex=regex)\n\n    def setup(self, dtype, regex):\n        super().setup(dtype)"", ""strings.Extract.time_extract_single_group"": ""class Extract:\n    def time_extract_single_group(self, dtype, expand):\n        with warnings.catch_warnings(record=True):\n            self.s.str.extract(\""(\\\\w*)A\"", expand=expand)\n\n    def setup(self, dtype, expand):\n        super().setup(dtype)"", ""strings.Methods.time_endswith"": ""class Methods:\n    def time_endswith(self, dtype):\n        self.s.str.endswith(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_find"": ""class Methods:\n    def time_find(self, dtype):\n        self.s.str.find(\""[A-Z]+\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_fullmatch"": ""class Methods:\n    def time_fullmatch(self, dtype):\n        self.s.str.fullmatch(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_get"": ""class Methods:\n    def time_get(self, dtype):\n        self.s.str.get(0)\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_isdecimal"": ""class Methods:\n    def time_isdecimal(self, dtype):\n        self.s.str.isdecimal()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_isdigit"": ""class Methods:\n    def time_isdigit(self, dtype):\n        self.s.str.isdigit()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_join"": ""class Methods:\n    def time_join(self, dtype):\n        self.s.str.join(\"" \"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_lower"": ""class Methods:\n    def time_lower(self, dtype):\n        self.s.str.lower()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_partition"": ""class Methods:\n    def time_partition(self, dtype):\n        self.s.str.partition(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_rfind"": ""class Methods:\n    def time_rfind(self, dtype):\n        self.s.str.rfind(\""[A-Z]+\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_slice"": ""class Methods:\n    def time_slice(self, dtype):\n        self.s.str.slice(5, 15, 2)\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_startswith"": ""class Methods:\n    def time_startswith(self, dtype):\n        self.s.str.startswith(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_upper"": ""class Methods:\n    def time_upper(self, dtype):\n        self.s.str.upper()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_wrap"": ""class Methods:\n    def time_wrap(self, dtype):\n        self.s.str.wrap(10)\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_zfill"": ""class Methods:\n    def time_zfill(self, dtype):\n        self.s.str.zfill(10)\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Repeat.time_repeat"": ""class Repeat:\n    def time_repeat(self, repeats):\n        self.s.str.repeat(self.values)\n\n    def setup(self, repeats):\n        N = 10**5\n        self.s = Series(tm.makeStringIndex(N))\n        repeat = {\""int\"": 1, \""array\"": np.random.randint(1, 3, N)}\n        self.values = repeat[repeats]"", ""strings.Split.time_rsplit"": ""class Split:\n    def time_rsplit(self, dtype, expand):\n        self.s.str.rsplit(\""--\"", expand=expand)\n\n    def setup(self, dtype, expand):\n        super().setup(dtype)\n        self.s = self.s.str.join(\""--\"")"", ""strings.Split.time_split"": ""class Split:\n    def time_split(self, dtype, expand):\n        self.s.str.split(\""--\"", expand=expand)\n\n    def setup(self, dtype, expand):\n        super().setup(dtype)\n        self.s = self.s.str.join(\""--\"")""}",strings,time,-0.03157045654707103
30029,module-level,"terminus-2,gpt-5",1.058174615424848,1.061724355807378,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods,series_methods,"{""series_methods.Any.time_any"": ""class Any:\n    def time_any(self, N, case, dtype):\n        self.s.any()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Any:\n    def setup(self, N, case, dtype):\n        val = case == \""fast\""\n        self.s = Series([val] * N, dtype=dtype)"", ""series_methods.Clip.time_clip"": ""class Clip:\n    def time_clip(self, n):\n        self.s.clip(0, 1)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Clip:\n    def setup(self, n):\n        self.s = Series(np.random.randn(n))"", ""series_methods.Dropna.time_dropna"": ""class Dropna:\n    def time_dropna(self, dtype):\n        self.s.dropna()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Dropna:\n    def setup(self, dtype):\n        N = 10**6\n        data = {\n            \""int\"": np.random.randint(1, 10, N),\n            \""datetime\"": date_range(\""2000-01-01\"", freq=\""S\"", periods=N),\n        }\n        self.s = Series(data[dtype])\n        if dtype == \""datetime\"":\n            self.s[np.random.randint(1, N, 100)] = NaT"", ""series_methods.Fillna.time_fillna"": ""class Fillna:\n    def time_fillna(self, dtype, method):\n        value = self.fill_value if method is None else None\n        self.ser.fillna(value=value, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Fillna:\n    def setup(self, dtype, method):\n        N = 10**6\n        if dtype == \""datetime64[ns]\"":\n            data = date_range(\""2000-01-01\"", freq=\""S\"", periods=N)\n            na_value = NaT\n        elif dtype in (\""float64\"", \""Float64\""):\n            data = np.random.randn(N)\n            na_value = np.nan\n        elif dtype in (\""Int64\"", \""int64[pyarrow]\""):\n            data = np.arange(N)\n            na_value = NA\n        elif dtype in (\""string\"", \""string[pyarrow]\""):\n            data = tm.rands_array(5, N)\n            na_value = NA\n        else:\n            raise NotImplementedError\n        fill_value = data[0]\n        ser = Series(data, dtype=dtype)\n        ser[::2] = na_value\n        self.ser = ser\n        self.fill_value = fill_value"", ""series_methods.Iter.time_iter"": ""class Iter:\n    def time_iter(self, dtype):\n        for v in self.s:\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iter:\n    def setup(self, dtype):\n        N = 10**5\n        if dtype in [\""bool\"", \""boolean\""]:\n            data = np.repeat([True, False], N // 2)\n        elif dtype in [\""int64\"", \""Int64\""]:\n            data = np.arange(N)\n        elif dtype in [\""float64\"", \""Float64\""]:\n            data = np.random.randn(N)\n        elif dtype == \""datetime64[ns]\"":\n            data = date_range(\""2000-01-01\"", freq=\""s\"", periods=N)\n        else:\n            raise NotImplementedError\n    \n        self.s = Series(data, dtype=dtype)"", ""series_methods.Map.time_map"": ""class Map:\n    def time_map(self, mapper, *args, **kwargs):\n        self.s.map(self.map_data)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Map:\n    def setup(self, mapper, dtype):\n        map_size = 1000\n        map_data = Series(map_size - np.arange(map_size), dtype=dtype)\n    \n        # construct mapper\n        if mapper == \""Series\"":\n            self.map_data = map_data\n        elif mapper == \""dict\"":\n            self.map_data = map_data.to_dict()\n        elif mapper == \""lambda\"":\n            map_dict = map_data.to_dict()\n            self.map_data = lambda x: map_dict[x]\n        else:\n            raise NotImplementedError\n    \n        self.s = Series(np.random.randint(0, map_size, 10000), dtype=dtype)"", ""series_methods.Mode.time_mode"": ""class Mode:\n    def time_mode(self, N, dtype):\n        self.s.mode()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Mode:\n    def setup(self, N, dtype):\n        self.s = Series(np.random.randint(0, N, size=10 * N)).astype(dtype)"", ""series_methods.ModeObjectDropNAFalse.time_mode"": ""class ModeObjectDropNAFalse:\n    def time_mode(self, N):\n        self.s.mode(dropna=False)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ModeObjectDropNAFalse:\n    def setup(self, N):\n        self.s = Series(np.random.randint(0, N, size=10 * N)).astype(\""object\"")"", ""series_methods.NSort.time_nlargest"": ""class NSort:\n    def time_nlargest(self, keep):\n        self.s.nlargest(3, keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NSort:\n    def setup(self, keep):\n        self.s = Series(np.random.randint(1, 10, 100000))"", ""series_methods.NSort.time_nsmallest"": ""class NSort:\n    def time_nsmallest(self, keep):\n        self.s.nsmallest(3, keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NSort:\n    def setup(self, keep):\n        self.s = Series(np.random.randint(1, 10, 100000))"", ""series_methods.NanOps.time_func"": ""class NanOps:\n    def time_func(self, func, N, dtype):\n        self.func()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NanOps:\n    def setup(self, func, N, dtype):\n        if func == \""argmax\"" and dtype in {\""Int64\"", \""boolean\""}:\n            # Skip argmax for nullable int since this doesn't work yet (GH-24382)\n            raise NotImplementedError\n        self.s = Series([1] * N, dtype=dtype)\n        self.func = getattr(self.s, func)"", ""series_methods.SearchSorted.time_searchsorted"": ""class SearchSorted:\n    def time_searchsorted(self, dtype):\n        key = \""2\"" if dtype == \""str\"" else 2\n        self.s.searchsorted(key)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SearchSorted:\n    def setup(self, dtype):\n        N = 10**5\n        data = np.array([1] * N + [2] * N + [3] * N).astype(dtype)\n        self.s = Series(data)"", ""series_methods.SeriesConstructor.time_constructor_dict"": ""class SeriesConstructor:\n    def time_constructor_dict(self):\n        Series(data=self.data, index=self.idx)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesConstructor:\n    def setup(self):\n        self.idx = date_range(\n            start=datetime(2015, 10, 26), end=datetime(2016, 1, 1), freq=\""50s\""\n        )\n        self.data = dict(zip(self.idx, range(len(self.idx))))\n        self.array = np.array([1, 2, 3])\n        self.idx2 = Index([\""a\"", \""b\"", \""c\""])"", ""series_methods.ToFrame.time_to_frame"": ""class ToFrame:\n    def time_to_frame(self, dtype, name):\n        self.ser.to_frame(name)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToFrame:\n    def setup(self, dtype, name):\n        arr = np.arange(10**5)\n        ser = Series(arr, dtype=dtype)\n        self.ser = ser"", ""series_methods.ValueCounts.time_value_counts"": ""class ValueCounts:\n    def time_value_counts(self, N, dtype):\n        self.s.value_counts()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ValueCounts:\n    def setup(self, N, dtype):\n        self.s = Series(np.random.randint(0, N, size=10 * N)).astype(dtype)""}",series_methods,time,-0.0025104245986774937
30027,module-level,"terminus-2,gpt-5",1.0877892468349726,1.1125787124354924,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape,reshape,"{""reshape.Crosstab.time_crosstab_normalize"": ""class Crosstab:\n    def time_crosstab_normalize(self):\n        pd.crosstab(self.vec1, self.vec2, normalize=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Crosstab:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        self.ind1 = np.random.randint(0, 3, size=N)\n        self.ind2 = np.random.randint(0, 2, size=N)\n        self.vec1 = fac1.take(self.ind1)\n        self.vec2 = fac2.take(self.ind2)"", ""reshape.Crosstab.time_crosstab_values"": ""class Crosstab:\n    def time_crosstab_values(self):\n        pd.crosstab(self.vec1, self.vec2, values=self.ind1, aggfunc=\""sum\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Crosstab:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        self.ind1 = np.random.randint(0, 3, size=N)\n        self.ind2 = np.random.randint(0, 2, size=N)\n        self.vec1 = fac1.take(self.ind1)\n        self.vec2 = fac2.take(self.ind2)"", ""reshape.Cut.time_cut_datetime"": ""class Cut:\n    def time_cut_datetime(self, bins):\n        pd.cut(self.datetime_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Cut.time_cut_float"": ""class Cut:\n    def time_cut_float(self, bins):\n        pd.cut(self.float_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Cut.time_cut_int"": ""class Cut:\n    def time_cut_int(self, bins):\n        pd.cut(self.int_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Cut.time_cut_timedelta"": ""class Cut:\n    def time_cut_timedelta(self, bins):\n        pd.cut(self.timedelta_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Cut.time_qcut_datetime"": ""class Cut:\n    def time_qcut_datetime(self, bins):\n        pd.qcut(self.datetime_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Cut.time_qcut_float"": ""class Cut:\n    def time_qcut_float(self, bins):\n        pd.qcut(self.float_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Cut.time_qcut_timedelta"": ""class Cut:\n    def time_qcut_timedelta(self, bins):\n        pd.qcut(self.timedelta_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Explode.time_explode"": ""class Explode:\n    def time_explode(self, n_rows, max_list_length):\n        self.series.explode()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Explode:\n    def setup(self, n_rows, max_list_length):\n        data = [np.arange(np.random.randint(max_list_length)) for _ in range(n_rows)]\n        self.series = pd.Series(data)"", ""reshape.GetDummies.time_get_dummies_1d_sparse"": ""class GetDummies:\n    def time_get_dummies_1d_sparse(self):\n        pd.get_dummies(self.s, sparse=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GetDummies:\n    def setup(self):\n        categories = list(string.ascii_letters[:12])\n        s = pd.Series(\n            np.random.choice(categories, size=1000000),\n            dtype=CategoricalDtype(categories),\n        )\n        self.s = s"", ""reshape.Melt.time_melt_dataframe"": ""class Melt:\n    def time_melt_dataframe(self, dtype):\n        melt(self.df, id_vars=[\""id1\"", \""id2\""])\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Melt:\n    def setup(self, dtype):\n        self.df = DataFrame(\n            np.random.randn(100_000, 3), columns=[\""A\"", \""B\"", \""C\""], dtype=dtype\n        )\n        self.df[\""id1\""] = pd.Series(np.random.randint(0, 10, 10000))\n        self.df[\""id2\""] = pd.Series(np.random.randint(100, 1000, 10000))"", ""reshape.Pivot.time_reshape_pivot_time_series"": ""class Pivot:\n    def time_reshape_pivot_time_series(self):\n        self.df.pivot(index=\""date\"", columns=\""variable\"", values=\""value\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Pivot:\n    def setup(self):\n        N = 10000\n        index = date_range(\""1/1/2000\"", periods=N, freq=\""h\"")\n        data = {\n            \""value\"": np.random.randn(N * 50),\n            \""variable\"": np.arange(50).repeat(N),\n            \""date\"": np.tile(index.values, 50),\n        }\n        self.df = DataFrame(data)"", ""reshape.PivotTable.time_pivot_table"": ""class PivotTable:\n    def time_pivot_table(self):\n        self.df.pivot_table(index=\""key1\"", columns=[\""key2\"", \""key3\""])\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")"", ""reshape.PivotTable.time_pivot_table_agg"": ""class PivotTable:\n    def time_pivot_table_agg(self):\n        self.df.pivot_table(\n            index=\""key1\"", columns=[\""key2\"", \""key3\""], aggfunc=[\""sum\"", \""mean\""]\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")"", ""reshape.PivotTable.time_pivot_table_categorical"": ""class PivotTable:\n    def time_pivot_table_categorical(self):\n        self.df2.pivot_table(\n            index=\""col1\"", values=\""col3\"", columns=\""col2\"", aggfunc=np.sum, fill_value=0\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")"", ""reshape.PivotTable.time_pivot_table_categorical_observed"": ""class PivotTable:\n    def time_pivot_table_categorical_observed(self):\n        self.df2.pivot_table(\n            index=\""col1\"",\n            values=\""col3\"",\n            columns=\""col2\"",\n            aggfunc=np.sum,\n            fill_value=0,\n            observed=True,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")"", ""reshape.PivotTable.time_pivot_table_margins"": ""class PivotTable:\n    def time_pivot_table_margins(self):\n        self.df.pivot_table(index=\""key1\"", columns=[\""key2\"", \""key3\""], margins=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")"", ""reshape.PivotTable.time_pivot_table_margins_only_column"": ""class PivotTable:\n    def time_pivot_table_margins_only_column(self):\n        self.df.pivot_table(columns=[\""key1\"", \""key2\"", \""key3\""], margins=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")"", ""reshape.ReshapeExtensionDtype.time_stack"": ""class ReshapeExtensionDtype:\n    def time_stack(self, dtype):\n        self.df.stack()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReshapeExtensionDtype:\n    def setup(self, dtype):\n        lev = pd.Index(list(\""ABCDEFGHIJ\""))\n        ri = pd.Index(range(1000))\n        mi = MultiIndex.from_product([lev, ri], names=[\""foo\"", \""bar\""])\n    \n        index = date_range(\""2016-01-01\"", periods=10000, freq=\""s\"", tz=\""US/Pacific\"")\n        if dtype == \""Period[s]\"":\n            index = index.tz_localize(None).to_period(\""s\"")\n    \n        ser = pd.Series(index, index=mi)\n        df = ser.unstack(\""bar\"")\n        # roundtrips -> df.stack().equals(ser)\n    \n        self.ser = ser\n        self.df = df"", ""reshape.ReshapeExtensionDtype.time_transpose"": ""class ReshapeExtensionDtype:\n    def time_transpose(self, dtype):\n        self.df.T\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReshapeExtensionDtype:\n    def setup(self, dtype):\n        lev = pd.Index(list(\""ABCDEFGHIJ\""))\n        ri = pd.Index(range(1000))\n        mi = MultiIndex.from_product([lev, ri], names=[\""foo\"", \""bar\""])\n    \n        index = date_range(\""2016-01-01\"", periods=10000, freq=\""s\"", tz=\""US/Pacific\"")\n        if dtype == \""Period[s]\"":\n            index = index.tz_localize(None).to_period(\""s\"")\n    \n        ser = pd.Series(index, index=mi)\n        df = ser.unstack(\""bar\"")\n        # roundtrips -> df.stack().equals(ser)\n    \n        self.ser = ser\n        self.df = df"", ""reshape.SimpleReshape.time_stack"": ""class SimpleReshape:\n    def time_stack(self):\n        self.udf.stack()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SimpleReshape:\n    def setup(self):\n        arrays = [np.arange(100).repeat(100), np.roll(np.tile(np.arange(100), 100), 25)]\n        index = MultiIndex.from_arrays(arrays)\n        self.df = DataFrame(np.random.randn(10000, 4), index=index)\n        self.udf = self.df.unstack(1)"", ""reshape.SimpleReshape.time_unstack"": ""class SimpleReshape:\n    def time_unstack(self):\n        self.df.unstack(1)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SimpleReshape:\n    def setup(self):\n        arrays = [np.arange(100).repeat(100), np.roll(np.tile(np.arange(100), 100), 25)]\n        index = MultiIndex.from_arrays(arrays)\n        self.df = DataFrame(np.random.randn(10000, 4), index=index)\n        self.udf = self.df.unstack(1)"", ""reshape.SparseIndex.time_unstack"": ""class SparseIndex:\n    def time_unstack(self):\n        self.df.unstack()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SparseIndex:\n    def setup(self):\n        NUM_ROWS = 1000\n        self.df = DataFrame(\n            {\n                \""A\"": np.random.randint(50, size=NUM_ROWS),\n                \""B\"": np.random.randint(50, size=NUM_ROWS),\n                \""C\"": np.random.randint(-10, 10, size=NUM_ROWS),\n                \""D\"": np.random.randint(-10, 10, size=NUM_ROWS),\n                \""E\"": np.random.randint(10, size=NUM_ROWS),\n                \""F\"": np.random.randn(NUM_ROWS),\n            }\n        )\n        self.df = self.df.set_index([\""A\"", \""B\"", \""C\"", \""D\"", \""E\""])"", ""reshape.Unstack.time_full_product"": ""class Unstack:\n    def time_full_product(self, dtype):\n        self.df.unstack()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Unstack:\n    def setup(self, dtype):\n        m = 100\n        n = 1000\n    \n        levels = np.arange(m)\n        index = MultiIndex.from_product([levels] * 2)\n        columns = np.arange(n)\n        if dtype == \""int\"":\n            values = np.arange(m * m * n).reshape(m * m, n)\n            self.df = DataFrame(values, index, columns)\n        else:\n            # the category branch is ~20x slower than int. So we\n            # cut down the size a bit. Now it's only ~3x slower.\n            n = 50\n            columns = columns[:n]\n            indices = np.random.randint(0, 52, size=(m * m, n))\n            values = np.take(list(string.ascii_letters), indices)\n            values = [pd.Categorical(v) for v in values.T]\n    \n            self.df = DataFrame(dict(enumerate(values)), index, columns)\n    \n        self.df2 = self.df.iloc[:-1]"", ""reshape.WideToLong.time_wide_to_long_big"": ""class WideToLong:\n    def time_wide_to_long_big(self):\n        wide_to_long(self.df, self.letters, i=\""id\"", j=\""year\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass WideToLong:\n    def setup(self):\n        nyrs = 20\n        nidvars = 20\n        N = 5000\n        self.letters = list(\""ABCD\"")\n        yrvars = [\n            letter + str(num)\n            for letter, num in product(self.letters, range(1, nyrs + 1))\n        ]\n        columns = [str(i) for i in range(nidvars)] + yrvars\n        self.df = DataFrame(np.random.randn(N, nidvars + len(yrvars)), columns=columns)\n        self.df[\""id\""] = self.df.index""}",reshape,time,-0.01753144667646384
30032,module-level,"terminus-2,gpt-5",1.0245411181968536,1.0304621123323296,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,strftime,strftime,"{""strftime.BusinessHourStrftime.time_frame_offset_repr"": ""class BusinessHourStrftime:\n    def time_frame_offset_repr(self, obs):\n        self.data[\""off\""].apply(repr)\n\n    def setup(self, obs):\n        self.data = pd.DataFrame(\n            {\n                \""off\"": [offsets.BusinessHour()] * obs,\n            }\n        )"", ""strftime.BusinessHourStrftime.time_frame_offset_str"": ""class BusinessHourStrftime:\n    def time_frame_offset_str(self, obs):\n        self.data[\""off\""].apply(str)\n\n    def setup(self, obs):\n        self.data = pd.DataFrame(\n            {\n                \""off\"": [offsets.BusinessHour()] * obs,\n            }\n        )""}",strftime,time,-0.004187407450831745
30039,module-level,"terminus-2,gpt-5",1.0075868599150823,1.0124123880355616,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.resolution,tslibs.resolution,"{""tslibs.resolution.TimeResolution.time_get_resolution"": ""class TimeResolution:\n    def time_get_resolution(self, unit, size, tz):\n        get_resolution(self.i8data, tz)\n\n    def setup(self, unit, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        arr = arr.view(f\""M8[{unit}]\"").astype(\""M8[ns]\"").view(\""i8\"")\n        self.i8data = arr""}",tslibs.resolution,time,-0.003412679010239891
30036,module-level,"terminus-2,gpt-5",1.0027272859251488,0.9623454744132488,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.normalize,tslibs.normalize,"{""tslibs.normalize.Normalize.time_is_date_array_normalized"": ""class Normalize:\n    def time_is_date_array_normalized(self, size, tz):\n        # TODO: cases with different levels of short-circuiting\n        # 10 i.e. NPY_FR_ns\n        is_date_array_normalized(self.i8data, tz, 10)\n\n    def setup(self, size, tz):\n        # use an array that will have is_date_array_normalized give True,\n        #  so we do not short-circuit early.\n        dti = pd.date_range(\""2016-01-01\"", periods=10, tz=tz).repeat(size // 10)\n        self.i8data = dti.asi8\n    \n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError""}",tslibs.normalize,time,0.028558565425671863
30035,module-level,"terminus-2,gpt-5",1.0099745476207516,1.0268762871784285,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.fields,tslibs.fields,"{""tslibs.fields.TimeGetDateField.time_get_date_field"": ""class TimeGetDateField:\n    def time_get_date_field(self, size, field):\n        get_date_field(self.i8data, field)\n\n    def setup(self, size, field):\n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr"", ""tslibs.fields.TimeGetStartEndField.time_get_start_end_field"": ""class TimeGetStartEndField:\n    def time_get_start_end_field(self, size, side, period, freqstr, month_kw):\n        get_start_end_field(self.i8data, self.attrname, freqstr, month_kw=month_kw)\n\n    def setup(self, size, side, period, freqstr, month_kw):\n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr\n    \n        self.attrname = f\""is_{period}_{side}\"""", ""tslibs.fields.TimeGetTimedeltaField.time_get_timedelta_field"": ""class TimeGetTimedeltaField:\n    def time_get_timedelta_field(self, size, field):\n        get_timedelta_field(self.i8data, field)\n\n    def setup(self, size, field):\n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.fields,time,-0.011953139715471664
30041,module-level,"terminus-2,gpt-5",1.0180883364668896,1.01929183513544,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp,tslibs.timestamp,"{""tslibs.timestamp.TimestampAcrossDst.time_replace_across_dst"": ""class TimestampAcrossDst:\n    def time_replace_across_dst(self):\n        self.ts2.replace(tzinfo=self.tzinfo)\n\n    def setup(self):\n        dt = datetime(2016, 3, 27, 1)\n        self.tzinfo = pytz.timezone(\""CET\"").localize(dt, is_dst=False).tzinfo\n        self.ts2 = Timestamp(dt)"", ""tslibs.timestamp.TimestampConstruction.time_from_datetime_aware"": ""class TimestampConstruction:\n    def time_from_datetime_aware(self):\n        Timestamp(self.dttime_aware)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampConstruction.time_from_datetime_unaware"": ""class TimestampConstruction:\n    def time_from_datetime_unaware(self):\n        Timestamp(self.dttime_unaware)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampConstruction.time_from_npdatetime64"": ""class TimestampConstruction:\n    def time_from_npdatetime64(self):\n        Timestamp(self.npdatetime64)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampConstruction.time_fromordinal"": ""class TimestampConstruction:\n    def time_fromordinal(self):\n        Timestamp.fromordinal(730120)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampConstruction.time_fromtimestamp"": ""class TimestampConstruction:\n    def time_fromtimestamp(self):\n        Timestamp.fromtimestamp(1515448538)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampConstruction.time_parse_dateutil"": ""class TimestampConstruction:\n    def time_parse_dateutil(self):\n        Timestamp(\""2017/08/25 08:16:14 AM\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampConstruction.time_parse_iso8601_no_tz"": ""class TimestampConstruction:\n    def time_parse_iso8601_no_tz(self):\n        Timestamp(\""2017-08-25 08:16:14\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampConstruction.time_parse_iso8601_tz"": ""class TimestampConstruction:\n    def time_parse_iso8601_tz(self):\n        Timestamp(\""2017-08-25 08:16:14-0500\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampConstruction.time_parse_now"": ""class TimestampConstruction:\n    def time_parse_now(self):\n        Timestamp(\""now\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampOps.time_ceil"": ""class TimestampOps:\n    def time_ceil(self, tz):\n        self.ts.ceil(\""5T\"")\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_floor"": ""class TimestampOps:\n    def time_floor(self, tz):\n        self.ts.floor(\""5T\"")\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_normalize"": ""class TimestampOps:\n    def time_normalize(self, tz):\n        self.ts.normalize()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_replace_None"": ""class TimestampOps:\n    def time_replace_None(self, tz):\n        self.ts.replace(tzinfo=None)\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_replace_tz"": ""class TimestampOps:\n    def time_replace_tz(self, tz):\n        self.ts.replace(tzinfo=pytz.timezone(\""US/Eastern\""))\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_to_julian_date"": ""class TimestampOps:\n    def time_to_julian_date(self, tz):\n        self.ts.to_julian_date()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_to_pydatetime"": ""class TimestampOps:\n    def time_to_pydatetime(self, tz):\n        self.ts.to_pydatetime()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_tz_convert"": ""class TimestampOps:\n    def time_tz_convert(self, tz):\n        if self.ts.tz is not None:\n            self.ts.tz_convert(tz)\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_tz_localize"": ""class TimestampOps:\n    def time_tz_localize(self, tz):\n        if self.ts.tz is None:\n            self.ts.tz_localize(tz)\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampProperties.time_dayofweek"": ""class TimestampProperties:\n    def time_dayofweek(self, tz):\n        self.ts.dayofweek\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_days_in_month"": ""class TimestampProperties:\n    def time_days_in_month(self, tz):\n        self.ts.days_in_month\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_leap_year"": ""class TimestampProperties:\n    def time_is_leap_year(self, tz):\n        self.ts.is_leap_year\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_month_start"": ""class TimestampProperties:\n    def time_is_month_start(self, tz):\n        self.ts.is_month_start\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_quarter_end"": ""class TimestampProperties:\n    def time_is_quarter_end(self, tz):\n        self.ts.is_quarter_end\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_quarter_start"": ""class TimestampProperties:\n    def time_is_quarter_start(self, tz):\n        self.ts.is_quarter_start\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_year_end"": ""class TimestampProperties:\n    def time_is_year_end(self, tz):\n        self.ts.is_year_end\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_year_start"": ""class TimestampProperties:\n    def time_is_year_start(self, tz):\n        self.ts.is_year_start\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_microsecond"": ""class TimestampProperties:\n    def time_microsecond(self, tz):\n        self.ts.microsecond\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_month_name"": ""class TimestampProperties:\n    def time_month_name(self, tz):\n        self.ts.month_name()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_quarter"": ""class TimestampProperties:\n    def time_quarter(self, tz):\n        self.ts.quarter\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_tz"": ""class TimestampProperties:\n    def time_tz(self, tz):\n        self.ts.tz\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_week"": ""class TimestampProperties:\n    def time_week(self, tz):\n        self.ts.week\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp,time,-0.0008511306001064482
30030,module-level,"terminus-2,gpt-5",0.965711262236464,0.9104966439230252,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse,sparse,"{""sparse.Arithmetic.time_add"": ""class Arithmetic:\n    def time_add(self, dense_proportion, fill_value):\n        self.array1 + self.array2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)"", ""sparse.Arithmetic.time_divide"": ""class Arithmetic:\n    def time_divide(self, dense_proportion, fill_value):\n        self.array1 / self.array2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)"", ""sparse.Arithmetic.time_make_union"": ""class Arithmetic:\n    def time_make_union(self, dense_proportion, fill_value):\n        self.array1.sp_index.make_union(self.array2.sp_index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)"", ""sparse.ArithmeticBlock.time_addition"": ""class ArithmeticBlock:\n    def time_addition(self, fill_value):\n        self.arr1 + self.arr2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )"", ""sparse.ArithmeticBlock.time_division"": ""class ArithmeticBlock:\n    def time_division(self, fill_value):\n        self.arr1 / self.arr2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )"", ""sparse.ArithmeticBlock.time_intersect"": ""class ArithmeticBlock:\n    def time_intersect(self, fill_value):\n        self.arr2.sp_index.intersect(self.arr2.sp_index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )"", ""sparse.ArithmeticBlock.time_make_union"": ""class ArithmeticBlock:\n    def time_make_union(self, fill_value):\n        self.arr1.sp_index.make_union(self.arr2.sp_index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )"", ""sparse.FromCoo.time_sparse_series_from_coo"": ""class FromCoo:\n    def time_sparse_series_from_coo(self):\n        Series.sparse.from_coo(self.matrix)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromCoo:\n    def setup(self):\n        self.matrix = scipy.sparse.coo_matrix(\n            ([3.0, 1.0, 2.0], ([1, 0, 0], [0, 2, 3])), shape=(100, 100)\n        )"", ""sparse.GetItemMask.time_mask"": ""class GetItemMask:\n    def time_mask(self, fill_value):\n        self.sp_arr[self.sp_b_arr]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GetItemMask:\n    def setup(self, fill_value):\n        N = 1_000_000\n        d = 1e-5\n        arr = make_array(N, d, np.nan, np.float64)\n        self.sp_arr = SparseArray(arr)\n        b_arr = np.full(shape=N, fill_value=fill_value, dtype=np.bool_)\n        fv_inds = np.unique(\n            np.random.randint(low=0, high=N - 1, size=int(N * d), dtype=np.int32)\n        )\n        b_arr[fv_inds] = True if pd.isna(fill_value) else not fill_value\n        self.sp_b_arr = SparseArray(b_arr, dtype=np.bool_, fill_value=fill_value)"", ""sparse.MinMax.time_min_max"": ""class MinMax:\n    def time_min_max(self, func, fill_value):\n        getattr(self.sp_arr, func)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MinMax:\n    def setup(self, func, fill_value):\n        N = 1_000_000\n        arr = make_array(N, 1e-5, fill_value, np.float64)\n        self.sp_arr = SparseArray(arr, fill_value=fill_value)"", ""sparse.SparseArrayConstructor.time_sparse_array"": ""class SparseArrayConstructor:\n    def time_sparse_array(self, dense_proportion, fill_value, dtype):\n        SparseArray(self.array, fill_value=fill_value, dtype=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SparseArrayConstructor:\n    def setup(self, dense_proportion, fill_value, dtype):\n        N = 10**6\n        self.array = make_array(N, dense_proportion, fill_value, dtype)"", ""sparse.ToCoo.time_sparse_series_to_coo"": ""class ToCoo:\n    def time_sparse_series_to_coo(self, sort_labels):\n        self.ss_mult_lvl.sparse.to_coo(\n            row_levels=[0, 1], column_levels=[2, 3], sort_labels=sort_labels\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToCoo:\n    def setup(self, sort_labels):\n        s = Series([np.nan] * 10000)\n        s[0] = 3.0\n        s[100] = -1.0\n        s[999] = 12.1\n    \n        s_mult_lvl = s.set_axis(MultiIndex.from_product([range(10)] * 4))\n        self.ss_mult_lvl = s_mult_lvl.astype(\""Sparse\"")\n    \n        s_two_lvl = s.set_axis(MultiIndex.from_product([range(100)] * 2))\n        self.ss_two_lvl = s_two_lvl.astype(\""Sparse\"")""}",sparse,time,0.039048527802997716
30026,module-level,"terminus-2,gpt-5",1.09867621884166,0.9662950819021658,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,multiindex_object,multiindex_object,"{""multiindex_object.Integer.time_is_monotonic"": ""class Integer:\n    def time_is_monotonic(self):\n        self.mi_int.is_monotonic_increasing\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Integer:\n    def setup(self):\n        self.mi_int = MultiIndex.from_product(\n            [np.arange(1000), np.arange(1000)], names=[\""one\"", \""two\""]\n        )\n        self.obj_index = np.array(\n            [\n                (0, 10),\n                (0, 11),\n                (0, 12),\n                (0, 13),\n                (0, 14),\n                (0, 15),\n                (0, 16),\n                (0, 17),\n                (0, 18),\n                (0, 19),\n            ],\n            dtype=object,\n        )\n        self.other_mi_many_mismatches = MultiIndex.from_tuples(\n            [\n                (-7, 41),\n                (-2, 3),\n                (-0.7, 5),\n                (0, 0),\n                (0, 1.5),\n                (0, 340),\n                (0, 1001),\n                (1, -4),\n                (1, 20),\n                (1, 1040),\n                (432, -5),\n                (432, 17),\n                (439, 165.5),\n                (998, -4),\n                (998, 24065),\n                (999, 865.2),\n                (999, 1000),\n                (1045, -843),\n            ]\n        )"", ""multiindex_object.SetOperations.time_operation"": ""class SetOperations:\n    def time_operation(self, index_structure, dtype, method, sort):\n        getattr(self.left, method)(self.right, sort=sort)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SetOperations:\n    def setup(self, index_structure, dtype, method, sort):\n        N = 10**5\n        level1 = range(1000)\n    \n        level2 = date_range(start=\""1/1/2000\"", periods=N // 1000)\n        dates_left = MultiIndex.from_product([level1, level2])\n    \n        level2 = range(N // 1000)\n        int_left = MultiIndex.from_product([level1, level2])\n    \n        level2 = tm.makeStringIndex(N // 1000).values\n        str_left = MultiIndex.from_product([level1, level2])\n    \n        level2 = range(N // 1000)\n        ea_int_left = MultiIndex.from_product([level1, Series(level2, dtype=\""Int64\"")])\n    \n        data = {\n            \""datetime\"": dates_left,\n            \""int\"": int_left,\n            \""string\"": str_left,\n            \""ea_int\"": ea_int_left,\n        }\n    \n        if index_structure == \""non_monotonic\"":\n            data = {k: mi[::-1] for k, mi in data.items()}\n    \n        data = {k: {\""left\"": mi, \""right\"": mi[:-1]} for k, mi in data.items()}\n        self.left = data[dtype][\""left\""]\n        self.right = data[dtype][\""right\""]"", ""multiindex_object.SortValues.time_sort_values"": ""class SortValues:\n    def time_sort_values(self, dtype):\n        self.mi.sort_values()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SortValues:\n    def setup(self, dtype):\n        a = array(np.tile(np.arange(100), 1000), dtype=dtype)\n        b = array(np.tile(np.arange(1000), 100), dtype=dtype)\n        self.mi = MultiIndex.from_arrays([a, b])""}",multiindex_object,time,0.09362173758097175
30037,module-level,"terminus-2,gpt-5",1.046467200922746,1.047281440298551,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_add"": ""class OffestDatetimeArithmetic:\n    def time_add(self, offset):\n        self.date + offset\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")"", ""tslibs.offsets.OffestDatetimeArithmetic.time_add_10"": ""class OffestDatetimeArithmetic:\n    def time_add_10(self, offset):\n        self.date + (10 * offset)\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")"", ""tslibs.offsets.OffestDatetimeArithmetic.time_add_np_dt64"": ""class OffestDatetimeArithmetic:\n    def time_add_np_dt64(self, offset):\n        offset + self.dt64\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")"", ""tslibs.offsets.OffestDatetimeArithmetic.time_subtract"": ""class OffestDatetimeArithmetic:\n    def time_subtract(self, offset):\n        self.date - offset\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")"", ""tslibs.offsets.OffestDatetimeArithmetic.time_subtract_10"": ""class OffestDatetimeArithmetic:\n    def time_subtract_10(self, offset):\n        self.date - (10 * offset)\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")"", ""tslibs.offsets.OnOffset.time_on_offset"": ""class OnOffset:\n    def time_on_offset(self, offset):\n        for date in self.dates:\n            offset.is_on_offset(date)\n\n    def setup(self, offset):\n        self.dates = [\n            datetime(2016, m, d)\n            for m in [10, 11, 12]\n            for d in [1, 2, 3, 28, 29, 30, 31]\n            if not (m == 11 and d == 31)\n        ]""}",tslibs.offsets,time,-0.0005758411427192703
30034,module-level,"terminus-2,gpt-5",1.003683462228411,1.0009263835748243,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries,timeseries,"{""timeseries.DatetimeAccessor.time_dt_accessor_year"": ""class DatetimeAccessor:\n    def time_dt_accessor_year(self, tz):\n        self.series.dt.year\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeAccessor:\n    def setup(self, tz):\n        N = 100000\n        self.series = Series(date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"", tz=tz))"", ""timeseries.DatetimeIndex.time_get"": ""class DatetimeIndex:\n    def time_get(self, index_type):\n        self.index[0]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]"", ""timeseries.DatetimeIndex.time_is_dates_only"": ""class DatetimeIndex:\n    def time_is_dates_only(self, index_type):\n        self.index._is_dates_only\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]"", ""timeseries.DatetimeIndex.time_timeseries_is_month_start"": ""class DatetimeIndex:\n    def time_timeseries_is_month_start(self, index_type):\n        self.index.is_month_start\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]"", ""timeseries.DatetimeIndex.time_to_date"": ""class DatetimeIndex:\n    def time_to_date(self, index_type):\n        self.index.date\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]"", ""timeseries.DatetimeIndex.time_to_pydatetime"": ""class DatetimeIndex:\n    def time_to_pydatetime(self, index_type):\n        self.index.to_pydatetime()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]"", ""timeseries.DatetimeIndex.time_to_time"": ""class DatetimeIndex:\n    def time_to_time(self, index_type):\n        self.index.time\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]"", ""timeseries.DatetimeIndex.time_unique"": ""class DatetimeIndex:\n    def time_unique(self, index_type):\n        self.index.unique()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]"", ""timeseries.ResampleDataFrame.time_method"": ""class ResampleDataFrame:\n    def time_method(self, method):\n        self.resample()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ResampleDataFrame:\n    def setup(self, method):\n        rng = date_range(start=\""20130101\"", periods=100000, freq=\""50L\"")\n        df = DataFrame(np.random.randn(100000, 2), index=rng)\n        self.resample = getattr(df.resample(\""1s\""), method)"", ""timeseries.ResampleSeries.time_resample"": ""class ResampleSeries:\n    def time_resample(self, index, freq, method):\n        self.resample()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ResampleSeries:\n    def setup(self, index, freq, method):\n        indexes = {\n            \""period\"": period_range(start=\""1/1/2000\"", end=\""1/1/2001\"", freq=\""T\""),\n            \""datetime\"": date_range(start=\""1/1/2000\"", end=\""1/1/2001\"", freq=\""T\""),\n        }\n        idx = indexes[index]\n        ts = Series(np.random.randn(len(idx)), index=idx)\n        self.resample = getattr(ts.resample(freq), method)"", ""timeseries.TzLocalize.time_infer_dst"": ""class TzLocalize:\n    def time_infer_dst(self, tz):\n        self.index.tz_localize(tz, ambiguous=\""infer\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TzLocalize:\n    def setup(self, tz):\n        dst_rng = date_range(\n            start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n        )\n        self.index = date_range(start=\""10/29/2000\"", end=\""10/29/2000 00:59:59\"", freq=\""S\"")\n        self.index = self.index.append(dst_rng)\n        self.index = self.index.append(dst_rng)\n        self.index = self.index.append(\n            date_range(start=\""10/29/2000 2:00:00\"", end=\""10/29/2000 3:00:00\"", freq=\""S\"")\n        )""}",timeseries,time,0.0019498434608109288
30031,module-level,"terminus-2,gpt-5",1.0485958825607022,1.0377199781112894,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,stat_ops,stat_ops,"{""stat_ops.Correlation.time_corr_series"": ""class Correlation:\n    def time_corr_series(self, method):\n        self.s.corr(self.s2, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Correlation:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(500, 15))\n        self.df2 = pd.DataFrame(np.random.randn(500, 15))\n        self.df_wide = pd.DataFrame(np.random.randn(500, 100))\n        self.df_wide_nans = self.df_wide.where(np.random.random((500, 100)) < 0.9)\n        self.s = pd.Series(np.random.randn(500))\n        self.s2 = pd.Series(np.random.randn(500))"", ""stat_ops.FrameMultiIndexOps.time_op"": ""class FrameMultiIndexOps:\n    def time_op(self, op):\n        self.df_func()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FrameMultiIndexOps:\n    def setup(self, op):\n        levels = [np.arange(10), np.arange(100), np.arange(100)]\n        codes = [\n            np.arange(10).repeat(10000),\n            np.tile(np.arange(100).repeat(100), 10),\n            np.tile(np.tile(np.arange(100), 100), 10),\n        ]\n        index = pd.MultiIndex(levels=levels, codes=codes)\n        df = pd.DataFrame(np.random.randn(len(index), 4), index=index)\n        self.df_func = getattr(df, op)"", ""stat_ops.FrameOps.time_op"": ""class FrameOps:\n    def time_op(self, op, dtype, axis):\n        self.df_func(axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FrameOps:\n    def setup(self, op, dtype, axis):\n        values = np.random.randn(100000, 4)\n        if dtype == \""Int64\"":\n            values = values.astype(int)\n        df = pd.DataFrame(values).astype(dtype)\n        self.df_func = getattr(df, op)"", ""stat_ops.Rank.time_average_old"": ""class Rank:\n    def time_average_old(self, constructor, pct):\n        self.data.rank(pct=pct) / len(self.data)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Rank:\n    def setup(self, constructor, pct):\n        values = np.random.randn(10**5)\n        self.data = getattr(pd, constructor)(values)"", ""stat_ops.SeriesOps.time_op"": ""class SeriesOps:\n    def time_op(self, op, dtype):\n        self.s_func()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesOps:\n    def setup(self, op, dtype):\n        s = pd.Series(np.random.randn(100000)).astype(dtype)\n        self.s_func = getattr(s, op)""}",stat_ops,time,0.007691587305101053
30046,module-level,"terminus-2,oracle",0.915085859109042,0.915085859109042,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,arithmetic,arithmetic,"{""arithmetic.FrameWithFrameWide.time_op_different_blocks"": ""class FrameWithFrameWide:\n    def time_op_different_blocks(self, op, shape):\n        # blocks (and dtypes) are not aligned\n        op(self.left, self.right)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FrameWithFrameWide:\n    def setup(self, op, shape):\n        # we choose dtypes so as to make the blocks\n        #  a) not perfectly match between right and left\n        #  b) appreciably bigger than single columns\n        n_rows, n_cols = shape\n    \n        if op is operator.floordiv:\n            # floordiv is much slower than the other operations -> use less data\n            n_rows = n_rows // 10\n    \n        # construct dataframe with 2 blocks\n        arr1 = np.random.randn(n_rows, n_cols // 2).astype(\""f8\"")\n        arr2 = np.random.randn(n_rows, n_cols // 2).astype(\""f4\"")\n        df = pd.concat([DataFrame(arr1), DataFrame(arr2)], axis=1, ignore_index=True)\n        # should already be the case, but just to be sure\n        df._consolidate_inplace()\n    \n        # TODO: GH#33198 the setting here shouldn't need two steps\n        arr1 = np.random.randn(n_rows, max(n_cols // 4, 3)).astype(\""f8\"")\n        arr2 = np.random.randn(n_rows, n_cols // 2).astype(\""i8\"")\n        arr3 = np.random.randn(n_rows, n_cols // 4).astype(\""f8\"")\n        df2 = pd.concat(\n            [DataFrame(arr1), DataFrame(arr2), DataFrame(arr3)],\n            axis=1,\n            ignore_index=True,\n        )\n        # should already be the case, but just to be sure\n        df2._consolidate_inplace()\n    \n        self.left = df\n        self.right = df2"", ""arithmetic.FrameWithFrameWide.time_op_same_blocks"": ""class FrameWithFrameWide:\n    def time_op_same_blocks(self, op, shape):\n        # blocks (and dtypes) are aligned\n        op(self.left, self.left)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FrameWithFrameWide:\n    def setup(self, op, shape):\n        # we choose dtypes so as to make the blocks\n        #  a) not perfectly match between right and left\n        #  b) appreciably bigger than single columns\n        n_rows, n_cols = shape\n    \n        if op is operator.floordiv:\n            # floordiv is much slower than the other operations -> use less data\n            n_rows = n_rows // 10\n    \n        # construct dataframe with 2 blocks\n        arr1 = np.random.randn(n_rows, n_cols // 2).astype(\""f8\"")\n        arr2 = np.random.randn(n_rows, n_cols // 2).astype(\""f4\"")\n        df = pd.concat([DataFrame(arr1), DataFrame(arr2)], axis=1, ignore_index=True)\n        # should already be the case, but just to be sure\n        df._consolidate_inplace()\n    \n        # TODO: GH#33198 the setting here shouldn't need two steps\n        arr1 = np.random.randn(n_rows, max(n_cols // 4, 3)).astype(\""f8\"")\n        arr2 = np.random.randn(n_rows, n_cols // 2).astype(\""i8\"")\n        arr3 = np.random.randn(n_rows, n_cols // 4).astype(\""f8\"")\n        df2 = pd.concat(\n            [DataFrame(arr1), DataFrame(arr2), DataFrame(arr3)],\n            axis=1,\n            ignore_index=True,\n        )\n        # should already be the case, but just to be sure\n        df2._consolidate_inplace()\n    \n        self.left = df\n        self.right = df2"", ""arithmetic.IndexArithmetic.time_modulo"": ""class IndexArithmetic:\n    def time_modulo(self, dtype):\n        self.index % 2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IndexArithmetic:\n    def setup(self, dtype):\n        N = 10**6\n        indexes = {\""int\"": \""makeIntIndex\"", \""float\"": \""makeFloatIndex\""}\n        self.index = getattr(tm, indexes[dtype])(N)"", ""arithmetic.IntFrameWithScalar.time_frame_op_with_scalar"": ""class IntFrameWithScalar:\n    def time_frame_op_with_scalar(self, dtype, scalar, op):\n        op(self.df, scalar)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IntFrameWithScalar:\n    def setup(self, dtype, scalar, op):\n        arr = np.random.randn(20000, 100)\n        self.df = DataFrame(arr.astype(dtype))"", ""arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0"": ""class MixedFrameWithSeriesAxis:\n    def time_frame_op_with_series_axis0(self, opname):\n        getattr(self.df, opname)(self.ser, axis=0)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MixedFrameWithSeriesAxis:\n    def setup(self, opname):\n        arr = np.arange(10**6).reshape(1000, -1)\n        df = DataFrame(arr)\n        df[\""C\""] = 1.0\n        self.df = df\n        self.ser = df[0]\n        self.row = df.iloc[0]"", ""arithmetic.NumericInferOps.time_divide"": ""class NumericInferOps:\n    def time_divide(self, dtype):\n        self.df[\""A\""] / self.df[\""B\""]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericInferOps:\n    def setup(self, dtype):\n        N = 5 * 10**5\n        self.df = DataFrame(\n            {\""A\"": np.arange(N).astype(dtype), \""B\"": np.arange(N).astype(dtype)}\n        )"", ""arithmetic.OffsetArrayArithmetic.time_add_dti_offset"": ""class OffsetArrayArithmetic:\n    def time_add_dti_offset(self, offset):\n        with warnings.catch_warnings(record=True):\n            self.rng + offset\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass OffsetArrayArithmetic:\n    def setup(self, offset):\n        N = 10000\n        rng = date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"")\n        self.rng = rng\n        self.ser = Series(rng)"", ""arithmetic.OffsetArrayArithmetic.time_add_series_offset"": ""class OffsetArrayArithmetic:\n    def time_add_series_offset(self, offset):\n        with warnings.catch_warnings(record=True):\n            self.ser + offset\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass OffsetArrayArithmetic:\n    def setup(self, offset):\n        N = 10000\n        rng = date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"")\n        self.rng = rng\n        self.ser = Series(rng)"", ""arithmetic.Ops2.time_frame_dot"": ""class Ops2:\n    def time_frame_dot(self):\n        self.df.dot(self.df2)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Ops2:\n    def setup(self):\n        N = 10**3\n        self.df = DataFrame(np.random.randn(N, N))\n        self.df2 = DataFrame(np.random.randn(N, N))\n    \n        self.df_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n        self.df2_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n    \n        self.s = Series(np.random.randn(N))"", ""arithmetic.Ops2.time_frame_float_div_by_zero"": ""class Ops2:\n    def time_frame_float_div_by_zero(self):\n        self.df / 0\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Ops2:\n    def setup(self):\n        N = 10**3\n        self.df = DataFrame(np.random.randn(N, N))\n        self.df2 = DataFrame(np.random.randn(N, N))\n    \n        self.df_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n        self.df2_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n    \n        self.s = Series(np.random.randn(N))"", ""arithmetic.Ops2.time_frame_float_mod"": ""class Ops2:\n    def time_frame_float_mod(self):\n        self.df % self.df2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Ops2:\n    def setup(self):\n        N = 10**3\n        self.df = DataFrame(np.random.randn(N, N))\n        self.df2 = DataFrame(np.random.randn(N, N))\n    \n        self.df_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n        self.df2_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n    \n        self.s = Series(np.random.randn(N))"", ""arithmetic.Ops2.time_frame_int_div_by_zero"": ""class Ops2:\n    def time_frame_int_div_by_zero(self):\n        self.df_int / 0\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Ops2:\n    def setup(self):\n        N = 10**3\n        self.df = DataFrame(np.random.randn(N, N))\n        self.df2 = DataFrame(np.random.randn(N, N))\n    \n        self.df_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n        self.df2_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n    \n        self.s = Series(np.random.randn(N))"", ""arithmetic.Ops2.time_frame_series_dot"": ""class Ops2:\n    def time_frame_series_dot(self):\n        self.df.dot(self.s)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Ops2:\n    def setup(self):\n        N = 10**3\n        self.df = DataFrame(np.random.randn(N, N))\n        self.df2 = DataFrame(np.random.randn(N, N))\n    \n        self.df_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n        self.df2_int = DataFrame(\n            np.random.randint(\n                np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(N, N)\n            )\n        )\n    \n        self.s = Series(np.random.randn(N))"", ""arithmetic.TimedeltaOps.time_add_td_ts"": ""class TimedeltaOps:\n    def time_add_td_ts(self):\n        self.td + self.ts\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TimedeltaOps:\n    def setup(self):\n        self.td = to_timedelta(np.arange(1000000))\n        self.ts = Timestamp(\""2000\"")"", ""arithmetic.Timeseries.time_series_timestamp_compare"": ""class Timeseries:\n    def time_series_timestamp_compare(self, tz):\n        self.s <= self.ts\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Timeseries:\n    def setup(self, tz):\n        N = 10**6\n        halfway = (N // 2) - 1\n        self.s = Series(date_range(\""20010101\"", periods=N, freq=\""T\"", tz=tz))\n        self.ts = self.s[halfway]\n    \n        self.s2 = Series(date_range(\""20010101\"", periods=N, freq=\""s\"", tz=tz))"", ""arithmetic.Timeseries.time_timestamp_ops_diff"": ""class Timeseries:\n    def time_timestamp_ops_diff(self, tz):\n        self.s2.diff()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Timeseries:\n    def setup(self, tz):\n        N = 10**6\n        halfway = (N // 2) - 1\n        self.s = Series(date_range(\""20010101\"", periods=N, freq=\""T\"", tz=tz))\n        self.ts = self.s[halfway]\n    \n        self.s2 = Series(date_range(\""20010101\"", periods=N, freq=\""s\"", tz=tz))"", ""arithmetic.Timeseries.time_timestamp_ops_diff_with_shift"": ""class Timeseries:\n    def time_timestamp_ops_diff_with_shift(self, tz):\n        self.s - self.s.shift()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Timeseries:\n    def setup(self, tz):\n        N = 10**6\n        halfway = (N // 2) - 1\n        self.s = Series(date_range(\""20010101\"", periods=N, freq=\""T\"", tz=tz))\n        self.ts = self.s[halfway]\n    \n        self.s2 = Series(date_range(\""20010101\"", periods=N, freq=\""s\"", tz=tz))"", ""arithmetic.Timeseries.time_timestamp_series_compare"": ""class Timeseries:\n    def time_timestamp_series_compare(self, tz):\n        self.ts >= self.s\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Timeseries:\n    def setup(self, tz):\n        N = 10**6\n        halfway = (N // 2) - 1\n        self.s = Series(date_range(\""20010101\"", periods=N, freq=\""T\"", tz=tz))\n        self.ts = self.s[halfway]\n    \n        self.s2 = Series(date_range(\""20010101\"", periods=N, freq=\""s\"", tz=tz))""}",arithmetic,time,0.0
30054,module-level,"terminus-2,oracle",0.994961338034834,0.994961338034834,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,finalize,finalize,"{""finalize.Finalize.time_finalize_micro"": ""class Finalize:\n    def time_finalize_micro(self, param):\n        self.obj.__finalize__(self.obj, method=\""__finalize__\"")\n\n    def setup(self, param):\n        N = 1000\n        obj = param(dtype=float)\n        for i in range(N):\n            obj.attrs[i] = i\n        self.obj = obj""}",finalize,time,0.0
30050,module-level,"terminus-2,oracle",0.9282417903730092,0.9282417903730092,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,categoricals,categoricals,"{""categoricals.CategoricalSlicing.time_getitem_bool_array"": ""class CategoricalSlicing:\n    def time_getitem_bool_array(self, index):\n        self.data[self.data == self.cat_scalar]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalSlicing:\n    def setup(self, index):\n        N = 10**6\n        categories = [\""a\"", \""b\"", \""c\""]\n        values = [0] * N + [1] * N + [2] * N\n        if index == \""monotonic_incr\"":\n            self.data = pd.Categorical.from_codes(values, categories=categories)\n        elif index == \""monotonic_decr\"":\n            self.data = pd.Categorical.from_codes(\n                list(reversed(values)), categories=categories\n            )\n        elif index == \""non_monotonic\"":\n            self.data = pd.Categorical.from_codes([0, 1, 2] * N, categories=categories)\n        else:\n            raise ValueError(f\""Invalid index param: {index}\"")\n    \n        self.scalar = 10000\n        self.list = list(range(10000))\n        self.cat_scalar = \""b\"""", ""categoricals.CategoricalSlicing.time_getitem_list"": ""class CategoricalSlicing:\n    def time_getitem_list(self, index):\n        self.data[self.list]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalSlicing:\n    def setup(self, index):\n        N = 10**6\n        categories = [\""a\"", \""b\"", \""c\""]\n        values = [0] * N + [1] * N + [2] * N\n        if index == \""monotonic_incr\"":\n            self.data = pd.Categorical.from_codes(values, categories=categories)\n        elif index == \""monotonic_decr\"":\n            self.data = pd.Categorical.from_codes(\n                list(reversed(values)), categories=categories\n            )\n        elif index == \""non_monotonic\"":\n            self.data = pd.Categorical.from_codes([0, 1, 2] * N, categories=categories)\n        else:\n            raise ValueError(f\""Invalid index param: {index}\"")\n    \n        self.scalar = 10000\n        self.list = list(range(10000))\n        self.cat_scalar = \""b\"""", ""categoricals.CategoricalSlicing.time_getitem_scalar"": ""class CategoricalSlicing:\n    def time_getitem_scalar(self, index):\n        self.data[self.scalar]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalSlicing:\n    def setup(self, index):\n        N = 10**6\n        categories = [\""a\"", \""b\"", \""c\""]\n        values = [0] * N + [1] * N + [2] * N\n        if index == \""monotonic_incr\"":\n            self.data = pd.Categorical.from_codes(values, categories=categories)\n        elif index == \""monotonic_decr\"":\n            self.data = pd.Categorical.from_codes(\n                list(reversed(values)), categories=categories\n            )\n        elif index == \""non_monotonic\"":\n            self.data = pd.Categorical.from_codes([0, 1, 2] * N, categories=categories)\n        else:\n            raise ValueError(f\""Invalid index param: {index}\"")\n    \n        self.scalar = 10000\n        self.list = list(range(10000))\n        self.cat_scalar = \""b\"""", ""categoricals.Concat.time_append_overlapping_index"": ""class Concat:\n    def time_append_overlapping_index(self):\n        self.idx_a.append(self.idx_a)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Concat:\n    def setup(self):\n        N = 10**5\n        self.s = pd.Series(list(\""aabbcd\"") * N).astype(\""category\"")\n    \n        self.a = pd.Categorical(list(\""aabbcd\"") * N)\n        self.b = pd.Categorical(list(\""bbcdjk\"") * N)\n    \n        self.idx_a = pd.CategoricalIndex(range(N), range(N))\n        self.idx_b = pd.CategoricalIndex(range(N + 1), range(N + 1))\n        self.df_a = pd.DataFrame(range(N), columns=[\""a\""], index=self.idx_a)\n        self.df_b = pd.DataFrame(range(N + 1), columns=[\""a\""], index=self.idx_b)"", ""categoricals.Constructor.time_all_nan"": ""class Constructor:\n    def time_all_nan(self):\n        pd.Categorical(self.values_all_nan)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Constructor:\n    def setup(self):\n        N = 10**5\n        self.categories = list(\""abcde\"")\n        self.cat_idx = pd.Index(self.categories)\n        self.values = np.tile(self.categories, N)\n        self.codes = np.tile(range(len(self.categories)), N)\n    \n        self.datetimes = pd.Series(\n            pd.date_range(\""1995-01-01 00:00:00\"", periods=N / 10, freq=\""s\"")\n        )\n        self.datetimes_with_nat = self.datetimes.copy()\n        self.datetimes_with_nat.iloc[-1] = pd.NaT\n    \n        self.values_some_nan = list(np.tile(self.categories + [np.nan], N))\n        self.values_all_nan = [np.nan] * len(self.values)\n        self.values_all_int8 = np.ones(N, \""int8\"")\n        self.categorical = pd.Categorical(self.values, self.categories)\n        self.series = pd.Series(self.categorical)\n        self.intervals = pd.interval_range(0, 1, periods=N // 10)"", ""categoricals.Constructor.time_datetimes"": ""class Constructor:\n    def time_datetimes(self):\n        pd.Categorical(self.datetimes)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Constructor:\n    def setup(self):\n        N = 10**5\n        self.categories = list(\""abcde\"")\n        self.cat_idx = pd.Index(self.categories)\n        self.values = np.tile(self.categories, N)\n        self.codes = np.tile(range(len(self.categories)), N)\n    \n        self.datetimes = pd.Series(\n            pd.date_range(\""1995-01-01 00:00:00\"", periods=N / 10, freq=\""s\"")\n        )\n        self.datetimes_with_nat = self.datetimes.copy()\n        self.datetimes_with_nat.iloc[-1] = pd.NaT\n    \n        self.values_some_nan = list(np.tile(self.categories + [np.nan], N))\n        self.values_all_nan = [np.nan] * len(self.values)\n        self.values_all_int8 = np.ones(N, \""int8\"")\n        self.categorical = pd.Categorical(self.values, self.categories)\n        self.series = pd.Series(self.categorical)\n        self.intervals = pd.interval_range(0, 1, periods=N // 10)"", ""categoricals.Constructor.time_existing_series"": ""class Constructor:\n    def time_existing_series(self):\n        pd.Categorical(self.series)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Constructor:\n    def setup(self):\n        N = 10**5\n        self.categories = list(\""abcde\"")\n        self.cat_idx = pd.Index(self.categories)\n        self.values = np.tile(self.categories, N)\n        self.codes = np.tile(range(len(self.categories)), N)\n    \n        self.datetimes = pd.Series(\n            pd.date_range(\""1995-01-01 00:00:00\"", periods=N / 10, freq=\""s\"")\n        )\n        self.datetimes_with_nat = self.datetimes.copy()\n        self.datetimes_with_nat.iloc[-1] = pd.NaT\n    \n        self.values_some_nan = list(np.tile(self.categories + [np.nan], N))\n        self.values_all_nan = [np.nan] * len(self.values)\n        self.values_all_int8 = np.ones(N, \""int8\"")\n        self.categorical = pd.Categorical(self.values, self.categories)\n        self.series = pd.Series(self.categorical)\n        self.intervals = pd.interval_range(0, 1, periods=N // 10)"", ""categoricals.Constructor.time_fastpath"": ""class Constructor:\n    def time_fastpath(self):\n        pd.Categorical(self.codes, self.cat_idx, fastpath=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Constructor:\n    def setup(self):\n        N = 10**5\n        self.categories = list(\""abcde\"")\n        self.cat_idx = pd.Index(self.categories)\n        self.values = np.tile(self.categories, N)\n        self.codes = np.tile(range(len(self.categories)), N)\n    \n        self.datetimes = pd.Series(\n            pd.date_range(\""1995-01-01 00:00:00\"", periods=N / 10, freq=\""s\"")\n        )\n        self.datetimes_with_nat = self.datetimes.copy()\n        self.datetimes_with_nat.iloc[-1] = pd.NaT\n    \n        self.values_some_nan = list(np.tile(self.categories + [np.nan], N))\n        self.values_all_nan = [np.nan] * len(self.values)\n        self.values_all_int8 = np.ones(N, \""int8\"")\n        self.categorical = pd.Categorical(self.values, self.categories)\n        self.series = pd.Series(self.categorical)\n        self.intervals = pd.interval_range(0, 1, periods=N // 10)"", ""categoricals.Constructor.time_from_codes_all_int8"": ""class Constructor:\n    def time_from_codes_all_int8(self):\n        pd.Categorical.from_codes(self.values_all_int8, self.categories)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Constructor:\n    def setup(self):\n        N = 10**5\n        self.categories = list(\""abcde\"")\n        self.cat_idx = pd.Index(self.categories)\n        self.values = np.tile(self.categories, N)\n        self.codes = np.tile(range(len(self.categories)), N)\n    \n        self.datetimes = pd.Series(\n            pd.date_range(\""1995-01-01 00:00:00\"", periods=N / 10, freq=\""s\"")\n        )\n        self.datetimes_with_nat = self.datetimes.copy()\n        self.datetimes_with_nat.iloc[-1] = pd.NaT\n    \n        self.values_some_nan = list(np.tile(self.categories + [np.nan], N))\n        self.values_all_nan = [np.nan] * len(self.values)\n        self.values_all_int8 = np.ones(N, \""int8\"")\n        self.categorical = pd.Categorical(self.values, self.categories)\n        self.series = pd.Series(self.categorical)\n        self.intervals = pd.interval_range(0, 1, periods=N // 10)"", ""categoricals.Constructor.time_interval"": ""class Constructor:\n    def time_interval(self):\n        pd.Categorical(self.datetimes, categories=self.datetimes)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Constructor:\n    def setup(self):\n        N = 10**5\n        self.categories = list(\""abcde\"")\n        self.cat_idx = pd.Index(self.categories)\n        self.values = np.tile(self.categories, N)\n        self.codes = np.tile(range(len(self.categories)), N)\n    \n        self.datetimes = pd.Series(\n            pd.date_range(\""1995-01-01 00:00:00\"", periods=N / 10, freq=\""s\"")\n        )\n        self.datetimes_with_nat = self.datetimes.copy()\n        self.datetimes_with_nat.iloc[-1] = pd.NaT\n    \n        self.values_some_nan = list(np.tile(self.categories + [np.nan], N))\n        self.values_all_nan = [np.nan] * len(self.values)\n        self.values_all_int8 = np.ones(N, \""int8\"")\n        self.categorical = pd.Categorical(self.values, self.categories)\n        self.series = pd.Series(self.categorical)\n        self.intervals = pd.interval_range(0, 1, periods=N // 10)"", ""categoricals.Constructor.time_with_nan"": ""class Constructor:\n    def time_with_nan(self):\n        pd.Categorical(self.values_some_nan)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Constructor:\n    def setup(self):\n        N = 10**5\n        self.categories = list(\""abcde\"")\n        self.cat_idx = pd.Index(self.categories)\n        self.values = np.tile(self.categories, N)\n        self.codes = np.tile(range(len(self.categories)), N)\n    \n        self.datetimes = pd.Series(\n            pd.date_range(\""1995-01-01 00:00:00\"", periods=N / 10, freq=\""s\"")\n        )\n        self.datetimes_with_nat = self.datetimes.copy()\n        self.datetimes_with_nat.iloc[-1] = pd.NaT\n    \n        self.values_some_nan = list(np.tile(self.categories + [np.nan], N))\n        self.values_all_nan = [np.nan] * len(self.values)\n        self.values_all_int8 = np.ones(N, \""int8\"")\n        self.categorical = pd.Categorical(self.values, self.categories)\n        self.series = pd.Series(self.categorical)\n        self.intervals = pd.interval_range(0, 1, periods=N // 10)"", ""categoricals.Indexing.time_align"": ""class Indexing:\n    def time_align(self):\n        pd.DataFrame({\""a\"": self.series, \""b\"": self.series[:500]})\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self):\n        N = 10**5\n        self.index = pd.CategoricalIndex(range(N), range(N))\n        self.series = pd.Series(range(N), index=self.index).sort_index()\n        self.category = self.index[500]"", ""categoricals.Indexing.time_reindex_missing"": ""class Indexing:\n    def time_reindex_missing(self):\n        self.index.reindex([\""a\"", \""b\"", \""c\"", \""d\""])\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self):\n        N = 10**5\n        self.index = pd.CategoricalIndex(range(N), range(N))\n        self.series = pd.Series(range(N), index=self.index).sort_index()\n        self.category = self.index[500]"", ""categoricals.Indexing.time_shallow_copy"": ""class Indexing:\n    def time_shallow_copy(self):\n        self.index._view()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self):\n        N = 10**5\n        self.index = pd.CategoricalIndex(range(N), range(N))\n        self.series = pd.Series(range(N), index=self.index).sort_index()\n        self.category = self.index[500]"", ""categoricals.Indexing.time_sort_values"": ""class Indexing:\n    def time_sort_values(self):\n        self.index.sort_values(ascending=False)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self):\n        N = 10**5\n        self.index = pd.CategoricalIndex(range(N), range(N))\n        self.series = pd.Series(range(N), index=self.index).sort_index()\n        self.category = self.index[500]"", ""categoricals.Indexing.time_unique"": ""class Indexing:\n    def time_unique(self):\n        self.index.unique()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self):\n        N = 10**5\n        self.index = pd.CategoricalIndex(range(N), range(N))\n        self.series = pd.Series(range(N), index=self.index).sort_index()\n        self.category = self.index[500]"", ""categoricals.Rank.time_rank_int"": ""class Rank:\n    def time_rank_int(self):\n        self.s_int.rank()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Rank:\n    def setup(self):\n        N = 10**5\n        ncats = 15\n    \n        self.s_str = pd.Series(tm.makeCategoricalIndex(N, ncats)).astype(str)\n        self.s_str_cat = pd.Series(self.s_str, dtype=\""category\"")\n        with warnings.catch_warnings(record=True):\n            str_cat_type = pd.CategoricalDtype(set(self.s_str), ordered=True)\n            self.s_str_cat_ordered = self.s_str.astype(str_cat_type)\n    \n        self.s_int = pd.Series(np.random.randint(0, ncats, size=N))\n        self.s_int_cat = pd.Series(self.s_int, dtype=\""category\"")\n        with warnings.catch_warnings(record=True):\n            int_cat_type = pd.CategoricalDtype(set(self.s_int), ordered=True)\n            self.s_int_cat_ordered = self.s_int.astype(int_cat_type)"", ""categoricals.Rank.time_rank_string_cat"": ""class Rank:\n    def time_rank_string_cat(self):\n        self.s_str_cat.rank()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Rank:\n    def setup(self):\n        N = 10**5\n        ncats = 15\n    \n        self.s_str = pd.Series(tm.makeCategoricalIndex(N, ncats)).astype(str)\n        self.s_str_cat = pd.Series(self.s_str, dtype=\""category\"")\n        with warnings.catch_warnings(record=True):\n            str_cat_type = pd.CategoricalDtype(set(self.s_str), ordered=True)\n            self.s_str_cat_ordered = self.s_str.astype(str_cat_type)\n    \n        self.s_int = pd.Series(np.random.randint(0, ncats, size=N))\n        self.s_int_cat = pd.Series(self.s_int, dtype=\""category\"")\n        with warnings.catch_warnings(record=True):\n            int_cat_type = pd.CategoricalDtype(set(self.s_int), ordered=True)\n            self.s_int_cat_ordered = self.s_int.astype(int_cat_type)"", ""categoricals.RemoveCategories.time_remove_categories"": ""class RemoveCategories:\n    def time_remove_categories(self):\n        self.ts.cat.remove_categories(self.ts.cat.categories[::2])\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass RemoveCategories:\n    def setup(self):\n        n = 5 * 10**5\n        arr = [f\""s{i:04d}\"" for i in np.random.randint(0, n // 10, size=n)]\n        self.ts = pd.Series(arr).astype(\""category\"")"", ""categoricals.SearchSorted.time_categorical_contains"": ""class SearchSorted:\n    def time_categorical_contains(self):\n        self.c.searchsorted(self.key)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SearchSorted:\n    def setup(self):\n        N = 10**5\n        self.ci = tm.makeCategoricalIndex(N).sort_values()\n        self.c = self.ci.values\n        self.key = self.ci.categories[1]"", ""categoricals.SetCategories.time_set_categories"": ""class SetCategories:\n    def time_set_categories(self):\n        self.ts.cat.set_categories(self.ts.cat.categories[::2])\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SetCategories:\n    def setup(self):\n        n = 5 * 10**5\n        arr = [f\""s{i:04d}\"" for i in np.random.randint(0, n // 10, size=n)]\n        self.ts = pd.Series(arr).astype(\""category\"")""}",categoricals,time,0.0
30048,module-level,"terminus-2,oracle",0.971147038533247,0.971147038533247,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,attrs_caching,attrs_caching,"{""attrs_caching.SeriesArrayAttribute.time_array"": ""class SeriesArrayAttribute:\n    def time_array(self, dtype):\n        self.series.array\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesArrayAttribute:\n    def setup(self, dtype):\n        if dtype == \""numeric\"":\n            self.series = pd.Series([1, 2, 3])\n        elif dtype == \""object\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=object)\n        elif dtype == \""category\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=\""category\"")\n        elif dtype == \""datetime64\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3))\n        elif dtype == \""datetime64tz\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3, tz=\""UTC\""))"", ""attrs_caching.SeriesArrayAttribute.time_extract_array"": ""class SeriesArrayAttribute:\n    def time_extract_array(self, dtype):\n        extract_array(self.series)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesArrayAttribute:\n    def setup(self, dtype):\n        if dtype == \""numeric\"":\n            self.series = pd.Series([1, 2, 3])\n        elif dtype == \""object\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=object)\n        elif dtype == \""category\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=\""category\"")\n        elif dtype == \""datetime64\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3))\n        elif dtype == \""datetime64tz\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3, tz=\""UTC\""))"", ""attrs_caching.SeriesArrayAttribute.time_extract_array_numpy"": ""class SeriesArrayAttribute:\n    def time_extract_array_numpy(self, dtype):\n        extract_array(self.series, extract_numpy=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesArrayAttribute:\n    def setup(self, dtype):\n        if dtype == \""numeric\"":\n            self.series = pd.Series([1, 2, 3])\n        elif dtype == \""object\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=object)\n        elif dtype == \""category\"":\n            self.series = pd.Series([\""a\"", \""b\"", \""c\""], dtype=\""category\"")\n        elif dtype == \""datetime64\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3))\n        elif dtype == \""datetime64tz\"":\n            self.series = pd.Series(pd.date_range(\""2013\"", periods=3, tz=\""UTC\""))""}",attrs_caching,time,0.0
30047,module-level,"terminus-2,oracle",0.958506756052618,0.958506756052618,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,array,array,"{""array.ArrowExtensionArray.time_to_numpy"": ""class ArrowExtensionArray:\n    def time_to_numpy(self, dtype, hasna):\n        self.arr.to_numpy()\n\n    def setup(self, dtype, hasna):\n        N = 100_000\n        if dtype == \""boolean[pyarrow]\"":\n            data = np.random.choice([True, False], N, replace=True)\n        elif dtype == \""float64[pyarrow]\"":\n            data = np.random.randn(N)\n        elif dtype == \""int64[pyarrow]\"":\n            data = np.arange(N)\n        elif dtype == \""string[pyarrow]\"":\n            data = tm.rands_array(10, N)\n        elif dtype == \""timestamp[ns][pyarrow]\"":\n            data = pd.date_range(\""2000-01-01\"", freq=\""s\"", periods=N)\n        else:\n            raise NotImplementedError\n    \n        arr = pd.array(data, dtype=dtype)\n        if hasna:\n            arr[::2] = pd.NA\n        self.arr = arr"", ""array.ArrowStringArray.time_setitem_list"": ""class ArrowStringArray:\n    def time_setitem_list(self, multiple_chunks):\n        indexer = list(range(0, 50)) + list(range(-1000, 0, 50))\n        self.array[indexer] = [\""foo\""] * len(indexer)\n\n    def setup(self, multiple_chunks):\n        try:\n            import pyarrow as pa\n        except ImportError:\n            raise NotImplementedError\n        strings = tm.rands_array(3, 10_000)\n        if multiple_chunks:\n            chunks = [strings[i : i + 100] for i in range(0, len(strings), 100)]\n            self.array = pd.arrays.ArrowStringArray(pa.chunked_array(chunks))\n        else:\n            self.array = pd.arrays.ArrowStringArray(pa.array(strings))"", ""array.ArrowStringArray.time_tolist"": ""class ArrowStringArray:\n    def time_tolist(self, multiple_chunks):\n        self.array.tolist()\n\n    def setup(self, multiple_chunks):\n        try:\n            import pyarrow as pa\n        except ImportError:\n            raise NotImplementedError\n        strings = tm.rands_array(3, 10_000)\n        if multiple_chunks:\n            chunks = [strings[i : i + 100] for i in range(0, len(strings), 100)]\n            self.array = pd.arrays.ArrowStringArray(pa.chunked_array(chunks))\n        else:\n            self.array = pd.arrays.ArrowStringArray(pa.array(strings))"", ""array.BooleanArray.time_from_bool_array"": ""class BooleanArray:\n    def time_from_bool_array(self):\n        pd.array(self.values_bool, dtype=\""boolean\"")\n\n    def setup(self):\n        self.values_bool = np.array([True, False, True, False])\n        self.values_float = np.array([1.0, 0.0, 1.0, 0.0])\n        self.values_integer = np.array([1, 0, 1, 0])\n        self.values_integer_like = [1, 0, 1, 0]\n        self.data = np.array([True, False, True, False])\n        self.mask = np.array([False, False, True, False])"", ""array.IntegerArray.time_constructor"": ""class IntegerArray:\n    def time_constructor(self):\n        pd.arrays.IntegerArray(self.data, self.mask)\n\n    def setup(self):\n        N = 250_000\n        self.values_integer = np.array([1, 0, 1, 0] * N)\n        self.data = np.array([1, 2, 3, 4] * N, dtype=\""int64\"")\n        self.mask = np.array([False, False, True, False] * N)"", ""array.IntervalArray.time_from_tuples"": ""class IntervalArray:\n    def time_from_tuples(self):\n        pd.arrays.IntervalArray.from_tuples(self.tuples)\n\n    def setup(self):\n        N = 10_000\n        self.tuples = [(i, i + 1) for i in range(N)]"", ""array.StringArray.time_from_list"": ""class StringArray:\n    def time_from_list(self):\n        pd.array(self.values_list, dtype=\""string\"")\n\n    def setup(self):\n        N = 100_000\n        values = tm.rands_array(3, N)\n        self.values_obj = np.array(values, dtype=\""object\"")\n        self.values_str = np.array(values, dtype=\""U\"")\n        self.values_list = values.tolist()""}",array,time,0.0
30044,module-level,"terminus-2,oracle",1.0961274099059868,1.0961274099059868,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algorithms,algorithms,"{""algorithms.Duplicated.time_duplicated"": ""class Duplicated:\n    def time_duplicated(self, unique, keep, dtype):\n        self.idx.duplicated(keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Duplicated:\n    def setup(self, unique, keep, dtype):\n        N = 10**5\n        data = {\n            \""int\"": pd.Index(np.arange(N), dtype=\""int64\""),\n            \""uint\"": pd.Index(np.arange(N), dtype=\""uint64\""),\n            \""float\"": pd.Index(np.random.randn(N), dtype=\""float64\""),\n            \""string\"": tm.makeStringIndex(N),\n            \""datetime64[ns]\"": pd.date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n            \""datetime64[ns, tz]\"": pd.date_range(\n                \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n            ),\n        }[dtype]\n        if not unique:\n            data = data.repeat(5)\n        self.idx = data\n        # cache is_unique\n        self.idx.is_unique"", ""algorithms.Factorize.time_factorize"": ""class Factorize:\n    def time_factorize(self, unique, sort, dtype):\n        pd.factorize(self.data, sort=sort)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Factorize:\n    def setup(self, unique, sort, dtype):\n        N = 10**5\n        string_index = tm.makeStringIndex(N)\n        string_arrow = None\n        if dtype == \""string[pyarrow]\"":\n            try:\n                string_arrow = pd.array(string_index, dtype=\""string[pyarrow]\"")\n            except ImportError:\n                raise NotImplementedError\n    \n        data = {\n            \""int\"": pd.Index(np.arange(N), dtype=\""int64\""),\n            \""uint\"": pd.Index(np.arange(N), dtype=\""uint64\""),\n            \""float\"": pd.Index(np.random.randn(N), dtype=\""float64\""),\n            \""object\"": string_index,\n            \""datetime64[ns]\"": pd.date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n            \""datetime64[ns, tz]\"": pd.date_range(\n                \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n            ),\n            \""Int64\"": pd.array(np.arange(N), dtype=\""Int64\""),\n            \""boolean\"": pd.array(np.random.randint(0, 2, N), dtype=\""boolean\""),\n            \""string[pyarrow]\"": string_arrow,\n        }[dtype]\n        if not unique:\n            data = data.repeat(5)\n        self.data = data"", ""algorithms.Quantile.time_quantile"": ""class Quantile:\n    def time_quantile(self, quantile, interpolation, dtype):\n        self.idx.quantile(quantile, interpolation=interpolation)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, quantile, interpolation, dtype):\n        N = 10**5\n        data = {\n            \""int\"": np.arange(N),\n            \""uint\"": np.arange(N).astype(np.uint64),\n            \""float\"": np.random.randn(N),\n        }\n        self.idx = pd.Series(data[dtype].repeat(5))""}",algorithms,time,0.0
30038,module-level,"terminus-2,gpt-5",1.011274402888825,1.0143956869978314,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period,tslibs.period,"{""tslibs.period.PeriodConstructor.time_period_constructor"": ""class PeriodConstructor:\n    def time_period_constructor(self, freq, is_offset):\n        Period(\""2012-06-01\"", freq=freq)\n\n    def setup(self, freq, is_offset):\n        if is_offset:\n            self.freq = to_offset(freq)\n        else:\n            self.freq = freq"", ""tslibs.period.PeriodProperties.time_property"": ""class PeriodProperties:\n    def time_property(self, freq, attr):\n        getattr(self.per, attr)\n\n    def setup(self, freq, attr):\n        self.per = Period(\""2012-06-01\"", freq=freq)"", ""tslibs.period.PeriodUnaryMethods.time_asfreq"": ""class PeriodUnaryMethods:\n    def time_asfreq(self, freq):\n        self.per.asfreq(\""A\"")\n\n    def setup(self, freq):\n        self.per = Period(\""2012-06-01\"", freq=freq)"", ""tslibs.period.PeriodUnaryMethods.time_now"": ""class PeriodUnaryMethods:\n    def time_now(self, freq):\n        self.per.now(freq)\n\n    def setup(self, freq):\n        self.per = Period(\""2012-06-01\"", freq=freq)"", ""tslibs.period.PeriodUnaryMethods.time_to_timestamp"": ""class PeriodUnaryMethods:\n    def time_to_timestamp(self, freq):\n        self.per.to_timestamp()\n\n    def setup(self, freq):\n        self.per = Period(\""2012-06-01\"", freq=freq)"", ""tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr"": ""class TimeDT64ArrToPeriodArr:\n    def time_dt64arr_to_periodarr(self, size, freq, tz):\n        dt64arr_to_periodarr(self.i8values, freq, tz)\n\n    def setup(self, size, freq, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.arange(10, dtype=\""i8\"").repeat(size // 10)\n        self.i8values = arr"", ""tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr"": ""class TimePeriodArrToDT64Arr:\n    def time_periodarray_to_dt64arr(self, size, freq):\n        periodarr_to_dt64arr(self.i8values, freq)\n\n    def setup(self, size, freq):\n        arr = np.arange(10, dtype=\""i8\"").repeat(size // 10)\n        self.i8values = arr""}",tslibs.period,time,-0.002207414504247859
30045,module-level,"terminus-2,oracle",1.069039740249108,1.069039740249108,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,algos.isin,algos.isin,"{""algos.isin.IsIn.time_isin_categorical"": ""class IsIn:\n    def time_isin_categorical(self, dtype):\n        self.series.isin(self.cat_values)\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)"", ""algos.isin.IsIn.time_isin_empty"": ""class IsIn:\n    def time_isin_empty(self, dtype):\n        self.series.isin([])\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)"", ""algos.isin.IsIn.time_isin_mismatched_dtype"": ""class IsIn:\n    def time_isin_mismatched_dtype(self, dtype):\n        self.series.isin(self.mismatched)\n\n    def setup(self, dtype):\n        N = 10000\n    \n        self.mismatched = [NaT.to_datetime64()] * 2\n    \n        if dtype in [\""boolean\"", \""bool\""]:\n            self.series = Series(np.random.randint(0, 2, N)).astype(dtype)\n            self.values = [True, False]\n    \n        elif dtype == \""datetime64[ns]\"":\n            # Note: values here is much larger than non-dt64ns cases\n    \n            # dti has length=115777\n            dti = date_range(start=\""2015-10-26\"", end=\""2016-01-01\"", freq=\""50s\"")\n            self.series = Series(dti)\n            self.values = self.series._values[::3]\n            self.mismatched = [1, 2]\n    \n        elif dtype in [\""category[object]\"", \""category[int]\""]:\n            # Note: sizes are different in this case than others\n            n = 5 * 10**5\n            sample_size = 100\n    \n            arr = list(np.random.randint(0, n // 10, size=n))\n            if dtype == \""category[object]\"":\n                arr = [f\""s{i:04d}\"" for i in arr]\n    \n            self.values = np.random.choice(arr, sample_size)\n            self.series = Series(arr).astype(\""category\"")\n    \n        elif dtype in [\""str\"", \""string[python]\"", \""string[pyarrow]\""]:\n            try:\n                self.series = Series(tm.makeStringIndex(N), dtype=dtype)\n            except ImportError:\n                raise NotImplementedError\n            self.values = list(self.series[:2])\n    \n        else:\n            self.series = Series(np.random.randint(1, 10, N)).astype(dtype)\n            self.values = [1, 2]\n    \n        self.cat_values = Categorical(self.values)"", ""algos.isin.IsInLongSeriesLookUpDominates.time_isin"": ""class IsInLongSeriesLookUpDominates:\n    def time_isin(self, dtypes, MaxNumber, series_type):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, MaxNumber, series_type):\n        N = 10**7\n    \n        if series_type == \""random_hits\"":\n            array = np.random.randint(0, MaxNumber, N)\n        if series_type == \""random_misses\"":\n            array = np.random.randint(0, MaxNumber, N) + MaxNumber\n        if series_type == \""monotone_hits\"":\n            array = np.repeat(np.arange(MaxNumber), N // MaxNumber)\n        if series_type == \""monotone_misses\"":\n            array = np.arange(N) + MaxNumber\n    \n        self.series = Series(array).astype(dtype)\n    \n        self.values = np.arange(MaxNumber).astype(dtype.lower())"", ""algos.isin.IsinAlmostFullWithRandomInt.time_isin"": ""class IsinAlmostFullWithRandomInt:\n    def time_isin(self, dtype, exponent, title):\n        self.series.isin(self.values)\n\n    def setup(self, dtype, exponent, title):\n        M = 3 * 2 ** (exponent - 2)\n        # 0.77-the maximal share of occupied buckets\n        self.series = Series(np.random.randint(0, M, M)).astype(dtype)\n    \n        values = np.random.randint(0, M, M).astype(dtype)\n        if title == \""inside\"":\n            self.values = values\n        elif title == \""outside\"":\n            self.values = values + M\n        else:\n            raise ValueError(title)""}",algos.isin,time,0.0
30053,module-level,"terminus-2,oracle",1.1970532082915912,1.1970532082915912,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,eval,eval,"{""eval.Eval.time_add"": ""class Eval:\n    def time_add(self, engine, threads):\n        pd.eval(\""self.df + self.df2 + self.df3 + self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)"", ""eval.Eval.time_and"": ""class Eval:\n    def time_and(self, engine, threads):\n        pd.eval(\n            \""(self.df > 0) & (self.df2 > 0) & (self.df3 > 0) & (self.df4 > 0)\"",\n            engine=engine,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)"", ""eval.Eval.time_chained_cmp"": ""class Eval:\n    def time_chained_cmp(self, engine, threads):\n        pd.eval(\""self.df < self.df2 < self.df3 < self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)"", ""eval.Eval.time_mult"": ""class Eval:\n    def time_mult(self, engine, threads):\n        pd.eval(\""self.df * self.df2 * self.df3 * self.df4\"", engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Eval:\n    def setup(self, engine, threads):\n        self.df = pd.DataFrame(np.random.randn(20000, 100))\n        self.df2 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df3 = pd.DataFrame(np.random.randn(20000, 100))\n        self.df4 = pd.DataFrame(np.random.randn(20000, 100))\n    \n        if threads == 1:\n            expr.set_numexpr_threads(1)"", ""eval.Query.time_query_datetime_column"": ""class Query:\n    def time_query_datetime_column(self):\n        self.df.query(\""dates < @self.ts\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Query:\n    def setup(self):\n        N = 10**6\n        halfway = (N // 2) - 1\n        index = pd.date_range(\""20010101\"", periods=N, freq=\""T\"")\n        s = pd.Series(index)\n        self.ts = s.iloc[halfway]\n        self.df = pd.DataFrame({\""a\"": np.random.randn(N), \""dates\"": index}, index=index)\n        data = np.random.randn(N)\n        self.min_val = data.min()\n        self.max_val = data.max()"", ""eval.Query.time_query_with_boolean_selection"": ""class Query:\n    def time_query_with_boolean_selection(self):\n        self.df.query(\""(a >= @self.min_val) & (a <= @self.max_val)\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Query:\n    def setup(self):\n        N = 10**6\n        halfway = (N // 2) - 1\n        index = pd.date_range(\""20010101\"", periods=N, freq=\""T\"")\n        s = pd.Series(index)\n        self.ts = s.iloc[halfway]\n        self.df = pd.DataFrame({\""a\"": np.random.randn(N), \""dates\"": index}, index=index)\n        data = np.random.randn(N)\n        self.min_val = data.min()\n        self.max_val = data.max()""}",eval,time,0.0
30052,module-level,"terminus-2,oracle",1.004290039604805,1.004290039604805,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,dtypes,dtypes,"{""dtypes.Dtypes.time_pandas_dtype"": ""class Dtypes:\n    def time_pandas_dtype(self, dtype):\n        pandas_dtype(dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)"", ""dtypes.DtypesInvalid.time_pandas_dtype_invalid"": ""class DtypesInvalid:\n    def time_pandas_dtype_invalid(self, dtype):\n        try:\n            pandas_dtype(self.data_dict[dtype])\n        except TypeError:\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)"", ""dtypes.SelectDtypes.time_select_dtype_bool_include"": ""class SelectDtypes:\n    def time_select_dtype_bool_include(self, dtype):\n        self.df_bool.select_dtypes(include=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_float_include"": ""class SelectDtypes:\n    def time_select_dtype_float_include(self, dtype):\n        self.df_float.select_dtypes(include=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_int_exclude"": ""class SelectDtypes:\n    def time_select_dtype_int_exclude(self, dtype):\n        self.df_int.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_string_exclude"": ""class SelectDtypes:\n    def time_select_dtype_string_exclude(self, dtype):\n        self.df_string.select_dtypes(exclude=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )"", ""dtypes.SelectDtypes.time_select_dtype_string_include"": ""class SelectDtypes:\n    def time_select_dtype_string_include(self, dtype):\n        self.df_string.select_dtypes(include=dtype)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SelectDtypes:\n    def setup(self, dtype):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n    \n        def create_df(data):\n            return DataFrame(data, index=self.index, columns=self.columns)\n    \n        self.df_int = create_df(np.random.randint(low=100, size=(N, K)))\n        self.df_float = create_df(np.random.randn(N, K))\n        self.df_bool = create_df(np.random.choice([True, False], size=(N, K)))\n        self.df_string = create_df(\n            np.random.choice(list(string.ascii_letters), size=(N, K))\n        )""}",dtypes,time,0.0
30049,module-level,"terminus-2,oracle",0.9711150307596093,0.9711150307596093,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,boolean,boolean,"{""boolean.TimeLogicalOps.time_and_scalar"": ""class TimeLogicalOps:\n    def time_and_scalar(self):\n        self.left & True\n        self.left & False\n\n    def setup(self):\n        N = 10_000\n        left, right, lmask, rmask = np.random.randint(0, 2, size=(4, N)).astype(\""bool\"")\n        self.left = pd.arrays.BooleanArray(left, lmask)\n        self.right = pd.arrays.BooleanArray(right, rmask)"", ""boolean.TimeLogicalOps.time_xor_array"": ""class TimeLogicalOps:\n    def time_xor_array(self):\n        self.left ^ self.right\n\n    def setup(self):\n        N = 10_000\n        left, right, lmask, rmask = np.random.randint(0, 2, size=(4, N)).astype(\""bool\"")\n        self.left = pd.arrays.BooleanArray(left, lmask)\n        self.right = pd.arrays.BooleanArray(right, rmask)""}",boolean,time,0.0
30061,module-level,"terminus-2,oracle",1.0909545627151167,1.0909545627151167,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_object,index_object,"{""index_object.Indexing.time_get"": ""class Indexing:\n    def time_get(self, dtype):\n        self.idx[1]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self, dtype):\n        N = 10**6\n        self.idx = getattr(tm, f\""make{dtype}Index\"")(N)\n        self.array_mask = (np.arange(N) % 3) == 0\n        self.series_mask = Series(self.array_mask)\n        self.sorted = self.idx.sort_values()\n        half = N // 2\n        self.non_unique = self.idx[:half].append(self.idx[:half])\n        self.non_unique_sorted = (\n            self.sorted[:half].append(self.sorted[:half]).sort_values()\n        )\n        self.key = self.sorted[N // 4]"", ""index_object.Indexing.time_get_loc_sorted"": ""class Indexing:\n    def time_get_loc_sorted(self, dtype):\n        self.sorted.get_loc(self.key)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Indexing:\n    def setup(self, dtype):\n        N = 10**6\n        self.idx = getattr(tm, f\""make{dtype}Index\"")(N)\n        self.array_mask = (np.arange(N) % 3) == 0\n        self.series_mask = Series(self.array_mask)\n        self.sorted = self.idx.sort_values()\n        half = N // 2\n        self.non_unique = self.idx[:half].append(self.idx[:half])\n        self.non_unique_sorted = (\n            self.sorted[:half].append(self.sorted[:half]).sort_values()\n        )\n        self.key = self.sorted[N // 4]"", ""index_object.SetOperations.time_operation"": ""class SetOperations:\n    def time_operation(self, index_structure, dtype, method):\n        getattr(self.left, method)(self.right)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SetOperations:\n    def setup(self, index_structure, dtype, method):\n        N = 10**5\n        dates_left = date_range(\""1/1/2000\"", periods=N, freq=\""T\"")\n        fmt = \""%Y-%m-%d %H:%M:%S\""\n        date_str_left = Index(dates_left.strftime(fmt))\n        int_left = Index(np.arange(N))\n        ea_int_left = Index(np.arange(N), dtype=\""Int64\"")\n        str_left = tm.makeStringIndex(N)\n    \n        data = {\n            \""datetime\"": dates_left,\n            \""date_string\"": date_str_left,\n            \""int\"": int_left,\n            \""strings\"": str_left,\n            \""ea_int\"": ea_int_left,\n        }\n    \n        if index_structure == \""non_monotonic\"":\n            data = {k: mi[::-1] for k, mi in data.items()}\n    \n        data = {k: {\""left\"": idx, \""right\"": idx[:-1]} for k, idx in data.items()}\n    \n        self.left = data[dtype][\""left\""]\n        self.right = data[dtype][\""right\""]""}",index_object,time,0.0
30060,module-level,"terminus-2,oracle",1.0570132078439995,1.0570132078439995,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,index_cached_properties,index_cached_properties,"{""index_cached_properties.IndexCache.time_engine"": ""class IndexCache:\n    def time_engine(self, index_type):\n        self.idx._engine\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}"", ""index_cached_properties.IndexCache.time_is_monotonic_decreasing"": ""class IndexCache:\n    def time_is_monotonic_decreasing(self, index_type):\n        self.idx.is_monotonic_decreasing\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}"", ""index_cached_properties.IndexCache.time_is_monotonic_increasing"": ""class IndexCache:\n    def time_is_monotonic_increasing(self, index_type):\n        self.idx.is_monotonic_increasing\n\n    def setup(self, index_type):\n        N = 10**5\n        if index_type == \""MultiIndex\"":\n            self.idx = pd.MultiIndex.from_product(\n                [pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N // 2), [\""a\"", \""b\""]]\n            )\n        elif index_type == \""DatetimeIndex\"":\n            self.idx = pd.date_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""Int64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""int64\"")\n        elif index_type == \""PeriodIndex\"":\n            self.idx = pd.period_range(\""1/1/2000\"", freq=\""T\"", periods=N)\n        elif index_type == \""RangeIndex\"":\n            self.idx = pd.RangeIndex(start=0, stop=N)\n        elif index_type == \""IntervalIndex\"":\n            self.idx = pd.IntervalIndex.from_arrays(range(N), range(1, N + 1))\n        elif index_type == \""TimedeltaIndex\"":\n            self.idx = pd.TimedeltaIndex(range(N))\n        elif index_type == \""Float64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""float64\"")\n        elif index_type == \""UInt64Index\"":\n            self.idx = pd.Index(range(N), dtype=\""uint64\"")\n        elif index_type == \""CategoricalIndex\"":\n            self.idx = pd.CategoricalIndex(range(N), range(N))\n        else:\n            raise ValueError\n        assert len(self.idx) == N\n        self.idx._cache = {}""}",index_cached_properties,time,0.0
30064,module-level,"terminus-2,oracle",1.007803130150536,1.007803130150536,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,inference,inference,"{""inference.ToDatetimeCache.time_dup_string_dates_and_format"": ""class ToDatetimeCache:\n    def time_dup_string_dates_and_format(self, cache):\n        to_datetime(self.dup_string_dates, format=\""%Y-%m-%d\"", cache=cache)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToDatetimeCache:\n    def setup(self, cache):\n        N = 10000\n        self.unique_numeric_seconds = list(range(N))\n        self.dup_numeric_seconds = [1000] * N\n        self.dup_string_dates = [\""2000-02-11\""] * N\n        self.dup_string_with_tz = [\""2000-02-11 15:00:00-0800\""] * N"", ""inference.ToNumericDowncast.time_downcast"": ""class ToNumericDowncast:\n    def time_downcast(self, dtype, downcast):\n        to_numeric(self.data, downcast=downcast)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToNumericDowncast:\n    def setup(self, dtype, downcast):\n        self.data = self.data_dict[dtype]""}",inference,time,0.0
30057,module-level,"terminus-2,oracle",1.0199719829228913,1.0199719829228913,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,gil,gil,"{""gil.ParallelGroupbyMethods.time_parallel"": ""class ParallelGroupbyMethods:\n    def time_parallel(self, threads, method):\n        self.parallel()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ParallelGroupbyMethods:\n    def setup(self, threads, method):\n        N = 10**6\n        ngroups = 10**3\n        df = DataFrame(\n            {\""key\"": np.random.randint(0, ngroups, size=N), \""data\"": np.random.randn(N)}\n        )\n    \n        @test_parallel(num_threads=threads)\n        def parallel():\n            getattr(df.groupby(\""key\"")[\""data\""], method)()\n    \n        self.parallel = parallel\n    \n        def loop():\n            getattr(df.groupby(\""key\"")[\""data\""], method)()\n    \n        self.loop = loop"", ""gil.ParallelRolling.time_rolling"": ""class ParallelRolling:\n    def time_rolling(self, method):\n        self.parallel_rolling()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ParallelRolling:\n    def setup(self, method):\n        win = 100\n        arr = np.random.rand(100000)\n        if hasattr(DataFrame, \""rolling\""):\n            df = DataFrame(arr).rolling(win)\n    \n            @test_parallel(num_threads=2)\n            def parallel_rolling():\n                getattr(df, method)()\n    \n            self.parallel_rolling = parallel_rolling\n        elif have_rolling_methods:\n            rolling = {\n                \""median\"": rolling_median,\n                \""mean\"": rolling_mean,\n                \""min\"": rolling_min,\n                \""max\"": rolling_max,\n                \""var\"": rolling_var,\n                \""skew\"": rolling_skew,\n                \""kurt\"": rolling_kurt,\n                \""std\"": rolling_std,\n            }\n    \n            @test_parallel(num_threads=2)\n            def parallel_rolling():\n                rolling[method](arr, win)\n    \n            self.parallel_rolling = parallel_rolling\n        else:\n            raise NotImplementedError"", ""gil.ParallelTake1D.time_take1d"": ""class ParallelTake1D:\n    def time_take1d(self, dtype):\n        self.parallel_take1d()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ParallelTake1D:\n    def setup(self, dtype):\n        N = 10**6\n        df = DataFrame({\""col\"": np.arange(N, dtype=dtype)})\n        indexer = np.arange(100, len(df) - 100)\n    \n        @test_parallel(num_threads=2)\n        def parallel_take1d():\n            take_nd(df[\""col\""].values, indexer)\n    \n        self.parallel_take1d = parallel_take1d""}",gil,time,0.0
30062,module-level,"terminus-2,oracle",1.0980385631701033,1.0980385631701033,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing,indexing,"{""indexing.CategoricalIndexIndexing.time_getitem_scalar"": ""class CategoricalIndexIndexing:\n    def time_getitem_scalar(self, index):\n        self.data[self.int_scalar]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass CategoricalIndexIndexing:\n    def setup(self, index):\n        N = 10**5\n        values = list(\""a\"" * N + \""b\"" * N + \""c\"" * N)\n        indices = {\n            \""monotonic_incr\"": CategoricalIndex(values),\n            \""monotonic_decr\"": CategoricalIndex(reversed(values)),\n            \""non_monotonic\"": CategoricalIndex(list(\""abc\"" * N)),\n        }\n        self.data = indices[index]\n        self.data_unique = CategoricalIndex([str(i) for i in range(N * 3)])\n    \n        self.int_scalar = 10000\n        self.int_list = list(range(10000))\n    \n        self.cat_scalar = \""b\""\n        self.cat_list = [\""1\"", \""3\""]"", ""indexing.DataFrameStringIndexing.time_at_setitem"": ""class DataFrameStringIndexing:\n    def time_at_setitem(self):\n        self.df.at[self.idx_scalar, self.col_scalar] = 0.0\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DataFrameStringIndexing:\n    def setup(self):\n        index = tm.makeStringIndex(1000)\n        columns = tm.makeStringIndex(30)\n        with warnings.catch_warnings(record=True):\n            self.df = DataFrame(np.random.randn(1000, 30), index=index, columns=columns)\n        self.idx_scalar = index[100]\n        self.col_scalar = columns[10]\n        self.bool_indexer = self.df[self.col_scalar] > 0\n        self.bool_obj_indexer = self.bool_indexer.astype(object)\n        self.boolean_indexer = (self.df[self.col_scalar] > 0).astype(\""boolean\"")"", ""indexing.DatetimeIndexIndexing.time_get_indexer_mismatched_tz"": ""class DatetimeIndexIndexing:\n    def time_get_indexer_mismatched_tz(self):\n        # reached via e.g.\n        #  ser = Series(range(len(dti)), index=dti)\n        #  ser[dti2]\n        self.dti.get_indexer(self.dti2)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndexIndexing:\n    def setup(self):\n        dti = date_range(\""2016-01-01\"", periods=10000, tz=\""US/Pacific\"")\n        dti2 = dti.tz_convert(\""UTC\"")\n        self.dti = dti\n        self.dti2 = dti2"", ""indexing.GetItemSingleColumn.time_frame_getitem_single_column_int"": ""class GetItemSingleColumn:\n    def time_frame_getitem_single_column_int(self):\n        self.df_int_col[0]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GetItemSingleColumn:\n    def setup(self):\n        self.df_string_col = DataFrame(np.random.randn(3000, 1), columns=[\""A\""])\n        self.df_int_col = DataFrame(np.random.randn(3000, 1))"", ""indexing.IndexSingleRow.time_loc_row"": ""class IndexSingleRow:\n    def time_loc_row(self, unique_cols):\n        self.df.loc[10000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IndexSingleRow:\n    def setup(self, unique_cols):\n        arr = np.arange(10**7).reshape(-1, 10)\n        df = DataFrame(arr)\n        dtypes = [\""u1\"", \""u2\"", \""u4\"", \""u8\"", \""i1\"", \""i2\"", \""i4\"", \""i8\"", \""f8\"", \""f4\""]\n        for i, d in enumerate(dtypes):\n            df[i] = df[i].astype(d)\n    \n        if not unique_cols:\n            # GH#33032 single-row lookups with non-unique columns were\n            #  15x slower than with unique columns\n            df.columns = [\""A\"", \""A\""] + list(df.columns[2:])\n    \n        self.df = df"", ""indexing.IntervalIndexing.time_loc_scalar"": ""class IntervalIndexing:\n    def time_loc_scalar(self, monotonic):\n        monotonic.loc[80000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass IntervalIndexing:\n    def setup_cache(self):\n        idx = IntervalIndex.from_breaks(np.arange(1000001))\n        monotonic = Series(np.arange(1000000), index=idx)\n        return monotonic"", ""indexing.MethodLookup.time_lookup_iloc"": ""class MethodLookup:\n    def time_lookup_iloc(self, s):\n        s.iloc\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MethodLookup:\n    def setup_cache(self):\n        s = Series()\n        return s"", ""indexing.MethodLookup.time_lookup_loc"": ""class MethodLookup:\n    def time_lookup_loc(self, s):\n        s.loc\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MethodLookup:\n    def setup_cache(self):\n        s = Series()\n        return s"", ""indexing.MultiIndexing.time_loc_all_null_slices"": ""class MultiIndexing:\n    def time_loc_all_null_slices(self, unique_levels):\n        target = tuple([self.tgt_null_slice] * self.nlevels)\n        self.df.loc[target, :]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MultiIndexing:\n    def setup(self, unique_levels):\n        self.nlevels = 2\n        if unique_levels:\n            mi = MultiIndex.from_arrays([range(1000000)] * self.nlevels)\n        else:\n            mi = MultiIndex.from_product([range(1000)] * self.nlevels)\n        self.df = DataFrame(np.random.randn(len(mi)), index=mi)\n    \n        self.tgt_slice = slice(200, 800)\n        self.tgt_null_slice = slice(None)\n        self.tgt_list = list(range(0, 1000, 10))\n        self.tgt_scalar = 500\n    \n        bool_indexer = np.zeros(len(mi), dtype=np.bool_)\n        bool_indexer[slice(0, len(mi), 100)] = True\n        self.tgt_bool_indexer = bool_indexer"", ""indexing.MultiIndexing.time_loc_all_scalars"": ""class MultiIndexing:\n    def time_loc_all_scalars(self, unique_levels):\n        target = tuple([self.tgt_scalar] * self.nlevels)\n        self.df.loc[target, :]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MultiIndexing:\n    def setup(self, unique_levels):\n        self.nlevels = 2\n        if unique_levels:\n            mi = MultiIndex.from_arrays([range(1000000)] * self.nlevels)\n        else:\n            mi = MultiIndex.from_product([range(1000)] * self.nlevels)\n        self.df = DataFrame(np.random.randn(len(mi)), index=mi)\n    \n        self.tgt_slice = slice(200, 800)\n        self.tgt_null_slice = slice(None)\n        self.tgt_list = list(range(0, 1000, 10))\n        self.tgt_scalar = 500\n    \n        bool_indexer = np.zeros(len(mi), dtype=np.bool_)\n        bool_indexer[slice(0, len(mi), 100)] = True\n        self.tgt_bool_indexer = bool_indexer"", ""indexing.NonNumericSeriesIndexing.time_getitem_label_slice"": ""class NonNumericSeriesIndexing:\n    def time_getitem_label_slice(self, index, index_structure):\n        self.s[: self.lbl]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]"", ""indexing.NonNumericSeriesIndexing.time_getitem_list_like"": ""class NonNumericSeriesIndexing:\n    def time_getitem_list_like(self, index, index_structure):\n        self.s[[self.lbl]]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]"", ""indexing.NonNumericSeriesIndexing.time_getitem_scalar"": ""class NonNumericSeriesIndexing:\n    def time_getitem_scalar(self, index, index_structure):\n        self.s[self.lbl]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NonNumericSeriesIndexing:\n    def setup(self, index, index_structure):\n        N = 10**6\n        if index == \""string\"":\n            index = tm.makeStringIndex(N)\n        elif index == \""datetime\"":\n            index = date_range(\""1900\"", periods=N, freq=\""s\"")\n        elif index == \""period\"":\n            index = period_range(\""1900\"", periods=N, freq=\""s\"")\n        index = index.sort_values()\n        assert index.is_unique and index.is_monotonic_increasing\n        if index_structure == \""nonunique_monotonic_inc\"":\n            index = index.insert(item=index[2], loc=2)[:-1]\n        elif index_structure == \""non_monotonic\"":\n            index = index[::2].append(index[1::2])\n            assert len(index) == N\n        self.s = Series(np.random.rand(N), index=index)\n        self.lbl = index[80000]\n        # warm up index mapping\n        self.s[self.lbl]"", ""indexing.NumericMaskedIndexing.time_get_indexer"": ""class NumericMaskedIndexing:\n    def time_get_indexer(self, dtype, monotonic):\n        self.data.get_indexer(self.indexer)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericMaskedIndexing:\n    def setup(self, dtype, monotonic):\n        indices = {\n            True: Index(self.monotonic_list, dtype=dtype),\n            False: Index(self.non_monotonic_list, dtype=dtype).append(\n                Index([NA], dtype=dtype)\n            ),\n        }\n        self.data = indices[monotonic]\n        self.indexer = np.arange(300, 1_000)\n        self.data_dups = self.data.append(self.data)"", ""indexing.NumericMaskedIndexing.time_get_indexer_dups"": ""class NumericMaskedIndexing:\n    def time_get_indexer_dups(self, dtype, monotonic):\n        self.data.get_indexer_for(self.indexer)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericMaskedIndexing:\n    def setup(self, dtype, monotonic):\n        indices = {\n            True: Index(self.monotonic_list, dtype=dtype),\n            False: Index(self.non_monotonic_list, dtype=dtype).append(\n                Index([NA], dtype=dtype)\n            ),\n        }\n        self.data = indices[monotonic]\n        self.indexer = np.arange(300, 1_000)\n        self.data_dups = self.data.append(self.data)"", ""indexing.NumericSeriesIndexing.time_getitem_array"": ""class NumericSeriesIndexing:\n    def time_getitem_array(self, index, index_structure):\n        self.data[self.array]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_getitem_list_like"": ""class NumericSeriesIndexing:\n    def time_getitem_list_like(self, index, index_structure):\n        self.data[[800000]]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_getitem_lists"": ""class NumericSeriesIndexing:\n    def time_getitem_lists(self, index, index_structure):\n        self.data[self.array_list]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_getitem_scalar"": ""class NumericSeriesIndexing:\n    def time_getitem_scalar(self, index, index_structure):\n        self.data[800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_getitem_slice"": ""class NumericSeriesIndexing:\n    def time_getitem_slice(self, index, index_structure):\n        self.data[:800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_iloc_slice"": ""class NumericSeriesIndexing:\n    def time_iloc_slice(self, index, index_structure):\n        self.data.iloc[:800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_loc_array"": ""class NumericSeriesIndexing:\n    def time_loc_array(self, index, index_structure):\n        self.data.loc[self.array]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_loc_scalar"": ""class NumericSeriesIndexing:\n    def time_loc_scalar(self, index, index_structure):\n        self.data.loc[800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.NumericSeriesIndexing.time_loc_slice"": ""class NumericSeriesIndexing:\n    def time_loc_slice(self, index, index_structure):\n        self.data.loc[:800000]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NumericSeriesIndexing:\n    def setup(self, dtype, index_structure):\n        N = 10**6\n        indices = {\n            \""unique_monotonic_inc\"": Index(range(N), dtype=dtype),\n            \""nonunique_monotonic_inc\"": Index(\n                list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype\n            ),\n        }\n        self.data = Series(np.random.rand(N), index=indices[index_structure])\n        self.array = np.arange(10000)\n        self.array_list = self.array.tolist()"", ""indexing.Take.time_take"": ""class Take:\n    def time_take(self, index):\n        self.s.take(self.indexer)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Take:\n    def setup(self, index):\n        N = 100000\n        indexes = {\n            \""int\"": Index(np.arange(N), dtype=np.int64),\n            \""datetime\"": date_range(\""2011-01-01\"", freq=\""S\"", periods=N),\n        }\n        index = indexes[index]\n        self.s = Series(np.random.rand(N), index=index)\n        self.indexer = np.random.randint(0, N, size=N)""}",indexing,time,0.0
30066,module-level,"terminus-2,oracle",1.027590940682821,1.027590940682821,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.excel,io.excel,"{""io.excel.ReadExcel.time_read_excel"": ""class ReadExcel:\n    def time_read_excel(self, engine):\n        if engine == \""odf\"":\n            fname = self.fname_odf\n        else:\n            fname = self.fname_excel\n        read_excel(fname, engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadExcel:\n    def setup_cache(self):\n        self.df = _generate_dataframe()\n    \n        self.df.to_excel(self.fname_excel, sheet_name=\""Sheet1\"")\n        self._create_odf()""}",io.excel,time,0.0
30055,module-level,"terminus-2,oracle",1.0515576205688943,1.0515576205688943,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_ctor,frame_ctor,"{""frame_ctor.FromArrays.time_frame_from_arrays_sparse"": ""class FromArrays:\n    def time_frame_from_arrays_sparse(self):\n        self.df = DataFrame._from_arrays(\n            self.sparse_arrays,\n            index=self.index,\n            columns=self.columns,\n            verify_integrity=False,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromArrays:\n    def setup(self):\n        N_rows = 1000\n        N_cols = 1000\n        self.float_arrays = [np.random.randn(N_rows) for _ in range(N_cols)]\n        self.sparse_arrays = [\n            pd.arrays.SparseArray(np.random.randint(0, 2, N_rows), dtype=\""float64\"")\n            for _ in range(N_cols)\n        ]\n        self.int_arrays = [\n            pd.array(np.random.randint(1000, size=N_rows), dtype=\""Int64\"")\n            for _ in range(N_cols)\n        ]\n        self.index = pd.Index(range(N_rows))\n        self.columns = pd.Index(range(N_cols))"", ""frame_ctor.FromDicts.time_dict_of_categoricals"": ""class FromDicts:\n    def time_dict_of_categoricals(self):\n        # dict of arrays that we won't consolidate\n        DataFrame(self.dict_of_categoricals)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDicts:\n    def setup(self):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n        frame = DataFrame(np.random.randn(N, K), index=self.index, columns=self.columns)\n        self.data = frame.to_dict()\n        self.dict_list = frame.to_dict(orient=\""records\"")\n        self.data2 = {i: {j: float(j) for j in range(100)} for i in range(2000)}\n    \n        # arrays which we won't consolidate\n        self.dict_of_categoricals = {i: Categorical(np.arange(N)) for i in range(K)}"", ""frame_ctor.FromDicts.time_list_of_dict"": ""class FromDicts:\n    def time_list_of_dict(self):\n        DataFrame(self.dict_list)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDicts:\n    def setup(self):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n        frame = DataFrame(np.random.randn(N, K), index=self.index, columns=self.columns)\n        self.data = frame.to_dict()\n        self.dict_list = frame.to_dict(orient=\""records\"")\n        self.data2 = {i: {j: float(j) for j in range(100)} for i in range(2000)}\n    \n        # arrays which we won't consolidate\n        self.dict_of_categoricals = {i: Categorical(np.arange(N)) for i in range(K)}"", ""frame_ctor.FromDicts.time_nested_dict_int64"": ""class FromDicts:\n    def time_nested_dict_int64(self):\n        # nested dict, integer indexes, regression described in #621\n        DataFrame(self.data2)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDicts:\n    def setup(self):\n        N, K = 5000, 50\n        self.index = tm.makeStringIndex(N)\n        self.columns = tm.makeStringIndex(K)\n        frame = DataFrame(np.random.randn(N, K), index=self.index, columns=self.columns)\n        self.data = frame.to_dict()\n        self.dict_list = frame.to_dict(orient=\""records\"")\n        self.data2 = {i: {j: float(j) for j in range(100)} for i in range(2000)}\n    \n        # arrays which we won't consolidate\n        self.dict_of_categoricals = {i: Categorical(np.arange(N)) for i in range(K)}"", ""frame_ctor.FromDictwithTimestamp.time_dict_with_timestamp_offsets"": ""class FromDictwithTimestamp:\n    def time_dict_with_timestamp_offsets(self, offset):\n        DataFrame(self.d)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromDictwithTimestamp:\n    def setup(self, offset):\n        N = 10**3\n        idx = date_range(Timestamp(\""1/1/1900\""), freq=offset, periods=N)\n        df = DataFrame(np.random.randn(N, 10), index=idx)\n        self.d = df.to_dict()"", ""frame_ctor.FromLists.time_frame_from_lists"": ""class FromLists:\n    def time_frame_from_lists(self):\n        self.df = DataFrame(self.data)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromLists:\n    def setup(self):\n        N = 1000\n        M = 100\n        self.data = [list(range(M)) for i in range(N)]"", ""frame_ctor.FromRecords.time_frame_from_records_generator"": ""class FromRecords:\n    def time_frame_from_records_generator(self, nrows):\n        # issue-6700\n        self.df = DataFrame.from_records(self.gen, nrows=nrows)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromRecords:\n    def setup(self, nrows):\n        N = 100000\n        self.gen = ((x, (x * 20), (x * 100)) for x in range(N))"", ""frame_ctor.FromScalar.time_frame_from_scalar_ea_float64"": ""class FromScalar:\n    def time_frame_from_scalar_ea_float64(self):\n        DataFrame(\n            1.0,\n            index=range(self.nrows),\n            columns=list(\""abc\""),\n            dtype=Float64Dtype(),\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromScalar:\n    def setup(self):\n        self.nrows = 100_000"", ""frame_ctor.FromScalar.time_frame_from_scalar_ea_float64_na"": ""class FromScalar:\n    def time_frame_from_scalar_ea_float64_na(self):\n        DataFrame(\n            NA,\n            index=range(self.nrows),\n            columns=list(\""abc\""),\n            dtype=Float64Dtype(),\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FromScalar:\n    def setup(self):\n        self.nrows = 100_000""}",frame_ctor,time,0.0
30059,module-level,"terminus-2,oracle",1.060974938794787,1.060974938794787,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,hash_functions,hash_functions,"{""hash_functions.NumericSeriesIndexing.time_loc_slice"": ""class NumericSeriesIndexing:\n    def time_loc_slice(self, index, N):\n        # trigger building of mapping\n        self.data.loc[:800]\n\n    def setup(self, dtype, N):\n        vals = np.array(list(range(55)) + [54] + list(range(55, N - 1)), dtype=dtype)\n        indices = pd.Index(vals)\n        self.data = pd.Series(np.arange(N), index=indices)"", ""hash_functions.UniqueAndFactorizeArange.time_factorize"": ""class UniqueAndFactorizeArange:\n    def time_factorize(self, exponent):\n        pd.factorize(self.a2)\n\n    def setup(self, exponent):\n        a = np.arange(10**4, dtype=\""float64\"")\n        self.a2 = (a + 10**exponent).repeat(100)""}",hash_functions,time,0.0
30042,module-level,"terminus-2,gpt-5",1.024648415455555,1.0298420080843886,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.tslib,tslibs.tslib,"{""tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime"": ""class TimeIntsToPydatetime:\n    def time_ints_to_pydatetime(self, box, size, tz):\n        ints_to_pydatetime(self.i8data, tz, box=box)\n\n    def setup(self, box, size, tz):\n        if box == \""date\"" and tz is not None:\n            # tz is ignored, so avoid running redundant benchmarks\n            raise NotImplementedError  # skip benchmark\n        if size == 10**6 and tz is _tzs[-1]:\n            # This is cumbersomely-slow, so skip to trim runtime\n            raise NotImplementedError  # skip benchmark\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.tslib,time,-0.0036729792283122812
30051,module-level,"terminus-2,oracle",0.9130871086097458,0.9130871086097458,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,ctors,ctors,"{""ctors.DatetimeIndexConstructor.time_from_list_of_dates"": ""class DatetimeIndexConstructor:\n    def time_from_list_of_dates(self):\n        DatetimeIndex(self.list_of_dates)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndexConstructor:\n    def setup(self):\n        N = 20_000\n        dti = date_range(\""1900-01-01\"", periods=N)\n    \n        self.list_of_timestamps = dti.tolist()\n        self.list_of_dates = dti.date.tolist()\n        self.list_of_datetimes = dti.to_pydatetime().tolist()\n        self.list_of_str = dti.strftime(\""%Y-%m-%d\"").tolist()"", ""ctors.SeriesConstructors.time_series_constructor"": ""class SeriesConstructors:\n    def time_series_constructor(self, data_fmt, with_index, dtype):\n        Series(self.data, index=self.index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesConstructors:\n    def setup(self, data_fmt, with_index, dtype):\n        if data_fmt in (gen_of_str, gen_of_tuples) and with_index:\n            raise NotImplementedError(\n                \""Series constructors do not support using generators with indexes\""\n            )\n        N = 10**4\n        if dtype == \""float\"":\n            arr = np.random.randn(N)\n        else:\n            arr = np.arange(N)\n        self.data = data_fmt(arr)\n        self.index = np.arange(N) if with_index else None""}",ctors,time,0.0
30067,module-level,"terminus-2,oracle",1.1630325669190456,1.1630325669190456,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.hdf,io.hdf,"{""io.hdf.HDFStoreDataFrame.time_query_store_table"": ""class HDFStoreDataFrame:\n    def time_query_store_table(self):\n        self.store.select(\""table\"", where=\""index > self.start and index < self.stop\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass HDFStoreDataFrame:\n    def setup(self):\n        N = 25000\n        index = tm.makeStringIndex(N)\n        self.df = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)}, index=index\n        )\n        self.df_mixed = DataFrame(\n            {\n                \""float1\"": np.random.randn(N),\n                \""float2\"": np.random.randn(N),\n                \""string1\"": [\""foo\""] * N,\n                \""bool1\"": [True] * N,\n                \""int1\"": np.random.randint(0, N, size=N),\n            },\n            index=index,\n        )\n        self.df_wide = DataFrame(np.random.randn(N, 100))\n        self.start_wide = self.df_wide.index[10000]\n        self.stop_wide = self.df_wide.index[15000]\n        self.df2 = DataFrame(\n            {\""float1\"": np.random.randn(N), \""float2\"": np.random.randn(N)},\n            index=date_range(\""1/1/2000\"", periods=N),\n        )\n        self.start = self.df2.index[10000]\n        self.stop = self.df2.index[15000]\n        self.df_wide2 = DataFrame(\n            np.random.randn(N, 100), index=date_range(\""1/1/2000\"", periods=N)\n        )\n        self.df_dc = DataFrame(\n            np.random.randn(N, 10), columns=[f\""C{i:03d}\"" for i in range(10)]\n        )\n    \n        self.fname = \""__test__.h5\""\n    \n        self.store = HDFStore(self.fname)\n        self.store.put(\""fixed\"", self.df)\n        self.store.put(\""fixed_mixed\"", self.df_mixed)\n        self.store.append(\""table\"", self.df2)\n        self.store.append(\""table_mixed\"", self.df_mixed)\n        self.store.append(\""table_wide\"", self.df_wide)\n        self.store.append(\""table_wide2\"", self.df_wide2)""}",io.hdf,time,0.0
30040,module-level,"terminus-2,gpt-5",1.027340473221818,1.023426105382882,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timedelta,tslibs.timedelta,"{""tslibs.timedelta.TimedeltaConstructor.time_from_components"": ""class TimedeltaConstructor:\n    def time_from_components(self):\n        Timedelta(\n            days=1,\n            hours=2,\n            minutes=3,\n            seconds=4,\n            milliseconds=5,\n            microseconds=6,\n            nanoseconds=7,\n        )\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")"", ""tslibs.timedelta.TimedeltaConstructor.time_from_datetime_timedelta"": ""class TimedeltaConstructor:\n    def time_from_datetime_timedelta(self):\n        Timedelta(self.dttimedelta)\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")"", ""tslibs.timedelta.TimedeltaConstructor.time_from_int"": ""class TimedeltaConstructor:\n    def time_from_int(self):\n        Timedelta(123456789)\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")"", ""tslibs.timedelta.TimedeltaConstructor.time_from_iso_format"": ""class TimedeltaConstructor:\n    def time_from_iso_format(self):\n        Timedelta(\""P4DT12H30M5S\"")\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")"", ""tslibs.timedelta.TimedeltaConstructor.time_from_missing"": ""class TimedeltaConstructor:\n    def time_from_missing(self):\n        Timedelta(\""nat\"")\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")"", ""tslibs.timedelta.TimedeltaConstructor.time_from_np_timedelta"": ""class TimedeltaConstructor:\n    def time_from_np_timedelta(self):\n        Timedelta(self.nptimedelta64)\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")"", ""tslibs.timedelta.TimedeltaConstructor.time_from_string"": ""class TimedeltaConstructor:\n    def time_from_string(self):\n        Timedelta(\""1 days\"")\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")""}",tslibs.timedelta,time,0.0027682940869421307
30043,module-level,"terminus-2,gpt-5",1.020449862197934,0.9598299359561444,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.tz_convert,tslibs.tz_convert,"{""tslibs.tz_convert.TimeTZConvert.time_tz_convert_from_utc"": ""class TimeTZConvert:\n    def time_tz_convert_from_utc(self, size, tz):\n        # effectively:\n        #  dti = DatetimeIndex(self.i8data, tz=tz)\n        #  dti.tz_localize(None)\n        if old_sig:\n            tz_convert_from_utc(self.i8data, UTC, tz)\n        else:\n            tz_convert_from_utc(self.i8data, tz)\n\n    def setup(self, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr"", ""tslibs.tz_convert.TimeTZConvert.time_tz_localize_to_utc"": ""class TimeTZConvert:\n    def time_tz_localize_to_utc(self, size, tz):\n        # effectively:\n        #  dti = DatetimeIndex(self.i8data)\n        #  dti.tz_localize(tz, ambiguous=\""NaT\"", nonexistent=\""NaT\"")\n        tz_localize_to_utc(self.i8data, tz, ambiguous=\""NaT\"", nonexistent=\""NaT\"")\n\n    def setup(self, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.tz_convert,time,0.04287123496590502
30063,module-level,"terminus-2,oracle",1.0259987578359806,1.0259987578359806,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,indexing_engines,indexing_engines,"{""indexing_engines.MaskedNumericEngineIndexing.time_get_loc"": ""class MaskedNumericEngineIndexing:\n    def time_get_loc(self, engine_and_dtype, index_type, unique, N):\n        self.data.get_loc(self.key_early)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())[::-1]\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.zeros(N * 3, dtype=dtype.lower())\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype.lower())\n                arr[N:] = np.arange(N * 2, dtype=dtype.lower())\n    \n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n            mask[-1] = True\n    \n        self.data = engine(BaseMaskedArray(arr, mask))\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]"", ""indexing_engines.MaskedNumericEngineIndexing.time_get_loc_near_middle"": ""class MaskedNumericEngineIndexing:\n    def time_get_loc_near_middle(self, engine_and_dtype, index_type, unique, N):\n        # searchsorted performance may be different near the middle of a range\n        #  vs near an endpoint\n        self.data.get_loc(self.key_middle)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype.lower())[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype.lower())[::-1]\n            mask = np.zeros(N * 3, dtype=np.bool_)\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.zeros(N * 3, dtype=dtype.lower())\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype.lower())\n                arr[N:] = np.arange(N * 2, dtype=dtype.lower())\n    \n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype.lower())\n            mask = np.zeros(N * 3, dtype=np.bool_)\n            mask[-1] = True\n    \n        self.data = engine(BaseMaskedArray(arr, mask))\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]"", ""indexing_engines.NumericEngineIndexing.time_get_loc"": ""class NumericEngineIndexing:\n    def time_get_loc(self, engine_and_dtype, index_type, unique, N):\n        self.data.get_loc(self.key_early)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)[::-1]\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.empty(N * 3, dtype=dtype)\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype)\n                arr[N:] = np.arange(N * 2, dtype=dtype)\n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype)\n    \n        self.data = engine(arr)\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]"", ""indexing_engines.NumericEngineIndexing.time_get_loc_near_middle"": ""class NumericEngineIndexing:\n    def time_get_loc_near_middle(self, engine_and_dtype, index_type, unique, N):\n        # searchsorted performance may be different near the middle of a range\n        #  vs near an endpoint\n        self.data.get_loc(self.key_middle)\n\n    def setup(self, engine_and_dtype, index_type, unique, N):\n        engine, dtype = engine_and_dtype\n    \n        if index_type == \""monotonic_incr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)\n        elif index_type == \""monotonic_decr\"":\n            if unique:\n                arr = np.arange(N * 3, dtype=dtype)[::-1]\n            else:\n                values = list([1] * N + [2] * N + [3] * N)\n                arr = np.array(values, dtype=dtype)[::-1]\n        else:\n            assert index_type == \""non_monotonic\""\n            if unique:\n                arr = np.empty(N * 3, dtype=dtype)\n                arr[:N] = np.arange(N * 2, N * 3, dtype=dtype)\n                arr[N:] = np.arange(N * 2, dtype=dtype)\n            else:\n                arr = np.array([1, 2, 3] * N, dtype=dtype)\n    \n        self.data = engine(arr)\n        # code belows avoids populating the mapping etc. while timing.\n        self.data.get_loc(2)\n    \n        self.key_middle = arr[len(arr) // 2]\n        self.key_early = arr[2]""}",indexing_engines,time,0.0
30069,module-level,"terminus-2,oracle",0.9727904499500332,0.9727904499500332,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.parsers,io.parsers,"{""io.parsers.ConcatDateCols.time_check_concat"": ""class ConcatDateCols:\n    def time_check_concat(self, value, dim):\n        concat_date_cols(self.object)\n\n    def setup(self, value, dim):\n        count_elem = 10000\n        if dim == 1:\n            self.object = (np.array([value] * count_elem),)\n        if dim == 2:\n            self.object = (\n                np.array([value] * count_elem),\n                np.array([value] * count_elem),\n            )"", ""io.parsers.DoesStringLookLikeDatetime.time_check_datetimes"": ""class DoesStringLookLikeDatetime:\n    def time_check_datetimes(self, value):\n        for obj in self.objects:\n            _does_string_look_like_datetime(obj)\n\n    def setup(self, value):\n        self.objects = [value] * 1000000""}",io.parsers,time,0.0
30058,module-level,"terminus-2,oracle",1.0518611893626415,1.0518611893626415,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,groupby,groupby,"{""groupby.Apply.time_scalar_function_multi_col"": ""class Apply:\n    def time_scalar_function_multi_col(self, factor):\n        self.df.groupby([\""key\"", \""key2\""]).apply(lambda x: 1)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, factor):\n        N = 10**factor\n        # two cases:\n        # - small groups: small data (N**4) + many labels (2000) -> average group\n        #   size of 5 (-> larger overhead of slicing method)\n        # - larger groups: larger data (N**5) + fewer labels (20) -> average group\n        #   size of 5000\n        labels = np.random.randint(0, 2000 if factor == 4 else 20, size=N)\n        labels2 = np.random.randint(0, 3, size=N)\n        df = DataFrame(\n            {\n                \""key\"": labels,\n                \""key2\"": labels2,\n                \""value1\"": np.random.randn(N),\n                \""value2\"": [\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (N // 4),\n            }\n        )\n        self.df = df"", ""groupby.ApplyNonUniqueUnsortedIndex.time_groupby_apply_non_unique_unsorted_index"": ""class ApplyNonUniqueUnsortedIndex:\n    def time_groupby_apply_non_unique_unsorted_index(self):\n        self.df.groupby(\""key\"", group_keys=False).apply(lambda x: x)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ApplyNonUniqueUnsortedIndex:\n    def setup(self):\n        # GH 46527\n        # unsorted and non-unique index\n        idx = np.arange(100)[::-1]\n        idx = Index(np.repeat(idx, 200), name=\""key\"")\n        self.df = DataFrame(np.random.randn(len(idx), 10), index=idx)"", ""groupby.Cumulative.time_frame_transform"": ""class Cumulative:\n    def time_frame_transform(self, dtype, method, with_nans):\n        self.df.groupby(\""key\"").transform(method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cumulative:\n    def setup(self, dtype, method, with_nans):\n        if with_nans and dtype == \""int64\"":\n            raise NotImplementedError(\""Construction of df would raise\"")\n    \n        N = 500_000\n        keys = np.random.randint(0, 100, size=N)\n        vals = np.random.randint(-10, 10, (N, 5))\n    \n        if with_nans:\n            null_vals = vals.astype(float, copy=True)\n            null_vals[::2, :] = np.nan\n            null_vals[::3, :] = np.nan\n            df = DataFrame(null_vals, columns=list(\""abcde\""), dtype=dtype)\n            df[\""key\""] = keys\n            self.df = df\n        else:\n            df = DataFrame(vals, columns=list(\""abcde\"")).astype(dtype, copy=False)\n            df[\""key\""] = keys\n            self.df = df"", ""groupby.GroupByMethods.time_dtype_as_field"": ""class GroupByMethods:\n    def time_dtype_as_field(self, dtype, method, application, ncols):\n        self.as_field_method()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupByMethods:\n    def setup(self, dtype, method, application, ncols):\n        if method in method_blocklist.get(dtype, {}):\n            raise NotImplementedError  # skip benchmark\n    \n        if ncols != 1 and method in [\""value_counts\"", \""unique\""]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if application == \""transformation\"" and method in [\n            \""describe\"",\n            \""head\"",\n            \""tail\"",\n            \""unique\"",\n            \""value_counts\"",\n            \""size\"",\n        ]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if method == \""describe\"":\n            ngroups = 20\n        elif method == \""skew\"":\n            ngroups = 100\n        else:\n            ngroups = 1000\n        size = ngroups * 2\n        rng = np.arange(ngroups).reshape(-1, 1)\n        rng = np.broadcast_to(rng, (len(rng), ncols))\n        taker = np.random.randint(0, ngroups, size=size)\n        values = rng.take(taker, axis=0)\n        if dtype == \""int\"":\n            key = np.random.randint(0, size, size=size)\n        elif dtype in (\""int16\"", \""uint\""):\n            key = np.random.randint(0, size, size=size, dtype=dtype)\n        elif dtype == \""float\"":\n            key = np.concatenate(\n                [np.random.random(ngroups) * 0.1, np.random.random(ngroups) * 10.0]\n            )\n        elif dtype == \""object\"":\n            key = [\""foo\""] * size\n        elif dtype == \""datetime\"":\n            key = date_range(\""1/1/2011\"", periods=size, freq=\""s\"")\n    \n        cols = [f\""values{n}\"" for n in range(ncols)]\n        df = DataFrame(values, columns=cols)\n        df[\""key\""] = key\n    \n        if len(cols) == 1:\n            cols = cols[0]\n    \n        if application == \""transformation\"":\n            self.as_group_method = lambda: df.groupby(\""key\"")[cols].transform(method)\n            self.as_field_method = lambda: df.groupby(cols)[\""key\""].transform(method)\n        else:\n            self.as_group_method = getattr(df.groupby(\""key\"")[cols], method)\n            self.as_field_method = getattr(df.groupby(cols)[\""key\""], method)"", ""groupby.GroupByMethods.time_dtype_as_group"": ""class GroupByMethods:\n    def time_dtype_as_group(self, dtype, method, application, ncols):\n        self.as_group_method()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupByMethods:\n    def setup(self, dtype, method, application, ncols):\n        if method in method_blocklist.get(dtype, {}):\n            raise NotImplementedError  # skip benchmark\n    \n        if ncols != 1 and method in [\""value_counts\"", \""unique\""]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if application == \""transformation\"" and method in [\n            \""describe\"",\n            \""head\"",\n            \""tail\"",\n            \""unique\"",\n            \""value_counts\"",\n            \""size\"",\n        ]:\n            # DataFrameGroupBy doesn't have these methods\n            raise NotImplementedError\n    \n        if method == \""describe\"":\n            ngroups = 20\n        elif method == \""skew\"":\n            ngroups = 100\n        else:\n            ngroups = 1000\n        size = ngroups * 2\n        rng = np.arange(ngroups).reshape(-1, 1)\n        rng = np.broadcast_to(rng, (len(rng), ncols))\n        taker = np.random.randint(0, ngroups, size=size)\n        values = rng.take(taker, axis=0)\n        if dtype == \""int\"":\n            key = np.random.randint(0, size, size=size)\n        elif dtype in (\""int16\"", \""uint\""):\n            key = np.random.randint(0, size, size=size, dtype=dtype)\n        elif dtype == \""float\"":\n            key = np.concatenate(\n                [np.random.random(ngroups) * 0.1, np.random.random(ngroups) * 10.0]\n            )\n        elif dtype == \""object\"":\n            key = [\""foo\""] * size\n        elif dtype == \""datetime\"":\n            key = date_range(\""1/1/2011\"", periods=size, freq=\""s\"")\n    \n        cols = [f\""values{n}\"" for n in range(ncols)]\n        df = DataFrame(values, columns=cols)\n        df[\""key\""] = key\n    \n        if len(cols) == 1:\n            cols = cols[0]\n    \n        if application == \""transformation\"":\n            self.as_group_method = lambda: df.groupby(\""key\"")[cols].transform(method)\n            self.as_field_method = lambda: df.groupby(cols)[\""key\""].transform(method)\n        else:\n            self.as_group_method = getattr(df.groupby(\""key\"")[cols], method)\n            self.as_field_method = getattr(df.groupby(cols)[\""key\""], method)"", ""groupby.GroupManyLabels.time_sum"": ""class GroupManyLabels:\n    def time_sum(self, ncols):\n        self.df.groupby(self.labels).sum()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupManyLabels:\n    def setup(self, ncols):\n        N = 1000\n        data = np.random.randn(N, ncols)\n        self.labels = np.random.randint(0, 100, size=N)\n        self.df = DataFrame(data)"", ""groupby.Groups.time_series_groups"": ""class Groups:\n    def time_series_groups(self, data, key):\n        self.ser.groupby(self.ser).groups\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Groups:\n    def setup(self, data, key):\n        self.ser = data[key]\n\n    def setup_cache(self):\n        size = 10**6\n        data = {\n            \""int64_small\"": Series(np.random.randint(0, 100, size=size)),\n            \""int64_large\"": Series(np.random.randint(0, 10000, size=size)),\n            \""object_small\"": Series(\n                tm.makeStringIndex(100).take(np.random.randint(0, 100, size=size))\n            ),\n            \""object_large\"": Series(\n                tm.makeStringIndex(10000).take(np.random.randint(0, 10000, size=size))\n            ),\n        }\n        return data"", ""groupby.RankWithTies.time_rank_ties"": ""class RankWithTies:\n    def time_rank_ties(self, dtype, tie_method):\n        self.df.groupby(\""key\"").rank(method=tie_method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass RankWithTies:\n    def setup(self, dtype, tie_method):\n        N = 10**4\n        if dtype == \""datetime64\"":\n            data = np.array([Timestamp(\""2011/01/01\"")] * N, dtype=dtype)\n        else:\n            data = np.array([1] * N, dtype=dtype)\n        self.df = DataFrame({\""values\"": data, \""key\"": [\""foo\""] * N})"", ""groupby.Size.time_category_size"": ""class Size:\n    def time_category_size(self):\n        self.draws.groupby(self.cats).size()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Size:\n    def setup(self):\n        n = 10**5\n        offsets = np.random.randint(n, size=n).astype(\""timedelta64[ns]\"")\n        dates = np.datetime64(\""now\"") + offsets\n        self.df = DataFrame(\n            {\n                \""key1\"": np.random.randint(0, 500, size=n),\n                \""key2\"": np.random.randint(0, 100, size=n),\n                \""value1\"": np.random.randn(n),\n                \""value2\"": np.random.randn(n),\n                \""value3\"": np.random.randn(n),\n                \""dates\"": dates,\n            }\n        )\n        self.draws = Series(np.random.randn(n))\n        labels = Series([\""foo\"", \""bar\"", \""baz\"", \""qux\""] * (n // 4))\n        self.cats = labels.astype(\""category\"")"", ""groupby.String.time_str_func"": ""class String:\n    def time_str_func(self, dtype, method):\n        self.df.groupby(\""a\"")[self.df.columns[1:]].agg(method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass String:\n    def setup(self, dtype, method):\n        cols = list(\""abcdefghjkl\"")\n        self.df = DataFrame(\n            np.random.randint(0, 100, size=(10_000, len(cols))),\n            columns=cols,\n            dtype=dtype,\n        )"", ""groupby.Transform.time_transform_lambda_max_wide"": ""class Transform:\n    def time_transform_lambda_max_wide(self):\n        self.df_wide.groupby(level=0).transform(lambda x: np.max(x, axis=0))\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Transform:\n    def setup(self):\n        n1 = 400\n        n2 = 250\n        index = MultiIndex(\n            levels=[np.arange(n1), tm.makeStringIndex(n2)],\n            codes=[np.repeat(range(n1), n2).tolist(), list(range(n2)) * n1],\n            names=[\""lev1\"", \""lev2\""],\n        )\n        arr = np.random.randn(n1 * n2, 3)\n        arr[::10000, 0] = np.nan\n        arr[1::10000, 1] = np.nan\n        arr[2::10000, 2] = np.nan\n        data = DataFrame(arr, index=index, columns=[\""col1\"", \""col20\"", \""col3\""])\n        self.df = data\n    \n        n = 1000\n        self.df_wide = DataFrame(\n            np.random.randn(n, n),\n            index=np.random.choice(range(10), n),\n        )\n    \n        n = 1_000_000\n        self.df_tall = DataFrame(\n            np.random.randn(n, 3),\n            index=np.random.randint(0, 5, n),\n        )\n    \n        n = 20000\n        self.df1 = DataFrame(\n            np.random.randint(1, n, (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df2 = self.df1.copy()\n        self.df2[\""jim\""] = self.df2[\""joe\""]\n    \n        self.df3 = DataFrame(\n            np.random.randint(1, (n / 10), (n, 3)), columns=[\""jim\"", \""joe\"", \""jolie\""]\n        )\n        self.df4 = self.df3.copy()\n        self.df4[\""jim\""] = self.df4[\""joe\""]""}",groupby,time,0.0
30056,module-level,"terminus-2,oracle",1.0976031904766277,1.0976031904766277,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,frame_methods,frame_methods,"{""frame_methods.Apply.time_apply_axis_1"": ""class Apply:\n    def time_apply_axis_1(self):\n        self.df.apply(lambda x: x + 1, axis=1)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self):\n        self.df = DataFrame(np.random.randn(1000, 100))\n    \n        self.s = Series(np.arange(1028.0))\n        self.df2 = DataFrame({i: self.s for i in range(1028)})\n        self.df3 = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))"", ""frame_methods.Apply.time_apply_lambda_mean"": ""class Apply:\n    def time_apply_lambda_mean(self):\n        self.df.apply(lambda x: x.mean())\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self):\n        self.df = DataFrame(np.random.randn(1000, 100))\n    \n        self.s = Series(np.arange(1028.0))\n        self.df2 = DataFrame({i: self.s for i in range(1028)})\n        self.df3 = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))"", ""frame_methods.Apply.time_apply_np_mean"": ""class Apply:\n    def time_apply_np_mean(self):\n        self.df.apply(np.mean)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self):\n        self.df = DataFrame(np.random.randn(1000, 100))\n    \n        self.s = Series(np.arange(1028.0))\n        self.df2 = DataFrame({i: self.s for i in range(1028)})\n        self.df3 = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))"", ""frame_methods.Apply.time_apply_pass_thru"": ""class Apply:\n    def time_apply_pass_thru(self):\n        self.df.apply(lambda x: x)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self):\n        self.df = DataFrame(np.random.randn(1000, 100))\n    \n        self.s = Series(np.arange(1028.0))\n        self.df2 = DataFrame({i: self.s for i in range(1028)})\n        self.df3 = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))"", ""frame_methods.Count.time_count_level_mixed_dtypes_multi"": ""class Count:\n    def time_count_level_mixed_dtypes_multi(self, axis):\n        self.df_mixed.count(axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Count:\n    def setup(self, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\""\n    \n        self.df.index = MultiIndex.from_arrays([self.df.index, self.df.index])\n        self.df.columns = MultiIndex.from_arrays([self.df.columns, self.df.columns])\n        self.df_mixed.index = MultiIndex.from_arrays(\n            [self.df_mixed.index, self.df_mixed.index]\n        )\n        self.df_mixed.columns = MultiIndex.from_arrays(\n            [self.df_mixed.columns, self.df_mixed.columns]\n        )"", ""frame_methods.Describe.time_dataframe_describe"": ""class Describe:\n    def time_dataframe_describe(self):\n        self.df.describe()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Describe:\n    def setup(self):\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(0, 100, 10**6),\n                \""b\"": np.random.randint(0, 100, 10**6),\n                \""c\"": np.random.randint(0, 100, 10**6),\n            }\n        )"", ""frame_methods.Describe.time_series_describe"": ""class Describe:\n    def time_series_describe(self):\n        self.df[\""a\""].describe()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Describe:\n    def setup(self):\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(0, 100, 10**6),\n                \""b\"": np.random.randint(0, 100, 10**6),\n                \""c\"": np.random.randint(0, 100, 10**6),\n            }\n        )"", ""frame_methods.Dropna.time_dropna"": ""class Dropna:\n    def time_dropna(self, how, axis):\n        self.df.dropna(how=how, axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Dropna:\n    def setup(self, how, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\"""", ""frame_methods.Dropna.time_dropna_axis_mixed_dtypes"": ""class Dropna:\n    def time_dropna_axis_mixed_dtypes(self, how, axis):\n        self.df_mixed.dropna(how=how, axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Dropna:\n    def setup(self, how, axis):\n        self.df = DataFrame(np.random.randn(10000, 1000))\n        self.df.iloc[50:1000, 20:50] = np.nan\n        self.df.iloc[2000:3000] = np.nan\n        self.df.iloc[:, 60:70] = np.nan\n        self.df_mixed = self.df.copy()\n        self.df_mixed[\""foo\""] = \""bar\"""", ""frame_methods.Duplicated.time_frame_duplicated_wide"": ""class Duplicated:\n    def time_frame_duplicated_wide(self):\n        self.df2.duplicated()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Duplicated:\n    def setup(self):\n        n = 1 << 20\n        t = date_range(\""2015-01-01\"", freq=\""S\"", periods=(n // 64))\n        xs = np.random.randn(n // 64).round(2)\n        self.df = DataFrame(\n            {\n                \""a\"": np.random.randint(-1 << 8, 1 << 8, n),\n                \""b\"": np.random.choice(t, n),\n                \""c\"": np.random.choice(xs, n),\n            }\n        )\n        self.df2 = DataFrame(np.random.randn(1000, 100).astype(str)).T"", ""frame_methods.Equals.time_frame_float_equal"": ""class Equals:\n    def time_frame_float_equal(self):\n        self.float_df.equals(self.float_df)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Equals:\n    def setup(self):\n        N = 10**3\n        self.float_df = DataFrame(np.random.randn(N, N))\n        self.float_df_nan = self.float_df.copy()\n        self.float_df_nan.iloc[-1, -1] = np.nan\n    \n        self.object_df = DataFrame(\""foo\"", index=range(N), columns=range(N))\n        self.object_df_nan = self.object_df.copy()\n        self.object_df_nan.iloc[-1, -1] = np.nan\n    \n        self.nonunique_cols = self.object_df.copy()\n        self.nonunique_cols.columns = [\""A\""] * len(self.nonunique_cols.columns)\n        self.nonunique_cols_nan = self.nonunique_cols.copy()\n        self.nonunique_cols_nan.iloc[-1, -1] = np.nan"", ""frame_methods.Fillna.time_frame_fillna"": ""class Fillna:\n    def time_frame_fillna(self, inplace, method, dtype):\n        self.df.fillna(inplace=inplace, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Fillna:\n    def setup(self, inplace, method, dtype):\n        N, M = 10000, 100\n        if dtype in (\""datetime64[ns]\"", \""datetime64[ns, tz]\"", \""timedelta64[ns]\""):\n            data = {\n                \""datetime64[ns]\"": date_range(\""2011-01-01\"", freq=\""H\"", periods=N),\n                \""datetime64[ns, tz]\"": date_range(\n                    \""2011-01-01\"", freq=\""H\"", periods=N, tz=\""Asia/Tokyo\""\n                ),\n                \""timedelta64[ns]\"": timedelta_range(start=\""1 day\"", periods=N, freq=\""1D\""),\n            }\n            self.df = DataFrame({f\""col_{i}\"": data[dtype] for i in range(M)})\n            self.df[::2] = None\n        else:\n            values = np.random.randn(N, M)\n            values[::2] = np.nan\n            if dtype == \""Int64\"":\n                values = values.round()\n            self.df = DataFrame(values, dtype=dtype)"", ""frame_methods.Interpolate.time_interpolate_some_good"": ""class Interpolate:\n    def time_interpolate_some_good(self, downcast):\n        self.df2.interpolate(downcast=downcast)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Interpolate:\n    def setup(self, downcast):\n        N = 10000\n        # this is the worst case, where every column has NaNs.\n        arr = np.random.randn(N, 100)\n        # NB: we need to set values in array, not in df.values, otherwise\n        #  the benchmark will be misleading for ArrayManager\n        arr[::2] = np.nan\n    \n        self.df = DataFrame(arr)\n    \n        self.df2 = DataFrame(\n            {\n                \""A\"": np.arange(0, N),\n                \""B\"": np.random.randint(0, 100, N),\n                \""C\"": np.random.randn(N),\n                \""D\"": np.random.randn(N),\n            }\n        )\n        self.df2.loc[1::5, \""A\""] = np.nan\n        self.df2.loc[1::5, \""C\""] = np.nan"", ""frame_methods.Iteration.time_items"": ""class Iteration:\n    def time_items(self):\n        # (monitor no-copying behaviour)\n        if hasattr(self.df, \""_item_cache\""):\n            self.df._item_cache.clear()\n        for name, col in self.df.items():\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))"", ""frame_methods.Iteration.time_items_cached"": ""class Iteration:\n    def time_items_cached(self):\n        for name, col in self.df.items():\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))"", ""frame_methods.Iteration.time_iteritems_indexing"": ""class Iteration:\n    def time_iteritems_indexing(self):\n        for col in self.df3:\n            self.df3[col]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))"", ""frame_methods.Iteration.time_iterrows"": ""class Iteration:\n    def time_iterrows(self):\n        for row in self.df.iterrows():\n            pass\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))"", ""frame_methods.Iteration.time_itertuples_raw_start"": ""class Iteration:\n    def time_itertuples_raw_start(self):\n        self.df4.itertuples(index=False, name=None)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Iteration:\n    def setup(self):\n        N = 1000\n        self.df = DataFrame(np.random.randn(N * 10, N))\n        self.df2 = DataFrame(np.random.randn(N * 50, 10))\n        self.df3 = DataFrame(\n            np.random.randn(N, 5 * N), columns=[\""C\"" + str(c) for c in range(N * 5)]\n        )\n        self.df4 = DataFrame(np.random.randn(N * 1000, 10))"", ""frame_methods.NSort.time_nlargest_two_columns"": ""class NSort:\n    def time_nlargest_two_columns(self, keep):\n        self.df.nlargest(100, [\""A\"", \""B\""], keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NSort:\n    def setup(self, keep):\n        self.df = DataFrame(np.random.randn(100000, 3), columns=list(\""ABC\""))"", ""frame_methods.NSort.time_nsmallest_one_column"": ""class NSort:\n    def time_nsmallest_one_column(self, keep):\n        self.df.nsmallest(100, \""A\"", keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NSort:\n    def setup(self, keep):\n        self.df = DataFrame(np.random.randn(100000, 3), columns=list(\""ABC\""))"", ""frame_methods.NSort.time_nsmallest_two_columns"": ""class NSort:\n    def time_nsmallest_two_columns(self, keep):\n        self.df.nsmallest(100, [\""A\"", \""B\""], keep=keep)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NSort:\n    def setup(self, keep):\n        self.df = DataFrame(np.random.randn(100000, 3), columns=list(\""ABC\""))"", ""frame_methods.Quantile.time_frame_quantile"": ""class Quantile:\n    def time_frame_quantile(self, axis):\n        self.df.quantile([0.1, 0.5], axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, axis):\n        self.df = DataFrame(np.random.randn(1000, 3), columns=list(\""ABC\""))"", ""frame_methods.ToDict.time_to_dict_ints"": ""class ToDict:\n    def time_to_dict_ints(self, orient):\n        self.int_df.to_dict(orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToDict:\n    def setup(self, orient):\n        data = np.random.randint(0, 1000, size=(10000, 4))\n        self.int_df = DataFrame(data)\n        self.datetimelike_df = self.int_df.astype(\""timedelta64[ns]\"")"", ""frame_methods.ToNumpy.time_to_numpy_tall"": ""class ToNumpy:\n    def time_to_numpy_tall(self):\n        self.df_tall.to_numpy()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToNumpy:\n    def setup(self):\n        N = 10000\n        M = 10\n        self.df_tall = DataFrame(np.random.randn(N, M))\n        self.df_wide = DataFrame(np.random.randn(M, N))\n        self.df_mixed_tall = self.df_tall.copy()\n        self.df_mixed_tall[\""foo\""] = \""bar\""\n        self.df_mixed_tall[0] = period_range(\""2000\"", periods=N)\n        self.df_mixed_tall[1] = range(N)\n        self.df_mixed_wide = self.df_wide.copy()\n        self.df_mixed_wide[\""foo\""] = \""bar\""\n        self.df_mixed_wide[0] = period_range(\""2000\"", periods=M)\n        self.df_mixed_wide[1] = range(M)"", ""frame_methods.ToNumpy.time_to_numpy_wide"": ""class ToNumpy:\n    def time_to_numpy_wide(self):\n        self.df_wide.to_numpy()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToNumpy:\n    def setup(self):\n        N = 10000\n        M = 10\n        self.df_tall = DataFrame(np.random.randn(N, M))\n        self.df_wide = DataFrame(np.random.randn(M, N))\n        self.df_mixed_tall = self.df_tall.copy()\n        self.df_mixed_tall[\""foo\""] = \""bar\""\n        self.df_mixed_tall[0] = period_range(\""2000\"", periods=N)\n        self.df_mixed_tall[1] = range(N)\n        self.df_mixed_wide = self.df_wide.copy()\n        self.df_mixed_wide[\""foo\""] = \""bar\""\n        self.df_mixed_wide[0] = period_range(\""2000\"", periods=M)\n        self.df_mixed_wide[1] = range(M)"", ""frame_methods.ToNumpy.time_values_tall"": ""class ToNumpy:\n    def time_values_tall(self):\n        self.df_tall.values\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToNumpy:\n    def setup(self):\n        N = 10000\n        M = 10\n        self.df_tall = DataFrame(np.random.randn(N, M))\n        self.df_wide = DataFrame(np.random.randn(M, N))\n        self.df_mixed_tall = self.df_tall.copy()\n        self.df_mixed_tall[\""foo\""] = \""bar\""\n        self.df_mixed_tall[0] = period_range(\""2000\"", periods=N)\n        self.df_mixed_tall[1] = range(N)\n        self.df_mixed_wide = self.df_wide.copy()\n        self.df_mixed_wide[\""foo\""] = \""bar\""\n        self.df_mixed_wide[0] = period_range(\""2000\"", periods=M)\n        self.df_mixed_wide[1] = range(M)"", ""frame_methods.ToNumpy.time_values_wide"": ""class ToNumpy:\n    def time_values_wide(self):\n        self.df_wide.values\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToNumpy:\n    def setup(self):\n        N = 10000\n        M = 10\n        self.df_tall = DataFrame(np.random.randn(N, M))\n        self.df_wide = DataFrame(np.random.randn(M, N))\n        self.df_mixed_tall = self.df_tall.copy()\n        self.df_mixed_tall[\""foo\""] = \""bar\""\n        self.df_mixed_tall[0] = period_range(\""2000\"", periods=N)\n        self.df_mixed_tall[1] = range(N)\n        self.df_mixed_wide = self.df_wide.copy()\n        self.df_mixed_wide[\""foo\""] = \""bar\""\n        self.df_mixed_wide[0] = period_range(\""2000\"", periods=M)\n        self.df_mixed_wide[1] = range(M)""}",frame_methods,time,0.0
30070,module-level,"terminus-2,oracle",1.0472118667839048,1.0472118667839048,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.sql,io.sql,"{""io.sql.WriteSQLDtypes.time_read_sql_query_select_column"": ""class WriteSQLDtypes:\n    def time_read_sql_query_select_column(self, connection, dtype):\n        read_sql_query(self.query_col, self.con)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass WriteSQLDtypes:\n    def setup(self, connection, dtype):\n        N = 10000\n        con = {\n            \""sqlalchemy\"": create_engine(\""sqlite:///:memory:\""),\n            \""sqlite\"": sqlite3.connect(\"":memory:\""),\n        }\n        self.table_name = \""test_type\""\n        self.query_col = f\""SELECT {dtype} FROM {self.table_name}\""\n        self.con = con[connection]\n        self.df = DataFrame(\n            {\n                \""float\"": np.random.randn(N),\n                \""float_with_nan\"": np.random.randn(N),\n                \""string\"": [\""foo\""] * N,\n                \""bool\"": [True] * N,\n                \""int\"": np.random.randint(0, N, size=N),\n                \""datetime\"": date_range(\""2000-01-01\"", periods=N, freq=\""s\""),\n            },\n            index=tm.makeStringIndex(N),\n        )\n        self.df.iloc[1000:3000, 1] = np.nan\n        self.df[\""date\""] = self.df[\""datetime\""].dt.date\n        self.df[\""time\""] = self.df[\""datetime\""].dt.time\n        self.df[\""datetime_string\""] = self.df[\""datetime\""].astype(str)\n        self.df.to_sql(self.table_name, self.con, if_exists=\""replace\"")""}",io.sql,time,0.0
30071,module-level,"terminus-2,oracle",0.97232287374828,0.97232287374828,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.style,io.style,"{""io.style.Render.time_format_render"": ""class Render:\n    def time_format_render(self, cols, rows):\n        self._style_format()\n        self.st._render_html(True, True)\n\n    def setup(self, cols, rows):\n        self.df = DataFrame(\n            np.random.randn(rows, cols),\n            columns=[f\""float_{i+1}\"" for i in range(cols)],\n            index=[f\""row_{i+1}\"" for i in range(rows)],\n        )""}",io.style,time,0.0
30068,module-level,"terminus-2,oracle",1.046659047849883,1.046659047849883,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.json,io.json,"{""io.json.NormalizeJSON.time_normalize_json"": ""class NormalizeJSON:\n    def time_normalize_json(self, orient, frame):\n        json_normalize(self.data)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NormalizeJSON:\n    def setup(self, orient, frame):\n        data = {\n            \""hello\"": [\""thisisatest\"", 999898, \""mixed types\""],\n            \""nest1\"": {\""nest2\"": {\""nest3\"": \""nest3_value\"", \""nest3_int\"": 3445}},\n            \""nest1_list\"": {\""nest2\"": [\""blah\"", 32423, 546456.876, 92030234]},\n            \""hello2\"": \""string\"",\n        }\n        self.data = [data for i in range(10000)]"", ""io.json.ToJSON.time_to_json"": ""class ToJSON:\n    def time_to_json(self, orient, frame):\n        getattr(self, frame).to_json(self.fname, orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSON:\n    def setup(self, orient, frame):\n        N = 10**5\n        ncols = 5\n        index = date_range(\""20000101\"", periods=N, freq=\""H\"")\n        timedeltas = timedelta_range(start=1, periods=N, freq=\""s\"")\n        datetimes = date_range(start=1, periods=N, freq=\""s\"")\n        ints = np.random.randint(100000000, size=N)\n        longints = sys.maxsize * np.random.randint(100000000, size=N)\n        floats = np.random.randn(N)\n        strings = tm.makeStringIndex(N)\n        self.df = DataFrame(np.random.randn(N, ncols), index=np.arange(N))\n        self.df_date_idx = DataFrame(np.random.randn(N, ncols), index=index)\n        self.df_td_int_ts = DataFrame(\n            {\n                \""td_1\"": timedeltas,\n                \""td_2\"": timedeltas,\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""ts_1\"": datetimes,\n                \""ts_2\"": datetimes,\n            },\n            index=index,\n        )\n        self.df_int_floats = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""int_3\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""float_3\"": floats,\n            },\n            index=index,\n        )\n        self.df_int_float_str = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )\n    \n        self.df_longint_float_str = DataFrame(\n            {\n                \""longint_1\"": longints,\n                \""longint_2\"": longints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )"", ""io.json.ToJSONLines.time_delta_int_tstamp_lines"": ""class ToJSONLines:\n    def time_delta_int_tstamp_lines(self):\n        self.df_td_int_ts.to_json(self.fname, orient=\""records\"", lines=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSONLines:\n    def setup(self):\n        N = 10**5\n        ncols = 5\n        index = date_range(\""20000101\"", periods=N, freq=\""H\"")\n        timedeltas = timedelta_range(start=1, periods=N, freq=\""s\"")\n        datetimes = date_range(start=1, periods=N, freq=\""s\"")\n        ints = np.random.randint(100000000, size=N)\n        longints = sys.maxsize * np.random.randint(100000000, size=N)\n        floats = np.random.randn(N)\n        strings = tm.makeStringIndex(N)\n        self.df = DataFrame(np.random.randn(N, ncols), index=np.arange(N))\n        self.df_date_idx = DataFrame(np.random.randn(N, ncols), index=index)\n        self.df_td_int_ts = DataFrame(\n            {\n                \""td_1\"": timedeltas,\n                \""td_2\"": timedeltas,\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""ts_1\"": datetimes,\n                \""ts_2\"": datetimes,\n            },\n            index=index,\n        )\n        self.df_int_floats = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""int_3\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""float_3\"": floats,\n            },\n            index=index,\n        )\n        self.df_int_float_str = DataFrame(\n            {\n                \""int_1\"": ints,\n                \""int_2\"": ints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )\n        self.df_longint_float_str = DataFrame(\n            {\n                \""longint_1\"": longints,\n                \""longint_2\"": longints,\n                \""float_1\"": floats,\n                \""float_2\"": floats,\n                \""str_1\"": strings,\n                \""str_2\"": strings,\n            },\n            index=index,\n        )"", ""io.json.ToJSONWide.time_to_json"": ""class ToJSON:\n    def time_to_json(self, orient, frame):\n        getattr(self, frame).to_json(self.fname, orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSONWide:\n    def setup(self, orient, frame):\n        super().setup(orient, frame)\n        base_df = getattr(self, frame).copy()\n        df_wide = concat([base_df.iloc[:100]] * 1000, ignore_index=True, axis=1)\n        self.df_wide = df_wide"", ""io.json.ToJSONWide.time_to_json_wide"": ""class ToJSONWide:\n    def time_to_json_wide(self, orient, frame):\n        self.df_wide.to_json(self.fname, orient=orient)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToJSONWide:\n    def setup(self, orient, frame):\n        super().setup(orient, frame)\n        base_df = getattr(self, frame).copy()\n        df_wide = concat([base_df.iloc[:100]] * 1000, ignore_index=True, axis=1)\n        self.df_wide = df_wide""}",io.json,time,0.0
30078,module-level,"terminus-2,oracle",0.9481537929203344,0.9481537929203344,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,replace,replace,"{""replace.ReplaceList.time_replace_list"": ""class ReplaceList:\n    def time_replace_list(self, inplace):\n        self.df.replace([np.inf, -np.inf], np.nan, inplace=inplace)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReplaceList:\n    def setup(self, inplace):\n        self.df = pd.DataFrame({\""A\"": 0, \""B\"": 0}, index=range(4 * 10**7))""}",replace,time,0.0
30081,module-level,"terminus-2,oracle",1.0511605422863617,1.0511605422863617,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,series_methods,series_methods,"{""series_methods.Any.time_any"": ""class Any:\n    def time_any(self, N, case, dtype):\n        self.s.any()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Any:\n    def setup(self, N, case, dtype):\n        val = case == \""fast\""\n        self.s = Series([val] * N, dtype=dtype)"", ""series_methods.Fillna.time_fillna"": ""class Fillna:\n    def time_fillna(self, dtype, method):\n        value = self.fill_value if method is None else None\n        self.ser.fillna(value=value, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Fillna:\n    def setup(self, dtype, method):\n        N = 10**6\n        if dtype == \""datetime64[ns]\"":\n            data = date_range(\""2000-01-01\"", freq=\""S\"", periods=N)\n            na_value = NaT\n        elif dtype in (\""float64\"", \""Float64\""):\n            data = np.random.randn(N)\n            na_value = np.nan\n        elif dtype in (\""Int64\"", \""int64[pyarrow]\""):\n            data = np.arange(N)\n            na_value = NA\n        elif dtype in (\""string\"", \""string[pyarrow]\""):\n            data = tm.rands_array(5, N)\n            na_value = NA\n        else:\n            raise NotImplementedError\n        fill_value = data[0]\n        ser = Series(data, dtype=dtype)\n        ser[::2] = na_value\n        self.ser = ser\n        self.fill_value = fill_value"", ""series_methods.NanOps.time_func"": ""class NanOps:\n    def time_func(self, func, N, dtype):\n        self.func()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass NanOps:\n    def setup(self, func, N, dtype):\n        if func == \""argmax\"" and dtype in {\""Int64\"", \""boolean\""}:\n            # Skip argmax for nullable int since this doesn't work yet (GH-24382)\n            raise NotImplementedError\n        self.s = Series([1] * N, dtype=dtype)\n        self.func = getattr(self.s, func)"", ""series_methods.SearchSorted.time_searchsorted"": ""class SearchSorted:\n    def time_searchsorted(self, dtype):\n        key = \""2\"" if dtype == \""str\"" else 2\n        self.s.searchsorted(key)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SearchSorted:\n    def setup(self, dtype):\n        N = 10**5\n        data = np.array([1] * N + [2] * N + [3] * N).astype(dtype)\n        self.s = Series(data)"", ""series_methods.SeriesConstructor.time_constructor_dict"": ""class SeriesConstructor:\n    def time_constructor_dict(self):\n        Series(data=self.data, index=self.idx)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesConstructor:\n    def setup(self):\n        self.idx = date_range(\n            start=datetime(2015, 10, 26), end=datetime(2016, 1, 1), freq=\""50s\""\n        )\n        self.data = dict(zip(self.idx, range(len(self.idx))))\n        self.array = np.array([1, 2, 3])\n        self.idx2 = Index([\""a\"", \""b\"", \""c\""])"", ""series_methods.ToFrame.time_to_frame"": ""class ToFrame:\n    def time_to_frame(self, dtype, name):\n        self.ser.to_frame(name)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToFrame:\n    def setup(self, dtype, name):\n        arr = np.arange(10**5)\n        ser = Series(arr, dtype=dtype)\n        self.ser = ser""}",series_methods,time,0.0
30065,module-level,"terminus-2,oracle",1.0583045297627374,1.0583045297627374,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,io.csv,io.csv,"{""io.csv.ReadCSVCategorical.time_convert_direct"": ""class ReadCSVCategorical:\n    def time_convert_direct(self, engine):\n        read_csv(self.fname, engine=engine, dtype=\""category\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVCategorical:\n    def setup(self, engine):\n        N = 100000\n        group1 = [\""aaaaaaaa\"", \""bbbbbbb\"", \""cccccccc\"", \""dddddddd\"", \""eeeeeeee\""]\n        df = DataFrame(np.random.choice(group1, (N, 3)), columns=list(\""abc\""))\n        df.to_csv(self.fname, index=False)"", ""io.csv.ReadCSVCategorical.time_convert_post"": ""class ReadCSVCategorical:\n    def time_convert_post(self, engine):\n        read_csv(self.fname, engine=engine).apply(Categorical)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVCategorical:\n    def setup(self, engine):\n        N = 100000\n        group1 = [\""aaaaaaaa\"", \""bbbbbbb\"", \""cccccccc\"", \""dddddddd\"", \""eeeeeeee\""]\n        df = DataFrame(np.random.choice(group1, (N, 3)), columns=list(\""abc\""))\n        df.to_csv(self.fname, index=False)"", ""io.csv.ReadCSVEngine.time_read_bytescsv"": ""class ReadCSVEngine:\n    def time_read_bytescsv(self, engine):\n        read_csv(self.data(self.BytesIO_input), engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVEngine:\n    def setup(self, engine):\n        data = [\""A,B,C,D,E\""] + ([\""1,2,3,4,5\""] * 100000)\n        self.StringIO_input = StringIO(\""\\n\"".join(data))\n        # simulate reading from file\n        self.BytesIO_input = BytesIO(self.StringIO_input.read().encode(\""utf-8\""))"", ""io.csv.ReadCSVEngine.time_read_stringcsv"": ""class ReadCSVEngine:\n    def time_read_stringcsv(self, engine):\n        read_csv(self.data(self.StringIO_input), engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReadCSVEngine:\n    def setup(self, engine):\n        data = [\""A,B,C,D,E\""] + ([\""1,2,3,4,5\""] * 100000)\n        self.StringIO_input = StringIO(\""\\n\"".join(data))\n        # simulate reading from file\n        self.BytesIO_input = BytesIO(self.StringIO_input.read().encode(\""utf-8\""))"", ""io.csv.ToCSVDatetimeIndex.time_frame_date_formatting_index"": ""class ToCSVDatetimeIndex:\n    def time_frame_date_formatting_index(self):\n        self.data.to_csv(self.fname, date_format=\""%Y-%m-%d %H:%M:%S\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ToCSVDatetimeIndex:\n    def setup(self):\n        rng = date_range(\""2000\"", periods=100_000, freq=\""S\"")\n        self.data = DataFrame({\""a\"": 1}, index=rng)""}",io.csv,time,0.0
30073,module-level,"terminus-2,oracle",1.0271142492964604,1.0271142492964604,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,libs,libs,"{""libs.InferDtype.time_infer_dtype"": ""class InferDtype:\n    def time_infer_dtype(self, dtype):\n        infer_dtype(self.data_dict[dtype], skipna=False)"", ""libs.ScalarListLike.time_is_list_like"": ""class ScalarListLike:\n    def time_is_list_like(self, param):\n        is_list_like(param)"", ""libs.ScalarListLike.time_is_scalar"": ""class ScalarListLike:\n    def time_is_scalar(self, param):\n        is_scalar(param)""}",libs,time,0.0
30079,module-level,"terminus-2,oracle",1.0930721585001462,1.0930721585001462,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reshape,reshape,"{""reshape.Cut.time_cut_datetime"": ""class Cut:\n    def time_cut_datetime(self, bins):\n        pd.cut(self.datetime_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Cut.time_cut_interval"": ""class Cut:\n    def time_cut_interval(self, bins):\n        # GH 27668\n        pd.cut(self.int_series, self.interval_bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Cut.time_cut_timedelta"": ""class Cut:\n    def time_cut_timedelta(self, bins):\n        pd.cut(self.timedelta_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Cut.time_qcut_datetime"": ""class Cut:\n    def time_qcut_datetime(self, bins):\n        pd.qcut(self.datetime_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Cut.time_qcut_float"": ""class Cut:\n    def time_qcut_float(self, bins):\n        pd.qcut(self.float_series, bins)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Cut:\n    def setup(self, bins):\n        N = 10**5\n        self.int_series = pd.Series(np.arange(N).repeat(5))\n        self.float_series = pd.Series(np.random.randn(N).repeat(5))\n        self.timedelta_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""timedelta64[ns]\""\n        )\n        self.datetime_series = pd.Series(\n            np.random.randint(N, size=N), dtype=\""datetime64[ns]\""\n        )\n        self.interval_bins = pd.IntervalIndex.from_breaks(np.linspace(0, N, bins))"", ""reshape.Explode.time_explode"": ""class Explode:\n    def time_explode(self, n_rows, max_list_length):\n        self.series.explode()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Explode:\n    def setup(self, n_rows, max_list_length):\n        data = [np.arange(np.random.randint(max_list_length)) for _ in range(n_rows)]\n        self.series = pd.Series(data)"", ""reshape.Melt.time_melt_dataframe"": ""class Melt:\n    def time_melt_dataframe(self, dtype):\n        melt(self.df, id_vars=[\""id1\"", \""id2\""])\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Melt:\n    def setup(self, dtype):\n        self.df = DataFrame(\n            np.random.randn(100_000, 3), columns=[\""A\"", \""B\"", \""C\""], dtype=dtype\n        )\n        self.df[\""id1\""] = pd.Series(np.random.randint(0, 10, 10000))\n        self.df[\""id2\""] = pd.Series(np.random.randint(100, 1000, 10000))"", ""reshape.PivotTable.time_pivot_table_categorical"": ""class PivotTable:\n    def time_pivot_table_categorical(self):\n        self.df2.pivot_table(\n            index=\""col1\"", values=\""col3\"", columns=\""col2\"", aggfunc=np.sum, fill_value=0\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")"", ""reshape.PivotTable.time_pivot_table_categorical_observed"": ""class PivotTable:\n    def time_pivot_table_categorical_observed(self):\n        self.df2.pivot_table(\n            index=\""col1\"",\n            values=\""col3\"",\n            columns=\""col2\"",\n            aggfunc=np.sum,\n            fill_value=0,\n            observed=True,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")"", ""reshape.PivotTable.time_pivot_table_margins"": ""class PivotTable:\n    def time_pivot_table_margins(self):\n        self.df.pivot_table(index=\""key1\"", columns=[\""key2\"", \""key3\""], margins=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")"", ""reshape.PivotTable.time_pivot_table_margins_only_column"": ""class PivotTable:\n    def time_pivot_table_margins_only_column(self):\n        self.df.pivot_table(columns=[\""key1\"", \""key2\"", \""key3\""], margins=True)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass PivotTable:\n    def setup(self):\n        N = 100000\n        fac1 = np.array([\""A\"", \""B\"", \""C\""], dtype=\""O\"")\n        fac2 = np.array([\""one\"", \""two\""], dtype=\""O\"")\n        ind1 = np.random.randint(0, 3, size=N)\n        ind2 = np.random.randint(0, 2, size=N)\n        self.df = DataFrame(\n            {\n                \""key1\"": fac1.take(ind1),\n                \""key2\"": fac2.take(ind2),\n                \""key3\"": fac2.take(ind2),\n                \""value1\"": np.random.randn(N),\n                \""value2\"": np.random.randn(N),\n                \""value3\"": np.random.randn(N),\n            }\n        )\n        self.df2 = DataFrame(\n            {\""col1\"": list(\""abcde\""), \""col2\"": list(\""fghij\""), \""col3\"": [1, 2, 3, 4, 5]}\n        )\n        self.df2.col1 = self.df2.col1.astype(\""category\"")\n        self.df2.col2 = self.df2.col2.astype(\""category\"")"", ""reshape.ReshapeExtensionDtype.time_stack"": ""class ReshapeExtensionDtype:\n    def time_stack(self, dtype):\n        self.df.stack()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ReshapeExtensionDtype:\n    def setup(self, dtype):\n        lev = pd.Index(list(\""ABCDEFGHIJ\""))\n        ri = pd.Index(range(1000))\n        mi = MultiIndex.from_product([lev, ri], names=[\""foo\"", \""bar\""])\n    \n        index = date_range(\""2016-01-01\"", periods=10000, freq=\""s\"", tz=\""US/Pacific\"")\n        if dtype == \""Period[s]\"":\n            index = index.tz_localize(None).to_period(\""s\"")\n    \n        ser = pd.Series(index, index=mi)\n        df = ser.unstack(\""bar\"")\n        # roundtrips -> df.stack().equals(ser)\n    \n        self.ser = ser\n        self.df = df"", ""reshape.WideToLong.time_wide_to_long_big"": ""class WideToLong:\n    def time_wide_to_long_big(self):\n        wide_to_long(self.df, self.letters, i=\""id\"", j=\""year\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass WideToLong:\n    def setup(self):\n        nyrs = 20\n        nidvars = 20\n        N = 5000\n        self.letters = list(\""ABCD\"")\n        yrvars = [\n            letter + str(num)\n            for letter, num in product(self.letters, range(1, nyrs + 1))\n        ]\n        columns = [str(i) for i in range(nidvars)] + yrvars\n        self.df = DataFrame(np.random.randn(N, nidvars + len(yrvars)), columns=columns)\n        self.df[\""id\""] = self.df.index""}",reshape,time,0.0
30082,module-level,"terminus-2,oracle",0.9171397446812648,0.9171397446812648,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,sparse,sparse,"{""sparse.Arithmetic.time_add"": ""class Arithmetic:\n    def time_add(self, dense_proportion, fill_value):\n        self.array1 + self.array2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)"", ""sparse.Arithmetic.time_divide"": ""class Arithmetic:\n    def time_divide(self, dense_proportion, fill_value):\n        self.array1 / self.array2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)"", ""sparse.Arithmetic.time_make_union"": ""class Arithmetic:\n    def time_make_union(self, dense_proportion, fill_value):\n        self.array1.sp_index.make_union(self.array2.sp_index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Arithmetic:\n    def setup(self, dense_proportion, fill_value):\n        N = 10**6\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\n        self.array2 = SparseArray(arr2, fill_value=fill_value)"", ""sparse.ArithmeticBlock.time_addition"": ""class ArithmeticBlock:\n    def time_addition(self, fill_value):\n        self.arr1 + self.arr2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )"", ""sparse.ArithmeticBlock.time_division"": ""class ArithmeticBlock:\n    def time_division(self, fill_value):\n        self.arr1 / self.arr2\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )"", ""sparse.ArithmeticBlock.time_make_union"": ""class ArithmeticBlock:\n    def time_make_union(self, fill_value):\n        self.arr1.sp_index.make_union(self.arr2.sp_index)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ArithmeticBlock:\n    def setup(self, fill_value):\n        N = 10**6\n        self.arr1 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )\n        self.arr2 = self.make_block_array(\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\n        )"", ""sparse.Take.time_take"": ""class Take:\n    def time_take(self, indices, allow_fill):\n        self.sp_arr.take(indices, allow_fill=allow_fill)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Take:\n    def setup(self, indices, allow_fill):\n        N = 1_000_000\n        fill_value = 0.0\n        arr = make_array(N, 1e-5, fill_value, np.float64)\n        self.sp_arr = SparseArray(arr, fill_value=fill_value)""}",sparse,time,0.0
30075,module-level,"terminus-2,oracle",1.0373042149529246,1.0373042149529246,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,period,period,"{""period.Indexing.time_align"": ""class Indexing:\n    def time_align(self):\n        DataFrame({\""a\"": self.series, \""b\"": self.series[:500]})\n\n    def setup(self):\n        self.index = period_range(start=\""1985\"", periods=1000, freq=\""D\"")\n        self.series = Series(range(1000), index=self.index)\n        self.period = self.index[500]"", ""period.Indexing.time_intersection"": ""class Indexing:\n    def time_intersection(self):\n        self.index[:750].intersection(self.index[250:])\n\n    def setup(self):\n        self.index = period_range(start=\""1985\"", periods=1000, freq=\""D\"")\n        self.series = Series(range(1000), index=self.index)\n        self.period = self.index[500]"", ""period.Indexing.time_series_loc"": ""class Indexing:\n    def time_series_loc(self):\n        self.series.loc[self.period]\n\n    def setup(self):\n        self.index = period_range(start=\""1985\"", periods=1000, freq=\""D\"")\n        self.series = Series(range(1000), index=self.index)\n        self.period = self.index[500]"", ""period.PeriodIndexConstructor.time_from_date_range"": ""class PeriodIndexConstructor:\n    def time_from_date_range(self, freq, is_offset):\n        PeriodIndex(self.rng, freq=freq)\n\n    def setup(self, freq, is_offset):\n        self.rng = date_range(\""1985\"", periods=1000)\n        self.rng2 = date_range(\""1985\"", periods=1000).to_pydatetime()\n        self.ints = list(range(2000, 3000))\n        self.daily_ints = (\n            date_range(\""1/1/2000\"", periods=1000, freq=freq).strftime(\""%Y%m%d\"").map(int)\n        )\n        if is_offset:\n            self.freq = to_offset(freq)\n        else:\n            self.freq = freq""}",period,time,0.0
30080,module-level,"terminus-2,oracle",1.0550472257038015,1.0550472257038015,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,rolling,rolling,"{""rolling.Apply.time_rolling"": ""class Apply:\n    def time_rolling(self, constructor, window, dtype, function, raw):\n        self.roll.apply(function, raw=raw)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Apply:\n    def setup(self, constructor, window, dtype, function, raw):\n        N = 10**3\n        arr = (100 * np.random.random(N)).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)"", ""rolling.EWMMethods.time_ewm"": ""class EWMMethods:\n    def time_ewm(self, constructor, kwargs_method, dtype):\n        getattr(self.ewm, self.method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass EWMMethods:\n    def setup(self, constructor, kwargs_method, dtype):\n        N = 10**5\n        kwargs, method = kwargs_method\n        arr = (100 * np.random.random(N)).astype(dtype)\n        self.method = method\n        self.ewm = getattr(pd, constructor)(arr).ewm(**kwargs)"", ""rolling.ForwardWindowMethods.time_rolling"": ""class ForwardWindowMethods:\n    def time_rolling(self, constructor, window_size, dtype, method):\n        getattr(self.roll, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ForwardWindowMethods:\n    def setup(self, constructor, window_size, dtype, method):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=window_size)\n        self.roll = getattr(pd, constructor)(arr).rolling(window=indexer)"", ""rolling.Groupby.time_method"": ""class Groupby:\n    def time_method(self, method, window_kwargs):\n        getattr(self.groupby_window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Groupby:\n    def setup(self, method, window_kwargs):\n        N = 1000\n        window, kwargs = window_kwargs\n        df = pd.DataFrame(\n            {\n                \""A\"": [str(i) for i in range(N)] * 10,\n                \""B\"": list(range(N)) * 10,\n            }\n        )\n        if isinstance(kwargs.get(\""window\"", None), str):\n            df.index = pd.date_range(start=\""1900-01-01\"", freq=\""1min\"", periods=N * 10)\n        self.groupby_window = getattr(df.groupby(\""A\""), window)(**kwargs)"", ""rolling.GroupbyEWM.time_groupby_method"": ""class GroupbyEWM:\n    def time_groupby_method(self, method):\n        getattr(self.gb_ewm, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupbyEWM:\n    def setup(self, method):\n        df = pd.DataFrame({\""A\"": range(50), \""B\"": range(50)})\n        self.gb_ewm = df.groupby(\""A\"").ewm(com=1.0)"", ""rolling.GroupbyEWMEngine.time_groupby_mean"": ""class GroupbyEWMEngine:\n    def time_groupby_mean(self, engine):\n        self.gb_ewm.mean(engine=engine)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupbyEWMEngine:\n    def setup(self, engine):\n        df = pd.DataFrame({\""A\"": range(50), \""B\"": range(50)})\n        self.gb_ewm = df.groupby(\""A\"").ewm(com=1.0)"", ""rolling.GroupbyLargeGroups.time_rolling_multiindex_creation"": ""class GroupbyLargeGroups:\n    def time_rolling_multiindex_creation(self):\n        self.df.groupby(\""A\"").rolling(3).mean()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GroupbyLargeGroups:\n    def setup(self):\n        N = 100000\n        self.df = pd.DataFrame({\""A\"": [1, 2] * (N // 2), \""B\"": np.random.randn(N)})"", ""rolling.Methods.time_method"": ""class Methods:\n    def time_method(self, constructor, window_kwargs, dtype, method):\n        getattr(self.window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Methods:\n    def setup(self, constructor, window_kwargs, dtype, method):\n        N = 10**5\n        window, kwargs = window_kwargs\n        arr = (100 * np.random.random(N)).astype(dtype)\n        obj = getattr(pd, constructor)(arr)\n        self.window = getattr(obj, window)(**kwargs)"", ""rolling.Pairwise.time_groupby"": ""class Pairwise:\n    def time_groupby(self, kwargs_window, method, pairwise):\n        getattr(self.window_group, method)(self.df, pairwise=pairwise)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Pairwise:\n    def setup(self, kwargs_window, method, pairwise):\n        N = 10**4\n        n_groups = 20\n        kwargs, window = kwargs_window\n        groups = [i for _ in range(N // n_groups) for i in range(n_groups)]\n        arr = np.random.random(N)\n        self.df = pd.DataFrame(arr)\n        self.window = getattr(self.df, window)(**kwargs)\n        self.window_group = getattr(\n            pd.DataFrame({\""A\"": groups, \""B\"": arr}).groupby(\""A\""), window\n        )(**kwargs)"", ""rolling.Pairwise.time_pairwise"": ""class Pairwise:\n    def time_pairwise(self, kwargs_window, method, pairwise):\n        getattr(self.window, method)(self.df, pairwise=pairwise)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Pairwise:\n    def setup(self, kwargs_window, method, pairwise):\n        N = 10**4\n        n_groups = 20\n        kwargs, window = kwargs_window\n        groups = [i for _ in range(N // n_groups) for i in range(n_groups)]\n        arr = np.random.random(N)\n        self.df = pd.DataFrame(arr)\n        self.window = getattr(self.df, window)(**kwargs)\n        self.window_group = getattr(\n            pd.DataFrame({\""A\"": groups, \""B\"": arr}).groupby(\""A\""), window\n        )(**kwargs)"", ""rolling.Quantile.time_quantile"": ""class Quantile:\n    def time_quantile(self, constructor, window, dtype, percentile, interpolation):\n        self.roll.quantile(percentile, interpolation=interpolation)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Quantile:\n    def setup(self, constructor, window, dtype, percentile, interpolation):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)"", ""rolling.Rank.time_rank"": ""class Rank:\n    def time_rank(self, constructor, window, dtype, percentile, ascending, method):\n        self.roll.rank(pct=percentile, ascending=ascending, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Rank:\n    def setup(self, constructor, window, dtype, percentile, ascending, method):\n        N = 10**5\n        arr = np.random.random(N).astype(dtype)\n        self.roll = getattr(pd, constructor)(arr).rolling(window)"", ""rolling.TableMethod.time_apply"": ""class TableMethod:\n    def time_apply(self, method):\n        self.df.rolling(2, method=method).apply(\n            table_method_func, raw=True, engine=\""numba\""\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TableMethod:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(10, 1000))"", ""rolling.VariableWindowMethods.time_method"": ""class Methods:\n    def time_method(self, constructor, window_kwargs, dtype, method):\n        getattr(self.window, method)()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass VariableWindowMethods:\n    def setup(self, constructor, window, dtype, method):\n        N = 10**5\n        arr = (100 * np.random.random(N)).astype(dtype)\n        index = pd.date_range(\""2017-01-01\"", periods=N, freq=\""5s\"")\n        self.window = getattr(pd, constructor)(arr, index=index).rolling(window)""}",rolling,time,0.0
30072,module-level,"terminus-2,oracle",0.955488002603868,0.955488002603868,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,join_merge,join_merge,"{""join_merge.Align.time_series_align_left_monotonic"": ""class Align:\n    def time_series_align_left_monotonic(self):\n        self.ts1.align(self.ts2, join=\""left\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Align:\n    def setup(self):\n        size = 5 * 10**5\n        rng = np.arange(0, 10**13, 10**7)\n        stamps = np.datetime64(\""now\"").view(\""i8\"") + rng\n        idx1 = np.sort(np.random.choice(stamps, size, replace=False))\n        idx2 = np.sort(np.random.choice(stamps, size, replace=False))\n        self.ts1 = Series(np.random.randn(size), idx1)\n        self.ts2 = Series(np.random.randn(size), idx2)"", ""join_merge.Concat.time_concat_empty_left"": ""class Concat:\n    def time_concat_empty_left(self, axis):\n        concat(self.empty_left, axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Concat:\n    def setup(self, axis):\n        N = 1000\n        s = Series(N, index=tm.makeStringIndex(N))\n        self.series = [s[i:-i] for i in range(1, 10)] * 50\n        self.small_frames = [DataFrame(np.random.randn(5, 4))] * 1000\n        df = DataFrame(\n            {\""A\"": range(N)}, index=date_range(\""20130101\"", periods=N, freq=\""s\"")\n        )\n        self.empty_left = [DataFrame(), df]\n        self.empty_right = [df, DataFrame()]\n        self.mixed_ndims = [df, df.head(N // 2)]"", ""join_merge.ConcatIndexDtype.time_concat_series"": ""class ConcatIndexDtype:\n    def time_concat_series(self, dtype, structure, axis, sort):\n        concat(self.series, axis=axis, sort=sort)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ConcatIndexDtype:\n    def setup(self, dtype, structure, axis, sort):\n        N = 10_000\n        if dtype == \""datetime64[ns]\"":\n            vals = date_range(\""1970-01-01\"", periods=N)\n        elif dtype in (\""int64\"", \""Int64\""):\n            vals = np.arange(N, dtype=np.int64)\n        elif dtype in (\""string[python]\"", \""string[pyarrow]\""):\n            vals = tm.makeStringIndex(N)\n        else:\n            raise NotImplementedError\n    \n        idx = Index(vals, dtype=dtype)\n    \n        if structure == \""monotonic\"":\n            idx = idx.sort_values()\n        elif structure == \""non_monotonic\"":\n            idx = idx[::-1]\n        elif structure == \""has_na\"":\n            if not idx._can_hold_na:\n                raise NotImplementedError\n            idx = Index([None], dtype=dtype).append(idx)\n        else:\n            raise NotImplementedError\n    \n        self.series = [Series(i, idx[:-i]) for i in range(1, 6)]"", ""join_merge.MergeAsof.time_by_object"": ""class MergeAsof:\n    def time_by_object(self, direction, tolerance):\n        merge_asof(\n            self.df1b,\n            self.df2b,\n            on=\""time\"",\n            by=\""key\"",\n            direction=direction,\n            tolerance=tolerance,\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MergeAsof:\n    def setup(self, direction, tolerance):\n        one_count = 200000\n        two_count = 1000000\n    \n        df1 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, one_count / 20, one_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), one_count),\n                \""key2\"": np.random.randint(0, 25, one_count),\n                \""value1\"": np.random.randn(one_count),\n            }\n        )\n        df2 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, two_count / 20, two_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), two_count),\n                \""key2\"": np.random.randint(0, 25, two_count),\n                \""value2\"": np.random.randn(two_count),\n            }\n        )\n    \n        df1 = df1.sort_values(\""time\"")\n        df2 = df2.sort_values(\""time\"")\n    \n        df1[\""time32\""] = np.int32(df1.time)\n        df2[\""time32\""] = np.int32(df2.time)\n    \n        df1[\""timeu64\""] = np.uint64(df1.time)\n        df2[\""timeu64\""] = np.uint64(df2.time)\n    \n        self.df1a = df1[[\""time\"", \""value1\""]]\n        self.df2a = df2[[\""time\"", \""value2\""]]\n        self.df1b = df1[[\""time\"", \""key\"", \""value1\""]]\n        self.df2b = df2[[\""time\"", \""key\"", \""value2\""]]\n        self.df1c = df1[[\""time\"", \""key2\"", \""value1\""]]\n        self.df2c = df2[[\""time\"", \""key2\"", \""value2\""]]\n        self.df1d = df1[[\""time32\"", \""value1\""]]\n        self.df2d = df2[[\""time32\"", \""value2\""]]\n        self.df1e = df1[[\""time\"", \""key\"", \""key2\"", \""value1\""]]\n        self.df2e = df2[[\""time\"", \""key\"", \""key2\"", \""value2\""]]\n        self.df1f = df1[[\""timeu64\"", \""value1\""]]\n        self.df2f = df2[[\""timeu64\"", \""value2\""]]"", ""join_merge.MergeAsof.time_on_int32"": ""class MergeAsof:\n    def time_on_int32(self, direction, tolerance):\n        merge_asof(\n            self.df1d, self.df2d, on=\""time32\"", direction=direction, tolerance=tolerance\n        )\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass MergeAsof:\n    def setup(self, direction, tolerance):\n        one_count = 200000\n        two_count = 1000000\n    \n        df1 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, one_count / 20, one_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), one_count),\n                \""key2\"": np.random.randint(0, 25, one_count),\n                \""value1\"": np.random.randn(one_count),\n            }\n        )\n        df2 = DataFrame(\n            {\n                \""time\"": np.random.randint(0, two_count / 20, two_count),\n                \""key\"": np.random.choice(list(string.ascii_uppercase), two_count),\n                \""key2\"": np.random.randint(0, 25, two_count),\n                \""value2\"": np.random.randn(two_count),\n            }\n        )\n    \n        df1 = df1.sort_values(\""time\"")\n        df2 = df2.sort_values(\""time\"")\n    \n        df1[\""time32\""] = np.int32(df1.time)\n        df2[\""time32\""] = np.int32(df2.time)\n    \n        df1[\""timeu64\""] = np.uint64(df1.time)\n        df2[\""timeu64\""] = np.uint64(df2.time)\n    \n        self.df1a = df1[[\""time\"", \""value1\""]]\n        self.df2a = df2[[\""time\"", \""value2\""]]\n        self.df1b = df1[[\""time\"", \""key\"", \""value1\""]]\n        self.df2b = df2[[\""time\"", \""key\"", \""value2\""]]\n        self.df1c = df1[[\""time\"", \""key2\"", \""value1\""]]\n        self.df2c = df2[[\""time\"", \""key2\"", \""value2\""]]\n        self.df1d = df1[[\""time32\"", \""value1\""]]\n        self.df2d = df2[[\""time32\"", \""value2\""]]\n        self.df1e = df1[[\""time\"", \""key\"", \""key2\"", \""value1\""]]\n        self.df2e = df2[[\""time\"", \""key\"", \""key2\"", \""value2\""]]\n        self.df1f = df1[[\""timeu64\"", \""value1\""]]\n        self.df2f = df2[[\""timeu64\"", \""value2\""]]""}",join_merge,time,0.0
30076,module-level,"terminus-2,oracle",0.97254858342969,0.97254858342969,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,plotting,plotting,"{""plotting.SeriesPlotting.time_series_plot"": ""class SeriesPlotting:\n    def time_series_plot(self, kind):\n        self.s.plot(kind=kind)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesPlotting:\n    def setup(self, kind):\n        if kind in [\""bar\"", \""barh\"", \""pie\""]:\n            n = 100\n        elif kind in [\""kde\""]:\n            n = 10000\n        else:\n            n = 1000000\n    \n        self.s = Series(np.random.randn(n))\n        if kind in [\""area\"", \""pie\""]:\n            self.s = self.s.abs()""}",plotting,time,0.0
30083,module-level,"terminus-2,oracle",1.0307116223909487,1.0307116223909487,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,stat_ops,stat_ops,"{""stat_ops.Correlation.time_corr"": ""class Correlation:\n    def time_corr(self, method):\n        self.df.corr(method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Correlation:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(500, 15))\n        self.df2 = pd.DataFrame(np.random.randn(500, 15))\n        self.df_wide = pd.DataFrame(np.random.randn(500, 100))\n        self.df_wide_nans = self.df_wide.where(np.random.random((500, 100)) < 0.9)\n        self.s = pd.Series(np.random.randn(500))\n        self.s2 = pd.Series(np.random.randn(500))"", ""stat_ops.Correlation.time_corr_series"": ""class Correlation:\n    def time_corr_series(self, method):\n        self.s.corr(self.s2, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Correlation:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(500, 15))\n        self.df2 = pd.DataFrame(np.random.randn(500, 15))\n        self.df_wide = pd.DataFrame(np.random.randn(500, 100))\n        self.df_wide_nans = self.df_wide.where(np.random.random((500, 100)) < 0.9)\n        self.s = pd.Series(np.random.randn(500))\n        self.s2 = pd.Series(np.random.randn(500))"", ""stat_ops.Correlation.time_corr_wide_nans"": ""class Correlation:\n    def time_corr_wide_nans(self, method):\n        self.df_wide_nans.corr(method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Correlation:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(500, 15))\n        self.df2 = pd.DataFrame(np.random.randn(500, 15))\n        self.df_wide = pd.DataFrame(np.random.randn(500, 100))\n        self.df_wide_nans = self.df_wide.where(np.random.random((500, 100)) < 0.9)\n        self.s = pd.Series(np.random.randn(500))\n        self.s2 = pd.Series(np.random.randn(500))"", ""stat_ops.Correlation.time_corrwith_cols"": ""class Correlation:\n    def time_corrwith_cols(self, method):\n        self.df.corrwith(self.df2, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Correlation:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(500, 15))\n        self.df2 = pd.DataFrame(np.random.randn(500, 15))\n        self.df_wide = pd.DataFrame(np.random.randn(500, 100))\n        self.df_wide_nans = self.df_wide.where(np.random.random((500, 100)) < 0.9)\n        self.s = pd.Series(np.random.randn(500))\n        self.s2 = pd.Series(np.random.randn(500))"", ""stat_ops.Correlation.time_corrwith_rows"": ""class Correlation:\n    def time_corrwith_rows(self, method):\n        self.df.corrwith(self.df2, axis=1, method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Correlation:\n    def setup(self, method):\n        self.df = pd.DataFrame(np.random.randn(500, 15))\n        self.df2 = pd.DataFrame(np.random.randn(500, 15))\n        self.df_wide = pd.DataFrame(np.random.randn(500, 100))\n        self.df_wide_nans = self.df_wide.where(np.random.random((500, 100)) < 0.9)\n        self.s = pd.Series(np.random.randn(500))\n        self.s2 = pd.Series(np.random.randn(500))"", ""stat_ops.FrameMultiIndexOps.time_op"": ""class FrameMultiIndexOps:\n    def time_op(self, op):\n        self.df_func()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FrameMultiIndexOps:\n    def setup(self, op):\n        levels = [np.arange(10), np.arange(100), np.arange(100)]\n        codes = [\n            np.arange(10).repeat(10000),\n            np.tile(np.arange(100).repeat(100), 10),\n            np.tile(np.tile(np.arange(100), 100), 10),\n        ]\n        index = pd.MultiIndex(levels=levels, codes=codes)\n        df = pd.DataFrame(np.random.randn(len(index), 4), index=index)\n        self.df_func = getattr(df, op)"", ""stat_ops.FrameOps.time_op"": ""class FrameOps:\n    def time_op(self, op, dtype, axis):\n        self.df_func(axis=axis)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass FrameOps:\n    def setup(self, op, dtype, axis):\n        values = np.random.randn(100000, 4)\n        if dtype == \""Int64\"":\n            values = values.astype(int)\n        df = pd.DataFrame(values).astype(dtype)\n        self.df_func = getattr(df, op)"", ""stat_ops.SeriesOps.time_op"": ""class SeriesOps:\n    def time_op(self, op, dtype):\n        self.s_func()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SeriesOps:\n    def setup(self, op, dtype):\n        s = pd.Series(np.random.randn(100000)).astype(dtype)\n        self.s_func = getattr(s, op)""}",stat_ops,time,0.0
30074,module-level,"terminus-2,oracle",1.0982606407637994,1.0982606407637994,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,multiindex_object,multiindex_object,"{""multiindex_object.GetLoc.time_large_get_loc_warm"": ""class GetLoc:\n    def time_large_get_loc_warm(self):\n        for _ in range(1000):\n            self.mi_large.get_loc((999, 19, \""Z\""))\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GetLoc:\n    def setup(self):\n        self.mi_large = MultiIndex.from_product(\n            [np.arange(1000), np.arange(20), list(string.ascii_letters)],\n            names=[\""one\"", \""two\"", \""three\""],\n        )\n        self.mi_med = MultiIndex.from_product(\n            [np.arange(1000), np.arange(10), list(\""A\"")], names=[\""one\"", \""two\"", \""three\""]\n        )\n        self.mi_small = MultiIndex.from_product(\n            [np.arange(100), list(\""A\""), list(\""A\"")], names=[\""one\"", \""two\"", \""three\""]\n        )"", ""multiindex_object.GetLoc.time_med_get_loc"": ""class GetLoc:\n    def time_med_get_loc(self):\n        self.mi_med.get_loc((999, 9, \""A\""))\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass GetLoc:\n    def setup(self):\n        self.mi_large = MultiIndex.from_product(\n            [np.arange(1000), np.arange(20), list(string.ascii_letters)],\n            names=[\""one\"", \""two\"", \""three\""],\n        )\n        self.mi_med = MultiIndex.from_product(\n            [np.arange(1000), np.arange(10), list(\""A\"")], names=[\""one\"", \""two\"", \""three\""]\n        )\n        self.mi_small = MultiIndex.from_product(\n            [np.arange(100), list(\""A\""), list(\""A\"")], names=[\""one\"", \""two\"", \""three\""]\n        )"", ""multiindex_object.Integer.time_is_monotonic"": ""class Integer:\n    def time_is_monotonic(self):\n        self.mi_int.is_monotonic_increasing\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Integer:\n    def setup(self):\n        self.mi_int = MultiIndex.from_product(\n            [np.arange(1000), np.arange(1000)], names=[\""one\"", \""two\""]\n        )\n        self.obj_index = np.array(\n            [\n                (0, 10),\n                (0, 11),\n                (0, 12),\n                (0, 13),\n                (0, 14),\n                (0, 15),\n                (0, 16),\n                (0, 17),\n                (0, 18),\n                (0, 19),\n            ],\n            dtype=object,\n        )\n        self.other_mi_many_mismatches = MultiIndex.from_tuples(\n            [\n                (-7, 41),\n                (-2, 3),\n                (-0.7, 5),\n                (0, 0),\n                (0, 1.5),\n                (0, 340),\n                (0, 1001),\n                (1, -4),\n                (1, 20),\n                (1, 1040),\n                (432, -5),\n                (432, 17),\n                (439, 165.5),\n                (998, -4),\n                (998, 24065),\n                (999, 865.2),\n                (999, 1000),\n                (1045, -843),\n            ]\n        )"", ""multiindex_object.SetOperations.time_operation"": ""class SetOperations:\n    def time_operation(self, index_structure, dtype, method, sort):\n        getattr(self.left, method)(self.right, sort=sort)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass SetOperations:\n    def setup(self, index_structure, dtype, method, sort):\n        N = 10**5\n        level1 = range(1000)\n    \n        level2 = date_range(start=\""1/1/2000\"", periods=N // 1000)\n        dates_left = MultiIndex.from_product([level1, level2])\n    \n        level2 = range(N // 1000)\n        int_left = MultiIndex.from_product([level1, level2])\n    \n        level2 = tm.makeStringIndex(N // 1000).values\n        str_left = MultiIndex.from_product([level1, level2])\n    \n        level2 = range(N // 1000)\n        ea_int_left = MultiIndex.from_product([level1, Series(level2, dtype=\""Int64\"")])\n    \n        data = {\n            \""datetime\"": dates_left,\n            \""int\"": int_left,\n            \""string\"": str_left,\n            \""ea_int\"": ea_int_left,\n        }\n    \n        if index_structure == \""non_monotonic\"":\n            data = {k: mi[::-1] for k, mi in data.items()}\n    \n        data = {k: {\""left\"": mi, \""right\"": mi[:-1]} for k, mi in data.items()}\n        self.left = data[dtype][\""left\""]\n        self.right = data[dtype][\""right\""]""}",multiindex_object,time,0.0
30087,module-level,"terminus-2,oracle",0.9763405262509968,0.9763405262509968,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timeseries,timeseries,"{""timeseries.DatetimeAccessor.time_dt_accessor_day_name"": ""class DatetimeAccessor:\n    def time_dt_accessor_day_name(self, tz):\n        self.series.dt.day_name()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeAccessor:\n    def setup(self, tz):\n        N = 100000\n        self.series = Series(date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"", tz=tz))"", ""timeseries.DatetimeAccessor.time_dt_accessor_month_name"": ""class DatetimeAccessor:\n    def time_dt_accessor_month_name(self, tz):\n        self.series.dt.month_name()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeAccessor:\n    def setup(self, tz):\n        N = 100000\n        self.series = Series(date_range(start=\""1/1/2000\"", periods=N, freq=\""T\"", tz=tz))"", ""timeseries.DatetimeIndex.time_get"": ""class DatetimeIndex:\n    def time_get(self, index_type):\n        self.index[0]\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]"", ""timeseries.DatetimeIndex.time_timeseries_is_month_start"": ""class DatetimeIndex:\n    def time_timeseries_is_month_start(self, index_type):\n        self.index.is_month_start\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]"", ""timeseries.DatetimeIndex.time_to_date"": ""class DatetimeIndex:\n    def time_to_date(self, index_type):\n        self.index.date\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]"", ""timeseries.DatetimeIndex.time_to_time"": ""class DatetimeIndex:\n    def time_to_time(self, index_type):\n        self.index.time\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass DatetimeIndex:\n    def setup(self, index_type):\n        N = 100000\n        dtidxes = {\n            \""dst\"": date_range(\n                start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n            ),\n            \""repeated\"": date_range(start=\""2000\"", periods=N / 10, freq=\""s\"").repeat(10),\n            \""tz_aware\"": date_range(start=\""2000\"", periods=N, freq=\""s\"", tz=\""US/Eastern\""),\n            \""tz_local\"": date_range(\n                start=\""2000\"", periods=N, freq=\""s\"", tz=dateutil.tz.tzlocal()\n            ),\n            \""tz_naive\"": date_range(start=\""2000\"", periods=N, freq=\""s\""),\n        }\n        self.index = dtidxes[index_type]"", ""timeseries.ResetIndex.time_reset_datetimeindex"": ""class ResetIndex:\n    def time_reset_datetimeindex(self, tz):\n        self.df.reset_index()\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass ResetIndex:\n    def setup(self, tz):\n        idx = date_range(start=\""1/1/2000\"", periods=1000, freq=\""H\"", tz=tz)\n        self.df = DataFrame(np.random.randn(1000, 2), index=idx)"", ""timeseries.TzLocalize.time_infer_dst"": ""class TzLocalize:\n    def time_infer_dst(self, tz):\n        self.index.tz_localize(tz, ambiguous=\""infer\"")\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass TzLocalize:\n    def setup(self, tz):\n        dst_rng = date_range(\n            start=\""10/29/2000 1:00:00\"", end=\""10/29/2000 1:59:59\"", freq=\""S\""\n        )\n        self.index = date_range(start=\""10/29/2000\"", end=\""10/29/2000 00:59:59\"", freq=\""S\"")\n        self.index = self.index.append(dst_rng)\n        self.index = self.index.append(dst_rng)\n        self.index = self.index.append(\n            date_range(start=\""10/29/2000 2:00:00\"", end=\""10/29/2000 3:00:00\"", freq=\""S\"")\n        )""}",timeseries,time,0.0
30077,module-level,"terminus-2,oracle",0.9804572743060842,0.9804572743060842,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,reindex,reindex,"{""reindex.Fillna.time_reindexed"": ""class Fillna:\n    def time_reindexed(self, method):\n        self.ts_reindexed.fillna(method=method)\n\ndef setup(*args, **kwargs):\n    # This function just needs to be imported into each benchmark file to\n    # set up the random seed before each function.\n    # https://asv.readthedocs.io/en/latest/writing_benchmarks.html\n    np.random.seed(1234)\n\nclass Fillna:\n    def setup(self, method):\n        N = 100000\n        self.idx = date_range(\""1/1/2000\"", periods=N, freq=\""1min\"")\n        ts = Series(np.random.randn(N), index=self.idx)[::2]\n        self.ts_reindexed = ts.reindex(self.idx)\n        self.ts_float32 = self.ts_reindexed.astype(\""float32\"")""}",reindex,time,0.0
30089,module-level,"terminus-2,oracle",0.9708070656940136,0.9708070656940136,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.normalize,tslibs.normalize,"{""tslibs.normalize.Normalize.time_is_date_array_normalized"": ""class Normalize:\n    def time_is_date_array_normalized(self, size, tz):\n        # TODO: cases with different levels of short-circuiting\n        # 10 i.e. NPY_FR_ns\n        is_date_array_normalized(self.i8data, tz, 10)\n\n    def setup(self, size, tz):\n        # use an array that will have is_date_array_normalized give True,\n        #  so we do not short-circuit early.\n        dti = pd.date_range(\""2016-01-01\"", periods=10, tz=tz).repeat(size // 10)\n        self.i8data = dti.asi8\n    \n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError"", ""tslibs.normalize.Normalize.time_normalize_i8_timestamps"": ""class Normalize:\n    def time_normalize_i8_timestamps(self, size, tz):\n        # 10 i.e. NPY_FR_ns\n        normalize_i8_timestamps(self.i8data, tz, 10)\n\n    def setup(self, size, tz):\n        # use an array that will have is_date_array_normalized give True,\n        #  so we do not short-circuit early.\n        dti = pd.date_range(\""2016-01-01\"", periods=10, tz=tz).repeat(size // 10)\n        self.i8data = dti.asi8\n    \n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError""}",tslibs.normalize,time,0.0
30084,module-level,"terminus-2,oracle",1.0244894724021607,1.0244894724021607,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strftime,strftime,"{""strftime.BusinessHourStrftime.time_frame_offset_repr"": ""class BusinessHourStrftime:\n    def time_frame_offset_repr(self, obs):\n        self.data[\""off\""].apply(repr)\n\n    def setup(self, obs):\n        self.data = pd.DataFrame(\n            {\n                \""off\"": [offsets.BusinessHour()] * obs,\n            }\n        )"", ""strftime.BusinessHourStrftime.time_frame_offset_str"": ""class BusinessHourStrftime:\n    def time_frame_offset_str(self, obs):\n        self.data[\""off\""].apply(str)\n\n    def setup(self, obs):\n        self.data = pd.DataFrame(\n            {\n                \""off\"": [offsets.BusinessHour()] * obs,\n            }\n        )"", ""strftime.DatetimeStrftime.time_frame_datetime_to_str"": ""class DatetimeStrftime:\n    def time_frame_datetime_to_str(self, obs):\n        self.data[\""dt\""].astype(str)\n\n    def setup(self, obs):\n        d = \""2018-11-29\""\n        dt = \""2018-11-26 11:18:27.0\""\n        self.data = pd.DataFrame(\n            {\n                \""dt\"": [np.datetime64(dt)] * obs,\n                \""d\"": [np.datetime64(d)] * obs,\n                \""r\"": [np.random.uniform()] * obs,\n            }\n        )""}",strftime,time,0.0
30088,module-level,"terminus-2,oracle",1.0199728954916425,1.0199728954916425,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.fields,tslibs.fields,"{""tslibs.fields.TimeGetDateField.time_get_date_field"": ""class TimeGetDateField:\n    def time_get_date_field(self, size, field):\n        get_date_field(self.i8data, field)\n\n    def setup(self, size, field):\n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr"", ""tslibs.fields.TimeGetStartEndField.time_get_start_end_field"": ""class TimeGetStartEndField:\n    def time_get_start_end_field(self, size, side, period, freqstr, month_kw):\n        get_start_end_field(self.i8data, self.attrname, freqstr, month_kw=month_kw)\n\n    def setup(self, size, side, period, freqstr, month_kw):\n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr\n    \n        self.attrname = f\""is_{period}_{side}\"""", ""tslibs.fields.TimeGetTimedeltaField.time_get_timedelta_field"": ""class TimeGetTimedeltaField:\n    def time_get_timedelta_field(self, size, field):\n        get_timedelta_field(self.i8data, field)\n\n    def setup(self, size, field):\n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.fields,time,0.0
30094,module-level,"terminus-2,oracle",1.0169829311759029,1.0169829311759029,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timestamp,tslibs.timestamp,"{""tslibs.timestamp.TimestampAcrossDst.time_replace_across_dst"": ""class TimestampAcrossDst:\n    def time_replace_across_dst(self):\n        self.ts2.replace(tzinfo=self.tzinfo)\n\n    def setup(self):\n        dt = datetime(2016, 3, 27, 1)\n        self.tzinfo = pytz.timezone(\""CET\"").localize(dt, is_dst=False).tzinfo\n        self.ts2 = Timestamp(dt)"", ""tslibs.timestamp.TimestampConstruction.time_from_datetime_aware"": ""class TimestampConstruction:\n    def time_from_datetime_aware(self):\n        Timestamp(self.dttime_aware)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampConstruction.time_from_datetime_unaware"": ""class TimestampConstruction:\n    def time_from_datetime_unaware(self):\n        Timestamp(self.dttime_unaware)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampConstruction.time_from_npdatetime64"": ""class TimestampConstruction:\n    def time_from_npdatetime64(self):\n        Timestamp(self.npdatetime64)\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampConstruction.time_parse_dateutil"": ""class TimestampConstruction:\n    def time_parse_dateutil(self):\n        Timestamp(\""2017/08/25 08:16:14 AM\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampConstruction.time_parse_iso8601_no_tz"": ""class TimestampConstruction:\n    def time_parse_iso8601_no_tz(self):\n        Timestamp(\""2017-08-25 08:16:14\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampConstruction.time_parse_iso8601_tz"": ""class TimestampConstruction:\n    def time_parse_iso8601_tz(self):\n        Timestamp(\""2017-08-25 08:16:14-0500\"")\n\n    def setup(self):\n        self.npdatetime64 = np.datetime64(\""2020-01-01 00:00:00\"")\n        self.dttime_unaware = datetime(2020, 1, 1, 0, 0, 0)\n        self.dttime_aware = datetime(2020, 1, 1, 0, 0, 0, 0, pytz.UTC)\n        self.ts = Timestamp(\""2020-01-01 00:00:00\"")"", ""tslibs.timestamp.TimestampOps.time_ceil"": ""class TimestampOps:\n    def time_ceil(self, tz):\n        self.ts.ceil(\""5T\"")\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_floor"": ""class TimestampOps:\n    def time_floor(self, tz):\n        self.ts.floor(\""5T\"")\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_normalize"": ""class TimestampOps:\n    def time_normalize(self, tz):\n        self.ts.normalize()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_replace_None"": ""class TimestampOps:\n    def time_replace_None(self, tz):\n        self.ts.replace(tzinfo=None)\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_replace_tz"": ""class TimestampOps:\n    def time_replace_tz(self, tz):\n        self.ts.replace(tzinfo=pytz.timezone(\""US/Eastern\""))\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_to_julian_date"": ""class TimestampOps:\n    def time_to_julian_date(self, tz):\n        self.ts.to_julian_date()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_tz_convert"": ""class TimestampOps:\n    def time_tz_convert(self, tz):\n        if self.ts.tz is not None:\n            self.ts.tz_convert(tz)\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampOps.time_tz_localize"": ""class TimestampOps:\n    def time_tz_localize(self, tz):\n        if self.ts.tz is None:\n            self.ts.tz_localize(tz)\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tz=tz)"", ""tslibs.timestamp.TimestampProperties.time_dayofyear"": ""class TimestampProperties:\n    def time_dayofyear(self, tz):\n        self.ts.dayofyear\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_days_in_month"": ""class TimestampProperties:\n    def time_days_in_month(self, tz):\n        self.ts.days_in_month\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_leap_year"": ""class TimestampProperties:\n    def time_is_leap_year(self, tz):\n        self.ts.is_leap_year\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_quarter_end"": ""class TimestampProperties:\n    def time_is_quarter_end(self, tz):\n        self.ts.is_quarter_end\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_quarter_start"": ""class TimestampProperties:\n    def time_is_quarter_start(self, tz):\n        self.ts.is_quarter_start\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_year_end"": ""class TimestampProperties:\n    def time_is_year_end(self, tz):\n        self.ts.is_year_end\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_is_year_start"": ""class TimestampProperties:\n    def time_is_year_start(self, tz):\n        self.ts.is_year_start\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_microsecond"": ""class TimestampProperties:\n    def time_microsecond(self, tz):\n        self.ts.microsecond\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_month_name"": ""class TimestampProperties:\n    def time_month_name(self, tz):\n        self.ts.month_name()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_quarter"": ""class TimestampProperties:\n    def time_quarter(self, tz):\n        self.ts.quarter\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_week"": ""class TimestampProperties:\n    def time_week(self, tz):\n        self.ts.week\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)"", ""tslibs.timestamp.TimestampProperties.time_weekday_name"": ""class TimestampProperties:\n    def time_weekday_name(self, tz):\n        self.ts.day_name()\n\n    def setup(self, tz):\n        self.ts = Timestamp(\""2017-08-25 08:16:14\"", tzinfo=tz)""}",tslibs.timestamp,time,0.0
30086,module-level,"terminus-2,oracle",1.0412118411933815,1.0412118411933815,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,timedelta,timedelta,"{""timedelta.DatetimeAccessor.time_timedelta_seconds"": ""class DatetimeAccessor:\n    def time_timedelta_seconds(self, series):\n        series.dt.seconds\n\n    def setup_cache(self):\n        N = 100000\n        series = Series(timedelta_range(\""1 days\"", periods=N, freq=\""h\""))\n        return series"", ""timedelta.TimedeltaIndexing.time_intersection"": ""class TimedeltaIndexing:\n    def time_intersection(self):\n        self.index.intersection(self.index2)\n\n    def setup(self):\n        self.index = timedelta_range(start=\""1985\"", periods=1000, freq=\""D\"")\n        self.index2 = timedelta_range(start=\""1986\"", periods=1000, freq=\""D\"")\n        self.series = Series(range(1000), index=self.index)\n        self.timedelta = self.index[500]"", ""timedelta.TimedeltaIndexing.time_series_loc"": ""class TimedeltaIndexing:\n    def time_series_loc(self):\n        self.series.loc[self.timedelta]\n\n    def setup(self):\n        self.index = timedelta_range(start=\""1985\"", periods=1000, freq=\""D\"")\n        self.index2 = timedelta_range(start=\""1986\"", periods=1000, freq=\""D\"")\n        self.series = Series(range(1000), index=self.index)\n        self.timedelta = self.index[500]""}",timedelta,time,0.0
30093,module-level,"terminus-2,oracle",1.023426105382882,1.023426105382882,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.timedelta,tslibs.timedelta,"{""tslibs.timedelta.TimedeltaConstructor.time_from_datetime_timedelta"": ""class TimedeltaConstructor:\n    def time_from_datetime_timedelta(self):\n        Timedelta(self.dttimedelta)\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")"", ""tslibs.timedelta.TimedeltaConstructor.time_from_iso_format"": ""class TimedeltaConstructor:\n    def time_from_iso_format(self):\n        Timedelta(\""P4DT12H30M5S\"")\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")"", ""tslibs.timedelta.TimedeltaConstructor.time_from_np_timedelta"": ""class TimedeltaConstructor:\n    def time_from_np_timedelta(self):\n        Timedelta(self.nptimedelta64)\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")"", ""tslibs.timedelta.TimedeltaConstructor.time_from_string"": ""class TimedeltaConstructor:\n    def time_from_string(self):\n        Timedelta(\""1 days\"")\n\n    def setup(self):\n        self.nptimedelta64 = np.timedelta64(3600)\n        self.dttimedelta = datetime.timedelta(seconds=3600)\n        self.td = Timedelta(3600, unit=\""s\"")""}",tslibs.timedelta,time,0.0
30090,module-level,"terminus-2,oracle",1.0440590721071132,1.0440590721071132,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.offsets,tslibs.offsets,"{""tslibs.offsets.OffestDatetimeArithmetic.time_add"": ""class OffestDatetimeArithmetic:\n    def time_add(self, offset):\n        self.date + offset\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")"", ""tslibs.offsets.OffestDatetimeArithmetic.time_add_10"": ""class OffestDatetimeArithmetic:\n    def time_add_10(self, offset):\n        self.date + (10 * offset)\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")"", ""tslibs.offsets.OffestDatetimeArithmetic.time_add_np_dt64"": ""class OffestDatetimeArithmetic:\n    def time_add_np_dt64(self, offset):\n        offset + self.dt64\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")"", ""tslibs.offsets.OffestDatetimeArithmetic.time_subtract"": ""class OffestDatetimeArithmetic:\n    def time_subtract(self, offset):\n        self.date - offset\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")"", ""tslibs.offsets.OffestDatetimeArithmetic.time_subtract_10"": ""class OffestDatetimeArithmetic:\n    def time_subtract_10(self, offset):\n        self.date - (10 * offset)\n\n    def setup(self, offset):\n        self.date = datetime(2011, 1, 1)\n        self.dt64 = np.datetime64(\""2011-01-01 09:00Z\"")"", ""tslibs.offsets.OnOffset.time_on_offset"": ""class OnOffset:\n    def time_on_offset(self, offset):\n        for date in self.dates:\n            offset.is_on_offset(date)\n\n    def setup(self, offset):\n        self.dates = [\n            datetime(2016, m, d)\n            for m in [10, 11, 12]\n            for d in [1, 2, 3, 28, 29, 30, 31]\n            if not (m == 11 and d == 31)\n        ]""}",tslibs.offsets,time,0.0
30085,module-level,"terminus-2,oracle",1.061318817259515,1.061318817259515,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,strings,strings,"{""strings.Contains.time_contains"": ""class Contains:\n    def time_contains(self, dtype, regex):\n        self.s.str.contains(\""A\"", regex=regex)\n\n    def setup(self, dtype, regex):\n        super().setup(dtype)"", ""strings.Dummies.time_get_dummies"": ""class Dummies:\n    def time_get_dummies(self, dtype):\n        self.s.str.get_dummies(\""|\"")\n\n    def setup(self, dtype):\n        super().setup(dtype)\n        self.s = self.s.str.join(\""|\"")"", ""strings.Methods.time_endswith"": ""class Methods:\n    def time_endswith(self, dtype):\n        self.s.str.endswith(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_findall"": ""class Methods:\n    def time_findall(self, dtype):\n        self.s.str.findall(\""[A-Z]+\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_fullmatch"": ""class Methods:\n    def time_fullmatch(self, dtype):\n        self.s.str.fullmatch(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_get"": ""class Methods:\n    def time_get(self, dtype):\n        self.s.str.get(0)\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_isdigit"": ""class Methods:\n    def time_isdigit(self, dtype):\n        self.s.str.isdigit()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_istitle"": ""class Methods:\n    def time_istitle(self, dtype):\n        self.s.str.istitle()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_lower"": ""class Methods:\n    def time_lower(self, dtype):\n        self.s.str.lower()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_match"": ""class Methods:\n    def time_match(self, dtype):\n        self.s.str.match(\""A\"")\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_upper"": ""class Methods:\n    def time_upper(self, dtype):\n        self.s.str.upper()\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError"", ""strings.Methods.time_wrap"": ""class Methods:\n    def time_wrap(self, dtype):\n        self.s.str.wrap(10)\n\nclass Dtypes:\n    def setup(self, dtype):\n        try:\n            self.s = Series(tm.makeStringIndex(10**5), dtype=dtype)\n        except ImportError:\n            raise NotImplementedError""}",strings,time,0.0
30095,module-level,"terminus-2,oracle",1.0124058949042414,1.0124058949042414,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.tslib,tslibs.tslib,"{""tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime"": ""class TimeIntsToPydatetime:\n    def time_ints_to_pydatetime(self, box, size, tz):\n        ints_to_pydatetime(self.i8data, tz, box=box)\n\n    def setup(self, box, size, tz):\n        if box == \""date\"" and tz is not None:\n            # tz is ignored, so avoid running redundant benchmarks\n            raise NotImplementedError  # skip benchmark\n        if size == 10**6 and tz is _tzs[-1]:\n            # This is cumbersomely-slow, so skip to trim runtime\n            raise NotImplementedError  # skip benchmark\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.tslib,time,0.0
30092,module-level,"terminus-2,oracle",1.0065466865962085,1.0065466865962085,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.resolution,tslibs.resolution,"{""tslibs.resolution.TimeResolution.time_get_resolution"": ""class TimeResolution:\n    def time_get_resolution(self, unit, size, tz):\n        get_resolution(self.i8data, tz)\n\n    def setup(self, unit, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        arr = arr.view(f\""M8[{unit}]\"").astype(\""M8[ns]\"").view(\""i8\"")\n        self.i8data = arr""}",tslibs.resolution,time,0.0
30096,module-level,"terminus-2,oracle",1.0302587692888638,1.0302587692888638,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.tz_convert,tslibs.tz_convert,"{""tslibs.tz_convert.TimeTZConvert.time_tz_convert_from_utc"": ""class TimeTZConvert:\n    def time_tz_convert_from_utc(self, size, tz):\n        # effectively:\n        #  dti = DatetimeIndex(self.i8data, tz=tz)\n        #  dti.tz_localize(None)\n        if old_sig:\n            tz_convert_from_utc(self.i8data, UTC, tz)\n        else:\n            tz_convert_from_utc(self.i8data, tz)\n\n    def setup(self, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr"", ""tslibs.tz_convert.TimeTZConvert.time_tz_localize_to_utc"": ""class TimeTZConvert:\n    def time_tz_localize_to_utc(self, size, tz):\n        # effectively:\n        #  dti = DatetimeIndex(self.i8data)\n        #  dti.tz_localize(tz, ambiguous=\""NaT\"", nonexistent=\""NaT\"")\n        tz_localize_to_utc(self.i8data, tz, ambiguous=\""NaT\"", nonexistent=\""NaT\"")\n\n    def setup(self, size, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.random.randint(0, 10, size=size, dtype=\""i8\"")\n        self.i8data = arr""}",tslibs.tz_convert,time,0.0
30091,module-level,"terminus-2,oracle",1.0023979957946785,1.0023979957946785,pandas_dev-pandas_15,/recordings/2025-10-01__09-26-33/pandas_dev-pandas_15/pandas_dev-pandas_15.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,pandas_dev-pandas,tslibs.period,tslibs.period,"{""tslibs.period.PeriodConstructor.time_period_constructor"": ""class PeriodConstructor:\n    def time_period_constructor(self, freq, is_offset):\n        Period(\""2012-06-01\"", freq=freq)\n\n    def setup(self, freq, is_offset):\n        if is_offset:\n            self.freq = to_offset(freq)\n        else:\n            self.freq = freq"", ""tslibs.period.PeriodProperties.time_property"": ""class PeriodProperties:\n    def time_property(self, freq, attr):\n        getattr(self.per, attr)\n\n    def setup(self, freq, attr):\n        self.per = Period(\""2012-06-01\"", freq=freq)"", ""tslibs.period.PeriodUnaryMethods.time_asfreq"": ""class PeriodUnaryMethods:\n    def time_asfreq(self, freq):\n        self.per.asfreq(\""A\"")\n\n    def setup(self, freq):\n        self.per = Period(\""2012-06-01\"", freq=freq)"", ""tslibs.period.PeriodUnaryMethods.time_now"": ""class PeriodUnaryMethods:\n    def time_now(self, freq):\n        self.per.now(freq)\n\n    def setup(self, freq):\n        self.per = Period(\""2012-06-01\"", freq=freq)"", ""tslibs.period.PeriodUnaryMethods.time_to_timestamp"": ""class PeriodUnaryMethods:\n    def time_to_timestamp(self, freq):\n        self.per.to_timestamp()\n\n    def setup(self, freq):\n        self.per = Period(\""2012-06-01\"", freq=freq)"", ""tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr"": ""class TimeDT64ArrToPeriodArr:\n    def time_dt64arr_to_periodarr(self, size, freq, tz):\n        dt64arr_to_periodarr(self.i8values, freq, tz)\n\n    def setup(self, size, freq, tz):\n        if size == 10**6 and tz is tzlocal_obj:\n            # tzlocal is cumbersomely slow, so skip to keep runtime in check\n            raise NotImplementedError\n    \n        arr = np.arange(10, dtype=\""i8\"").repeat(size // 10)\n        self.i8values = arr"", ""tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr"": ""class TimePeriodArrToDT64Arr:\n    def time_periodarray_to_dt64arr(self, size, freq):\n        periodarr_to_dt64arr(self.i8values, freq)\n\n    def setup(self, size, freq):\n        arr = np.arange(10, dtype=\""i8\"").repeat(size // 10)\n        self.i8values = arr""}",tslibs.period,time,0.0
31465,module-level,"terminus-2,claude",1.1238999263904288,1.7805308932167616,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,ensemble,ensemble,"{""ensemble.HistGradientBoostingClassifierBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass HistGradientBoostingClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""ensemble.RandomForestClassifierBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RandomForestClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""ensemble.RandomForestClassifierBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RandomForestClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",ensemble,time,-0.46437833580363
31464,module-level,"terminus-2,claude",1.008237729969542,0.9669090938887188,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,cluster,cluster,"{""cluster.KMeansBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""cluster.MiniBatchKMeansBenchmark.time_transform"": ""class Transformer:\n    def time_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchKMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",cluster,time,0.02922817261727239
31471,module-level,"terminus-2,oracle",1.2599504172927103,1.2599504172927103,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,cluster,cluster,"{""cluster.KMeansBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""cluster.KMeansBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""cluster.KMeansBenchmark.time_transform"": ""class Transformer:\n    def time_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""cluster.MiniBatchKMeansBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchKMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""cluster.MiniBatchKMeansBenchmark.time_transform"": ""class Transformer:\n    def time_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchKMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",cluster,time,0.0
31475,module-level,"terminus-2,oracle",0.9564125578038288,0.9564125578038288,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,neighbors,neighbors,"{""neighbors.KNeighborsClassifierBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KNeighborsClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""neighbors.KNeighborsClassifierBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KNeighborsClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",neighbors,time,0.0
31474,module-level,"terminus-2,oracle",0.9384013064633068,0.9384013064633068,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,metrics,metrics,"{""metrics.PairwiseDistancesBenchmark.time_pairwise_distances"": ""class PairwiseDistancesBenchmark:\n    def time_pairwise_distances(self, *args):\n        pairwise_distances(self.X, **self.pdist_params)\n\n    def setup(self, *params):\n        representation, metric, n_jobs = params\n    \n        if representation == \""sparse\"" and metric == \""correlation\"":\n            raise NotImplementedError\n    \n        if Benchmark.data_size == \""large\"":\n            if metric in (\""manhattan\"", \""correlation\""):\n                n_samples = 8000\n            else:\n                n_samples = 24000\n        else:\n            if metric in (\""manhattan\"", \""correlation\""):\n                n_samples = 4000\n            else:\n                n_samples = 12000\n    \n        data = _random_dataset(n_samples=n_samples, representation=representation)\n        self.X, self.X_val, self.y, self.y_val = data\n    \n        self.pdist_params = {\""metric\"": metric, \""n_jobs\"": n_jobs}""}",metrics,time,0.0
31472,module-level,"terminus-2,oracle",1.5203797092703242,1.5203797092703242,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,ensemble,ensemble,"{""ensemble.HistGradientBoostingClassifierBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass HistGradientBoostingClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""ensemble.HistGradientBoostingClassifierBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass HistGradientBoostingClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""ensemble.RandomForestClassifierBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RandomForestClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",ensemble,time,0.0
31473,module-level,"terminus-2,oracle",0.9010748874892208,0.9010748874892208,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,linear_model,linear_model,"{""linear_model.ElasticNetBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass ElasticNetBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""linear_model.LassoBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LassoBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""linear_model.LogisticRegressionBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LogisticRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""linear_model.SGDRegressorBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SGDRegressorBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""linear_model.SGDRegressorBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SGDRegressorBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",linear_model,time,0.0
31466,module-level,"terminus-2,claude",0.9002117666510273,0.9384013064633068,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-3-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,metrics,metrics,"{""metrics.PairwiseDistancesBenchmark.time_pairwise_distances"": ""class PairwiseDistancesBenchmark:\n    def time_pairwise_distances(self, *args):\n        pairwise_distances(self.X, **self.pdist_params)\n\n    def setup(self, *params):\n        representation, metric, n_jobs = params\n    \n        if representation == \""sparse\"" and metric == \""correlation\"":\n            raise NotImplementedError\n    \n        if Benchmark.data_size == \""large\"":\n            if metric in (\""manhattan\"", \""correlation\""):\n                n_samples = 8000\n            else:\n                n_samples = 24000\n        else:\n            if metric in (\""manhattan\"", \""correlation\""):\n                n_samples = 4000\n            else:\n                n_samples = 12000\n    \n        data = _random_dataset(n_samples=n_samples, representation=representation)\n        self.X, self.X_val, self.y, self.y_val = data\n    \n        self.pdist_params = {\""metric\"": metric, \""n_jobs\"": n_jobs}""}",metrics,time,-0.027008161111937386
31469,module-level,"terminus-2,gpt-5",1.1208197595977412,1.010540831267532,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,linear_model,linear_model,"{""linear_model.LassoBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LassoBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""linear_model.LogisticRegressionBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LogisticRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""linear_model.LogisticRegressionBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LogisticRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""linear_model.SGDRegressorBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SGDRegressorBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",linear_model,time,0.07799075553763031
31468,module-level,"terminus-2,gpt-5",1.3599448604649618,1.7805308932167616,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,ensemble,ensemble,"{""ensemble.GradientBoostingClassifierBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass GradientBoostingClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""ensemble.HistGradientBoostingClassifierBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass HistGradientBoostingClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""ensemble.RandomForestClassifierBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RandomForestClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",ensemble,time,-0.29744415328981605
31467,module-level,"terminus-2,gpt-5",1.0349332922636336,1.513804722063677,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,cluster,cluster,"{""cluster.KMeansBenchmark.time_transform"": ""class Transformer:\n    def time_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",cluster,time,-0.3386643775106389
31470,module-level,"terminus-2,gpt-5",0.9701632232173348,0.9771958841499164,scikit-learn_scikit-learn_9,/recordings/2025-10-01__09-26-33/scikit-learn_scikit-learn_9/scikit-learn_scikit-learn_9.1-of-1.2025-10-01__09-26-33.agent-4-terminus-2/sessions/agent.cast,scikit-learn_scikit-learn,neighbors,neighbors,"{""neighbors.KNeighborsClassifierBenchmark.time_fit"": ""class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KNeighborsClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()"", ""neighbors.KNeighborsClassifierBenchmark.time_predict"": ""class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \""\""\""Generate dataset and load the fitted estimator\""\""\""\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(\n            self, Benchmark.save_dir, params, Benchmark.save_estimators\n        )\n        with est_path.open(mode=\""rb\"") as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KNeighborsClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()""}",neighbors,time,-0.004973593304513148
