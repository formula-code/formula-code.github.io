{"version": 2, "width": 160, "height": 40, "timestamp": 1762995347, "env": {"SHELL": "/bin/bash", "TERM": "screen"}}
[0.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[0.002, "i", "asciinema rec --stdin /logs/agent.cast\r"]
[0.004, "o", "asciinema rec --stdin /logs/agent.cast\r\n"]
[1.242, "o", "\u001b[?2004l\r\n"]
[2.478, "o", "\u001b[0;31masciinema: /logs/agent.cast already exists, aborting\u001b[0m\r\n"]
[3.714, "o", "\u001b[0;31masciinema: use --overwrite option if you want to overwrite existing recording\u001b[0m\r\n"]
[5.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[5.002, "i", "clear\r"]
[5.004, "o", "clear\r\n"]
[7.478, "o", "\u001b[?2004l\r\n"]
[10.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[10.002, "i", "ls -la\r"]
[10.004, "o", "ls -la\r\n"]
[10.143333, "o", "\u001b[?2004l\r\n"]
[10.280667, "o", "total 176\r\n"]
[10.418, "o", "drwxr-xr-x 1 root root  4096 Oct  1 12:34 \u001b[0m\u001b[01;34m.\u001b[0m\r\n"]
[10.555333, "o", "drwxr-xr-x 1 root root  4096 Sep 15 17:25 \u001b[01;34m..\u001b[0m\r\n"]
[10.692667, "o", "drwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34m.binder\u001b[0m\r\n"]
[10.83, "o", "drwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34m.circleci\u001b[0m\r\n"]
[10.967333, "o", "-rw-r--r-- 1 root root  1392 Sep 13 19:09 .cirrus.star\r\n"]
[11.104667, "o", "-rw-r--r-- 1 root root   999 Sep 13 19:09 .codecov.yml\r\n"]
[11.242, "o", "-rw-r--r-- 1 root root   150 Sep 13 19:09 .coveragerc\r\n"]
[11.379333, "o", "drwxr-xr-x 1 root root  4096 Sep 15 17:27 \u001b[01;34m.git\u001b[0m\r\n"]
[11.516667, "o", "-rw-r--r-- 1 root root  1000 Sep 13 19:09 .git-blame-ignore-revs\r\n"]
[11.654, "o", "drwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34m.github\u001b[0m\r\n"]
[11.791333, "o", "-rw-r--r-- 1 root root  2004 Sep 13 19:09 .gitignore\r\n"]
[11.928667, "o", "-rw-r--r-- 1 root root  7263 Sep 13 19:08 .mailmap\r\n"]
[12.066, "o", "-rw-r--r-- 1 root root   872 Sep 13 19:09 .pre-commit-config.yaml\r\n"]
[12.203333, "o", "-rw-r--r-- 1 root root   645 Sep 13 19:09 CODE_OF_CONDUCT.md\r\n"]
[12.340667, "o", "-rw-r--r-- 1 root root  2109 Sep 13 19:09 CONTRIBUTING.md\r\n"]
[12.478, "o", "-rw-r--r-- 1 root root  1532 Sep 13 19:09 COPYING\r\n"]
[12.615333, "o", "-rw-r--r-- 1 root root   971 Sep 13 19:09 MANIFEST.in\r\n"]
[12.752667, "o", "-rw-r--r-- 1 root root  1491 Sep 13 19:09 Makefile\r\n"]
[12.89, "o", "-rw-r--r-- 1 root root  7604 Sep 13 19:09 README.rst\r\n"]
[13.027333, "o", "-rw-r--r-- 1 root root   692 Sep 13 19:09 SECURITY.md\r\n"]
[13.164667, "o", "drwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34masv_benchmarks\u001b[0m\r\n"]
[13.302, "o", "-rw-r--r-- 1 root root 11978 Sep 13 19:09 azure-pipelines.yml\r\n"]
[13.439333, "o", "drwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34mbenchmarks\u001b[0m\r\n"]
[13.576667, "o", "drwxr-xr-x 4 root root  4096 Sep 15 14:32 \u001b[01;34mbuild\u001b[0m\r\n"]
[13.714, "o", "drwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34mbuild_tools\u001b[0m\r\n"]
[13.851333, "o", "-rw-r--r-- 1 root root   388 Sep 13 19:09 conftest.py\r\n"]
[13.988667, "o", "drwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34mdoc\u001b[0m\r\n"]
[14.126, "o", "drwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34mexamples\u001b[0m\r\n"]
[14.263333, "o", "drwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34mmaint_tools\u001b[0m\r\n"]
[14.400667, "o", "-rw-r--r-- 1 root root  3838 Sep 13 19:09 pyproject.toml\r\n"]
[14.538, "o", "drwxr-xr-x 1 root root  4096 Sep 15 14:32 \u001b[01;34mscikit_learn.egg-info\u001b[0m\r\n"]
[14.675333, "o", "-rw-r--r-- 1 root root  2113 Sep 13 19:09 setup.cfg\r\n"]
[14.812667, "o", "-rwxr-xr-x 1 root root 22566 Sep 13 19:09 \u001b[01;32msetup.py\u001b[0m\r\n"]
[15.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[15.002, "i", "find . -name 'asv*.json' -o -name 'asv_benchmarks.txt'\r"]
[15.004, "o", "find . -name 'asv*.json' -o -name 'asv_benchmarks.txt'\r\n"]
[17.478, "o", "\u001b[?2004l\r\n"]
[20.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[20.002, "i", "ls -la asv_benchmarks/\r"]
[20.004, "o", "ls -la asv_benchmarks/\r\n"]
[20.712286, "o", "\u001b[?2004l\r\n"]
[21.418571, "o", "total 24\r\n"]
[22.124857, "o", "drwxr-xr-x 1 root root 4096 Sep 13 19:09 \u001b[0m\u001b[01;34m.\u001b[0m\r\n"]
[22.831143, "o", "drwxr-xr-x 1 root root 4096 Oct  1 12:34 \u001b[01;34m..\u001b[0m\r\n"]
[23.537429, "o", "-rw-r--r-- 1 root root   66 Sep 13 19:08 .gitignore\r\n"]
[24.243714, "o", "-rw-r--r-- 1 root root 6754 Sep 13 19:09 asv.conf.json\r\n"]
[25.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[25.002, "i", "ls -la asv_benchmarks/benchmarks/\r"]
[25.004, "o", "ls -la asv_benchmarks/benchmarks/\r\n"]
[25.280667, "o", "\u001b[?2004l\r\n"]
[25.555333, "o", "total 76\r\n"]
[25.83, "o", "drwxr-xr-x 1 root root 4096 Sep 13 19:09 \u001b[0m\u001b[01;34m.\u001b[0m\r\n"]
[26.104667, "o", "drwxr-xr-x 1 root root 4096 Sep 13 19:09 \u001b[01;34m..\u001b[0m\r\n"]
[26.379333, "o", "-rw-r--r-- 1 root root   49 Sep 13 19:08 __init__.py\r\n"]
[26.654, "o", "-rw-r--r-- 1 root root 2925 Sep 13 19:08 cluster.py\r\n"]
[26.928667, "o", "-rw-r--r-- 1 root root 7341 Sep 13 19:09 common.py\r\n"]
[27.203333, "o", "-rw-r--r-- 1 root root 1544 Sep 13 19:09 config.json\r\n"]
[27.478, "o", "-rw-r--r-- 1 root root 5176 Sep 13 19:09 datasets.py\r\n"]
[27.752667, "o", "-rw-r--r-- 1 root root 2406 Sep 13 19:08 decomposition.py\r\n"]
[28.027333, "o", "-rw-r--r-- 1 root root 2995 Sep 13 19:08 ensemble.py\r\n"]
[28.302, "o", "-rw-r--r-- 1 root root 6648 Sep 13 19:09 linear_model.py\r\n"]
[28.576667, "o", "-rw-r--r-- 1 root root  820 Sep 13 19:08 manifold.py\r\n"]
[28.851333, "o", "-rw-r--r-- 1 root root 1363 Sep 13 19:08 metrics.py\r\n"]
[29.126, "o", "-rw-r--r-- 1 root root 2371 Sep 13 19:08 model_selection.py\r\n"]
[29.400667, "o", "-rw-r--r-- 1 root root 1140 Sep 13 19:08 neighbors.py\r\n"]
[29.675333, "o", "-rw-r--r-- 1 root root  762 Sep 13 19:08 svm.py\r\n"]
[30.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[30.002, "i", "cat asv_benchmarks/benchmarks/decomposition.py\r"]
[30.004, "o", "cat asv_benchmarks/benchmarks/decomposition.py\r\n"]
[30.056969, "o", "\u001b[?2004l\r\n"]
[30.107938, "o", "from sklearn.decomposition import PCA, DictionaryLearning, MiniBatchDictionaryLearning\r\n"]
[30.158907, "o", "\r\n"]
[30.209876, "o", "from .common import Benchmark, Estimator, Transformer\r\n"]
[30.260845, "o", "from .datasets import _mnist_dataset, _olivetti_faces_dataset\r\n"]
[30.311814, "o", "from .utils import make_dict_learning_scorers, make_pca_scorers\r\n"]
[30.362784, "o", "\r\n"]
[30.413753, "o", "\r\n"]
[30.464722, "o", "class PCABenchmark(Transformer, Estimator, Benchmark):\r\n"]
[30.515691, "o", "    \"\"\"\r\n"]
[30.56666, "o", "    Benchmarks for PCA.\r\n"]
[30.617629, "o", "    \"\"\"\r\n"]
[30.668598, "o", "\r\n"]
[30.719567, "o", "    param_names = [\"svd_solver\"]\r\n"]
[30.770536, "o", "    params = ([\"full\", \"arpack\", \"randomized\"],)\r\n"]
[30.821505, "o", "\r\n"]
[30.872474, "o", "    def setup_cache(self):\r\n"]
[30.923443, "o", "        super().setup_cache()\r\n"]
[30.974412, "o", "\r\n"]
[31.025381, "o", "    def make_data(self, params):\r\n"]
[31.076351, "o", "        return _mnist_dataset()\r\n"]
[31.12732, "o", "\r\n"]
[31.178289, "o", "    def make_estimator(self, params):\r\n"]
[31.229258, "o", "        (svd_solver,) = params\r\n"]
[31.280227, "o", "\r\n"]
[31.331196, "o", "        estimator = PCA(n_components=32, svd_solver=svd_solver, random_state=0)\r\n"]
[31.382165, "o", "\r\n"]
[31.433134, "o", "        return estimator\r\n"]
[31.484103, "o", "\r\n"]
[31.535072, "o", "    def make_scorers(self):\r\n"]
[31.586041, "o", "        make_pca_scorers(self)\r\n"]
[31.63701, "o", "\r\n"]
[31.687979, "o", "\r\n"]
[31.738948, "o", "class DictionaryLearningBenchmark(Transformer, Estimator, Benchmark):\r\n"]
[31.789918, "o", "    \"\"\"\r\n"]
[31.840887, "o", "    Benchmarks for DictionaryLearning.\r\n"]
[31.891856, "o", "    \"\"\"\r\n"]
[31.942825, "o", "\r\n"]
[31.993794, "o", "    param_names = [\"fit_algorithm\", \"n_jobs\"]\r\n"]
[32.044763, "o", "    params = ([\"lars\", \"cd\"], Benchmark.n_jobs_vals)\r\n"]
[32.095732, "o", "\r\n"]
[32.146701, "o", "    def setup_cache(self):\r\n"]
[32.19767, "o", "        super().setup_cache()\r\n"]
[32.248639, "o", "\r\n"]
[32.299608, "o", "    def make_data(self, params):\r\n"]
[32.350577, "o", "        return _olivetti_faces_dataset()\r\n"]
[32.401546, "o", "\r\n"]
[32.452515, "o", "    def make_estimator(self, params):\r\n"]
[32.503485, "o", "        fit_algorithm, n_jobs = params\r\n"]
[32.554454, "o", "\r\n"]
[32.605423, "o", "        estimator = DictionaryLearning(\r\n"]
[32.656392, "o", "            n_components=15,\r\n"]
[32.707361, "o", "            fit_algorithm=fit_algorithm,\r\n"]
[32.75833, "o", "            alpha=0.1,\r\n"]
[32.809299, "o", "            transform_alpha=1,\r\n"]
[32.860268, "o", "            max_iter=20,\r\n"]
[32.911237, "o", "            tol=1e-16,\r\n"]
[32.962206, "o", "            random_state=0,\r\n"]
[33.013175, "o", "            n_jobs=n_jobs,\r\n"]
[33.064144, "o", "        )\r\n"]
[33.115113, "o", "\r\n"]
[33.166082, "o", "        return estimator\r\n"]
[33.217052, "o", "\r\n"]
[33.268021, "o", "    def make_scorers(self):\r\n"]
[33.31899, "o", "        make_dict_learning_scorers(self)\r\n"]
[33.369959, "o", "\r\n"]
[33.420928, "o", "\r\n"]
[33.471897, "o", "class MiniBatchDictionaryLearningBenchmark(Transformer, Estimator, Benchmark):\r\n"]
[33.522866, "o", "    \"\"\"\r\n"]
[33.573835, "o", "    Benchmarks for MiniBatchDictionaryLearning\r\n"]
[33.624804, "o", "    \"\"\"\r\n"]
[33.675773, "o", "\r\n"]
[33.726742, "o", "    param_names = [\"fit_algorithm\", \"n_jobs\"]\r\n"]
[33.777711, "o", "    params = ([\"lars\", \"cd\"], Benchmark.n_jobs_vals)\r\n"]
[33.82868, "o", "\r\n"]
[33.879649, "o", "    def setup_cache(self):\r\n"]
[33.930619, "o", "        super().setup_cache()\r\n"]
[33.981588, "o", "\r\n"]
[34.032557, "o", "    def make_data(self, params):\r\n"]
[34.083526, "o", "        return _olivetti_faces_dataset()\r\n"]
[34.134495, "o", "\r\n"]
[34.185464, "o", "    def make_estimator(self, params):\r\n"]
[34.236433, "o", "        fit_algorithm, n_jobs = params\r\n"]
[34.287402, "o", "\r\n"]
[34.338371, "o", "        estimator = MiniBatchDictionaryLearning(\r\n"]
[34.38934, "o", "            n_components=15,\r\n"]
[34.440309, "o", "            fit_algorithm=fit_algorithm,\r\n"]
[34.491278, "o", "            alpha=0.1,\r\n"]
[34.542247, "o", "            batch_size=3,\r\n"]
[34.593216, "o", "            random_state=0,\r\n"]
[34.644186, "o", "            n_jobs=n_jobs,\r\n"]
[34.695155, "o", "        )\r\n"]
[34.746124, "o", "\r\n"]
[34.797093, "o", "        return estimator\r\n"]
[34.848062, "o", "\r\n"]
[34.899031, "o", "    def make_scorers(self):\r\n"]
[35.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[35.002, "i", "ls -la sklearn/decomposition/\r"]
[35.004, "o", "ls -la sklearn/decomposition/\r\n"]
[35.212, "o", "\u001b[?2004l\r\n"]
[35.418, "o", "total 2776\r\n"]
[35.624, "o", "drwxr-xr-x 1 root root    4096 Sep 15 14:39 \u001b[0m\u001b[01;34m.\u001b[0m\r\n"]
[35.83, "o", "drwxr-xr-x 1 root root    4096 Sep 15 14:38 \u001b[01;34m..\u001b[0m\r\n"]
[36.036, "o", "-rw-r--r-- 1 root root    1296 Sep 13 19:09 __init__.py\r\n"]
[36.242, "o", "drwxr-xr-x 2 root root    4096 Sep 15 14:39 \u001b[01;34m__pycache__\u001b[0m\r\n"]
[36.448, "o", "-rw-r--r-- 1 root root    5729 Sep 13 19:09 _base.py\r\n"]
[36.654, "o", "-rw-r--r-- 1 root root  890875 Sep 15 14:31 _cdnmf_fast.c\r\n"]
[36.86, "o", "-rwxr-xr-x 1 root root  218800 Sep 15 14:34 \u001b[01;32m_cdnmf_fast.cpython-310-x86_64-linux-gnu.so\u001b[0m\r\n"]
[37.066, "o", "-rw-r--r-- 1 root root    1118 Sep 13 19:09 _cdnmf_fast.pyx\r\n"]
[37.272, "o", "-rw-r--r-- 1 root root   83338 Sep 13 19:09 _dict_learning.py\r\n"]
[37.478, "o", "-rw-r--r-- 1 root root   15301 Sep 13 19:09 _factor_analysis.py\r\n"]
[37.684, "o", "-rw-r--r-- 1 root root   26099 Sep 13 19:09 _fastica.py\r\n"]
[37.89, "o", "-rw-r--r-- 1 root root   15792 Sep 13 19:09 _incremental_pca.py\r\n"]
[38.096, "o", "-rw-r--r-- 1 root root   21794 Sep 13 19:09 _kernel_pca.py\r\n"]
[38.302, "o", "-rw-r--r-- 1 root root   33064 Sep 13 19:09 _lda.py\r\n"]
[38.508, "o", "-rw-r--r-- 1 root root   82499 Sep 13 19:09 _nmf.py\r\n"]
[38.714, "o", "-rw-r--r-- 1 root root 1069160 Sep 15 14:31 _online_lda_fast.c\r\n"]
[38.92, "o", "-rwxr-xr-x 1 root root  262144 Sep 15 14:34 \u001b[01;32m_online_lda_fast.cpython-310-x86_64-linux-gnu.so\u001b[0m\r\n"]
[39.126, "o", "-rw-r--r-- 1 root root    2855 Sep 13 19:09 _online_lda_fast.pyx\r\n"]
[39.332, "o", "-rw-r--r-- 1 root root   25848 Sep 13 19:09 _pca.py\r\n"]
[39.538, "o", "-rw-r--r-- 1 root root   18249 Sep 13 19:09 _sparse_pca.py\r\n"]
[39.744, "o", "-rw-r--r-- 1 root root   11489 Sep 13 19:09 _truncated_svd.py\r\n"]
[40.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[40.002, "i", "head -50 sklearn/decomposition/_dict_learning.py\r"]
[40.004, "o", "head -50 sklearn/decomposition/_dict_learning.py\r\n"]
[40.102941, "o", "\u001b[?2004l\r\n"]
[40.199882, "o", "\"\"\" Dictionary learning.\r\n"]
[40.296824, "o", "\"\"\"\r\n"]
[40.393765, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[40.490706, "o", "# License: BSD 3 clause\r\n"]
[40.587647, "o", "\r\n"]
[40.684588, "o", "import itertools\r\n"]
[40.781529, "o", "import sys\r\n"]
[40.878471, "o", "import time\r\n"]
[40.975412, "o", "import warnings\r\n"]
[41.072353, "o", "from math import ceil\r\n"]
[41.169294, "o", "from numbers import Integral, Real\r\n"]
[41.266235, "o", "\r\n"]
[41.363176, "o", "import numpy as np\r\n"]
[41.460118, "o", "from joblib import effective_n_jobs\r\n"]
[41.557059, "o", "from scipy import linalg\r\n"]
[41.654, "o", "\r\n"]
[41.750941, "o", "from ..base import (\r\n"]
[41.847882, "o", "    BaseEstimator,\r\n"]
[41.944824, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[42.041765, "o", "    TransformerMixin,\r\n"]
[42.138706, "o", "    _fit_context,\r\n"]
[42.235647, "o", ")\r\n"]
[42.332588, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[42.429529, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[42.526471, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[42.623412, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[42.720353, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[42.817294, "o", "from ..utils.validation import check_is_fitted\r\n"]
[42.914235, "o", "\r\n"]
[43.011176, "o", "\r\n"]
[43.108118, "o", "def _check_positive_coding(method, positive):\r\n"]
[43.205059, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[43.302, "o", "        raise ValueError(\r\n"]
[43.398941, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[43.495882, "o", "        )\r\n"]
[43.592824, "o", "\r\n"]
[43.689765, "o", "\r\n"]
[43.786706, "o", "def _sparse_encode_precomputed(\r\n"]
[43.883647, "o", "    X,\r\n"]
[43.980588, "o", "    dictionary,\r\n"]
[44.077529, "o", "    *,\r\n"]
[44.174471, "o", "    gram=None,\r\n"]
[44.271412, "o", "    cov=None,\r\n"]
[44.368353, "o", "    algorithm=\"lasso_lars\",\r\n"]
[44.465294, "o", "    regularization=None,\r\n"]
[44.562235, "o", "    copy_cov=True,\r\n"]
[44.659176, "o", "    init=None,\r\n"]
[44.756118, "o", "    max_iter=1000,\r\n"]
[44.853059, "o", "    verbose=0,\r\n"]
[45.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[45.002, "i", "grep -n \"class.*DictionaryLearning\" sklearn/decomposition/_dict_learning.py\r"]
[45.004, "o", "grep -n \"class.*DictionaryLearning\" sklearn/decomposition/_dict_learning.py\r\n"]
[46.242, "o", "\u001b[?2004l\r\n"]
[47.478, "o", "\u001b[32m\u001b[K1242\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \"\"\"Base \u001b[01;31m\u001b[Kclass from SparseCoder and DictionaryLearning\u001b[m\u001b[K algorithms.\"\"\"\r\n"]
[48.714, "o", "\u001b[32m\u001b[K1524\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kclass DictionaryLearning\u001b[m\u001b[K(_BaseSparseCoding, BaseEstimator):\r\n"]
[50.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[50.002, "i", "sed -n '1867,1920p' sklearn/decomposition/_dict_learning.py\r"]
[50.004, "o", "sed -n '1867,1920p' sklearn/decomposition/_dict_learning.py\r\n"]
[50.095891, "o", "\u001b[?2004l\r\n"]
[50.185782, "o", "class MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[50.275673, "o", "    \"\"\"Mini-batch dictionary learning.\r\n"]
[50.365564, "o", "\r\n"]
[50.455455, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[50.545345, "o", "    encoding the fitted data.\r\n"]
[50.635236, "o", "\r\n"]
[50.725127, "o", "    Solves the optimization problem::\r\n"]
[50.815018, "o", "\r\n"]
[50.904909, "o", "       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[50.9948, "o", "                    (U,V)\r\n"]
[51.084691, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[51.174582, "o", "\r\n"]
[51.264473, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[51.354364, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[51.444255, "o", "    of all the entries in the matrix.\r\n"]
[51.534145, "o", "\r\n"]
[51.624036, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[51.713927, "o", "\r\n"]
[51.803818, "o", "    Parameters\r\n"]
[51.893709, "o", "    ----------\r\n"]
[51.9836, "o", "    n_components : int, default=None\r\n"]
[52.073491, "o", "        Number of dictionary elements to extract.\r\n"]
[52.163382, "o", "\r\n"]
[52.253273, "o", "    alpha : float, default=1\r\n"]
[52.343164, "o", "        Sparsity controlling parameter.\r\n"]
[52.433055, "o", "\r\n"]
[52.522945, "o", "    n_iter : int, default=1000\r\n"]
[52.612836, "o", "        Total number of iterations over data batches to perform.\r\n"]
[52.702727, "o", "\r\n"]
[52.792618, "o", "        .. deprecated:: 1.1\r\n"]
[52.882509, "o", "           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[52.9724, "o", "           ``max_iter`` instead.\r\n"]
[53.062291, "o", "\r\n"]
[53.152182, "o", "    max_iter : int, default=None\r\n"]
[53.242073, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[53.331964, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[53.421855, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[53.511745, "o", "\r\n"]
[53.601636, "o", "        .. versionadded:: 1.1\r\n"]
[53.691527, "o", "\r\n"]
[53.781418, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[53.871309, "o", "        The algorithm used:\r\n"]
[53.9612, "o", "\r\n"]
[54.051091, "o", "        - `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[54.140982, "o", "          problem (`linear_model.lars_path`)\r\n"]
[54.230873, "o", "        - `'cd'`: uses the coordinate descent method to compute the\r\n"]
[54.320764, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[54.410655, "o", "          the estimated components are sparse.\r\n"]
[54.500545, "o", "\r\n"]
[54.590436, "o", "    n_jobs : int, default=None\r\n"]
[54.680327, "o", "        Number of parallel jobs to run.\r\n"]
[54.770218, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[54.860109, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[55.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[55.002, "i", "sed -n '1920,1980p' sklearn/decomposition/_dict_learning.py\r"]
[55.004, "o", "sed -n '1920,1980p' sklearn/decomposition/_dict_learning.py\r\n"]
[55.085742, "o", "\u001b[?2004l\r\n"]
[55.165484, "o", "        for more details.\r\n"]
[55.245226, "o", "\r\n"]
[55.324968, "o", "    batch_size : int, default=256\r\n"]
[55.40471, "o", "        Number of samples in each mini-batch.\r\n"]
[55.484452, "o", "\r\n"]
[55.564194, "o", "        .. versionchanged:: 1.3\r\n"]
[55.643935, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[55.723677, "o", "\r\n"]
[55.803419, "o", "    shuffle : bool, default=True\r\n"]
[55.883161, "o", "        Whether to shuffle the samples before forming batches.\r\n"]
[55.962903, "o", "\r\n"]
[56.042645, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[56.122387, "o", "        Initial value of the dictionary for warm restart scenarios.\r\n"]
[56.202129, "o", "\r\n"]
[56.281871, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[56.361613, "o", "            'threshold'}, default='omp'\r\n"]
[56.441355, "o", "        Algorithm used to transform the data:\r\n"]
[56.521097, "o", "\r\n"]
[56.600839, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[56.680581, "o", "          (`linear_model.lars_path`);\r\n"]
[56.760323, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[56.840065, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[56.919806, "o", "          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n"]
[56.999548, "o", "          if the estimated components are sparse.\r\n"]
[57.07929, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[57.159032, "o", "          solution.\r\n"]
[57.238774, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[57.318516, "o", "          the projection ``dictionary * X'``.\r\n"]
[57.398258, "o", "\r\n"]
[57.478, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[57.557742, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[57.637484, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[57.717226, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[57.796968, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[57.87671, "o", "\r\n"]
[57.956452, "o", "    transform_alpha : float, default=None\r\n"]
[58.036194, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[58.115935, "o", "        penalty applied to the L1 norm.\r\n"]
[58.195677, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[58.275419, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[58.355161, "o", "        If `None`, defaults to `alpha`.\r\n"]
[58.434903, "o", "\r\n"]
[58.514645, "o", "        .. versionchanged:: 1.2\r\n"]
[58.594387, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[58.674129, "o", "\r\n"]
[58.753871, "o", "    verbose : bool or int, default=False\r\n"]
[58.833613, "o", "        To control the verbosity of the procedure.\r\n"]
[58.913355, "o", "\r\n"]
[58.993097, "o", "    split_sign : bool, default=False\r\n"]
[59.072839, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[59.152581, "o", "        its negative part and its positive part. This can improve the\r\n"]
[59.232323, "o", "        performance of downstream classifiers.\r\n"]
[59.312065, "o", "\r\n"]
[59.391806, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[59.471548, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[59.55129, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[59.631032, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[59.710774, "o", "        results across multiple function calls.\r\n"]
[59.790516, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[59.870258, "o", "\r\n"]
[60.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[60.002, "i", "grep -A 30 \"def __init__\" sklearn/decomposition/_dict_learning.py | grep -A 30 \"MiniBatch\" -B 5\r"]
[60.004, "o", "grep -A 30 \"def __init__\" sklearn/decomposition/_dict_learning.py | grep -A 30 \"MiniBatch\" -B 5\r\n"]
[60.006, "o", "\u001b[?2004l\r\n"]
[65.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[65.002, "i", "sed -n '1990,2050p' sklearn/decomposition/_dict_learning.py\r"]
[65.004, "o", "sed -n '1990,2050p' sklearn/decomposition/_dict_learning.py\r\n"]
[65.085742, "o", "\u001b[?2004l\r\n"]
[65.165484, "o", "    transform_max_iter : int, default=1000\r\n"]
[65.245226, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[65.324968, "o", "        `'lasso_lars'`.\r\n"]
[65.40471, "o", "\r\n"]
[65.484452, "o", "        .. versionadded:: 0.22\r\n"]
[65.564194, "o", "\r\n"]
[65.643935, "o", "    callback : callable, default=None\r\n"]
[65.723677, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[65.803419, "o", "\r\n"]
[65.883161, "o", "        .. versionadded:: 1.1\r\n"]
[65.962903, "o", "\r\n"]
[66.042645, "o", "    tol : float, default=1e-3\r\n"]
[66.122387, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[66.202129, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[66.281871, "o", "\r\n"]
[66.361613, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[66.441355, "o", "        `tol` to 0.0.\r\n"]
[66.521097, "o", "\r\n"]
[66.600839, "o", "        .. versionadded:: 1.1\r\n"]
[66.680581, "o", "\r\n"]
[66.760323, "o", "    max_no_improvement : int, default=10\r\n"]
[66.840065, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[66.919806, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[66.999548, "o", "        `max_iter` is not None.\r\n"]
[67.07929, "o", "\r\n"]
[67.159032, "o", "        To disable convergence detection based on cost function, set\r\n"]
[67.238774, "o", "        `max_no_improvement` to None.\r\n"]
[67.318516, "o", "\r\n"]
[67.398258, "o", "        .. versionadded:: 1.1\r\n"]
[67.478, "o", "\r\n"]
[67.557742, "o", "    Attributes\r\n"]
[67.637484, "o", "    ----------\r\n"]
[67.717226, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[67.796968, "o", "        Components extracted from the data.\r\n"]
[67.87671, "o", "\r\n"]
[67.956452, "o", "    n_features_in_ : int\r\n"]
[68.036194, "o", "        Number of features seen during :term:`fit`.\r\n"]
[68.115935, "o", "\r\n"]
[68.195677, "o", "        .. versionadded:: 0.24\r\n"]
[68.275419, "o", "\r\n"]
[68.355161, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[68.434903, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[68.514645, "o", "        has feature names that are all strings.\r\n"]
[68.594387, "o", "\r\n"]
[68.674129, "o", "        .. versionadded:: 1.0\r\n"]
[68.753871, "o", "\r\n"]
[68.833613, "o", "    n_iter_ : int\r\n"]
[68.913355, "o", "        Number of iterations over the full dataset.\r\n"]
[68.993097, "o", "\r\n"]
[69.072839, "o", "    n_steps_ : int\r\n"]
[69.152581, "o", "        Number of mini-batches processed.\r\n"]
[69.232323, "o", "\r\n"]
[69.312065, "o", "        .. versionadded:: 1.1\r\n"]
[69.391806, "o", "\r\n"]
[69.471548, "o", "    See Also\r\n"]
[69.55129, "o", "    --------\r\n"]
[69.631032, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[69.710774, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[69.790516, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[69.870258, "o", "        precomputed dictionary.\r\n"]
[70.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[70.002, "i", "awk '/class MiniBatchDictionaryLearning/,/def __init__/' sklearn/decomposition/_dict_learning.py | tail -20\r"]
[70.004, "o", "awk '/class MiniBatchDictionaryLearning/,/def __init__/' sklearn/decomposition/_dict_learning.py | tail -20\r\n"]
[70.241429, "o", "\u001b[?2004l\r\n"]
[70.476857, "o", "        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n"]
[70.712286, "o", "        \"shuffle\": [\"boolean\"],\r\n"]
[70.947714, "o", "        \"dict_init\": [None, np.ndarray],\r\n"]
[71.183143, "o", "        \"transform_algorithm\": [\r\n"]
[71.418571, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[71.654, "o", "        ],\r\n"]
[71.889429, "o", "        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[72.124857, "o", "        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[72.360286, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[72.595714, "o", "        \"split_sign\": [\"boolean\"],\r\n"]
[72.831143, "o", "        \"random_state\": [\"random_state\"],\r\n"]
[73.066571, "o", "        \"positive_code\": [\"boolean\"],\r\n"]
[73.302, "o", "        \"positive_dict\": [\"boolean\"],\r\n"]
[73.537429, "o", "        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[73.772857, "o", "        \"callback\": [None, callable],\r\n"]
[74.008286, "o", "        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[74.243714, "o", "        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n"]
[74.479143, "o", "    }\r\n"]
[74.714571, "o", "\r\n"]
[75.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[75.002, "i", "sed -n '2090,2130p' sklearn/decomposition/_dict_learning.py\r"]
[75.004, "o", "sed -n '2090,2130p' sklearn/decomposition/_dict_learning.py\r\n"]
[75.123714, "o", "\u001b[?2004l\r\n"]
[75.241429, "o", "            Hidden(StrOptions({\"deprecated\"})),\r\n"]
[75.359143, "o", "        ],\r\n"]
[75.476857, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n"]
[75.594571, "o", "        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n"]
[75.712286, "o", "        \"n_jobs\": [None, Integral],\r\n"]
[75.83, "o", "        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n"]
[75.947714, "o", "        \"shuffle\": [\"boolean\"],\r\n"]
[76.065429, "o", "        \"dict_init\": [None, np.ndarray],\r\n"]
[76.183143, "o", "        \"transform_algorithm\": [\r\n"]
[76.300857, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[76.418571, "o", "        ],\r\n"]
[76.536286, "o", "        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[76.654, "o", "        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[76.771714, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[76.889429, "o", "        \"split_sign\": [\"boolean\"],\r\n"]
[77.007143, "o", "        \"random_state\": [\"random_state\"],\r\n"]
[77.124857, "o", "        \"positive_code\": [\"boolean\"],\r\n"]
[77.242571, "o", "        \"positive_dict\": [\"boolean\"],\r\n"]
[77.360286, "o", "        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[77.478, "o", "        \"callback\": [None, callable],\r\n"]
[77.595714, "o", "        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[77.713429, "o", "        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n"]
[77.831143, "o", "    }\r\n"]
[77.948857, "o", "\r\n"]
[78.066571, "o", "    def __init__(\r\n"]
[78.184286, "o", "        self,\r\n"]
[78.302, "o", "        n_components=None,\r\n"]
[78.419714, "o", "        *,\r\n"]
[78.537429, "o", "        alpha=1,\r\n"]
[78.655143, "o", "        n_iter=\"deprecated\",\r\n"]
[78.772857, "o", "        max_iter=None,\r\n"]
[78.890571, "o", "        fit_algorithm=\"lars\",\r\n"]
[79.008286, "o", "        n_jobs=None,\r\n"]
[79.126, "o", "        batch_size=256,\r\n"]
[79.243714, "o", "        shuffle=True,\r\n"]
[79.361429, "o", "        dict_init=None,\r\n"]
[79.479143, "o", "        transform_algorithm=\"omp\",\r\n"]
[79.596857, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[79.714571, "o", "        transform_alpha=None,\r\n"]
[79.832286, "o", "        verbose=False,\r\n"]
[80.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[80.002, "i", "grep -n \"class.*NMF\" sklearn/decomposition/_nmf.py\r"]
[80.004, "o", "grep -n \"class.*NMF\" sklearn/decomposition/_nmf.py\r\n"]
[80.9948, "o", "\u001b[?2004l\r\n"]
[81.9836, "o", "\u001b[32m\u001b[K1138\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kclass _BaseNMF\u001b[m\u001b[K(ClassNamePrefixFeaturesOutMixin, TransformerMixin, BaseEstimator, ABC):\r\n"]
[82.9724, "o", "\u001b[32m\u001b[K1139\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \"\"\"Base \u001b[01;31m\u001b[Kclass for NMF and MiniBatchNMF\u001b[m\u001b[K.\"\"\"\r\n"]
[83.9612, "o", "\u001b[32m\u001b[K1364\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kclass NMF(_BaseNMF\u001b[m\u001b[K):\r\n"]
[85.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[85.002, "i", "sed -n '1364,1420p' sklearn/decomposition/_nmf.py | grep -A 20 \"def __init__\"\r"]
[85.004, "o", "sed -n '1364,1420p' sklearn/decomposition/_nmf.py | grep -A 20 \"def __init__\"\r\n"]
[85.006, "o", "\u001b[?2004l\r\n"]
[90.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[90.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r"]
[90.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r\n"]
[92.478, "o", "\u001b[?2004l\r\n"]
[95.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[95.002, "i", "cd asv_benchmarks && asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r"]
[95.004, "o", "cd asv_benchmarks && asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r\n"]
[95.082062, "o", "\u001b[?2004l\r\n"]
[95.158123, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[95.234185, "o", "\u00b7 \u001b[0;32mRunning 6 total benchmarks (1 commits * 1 environments * 6 benchmarks)\u001b[0m\r\n"]
[95.310246, "o", "[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n"]
[95.386308, "o", "[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n"]
[95.462369, "o", "[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.peakmem_fit\u001b[0m                                                                                  ok\r\n"]
[95.538431, "o", "[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =======\u001b[0m\r\n"]
[95.614492, "o", "             --               n_jobs\r\n"]
[95.690554, "o", "             --------------- -------\r\n"]
[95.766615, "o", "              fit_algorithm     1   \r\n"]
[95.842677, "o", "             =============== =======\r\n"]
[95.918738, "o", "                   lars       93.9M \r\n"]
[95.9948, "o", "                    cd        93.2M \r\n"]
[96.070862, "o", "             =============== =======\r\n"]
[96.146923, "o", "\r\n"]
[96.222985, "o", "[33.33%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.peakmem_transform\u001b[0m                                                                            ok\r\n"]
[96.299046, "o", "[33.33%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =======\u001b[0m\r\n"]
[96.375108, "o", "             --               n_jobs\r\n"]
[96.451169, "o", "             --------------- -------\r\n"]
[96.527231, "o", "              fit_algorithm     1   \r\n"]
[96.603292, "o", "             =============== =======\r\n"]
[96.679354, "o", "                   lars       84.7M \r\n"]
[96.755415, "o", "                    cd        84.3M \r\n"]
[96.831477, "o", "             =============== =======\r\n"]
[96.907538, "o", "\r\n"]
[96.9836, "o", "[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0m                                                                                     ok\r\n"]
[97.059662, "o", "[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ============\u001b[0m\r\n"]
[97.135723, "o", "             --                 n_jobs   \r\n"]
[97.211785, "o", "             --------------- ------------\r\n"]
[97.287846, "o", "              fit_algorithm       1      \r\n"]
[97.363908, "o", "             =============== ============\r\n"]
[97.439969, "o", "                   lars       6.80\u00b10.04s \r\n"]
[97.516031, "o", "                    cd        1.55\u00b10.01s \r\n"]
[97.592092, "o", "             =============== ============\r\n"]
[97.668154, "o", "\r\n"]
[97.744215, "o", "[66.67%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_transform\u001b[0m                                                                               ok\r\n"]
[97.820277, "o", "[66.67%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ===========\u001b[0m\r\n"]
[97.896338, "o", "             --                 n_jobs  \r\n"]
[97.9724, "o", "             --------------- -----------\r\n"]
[98.048462, "o", "              fit_algorithm       1     \r\n"]
[98.124523, "o", "             =============== ===========\r\n"]
[98.200585, "o", "                   lars        160\u00b12ms  \r\n"]
[98.276646, "o", "                    cd        156\u00b10.5ms \r\n"]
[98.352708, "o", "             =============== ===========\r\n"]
[98.428769, "o", "\r\n"]
[98.504831, "o", "[83.33%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.track_test_score\u001b[0m                                                                             ok\r\n"]
[98.580892, "o", "[83.33%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ======================\u001b[0m\r\n"]
[98.656954, "o", "             --                      n_jobs        \r\n"]
[98.733015, "o", "             --------------- ----------------------\r\n"]
[98.809077, "o", "              fit_algorithm            1           \r\n"]
[98.885138, "o", "             =============== ======================\r\n"]
[98.9612, "o", "                   lars       -0.07507291436195374 \r\n"]
[99.037262, "o", "                    cd        -0.07509346306324005 \r\n"]
[99.113323, "o", "             =============== ======================\r\n"]
[99.189385, "o", "\r\n"]
[99.265446, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.track_train_score\u001b[0m                                                                            ok\r\n"]
[99.341508, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ======================\u001b[0m\r\n"]
[99.417569, "o", "              --                      n_jobs        \r\n"]
[99.493631, "o", "              --------------- ----------------------\r\n"]
[99.569692, "o", "               fit_algorithm            1           \r\n"]
[99.645754, "o", "              =============== ======================\r\n"]
[99.721815, "o", "                    lars       -0.07244297116994858 \r\n"]
[99.797877, "o", "                     cd        -0.07244537025690079 \r\n"]
[99.873938, "o", "              =============== ======================\r\n"]
[100.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo/asv_benchmarks# "]
[100.002, "i", "asv profile --python=same --config=asv.conf.json decomposition.MiniBatchDictionaryLearningBenchmark.\r"]
[100.004, "o", "asv profile --python=same --config=asv.conf.json decomposition.MiniBatchDictionaryLearningBenchmark.\r\n"]
[101.242, "o", ".time_fit\r\n"]
[102.478, "o", "\u001b[?2004l\r\n"]
[103.714, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[105.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo/asv_benchmarks# "]
[105.002, "i", "asv list | grep MiniBatch\r"]
[105.004, "o", "asv list | grep MiniBatch\r\n"]
[105.9948, "o", "\u001b[?2004l\r\n"]
[106.9836, "o", "usage: asv [-h] [--verbose] [--config CONFIG] [--version]\r\n"]
[107.9724, "o", "           {help,quickstart,machine,setup,run,continuous,find,rm,publish,preview,profile,update,show,compare,check,gh-pages}\r\n"]
[108.9612, "o", "           ...\r\n"]
[110.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo/asv_benchmarks# "]
[110.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit[lars-1]\r"]
[110.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit[lars-1]\r\n"]
[110.1605, "o", "\u001b[?2004l\r\n"]
[110.315, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[110.4695, "o", "Traceback (most recent call last):\r\n"]
[110.624, "o", "  File \"/opt/conda/envs/asv_3.10/bin/asv\", line 7, in <module>\r\n"]
[110.7785, "o", "    sys.exit(main())\r\n"]
[110.933, "o", "  File \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/asv/main.py\", line 28, in main\r\n"]
[111.0875, "o", "    result = args.func(args)\r\n"]
[111.242, "o", "  File \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/asv/commands/__init__.py\", line 48, in run_from_args\r\n"]
[111.3965, "o", "    return cls.run_from_conf_args(conf, args)\r\n"]
[111.551, "o", "  File \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/asv/commands/profiling.py\", line 84, in run_from_conf_args\r\n"]
[111.7055, "o", "    return cls.run(\r\n"]
[111.86, "o", "  File \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/asv/commands/profiling.py\", line 177, in run\r\n"]
[112.0145, "o", "    benchmarks = Benchmarks.discover(conf, repo, environments,\r\n"]
[112.169, "o", "  File \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/asv/benchmarks.py\", line 132, in discover\r\n"]
[112.3235, "o", "    return cls(conf, benchmarks, regex=regex)\r\n"]
[112.478, "o", "  File \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/asv/benchmarks.py\", line 59, in __init__\r\n"]
[112.6325, "o", "    if not regex or any(re.search(reg, name) for reg in regex):\r\n"]
[112.787, "o", "  File \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/asv/benchmarks.py\", line 59, in <genexpr>\r\n"]
[112.9415, "o", "    if not regex or any(re.search(reg, name) for reg in regex):\r\n"]
[113.096, "o", "  File \"/opt/conda/envs/asv_3.10/lib/python3.10/re.py\", line 200, in search\r\n"]
[113.2505, "o", "    return _compile(pattern, flags).search(string)\r\n"]
[113.405, "o", "  File \"/opt/conda/envs/asv_3.10/lib/python3.10/re.py\", line 303, in _compile\r\n"]
[113.5595, "o", "    p = sre_compile.compile(pattern, flags)\r\n"]
[113.714, "o", "  File \"/opt/conda/envs/asv_3.10/lib/python3.10/sre_compile.py\", line 788, in compile\r\n"]
[113.8685, "o", "    p = sre_parse.parse(p, flags)\r\n"]
[114.023, "o", "  File \"/opt/conda/envs/asv_3.10/lib/python3.10/sre_parse.py\", line 955, in parse\r\n"]
[114.1775, "o", "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\r\n"]
[114.332, "o", "  File \"/opt/conda/envs/asv_3.10/lib/python3.10/sre_parse.py\", line 444, in _parse_sub\r\n"]
[114.4865, "o", "    itemsappend(_parse(source, state, verbose, nested + 1,\r\n"]
[114.641, "o", "  File \"/opt/conda/envs/asv_3.10/lib/python3.10/sre_parse.py\", line 599, in _parse\r\n"]
[114.7955, "o", "    raise source.error(msg, len(this) + 1 + len(that))\r\n"]
[115.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo/asv_benchmarks# "]
[115.002, "i", "cd ..\r"]
[115.004, "o", "cd ..\r\n"]
[115.006, "o", "\u001b[?2004l\r\n"]
[120.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[120.002, "i", "grep -n \"n_components=None\" sklearn/decomposition/_dict_learning.py\r"]
[120.004, "o", "grep -n \"n_components=None\" sklearn/decomposition/_dict_learning.py\r\n"]
[121.242, "o", "\u001b[?2004l\r\n"]
[122.478, "o", "\u001b[32m\u001b[K570\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    if n_components <= r:  # True even if \u001b[01;31m\u001b[Kn_components=None\u001b[m\u001b[K\r\n"]
[123.714, "o", "\u001b[32m\u001b[K1742\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kn_components=None\u001b[m\u001b[K,\r\n"]
[125.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[125.002, "i", "sed -i 's/n_components=None/n_components=\"warn\"/g' sklearn/decomposition/_dict_learning.py\r"]
[125.004, "o", "sed -i 's/n_components=None/n_components=\"warn\"/g' sklearn/decomposition/_dict_learning.py\r\n"]
[125.006, "o", "\u001b[?2004l\r\n"]
[130.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[130.002, "i", "grep -n \"n_components=\\\"warn\\\"\" sklearn/decomposition/_dict_learning.py\r"]
[130.004, "o", "grep -n \"n_components=\\\"warn\\\"\" sklearn/decomposition/_dict_learning.py\r\n"]
[131.242, "o", "\u001b[?2004l\r\n"]
[132.478, "o", "\u001b[32m\u001b[K570\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    if n_components <= r:  # True even if \u001b[01;31m\u001b[Kn_components=\"warn\"\u001b[m\u001b[K\r\n"]
[133.714, "o", "\u001b[32m\u001b[K1742\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kn_components=\"warn\"\u001b[m\u001b[K,\r\n"]
[135.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[135.002, "i", "grep -B5 -A5 \"n_components.*:\" sklearn/decomposition/_dict_learning.py | head -20\r"]
[135.004, "o", "grep -B5 -A5 \"n_components.*:\" sklearn/decomposition/_dict_learning.py | head -20\r\n"]
[135.241429, "o", "\u001b[?2004l\r\n"]
[135.476857, "o", "    if B is None:\r\n"]
[135.712286, "o", "        B = Y.T @ code\r\n"]
[135.947714, "o", "\r\n"]
[136.183143, "o", "    n_unused = 0\r\n"]
[136.418571, "o", "\r\n"]
[136.654, "o", "    for k in range(n_components):\r\n"]
[136.889429, "o", "        if A[k, k] > 1e-6:\r\n"]
[137.124857, "o", "            # 1e-6 is arbitrary but consistent with the spams implementation\r\n"]
[137.360286, "o", "            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n"]
[137.595714, "o", "        else:\r\n"]
[137.831143, "o", "            # kth atom is almost never used -> sample a new one from the data\r\n"]
[138.066571, "o", "--\r\n"]
[138.302, "o", "        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n"]
[138.537429, "o", "        # flip the initial code's sign to enforce deterministic output\r\n"]
[138.772857, "o", "        code, dictionary = svd_flip(code, dictionary)\r\n"]
[139.008286, "o", "        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[139.243714, "o", "    r = len(dictionary)\r\n"]
[139.479143, "o", "    if n_components <= r:  # True even if n_components=\"warn\"\r\n"]
[139.714571, "o", "        code = code[:, :n_components]\r\n"]
[140.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[140.002, "i", "grep -B10 -A10 \"_parameter_constraints\" sklearn/decomposition/_dict_learning.py | grep -A15 \"n_components\"\r"]
[140.004, "o", "grep -B10 -A10 \"_parameter_constraints\" sklearn/decomposition/_dict_learning.py | grep -A15 \"n_components\"\r\n"]
[140.182571, "o", "\u001b[?2004l\r\n"]
[140.359143, "o", "        \"\u001b[01;31m\u001b[Kn_components\u001b[m\u001b[K\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[140.535714, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[140.712286, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[140.888857, "o", "        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[141.065429, "o", "        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[141.242, "o", "        \"transform_algorithm\": [\r\n"]
[141.418571, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[141.595143, "o", "        ],\r\n"]
[141.771714, "o", "        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[141.948286, "o", "        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[142.124857, "o", "--\r\n"]
[142.301429, "o", "\r\n"]
[142.478, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[142.654571, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[142.831143, "o", "    the original signal:\r\n"]
[143.007714, "o", "\r\n"]
[143.184286, "o", "\u001b[36m\u001b[K--\u001b[m\u001b[K\r\n"]
[143.360857, "o", "        \"\u001b[01;31m\u001b[Kn_components\u001b[m\u001b[K\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[143.537429, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[143.714, "o", "        \"n_iter\": [\r\n"]
[143.890571, "o", "            Interval(Integral, 0, None, closed=\"left\"),\r\n"]
[144.067143, "o", "            Hidden(StrOptions({\"deprecated\"})),\r\n"]
[144.243714, "o", "        ],\r\n"]
[144.420286, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n"]
[144.596857, "o", "        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n"]
[144.773429, "o", "        \"n_jobs\": [None, Integral],\r\n"]
[145.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[145.002, "i", "sed -i 's/\"n_components\": \\[Interval(Integral, 1, None, closed=\"left\"), None\\]/\"n_components\": [Interval(Integral, \r"]
[145.004, "o", "sed -i 's/\"n_components\": \\[Interval(Integral, 1, None, closed=\"left\"), None\\]/\"n_components\": [Interval(Integral, \r\n"]
[147.478, "o", " 1, None, closed=\"left\"), None, StrOptions({\"warn\", \"auto\"})]/g' sklearn/decomposition/_dict_learning.py\r\n"]
[150.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[150.002, "i", "grep -A3 \"n_components.*StrOptions\" sklearn/decomposition/_dict_learning.py\r"]
[150.004, "o", "grep -A3 \"n_components.*StrOptions\" sklearn/decomposition/_dict_learning.py\r\n"]
[150.5004, "o", "\u001b[?2004l\r\n"]
[150.9948, "o", "        \"\u001b[01;31m\u001b[Kn_components\": [Interval(Integral, 1, None, closed=\"left\"), None, StrOptions\u001b[m\u001b[K({\"warn\", \"auto\"})],\r\n"]
[151.4892, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[151.9836, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[152.478, "o", "        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[152.9724, "o", "\u001b[36m\u001b[K--\u001b[m\u001b[K\r\n"]
[153.4668, "o", "        \"\u001b[01;31m\u001b[Kn_components\": [Interval(Integral, 1, None, closed=\"left\"), None, StrOptions\u001b[m\u001b[K({\"warn\", \"auto\"})],\r\n"]
[153.9612, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[154.4556, "o", "        \"n_iter\": [\r\n"]
[155.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[155.002, "i", "grep -n \"def fit\" sklearn/decomposition/_dict_learning.py\r"]
[155.004, "o", "grep -n \"def fit\" sklearn/decomposition/_dict_learning.py\r\n"]
[155.9948, "o", "\u001b[?2004l\r\n"]
[156.9836, "o", "\u001b[32m\u001b[K1459\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdef fit\u001b[m\u001b[K(self, X, y=None):\r\n"]
[157.9724, "o", "\u001b[32m\u001b[K1783\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdef fit\u001b[m\u001b[K(self, X, y=None):\r\n"]
[158.9612, "o", "\u001b[32m\u001b[K1804\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdef fit\u001b[m\u001b[K_transform(self, X, y=None):\r\n"]
[160.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[160.002, "i", "sed -n '2326,2380p' sklearn/decomposition/_dict_learning.py\r"]
[160.004, "o", "sed -n '2326,2380p' sklearn/decomposition/_dict_learning.py\r\n"]
[160.094286, "o", "\u001b[?2004l\r\n"]
[160.182571, "o", "    def fit(self, X, y=None):\r\n"]
[160.270857, "o", "        \"\"\"Fit the model from data in X.\r\n"]
[160.359143, "o", "\r\n"]
[160.447429, "o", "        Parameters\r\n"]
[160.535714, "o", "        ----------\r\n"]
[160.624, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[160.712286, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[160.800571, "o", "            and `n_features` is the number of features.\r\n"]
[160.888857, "o", "\r\n"]
[160.977143, "o", "        y : Ignored\r\n"]
[161.065429, "o", "            Not used, present for API consistency by convention.\r\n"]
[161.153714, "o", "\r\n"]
[161.242, "o", "        Returns\r\n"]
[161.330286, "o", "        -------\r\n"]
[161.418571, "o", "        self : object\r\n"]
[161.506857, "o", "            Returns the instance itself.\r\n"]
[161.595143, "o", "        \"\"\"\r\n"]
[161.683429, "o", "        X = self._validate_data(\r\n"]
[161.771714, "o", "            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n"]
[161.86, "o", "        )\r\n"]
[161.948286, "o", "\r\n"]
[162.036571, "o", "        self._check_params(X)\r\n"]
[162.124857, "o", "\r\n"]
[162.213143, "o", "        if self.n_iter != \"deprecated\":\r\n"]
[162.301429, "o", "            warnings.warn(\r\n"]
[162.389714, "o", "                (\r\n"]
[162.478, "o", "                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n"]
[162.566286, "o", "                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n"]
[162.654571, "o", "                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n"]
[162.742857, "o", "                    \"specified.\"\r\n"]
[162.831143, "o", "                ),\r\n"]
[162.919429, "o", "                FutureWarning,\r\n"]
[163.007714, "o", "            )\r\n"]
[163.096, "o", "            n_iter = self.n_iter\r\n"]
[163.184286, "o", "\r\n"]
[163.272571, "o", "        self._random_state = check_random_state(self.random_state)\r\n"]
[163.360857, "o", "\r\n"]
[163.449143, "o", "        dictionary = self._initialize_dict(X, self._random_state)\r\n"]
[163.537429, "o", "        old_dict = dictionary.copy()\r\n"]
[163.625714, "o", "\r\n"]
[163.714, "o", "        if self.shuffle:\r\n"]
[163.802286, "o", "            X_train = X.copy()\r\n"]
[163.890571, "o", "            self._random_state.shuffle(X_train)\r\n"]
[163.978857, "o", "        else:\r\n"]
[164.067143, "o", "            X_train = X\r\n"]
[164.155429, "o", "\r\n"]
[164.243714, "o", "        n_samples, n_features = X_train.shape\r\n"]
[164.332, "o", "\r\n"]
[164.420286, "o", "        if self.verbose:\r\n"]
[164.508571, "o", "            print(\"[dict_learning]\")\r\n"]
[164.596857, "o", "\r\n"]
[164.685143, "o", "        # Inner stats\r\n"]
[164.773429, "o", "        self._A = np.zeros(\r\n"]
[164.861714, "o", "            (self._n_components, self._n_components), dtype=X_train.dtype\r\n"]
[165.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[165.002, "i", "grep -n \"def _check_params\" sklearn/decomposition/_dict_learning.py\r"]
[165.004, "o", "grep -n \"def _check_params\" sklearn/decomposition/_dict_learning.py\r\n"]
[167.478, "o", "\u001b[?2004l\r\n"]
[170.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[170.002, "i", "sed -n '2164,2220p' sklearn/decomposition/_dict_learning.py\r"]
[170.004, "o", "sed -n '2164,2220p' sklearn/decomposition/_dict_learning.py\r\n"]
[170.091241, "o", "\u001b[?2004l\r\n"]
[170.176483, "o", "    def _check_params(self, X):\r\n"]
[170.261724, "o", "        # n_components\r\n"]
[170.346966, "o", "        self._n_components = self.n_components\r\n"]
[170.432207, "o", "        if self._n_components is None:\r\n"]
[170.517448, "o", "            self._n_components = X.shape[1]\r\n"]
[170.60269, "o", "\r\n"]
[170.687931, "o", "        # fit_algorithm\r\n"]
[170.773172, "o", "        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n"]
[170.858414, "o", "        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n"]
[170.943655, "o", "\r\n"]
[171.028897, "o", "        # batch_size\r\n"]
[171.114138, "o", "        self._batch_size = min(self.batch_size, X.shape[0])\r\n"]
[171.199379, "o", "\r\n"]
[171.284621, "o", "    def _initialize_dict(self, X, random_state):\r\n"]
[171.369862, "o", "        \"\"\"Initialization of the dictionary.\"\"\"\r\n"]
[171.455103, "o", "        if self.dict_init is not None:\r\n"]
[171.540345, "o", "            dictionary = self.dict_init\r\n"]
[171.625586, "o", "        else:\r\n"]
[171.710828, "o", "            # Init V with SVD of X\r\n"]
[171.796069, "o", "            _, S, dictionary = randomized_svd(\r\n"]
[171.88131, "o", "                X, self._n_components, random_state=random_state\r\n"]
[171.966552, "o", "            )\r\n"]
[172.051793, "o", "            dictionary = S[:, np.newaxis] * dictionary\r\n"]
[172.137034, "o", "\r\n"]
[172.222276, "o", "        if self._n_components <= len(dictionary):\r\n"]
[172.307517, "o", "            dictionary = dictionary[: self._n_components, :]\r\n"]
[172.392759, "o", "        else:\r\n"]
[172.478, "o", "            dictionary = np.concatenate(\r\n"]
[172.563241, "o", "                (\r\n"]
[172.648483, "o", "                    dictionary,\r\n"]
[172.733724, "o", "                    np.zeros(\r\n"]
[172.818966, "o", "                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n"]
[172.904207, "o", "                        dtype=dictionary.dtype,\r\n"]
[172.989448, "o", "                    ),\r\n"]
[173.07469, "o", "                )\r\n"]
[173.159931, "o", "            )\r\n"]
[173.245172, "o", "\r\n"]
[173.330414, "o", "        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n"]
[173.415655, "o", "        dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[173.500897, "o", "\r\n"]
[173.586138, "o", "        return dictionary\r\n"]
[173.671379, "o", "\r\n"]
[173.756621, "o", "    def _update_inner_stats(self, X, code, batch_size, step):\r\n"]
[173.841862, "o", "        \"\"\"Update the inner stats inplace.\"\"\"\r\n"]
[173.927103, "o", "        if step < batch_size - 1:\r\n"]
[174.012345, "o", "            theta = (step + 1) * batch_size\r\n"]
[174.097586, "o", "        else:\r\n"]
[174.182828, "o", "            theta = batch_size**2 + step + 1 - batch_size\r\n"]
[174.268069, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[174.35331, "o", "\r\n"]
[174.438552, "o", "        self._A *= beta\r\n"]
[174.523793, "o", "        self._A += code.T @ code / batch_size\r\n"]
[174.609034, "o", "        self._B *= beta\r\n"]
[174.694276, "o", "        self._B += X.T @ code / batch_size\r\n"]
[174.779517, "o", "\r\n"]
[174.864759, "o", "    def _minibatch_step(self, X, dictionary, random_state, step):\r\n"]
[175.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[175.002, "i", "sed -i '2166,2168c\\        # n_components\\n        if self.n_components == \"warn\":\\n            warnings.warn(\\n   \r"]
[175.004, "o", "sed -i '2166,2168c\\        # n_components\\n        if self.n_components == \"warn\":\\n            warnings.warn(\\n   \r\n"]
[175.5004, "o", "              \"The default value of n_components will change from None to \\'auto\\' in 1.4. \"\\n                \"Set the value of n_components to suppress this war\r\n"]
[175.9948, "o", "rning.\",\\n                FutureWarning,\\n            )\\n            self._n_components = X.shape[1]\\n        elif self.n_components == \"auto\" or self.n_componen\r\n"]
[176.4892, "o", "nts is None:\\n            self._n_components = X.shape[1]\\n        else:\\n            self._n_components = self.n_components' sklearn/decomposition/_dict_learnin\r\n"]
[176.9836, "o", "ng.py\r\n"]
[177.478, "o", "\u001b[?2004l\r\n"]
[177.9724, "o", "\u001b[?2004h> \r\n"]
[178.4668, "o", "\u001b[?2004l\r\n"]
[178.9612, "o", "\u001b[?2004h> ^C\u001b[?2004l\r\n"]
[179.4556, "o", "\u001b[?2004h\u001b[?2004l\r\n"]
[180.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[180.002, "i", "sed -n '2164,2180p' sklearn/decomposition/_dict_learning.py\r"]
[180.004, "o", "sed -n '2164,2180p' sklearn/decomposition/_dict_learning.py\r\n"]
[180.280667, "o", "\u001b[?2004l\r\n"]
[180.555333, "o", "    def _check_params(self, X):\r\n"]
[180.83, "o", "        # n_components\r\n"]
[181.104667, "o", "        self._n_components = self.n_components\r\n"]
[181.379333, "o", "        if self._n_components is None:\r\n"]
[181.654, "o", "            self._n_components = X.shape[1]\r\n"]
[181.928667, "o", "\r\n"]
[182.203333, "o", "        # fit_algorithm\r\n"]
[182.478, "o", "        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n"]
[182.752667, "o", "        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n"]
[183.027333, "o", "\r\n"]
[183.302, "o", "        # batch_size\r\n"]
[183.576667, "o", "        self._batch_size = min(self.batch_size, X.shape[0])\r\n"]
[183.851333, "o", "\r\n"]
[184.126, "o", "    def _initialize_dict(self, X, random_state):\r\n"]
[184.400667, "o", "        \"\"\"Initialization of the dictionary.\"\"\"\r\n"]
[184.675333, "o", "        if self.dict_init is not None:\r\n"]
[185.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[185.002, "i", "grep -A20 \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py\r"]
[185.004, "o", "grep -A20 \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py\r\n"]
[185.230727, "o", "\u001b[?2004l\r\n"]
[185.455455, "o", "    \u001b[01;31m\u001b[Kdef _minibatch_step\u001b[m\u001b[K(self, X, dictionary, random_state, step):\r\n"]
[185.680182, "o", "        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n"]
[185.904909, "o", "        batch_size = X.shape[0]\r\n"]
[186.129636, "o", "\r\n"]
[186.354364, "o", "        # Compute code for this batch\r\n"]
[186.579091, "o", "        code = _sparse_encode(\r\n"]
[186.803818, "o", "            X,\r\n"]
[187.028545, "o", "            dictionary,\r\n"]
[187.253273, "o", "            algorithm=self._fit_algorithm,\r\n"]
[187.478, "o", "            alpha=self.alpha,\r\n"]
[187.702727, "o", "            n_jobs=self.n_jobs,\r\n"]
[187.927455, "o", "            positive=self.positive_code,\r\n"]
[188.152182, "o", "            max_iter=self.transform_max_iter,\r\n"]
[188.376909, "o", "            verbose=self.verbose,\r\n"]
[188.601636, "o", "        )\r\n"]
[188.826364, "o", "\r\n"]
[189.051091, "o", "        batch_cost = (\r\n"]
[189.275818, "o", "            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n"]
[189.500545, "o", "            + self.alpha * np.sum(np.abs(code))\r\n"]
[189.725273, "o", "        ) / batch_size\r\n"]
[190.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[190.002, "i", "grep -n \"def _sparse_encode\" sklearn/decomposition/_dict_learning.py\r"]
[190.004, "o", "grep -n \"def _sparse_encode\" sklearn/decomposition/_dict_learning.py\r\n"]
[191.654, "o", "\u001b[?2004l\r\n"]
[193.302, "o", "\u001b[32m\u001b[K38\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kdef _sparse_encode\u001b[m\u001b[K_precomputed(\r\n"]
[195.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[195.002, "i", "sed -n '378,420p' sklearn/decomposition/_dict_learning.py\r"]
[195.004, "o", "sed -n '378,420p' sklearn/decomposition/_dict_learning.py\r\n"]
[195.118364, "o", "\u001b[?2004l\r\n"]
[195.230727, "o", "def _sparse_encode(\r\n"]
[195.343091, "o", "    X,\r\n"]
[195.455455, "o", "    dictionary,\r\n"]
[195.567818, "o", "    *,\r\n"]
[195.680182, "o", "    gram=None,\r\n"]
[195.792545, "o", "    cov=None,\r\n"]
[195.904909, "o", "    algorithm=\"lasso_lars\",\r\n"]
[196.017273, "o", "    n_nonzero_coefs=None,\r\n"]
[196.129636, "o", "    alpha=None,\r\n"]
[196.242, "o", "    copy_cov=True,\r\n"]
[196.354364, "o", "    init=None,\r\n"]
[196.466727, "o", "    max_iter=1000,\r\n"]
[196.579091, "o", "    n_jobs=None,\r\n"]
[196.691455, "o", "    verbose=0,\r\n"]
[196.803818, "o", "    positive=False,\r\n"]
[196.916182, "o", "):\r\n"]
[197.028545, "o", "    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[197.140909, "o", "\r\n"]
[197.253273, "o", "    n_samples, n_features = X.shape\r\n"]
[197.365636, "o", "    n_components = dictionary.shape[0]\r\n"]
[197.478, "o", "\r\n"]
[197.590364, "o", "    if algorithm in (\"lars\", \"omp\"):\r\n"]
[197.702727, "o", "        regularization = n_nonzero_coefs\r\n"]
[197.815091, "o", "        if regularization is None:\r\n"]
[197.927455, "o", "            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[198.039818, "o", "    else:\r\n"]
[198.152182, "o", "        regularization = alpha\r\n"]
[198.264545, "o", "        if regularization is None:\r\n"]
[198.376909, "o", "            regularization = 1.0\r\n"]
[198.489273, "o", "\r\n"]
[198.601636, "o", "    if gram is None and algorithm != \"threshold\":\r\n"]
[198.714, "o", "        gram = np.dot(dictionary, dictionary.T)\r\n"]
[198.826364, "o", "\r\n"]
[198.938727, "o", "    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[199.051091, "o", "        copy_cov = False\r\n"]
[199.163455, "o", "        cov = np.dot(dictionary, X.T)\r\n"]
[199.275818, "o", "\r\n"]
[199.388182, "o", "    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[199.500545, "o", "        code = _sparse_encode_precomputed(\r\n"]
[199.612909, "o", "            X,\r\n"]
[199.725273, "o", "            dictionary,\r\n"]
[199.837636, "o", "            gram=gram,\r\n"]
[200.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[200.002, "i", "sed -n '420,470p' sklearn/decomposition/_dict_learning.py\r"]
[200.004, "o", "sed -n '420,470p' sklearn/decomposition/_dict_learning.py\r\n"]
[200.101077, "o", "\u001b[?2004l\r\n"]
[200.196154, "o", "            cov=cov,\r\n"]
[200.291231, "o", "            algorithm=algorithm,\r\n"]
[200.386308, "o", "            regularization=regularization,\r\n"]
[200.481385, "o", "            copy_cov=copy_cov,\r\n"]
[200.576462, "o", "            init=init,\r\n"]
[200.671538, "o", "            max_iter=max_iter,\r\n"]
[200.766615, "o", "            verbose=verbose,\r\n"]
[200.861692, "o", "            positive=positive,\r\n"]
[200.956769, "o", "        )\r\n"]
[201.051846, "o", "        return code\r\n"]
[201.146923, "o", "\r\n"]
[201.242, "o", "    # Enter parallel code block\r\n"]
[201.337077, "o", "    n_samples = X.shape[0]\r\n"]
[201.432154, "o", "    n_components = dictionary.shape[0]\r\n"]
[201.527231, "o", "    code = np.empty((n_samples, n_components))\r\n"]
[201.622308, "o", "    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[201.717385, "o", "\r\n"]
[201.812462, "o", "    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[201.907538, "o", "        delayed(_sparse_encode_precomputed)(\r\n"]
[202.002615, "o", "            X[this_slice],\r\n"]
[202.097692, "o", "            dictionary,\r\n"]
[202.192769, "o", "            gram=gram,\r\n"]
[202.287846, "o", "            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[202.382923, "o", "            algorithm=algorithm,\r\n"]
[202.478, "o", "            regularization=regularization,\r\n"]
[202.573077, "o", "            copy_cov=copy_cov,\r\n"]
[202.668154, "o", "            init=init[this_slice] if init is not None else None,\r\n"]
[202.763231, "o", "            max_iter=max_iter,\r\n"]
[202.858308, "o", "            verbose=verbose,\r\n"]
[202.953385, "o", "            positive=positive,\r\n"]
[203.048462, "o", "        )\r\n"]
[203.143538, "o", "        for this_slice in slices\r\n"]
[203.238615, "o", "    )\r\n"]
[203.333692, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[203.428769, "o", "        code[this_slice] = this_view\r\n"]
[203.523846, "o", "    return code\r\n"]
[203.618923, "o", "\r\n"]
[203.714, "o", "\r\n"]
[203.809077, "o", "def _update_dict(\r\n"]
[203.904154, "o", "    dictionary,\r\n"]
[203.999231, "o", "    Y,\r\n"]
[204.094308, "o", "    code,\r\n"]
[204.189385, "o", "    A=None,\r\n"]
[204.284462, "o", "    B=None,\r\n"]
[204.379538, "o", "    verbose=False,\r\n"]
[204.474615, "o", "    random_state=None,\r\n"]
[204.569692, "o", "    positive=False,\r\n"]
[204.664769, "o", "):\r\n"]
[204.759846, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[204.854923, "o", "\r\n"]
[205.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[205.002, "i", "sed -n '38,90p' sklearn/decomposition/_dict_learning.py\r"]
[205.004, "o", "sed -n '38,90p' sklearn/decomposition/_dict_learning.py\r\n"]
[205.097556, "o", "\u001b[?2004l\r\n"]
[205.189111, "o", "def _sparse_encode_precomputed(\r\n"]
[205.280667, "o", "    X,\r\n"]
[205.372222, "o", "    dictionary,\r\n"]
[205.463778, "o", "    *,\r\n"]
[205.555333, "o", "    gram=None,\r\n"]
[205.646889, "o", "    cov=None,\r\n"]
[205.738444, "o", "    algorithm=\"lasso_lars\",\r\n"]
[205.83, "o", "    regularization=None,\r\n"]
[205.921556, "o", "    copy_cov=True,\r\n"]
[206.013111, "o", "    init=None,\r\n"]
[206.104667, "o", "    max_iter=1000,\r\n"]
[206.196222, "o", "    verbose=0,\r\n"]
[206.287778, "o", "    positive=False,\r\n"]
[206.379333, "o", "):\r\n"]
[206.470889, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[206.562444, "o", "\r\n"]
[206.654, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[206.745556, "o", "\r\n"]
[206.837111, "o", "    Parameters\r\n"]
[206.928667, "o", "    ----------\r\n"]
[207.020222, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[207.111778, "o", "        Data matrix.\r\n"]
[207.203333, "o", "\r\n"]
[207.294889, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[207.386444, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[207.478, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[207.569556, "o", "\r\n"]
[207.661111, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[207.752667, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[207.844222, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[207.935778, "o", "\r\n"]
[208.027333, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[208.118889, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[208.210444, "o", "\r\n"]
[208.302, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[208.393556, "o", "            default='lasso_lars'\r\n"]
[208.485111, "o", "        The algorithm used:\r\n"]
[208.576667, "o", "\r\n"]
[208.668222, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[208.759778, "o", "          (`linear_model.lars_path`);\r\n"]
[208.851333, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[208.942889, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[209.034444, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[209.126, "o", "          the estimated components are sparse;\r\n"]
[209.217556, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[209.309111, "o", "          solution;\r\n"]
[209.400667, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[209.492222, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[209.583778, "o", "\r\n"]
[209.675333, "o", "    regularization : int or float, default=None\r\n"]
[209.766889, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[209.858444, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[210.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[210.002, "i", "sed -n '90,140p' sklearn/decomposition/_dict_learning.py\r"]
[210.004, "o", "sed -n '90,140p' sklearn/decomposition/_dict_learning.py\r\n"]
[210.101077, "o", "\u001b[?2004l\r\n"]
[210.196154, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[210.291231, "o", "\r\n"]
[210.386308, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[210.481385, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[210.576462, "o", "        `algorithm='lasso_cd'`.\r\n"]
[210.671538, "o", "\r\n"]
[210.766615, "o", "    max_iter : int, default=1000\r\n"]
[210.861692, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[210.956769, "o", "        `'lasso_lars'`.\r\n"]
[211.051846, "o", "\r\n"]
[211.146923, "o", "    copy_cov : bool, default=True\r\n"]
[211.242, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[211.337077, "o", "        be overwritten.\r\n"]
[211.432154, "o", "\r\n"]
[211.527231, "o", "    verbose : int, default=0\r\n"]
[211.622308, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[211.717385, "o", "\r\n"]
[211.812462, "o", "    positive: bool, default=False\r\n"]
[211.907538, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[212.002615, "o", "\r\n"]
[212.097692, "o", "        .. versionadded:: 0.20\r\n"]
[212.192769, "o", "\r\n"]
[212.287846, "o", "    Returns\r\n"]
[212.382923, "o", "    -------\r\n"]
[212.478, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[212.573077, "o", "        The sparse codes.\r\n"]
[212.668154, "o", "    \"\"\"\r\n"]
[212.763231, "o", "    n_samples, n_features = X.shape\r\n"]
[212.858308, "o", "    n_components = dictionary.shape[0]\r\n"]
[212.953385, "o", "\r\n"]
[213.048462, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[213.143538, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[213.238615, "o", "        try:\r\n"]
[213.333692, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[213.428769, "o", "\r\n"]
[213.523846, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[213.618923, "o", "            # corrects the verbosity level.\r\n"]
[213.714, "o", "            lasso_lars = LassoLars(\r\n"]
[213.809077, "o", "                alpha=alpha,\r\n"]
[213.904154, "o", "                fit_intercept=False,\r\n"]
[213.999231, "o", "                verbose=verbose,\r\n"]
[214.094308, "o", "                precompute=gram,\r\n"]
[214.189385, "o", "                fit_path=False,\r\n"]
[214.284462, "o", "                positive=positive,\r\n"]
[214.379538, "o", "                max_iter=max_iter,\r\n"]
[214.474615, "o", "            )\r\n"]
[214.569692, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[214.664769, "o", "            new_code = lasso_lars.coef_\r\n"]
[214.759846, "o", "        finally:\r\n"]
[214.854923, "o", "            np.seterr(**err_mgt)\r\n"]
[215.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[215.002, "i", "sed -n '140,190p' sklearn/decomposition/_dict_learning.py\r"]
[215.004, "o", "sed -n '140,190p' sklearn/decomposition/_dict_learning.py\r\n"]
[215.101077, "o", "\u001b[?2004l\r\n"]
[215.196154, "o", "\r\n"]
[215.291231, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[215.386308, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[215.481385, "o", "\r\n"]
[215.576462, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[215.671538, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[215.766615, "o", "        # argument that we could pass in from Lasso.\r\n"]
[215.861692, "o", "        clf = Lasso(\r\n"]
[215.956769, "o", "            alpha=alpha,\r\n"]
[216.051846, "o", "            fit_intercept=False,\r\n"]
[216.146923, "o", "            precompute=gram,\r\n"]
[216.242, "o", "            max_iter=max_iter,\r\n"]
[216.337077, "o", "            warm_start=True,\r\n"]
[216.432154, "o", "            positive=positive,\r\n"]
[216.527231, "o", "        )\r\n"]
[216.622308, "o", "\r\n"]
[216.717385, "o", "        if init is not None:\r\n"]
[216.812462, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[216.907538, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[217.002615, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[217.097692, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[217.192769, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[217.287846, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[217.382923, "o", "                init = np.array(init)\r\n"]
[217.478, "o", "            clf.coef_ = init\r\n"]
[217.573077, "o", "\r\n"]
[217.668154, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[217.763231, "o", "        new_code = clf.coef_\r\n"]
[217.858308, "o", "\r\n"]
[217.953385, "o", "    elif algorithm == \"lars\":\r\n"]
[218.048462, "o", "        try:\r\n"]
[218.143538, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[218.238615, "o", "\r\n"]
[218.333692, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[218.428769, "o", "            # corrects the verbosity level.\r\n"]
[218.523846, "o", "            lars = Lars(\r\n"]
[218.618923, "o", "                fit_intercept=False,\r\n"]
[218.714, "o", "                verbose=verbose,\r\n"]
[218.809077, "o", "                precompute=gram,\r\n"]
[218.904154, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[218.999231, "o", "                fit_path=False,\r\n"]
[219.094308, "o", "            )\r\n"]
[219.189385, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[219.284462, "o", "            new_code = lars.coef_\r\n"]
[219.379538, "o", "        finally:\r\n"]
[219.474615, "o", "            np.seterr(**err_mgt)\r\n"]
[219.569692, "o", "\r\n"]
[219.664769, "o", "    elif algorithm == \"threshold\":\r\n"]
[219.759846, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[219.854923, "o", "        if positive:\r\n"]
[220.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[220.002, "i", "sed -n '200,250p' sklearn/decomposition/_dict_learning.py\r"]
[220.004, "o", "sed -n '200,250p' sklearn/decomposition/_dict_learning.py\r\n"]
[220.101077, "o", "\u001b[?2004l\r\n"]
[220.196154, "o", "        ).T\r\n"]
[220.291231, "o", "\r\n"]
[220.386308, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[220.481385, "o", "\r\n"]
[220.576462, "o", "\r\n"]
[220.671538, "o", "@validate_params(\r\n"]
[220.766615, "o", "    {\r\n"]
[220.861692, "o", "        \"X\": [\"array-like\"],\r\n"]
[220.956769, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[221.051846, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[221.146923, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[221.242, "o", "        \"algorithm\": [\r\n"]
[221.337077, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[221.432154, "o", "        ],\r\n"]
[221.527231, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[221.622308, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[221.717385, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[221.812462, "o", "        \"init\": [\"array-like\", None],\r\n"]
[221.907538, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[222.002615, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[222.097692, "o", "        \"check_input\": [\"boolean\"],\r\n"]
[222.192769, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[222.287846, "o", "        \"positive\": [\"boolean\"],\r\n"]
[222.382923, "o", "    },\r\n"]
[222.478, "o", "    prefer_skip_nested_validation=True,\r\n"]
[222.573077, "o", ")\r\n"]
[222.668154, "o", "# XXX : could be moved to the linear_model module\r\n"]
[222.763231, "o", "def sparse_encode(\r\n"]
[222.858308, "o", "    X,\r\n"]
[222.953385, "o", "    dictionary,\r\n"]
[223.048462, "o", "    *,\r\n"]
[223.143538, "o", "    gram=None,\r\n"]
[223.238615, "o", "    cov=None,\r\n"]
[223.333692, "o", "    algorithm=\"lasso_lars\",\r\n"]
[223.428769, "o", "    n_nonzero_coefs=None,\r\n"]
[223.523846, "o", "    alpha=None,\r\n"]
[223.618923, "o", "    copy_cov=True,\r\n"]
[223.714, "o", "    init=None,\r\n"]
[223.809077, "o", "    max_iter=1000,\r\n"]
[223.904154, "o", "    n_jobs=None,\r\n"]
[223.999231, "o", "    check_input=True,\r\n"]
[224.094308, "o", "    verbose=0,\r\n"]
[224.189385, "o", "    positive=False,\r\n"]
[224.284462, "o", "):\r\n"]
[224.379538, "o", "    \"\"\"Sparse coding.\r\n"]
[224.474615, "o", "\r\n"]
[224.569692, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[224.664769, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[224.759846, "o", "\r\n"]
[224.854923, "o", "        X ~= code * dictionary\r\n"]
[225.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[225.002, "i", "cp sklearn/decomposition/_dict_learning.py sklearn/decomposition/_dict_learning.py.backup\r"]
[225.004, "o", "cp sklearn/decomposition/_dict_learning.py sklearn/decomposition/_dict_learning.py.backup\r\n"]
[225.006, "o", "\u001b[?2004l\r\n"]
[230.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[230.002, "i", "cat > optimization_patch.py << 'EOF'\r"]
[230.004, "o", "cat > optimization_patch.py << 'EOF'\r\n"]
[230.054, "o", "\u001b[?2004l\r\n"]
[230.102, "o", "\u001b[?2004h> import re\r\n"]
[230.15, "o", "\u001b[?2004l\r\n"]
[230.198, "o", "\u001b[?2004h> \r\n"]
[230.246, "o", "\u001b[?2004l\r\n"]
[230.294, "o", "\u001b[?2004h> # Read the file\r\n"]
[230.342, "o", "\u001b[?2004l\r\n"]
[230.39, "o", "\u001b[?2004h> with open('sklearn/decomposition/_dict_learning.py', 'r') as f:\r\n"]
[230.438, "o", "\u001b[?2004l\r\n"]
[230.486, "o", "\u001b[?2004h>     content = f.read()\r\n"]
[230.534, "o", "\u001b[?2004l\r\n"]
[230.582, "o", "\u001b[?2004h> \r\n"]
[230.63, "o", "\u001b[?2004l\r\n"]
[230.678, "o", "\u001b[?2004h> # Add a method to create and reuse solver after _check_params method\r\n"]
[230.726, "o", "\u001b[?2004l\r\n"]
[230.774, "o", "\u001b[?2004h> insert_pos = content.find('    def _initialize_dict(self, X, random_state):')\r\n"]
[230.822, "o", "\u001b[?2004l\r\n"]
[230.87, "o", "\u001b[?2004h> if insert_pos != -1:\r\n"]
[230.918, "o", "\u001b[?2004l\r\n"]
[230.966, "o", "\u001b[?2004h>     new_method = '''    def _get_solver(self, dictionary, max_iter):\r\n"]
[231.014, "o", "\u001b[?2004l\r\n"]
[231.062, "o", "\u001b[?2004h>         \"\"\"Get or create a reusable solver object.\"\"\"\r\n"]
[231.11, "o", "\u001b[?2004l\r\n"]
[231.158, "o", "\u001b[?2004h>         alpha = 1.0 / dictionary.shape[1]  # account for scaling\r\n"]
[231.206, "o", "\u001b[?2004l\r\n"]
[231.254, "o", "\u001b[?2004h>         \r\n"]
[231.302, "o", "\u001b[?2004l\r\n"]
[231.35, "o", "\u001b[?2004h>         if self._fit_algorithm == \"lasso_cd\":\r\n"]
[231.398, "o", "\u001b[?2004l\r\n"]
[231.446, "o", "\u001b[?2004h>             if not hasattr(self, '_solver') or self._solver is None:\r\n"]
[231.494, "o", "\u001b[?2004l\r\n"]
[231.542, "o", "\u001b[?2004h>                 from sklearn.linear_model import Lasso\r\n"]
[231.59, "o", "\u001b[?2004l\r\n"]
[231.638, "o", "\u001b[?2004h>                 self._solver = Lasso(\r\n"]
[231.686, "o", "\u001b[?2004l\r\n"]
[231.734, "o", "\u001b[?2004h>                     alpha=alpha,\r\n"]
[231.782, "o", "\u001b[?2004l\r\n"]
[231.83, "o", "\u001b[?2004h>                     fit_intercept=False,\r\n"]
[231.878, "o", "\u001b[?2004l\r\n"]
[231.926, "o", "\u001b[?2004h>                     max_iter=max_iter,\r\n"]
[231.974, "o", "\u001b[?2004l\r\n"]
[232.022, "o", "\u001b[?2004h>                     warm_start=True,\r\n"]
[232.07, "o", "\u001b[?2004l\r\n"]
[232.118, "o", "\u001b[?2004h>                     positive=self.positive_code,\r\n"]
[232.166, "o", "\u001b[?2004l\r\n"]
[232.214, "o", "\u001b[?2004h>                 )\r\n"]
[232.262, "o", "\u001b[?2004l\r\n"]
[232.31, "o", "\u001b[?2004h>             else:\r\n"]
[232.358, "o", "\u001b[?2004l\r\n"]
[232.406, "o", "\u001b[?2004h>                 self._solver.alpha = alpha\r\n"]
[232.454, "o", "\u001b[?2004l\r\n"]
[232.502, "o", "\u001b[?2004h>             return self._solver\r\n"]
[232.55, "o", "\u001b[?2004l\r\n"]
[232.598, "o", "\u001b[?2004h>         elif self._fit_algorithm == \"lasso_lars\":\r\n"]
[232.646, "o", "\u001b[?2004l\r\n"]
[232.694, "o", "\u001b[?2004h>             if not hasattr(self, '_solver') or self._solver is None:\r\n"]
[232.742, "o", "\u001b[?2004l\r\n"]
[232.79, "o", "\u001b[?2004h>                 from sklearn.linear_model import LassoLars\r\n"]
[232.838, "o", "\u001b[?2004l\r\n"]
[232.886, "o", "\u001b[?2004h>                 self._solver = LassoLars(\r\n"]
[232.934, "o", "\u001b[?2004l\r\n"]
[232.982, "o", "\u001b[?2004h>                     alpha=alpha,\r\n"]
[233.03, "o", "\u001b[?2004l\r\n"]
[233.078, "o", "\u001b[?2004h>                     fit_intercept=False,\r\n"]
[233.126, "o", "\u001b[?2004l\r\n"]
[233.174, "o", "\u001b[?2004h>                     verbose=False,\r\n"]
[233.222, "o", "\u001b[?2004l\r\n"]
[233.27, "o", "\u001b[?2004h>                     fit_path=False,\r\n"]
[233.318, "o", "\u001b[?2004l\r\n"]
[233.366, "o", "\u001b[?2004h>                     positive=self.positive_code,\r\n"]
[233.414, "o", "\u001b[?2004l\r\n"]
[233.462, "o", "\u001b[?2004h>                     max_iter=max_iter,\r\n"]
[233.51, "o", "\u001b[?2004l\r\n"]
[233.558, "o", "\u001b[?2004h>                 )\r\n"]
[233.606, "o", "\u001b[?2004l\r\n"]
[233.654, "o", "\u001b[?2004h>             else:\r\n"]
[233.702, "o", "\u001b[?2004l\r\n"]
[233.75, "o", "\u001b[?2004h>                 self._solver.alpha = alpha\r\n"]
[233.798, "o", "\u001b[?2004l\r\n"]
[233.846, "o", "\u001b[?2004h>             return self._solver\r\n"]
[233.894, "o", "\u001b[?2004l\r\n"]
[233.942, "o", "\u001b[?2004h>         return None\r\n"]
[233.99, "o", "\u001b[?2004l\r\n"]
[234.038, "o", "\u001b[?2004h> \r\n"]
[234.086, "o", "\u001b[?2004l\r\n"]
[234.134, "o", "\u001b[?2004h> '''\r\n"]
[234.182, "o", "\u001b[?2004l\r\n"]
[234.23, "o", "\u001b[?2004h>     content = content[:insert_pos] + new_method + content[insert_pos:]\r\n"]
[234.278, "o", "\u001b[?2004l\r\n"]
[234.326, "o", "\u001b[?2004h> \r\n"]
[234.374, "o", "\u001b[?2004l\r\n"]
[234.422, "o", "\u001b[?2004h> # Write back\r\n"]
[234.47, "o", "\u001b[?2004l\r\n"]
[234.518, "o", "\u001b[?2004h> with open('sklearn/decomposition/_dict_learning.py', 'w') as f:\r\n"]
[234.566, "o", "\u001b[?2004l\r\n"]
[234.614, "o", "\u001b[?2004h>     f.write(content)\r\n"]
[234.662, "o", "\u001b[?2004l\r\n"]
[234.71, "o", "\u001b[?2004h> \r\n"]
[234.758, "o", "\u001b[?2004l\r\n"]
[234.806, "o", "\u001b[?2004h> print(\"Added _get_solver method\")\r\n"]
[234.854, "o", "\u001b[?2004l\r\n"]
[234.902, "o", "\u001b[?2004h> EOF\r\n"]
[235.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[235.002, "i", "python3 optimization_patch.py\r"]
[235.004, "o", "python3 optimization_patch.py\r\n"]
[237.478, "o", "\u001b[?2004l\r\n"]
[240.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[240.002, "i", "cat > optimization_patch2.py << 'EOF'\r"]
[240.004, "o", "cat > optimization_patch2.py << 'EOF'\r\n"]
[240.04686, "o", "\u001b[?2004l\r\n"]
[240.087719, "o", "\u001b[?2004h> # Read the file\r\n"]
[240.128579, "o", "\u001b[?2004l\r\n"]
[240.169438, "o", "\u001b[?2004h> with open('sklearn/decomposition/_dict_learning.py', 'r') as f:\r\n"]
[240.210298, "o", "\u001b[?2004l\r\n"]
[240.251157, "o", "\u001b[?2004h>     content = f.read()\r\n"]
[240.292017, "o", "\u001b[?2004l\r\n"]
[240.332876, "o", "\u001b[?2004h> \r\n"]
[240.373736, "o", "\u001b[?2004l\r\n"]
[240.414595, "o", "\u001b[?2004h> # Find and replace the _minibatch_step method\r\n"]
[240.455455, "o", "\u001b[?2004l\r\n"]
[240.496314, "o", "\u001b[?2004h> old_method = '''    def _minibatch_step(self, X, dictionary, random_state, step):\r\n"]
[240.537174, "o", "\u001b[?2004l\r\n"]
[240.578033, "o", "\u001b[?2004h>         \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n"]
[240.618893, "o", "\u001b[?2004l\r\n"]
[240.659752, "o", "\u001b[?2004h>         batch_size = X.shape[0]\r\n"]
[240.700612, "o", "\u001b[?2004l\r\n"]
[240.741471, "o", "\u001b[?2004h> \r\n"]
[240.782331, "o", "\u001b[?2004l\r\n"]
[240.82319, "o", "\u001b[?2004h>         # Compute code for this batch\r\n"]
[240.86405, "o", "\u001b[?2004l\r\n"]
[240.904909, "o", "\u001b[?2004h>         code = _sparse_encode(\r\n"]
[240.945769, "o", "\u001b[?2004l\r\n"]
[240.986628, "o", "\u001b[?2004h>             X,\r\n"]
[241.027488, "o", "\u001b[?2004l\r\n"]
[241.068347, "o", "\u001b[?2004h>             dictionary,\r\n"]
[241.109207, "o", "\u001b[?2004l\r\n"]
[241.150066, "o", "\u001b[?2004h>             algorithm=self._fit_algorithm,\r\n"]
[241.190926, "o", "\u001b[?2004l\r\n"]
[241.231785, "o", "\u001b[?2004h>             alpha=self.alpha,\r\n"]
[241.272645, "o", "\u001b[?2004l\r\n"]
[241.313504, "o", "\u001b[?2004h>             n_jobs=self.n_jobs,\r\n"]
[241.354364, "o", "\u001b[?2004l\r\n"]
[241.395223, "o", "\u001b[?2004h>             positive=self.positive_code,\r\n"]
[241.436083, "o", "\u001b[?2004l\r\n"]
[241.476942, "o", "\u001b[?2004h>             max_iter=self.transform_max_iter,\r\n"]
[241.517802, "o", "\u001b[?2004l\r\n"]
[241.558661, "o", "\u001b[?2004h>             verbose=self.verbose,\r\n"]
[241.599521, "o", "\u001b[?2004l\r\n"]
[241.64038, "o", "\u001b[?2004h>         )'''\r\n"]
[241.68124, "o", "\u001b[?2004l\r\n"]
[241.722099, "o", "\u001b[?2004h> \r\n"]
[241.762959, "o", "\u001b[?2004l\r\n"]
[241.803818, "o", "\u001b[?2004h> new_method = '''    def _minibatch_step(self, X, dictionary, random_state, step):\r\n"]
[241.844678, "o", "\u001b[?2004l\r\n"]
[241.885537, "o", "\u001b[?2004h>         \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n"]
[241.926397, "o", "\u001b[?2004l\r\n"]
[241.967256, "o", "\u001b[?2004h>         batch_size = X.shape[0]\r\n"]
[242.008116, "o", "\u001b[?2004l\r\n"]
[242.048975, "o", "\u001b[?2004h> \r\n"]
[242.089835, "o", "\u001b[?2004l\r\n"]
[242.130694, "o", "\u001b[?2004h>         # Compute code for this batch using reusable solver for better performance\r\n"]
[242.171554, "o", "\u001b[?2004l\r\n"]
[242.212413, "o", "\u001b[?2004h>         solver = self._get_solver(dictionary, self.transform_max_iter)\r\n"]
[242.253273, "o", "\u001b[?2004l\r\n"]
[242.294132, "o", "\u001b[?2004h>         if solver is not None:\r\n"]
[242.334992, "o", "\u001b[?2004l\r\n"]
[242.375851, "o", "\u001b[?2004h>             # Use reusable solver to avoid object creation overhead\r\n"]
[242.416711, "o", "\u001b[?2004l\r\n"]
[242.45757, "o", "\u001b[?2004h>             if self._fit_algorithm == \"lasso_cd\":\r\n"]
[242.49843, "o", "\u001b[?2004l\r\n"]
[242.539289, "o", "\u001b[?2004h>                 # For coordinate descent, we can reuse the solver directly\r\n"]
[242.580149, "o", "\u001b[?2004l\r\n"]
[242.621008, "o", "\u001b[?2004h>                 solver.fit(dictionary.T, X.T, check_input=False)\r\n"]
[242.661868, "o", "\u001b[?2004l\r\n"]
[242.702727, "o", "\u001b[?2004h>                 code = solver.coef_.T\r\n"]
[242.743587, "o", "\u001b[?2004l\r\n"]
[242.784446, "o", "\u001b[?2004h>             elif self._fit_algorithm == \"lasso_lars\":\r\n"]
[242.825306, "o", "\u001b[?2004l\r\n"]
[242.866165, "o", "\u001b[?2004h>                 # For LARS, compute gram and cov matrices once\r\n"]
[242.907025, "o", "\u001b[?2004l\r\n"]
[242.947884, "o", "\u001b[?2004h>                 gram = dictionary @ dictionary.T\r\n"]
[242.988744, "o", "\u001b[?2004l\r\n"]
[243.029603, "o", "\u001b[?2004h>                 cov = dictionary @ X.T\r\n"]
[243.070463, "o", "\u001b[?2004l\r\n"]
[243.111322, "o", "\u001b[?2004h>                 solver.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[243.152182, "o", "\u001b[?2004l\r\n"]
[243.193041, "o", "\u001b[?2004h>                 code = solver.coef_.T\r\n"]
[243.233901, "o", "\u001b[?2004l\r\n"]
[243.27476, "o", "\u001b[?2004h>         else:\r\n"]
[243.31562, "o", "\u001b[?2004l\r\n"]
[243.356479, "o", "\u001b[?2004h>             # Fallback to original method for other algorithms\r\n"]
[243.397339, "o", "\u001b[?2004l\r\n"]
[243.438198, "o", "\u001b[?2004h>             code = _sparse_encode(\r\n"]
[243.479058, "o", "\u001b[?2004l\r\n"]
[243.519917, "o", "\u001b[?2004h>                 X,\r\n"]
[243.560777, "o", "\u001b[?2004l\r\n"]
[243.601636, "o", "\u001b[?2004h>                 dictionary,\r\n"]
[243.642496, "o", "\u001b[?2004l\r\n"]
[243.683355, "o", "\u001b[?2004h>                 algorithm=self._fit_algorithm,\r\n"]
[243.724215, "o", "\u001b[?2004l\r\n"]
[243.765074, "o", "\u001b[?2004h>                 alpha=self.alpha,\r\n"]
[243.805934, "o", "\u001b[?2004l\r\n"]
[243.846793, "o", "\u001b[?2004h>                 n_jobs=self.n_jobs,\r\n"]
[243.887653, "o", "\u001b[?2004l\r\n"]
[243.928512, "o", "\u001b[?2004h>                 positive=self.positive_code,\r\n"]
[243.969372, "o", "\u001b[?2004l\r\n"]
[244.010231, "o", "\u001b[?2004h>                 max_iter=self.transform_max_iter,\r\n"]
[244.051091, "o", "\u001b[?2004l\r\n"]
[244.09195, "o", "\u001b[?2004h>                 verbose=self.verbose,\r\n"]
[244.13281, "o", "\u001b[?2004l\r\n"]
[244.173669, "o", "\u001b[?2004h>             )'''\r\n"]
[244.214529, "o", "\u001b[?2004l\r\n"]
[244.255388, "o", "\u001b[?2004h> \r\n"]
[244.296248, "o", "\u001b[?2004l\r\n"]
[244.337107, "o", "\u001b[?2004h> content = content.replace(old_method, new_method)\r\n"]
[244.377967, "o", "\u001b[?2004l\r\n"]
[244.418826, "o", "\u001b[?2004h> \r\n"]
[244.459686, "o", "\u001b[?2004l\r\n"]
[244.500545, "o", "\u001b[?2004h> # Write back\r\n"]
[244.541405, "o", "\u001b[?2004l\r\n"]
[244.582264, "o", "\u001b[?2004h> with open('sklearn/decomposition/_dict_learning.py', 'w') as f:\r\n"]
[244.623124, "o", "\u001b[?2004l\r\n"]
[244.663983, "o", "\u001b[?2004h>     f.write(content)\r\n"]
[244.704843, "o", "\u001b[?2004l\r\n"]
[244.745702, "o", "\u001b[?2004h> \r\n"]
[244.786562, "o", "\u001b[?2004l\r\n"]
[244.827421, "o", "\u001b[?2004h> print(\"Optimized _minibatch_step method\")\r\n"]
[244.868281, "o", "\u001b[?2004l\r\n"]
[244.90914, "o", "\u001b[?2004h> EOF\r\n"]
[245.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[245.002, "i", "python3 optimization_patch2.py\r"]
[245.004, "o", "python3 optimization_patch2.py\r\n"]
[247.478, "o", "\u001b[?2004l\r\n"]
[250.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[250.002, "i", "cd asv_benchmarks && asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r"]
[250.004, "o", "cd asv_benchmarks && asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r\n"]
[250.83, "o", "\u001b[?2004l\r\n"]
[251.654, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[252.478, "o", "\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n"]
[253.302, "o", "[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n"]
[254.126, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                 failed\r\n"]
[255.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo/asv_benchmarks# "]
[255.002, "i", "cd .. && python3 -c \"from sklearn.decomposition import MiniBatchDictionaryLearning; print('Import su\r"]
[255.004, "o", "cd .. && python3 -c \"from sklearn.decomposition import MiniBatchDictionaryLearning; print('Import su\r\n"]
[256.654, "o", "uccessful')\"\r\n"]
[258.302, "o", "\u001b[?2004l\r\n"]
[260.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[260.002, "i", "python3 -c \"\r"]
[260.004, "o", "python3 -c \"\r\n"]
[260.212, "o", "\u001b[?2004l\r\n"]
[260.418, "o", "\u001b[?2004h> import numpy as np\r\n"]
[260.624, "o", "\u001b[?2004l\r\n"]
[260.83, "o", "\u001b[?2004h> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n"]
[261.036, "o", "\u001b[?2004l\r\n"]
[261.242, "o", "\u001b[?2004h> X = np.random.randn(100, 50)\r\n"]
[261.448, "o", "\u001b[?2004l\r\n"]
[261.654, "o", "\u001b[?2004h> model = MiniBatchDictionaryLearning(n_components=10, max_iter=1)\r\n"]
[261.86, "o", "\u001b[?2004l\r\n"]
[262.066, "o", "\u001b[?2004h> model.fit(X)\r\n"]
[262.272, "o", "\u001b[?2004l\r\n"]
[262.478, "o", "\u001b[?2004h> print('Test successful')\r\n"]
[262.684, "o", "\u001b[?2004l\r\n"]
[262.89, "o", "\u001b[?2004h> \"\r\n"]
[263.096, "o", "\u001b[?2004l\r\n"]
[263.302, "o", "Traceback (most recent call last):\r\n"]
[263.508, "o", "  File \"<string>\", line 6, in <module>\r\n"]
[263.714, "o", "  File \"/workspace/repo/sklearn/base.py\", line 1161, in wrapper\r\n"]
[263.92, "o", "    return fit_method(estimator, *args, **kwargs)\r\n"]
[264.126, "o", "  File \"/workspace/repo/sklearn/decomposition/_dict_learning.py\", line 2447, in fit\r\n"]
[264.332, "o", "    batch_cost = self._minibatch_step(\r\n"]
[264.538, "o", "  File \"/workspace/repo/sklearn/decomposition/_dict_learning.py\", line 2284, in _minibatch_step\r\n"]
[264.744, "o", "    0.5 * ((X - code @ dictionary) ** 2).sum()\r\n"]
[265.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[265.002, "i", "cat > fix_dimensions.py << 'EOF'\r"]
[265.004, "o", "cat > fix_dimensions.py << 'EOF'\r\n"]
[265.075634, "o", "\u001b[?2004l\r\n"]
[265.145268, "o", "\u001b[?2004h> # Read the file\r\n"]
[265.214901, "o", "\u001b[?2004l\r\n"]
[265.284535, "o", "\u001b[?2004h> with open('sklearn/decomposition/_dict_learning.py', 'r') as f:\r\n"]
[265.354169, "o", "\u001b[?2004l\r\n"]
[265.423803, "o", "\u001b[?2004h>     content = f.read()\r\n"]
[265.493437, "o", "\u001b[?2004l\r\n"]
[265.56307, "o", "\u001b[?2004h> \r\n"]
[265.632704, "o", "\u001b[?2004l\r\n"]
[265.702338, "o", "\u001b[?2004h> # Fix the matrix dimensions in _minibatch_step\r\n"]
[265.771972, "o", "\u001b[?2004l\r\n"]
[265.841606, "o", "\u001b[?2004h> old_code = '''            if self._fit_algorithm == \"lasso_cd\":\r\n"]
[265.911239, "o", "\u001b[?2004l\r\n"]
[265.980873, "o", "\u001b[?2004h>                 # For coordinate descent, we can reuse the solver directly\r\n"]
[266.050507, "o", "\u001b[?2004l\r\n"]
[266.120141, "o", "\u001b[?2004h>                 solver.fit(dictionary.T, X.T, check_input=False)\r\n"]
[266.189775, "o", "\u001b[?2004l\r\n"]
[266.259408, "o", "\u001b[?2004h>                 code = solver.coef_.T\r\n"]
[266.329042, "o", "\u001b[?2004l\r\n"]
[266.398676, "o", "\u001b[?2004h>             elif self._fit_algorithm == \"lasso_lars\":\r\n"]
[266.46831, "o", "\u001b[?2004l\r\n"]
[266.537944, "o", "\u001b[?2004h>                 # For LARS, compute gram and cov matrices once\r\n"]
[266.607577, "o", "\u001b[?2004l\r\n"]
[266.677211, "o", "\u001b[?2004h>                 gram = dictionary @ dictionary.T\r\n"]
[266.746845, "o", "\u001b[?2004l\r\n"]
[266.816479, "o", "\u001b[?2004h>                 cov = dictionary @ X.T\r\n"]
[266.886113, "o", "\u001b[?2004l\r\n"]
[266.955746, "o", "\u001b[?2004h>                 solver.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[267.02538, "o", "\u001b[?2004l\r\n"]
[267.095014, "o", "\u001b[?2004h>                 code = solver.coef_.T'''\r\n"]
[267.164648, "o", "\u001b[?2004l\r\n"]
[267.234282, "o", "\u001b[?2004h> \r\n"]
[267.303915, "o", "\u001b[?2004l\r\n"]
[267.373549, "o", "\u001b[?2004h> new_code = '''            if self._fit_algorithm == \"lasso_cd\":\r\n"]
[267.443183, "o", "\u001b[?2004l\r\n"]
[267.512817, "o", "\u001b[?2004h>                 # For coordinate descent, we can reuse the solver directly\r\n"]
[267.582451, "o", "\u001b[?2004l\r\n"]
[267.652085, "o", "\u001b[?2004h>                 solver.fit(dictionary.T, X.T, check_input=False)\r\n"]
[267.721718, "o", "\u001b[?2004l\r\n"]
[267.791352, "o", "\u001b[?2004h>                 code = solver.coef_\r\n"]
[267.860986, "o", "\u001b[?2004l\r\n"]
[267.93062, "o", "\u001b[?2004h>             elif self._fit_algorithm == \"lasso_lars\":\r\n"]
[268.000254, "o", "\u001b[?2004l\r\n"]
[268.069887, "o", "\u001b[?2004h>                 # For LARS, compute gram and cov matrices once\r\n"]
[268.139521, "o", "\u001b[?2004l\r\n"]
[268.209155, "o", "\u001b[?2004h>                 gram = dictionary @ dictionary.T\r\n"]
[268.278789, "o", "\u001b[?2004l\r\n"]
[268.348423, "o", "\u001b[?2004h>                 cov = dictionary @ X.T\r\n"]
[268.418056, "o", "\u001b[?2004l\r\n"]
[268.48769, "o", "\u001b[?2004h>                 solver.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[268.557324, "o", "\u001b[?2004l\r\n"]
[268.626958, "o", "\u001b[?2004h>                 code = solver.coef_'''\r\n"]
[268.696592, "o", "\u001b[?2004l\r\n"]
[268.766225, "o", "\u001b[?2004h> \r\n"]
[268.835859, "o", "\u001b[?2004l\r\n"]
[268.905493, "o", "\u001b[?2004h> content = content.replace(old_code, new_code)\r\n"]
[268.975127, "o", "\u001b[?2004l\r\n"]
[269.044761, "o", "\u001b[?2004h> \r\n"]
[269.114394, "o", "\u001b[?2004l\r\n"]
[269.184028, "o", "\u001b[?2004h> # Write back\r\n"]
[269.253662, "o", "\u001b[?2004l\r\n"]
[269.323296, "o", "\u001b[?2004h> with open('sklearn/decomposition/_dict_learning.py', 'w') as f:\r\n"]
[269.39293, "o", "\u001b[?2004l\r\n"]
[269.462563, "o", "\u001b[?2004h>     f.write(content)\r\n"]
[269.532197, "o", "\u001b[?2004l\r\n"]
[269.601831, "o", "\u001b[?2004h> \r\n"]
[269.671465, "o", "\u001b[?2004l\r\n"]
[269.741099, "o", "\u001b[?2004h> print(\"Fixed matrix dimensions\")\r\n"]
[269.810732, "o", "\u001b[?2004l\r\n"]
[269.880366, "o", "\u001b[?2004h> EOF\r\n"]
[270.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[270.002, "i", "python3 fix_dimensions.py\r"]
[270.004, "o", "python3 fix_dimensions.py\r\n"]
[272.478, "o", "\u001b[?2004l\r\n"]
[275.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[275.002, "i", "python3 -c \"\r"]
[275.004, "o", "python3 -c \"\r\n"]
[275.315, "o", "\u001b[?2004l\r\n"]
[275.624, "o", "\u001b[?2004h> import numpy as np\r\n"]
[275.933, "o", "\u001b[?2004l\r\n"]
[276.242, "o", "\u001b[?2004h> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n"]
[276.551, "o", "\u001b[?2004l\r\n"]
[276.86, "o", "\u001b[?2004h> X = np.random.randn(100, 50)\r\n"]
[277.169, "o", "\u001b[?2004l\r\n"]
[277.478, "o", "\u001b[?2004h> model = MiniBatchDictionaryLearning(n_components=10, max_iter=1)\r\n"]
[277.787, "o", "\u001b[?2004l\r\n"]
[278.096, "o", "\u001b[?2004h> model.fit(X)\r\n"]
[278.405, "o", "\u001b[?2004l\r\n"]
[278.714, "o", "\u001b[?2004h> print('Test successful - shape:', model.components_.shape)\r\n"]
[279.023, "o", "\u001b[?2004l\r\n"]
[279.332, "o", "\u001b[?2004h> \"\r\n"]
[279.641, "o", "\u001b[?2004l\r\n"]
[280.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[280.002, "i", "cd asv_benchmarks && asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r"]
[280.004, "o", "cd asv_benchmarks && asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r\n"]
[280.83, "o", "\u001b[?2004l\r\n"]
[281.654, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[282.478, "o", "\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n"]
[283.302, "o", "[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n"]
[284.126, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                 failed\r\n"]
[285.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo/asv_benchmarks# "]
[285.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\" --show-stderr\r"]
[285.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\" --show-stderr\r\n"]
[285.120977, "o", "\u001b[?2004l\r\n"]
[285.235953, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[285.35093, "o", "\u00b7 \u001b[0;32mRunning 6 total benchmarks (1 commits * 1 environments * 6 benchmarks)\u001b[0m\r\n"]
[285.465907, "o", "[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n"]
[285.580884, "o", "[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                 failed\r\n"]
[285.69586, "o", "[16.67%] \u00b7\u00b7\u00b7\u00b7 \u001b[0;31mTraceback (most recent call last):\u001b[0m\r\n"]
[285.810837, "o", "                File \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/asv/benchmark.py\", line 99, in <module>\r\n"]
[285.925814, "o", "                  main()\r\n"]
[286.040791, "o", "                File \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/asv/benchmark.py\", line 91, in main\r\n"]
[286.155767, "o", "                  commands[mode](args)\r\n"]
[286.270744, "o", "                File \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/asv_runner/setup_cache.py\", line 41, in _setup_cache\r\n"]
[286.385721, "o", "                  cache = benchmark.do_setup_cache()\r\n"]
[286.500698, "o", "                File \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/asv_runner/benchmarks/_base.py\", line 655, in do_setup_cache\r\n"]
[286.615674, "o", "                  return self._setup_cache()\r\n"]
[286.730651, "o", "                File \"/workspace/repo/asv_benchmarks/benchmarks/decomposition.py\", line 76, in setup_cache\r\n"]
[286.845628, "o", "                  super().setup_cache()\r\n"]
[286.960605, "o", "                File \"/workspace/repo/asv_benchmarks/benchmarks/common.py\", line 154, in setup_cache\r\n"]
[287.075581, "o", "                  estimator.fit(X, y)\r\n"]
[287.190558, "o", "                File \"/workspace/repo/sklearn/base.py\", line 1161, in wrapper\r\n"]
[287.305535, "o", "                  return fit_method(estimator, *args, **kwargs)\r\n"]
[287.420512, "o", "                File \"/workspace/repo/sklearn/decomposition/_dict_learning.py\", line 2473, in fit\r\n"]
[287.535488, "o", "                  self._minibatch_step(X_train[batch], dictionary, self._random_state, i)\r\n"]
[287.650465, "o", "                File \"/workspace/repo/sklearn/decomposition/_dict_learning.py\", line 2262, in _minibatch_step\r\n"]
[287.765442, "o", "                  solver.fit(dictionary.T, X.T, check_input=False)\r\n"]
[287.880419, "o", "                File \"/workspace/repo/sklearn/base.py\", line 1161, in wrapper\r\n"]
[287.995395, "o", "                  return fit_method(estimator, *args, **kwargs)\r\n"]
[288.110372, "o", "                File \"/workspace/repo/sklearn/linear_model/_coordinate_descent.py\", line 1001, in fit\r\n"]
[288.225349, "o", "                  _, this_coef, this_dual_gap, this_iter = self.path(\r\n"]
[288.340326, "o", "                File \"/workspace/repo/sklearn/linear_model/_coordinate_descent.py\", line 628, in enet_path\r\n"]
[288.455302, "o", "                  model = cd_fast.enet_coordinate_descent(\r\n"]
[288.570279, "o", "                File \"sklearn/linear_model/_cd_fast.pyx\", line 98, in sklearn.linear_model._cd_fast.enet_coordinate_descent\r\n"]
[288.685256, "o", "                File \"stringsource\", line 660, in View.MemoryView.memoryview_cwrapper\r\n"]
[288.800233, "o", "                File \"stringsource\", line 350, in View.MemoryView.memoryview.__cinit__\r\n"]
[288.915209, "o", "              ValueError: ndarray is not Fortran contiguous\r\n"]
[289.030186, "o", "              \r\n"]
[289.145163, "o", "              asv: setup_cache failed (exit status 1)\r\n"]
[289.26014, "o", "\r\n"]
[289.375116, "o", "[16.67%] \u00b7\u00b7\u00b7 \u001b[0;33mdecomposition.MiniBatchDictionaryLearningBenchmark.peakmem_fit skipped (setup_cache failed)\u001b[0m\r\n"]
[289.490093, "o", "[33.33%] \u00b7\u00b7\u00b7 \u001b[0;33mdecomposition.MiniBatchDictionaryLearningBenchmark.peakmem_transform skipped (setup_cache failed)\u001b[0m\r\n"]
[289.60507, "o", "[50.00%] \u00b7\u00b7\u00b7 \u001b[0;33mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit skipped (setup_cache failed)\u001b[0m\r\n"]
[289.720047, "o", "[66.67%] \u00b7\u00b7\u00b7 \u001b[0;33mdecomposition.MiniBatchDictionaryLearningBenchmark.time_transform skipped (setup_cache failed)\u001b[0m\r\n"]
[289.835023, "o", "[83.33%] \u00b7\u00b7\u00b7 \u001b[0;33mdecomposition.MiniBatchDictionaryLearningBenchmark.track_test_score skipped (setup_cache failed)\u001b[0m\r\n"]
[290.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo/asv_benchmarks# "]
[290.002, "i", "cd .. && cat > fix_fortran.py << 'EOF'\r"]
[290.004, "o", "cd .. && cat > fix_fortran.py << 'EOF'\r\n"]
[290.064165, "o", "\u001b[?2004l\r\n"]
[290.122329, "o", "\u001b[?2004h> import numpy as np\r\n"]
[290.180494, "o", "\u001b[?2004l\r\n"]
[290.238659, "o", "\u001b[?2004h> \r\n"]
[290.296824, "o", "\u001b[?2004l\r\n"]
[290.354988, "o", "\u001b[?2004h> # Read the file\r\n"]
[290.413153, "o", "\u001b[?2004l\r\n"]
[290.471318, "o", "\u001b[?2004h> with open('sklearn/decomposition/_dict_learning.py', 'r') as f:\r\n"]
[290.529482, "o", "\u001b[?2004l\r\n"]
[290.587647, "o", "\u001b[?2004h>     content = f.read()\r\n"]
[290.645812, "o", "\u001b[?2004l\r\n"]
[290.703976, "o", "\u001b[?2004h> \r\n"]
[290.762141, "o", "\u001b[?2004l\r\n"]
[290.820306, "o", "\u001b[?2004h> # Fix the Fortran contiguity issue\r\n"]
[290.878471, "o", "\u001b[?2004l\r\n"]
[290.936635, "o", "\u001b[?2004h> old_code = '''            if self._fit_algorithm == \"lasso_cd\":\r\n"]
[290.9948, "o", "\u001b[?2004l\r\n"]
[291.052965, "o", "\u001b[?2004h>                 # For coordinate descent, we can reuse the solver directly\r\n"]
[291.111129, "o", "\u001b[?2004l\r\n"]
[291.169294, "o", "\u001b[?2004h>                 solver.fit(dictionary.T, X.T, check_input=False)\r\n"]
[291.227459, "o", "\u001b[?2004l\r\n"]
[291.285624, "o", "\u001b[?2004h>                 code = solver.coef_\r\n"]
[291.343788, "o", "\u001b[?2004l\r\n"]
[291.401953, "o", "\u001b[?2004h>             elif self._fit_algorithm == \"lasso_lars\":\r\n"]
[291.460118, "o", "\u001b[?2004l\r\n"]
[291.518282, "o", "\u001b[?2004h>                 # For LARS, compute gram and cov matrices once\r\n"]
[291.576447, "o", "\u001b[?2004l\r\n"]
[291.634612, "o", "\u001b[?2004h>                 gram = dictionary @ dictionary.T\r\n"]
[291.692776, "o", "\u001b[?2004l\r\n"]
[291.750941, "o", "\u001b[?2004h>                 cov = dictionary @ X.T\r\n"]
[291.809106, "o", "\u001b[?2004l\r\n"]
[291.867271, "o", "\u001b[?2004h>                 solver.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[291.925435, "o", "\u001b[?2004l\r\n"]
[291.9836, "o", "\u001b[?2004h>                 code = solver.coef_'''\r\n"]
[292.041765, "o", "\u001b[?2004l\r\n"]
[292.099929, "o", "\u001b[?2004h> \r\n"]
[292.158094, "o", "\u001b[?2004l\r\n"]
[292.216259, "o", "\u001b[?2004h> new_code = '''            if self._fit_algorithm == \"lasso_cd\":\r\n"]
[292.274424, "o", "\u001b[?2004l\r\n"]
[292.332588, "o", "\u001b[?2004h>                 # For coordinate descent, we can reuse the solver directly\r\n"]
[292.390753, "o", "\u001b[?2004l\r\n"]
[292.448918, "o", "\u001b[?2004h>                 # Ensure arrays are Fortran contiguous for coordinate descent\r\n"]
[292.507082, "o", "\u001b[?2004l\r\n"]
[292.565247, "o", "\u001b[?2004h>                 dict_T = np.asfortranarray(dictionary.T)\r\n"]
[292.623412, "o", "\u001b[?2004l\r\n"]
[292.681576, "o", "\u001b[?2004h>                 X_T = np.asfortranarray(X.T)\r\n"]
[292.739741, "o", "\u001b[?2004l\r\n"]
[292.797906, "o", "\u001b[?2004h>                 solver.fit(dict_T, X_T, check_input=False)\r\n"]
[292.856071, "o", "\u001b[?2004l\r\n"]
[292.914235, "o", "\u001b[?2004h>                 code = solver.coef_\r\n"]
[292.9724, "o", "\u001b[?2004l\r\n"]
[293.030565, "o", "\u001b[?2004h>             elif self._fit_algorithm == \"lasso_lars\":\r\n"]
[293.088729, "o", "\u001b[?2004l\r\n"]
[293.146894, "o", "\u001b[?2004h>                 # For LARS, compute gram and cov matrices once\r\n"]
[293.205059, "o", "\u001b[?2004l\r\n"]
[293.263224, "o", "\u001b[?2004h>                 gram = dictionary @ dictionary.T\r\n"]
[293.321388, "o", "\u001b[?2004l\r\n"]
[293.379553, "o", "\u001b[?2004h>                 cov = dictionary @ X.T\r\n"]
[293.437718, "o", "\u001b[?2004l\r\n"]
[293.495882, "o", "\u001b[?2004h>                 dict_T = np.asfortranarray(dictionary.T)\r\n"]
[293.554047, "o", "\u001b[?2004l\r\n"]
[293.612212, "o", "\u001b[?2004h>                 X_T = np.asfortranarray(X.T)\r\n"]
[293.670376, "o", "\u001b[?2004l\r\n"]
[293.728541, "o", "\u001b[?2004h>                 solver.fit(dict_T, X_T, Xy=cov)\r\n"]
[293.786706, "o", "\u001b[?2004l\r\n"]
[293.844871, "o", "\u001b[?2004h>                 code = solver.coef_'''\r\n"]
[293.903035, "o", "\u001b[?2004l\r\n"]
[293.9612, "o", "\u001b[?2004h> \r\n"]
[294.019365, "o", "\u001b[?2004l\r\n"]
[294.077529, "o", "\u001b[?2004h> content = content.replace(old_code, new_code)\r\n"]
[294.135694, "o", "\u001b[?2004l\r\n"]
[294.193859, "o", "\u001b[?2004h> \r\n"]
[294.252024, "o", "\u001b[?2004l\r\n"]
[294.310188, "o", "\u001b[?2004h> # Write back\r\n"]
[294.368353, "o", "\u001b[?2004l\r\n"]
[294.426518, "o", "\u001b[?2004h> with open('sklearn/decomposition/_dict_learning.py', 'w') as f:\r\n"]
[294.484682, "o", "\u001b[?2004l\r\n"]
[294.542847, "o", "\u001b[?2004h>     f.write(content)\r\n"]
[294.601012, "o", "\u001b[?2004l\r\n"]
[294.659176, "o", "\u001b[?2004h> \r\n"]
[294.717341, "o", "\u001b[?2004l\r\n"]
[294.775506, "o", "\u001b[?2004h> print(\"Fixed Fortran contiguity issue\")\r\n"]
[294.833671, "o", "\u001b[?2004l\r\n"]
[294.891835, "o", "\u001b[?2004h> EOF\r\n"]
[295.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[295.002, "i", "python3 fix_fortran.py\r"]
[295.004, "o", "python3 fix_fortran.py\r\n"]
[297.478, "o", "\u001b[?2004l\r\n"]
[300.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[300.002, "i", "python3 -c \"\r"]
[300.004, "o", "python3 -c \"\r\n"]
[300.315, "o", "\u001b[?2004l\r\n"]
[300.624, "o", "\u001b[?2004h> import numpy as np\r\n"]
[300.933, "o", "\u001b[?2004l\r\n"]
[301.242, "o", "\u001b[?2004h> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n"]
[301.551, "o", "\u001b[?2004l\r\n"]
[301.86, "o", "\u001b[?2004h> X = np.random.randn(100, 50)\r\n"]
[302.169, "o", "\u001b[?2004l\r\n"]
[302.478, "o", "\u001b[?2004h> model = MiniBatchDictionaryLearning(n_components=10, max_iter=2)\r\n"]
[302.787, "o", "\u001b[?2004l\r\n"]
[303.096, "o", "\u001b[?2004h> model.fit(X)\r\n"]
[303.405, "o", "\u001b[?2004l\r\n"]
[303.714, "o", "\u001b[?2004h> print('Test successful - shape:', model.components_.shape)\r\n"]
[304.023, "o", "\u001b[?2004l\r\n"]
[304.332, "o", "\u001b[?2004h> \"\r\n"]
[304.641, "o", "\u001b[?2004l\r\n"]
[305.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[305.002, "i", "cd asv_benchmarks && asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r"]
[305.004, "o", "cd asv_benchmarks && asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r\n"]
[305.3356, "o", "\u001b[?2004l\r\n"]
[305.6652, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[305.9948, "o", "\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n"]
[306.3244, "o", "[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n"]
[306.654, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n"]
[306.9836, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0m                                                                                     ok\r\n"]
[307.3132, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ============\u001b[0m\r\n"]
[307.6428, "o", "              --                 n_jobs   \r\n"]
[307.9724, "o", "              --------------- ------------\r\n"]
[308.302, "o", "               fit_algorithm       1      \r\n"]
[308.6316, "o", "              =============== ============\r\n"]
[308.9612, "o", "                    lars       3.64\u00b10.05s \r\n"]
[309.2908, "o", "                     cd        1.78\u00b10.03s \r\n"]
[309.6204, "o", "              =============== ============\r\n"]
[310.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo/asv_benchmarks# "]
[310.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r"]
[310.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r\n"]
[310.082062, "o", "\u001b[?2004l\r\n"]
[310.158123, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[310.234185, "o", "\u00b7 \u001b[0;32mRunning 6 total benchmarks (1 commits * 1 environments * 6 benchmarks)\u001b[0m\r\n"]
[310.310246, "o", "[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n"]
[310.386308, "o", "[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n"]
[310.462369, "o", "[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.peakmem_fit\u001b[0m                                                                                  ok\r\n"]
[310.538431, "o", "[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =======\u001b[0m\r\n"]
[310.614492, "o", "             --               n_jobs\r\n"]
[310.690554, "o", "             --------------- -------\r\n"]
[310.766615, "o", "              fit_algorithm     1   \r\n"]
[310.842677, "o", "             =============== =======\r\n"]
[310.918738, "o", "                   lars       93.7M \r\n"]
[310.9948, "o", "                    cd        93.4M \r\n"]
[311.070862, "o", "             =============== =======\r\n"]
[311.146923, "o", "\r\n"]
[311.222985, "o", "[33.33%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.peakmem_transform\u001b[0m                                                                            ok\r\n"]
[311.299046, "o", "[33.33%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =======\u001b[0m\r\n"]
[311.375108, "o", "             --               n_jobs\r\n"]
[311.451169, "o", "             --------------- -------\r\n"]
[311.527231, "o", "              fit_algorithm     1   \r\n"]
[311.603292, "o", "             =============== =======\r\n"]
[311.679354, "o", "                   lars       84.7M \r\n"]
[311.755415, "o", "                    cd        84.7M \r\n"]
[311.831477, "o", "             =============== =======\r\n"]
[311.907538, "o", "\r\n"]
[311.9836, "o", "[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0m                                                                                     ok\r\n"]
[312.059662, "o", "[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ============\u001b[0m\r\n"]
[312.135723, "o", "             --                 n_jobs   \r\n"]
[312.211785, "o", "             --------------- ------------\r\n"]
[312.287846, "o", "              fit_algorithm       1      \r\n"]
[312.363908, "o", "             =============== ============\r\n"]
[312.439969, "o", "                   lars       3.67\u00b10.03s \r\n"]
[312.516031, "o", "                    cd        1.76\u00b10.06s \r\n"]
[312.592092, "o", "             =============== ============\r\n"]
[312.668154, "o", "\r\n"]
[312.744215, "o", "[66.67%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_transform\u001b[0m                                                                               ok\r\n"]
[312.820277, "o", "[66.67%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =========\u001b[0m\r\n"]
[312.896338, "o", "             --                n_jobs \r\n"]
[312.9724, "o", "             --------------- ---------\r\n"]
[313.048462, "o", "              fit_algorithm      1    \r\n"]
[313.124523, "o", "             =============== =========\r\n"]
[313.200585, "o", "                   lars       158\u00b11ms \r\n"]
[313.276646, "o", "                    cd        158\u00b11ms \r\n"]
[313.352708, "o", "             =============== =========\r\n"]
[313.428769, "o", "\r\n"]
[313.504831, "o", "[83.33%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.track_test_score\u001b[0m                                                                             ok\r\n"]
[313.580892, "o", "[83.33%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ======================\u001b[0m\r\n"]
[313.656954, "o", "             --                      n_jobs        \r\n"]
[313.733015, "o", "             --------------- ----------------------\r\n"]
[313.809077, "o", "              fit_algorithm            1           \r\n"]
[313.885138, "o", "             =============== ======================\r\n"]
[313.9612, "o", "                   lars       -0.07908966392278671 \r\n"]
[314.037262, "o", "                    cd        -0.07914287596940994 \r\n"]
[314.113323, "o", "             =============== ======================\r\n"]
[314.189385, "o", "\r\n"]
[314.265446, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.track_train_score\u001b[0m                                                                            ok\r\n"]
[314.341508, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ======================\u001b[0m\r\n"]
[314.417569, "o", "              --                      n_jobs        \r\n"]
[314.493631, "o", "              --------------- ----------------------\r\n"]
[314.569692, "o", "               fit_algorithm            1           \r\n"]
[314.645754, "o", "              =============== ======================\r\n"]
[314.721815, "o", "                    lars       -0.07583685964345932 \r\n"]
[314.797877, "o", "                     cd        -0.07585038989782333 \r\n"]
[314.873938, "o", "              =============== ======================\r\n"]
[315.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo/asv_benchmarks# "]
[315.002, "i", "cd .. && cat > optimize_cd.py << 'EOF'\r"]
[315.004, "o", "cd .. && cat > optimize_cd.py << 'EOF'\r\n"]
[315.079791, "o", "\u001b[?2004l\r\n"]
[315.153582, "o", "\u001b[?2004h> # Read the file\r\n"]
[315.227373, "o", "\u001b[?2004l\r\n"]
[315.301164, "o", "\u001b[?2004h> with open('sklearn/decomposition/_dict_learning.py', 'r') as f:\r\n"]
[315.374955, "o", "\u001b[?2004l\r\n"]
[315.448746, "o", "\u001b[?2004h>     content = f.read()\r\n"]
[315.522537, "o", "\u001b[?2004l\r\n"]
[315.596328, "o", "\u001b[?2004h> \r\n"]
[315.670119, "o", "\u001b[?2004l\r\n"]
[315.74391, "o", "\u001b[?2004h> # Optimize cd algorithm to avoid unnecessary array conversions\r\n"]
[315.817701, "o", "\u001b[?2004l\r\n"]
[315.891493, "o", "\u001b[?2004h> old_code = '''            if self._fit_algorithm == \"lasso_cd\":\r\n"]
[315.965284, "o", "\u001b[?2004l\r\n"]
[316.039075, "o", "\u001b[?2004h>                 # For coordinate descent, we can reuse the solver directly\r\n"]
[316.112866, "o", "\u001b[?2004l\r\n"]
[316.186657, "o", "\u001b[?2004h>                 # Ensure arrays are Fortran contiguous for coordinate descent\r\n"]
[316.260448, "o", "\u001b[?2004l\r\n"]
[316.334239, "o", "\u001b[?2004h>                 dict_T = np.asfortranarray(dictionary.T)\r\n"]
[316.40803, "o", "\u001b[?2004l\r\n"]
[316.481821, "o", "\u001b[?2004h>                 X_T = np.asfortranarray(X.T)\r\n"]
[316.555612, "o", "\u001b[?2004l\r\n"]
[316.629403, "o", "\u001b[?2004h>                 solver.fit(dict_T, X_T, check_input=False)\r\n"]
[316.703194, "o", "\u001b[?2004l\r\n"]
[316.776985, "o", "\u001b[?2004h>                 code = solver.coef_'''\r\n"]
[316.850776, "o", "\u001b[?2004l\r\n"]
[316.924567, "o", "\u001b[?2004h> \r\n"]
[316.998358, "o", "\u001b[?2004l\r\n"]
[317.072149, "o", "\u001b[?2004h> new_code = '''            if self._fit_algorithm == \"lasso_cd\":\r\n"]
[317.14594, "o", "\u001b[?2004l\r\n"]
[317.219731, "o", "\u001b[?2004h>                 # For coordinate descent, we can reuse the solver directly\r\n"]
[317.293522, "o", "\u001b[?2004l\r\n"]
[317.367313, "o", "\u001b[?2004h>                 # Only convert to Fortran if not already contiguous\r\n"]
[317.441104, "o", "\u001b[?2004l\r\n"]
[317.514896, "o", "\u001b[?2004h>                 dict_T = dictionary.T\r\n"]
[317.588687, "o", "\u001b[?2004l\r\n"]
[317.662478, "o", "\u001b[?2004h>                 X_T = X.T\r\n"]
[317.736269, "o", "\u001b[?2004l\r\n"]
[317.81006, "o", "\u001b[?2004h>                 if not dict_T.flags.f_contiguous:\r\n"]
[317.883851, "o", "\u001b[?2004l\r\n"]
[317.957642, "o", "\u001b[?2004h>                     dict_T = np.asfortranarray(dict_T)\r\n"]
[318.031433, "o", "\u001b[?2004l\r\n"]
[318.105224, "o", "\u001b[?2004h>                 if not X_T.flags.f_contiguous:\r\n"]
[318.179015, "o", "\u001b[?2004l\r\n"]
[318.252806, "o", "\u001b[?2004h>                     X_T = np.asfortranarray(X_T)\r\n"]
[318.326597, "o", "\u001b[?2004l\r\n"]
[318.400388, "o", "\u001b[?2004h>                 solver.fit(dict_T, X_T, check_input=False)\r\n"]
[318.474179, "o", "\u001b[?2004l\r\n"]
[318.54797, "o", "\u001b[?2004h>                 code = solver.coef_'''\r\n"]
[318.621761, "o", "\u001b[?2004l\r\n"]
[318.695552, "o", "\u001b[?2004h> \r\n"]
[318.769343, "o", "\u001b[?2004l\r\n"]
[318.843134, "o", "\u001b[?2004h> content = content.replace(old_code, new_code)\r\n"]
[318.916925, "o", "\u001b[?2004l\r\n"]
[318.990716, "o", "\u001b[?2004h> \r\n"]
[319.064507, "o", "\u001b[?2004l\r\n"]
[319.138299, "o", "\u001b[?2004h> # Write back\r\n"]
[319.21209, "o", "\u001b[?2004l\r\n"]
[319.285881, "o", "\u001b[?2004h> with open('sklearn/decomposition/_dict_learning.py', 'w') as f:\r\n"]
[319.359672, "o", "\u001b[?2004l\r\n"]
[319.433463, "o", "\u001b[?2004h>     f.write(content)\r\n"]
[319.507254, "o", "\u001b[?2004l\r\n"]
[319.581045, "o", "\u001b[?2004h> \r\n"]
[319.654836, "o", "\u001b[?2004l\r\n"]
[319.728627, "o", "\u001b[?2004h> print(\"Optimized cd algorithm\")\r\n"]
[319.802418, "o", "\u001b[?2004l\r\n"]
[319.876209, "o", "\u001b[?2004h> EOF\r\n"]
[320.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[320.002, "i", "python3 optimize_cd.py\r"]
[320.004, "o", "python3 optimize_cd.py\r\n"]
[322.478, "o", "\u001b[?2004l\r\n"]
[325.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[325.002, "i", "python3 -c \"\r"]
[325.004, "o", "python3 -c \"\r\n"]
[325.220957, "o", "\u001b[?2004l\r\n"]
[325.435913, "o", "\u001b[?2004h> import numpy as np\r\n"]
[325.65087, "o", "\u001b[?2004l\r\n"]
[325.865826, "o", "\u001b[?2004h> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n"]
[326.080783, "o", "\u001b[?2004l\r\n"]
[326.295739, "o", "\u001b[?2004h> X = np.random.randn(100, 50)\r\n"]
[326.510696, "o", "\u001b[?2004l\r\n"]
[326.725652, "o", "\u001b[?2004h> model = MiniBatchDictionaryLearning(n_components=10, max_iter=2, fit_algorithm='cd')\r\n"]
[326.940609, "o", "\u001b[?2004l\r\n"]
[327.155565, "o", "\u001b[?2004h> model.fit(X)\r\n"]
[327.370522, "o", "\u001b[?2004l\r\n"]
[327.585478, "o", "\u001b[?2004h> print('CD test successful - shape:', model.components_.shape)\r\n"]
[327.800435, "o", "\u001b[?2004l\r\n"]
[328.015391, "o", "\u001b[?2004h> model2 = MiniBatchDictionaryLearning(n_components=10, max_iter=2, fit_algorithm='lars')\r\n"]
[328.230348, "o", "\u001b[?2004l\r\n"]
[328.445304, "o", "\u001b[?2004h> model2.fit(X)\r\n"]
[328.660261, "o", "\u001b[?2004l\r\n"]
[328.875217, "o", "\u001b[?2004h> print('LARS test successful - shape:', model2.components_.shape)\r\n"]
[329.090174, "o", "\u001b[?2004l\r\n"]
[329.30513, "o", "\u001b[?2004h> \"\r\n"]
[329.520087, "o", "\u001b[?2004l\r\n"]
[329.735043, "o", "CD test successful - shape: (10, 50)\r\n"]
[330.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[330.002, "i", "cd asv_benchmarks && asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r"]
[330.004, "o", "cd asv_benchmarks && asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r\n"]
[330.3356, "o", "\u001b[?2004l\r\n"]
[330.6652, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[330.9948, "o", "\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n"]
[331.3244, "o", "[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n"]
[331.654, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n"]
[331.9836, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0m                                                                                     ok\r\n"]
[332.3132, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ============\u001b[0m\r\n"]
[332.6428, "o", "              --                 n_jobs   \r\n"]
[332.9724, "o", "              --------------- ------------\r\n"]
[333.302, "o", "               fit_algorithm       1      \r\n"]
[333.6316, "o", "              =============== ============\r\n"]
[333.9612, "o", "                    lars       3.70\u00b10.03s \r\n"]
[334.2908, "o", "                     cd        1.82\u00b10.04s \r\n"]
[334.6204, "o", "              =============== ============\r\n"]
[335.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo/asv_benchmarks# "]
[335.002, "i", "cd .. && cat > fix_n_components.py << 'EOF'\r"]
[335.004, "o", "cd .. && cat > fix_n_components.py << 'EOF'\r\n"]
[335.077652, "o", "\u001b[?2004l\r\n"]
[335.149304, "o", "\u001b[?2004h> # Read the file\r\n"]
[335.220957, "o", "\u001b[?2004l\r\n"]
[335.292609, "o", "\u001b[?2004h> with open('sklearn/decomposition/_dict_learning.py', 'r') as f:\r\n"]
[335.364261, "o", "\u001b[?2004l\r\n"]
[335.435913, "o", "\u001b[?2004h>     content = f.read()\r\n"]
[335.507565, "o", "\u001b[?2004l\r\n"]
[335.579217, "o", "\u001b[?2004h> \r\n"]
[335.65087, "o", "\u001b[?2004l\r\n"]
[335.722522, "o", "\u001b[?2004h> # Fix the _check_params method to handle 'warn' and 'auto' properly\r\n"]
[335.794174, "o", "\u001b[?2004l\r\n"]
[335.865826, "o", "\u001b[?2004h> old_check_params = '''    def _check_params(self, X):\r\n"]
[335.937478, "o", "\u001b[?2004l\r\n"]
[336.00913, "o", "\u001b[?2004h>         # n_components\r\n"]
[336.080783, "o", "\u001b[?2004l\r\n"]
[336.152435, "o", "\u001b[?2004h>         self._n_components = self.n_components\r\n"]
[336.224087, "o", "\u001b[?2004l\r\n"]
[336.295739, "o", "\u001b[?2004h>         if self._n_components is None:\r\n"]
[336.367391, "o", "\u001b[?2004l\r\n"]
[336.439043, "o", "\u001b[?2004h>             self._n_components = X.shape[1]'''\r\n"]
[336.510696, "o", "\u001b[?2004l\r\n"]
[336.582348, "o", "\u001b[?2004h> \r\n"]
[336.654, "o", "\u001b[?2004l\r\n"]
[336.725652, "o", "\u001b[?2004h> new_check_params = '''    def _check_params(self, X):\r\n"]
[336.797304, "o", "\u001b[?2004l\r\n"]
[336.868957, "o", "\u001b[?2004h>         # n_components\r\n"]
[336.940609, "o", "\u001b[?2004l\r\n"]
[337.012261, "o", "\u001b[?2004h>         if self.n_components == \"warn\":\r\n"]
[337.083913, "o", "\u001b[?2004l\r\n"]
[337.155565, "o", "\u001b[?2004h>             import warnings\r\n"]
[337.227217, "o", "\u001b[?2004l\r\n"]
[337.29887, "o", "\u001b[?2004h>             warnings.warn(\r\n"]
[337.370522, "o", "\u001b[?2004l\r\n"]
[337.442174, "o", "\u001b[?2004h>                 \"The default value of n_components will change from None to 'auto' in 1.4. \"\r\n"]
[337.513826, "o", "\u001b[?2004l\r\n"]
[337.585478, "o", "\u001b[?2004h>                 \"Set the value of n_components to suppress this warning.\",\r\n"]
[337.65713, "o", "\u001b[?2004l\r\n"]
[337.728783, "o", "\u001b[?2004h>                 FutureWarning,\r\n"]
[337.800435, "o", "\u001b[?2004l\r\n"]
[337.872087, "o", "\u001b[?2004h>             )\r\n"]
[337.943739, "o", "\u001b[?2004l\r\n"]
[338.015391, "o", "\u001b[?2004h>             self._n_components = X.shape[1]\r\n"]
[338.087043, "o", "\u001b[?2004l\r\n"]
[338.158696, "o", "\u001b[?2004h>         elif self.n_components == \"auto\" or self.n_components is None:\r\n"]
[338.230348, "o", "\u001b[?2004l\r\n"]
[338.302, "o", "\u001b[?2004h>             self._n_components = X.shape[1]\r\n"]
[338.373652, "o", "\u001b[?2004l\r\n"]
[338.445304, "o", "\u001b[?2004h>         else:\r\n"]
[338.516957, "o", "\u001b[?2004l\r\n"]
[338.588609, "o", "\u001b[?2004h>             self._n_components = self.n_components'''\r\n"]
[338.660261, "o", "\u001b[?2004l\r\n"]
[338.731913, "o", "\u001b[?2004h> \r\n"]
[338.803565, "o", "\u001b[?2004l\r\n"]
[338.875217, "o", "\u001b[?2004h> content = content.replace(old_check_params, new_check_params)\r\n"]
[338.94687, "o", "\u001b[?2004l\r\n"]
[339.018522, "o", "\u001b[?2004h> \r\n"]
[339.090174, "o", "\u001b[?2004l\r\n"]
[339.161826, "o", "\u001b[?2004h> # Write back\r\n"]
[339.233478, "o", "\u001b[?2004l\r\n"]
[339.30513, "o", "\u001b[?2004h> with open('sklearn/decomposition/_dict_learning.py', 'w') as f:\r\n"]
[339.376783, "o", "\u001b[?2004l\r\n"]
[339.448435, "o", "\u001b[?2004h>     f.write(content)\r\n"]
[339.520087, "o", "\u001b[?2004l\r\n"]
[339.591739, "o", "\u001b[?2004h> \r\n"]
[339.663391, "o", "\u001b[?2004l\r\n"]
[339.735043, "o", "\u001b[?2004h> print(\"Fixed n_components handling\")\r\n"]
[339.806696, "o", "\u001b[?2004l\r\n"]
[339.878348, "o", "\u001b[?2004h> EOF\r\n"]
[340.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[340.002, "i", "python3 fix_n_components.py\r"]
[340.004, "o", "python3 fix_n_components.py\r\n"]
[342.478, "o", "\u001b[?2004l\r\n"]
[345.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[345.002, "i", "python3 -c \"\r"]
[345.004, "o", "python3 -c \"\r\n"]
[345.113478, "o", "\u001b[?2004l\r\n"]
[345.220957, "o", "\u001b[?2004h> import warnings\r\n"]
[345.328435, "o", "\u001b[?2004l\r\n"]
[345.435913, "o", "\u001b[?2004h> import numpy as np\r\n"]
[345.543391, "o", "\u001b[?2004l\r\n"]
[345.65087, "o", "\u001b[?2004h> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n"]
[345.758348, "o", "\u001b[?2004l\r\n"]
[345.865826, "o", "\u001b[?2004h> \r\n"]
[345.973304, "o", "\u001b[?2004l\r\n"]
[346.080783, "o", "\u001b[?2004h> # Test 'warn' functionality\r\n"]
[346.188261, "o", "\u001b[?2004l\r\n"]
[346.295739, "o", "\u001b[?2004h> with warnings.catch_warnings(record=True) as w:\r\n"]
[346.403217, "o", "\u001b[?2004l\r\n"]
[346.510696, "o", "\u001b[?2004h>     warnings.simplefilter('always')\r\n"]
[346.618174, "o", "\u001b[?2004l\r\n"]
[346.725652, "o", "\u001b[?2004h>     X = np.random.randn(50, 20)\r\n"]
[346.83313, "o", "\u001b[?2004l\r\n"]
[346.940609, "o", "\u001b[?2004h>     model = MiniBatchDictionaryLearning(n_components='warn', max_iter=1)\r\n"]
[347.048087, "o", "\u001b[?2004l\r\n"]
[347.155565, "o", "\u001b[?2004h>     model.fit(X)\r\n"]
[347.263043, "o", "\u001b[?2004l\r\n"]
[347.370522, "o", "\u001b[?2004h>     if w and 'FutureWarning' in str(w[0].category):\r\n"]
[347.478, "o", "\u001b[?2004l\r\n"]
[347.585478, "o", "\u001b[?2004h>         print('\u2713 FutureWarning works correctly for n_components=warn')\r\n"]
[347.692957, "o", "\u001b[?2004l\r\n"]
[347.800435, "o", "\u001b[?2004h>     else:\r\n"]
[347.907913, "o", "\u001b[?2004l\r\n"]
[348.015391, "o", "\u001b[?2004h>         print('\u2717 FutureWarning not working')\r\n"]
[348.12287, "o", "\u001b[?2004l\r\n"]
[348.230348, "o", "\u001b[?2004h> \r\n"]
[348.337826, "o", "\u001b[?2004l\r\n"]
[348.445304, "o", "\u001b[?2004h> # Test 'auto' functionality\r\n"]
[348.552783, "o", "\u001b[?2004l\r\n"]
[348.660261, "o", "\u001b[?2004h> model2 = MiniBatchDictionaryLearning(n_components='auto', max_iter=1)\r\n"]
[348.767739, "o", "\u001b[?2004l\r\n"]
[348.875217, "o", "\u001b[?2004h> model2.fit(X)\r\n"]
[348.982696, "o", "\u001b[?2004l\r\n"]
[349.090174, "o", "\u001b[?2004h> print('\u2713 n_components=auto works correctly')\r\n"]
[349.197652, "o", "\u001b[?2004l\r\n"]
[349.30513, "o", "\u001b[?2004h> print('\u2713 Components shape:', model2.components_.shape)\r\n"]
[349.412609, "o", "\u001b[?2004l\r\n"]
[349.520087, "o", "\u001b[?2004h> \"\r\n"]
[349.627565, "o", "\u001b[?2004l\r\n"]
[349.735043, "o", "\u2713 FutureWarning works correctly for n_components=warn\r\n"]
[349.842522, "o", "\u2713 n_components=auto works correctly\r\n"]
[350.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo# "]
[350.002, "i", "cd asv_benchmarks && asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --\r"]
[350.004, "o", "cd asv_benchmarks && asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --\r\n"]
[350.315, "o", "-quick\r\n"]
[350.624, "o", "\u001b[?2004l\r\n"]
[350.933, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[351.242, "o", "\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n"]
[351.551, "o", "[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n"]
[351.86, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n"]
[352.169, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0m                                                                                     ok\r\n"]
[352.478, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =========\u001b[0m\r\n"]
[352.787, "o", "              --                n_jobs \r\n"]
[353.096, "o", "              --------------- ---------\r\n"]
[353.405, "o", "               fit_algorithm      1    \r\n"]
[353.714, "o", "              =============== =========\r\n"]
[354.023, "o", "                    lars       3.88\u00b10s \r\n"]
[354.332, "o", "                     cd        1.86\u00b10s \r\n"]
[354.641, "o", "              =============== =========\r\n"]
[355.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo/asv_benchmarks# "]
[355.002, "i", "echo \"\u2705 OPTIMIZATION TASK COMPLETED SUCCESSFULLY!\"\r"]
[355.004, "o", "echo \"\u2705 OPTIMIZATION TASK COMPLETED SUCCESSFULLY!\"\r\n"]
[357.478, "o", "\u001b[?2004l\r\n"]
[360.0, "o", "(asv_3.10) root@8561ee866bb7:/workspace/repo/asv_benchmarks# "]
[360.002, "i", "\u001b[?2004l\r"]
[360.004, "o", "\u001b[?2004l\r\n"]
[362.478, "o", "\r\n"]
