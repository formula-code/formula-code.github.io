{"version": 2, "width": 160, "height": 40, "timestamp": 1762942243, "env": {"SHELL": "/bin/bash", "TERM": "screen"}}
[0.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[0.002, "i", "asciinema rec --stdin /logs/agent.cast\r"]
[0.004, "o", "asciinema rec --stdin /logs/agent.cast\r\n"]
[0.006, "o", "\u001b[?2004l\r\n\u001b[0;31masciinema: /logs/agent.cast already exists, aborting\u001b[0m\r\n\u001b[0;31masciinema: use --overwrite option if you want to overwrite existing recording\u001b[0m\r\n\u001b[0;31masciinema: use --append option if you want to append to existing recording\u001b[0m\r\n"]
[5.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[5.002, "i", "clear\r"]
[5.004, "o", "clear\r\n"]
[5.006, "o", "\u001b[?2004l\r\n\u001b[H\u001b[J\r\n"]
[10.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[10.002, "i", "ls -la\r"]
[10.004, "o", "ls -la\r\n"]
[10.006, "o", "\u001b[?2004l\r\ntotal 212\r\ndrwxr-xr-x 1 root root  4096 Oct  1 17:33 \u001b[0m\u001b[01;34m.\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 15 17:16 \u001b[01;34m..\u001b[0m\r\ndrwxr-xr-x 2 root root  4096 Sep 13 19:22 \u001b[01;34m.circleci\u001b[0m\r\n-rw-r--r-- 1 root root  1021 Sep 13 19:22 .devcontainer.json\r\ndrwxr-xr-x 1 root root  4096 Oct  1 17:33 \u001b[01;34m.git\u001b[0m\r\n-rw-r--r-- 1 root root   229 Sep 13 19:22 .gitattributes\r\ndrwxr-xr-x 1 root root  4096 Sep 13 19:22 \u001b[01;34m.github\u001b[0m\r\n-rw-r--r-- 1 root root  1740 Sep 13 19:22 .gitignore\r\n-rw-r--r-- 1 root root  2322 Sep 13 19:22 .gitpod.yml\r\n-rw-r--r-- 1 root root   761 Sep 13 19:22 .libcst.codemod.yaml\r\n-rw-r--r-- 1 root root   109 Sep 13 19:22 .pep8speaks.yml\r\n-rw-r--r-- 1 root root 16092 Sep 13 19:22 .pre-commit-config.yaml\r\n-rw-r--r-- 1 root root  2284 Sep 13 19:22 AUTHORS.md\r\n-rw-r--r-- 1 root root   372 Sep 13 19:22 CITATION.cff\r\n-rw-r--r-- 1 root root   369 Sep 13 19:22 Dockerfile\r\n-rw-r--r-- 1 root root  1634 Sep 13 19:22 LICENSE\r\ndrwxr-xr-x 1 root root  4096 Sep 13 19:22 \u001b[01;34mLICENSES\u001b[0m\r\n-rw-r--r-- 1 root root  1228 Sep 13 19:22 MANIFEST.in\r\n-rw-r--r-- 1 root root 10387 Sep 13 19:22 README.md\r\n-rw-r--r-- 1 root root   246 Sep 13 19:22 RELEASE.md\r\ndrwxr-xr-x 1 root root  4096 Sep 13 19:22 \u001b[01;34masv_bench\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 13 19:22 \u001b[01;34mci\u001b[0m\r\n-rw-r--r-- 1 root root   256 Sep 13 19:20 codecov.yml\r\ndrwxr-xr-x 1 root root  4096 Sep 13 19:22 \u001b[01;34mdoc\u001b[0m\r\n-rw-r--r-- 1 root root  2112 Sep 13 19:22 environment.yml\r\ndrwxr-xr-x 1 root root  4096 Sep 13 19:22 \u001b[01;34mgitpod\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 13 22:00 \u001b[01;34mpandas\u001b[0m\r\ndrwxr-xr-x 2 root root  4096 Sep 13 21:46 \u001b[01;34mpandas.egg-info\u001b[0m\r\n-rw-r--r-- 1 root root 15351 Sep 13 19:22 pyproject.toml\r\n-rw-r--r-- 1 root root  4429 Sep 13 19:22 pyright_reportGeneralTypeIssues.json\r\n-rw-r--r-- 1 root root  1099 Sep 13 19:22 requirements-dev.txt\r\ndrwxr-xr-x 1 root root  4096 Sep 13 19:22 \u001b[01;34mscripts\u001b[0m\r\n-rw-r--r-- 1 root root  2095 Sep 13 19:22 setup.cfg\r\n-rwxr-xr-x 1 root root 21964 Sep 13 19:22 \u001b[01;32msetup.py\u001b[0m\r\ndrwxr-xr-x 2 root root  4096 Sep 13 19:20 \u001b[01;34mtypings\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 13 19:22 \u001b[01;34mweb\u001b[0m\r\n"]
[15.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[15.002, "i", "ls -la asv.conf.* || true\r"]
[15.004, "o", "ls -la asv.conf.* || true\r\n"]
[15.006, "o", "\u001b[?2004l\r\nls: cannot access 'asv.conf.*': No such file or directory\r\n"]
[20.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[20.002, "i", "grep -R \"series_methods\" -n . || true\r"]
[20.004, "o", "grep -R \"series_methods\" -n . || true\r\n"]
[20.006, "o", "\u001b[?2004l\r\n\u001b[35m\u001b[K./pandas/tests/resample/conftest.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K30\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kseries_methods\u001b[m\u001b[K = [\"nunique\"]\r\n\u001b[35m\u001b[K./pandas/tests/resample/conftest.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K31\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kresample_methods = downsample_methods + upsample_methods + \u001b[01;31m\u001b[Kseries_methods\u001b[m\u001b[K\r\ngrep: ./.git/objects/pack/pack-ece4d485a4f8b9fb76f32ecec0ce6e8268077cba.pack: binary file matches\r\ngrep: ./.git/index: binary file matches\r\n"]
[25.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[25.002, "i", "grep -R \"ToNumpy\" -n . || true\r"]
[25.004, "o", "grep -R \"ToNumpy\" -n . || true\r\n"]
[25.006, "o", "\u001b[?2004l\r\n\u001b[35m\u001b[K./asv_bench/benchmarks/frame_methods.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K231\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kclass \u001b[01;31m\u001b[KToNumpy\u001b[m\u001b[K:\r\n\u001b[35m\u001b[K./asv_bench/benchmarks/series_methods.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K385\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kclass \u001b[01;31m\u001b[KToNumpy\u001b[m\u001b[K:\r\n\u001b[35m\u001b[K./pandas/tests/frame/methods/test_to_numpy.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K12\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kclass Test\u001b[01;31m\u001b[KToNumpy\u001b[m\u001b[K:\r\n"]
[30.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[30.002, "i", "git status -s\r"]
[30.004, "o", "git status -s\r\n"]
[30.006, "o", "\u001b[?2004l\r\n"]
[35.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[35.002, "i", "asv run --python=same --bench=\"series_methods.ToNumpy.*\"\r"]
[35.004, "o", "asv run --python=same --bench=\"series_methods.ToNumpy.*\"\r\n"]
[35.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;31mNo `asv.conf` file found for valid extensions: ['.json', '.jsonc'].\u001b[0m\r\n"]
[40.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[40.002, "i", "ls -la asv_bench\r"]
[40.004, "o", "ls -la asv_bench\r\n"]
[40.006, "o", "\u001b[?2004l\r\ntotal 24\r\ndrwxr-xr-x 1 root root 4096 Sep 13 19:22 \u001b[0m\u001b[01;34m.\u001b[0m\r\ndrwxr-xr-x 1 root root 4096 Oct  1 17:33 \u001b[01;34m..\u001b[0m\r\n-rw-r--r-- 1 root root 5217 Sep 13 19:22 asv.conf.json\r\ndrwxr-xr-x 1 root root 4096 Sep 13 19:22 \u001b[01;34mbenchmarks\u001b[0m\r\n"]
[45.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[45.002, "i", "ls -la asv_bench/asv.conf.*\r"]
[45.004, "o", "ls -la asv_bench/asv.conf.*\r\n"]
[45.006, "o", "\u001b[?2004l\r\n-rw-r--r-- 1 root root 5217 Sep 13 19:22 asv_bench/asv.conf.json\r\n"]
[50.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[50.002, "i", "sed -n '1,160p' asv_bench/benchmarks/series_methods.py\r"]
[50.004, "o", "sed -n '1,160p' asv_bench/benchmarks/series_methods.py\r\n"]
[50.006, "o", "\u001b[?2004l\r\nfrom datetime import datetime\r\n\r\nimport numpy as np\r\n\r\nfrom pandas import (\r\n    NA,\r\n    Index,\r\n    NaT,\r\n    Series,\r\n    date_range,\r\n)\r\n\r\nfrom .pandas_vb_common import tm\r\n\r\n\r\nclass SeriesConstructor:\r\n    def setup(self):\r\n        self.idx = date_range(\r\n            start=datetime(2015, 10, 26), end=datetime(2016, 1, 1), freq=\"50s\"\r\n        )\r\n        self.data = dict(zip(self.idx, range(len(self.idx))))\r\n        self.array = np.array([1, 2, 3])\r\n        self.idx2 = Index([\"a\", \"b\", \"c\"])\r\n\r\n    def time_constructor_dict(self):\r\n        Series(data=self.data, index=self.idx)\r\n\r\n    def time_constructor_no_data(self):\r\n        Series(data=None, index=self.idx)\r\n\r\n    def time_constructor_fastpath(self):\r\n        Series(self.array, index=self.idx2, name=\"name\", fastpath=True)\r\n\r\n\r\nclass ToFrame:\r\n    params = [[\"int64\", \"datetime64[ns]\", \"category\", \"Int64\"], [None, \"foo\"]]\r\n    param_names = [\"dtype\", \"name\"]\r\n\r\n    def setup(self, dtype, name):\r\n        arr = np.arange(10**5)\r\n        ser = Series(arr, dtype=dtype)\r\n        self.ser = ser\r\n\r\n    def time_to_frame(self, dtype, name):\r\n        self.ser.to_frame(name)\r\n\r\n\r\nclass NSort:\r\n\r\n    params = [\"first\", \"last\", \"all\"]\r\n    param_names = [\"keep\"]\r\n\r\n    def setup(self, keep):\r\n        self.s = Series(np.random.randint(1, 10, 100000))\r\n\r\n    def time_nlargest(self, keep):\r\n        self.s.nlargest(3, keep=keep)\r\n\r\n    def time_nsmallest(self, keep):\r\n        self.s.nsmallest(3, keep=keep)\r\n\r\n\r\nclass Dropna:\r\n\r\n    params = [\"int\", \"datetime\"]\r\n    param_names = [\"dtype\"]\r\n\r\n    def setup(self, dtype):\r\n        N = 10**6\r\n        data = {\r\n            \"int\": np.random.randint(1, 10, N),\r\n            \"datetime\": date_range(\"2000-01-01\", freq=\"S\", periods=N),\r\n        }\r\n        self.s = Series(data[dtype])\r\n        if dtype == \"datetime\":\r\n            self.s[np.random.randint(1, N, 100)] = NaT\r\n\r\n    def time_dropna(self, dtype):\r\n        self.s.dropna()\r\n\r\n\r\nclass Fillna:\r\n\r\n    params = [\r\n        [\r\n            \"datetime64[ns]\",\r\n            \"float64\",\r\n            \"Float64\",\r\n            \"Int64\",\r\n            \"int64[pyarrow]\",\r\n            \"string\",\r\n            \"string[pyarrow]\",\r\n        ],\r\n        [None, \"pad\", \"backfill\"],\r\n    ]\r\n    param_names = [\"dtype\", \"method\"]\r\n\r\n    def setup(self, dtype, method):\r\n        N = 10**6\r\n        if dtype == \"datetime64[ns]\":\r\n            data = date_range(\"2000-01-01\", freq=\"S\", periods=N)\r\n            na_value = NaT\r\n        elif dtype in (\"float64\", \"Float64\"):\r\n            data = np.random.randn(N)\r\n            na_value = np.nan\r\n        elif dtype in (\"Int64\", \"int64[pyarrow]\"):\r\n            data = np.arange(N)\r\n            na_value = NA\r\n        elif dtype in (\"string\", \"string[pyarrow]\"):\r\n            data = tm.rands_array(5, N)\r\n            na_value = NA\r\n        else:\r\n            raise NotImplementedError\r\n        fill_value = data[0]\r\n        ser = Series(data, dtype=dtype)\r\n        ser[::2] = na_value\r\n        self.ser = ser\r\n        self.fill_value = fill_value\r\n\r\n    def time_fillna(self, dtype, method):\r\n        value = self.fill_value if method is None else None\r\n        self.ser.fillna(value=value, method=method)\r\n\r\n\r\nclass SearchSorted:\r\n\r\n    goal_time = 0.2\r\n    params = [\r\n        \"int8\",\r\n        \"int16\",\r\n        \"int32\",\r\n        \"int64\",\r\n        \"uint8\",\r\n        \"uint16\",\r\n        \"uint32\",\r\n        \"uint64\",\r\n        \"float16\",\r\n        \"float32\",\r\n        \"float64\",\r\n        \"str\",\r\n    ]\r\n    param_names = [\"dtype\"]\r\n\r\n    def setup(self, dtype):\r\n        N = 10**5\r\n        data = np.array([1] * N + [2] * N + [3] * N).astype(dtype)\r\n        self.s = Series(data)\r\n\r\n    def time_searchsorted(self, dtype):\r\n        key = \"2\" if dtype == \"str\" else 2\r\n        self.s.searchsorted(key)\r\n\r\n\r\nclass Map:\r\n\r\n    params = ([\"dict\", \"Series\", \"lambda\"], [\"object\", \"category\", \"int\"])\r\n    param_names = \"mapper\"\r\n\r\n    def setup(self, mapper, dtype):\r\n        map_size = 1000\r\n"]
[55.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[55.002, "i", "sed -n '160,500p' asv_bench/benchmarks/series_methods.py\r"]
[55.004, "o", "sed -n '160,500p' asv_bench/benchmarks/series_methods.py\r\n"]
[55.006, "o", "\u001b[?2004l\r\n        map_size = 1000\r\n        map_data = Series(map_size - np.arange(map_size), dtype=dtype)\r\n\r\n        # construct mapper\r\n        if mapper == \"Series\":\r\n            self.map_data = map_data\r\n        elif mapper == \"dict\":\r\n            self.map_data = map_data.to_dict()\r\n        elif mapper == \"lambda\":\r\n            map_dict = map_data.to_dict()\r\n            self.map_data = lambda x: map_dict[x]\r\n        else:\r\n            raise NotImplementedError\r\n\r\n        self.s = Series(np.random.randint(0, map_size, 10000), dtype=dtype)\r\n\r\n    def time_map(self, mapper, *args, **kwargs):\r\n        self.s.map(self.map_data)\r\n\r\n\r\nclass Clip:\r\n    params = [50, 1000, 10**5]\r\n    param_names = [\"n\"]\r\n\r\n    def setup(self, n):\r\n        self.s = Series(np.random.randn(n))\r\n\r\n    def time_clip(self, n):\r\n        self.s.clip(0, 1)\r\n\r\n\r\nclass ClipDt:\r\n    def setup(self):\r\n        dr = date_range(\"20220101\", periods=100_000, freq=\"s\", tz=\"UTC\")\r\n        self.clipper_dt = dr[0:1_000].repeat(100)\r\n        self.s = Series(dr)\r\n\r\n    def time_clip(self):\r\n        self.s.clip(upper=self.clipper_dt)\r\n\r\n\r\nclass ValueCounts:\r\n\r\n    params = [[10**3, 10**4, 10**5], [\"int\", \"uint\", \"float\", \"object\"]]\r\n    param_names = [\"N\", \"dtype\"]\r\n\r\n    def setup(self, N, dtype):\r\n        self.s = Series(np.random.randint(0, N, size=10 * N)).astype(dtype)\r\n\r\n    def time_value_counts(self, N, dtype):\r\n        self.s.value_counts()\r\n\r\n\r\nclass ValueCountsEA:\r\n\r\n    params = [[10**3, 10**4, 10**5], [True, False]]\r\n    param_names = [\"N\", \"dropna\"]\r\n\r\n    def setup(self, N, dropna):\r\n        self.s = Series(np.random.randint(0, N, size=10 * N), dtype=\"Int64\")\r\n        self.s.loc[1] = NA\r\n\r\n    def time_value_counts(self, N, dropna):\r\n        self.s.value_counts(dropna=dropna)\r\n\r\n\r\nclass ValueCountsObjectDropNAFalse:\r\n\r\n    params = [10**3, 10**4, 10**5]\r\n    param_names = [\"N\"]\r\n\r\n    def setup(self, N):\r\n        self.s = Series(np.random.randint(0, N, size=10 * N)).astype(\"object\")\r\n\r\n    def time_value_counts(self, N):\r\n        self.s.value_counts(dropna=False)\r\n\r\n\r\nclass Mode:\r\n\r\n    params = [[10**3, 10**4, 10**5], [\"int\", \"uint\", \"float\", \"object\"]]\r\n    param_names = [\"N\", \"dtype\"]\r\n\r\n    def setup(self, N, dtype):\r\n        self.s = Series(np.random.randint(0, N, size=10 * N)).astype(dtype)\r\n\r\n    def time_mode(self, N, dtype):\r\n        self.s.mode()\r\n\r\n\r\nclass ModeObjectDropNAFalse:\r\n\r\n    params = [10**3, 10**4, 10**5]\r\n    param_names = [\"N\"]\r\n\r\n    def setup(self, N):\r\n        self.s = Series(np.random.randint(0, N, size=10 * N)).astype(\"object\")\r\n\r\n    def time_mode(self, N):\r\n        self.s.mode(dropna=False)\r\n\r\n\r\nclass Dir:\r\n    def setup(self):\r\n        self.s = Series(index=tm.makeStringIndex(10000))\r\n\r\n    def time_dir_strings(self):\r\n        dir(self.s)\r\n\r\n\r\nclass SeriesGetattr:\r\n    # https://github.com/pandas-dev/pandas/issues/19764\r\n    def setup(self):\r\n        self.s = Series(1, index=date_range(\"2012-01-01\", freq=\"s\", periods=10**6))\r\n\r\n    def time_series_datetimeindex_repr(self):\r\n        getattr(self.s, \"a\", None)\r\n\r\n\r\nclass All:\r\n\r\n    params = [[10**3, 10**6], [\"fast\", \"slow\"], [\"bool\", \"boolean\"]]\r\n    param_names = [\"N\", \"case\", \"dtype\"]\r\n\r\n    def setup(self, N, case, dtype):\r\n        val = case != \"fast\"\r\n        self.s = Series([val] * N, dtype=dtype)\r\n\r\n    def time_all(self, N, case, dtype):\r\n        self.s.all()\r\n\r\n\r\nclass Any:\r\n\r\n    params = [[10**3, 10**6], [\"fast\", \"slow\"], [\"bool\", \"boolean\"]]\r\n    param_names = [\"N\", \"case\", \"dtype\"]\r\n\r\n    def setup(self, N, case, dtype):\r\n        val = case == \"fast\"\r\n        self.s = Series([val] * N, dtype=dtype)\r\n\r\n    def time_any(self, N, case, dtype):\r\n        self.s.any()\r\n\r\n\r\nclass NanOps:\r\n\r\n    params = [\r\n        [\r\n            \"var\",\r\n            \"mean\",\r\n            \"median\",\r\n            \"max\",\r\n            \"min\",\r\n            \"sum\",\r\n            \"std\",\r\n            \"sem\",\r\n            \"argmax\",\r\n            \"skew\",\r\n            \"kurt\",\r\n            \"prod\",\r\n        ],\r\n        [10**3, 10**6],\r\n        [\"int8\", \"int32\", \"int64\", \"float64\", \"Int64\", \"boolean\"],\r\n    ]\r\n    param_names = [\"func\", \"N\", \"dtype\"]\r\n\r\n    def setup(self, func, N, dtype):\r\n        if func == \"argmax\" and dtype in {\"Int64\", \"boolean\"}:\r\n            # Skip argmax for nullable int since this doesn't work yet (GH-24382)\r\n            raise NotImplementedError\r\n        self.s = Series([1] * N, dtype=dtype)\r\n        self.func = getattr(self.s, func)\r\n\r\n    def time_func(self, func, N, dtype):\r\n        self.func()\r\n\r\n\r\nclass Rank:\r\n\r\n    param_names = [\"dtype\"]\r\n    params = [\r\n        [\"int\", \"uint\", \"float\", \"object\"],\r\n    ]\r\n\r\n    def setup(self, dtype):\r\n        self.s = Series(np.random.randint(0, 1000, size=100000), dtype=dtype)\r\n\r\n    def time_rank(self, dtype):\r\n        self.s.rank()\r\n\r\n\r\nclass Iter:\r\n\r\n    param_names = [\"dtype\"]\r\n    params = [\r\n        \"bool\",\r\n        \"boolean\",\r\n        \"int64\",\r\n        \"Int64\",\r\n        \"float64\",\r\n        \"Float64\",\r\n        \"datetime64[ns]\",\r\n    ]\r\n\r\n    def setup(self, dtype):\r\n        N = 10**5\r\n        if dtype in [\"bool\", \"boolean\"]:\r\n            data = np.repeat([True, False], N // 2)\r\n        elif dtype in [\"int64\", \"Int64\"]:\r\n            data = np.arange(N)\r\n        elif dtype in [\"float64\", \"Float64\"]:\r\n            data = np.random.randn(N)\r\n        elif dtype == \"datetime64[ns]\":\r\n            data = date_range(\"2000-01-01\", freq=\"s\", periods=N)\r\n        else:\r\n            raise NotImplementedError\r\n\r\n        self.s = Series(data, dtype=dtype)\r\n\r\n    def time_iter(self, dtype):\r\n        for v in self.s:\r\n            pass\r\n\r\n\r\nclass ToNumpy:\r\n    def setup(self):\r\n        N = 1_000_000\r\n        self.ser = Series(\r\n            np.random.randn(\r\n                N,\r\n            )\r\n        )\r\n\r\n    def time_to_numpy(self):\r\n        self.ser.to_numpy()\r\n\r\n    def time_to_numpy_double_copy(self):\r\n        self.ser.to_numpy(dtype=\"float64\", copy=True)\r\n\r\n    def time_to_numpy_copy(self):\r\n        self.ser.to_numpy(copy=True)\r\n\r\n\r\nfrom .pandas_vb_common import setup  # noqa: F401 isort:skip\r\n"]
[60.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[60.002, "i", "asv run --config asv_bench/asv.conf.json --python=same --bench=\"series_methods.ToNumpy.*\"\r"]
[60.004, "o", "asv run --config asv_bench/asv.conf.json --python=same --bench=\"series_methods.ToNumpy.*\"\r\n"]
[60.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;31mConfig file asv_bench/asv.conf.json not found.\u001b[0m\r\n"]
[65.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[65.002, "i", "pwd\r"]
[65.004, "o", "pwd\r\n"]
[65.006, "o", "\u001b[?2004l\r\n/workspace/repo\r\n"]
[70.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[70.002, "i", "ls -l asv_bench/asv.conf.json\r"]
[70.004, "o", "ls -l asv_bench/asv.conf.json\r\n"]
[70.006, "o", "\u001b[?2004l\r\n-rw-r--r-- 1 root root 5217 Sep 13 19:22 asv_bench/asv.conf.json\r\n"]
[75.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[75.002, "i", "cd asv_bench\r"]
[75.004, "o", "cd asv_bench\r\n"]
[75.006, "o", "\u001b[?2004l\r\n"]
[80.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[80.002, "i", "asv run --python=same --bench=\"series_methods.ToNumpy.*\"\r"]
[80.004, "o", "asv run --python=same --bench=\"series_methods.ToNumpy.*\"\r\n"]
[80.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 3 total benchmarks (1 commits * 1 environments * 3 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.8_bin_python\u001b[0m\r\n[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39mRunning (series_methods.ToNumpy.time_to_numpy--)\u001b[0m...\r\n[66.67%] \u00b7\u00b7\u00b7 \u001b[0;39mseries_methods.ToNumpy.time_to_numpy\u001b[0m                                                                                                   1.30\u00b10.01\u03bcs\r\n[83.33%] \u00b7\u00b7\u00b7 \u001b[0;39mseries_methods.ToNumpy.time_to_numpy_copy\u001b[0m                                                                                                 391\u00b110\u03bcs\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mseries_methods.ToNumpy.time_to_numpy_double_copy\u001b[0m                                                                                           399\u00b17\u03bcs\r\n"]
[85.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[85.002, "i", "grep -R \"def to_numpy\" -n pandas/core | head -n 20\r"]
[85.004, "o", "grep -R \"def to_numpy\" -n pandas/core | head -n 20\r\n"]
[85.006, "o", "\u001b[?2004l\r\ngrep: pandas/core: No such file or directory\r\n"]
[90.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[90.002, "i", "sed -n '1,220p' pandas/core/series.py\r"]
[90.004, "o", "sed -n '1,220p' pandas/core/series.py\r\n"]
[90.006, "o", "\u001b[?2004l\r\nsed: can't read pandas/core/series.py: No such file or directory\r\n"]
[95.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[95.002, "i", "sed -n '220,560p' pandas/core/series.py\r"]
[95.004, "o", "sed -n '220,560p' pandas/core/series.py\r\n"]
[95.006, "o", "\u001b[?2004l\r\nsed: can't read pandas/core/series.py: No such file or directory\r\n"]
[100.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[100.002, "i", "sed -n '560,1200p' pandas/core/series.py\r"]
[100.004, "o", "sed -n '560,1200p' pandas/core/series.py\r\n"]
[100.006, "o", "\u001b[?2004l\r\nsed: can't read pandas/core/series.py: No such file or directory\r\n"]
[105.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[105.002, "i", "grep -R \"to_numpy\\(\" -n pandas/core | grep -v tests | head -n 20\r"]
[105.004, "o", "grep -R \"to_numpy\\(\" -n pandas/core | grep -v tests | head -n 20\r\n"]
[105.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[110.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[110.002, "i", "cd asv_bench\r"]
[110.004, "o", "cd asv_bench\r\n"]
[110.006, "o", "\u001b[?2004l\r\n-bash: cd: asv_bench: No such file or directory\r\n"]
[115.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[115.002, "i", "asv profile --python=same --config asv.conf.json series_methods.ToNumpy.time_to_numpy_copy\r"]
[115.004, "o", "asv profile --python=same --config asv.conf.json series_methods.ToNumpy.time_to_numpy_copy\r\n"]
[115.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mProfile data does not already exist. Running profiler now.\u001b[0m\r\n\u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.8_bin_python\u001b[0m\r\n\u00b7\u00b7\u00b7 \u001b[0;39mRunning (series_methods.ToNumpy.time_to_numpy_copy--)\u001b[0m.\r\n\u00b7\u00b7\u00b7 \u001b[0;39mseries_methods.ToNumpy.time_to_numpy_copy\u001b[0m                                                                                                360\u00b10.7\u03bcs\r\n\r\nWed Oct  1 17:40:19 2025    /tmp/tmpvnofqhsl\r\n\r\n         22 function calls in 0.000 seconds\r\n\r\n   Ordered by: cumulative time\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n        1    0.000    0.000    0.000    0.000 {built-in method builtins.exec}\r\n        1    0.000    0.000    0.000    0.000 _base.py:689(method_caller)\r\n        1    0.000    0.000    0.000    0.000 series_methods.py:400(time_to_numpy_copy)\r\n        1    0.000    0.000    0.000    0.000 base.py:424(to_numpy)\r\n        1    0.000    0.000    0.000    0.000 {method 'copy' of 'numpy.ndarray' objects}\r\n        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(shares_memory)\r\n        1    0.000    0.000    0.000    0.000 series.py:571(dtype)\r\n        1    0.000    0.000    0.000    0.000 managers.py:2036(dtype)\r\n        1    0.000    0.000    0.000    0.000 common.py:1346(is_extension_array_dtype)\r\n        1    0.000    0.000    0.000    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\r\n        2    0.000    0.000    0.000    0.000 series.py:683(_values)\r\n        1    0.000    0.000    0.000    0.000 {built-in method numpy.asarray}\r\n        1    0.000    0.000    0.000    0.000 managers.py:1979(_block)\r\n        2    0.000    0.000    0.000    0.000 managers.py:2047(internal_values)\r\n        2    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\r\n        1    0.000    0.000    0.000    0.000 blocks.py:463(dtype)\r\n        1    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\r\n        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\r\n        1    0.000    0.000    0.000    0.000 multiarray.py:1317(shares_memory)\r\n\r\n\r\n"]
[120.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[120.002, "i", "cd ..\r"]
[120.004, "o", "cd ..\r\n"]
[120.006, "o", "\u001b[?2004l\r\n"]
[125.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[125.002, "i", "grep -R \"def to_numpy\" -n pandas/core | head -n 50\r"]
[125.004, "o", "grep -R \"def to_numpy\" -n pandas/core | head -n 50\r\n"]
[125.006, "o", "\u001b[?2004l\r\npandas/core/frame.py:1758:    def to_numpy(\r\npandas/core/base.py:425:    def to_numpy(\r\npandas/core/arrays/masked.py:349:    def to_numpy(\r\npandas/core/arrays/numpy_.py:386:    def to_numpy(\r\npandas/core/arrays/arrow/array.py:842:    def to_numpy(\r\npandas/core/arrays/base.py:457:    def to_numpy(\r\n"]
[130.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[130.002, "i", "sed -n '360,520p' pandas/core/base.py\r"]
[130.004, "o", "sed -n '360,520p' pandas/core/base.py\r\n"]
[130.006, "o", "\u001b[?2004l\r\n    def array(self) -> ExtensionArray:\r\n        \"\"\"\r\n        The ExtensionArray of the data backing this Series or Index.\r\n\r\n        Returns\r\n        -------\r\n        ExtensionArray\r\n            An ExtensionArray of the values stored within. For extension\r\n            types, this is the actual array. For NumPy native types, this\r\n            is a thin (no copy) wrapper around :class:`numpy.ndarray`.\r\n\r\n            ``.array`` differs ``.values`` which may require converting the\r\n            data to a different form.\r\n\r\n        See Also\r\n        --------\r\n        Index.to_numpy : Similar method that always returns a NumPy array.\r\n        Series.to_numpy : Similar method that always returns a NumPy array.\r\n\r\n        Notes\r\n        -----\r\n        This table lays out the different array types for each extension\r\n        dtype within pandas.\r\n\r\n        ================== =============================\r\n        dtype              array type\r\n        ================== =============================\r\n        category           Categorical\r\n        period             PeriodArray\r\n        interval           IntervalArray\r\n        IntegerNA          IntegerArray\r\n        string             StringArray\r\n        boolean            BooleanArray\r\n        datetime64[ns, tz] DatetimeArray\r\n        ================== =============================\r\n\r\n        For any 3rd-party extension types, the array type will be an\r\n        ExtensionArray.\r\n\r\n        For all remaining dtypes ``.array`` will be a\r\n        :class:`arrays.NumpyExtensionArray` wrapping the actual ndarray\r\n        stored within. If you absolutely need a NumPy array (possibly with\r\n        copying / coercing data), then use :meth:`Series.to_numpy` instead.\r\n\r\n        Examples\r\n        --------\r\n        For regular NumPy types like int, and float, a PandasArray\r\n        is returned.\r\n\r\n        >>> pd.Series([1, 2, 3]).array\r\n        <PandasArray>\r\n        [1, 2, 3]\r\n        Length: 3, dtype: int64\r\n\r\n        For extension types, like Categorical, the actual ExtensionArray\r\n        is returned\r\n\r\n        >>> ser = pd.Series(pd.Categorical(['a', 'b', 'a']))\r\n        >>> ser.array\r\n        ['a', 'b', 'a']\r\n        Categories (2, object): ['a', 'b']\r\n        \"\"\"\r\n        raise AbstractMethodError(self)\r\n\r\n    @final\r\n    def to_numpy(\r\n        self,\r\n        dtype: npt.DTypeLike | None = None,\r\n        copy: bool = False,\r\n        na_value: object = lib.no_default,\r\n        **kwargs,\r\n    ) -> np.ndarray:\r\n        \"\"\"\r\n        A NumPy ndarray representing the values in this Series or Index.\r\n\r\n        Parameters\r\n        ----------\r\n        dtype : str or numpy.dtype, optional\r\n            The dtype to pass to :meth:`numpy.asarray`.\r\n        copy : bool, default False\r\n            Whether to ensure that the returned value is not a view on\r\n            another array. Note that ``copy=False`` does not *ensure* that\r\n            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\r\n            a copy is made, even if not strictly necessary.\r\n        na_value : Any, optional\r\n            The value to use for missing values. The default value depends\r\n            on `dtype` and the type of the array.\r\n\r\n            .. versionadded:: 1.0.0\r\n\r\n        **kwargs\r\n            Additional keywords passed through to the ``to_numpy`` method\r\n            of the underlying array (for extension arrays).\r\n\r\n            .. versionadded:: 1.0.0\r\n\r\n        Returns\r\n        -------\r\n        numpy.ndarray\r\n\r\n        See Also\r\n        --------\r\n        Series.array : Get the actual data stored within.\r\n        Index.array : Get the actual data stored within.\r\n        DataFrame.to_numpy : Similar method for DataFrame.\r\n\r\n        Notes\r\n        -----\r\n        The returned array will be the same up to equality (values equal\r\n        in `self` will be equal in the returned array; likewise for values\r\n        that are not equal). When `self` contains an ExtensionArray, the\r\n        dtype may be different. For example, for a category-dtype Series,\r\n        ``to_numpy()`` will return a NumPy array and the categorical dtype\r\n        will be lost.\r\n\r\n        For NumPy dtypes, this will be a reference to the actual data stored\r\n        in this Series or Index (assuming ``copy=False``). Modifying the result\r\n        in place will modify the data stored in the Series or Index (not that\r\n        we recommend doing that).\r\n\r\n        For extension types, ``to_numpy()`` *may* require copying data and\r\n        coercing the result to a NumPy type (possibly object), which may be\r\n        expensive. When you need a no-copy reference to the underlying data,\r\n        :attr:`Series.array` should be used instead.\r\n\r\n        This table lays out the different dtypes and default return types of\r\n        ``to_numpy()`` for various dtypes within pandas.\r\n\r\n        ================== ================================\r\n        dtype              array type\r\n        ================== ================================\r\n        category[T]        ndarray[T] (same dtype as input)\r\n        period             ndarray[object] (Periods)\r\n        interval           ndarray[object] (Intervals)\r\n        IntegerNA          ndarray[object]\r\n        datetime64[ns]     datetime64[ns]\r\n        datetime64[ns, tz] ndarray[object] (Timestamps)\r\n        ================== ================================\r\n\r\n        Examples\r\n        --------\r\n        >>> ser = pd.Series(pd.Categorical(['a', 'b', 'a']))\r\n        >>> ser.to_numpy()\r\n        array(['a', 'b', 'a'], dtype=object)\r\n\r\n        Specify the `dtype` to control how datetime-aware data is represented.\r\n        Use ``dtype=object`` to return an ndarray of pandas :class:`Timestamp`\r\n        objects, each with the correct ``tz``.\r\n\r\n        >>> ser = pd.Series(pd.date_range('2000', periods=2, tz=\"CET\"))\r\n        >>> ser.to_numpy(dtype=object)\r\n        array([Timestamp('2000-01-01 00:00:00+0100', tz='CET'),\r\n               Timestamp('2000-01-02 00:00:00+0100', tz='CET')],\r\n              dtype=object)\r\n\r\n        Or ``dtype='datetime64[ns]'`` to return an ndarray of native\r\n        datetime64 values. The values are converted to UTC and the timezone\r\n        info is dropped.\r\n\r\n        >>> ser.to_numpy(dtype=\"datetime64[ns]\")\r\n        ... # doctest: +ELLIPSIS\r\n"]
[135.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[135.002, "i", "sed -n '1,220p' pandas/core/series.py\r"]
[135.004, "o", "sed -n '1,220p' pandas/core/series.py\r\n"]
[135.006, "o", "\u001b[?2004l\r\n\"\"\"\r\nData structure for 1-dimensional cross-sectional and time series data\r\n\"\"\"\r\nfrom __future__ import annotations\r\n\r\nimport sys\r\nfrom textwrap import dedent\r\nfrom typing import (\r\n    IO,\r\n    TYPE_CHECKING,\r\n    Any,\r\n    Callable,\r\n    Hashable,\r\n    Iterable,\r\n    Literal,\r\n    Mapping,\r\n    Sequence,\r\n    Union,\r\n    cast,\r\n    overload,\r\n)\r\nimport weakref\r\n\r\nimport numpy as np\r\n\r\nfrom pandas._config import (\r\n    get_option,\r\n    using_copy_on_write,\r\n)\r\n\r\nfrom pandas._libs import (\r\n    lib,\r\n    properties,\r\n    reshape,\r\n)\r\nfrom pandas._libs.lib import (\r\n    is_range_indexer,\r\n    no_default,\r\n)\r\nfrom pandas._typing import (\r\n    AggFuncType,\r\n    AlignJoin,\r\n    AnyAll,\r\n    AnyArrayLike,\r\n    ArrayLike,\r\n    Axis,\r\n    AxisInt,\r\n    CorrelationMethod,\r\n    DropKeep,\r\n    Dtype,\r\n    DtypeObj,\r\n    FilePath,\r\n    FillnaOptions,\r\n    Frequency,\r\n    IgnoreRaise,\r\n    IndexKeyFunc,\r\n    IndexLabel,\r\n    Level,\r\n    NaPosition,\r\n    QuantileInterpolation,\r\n    Renamer,\r\n    SingleManager,\r\n    SortKind,\r\n    StorageOptions,\r\n    TimedeltaConvertibleTypes,\r\n    TimestampConvertibleTypes,\r\n    ValueKeyFunc,\r\n    WriteBuffer,\r\n    npt,\r\n)\r\nfrom pandas.compat import PYPY\r\nfrom pandas.compat.numpy import function as nv\r\nfrom pandas.errors import (\r\n    ChainedAssignmentError,\r\n    InvalidIndexError,\r\n    _chained_assignment_msg,\r\n)\r\nfrom pandas.util._decorators import (\r\n    Appender,\r\n    Substitution,\r\n    doc,\r\n)\r\nfrom pandas.util._validators import (\r\n    validate_ascending,\r\n    validate_bool_kwarg,\r\n    validate_percentile,\r\n)\r\n\r\nfrom pandas.core.dtypes.cast import (\r\n    LossySetitemError,\r\n    convert_dtypes,\r\n    maybe_box_native,\r\n    maybe_cast_pointwise_result,\r\n)\r\nfrom pandas.core.dtypes.common import (\r\n    ensure_platform_int,\r\n    is_dict_like,\r\n    is_extension_array_dtype,\r\n    is_integer,\r\n    is_iterator,\r\n    is_list_like,\r\n    is_numeric_dtype,\r\n    is_object_dtype,\r\n    is_scalar,\r\n    pandas_dtype,\r\n    validate_all_hashable,\r\n)\r\nfrom pandas.core.dtypes.generic import ABCDataFrame\r\nfrom pandas.core.dtypes.inference import is_hashable\r\nfrom pandas.core.dtypes.missing import (\r\n    isna,\r\n    na_value_for_dtype,\r\n    notna,\r\n    remove_na_arraylike,\r\n)\r\n\r\nfrom pandas.core import (\r\n    algorithms,\r\n    base,\r\n    common as com,\r\n    missing,\r\n    nanops,\r\n    ops,\r\n)\r\nfrom pandas.core.accessor import CachedAccessor\r\nfrom pandas.core.apply import SeriesApply\r\nfrom pandas.core.arrays import ExtensionArray\r\nfrom pandas.core.arrays.categorical import CategoricalAccessor\r\nfrom pandas.core.arrays.sparse import SparseAccessor\r\nfrom pandas.core.construction import (\r\n    extract_array,\r\n    sanitize_array,\r\n)\r\nfrom pandas.core.generic import NDFrame\r\nfrom pandas.core.indexers import (\r\n    disallow_ndim_indexing,\r\n    unpack_1tuple,\r\n)\r\nfrom pandas.core.indexes.accessors import CombinedDatetimelikeProperties\r\nfrom pandas.core.indexes.api import (\r\n    DatetimeIndex,\r\n    Index,\r\n    MultiIndex,\r\n    PeriodIndex,\r\n    default_index,\r\n    ensure_index,\r\n)\r\nimport pandas.core.indexes.base as ibase\r\nfrom pandas.core.indexes.multi import maybe_droplevels\r\nfrom pandas.core.indexing import (\r\n    check_bool_indexer,\r\n    check_dict_or_set_indexers,\r\n)\r\nfrom pandas.core.internals import (\r\n    SingleArrayManager,\r\n    SingleBlockManager,\r\n)\r\nfrom pandas.core.shared_docs import _shared_docs\r\nfrom pandas.core.sorting import (\r\n    ensure_key_mapped,\r\n    nargsort,\r\n)\r\nfrom pandas.core.strings import StringMethods\r\nfrom pandas.core.tools.datetimes import to_datetime\r\n\r\nimport pandas.io.formats.format as fmt\r\nfrom pandas.io.formats.info import (\r\n    INFO_DOCSTRING,\r\n    SeriesInfo,\r\n    series_sub_kwargs,\r\n)\r\nimport pandas.plotting\r\n\r\nif TYPE_CHECKING:\r\n    from pandas._typing import (\r\n        NumpySorter,\r\n        NumpyValueArrayLike,\r\n        Suffixes,\r\n    )\r\n\r\n    from pandas.core.frame import DataFrame\r\n    from pandas.core.groupby.generic import SeriesGroupBy\r\n    from pandas.core.resample import Resampler\r\n\r\n__all__ = [\"Series\"]\r\n\r\n_shared_doc_kwargs = {\r\n    \"axes\": \"index\",\r\n    \"klass\": \"Series\",\r\n    \"axes_single_arg\": \"{0 or 'index'}\",\r\n    \"axis\": \"\"\"axis : {0 or 'index'}\r\n        Unused. Parameter needed for compatibility with DataFrame.\"\"\",\r\n    \"inplace\": \"\"\"inplace : bool, default False\r\n        If True, performs operation inplace and returns None.\"\"\",\r\n    \"unique\": \"np.ndarray\",\r\n    \"duplicated\": \"Series\",\r\n    \"optional_by\": \"\",\r\n    \"optional_mapper\": \"\",\r\n    \"optional_labels\": \"\",\r\n    \"optional_axis\": \"\",\r\n    \"replace_iloc\": \"\"\"\r\n    This differs from updating with ``.loc`` or ``.iloc``, which require\r\n    you to specify a location to update with some value.\"\"\",\r\n}\r\n\r\n\r\ndef _coerce_method(converter):\r\n    \"\"\"\r\n    Install the scalar coercion methods.\r\n    \"\"\"\r\n\r\n    def wrapper(self):\r\n        if len(self) == 1:\r\n            return converter(self.iloc[0])\r\n        raise TypeError(f\"cannot convert the series to {converter}\")\r\n\r\n    wrapper.__name__ = f\"__{converter.__name__}__\"\r\n    return wrapper\r\n\r\n\r\n"]
[140.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[140.002, "i", "sed -n '220,760p' pandas/core/series.py\r"]
[140.004, "o", "sed -n '220,760p' pandas/core/series.py\r\n"]
[140.006, "o", "\u001b[?2004l\r\n\r\n# ----------------------------------------------------------------------\r\n# Series class\r\n\r\n# error: Definition of \"max\" in base class \"IndexOpsMixin\" is incompatible with\r\n# definition in base class \"NDFrame\"\r\n# error: Definition of \"min\" in base class \"IndexOpsMixin\" is incompatible with\r\n# definition in base class \"NDFrame\"\r\nclass Series(base.IndexOpsMixin, NDFrame):  # type: ignore[misc]\r\n    \"\"\"\r\n    One-dimensional ndarray with axis labels (including time series).\r\n\r\n    Labels need not be unique but must be a hashable type. The object\r\n    supports both integer- and label-based indexing and provides a host of\r\n    methods for performing operations involving the index. Statistical\r\n    methods from ndarray have been overridden to automatically exclude\r\n    missing data (currently represented as NaN).\r\n\r\n    Operations between Series (+, -, /, \\\\*, \\\\*\\\\*) align values based on their\r\n    associated index values-- they need not be the same length. The result\r\n    index will be the sorted union of the two indexes.\r\n\r\n    Parameters\r\n    ----------\r\n    data : array-like, Iterable, dict, or scalar value\r\n        Contains data stored in Series. If data is a dict, argument order is\r\n        maintained.\r\n    index : array-like or Index (1d)\r\n        Values must be hashable and have the same length as `data`.\r\n        Non-unique index values are allowed. Will default to\r\n        RangeIndex (0, 1, 2, ..., n) if not provided. If data is dict-like\r\n        and index is None, then the keys in the data are used as the index. If the\r\n        index is not None, the resulting Series is reindexed with the index values.\r\n    dtype : str, numpy.dtype, or ExtensionDtype, optional\r\n        Data type for the output Series. If not specified, this will be\r\n        inferred from `data`.\r\n        See the :ref:`user guide <basics.dtypes>` for more usages.\r\n    name : str, optional\r\n        The name to give to the Series.\r\n    copy : bool, default False\r\n        Copy input data. Only affects Series or 1d ndarray input. See examples.\r\n\r\n    Notes\r\n    -----\r\n    Please reference the :ref:`User Guide <basics.series>` for more information.\r\n\r\n    Examples\r\n    --------\r\n    Constructing Series from a dictionary with an Index specified\r\n\r\n    >>> d = {'a': 1, 'b': 2, 'c': 3}\r\n    >>> ser = pd.Series(data=d, index=['a', 'b', 'c'])\r\n    >>> ser\r\n    a   1\r\n    b   2\r\n    c   3\r\n    dtype: int64\r\n\r\n    The keys of the dictionary match with the Index values, hence the Index\r\n    values have no effect.\r\n\r\n    >>> d = {'a': 1, 'b': 2, 'c': 3}\r\n    >>> ser = pd.Series(data=d, index=['x', 'y', 'z'])\r\n    >>> ser\r\n    x   NaN\r\n    y   NaN\r\n    z   NaN\r\n    dtype: float64\r\n\r\n    Note that the Index is first build with the keys from the dictionary.\r\n    After this the Series is reindexed with the given Index values, hence we\r\n    get all NaN as a result.\r\n\r\n    Constructing Series from a list with `copy=False`.\r\n\r\n    >>> r = [1, 2]\r\n    >>> ser = pd.Series(r, copy=False)\r\n    >>> ser.iloc[0] = 999\r\n    >>> r\r\n    [1, 2]\r\n    >>> ser\r\n    0    999\r\n    1      2\r\n    dtype: int64\r\n\r\n    Due to input data type the Series has a `copy` of\r\n    the original data even though `copy=False`, so\r\n    the data is unchanged.\r\n\r\n    Constructing Series from a 1d ndarray with `copy=False`.\r\n\r\n    >>> r = np.array([1, 2])\r\n    >>> ser = pd.Series(r, copy=False)\r\n    >>> ser.iloc[0] = 999\r\n    >>> r\r\n    array([999,   2])\r\n    >>> ser\r\n    0    999\r\n    1      2\r\n    dtype: int64\r\n\r\n    Due to input data type the Series has a `view` on\r\n    the original data, so\r\n    the data is changed as well.\r\n    \"\"\"\r\n\r\n    _typ = \"series\"\r\n    _HANDLED_TYPES = (Index, ExtensionArray, np.ndarray)\r\n\r\n    _name: Hashable\r\n    _metadata: list[str] = [\"name\"]\r\n    _internal_names_set = {\"index\"} | NDFrame._internal_names_set\r\n    _accessors = {\"dt\", \"cat\", \"str\", \"sparse\"}\r\n    _hidden_attrs = (\r\n        base.IndexOpsMixin._hidden_attrs\r\n        | NDFrame._hidden_attrs\r\n        | frozenset([\"compress\", \"ptp\"])\r\n    )\r\n\r\n    # Override cache_readonly bc Series is mutable\r\n    # error: Incompatible types in assignment (expression has type \"property\",\r\n    # base class \"IndexOpsMixin\" defined the type as \"Callable[[IndexOpsMixin], bool]\")\r\n    hasnans = property(  # type: ignore[assignment]\r\n        # error: \"Callable[[IndexOpsMixin], bool]\" has no attribute \"fget\"\r\n        base.IndexOpsMixin.hasnans.fget,  # type: ignore[attr-defined]\r\n        doc=base.IndexOpsMixin.hasnans.__doc__,\r\n    )\r\n    _mgr: SingleManager\r\n    div: Callable[[Series, Any], Series]\r\n    rdiv: Callable[[Series, Any], Series]\r\n\r\n    # ----------------------------------------------------------------------\r\n    # Constructors\r\n\r\n    def __init__(\r\n        self,\r\n        data=None,\r\n        index=None,\r\n        dtype: Dtype | None = None,\r\n        name=None,\r\n        copy: bool = False,\r\n        fastpath: bool = False,\r\n    ) -> None:\r\n\r\n        if (\r\n            isinstance(data, (SingleBlockManager, SingleArrayManager))\r\n            and index is None\r\n            and dtype is None\r\n            and copy is False\r\n        ):\r\n            # GH#33357 called with just the SingleBlockManager\r\n            NDFrame.__init__(self, data)\r\n            if fastpath:\r\n                # e.g. from _box_col_values, skip validation of name\r\n                object.__setattr__(self, \"_name\", name)\r\n            else:\r\n                self.name = name\r\n            return\r\n\r\n        # we are called internally, so short-circuit\r\n        if fastpath:\r\n            # data is a ndarray, index is defined\r\n            if not isinstance(data, (SingleBlockManager, SingleArrayManager)):\r\n                manager = get_option(\"mode.data_manager\")\r\n                if manager == \"block\":\r\n                    data = SingleBlockManager.from_array(data, index)\r\n                elif manager == \"array\":\r\n                    data = SingleArrayManager.from_array(data, index)\r\n            if copy:\r\n                data = data.copy()\r\n            # skips validation of the name\r\n            object.__setattr__(self, \"_name\", name)\r\n            NDFrame.__init__(self, data)\r\n            return\r\n\r\n        name = ibase.maybe_extract_name(name, data, type(self))\r\n\r\n        if index is not None:\r\n            index = ensure_index(index)\r\n\r\n        if dtype is not None:\r\n            dtype = self._validate_dtype(dtype)\r\n\r\n        if data is None:\r\n            index = index if index is not None else default_index(0)\r\n            if len(index) or dtype is not None:\r\n                data = na_value_for_dtype(pandas_dtype(dtype), compat=False)\r\n            else:\r\n                data = []\r\n\r\n        if isinstance(data, MultiIndex):\r\n            raise NotImplementedError(\r\n                \"initializing a Series from a MultiIndex is not supported\"\r\n            )\r\n        if isinstance(data, Index):\r\n\r\n            if dtype is not None:\r\n                # astype copies\r\n                data = data.astype(dtype)\r\n            else:\r\n                # GH#24096 we need to ensure the index remains immutable\r\n                data = data._values.copy()\r\n            copy = False\r\n\r\n        elif isinstance(data, np.ndarray):\r\n            if len(data.dtype):\r\n                # GH#13296 we are dealing with a compound dtype, which\r\n                #  should be treated as 2D\r\n                raise ValueError(\r\n                    \"Cannot construct a Series from an ndarray with \"\r\n                    \"compound dtype.  Use DataFrame instead.\"\r\n                )\r\n        elif isinstance(data, Series):\r\n            if index is None:\r\n                index = data.index\r\n                if using_copy_on_write():\r\n                    data = data._mgr.copy(deep=False)\r\n                else:\r\n                    data = data._mgr\r\n            else:\r\n                data = data.reindex(index, copy=copy)\r\n                copy = False\r\n                data = data._mgr\r\n        elif is_dict_like(data):\r\n            data, index = self._init_dict(data, index, dtype)\r\n            dtype = None\r\n            copy = False\r\n        elif isinstance(data, (SingleBlockManager, SingleArrayManager)):\r\n            if index is None:\r\n                index = data.index\r\n            elif not data.index.equals(index) or copy:\r\n                # GH#19275 SingleBlockManager input should only be called\r\n                # internally\r\n                raise AssertionError(\r\n                    \"Cannot pass both SingleBlockManager \"\r\n                    \"`data` argument and a different \"\r\n                    \"`index` argument. `copy` must be False.\"\r\n                )\r\n\r\n        elif isinstance(data, ExtensionArray):\r\n            pass\r\n        else:\r\n            data = com.maybe_iterable_to_list(data)\r\n            if is_list_like(data) and not len(data) and dtype is None:\r\n                # GH 29405: Pre-2.0, this defaulted to float.\r\n                dtype = np.dtype(object)\r\n\r\n        if index is None:\r\n            if not is_list_like(data):\r\n                data = [data]\r\n            index = default_index(len(data))\r\n        elif is_list_like(data):\r\n            com.require_length_match(data, index)\r\n\r\n        # create/copy the manager\r\n        if isinstance(data, (SingleBlockManager, SingleArrayManager)):\r\n            if dtype is not None:\r\n                data = data.astype(dtype=dtype, errors=\"ignore\", copy=copy)\r\n            elif copy:\r\n                data = data.copy()\r\n        else:\r\n            data = sanitize_array(data, index, dtype, copy)\r\n\r\n            manager = get_option(\"mode.data_manager\")\r\n            if manager == \"block\":\r\n                data = SingleBlockManager.from_array(data, index)\r\n            elif manager == \"array\":\r\n                data = SingleArrayManager.from_array(data, index)\r\n\r\n        NDFrame.__init__(self, data)\r\n        self.name = name\r\n        self._set_axis(0, index)\r\n\r\n    def _init_dict(\r\n        self, data, index: Index | None = None, dtype: DtypeObj | None = None\r\n    ):\r\n        \"\"\"\r\n        Derive the \"_mgr\" and \"index\" attributes of a new Series from a\r\n        dictionary input.\r\n\r\n        Parameters\r\n        ----------\r\n        data : dict or dict-like\r\n            Data used to populate the new Series.\r\n        index : Index or None, default None\r\n            Index for the new Series: if None, use dict keys.\r\n        dtype : np.dtype, ExtensionDtype, or None, default None\r\n            The dtype for the new Series: if None, infer from data.\r\n\r\n        Returns\r\n        -------\r\n        _data : BlockManager for the new Series\r\n        index : index for the new Series\r\n        \"\"\"\r\n        keys: Index | tuple\r\n\r\n        # Looking for NaN in dict doesn't work ({np.nan : 1}[float('nan')]\r\n        # raises KeyError), so we iterate the entire dict, and align\r\n        if data:\r\n            # GH:34717, issue was using zip to extract key and values from data.\r\n            # using generators in effects the performance.\r\n            # Below is the new way of extracting the keys and values\r\n\r\n            keys = tuple(data.keys())\r\n            values = list(data.values())  # Generating list of values- faster way\r\n        elif index is not None:\r\n            # fastpath for Series(data=None). Just use broadcasting a scalar\r\n            # instead of reindexing.\r\n            if len(index) or dtype is not None:\r\n                values = na_value_for_dtype(pandas_dtype(dtype), compat=False)\r\n            else:\r\n                values = []\r\n            keys = index\r\n        else:\r\n            keys, values = (), []\r\n\r\n        # Input is now list-like, so rely on \"standard\" construction:\r\n\r\n        s = self._constructor(\r\n            values,\r\n            index=keys,\r\n            dtype=dtype,\r\n        )\r\n\r\n        # Now we just make sure the order is respected, if any\r\n        if data and index is not None:\r\n            s = s.reindex(index, copy=False)\r\n        return s._mgr, s.index\r\n\r\n    # ----------------------------------------------------------------------\r\n\r\n    @property\r\n    def _constructor(self) -> Callable[..., Series]:\r\n        return Series\r\n\r\n    @property\r\n    def _constructor_expanddim(self) -> Callable[..., DataFrame]:\r\n        \"\"\"\r\n        Used when a manipulation result has one higher dimension as the\r\n        original, such as Series.to_frame()\r\n        \"\"\"\r\n        from pandas.core.frame import DataFrame\r\n\r\n        return DataFrame\r\n\r\n    # types\r\n    @property\r\n    def _can_hold_na(self) -> bool:\r\n        return self._mgr._can_hold_na\r\n\r\n    # ndarray compatibility\r\n    @property\r\n    def dtype(self) -> DtypeObj:\r\n        \"\"\"\r\n        Return the dtype object of the underlying data.\r\n        \"\"\"\r\n        return self._mgr.dtype\r\n\r\n    @property\r\n    def dtypes(self) -> DtypeObj:\r\n        \"\"\"\r\n        Return the dtype object of the underlying data.\r\n        \"\"\"\r\n        # DataFrame compatibility\r\n        return self.dtype\r\n\r\n    @property\r\n    def name(self) -> Hashable:\r\n        \"\"\"\r\n        Return the name of the Series.\r\n\r\n        The name of a Series becomes its index or column name if it is used\r\n        to form a DataFrame. It is also used whenever displaying the Series\r\n        using the interpreter.\r\n\r\n        Returns\r\n        -------\r\n        label (hashable object)\r\n            The name of the Series, also the column name if part of a DataFrame.\r\n\r\n        See Also\r\n        --------\r\n        Series.rename : Sets the Series name when given a scalar input.\r\n        Index.name : Corresponding Index property.\r\n\r\n        Examples\r\n        --------\r\n        The Series name can be set initially when calling the constructor.\r\n\r\n        >>> s = pd.Series([1, 2, 3], dtype=np.int64, name='Numbers')\r\n        >>> s\r\n        0    1\r\n        1    2\r\n        2    3\r\n        Name: Numbers, dtype: int64\r\n        >>> s.name = \"Integers\"\r\n        >>> s\r\n        0    1\r\n        1    2\r\n        2    3\r\n        Name: Integers, dtype: int64\r\n\r\n        The name of a Series within a DataFrame is its column name.\r\n\r\n        >>> df = pd.DataFrame([[1, 2], [3, 4], [5, 6]],\r\n        ...                   columns=[\"Odd Numbers\", \"Even Numbers\"])\r\n        >>> df\r\n           Odd Numbers  Even Numbers\r\n        0            1             2\r\n        1            3             4\r\n        2            5             6\r\n        >>> df[\"Even Numbers\"].name\r\n        'Even Numbers'\r\n        \"\"\"\r\n        return self._name\r\n\r\n    @name.setter\r\n    def name(self, value: Hashable) -> None:\r\n        validate_all_hashable(value, error_name=f\"{type(self).__name__}.name\")\r\n        object.__setattr__(self, \"_name\", value)\r\n\r\n    @property\r\n    def values(self):\r\n        \"\"\"\r\n        Return Series as ndarray or ndarray-like depending on the dtype.\r\n\r\n        .. warning::\r\n\r\n           We recommend using :attr:`Series.array` or\r\n           :meth:`Series.to_numpy`, depending on whether you need\r\n           a reference to the underlying data or a NumPy array.\r\n\r\n        Returns\r\n        -------\r\n        numpy.ndarray or ndarray-like\r\n\r\n        See Also\r\n        --------\r\n        Series.array : Reference to the underlying data.\r\n        Series.to_numpy : A NumPy array representing the underlying data.\r\n\r\n        Examples\r\n        --------\r\n        >>> pd.Series([1, 2, 3]).values\r\n        array([1, 2, 3])\r\n\r\n        >>> pd.Series(list('aabc')).values\r\n        array(['a', 'a', 'b', 'c'], dtype=object)\r\n\r\n        >>> pd.Series(list('aabc')).astype('category').values\r\n        ['a', 'a', 'b', 'c']\r\n        Categories (3, object): ['a', 'b', 'c']\r\n\r\n        Timezone aware datetime data is converted to UTC:\r\n\r\n        >>> pd.Series(pd.date_range('20130101', periods=3,\r\n        ...                         tz='US/Eastern')).values\r\n        array(['2013-01-01T05:00:00.000000000',\r\n               '2013-01-02T05:00:00.000000000',\r\n               '2013-01-03T05:00:00.000000000'], dtype='datetime64[ns]')\r\n        \"\"\"\r\n        return self._mgr.external_values()\r\n\r\n    @property\r\n    def _values(self):\r\n        \"\"\"\r\n        Return the internal repr of this data (defined by Block.interval_values).\r\n        This are the values as stored in the Block (ndarray or ExtensionArray\r\n        depending on the Block class), with datetime64[ns] and timedelta64[ns]\r\n        wrapped in ExtensionArrays to match Index._values behavior.\r\n\r\n        Differs from the public ``.values`` for certain data types, because of\r\n        historical backwards compatibility of the public attribute (e.g. period\r\n        returns object ndarray and datetimetz a datetime64[ns] ndarray for\r\n        ``.values`` while it returns an ExtensionArray for ``._values`` in those\r\n        cases).\r\n\r\n        Differs from ``.array`` in that this still returns the numpy array if\r\n        the Block is backed by a numpy array (except for datetime64 and\r\n        timedelta64 dtypes), while ``.array`` ensures to always return an\r\n        ExtensionArray.\r\n\r\n        Overview:\r\n\r\n        dtype       | values        | _values       | array         |\r\n        ----------- | ------------- | ------------- | ------------- |\r\n        Numeric     | ndarray       | ndarray       | PandasArray   |\r\n        Category    | Categorical   | Categorical   | Categorical   |\r\n        dt64[ns]    | ndarray[M8ns] | DatetimeArray | DatetimeArray |\r\n        dt64[ns tz] | ndarray[M8ns] | DatetimeArray | DatetimeArray |\r\n        td64[ns]    | ndarray[m8ns] | TimedeltaArray| ndarray[m8ns] |\r\n        Period      | ndarray[obj]  | PeriodArray   | PeriodArray   |\r\n        Nullable    | EA            | EA            | EA            |\r\n\r\n        \"\"\"\r\n        return self._mgr.internal_values()\r\n\r\n    # error: Decorated property not supported\r\n    @Appender(base.IndexOpsMixin.array.__doc__)  # type: ignore[misc]\r\n    @property\r\n    def array(self) -> ExtensionArray:\r\n        return self._mgr.array_values()\r\n\r\n    # ops\r\n    def ravel(self, order: str = \"C\") -> ArrayLike:\r\n        \"\"\"\r\n        Return the flattened underlying data as an ndarray or ExtensionArray.\r\n\r\n        Returns\r\n        -------\r\n        numpy.ndarray or ExtensionArray\r\n            Flattened data of the Series.\r\n\r\n        See Also\r\n        --------\r\n        numpy.ndarray.ravel : Return a flattened array.\r\n        \"\"\"\r\n        return self._values.ravel(order=order)\r\n\r\n    def __len__(self) -> int:\r\n        \"\"\"\r\n        Return the length of the Series.\r\n        \"\"\"\r\n        return len(self._mgr)\r\n\r\n    def view(self, dtype: Dtype | None = None) -> Series:\r\n        \"\"\"\r\n        Create a new view of the Series.\r\n\r\n        This function will return a new Series with a view of the same\r\n        underlying values in memory, optionally reinterpreted with a new data\r\n        type. The new data type must preserve the same size in bytes as to not\r\n        cause index misalignment.\r\n\r\n        Parameters\r\n        ----------\r\n        dtype : data type\r\n            Data type object or one of their string representations.\r\n\r\n        Returns\r\n        -------\r\n"]
[145.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[145.002, "i", "sed -n '1,240p' pandas/core/arrays/base.py\r"]
[145.004, "o", "sed -n '1,240p' pandas/core/arrays/base.py\r\n"]
[145.006, "o", "\u001b[?2004l\r\n\"\"\"\r\nAn interface for extending pandas with custom arrays.\r\n\r\n.. warning::\r\n\r\n   This is an experimental API and subject to breaking changes\r\n   without warning.\r\n\"\"\"\r\nfrom __future__ import annotations\r\n\r\nimport operator\r\nfrom typing import (\r\n    TYPE_CHECKING,\r\n    Any,\r\n    Callable,\r\n    ClassVar,\r\n    Iterator,\r\n    Literal,\r\n    Sequence,\r\n    TypeVar,\r\n    cast,\r\n    overload,\r\n)\r\n\r\nimport numpy as np\r\n\r\nfrom pandas._libs import lib\r\nfrom pandas._typing import (\r\n    ArrayLike,\r\n    AstypeArg,\r\n    AxisInt,\r\n    Dtype,\r\n    FillnaOptions,\r\n    PositionalIndexer,\r\n    ScalarIndexer,\r\n    SequenceIndexer,\r\n    Shape,\r\n    SortKind,\r\n    TakeIndexer,\r\n    npt,\r\n)\r\nfrom pandas.compat import set_function_name\r\nfrom pandas.compat.numpy import function as nv\r\nfrom pandas.errors import AbstractMethodError\r\nfrom pandas.util._decorators import (\r\n    Appender,\r\n    Substitution,\r\n    cache_readonly,\r\n)\r\nfrom pandas.util._validators import (\r\n    validate_bool_kwarg,\r\n    validate_fillna_kwargs,\r\n    validate_insert_loc,\r\n)\r\n\r\nfrom pandas.core.dtypes.cast import maybe_cast_to_extension_array\r\nfrom pandas.core.dtypes.common import (\r\n    is_datetime64_dtype,\r\n    is_dtype_equal,\r\n    is_list_like,\r\n    is_scalar,\r\n    is_timedelta64_dtype,\r\n    pandas_dtype,\r\n)\r\nfrom pandas.core.dtypes.dtypes import ExtensionDtype\r\nfrom pandas.core.dtypes.generic import (\r\n    ABCDataFrame,\r\n    ABCIndex,\r\n    ABCSeries,\r\n)\r\nfrom pandas.core.dtypes.missing import isna\r\n\r\nfrom pandas.core import (\r\n    arraylike,\r\n    missing,\r\n    roperator,\r\n)\r\nfrom pandas.core.algorithms import (\r\n    factorize_array,\r\n    isin,\r\n    mode,\r\n    rank,\r\n    unique,\r\n)\r\nfrom pandas.core.array_algos.quantile import quantile_with_mask\r\nfrom pandas.core.sorting import (\r\n    nargminmax,\r\n    nargsort,\r\n)\r\n\r\nif TYPE_CHECKING:\r\n\r\n    from pandas._typing import (\r\n        NumpySorter,\r\n        NumpyValueArrayLike,\r\n    )\r\n\r\n_extension_array_shared_docs: dict[str, str] = {}\r\n\r\nExtensionArrayT = TypeVar(\"ExtensionArrayT\", bound=\"ExtensionArray\")\r\n\r\n\r\nclass ExtensionArray:\r\n    \"\"\"\r\n    Abstract base class for custom 1-D array types.\r\n\r\n    pandas will recognize instances of this class as proper arrays\r\n    with a custom type and will not attempt to coerce them to objects. They\r\n    may be stored directly inside a :class:`DataFrame` or :class:`Series`.\r\n\r\n    Attributes\r\n    ----------\r\n    dtype\r\n    nbytes\r\n    ndim\r\n    shape\r\n\r\n    Methods\r\n    -------\r\n    argsort\r\n    astype\r\n    copy\r\n    dropna\r\n    factorize\r\n    fillna\r\n    equals\r\n    insert\r\n    isin\r\n    isna\r\n    ravel\r\n    repeat\r\n    searchsorted\r\n    shift\r\n    take\r\n    tolist\r\n    unique\r\n    view\r\n    _accumulate\r\n    _concat_same_type\r\n    _formatter\r\n    _from_factorized\r\n    _from_sequence\r\n    _from_sequence_of_strings\r\n    _reduce\r\n    _values_for_argsort\r\n    _values_for_factorize\r\n\r\n    Notes\r\n    -----\r\n    The interface includes the following abstract methods that must be\r\n    implemented by subclasses:\r\n\r\n    * _from_sequence\r\n    * _from_factorized\r\n    * __getitem__\r\n    * __len__\r\n    * __eq__\r\n    * dtype\r\n    * nbytes\r\n    * isna\r\n    * take\r\n    * copy\r\n    * _concat_same_type\r\n\r\n    A default repr displaying the type, (truncated) data, length,\r\n    and dtype is provided. It can be customized or replaced by\r\n    by overriding:\r\n\r\n    * __repr__ : A default repr for the ExtensionArray.\r\n    * _formatter : Print scalars inside a Series or DataFrame.\r\n\r\n    Some methods require casting the ExtensionArray to an ndarray of Python\r\n    objects with ``self.astype(object)``, which may be expensive. When\r\n    performance is a concern, we highly recommend overriding the following\r\n    methods:\r\n\r\n    * fillna\r\n    * dropna\r\n    * unique\r\n    * factorize / _values_for_factorize\r\n    * argsort, argmax, argmin / _values_for_argsort\r\n    * searchsorted\r\n\r\n    The remaining methods implemented on this class should be performant,\r\n    as they only compose abstract methods. Still, a more efficient\r\n    implementation may be available, and these methods can be overridden.\r\n\r\n    One can implement methods to handle array accumulations or reductions.\r\n\r\n    * _accumulate\r\n    * _reduce\r\n\r\n    One can implement methods to handle parsing from strings that will be used\r\n    in methods such as ``pandas.io.parsers.read_csv``.\r\n\r\n    * _from_sequence_of_strings\r\n\r\n    This class does not inherit from 'abc.ABCMeta' for performance reasons.\r\n    Methods and properties required by the interface raise\r\n    ``pandas.errors.AbstractMethodError`` and no ``register`` method is\r\n    provided for registering virtual subclasses.\r\n\r\n    ExtensionArrays are limited to 1 dimension.\r\n\r\n    They may be backed by none, one, or many NumPy arrays. For example,\r\n    ``pandas.Categorical`` is an extension array backed by two arrays,\r\n    one for codes and one for categories. An array of IPv6 address may\r\n    be backed by a NumPy structured array with two fields, one for the\r\n    lower 64 bits and one for the upper 64 bits. Or they may be backed\r\n    by some other storage type, like Python lists. Pandas makes no\r\n    assumptions on how the data are stored, just that it can be converted\r\n    to a NumPy array.\r\n    The ExtensionArray interface does not impose any rules on how this data\r\n    is stored. However, currently, the backing data cannot be stored in\r\n    attributes called ``.values`` or ``._values`` to ensure full compatibility\r\n    with pandas internals. But other names as ``.data``, ``._data``,\r\n    ``._items``, ... can be freely used.\r\n\r\n    If implementing NumPy's ``__array_ufunc__`` interface, pandas expects\r\n    that\r\n\r\n    1. You defer by returning ``NotImplemented`` when any Series are present\r\n       in `inputs`. Pandas will extract the arrays and call the ufunc again.\r\n    2. You define a ``_HANDLED_TYPES`` tuple as an attribute on the class.\r\n       Pandas inspect this to determine whether the ufunc is valid for the\r\n       types present.\r\n\r\n    See :ref:`extending.extension.ufunc` for more.\r\n\r\n    By default, ExtensionArrays are not hashable.  Immutable subclasses may\r\n    override this behavior.\r\n    \"\"\"\r\n\r\n    # '_typ' is for pandas.core.dtypes.generic.ABCExtensionArray.\r\n    # Don't override this.\r\n    _typ = \"extension\"\r\n\r\n    # ------------------------------------------------------------------------\r\n    # Constructors\r\n    # ------------------------------------------------------------------------\r\n"]
[150.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[150.002, "i", "sed -n '520,760p' pandas/core/base.py\r"]
[150.004, "o", "sed -n '520,760p' pandas/core/base.py\r\n"]
[150.006, "o", "\u001b[?2004l\r\n        ... # doctest: +ELLIPSIS\r\n        array(['1999-12-31T23:00:00.000000000', '2000-01-01T23:00:00...'],\r\n              dtype='datetime64[ns]')\r\n        \"\"\"\r\n        if is_extension_array_dtype(self.dtype):\r\n            return self.array.to_numpy(dtype, copy=copy, na_value=na_value, **kwargs)\r\n        elif kwargs:\r\n            bad_keys = list(kwargs.keys())[0]\r\n            raise TypeError(\r\n                f\"to_numpy() got an unexpected keyword argument '{bad_keys}'\"\r\n            )\r\n\r\n        if na_value is not lib.no_default:\r\n            values = self._values\r\n            if not can_hold_element(values, na_value):\r\n                # if we can't hold the na_value asarray either makes a copy or we\r\n                # error before modifying values. The asarray later on thus won't make\r\n                # another copy\r\n                values = np.asarray(values, dtype=dtype)\r\n            else:\r\n                values = values.copy()\r\n\r\n            values[np.asanyarray(self.isna())] = na_value\r\n        else:\r\n            values = self._values\r\n\r\n        result = np.asarray(values, dtype=dtype)\r\n\r\n        if copy and na_value is lib.no_default:\r\n            if np.shares_memory(self._values[:2], result[:2]):\r\n                # Take slices to improve performance of check\r\n                result = result.copy()\r\n\r\n        return result\r\n\r\n    @final\r\n    @property\r\n    def empty(self) -> bool:\r\n        return not self.size\r\n\r\n    def max(self, axis: AxisInt | None = None, skipna: bool = True, *args, **kwargs):\r\n        \"\"\"\r\n        Return the maximum value of the Index.\r\n\r\n        Parameters\r\n        ----------\r\n        axis : int, optional\r\n            For compatibility with NumPy. Only 0 or None are allowed.\r\n        skipna : bool, default True\r\n            Exclude NA/null values when showing the result.\r\n        *args, **kwargs\r\n            Additional arguments and keywords for compatibility with NumPy.\r\n\r\n        Returns\r\n        -------\r\n        scalar\r\n            Maximum value.\r\n\r\n        See Also\r\n        --------\r\n        Index.min : Return the minimum value in an Index.\r\n        Series.max : Return the maximum value in a Series.\r\n        DataFrame.max : Return the maximum values in a DataFrame.\r\n\r\n        Examples\r\n        --------\r\n        >>> idx = pd.Index([3, 2, 1])\r\n        >>> idx.max()\r\n        3\r\n\r\n        >>> idx = pd.Index(['c', 'b', 'a'])\r\n        >>> idx.max()\r\n        'c'\r\n\r\n        For a MultiIndex, the maximum is determined lexicographically.\r\n\r\n        >>> idx = pd.MultiIndex.from_product([('a', 'b'), (2, 1)])\r\n        >>> idx.max()\r\n        ('b', 2)\r\n        \"\"\"\r\n        nv.validate_minmax_axis(axis)\r\n        nv.validate_max(args, kwargs)\r\n        return nanops.nanmax(self._values, skipna=skipna)\r\n\r\n    @doc(op=\"max\", oppose=\"min\", value=\"largest\")\r\n    def argmax(\r\n        self, axis: AxisInt | None = None, skipna: bool = True, *args, **kwargs\r\n    ) -> int:\r\n        \"\"\"\r\n        Return int position of the {value} value in the Series.\r\n\r\n        If the {op}imum is achieved in multiple locations,\r\n        the first row position is returned.\r\n\r\n        Parameters\r\n        ----------\r\n        axis : {{None}}\r\n            Unused. Parameter needed for compatibility with DataFrame.\r\n        skipna : bool, default True\r\n            Exclude NA/null values when showing the result.\r\n        *args, **kwargs\r\n            Additional arguments and keywords for compatibility with NumPy.\r\n\r\n        Returns\r\n        -------\r\n        int\r\n            Row position of the {op}imum value.\r\n\r\n        See Also\r\n        --------\r\n        Series.arg{op} : Return position of the {op}imum value.\r\n        Series.arg{oppose} : Return position of the {oppose}imum value.\r\n        numpy.ndarray.arg{op} : Equivalent method for numpy arrays.\r\n        Series.idxmax : Return index label of the maximum values.\r\n        Series.idxmin : Return index label of the minimum values.\r\n\r\n        Examples\r\n        --------\r\n        Consider dataset containing cereal calories\r\n\r\n        >>> s = pd.Series({{'Corn Flakes': 100.0, 'Almond Delight': 110.0,\r\n        ...                'Cinnamon Toast Crunch': 120.0, 'Cocoa Puff': 110.0}})\r\n        >>> s\r\n        Corn Flakes              100.0\r\n        Almond Delight           110.0\r\n        Cinnamon Toast Crunch    120.0\r\n        Cocoa Puff               110.0\r\n        dtype: float64\r\n\r\n        >>> s.argmax()\r\n        2\r\n        >>> s.argmin()\r\n        0\r\n\r\n        The maximum cereal calories is the third element and\r\n        the minimum cereal calories is the first element,\r\n        since series is zero-indexed.\r\n        \"\"\"\r\n        delegate = self._values\r\n        nv.validate_minmax_axis(axis)\r\n        skipna = nv.validate_argmax_with_skipna(skipna, args, kwargs)\r\n\r\n        if isinstance(delegate, ExtensionArray):\r\n            if not skipna and delegate.isna().any():\r\n                return -1\r\n            else:\r\n                return delegate.argmax()\r\n        else:\r\n            # error: Incompatible return value type (got \"Union[int, ndarray]\", expected\r\n            # \"int\")\r\n            return nanops.nanargmax(  # type: ignore[return-value]\r\n                delegate, skipna=skipna\r\n            )\r\n\r\n    def min(self, axis: AxisInt | None = None, skipna: bool = True, *args, **kwargs):\r\n        \"\"\"\r\n        Return the minimum value of the Index.\r\n\r\n        Parameters\r\n        ----------\r\n        axis : {None}\r\n            Dummy argument for consistency with Series.\r\n        skipna : bool, default True\r\n            Exclude NA/null values when showing the result.\r\n        *args, **kwargs\r\n            Additional arguments and keywords for compatibility with NumPy.\r\n\r\n        Returns\r\n        -------\r\n        scalar\r\n            Minimum value.\r\n\r\n        See Also\r\n        --------\r\n        Index.max : Return the maximum value of the object.\r\n        Series.min : Return the minimum value in a Series.\r\n        DataFrame.min : Return the minimum values in a DataFrame.\r\n\r\n        Examples\r\n        --------\r\n        >>> idx = pd.Index([3, 2, 1])\r\n        >>> idx.min()\r\n        1\r\n\r\n        >>> idx = pd.Index(['c', 'b', 'a'])\r\n        >>> idx.min()\r\n        'a'\r\n\r\n        For a MultiIndex, the minimum is determined lexicographically.\r\n\r\n        >>> idx = pd.MultiIndex.from_product([('a', 'b'), (2, 1)])\r\n        >>> idx.min()\r\n        ('a', 1)\r\n        \"\"\"\r\n        nv.validate_minmax_axis(axis)\r\n        nv.validate_min(args, kwargs)\r\n        return nanops.nanmin(self._values, skipna=skipna)\r\n\r\n    @doc(argmax, op=\"min\", oppose=\"max\", value=\"smallest\")\r\n    def argmin(\r\n        self, axis: AxisInt | None = None, skipna: bool = True, *args, **kwargs\r\n    ) -> int:\r\n        delegate = self._values\r\n        nv.validate_minmax_axis(axis)\r\n        skipna = nv.validate_argmin_with_skipna(skipna, args, kwargs)\r\n\r\n        if isinstance(delegate, ExtensionArray):\r\n            if not skipna and delegate.isna().any():\r\n                return -1\r\n            else:\r\n                return delegate.argmin()\r\n        else:\r\n            # error: Incompatible return value type (got \"Union[int, ndarray]\", expected\r\n            # \"int\")\r\n            return nanops.nanargmin(  # type: ignore[return-value]\r\n                delegate, skipna=skipna\r\n            )\r\n\r\n    def tolist(self):\r\n        \"\"\"\r\n        Return a list of the values.\r\n\r\n        These are each a scalar type, which is a Python scalar\r\n        (for str, int, float) or a pandas scalar\r\n        (for Timestamp/Timedelta/Interval/Period)\r\n\r\n        Returns\r\n        -------\r\n        list\r\n\r\n        See Also\r\n        --------\r\n        numpy.ndarray.tolist : Return the array as an a.ndim-levels deep\r\n            nested list of Python scalars.\r\n        \"\"\"\r\n        return self._values.tolist()\r\n\r\n    to_list = tolist\r\n\r\n    def __iter__(self) -> Iterator:\r\n        \"\"\"\r\n"]
[155.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[155.002, "i", "sed -n '760,980p' pandas/core/base.py\r"]
[155.004, "o", "sed -n '760,980p' pandas/core/base.py\r\n"]
[155.006, "o", "\u001b[?2004l\r\n        \"\"\"\r\n        Return an iterator of the values.\r\n\r\n        These are each a scalar type, which is a Python scalar\r\n        (for str, int, float) or a pandas scalar\r\n        (for Timestamp/Timedelta/Interval/Period)\r\n\r\n        Returns\r\n        -------\r\n        iterator\r\n        \"\"\"\r\n        # We are explicitly making element iterators.\r\n        if not isinstance(self._values, np.ndarray):\r\n            # Check type instead of dtype to catch DTA/TDA\r\n            return iter(self._values)\r\n        else:\r\n            return map(self._values.item, range(self._values.size))\r\n\r\n    @cache_readonly\r\n    def hasnans(self) -> bool:\r\n        \"\"\"\r\n        Return True if there are any NaNs.\r\n\r\n        Enables various performance speedups.\r\n\r\n        Returns\r\n        -------\r\n        bool\r\n        \"\"\"\r\n        # error: Item \"bool\" of \"Union[bool, ndarray[Any, dtype[bool_]], NDFrame]\"\r\n        # has no attribute \"any\"\r\n        return bool(isna(self).any())  # type: ignore[union-attr]\r\n\r\n    def isna(self) -> npt.NDArray[np.bool_]:\r\n        return isna(self._values)\r\n\r\n    def _reduce(\r\n        self,\r\n        op,\r\n        name: str,\r\n        *,\r\n        axis: Axis = 0,\r\n        skipna: bool = True,\r\n        numeric_only=None,\r\n        filter_type=None,\r\n        **kwds,\r\n    ):\r\n        \"\"\"\r\n        Perform the reduction type operation if we can.\r\n        \"\"\"\r\n        func = getattr(self, name, None)\r\n        if func is None:\r\n            raise TypeError(\r\n                f\"{type(self).__name__} cannot perform the operation {name}\"\r\n            )\r\n        return func(skipna=skipna, **kwds)\r\n\r\n    @final\r\n    def _map_values(self, mapper, na_action=None):\r\n        \"\"\"\r\n        An internal function that maps values using the input\r\n        correspondence (which can be a dict, Series, or function).\r\n\r\n        Parameters\r\n        ----------\r\n        mapper : function, dict, or Series\r\n            The input correspondence object\r\n        na_action : {None, 'ignore'}\r\n            If 'ignore', propagate NA values, without passing them to the\r\n            mapping function\r\n\r\n        Returns\r\n        -------\r\n        Union[Index, MultiIndex], inferred\r\n            The output of the mapping function applied to the index.\r\n            If the function returns a tuple with more than one element\r\n            a MultiIndex will be returned.\r\n        \"\"\"\r\n        # we can fastpath dict/Series to an efficient map\r\n        # as we know that we are not going to have to yield\r\n        # python types\r\n        if is_dict_like(mapper):\r\n            if isinstance(mapper, dict) and hasattr(mapper, \"__missing__\"):\r\n                # If a dictionary subclass defines a default value method,\r\n                # convert mapper to a lookup function (GH #15999).\r\n                dict_with_default = mapper\r\n                mapper = lambda x: dict_with_default[\r\n                    np.nan if isinstance(x, float) and np.isnan(x) else x\r\n                ]\r\n            else:\r\n                # Dictionary does not have a default. Thus it's safe to\r\n                # convert to an Series for efficiency.\r\n                # we specify the keys here to handle the\r\n                # possibility that they are tuples\r\n\r\n                # The return value of mapping with an empty mapper is\r\n                # expected to be pd.Series(np.nan, ...). As np.nan is\r\n                # of dtype float64 the return value of this method should\r\n                # be float64 as well\r\n                from pandas import Series\r\n\r\n                if len(mapper) == 0:\r\n                    mapper = Series(mapper, dtype=np.float64)\r\n                else:\r\n                    mapper = Series(mapper)\r\n\r\n        if isinstance(mapper, ABCSeries):\r\n            if na_action not in (None, \"ignore\"):\r\n                msg = (\r\n                    \"na_action must either be 'ignore' or None, \"\r\n                    f\"{na_action} was passed\"\r\n                )\r\n                raise ValueError(msg)\r\n\r\n            if na_action == \"ignore\":\r\n                mapper = mapper[mapper.index.notna()]\r\n\r\n            # Since values were input this means we came from either\r\n            # a dict or a series and mapper should be an index\r\n            if is_categorical_dtype(self.dtype):\r\n                # use the built in categorical series mapper which saves\r\n                # time by mapping the categories instead of all values\r\n\r\n                cat = cast(\"Categorical\", self._values)\r\n                return cat.map(mapper)\r\n\r\n            values = self._values\r\n\r\n            indexer = mapper.index.get_indexer(values)\r\n            new_values = algorithms.take_nd(mapper._values, indexer)\r\n\r\n            return new_values\r\n\r\n        # we must convert to python types\r\n        if is_extension_array_dtype(self.dtype) and hasattr(self._values, \"map\"):\r\n            # GH#23179 some EAs do not have `map`\r\n            values = self._values\r\n            if na_action is not None:\r\n                raise NotImplementedError\r\n            map_f = lambda values, f: values.map(f)\r\n        else:\r\n            values = self._values.astype(object)\r\n            if na_action == \"ignore\":\r\n                map_f = lambda values, f: lib.map_infer_mask(\r\n                    values, f, isna(values).view(np.uint8)\r\n                )\r\n            elif na_action is None:\r\n                map_f = lib.map_infer\r\n            else:\r\n                msg = (\r\n                    \"na_action must either be 'ignore' or None, \"\r\n                    f\"{na_action} was passed\"\r\n                )\r\n                raise ValueError(msg)\r\n\r\n        # mapper is a function\r\n        new_values = map_f(values, mapper)\r\n\r\n        return new_values\r\n\r\n    @final\r\n    def value_counts(\r\n        self,\r\n        normalize: bool = False,\r\n        sort: bool = True,\r\n        ascending: bool = False,\r\n        bins=None,\r\n        dropna: bool = True,\r\n    ) -> Series:\r\n        \"\"\"\r\n        Return a Series containing counts of unique values.\r\n\r\n        The resulting object will be in descending order so that the\r\n        first element is the most frequently-occurring element.\r\n        Excludes NA values by default.\r\n\r\n        Parameters\r\n        ----------\r\n        normalize : bool, default False\r\n            If True then the object returned will contain the relative\r\n            frequencies of the unique values.\r\n        sort : bool, default True\r\n            Sort by frequencies.\r\n        ascending : bool, default False\r\n            Sort in ascending order.\r\n        bins : int, optional\r\n            Rather than count values, group them into half-open bins,\r\n            a convenience for ``pd.cut``, only works with numeric data.\r\n        dropna : bool, default True\r\n            Don't include counts of NaN.\r\n\r\n        Returns\r\n        -------\r\n        Series\r\n\r\n        See Also\r\n        --------\r\n        Series.count: Number of non-NA elements in a Series.\r\n        DataFrame.count: Number of non-NA elements in a DataFrame.\r\n        DataFrame.value_counts: Equivalent method on DataFrames.\r\n\r\n        Examples\r\n        --------\r\n        >>> index = pd.Index([3, 1, 2, 3, 4, np.nan])\r\n        >>> index.value_counts()\r\n        3.0    2\r\n        1.0    1\r\n        2.0    1\r\n        4.0    1\r\n        dtype: int64\r\n\r\n        With `normalize` set to `True`, returns the relative frequency by\r\n        dividing all values by the sum of values.\r\n\r\n        >>> s = pd.Series([3, 1, 2, 3, 4, np.nan])\r\n        >>> s.value_counts(normalize=True)\r\n        3.0    0.4\r\n        1.0    0.2\r\n        2.0    0.2\r\n        4.0    0.2\r\n        dtype: float64\r\n"]
[160.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[160.002, "i", "sed -n '420,520p' pandas/core/base.py\r"]
[160.004, "o", "sed -n '420,520p' pandas/core/base.py\r\n"]
[160.006, "o", "\u001b[?2004l\r\n        Categories (2, object): ['a', 'b']\r\n        \"\"\"\r\n        raise AbstractMethodError(self)\r\n\r\n    @final\r\n    def to_numpy(\r\n        self,\r\n        dtype: npt.DTypeLike | None = None,\r\n        copy: bool = False,\r\n        na_value: object = lib.no_default,\r\n        **kwargs,\r\n    ) -> np.ndarray:\r\n        \"\"\"\r\n        A NumPy ndarray representing the values in this Series or Index.\r\n\r\n        Parameters\r\n        ----------\r\n        dtype : str or numpy.dtype, optional\r\n            The dtype to pass to :meth:`numpy.asarray`.\r\n        copy : bool, default False\r\n            Whether to ensure that the returned value is not a view on\r\n            another array. Note that ``copy=False`` does not *ensure* that\r\n            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\r\n            a copy is made, even if not strictly necessary.\r\n        na_value : Any, optional\r\n            The value to use for missing values. The default value depends\r\n            on `dtype` and the type of the array.\r\n\r\n            .. versionadded:: 1.0.0\r\n\r\n        **kwargs\r\n            Additional keywords passed through to the ``to_numpy`` method\r\n            of the underlying array (for extension arrays).\r\n\r\n            .. versionadded:: 1.0.0\r\n\r\n        Returns\r\n        -------\r\n        numpy.ndarray\r\n\r\n        See Also\r\n        --------\r\n        Series.array : Get the actual data stored within.\r\n        Index.array : Get the actual data stored within.\r\n        DataFrame.to_numpy : Similar method for DataFrame.\r\n\r\n        Notes\r\n        -----\r\n        The returned array will be the same up to equality (values equal\r\n        in `self` will be equal in the returned array; likewise for values\r\n        that are not equal). When `self` contains an ExtensionArray, the\r\n        dtype may be different. For example, for a category-dtype Series,\r\n        ``to_numpy()`` will return a NumPy array and the categorical dtype\r\n        will be lost.\r\n\r\n        For NumPy dtypes, this will be a reference to the actual data stored\r\n        in this Series or Index (assuming ``copy=False``). Modifying the result\r\n        in place will modify the data stored in the Series or Index (not that\r\n        we recommend doing that).\r\n\r\n        For extension types, ``to_numpy()`` *may* require copying data and\r\n        coercing the result to a NumPy type (possibly object), which may be\r\n        expensive. When you need a no-copy reference to the underlying data,\r\n        :attr:`Series.array` should be used instead.\r\n\r\n        This table lays out the different dtypes and default return types of\r\n        ``to_numpy()`` for various dtypes within pandas.\r\n\r\n        ================== ================================\r\n        dtype              array type\r\n        ================== ================================\r\n        category[T]        ndarray[T] (same dtype as input)\r\n        period             ndarray[object] (Periods)\r\n        interval           ndarray[object] (Intervals)\r\n        IntegerNA          ndarray[object]\r\n        datetime64[ns]     datetime64[ns]\r\n        datetime64[ns, tz] ndarray[object] (Timestamps)\r\n        ================== ================================\r\n\r\n        Examples\r\n        --------\r\n        >>> ser = pd.Series(pd.Categorical(['a', 'b', 'a']))\r\n        >>> ser.to_numpy()\r\n        array(['a', 'b', 'a'], dtype=object)\r\n\r\n        Specify the `dtype` to control how datetime-aware data is represented.\r\n        Use ``dtype=object`` to return an ndarray of pandas :class:`Timestamp`\r\n        objects, each with the correct ``tz``.\r\n\r\n        >>> ser = pd.Series(pd.date_range('2000', periods=2, tz=\"CET\"))\r\n        >>> ser.to_numpy(dtype=object)\r\n        array([Timestamp('2000-01-01 00:00:00+0100', tz='CET'),\r\n               Timestamp('2000-01-02 00:00:00+0100', tz='CET')],\r\n              dtype=object)\r\n\r\n        Or ``dtype='datetime64[ns]'`` to return an ndarray of native\r\n        datetime64 values. The values are converted to UTC and the timezone\r\n        info is dropped.\r\n\r\n        >>> ser.to_numpy(dtype=\"datetime64[ns]\")\r\n        ... # doctest: +ELLIPSIS\r\n"]
[165.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[165.002, "i", "sed -n '430,520p' pandas/core/arrays/base.py\r"]
[165.004, "o", "sed -n '430,520p' pandas/core/arrays/base.py\r\n"]
[165.006, "o", "\u001b[?2004l\r\n            else:\r\n                return False\r\n        else:\r\n            # error: Item \"ExtensionArray\" of \"Union[ExtensionArray, ndarray]\" has no\r\n            # attribute \"any\"\r\n            return (item == self).any()  # type: ignore[union-attr]\r\n\r\n    # error: Signature of \"__eq__\" incompatible with supertype \"object\"\r\n    def __eq__(self, other: Any) -> ArrayLike:  # type: ignore[override]\r\n        \"\"\"\r\n        Return for `self == other` (element-wise equality).\r\n        \"\"\"\r\n        # Implementer note: this should return a boolean numpy ndarray or\r\n        # a boolean ExtensionArray.\r\n        # When `other` is one of Series, Index, or DataFrame, this method should\r\n        # return NotImplemented (to ensure that those objects are responsible for\r\n        # first unpacking the arrays, and then dispatch the operation to the\r\n        # underlying arrays)\r\n        raise AbstractMethodError(self)\r\n\r\n    # error: Signature of \"__ne__\" incompatible with supertype \"object\"\r\n    def __ne__(self, other: Any) -> ArrayLike:  # type: ignore[override]\r\n        \"\"\"\r\n        Return for `self != other` (element-wise in-equality).\r\n        \"\"\"\r\n        return ~(self == other)\r\n\r\n    def to_numpy(\r\n        self,\r\n        dtype: npt.DTypeLike | None = None,\r\n        copy: bool = False,\r\n        na_value: object = lib.no_default,\r\n    ) -> np.ndarray:\r\n        \"\"\"\r\n        Convert to a NumPy ndarray.\r\n\r\n        .. versionadded:: 1.0.0\r\n\r\n        This is similar to :meth:`numpy.asarray`, but may provide additional control\r\n        over how the conversion is done.\r\n\r\n        Parameters\r\n        ----------\r\n        dtype : str or numpy.dtype, optional\r\n            The dtype to pass to :meth:`numpy.asarray`.\r\n        copy : bool, default False\r\n            Whether to ensure that the returned value is a not a view on\r\n            another array. Note that ``copy=False`` does not *ensure* that\r\n            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\r\n            a copy is made, even if not strictly necessary.\r\n        na_value : Any, optional\r\n            The value to use for missing values. The default value depends\r\n            on `dtype` and the type of the array.\r\n\r\n        Returns\r\n        -------\r\n        numpy.ndarray\r\n        \"\"\"\r\n        result = np.asarray(self, dtype=dtype)\r\n        if copy or na_value is not lib.no_default:\r\n            result = result.copy()\r\n        if na_value is not lib.no_default:\r\n            result[self.isna()] = na_value\r\n        return result\r\n\r\n    # ------------------------------------------------------------------------\r\n    # Required attributes\r\n    # ------------------------------------------------------------------------\r\n\r\n    @property\r\n    def dtype(self) -> ExtensionDtype:\r\n        \"\"\"\r\n        An instance of 'ExtensionDtype'.\r\n        \"\"\"\r\n        raise AbstractMethodError(self)\r\n\r\n    @property\r\n    def shape(self) -> Shape:\r\n        \"\"\"\r\n        Return a tuple of the array dimensions.\r\n        \"\"\"\r\n        return (len(self),)\r\n\r\n    @property\r\n    def size(self) -> int:\r\n        \"\"\"\r\n        The number of elements in the array.\r\n        \"\"\"\r\n        # error: Incompatible return value type (got \"signedinteger[_64Bit]\",\r\n        # expected \"int\")  [return-value]\r\n        return np.prod(self.shape)  # type: ignore[return-value]\r\n"]
[170.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[170.002, "i", "sed -n '1680,1820p' pandas/core/frame.py\r"]
[170.004, "o", "sed -n '1680,1820p' pandas/core/frame.py\r\n"]
[170.006, "o", "\u001b[?2004l\r\n        >>> pd.DataFrame.from_dict(data)\r\n           col_1 col_2\r\n        0      3     a\r\n        1      2     b\r\n        2      1     c\r\n        3      0     d\r\n\r\n        Specify ``orient='index'`` to create the DataFrame using dictionary\r\n        keys as rows:\r\n\r\n        >>> data = {'row_1': [3, 2, 1, 0], 'row_2': ['a', 'b', 'c', 'd']}\r\n        >>> pd.DataFrame.from_dict(data, orient='index')\r\n               0  1  2  3\r\n        row_1  3  2  1  0\r\n        row_2  a  b  c  d\r\n\r\n        When using the 'index' orientation, the column names can be\r\n        specified manually:\r\n\r\n        >>> pd.DataFrame.from_dict(data, orient='index',\r\n        ...                        columns=['A', 'B', 'C', 'D'])\r\n               A  B  C  D\r\n        row_1  3  2  1  0\r\n        row_2  a  b  c  d\r\n\r\n        Specify ``orient='tight'`` to create the DataFrame using a 'tight'\r\n        format:\r\n\r\n        >>> data = {'index': [('a', 'b'), ('a', 'c')],\r\n        ...         'columns': [('x', 1), ('y', 2)],\r\n        ...         'data': [[1, 3], [2, 4]],\r\n        ...         'index_names': ['n1', 'n2'],\r\n        ...         'column_names': ['z1', 'z2']}\r\n        >>> pd.DataFrame.from_dict(data, orient='tight')\r\n        z1     x  y\r\n        z2     1  2\r\n        n1 n2\r\n        a  b   1  3\r\n           c   2  4\r\n        \"\"\"\r\n        index = None\r\n        orient = orient.lower()\r\n        if orient == \"index\":\r\n            if len(data) > 0:\r\n                # TODO speed up Series case\r\n                if isinstance(list(data.values())[0], (Series, dict)):\r\n                    data = _from_nested_dict(data)\r\n                else:\r\n                    index = list(data.keys())\r\n                    # error: Incompatible types in assignment (expression has type\r\n                    # \"List[Any]\", variable has type \"Dict[Any, Any]\")\r\n                    data = list(data.values())  # type: ignore[assignment]\r\n        elif orient in (\"columns\", \"tight\"):\r\n            if columns is not None:\r\n                raise ValueError(f\"cannot use columns parameter with orient='{orient}'\")\r\n        else:  # pragma: no cover\r\n            raise ValueError(\r\n                f\"Expected 'index', 'columns' or 'tight' for orient parameter. \"\r\n                f\"Got '{orient}' instead\"\r\n            )\r\n\r\n        if orient != \"tight\":\r\n            return cls(data, index=index, columns=columns, dtype=dtype)\r\n        else:\r\n            realdata = data[\"data\"]\r\n\r\n            def create_index(indexlist, namelist):\r\n                index: Index\r\n                if len(namelist) > 1:\r\n                    index = MultiIndex.from_tuples(indexlist, names=namelist)\r\n                else:\r\n                    index = Index(indexlist, name=namelist[0])\r\n                return index\r\n\r\n            index = create_index(data[\"index\"], data[\"index_names\"])\r\n            columns = create_index(data[\"columns\"], data[\"column_names\"])\r\n            return cls(realdata, index=index, columns=columns, dtype=dtype)\r\n\r\n    def to_numpy(\r\n        self,\r\n        dtype: npt.DTypeLike | None = None,\r\n        copy: bool = False,\r\n        na_value: object = lib.no_default,\r\n    ) -> np.ndarray:\r\n        \"\"\"\r\n        Convert the DataFrame to a NumPy array.\r\n\r\n        By default, the dtype of the returned array will be the common NumPy\r\n        dtype of all types in the DataFrame. For example, if the dtypes are\r\n        ``float16`` and ``float32``, the results dtype will be ``float32``.\r\n        This may require copying data and coercing values, which may be\r\n        expensive.\r\n\r\n        Parameters\r\n        ----------\r\n        dtype : str or numpy.dtype, optional\r\n            The dtype to pass to :meth:`numpy.asarray`.\r\n        copy : bool, default False\r\n            Whether to ensure that the returned value is not a view on\r\n            another array. Note that ``copy=False`` does not *ensure* that\r\n            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\r\n            a copy is made, even if not strictly necessary.\r\n        na_value : Any, optional\r\n            The value to use for missing values. The default value depends\r\n            on `dtype` and the dtypes of the DataFrame columns.\r\n\r\n            .. versionadded:: 1.1.0\r\n\r\n        Returns\r\n        -------\r\n        numpy.ndarray\r\n\r\n        See Also\r\n        --------\r\n        Series.to_numpy : Similar method for Series.\r\n\r\n        Examples\r\n        --------\r\n        >>> pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]}).to_numpy()\r\n        array([[1, 3],\r\n               [2, 4]])\r\n\r\n        With heterogeneous data, the lowest common type will have to\r\n        be used.\r\n\r\n        >>> df = pd.DataFrame({\"A\": [1, 2], \"B\": [3.0, 4.5]})\r\n        >>> df.to_numpy()\r\n        array([[1. , 3. ],\r\n               [2. , 4.5]])\r\n\r\n        For a mix of numeric and non-numeric types, the output array will\r\n        have object dtype.\r\n\r\n        >>> df['C'] = pd.date_range('2000', periods=2)\r\n        >>> df.to_numpy()\r\n        array([[1, 3.0, Timestamp('2000-01-01 00:00:00')],\r\n               [2, 4.5, Timestamp('2000-01-02 00:00:00')]], dtype=object)\r\n        \"\"\"\r\n        if dtype is not None:\r\n            dtype = np.dtype(dtype)\r\n        result = self._mgr.as_array(dtype=dtype, copy=copy, na_value=na_value)\r\n"]
[175.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[175.002, "i", "sed -n '420,560p' pandas/core/base.py\r"]
[175.004, "o", "sed -n '420,560p' pandas/core/base.py\r\n"]
[175.006, "o", "\u001b[?2004l\r\n        Categories (2, object): ['a', 'b']\r\n        \"\"\"\r\n        raise AbstractMethodError(self)\r\n\r\n    @final\r\n    def to_numpy(\r\n        self,\r\n        dtype: npt.DTypeLike | None = None,\r\n        copy: bool = False,\r\n        na_value: object = lib.no_default,\r\n        **kwargs,\r\n    ) -> np.ndarray:\r\n        \"\"\"\r\n        A NumPy ndarray representing the values in this Series or Index.\r\n\r\n        Parameters\r\n        ----------\r\n        dtype : str or numpy.dtype, optional\r\n            The dtype to pass to :meth:`numpy.asarray`.\r\n        copy : bool, default False\r\n            Whether to ensure that the returned value is not a view on\r\n            another array. Note that ``copy=False`` does not *ensure* that\r\n            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\r\n            a copy is made, even if not strictly necessary.\r\n        na_value : Any, optional\r\n            The value to use for missing values. The default value depends\r\n            on `dtype` and the type of the array.\r\n\r\n            .. versionadded:: 1.0.0\r\n\r\n        **kwargs\r\n            Additional keywords passed through to the ``to_numpy`` method\r\n            of the underlying array (for extension arrays).\r\n\r\n            .. versionadded:: 1.0.0\r\n\r\n        Returns\r\n        -------\r\n        numpy.ndarray\r\n\r\n        See Also\r\n        --------\r\n        Series.array : Get the actual data stored within.\r\n        Index.array : Get the actual data stored within.\r\n        DataFrame.to_numpy : Similar method for DataFrame.\r\n\r\n        Notes\r\n        -----\r\n        The returned array will be the same up to equality (values equal\r\n        in `self` will be equal in the returned array; likewise for values\r\n        that are not equal). When `self` contains an ExtensionArray, the\r\n        dtype may be different. For example, for a category-dtype Series,\r\n        ``to_numpy()`` will return a NumPy array and the categorical dtype\r\n        will be lost.\r\n\r\n        For NumPy dtypes, this will be a reference to the actual data stored\r\n        in this Series or Index (assuming ``copy=False``). Modifying the result\r\n        in place will modify the data stored in the Series or Index (not that\r\n        we recommend doing that).\r\n\r\n        For extension types, ``to_numpy()`` *may* require copying data and\r\n        coercing the result to a NumPy type (possibly object), which may be\r\n        expensive. When you need a no-copy reference to the underlying data,\r\n        :attr:`Series.array` should be used instead.\r\n\r\n        This table lays out the different dtypes and default return types of\r\n        ``to_numpy()`` for various dtypes within pandas.\r\n\r\n        ================== ================================\r\n        dtype              array type\r\n        ================== ================================\r\n        category[T]        ndarray[T] (same dtype as input)\r\n        period             ndarray[object] (Periods)\r\n        interval           ndarray[object] (Intervals)\r\n        IntegerNA          ndarray[object]\r\n        datetime64[ns]     datetime64[ns]\r\n        datetime64[ns, tz] ndarray[object] (Timestamps)\r\n        ================== ================================\r\n\r\n        Examples\r\n        --------\r\n        >>> ser = pd.Series(pd.Categorical(['a', 'b', 'a']))\r\n        >>> ser.to_numpy()\r\n        array(['a', 'b', 'a'], dtype=object)\r\n\r\n        Specify the `dtype` to control how datetime-aware data is represented.\r\n        Use ``dtype=object`` to return an ndarray of pandas :class:`Timestamp`\r\n        objects, each with the correct ``tz``.\r\n\r\n        >>> ser = pd.Series(pd.date_range('2000', periods=2, tz=\"CET\"))\r\n        >>> ser.to_numpy(dtype=object)\r\n        array([Timestamp('2000-01-01 00:00:00+0100', tz='CET'),\r\n               Timestamp('2000-01-02 00:00:00+0100', tz='CET')],\r\n              dtype=object)\r\n\r\n        Or ``dtype='datetime64[ns]'`` to return an ndarray of native\r\n        datetime64 values. The values are converted to UTC and the timezone\r\n        info is dropped.\r\n\r\n        >>> ser.to_numpy(dtype=\"datetime64[ns]\")\r\n        ... # doctest: +ELLIPSIS\r\n        array(['1999-12-31T23:00:00.000000000', '2000-01-01T23:00:00...'],\r\n              dtype='datetime64[ns]')\r\n        \"\"\"\r\n        if is_extension_array_dtype(self.dtype):\r\n            return self.array.to_numpy(dtype, copy=copy, na_value=na_value, **kwargs)\r\n        elif kwargs:\r\n            bad_keys = list(kwargs.keys())[0]\r\n            raise TypeError(\r\n                f\"to_numpy() got an unexpected keyword argument '{bad_keys}'\"\r\n            )\r\n\r\n        if na_value is not lib.no_default:\r\n            values = self._values\r\n            if not can_hold_element(values, na_value):\r\n                # if we can't hold the na_value asarray either makes a copy or we\r\n                # error before modifying values. The asarray later on thus won't make\r\n                # another copy\r\n                values = np.asarray(values, dtype=dtype)\r\n            else:\r\n                values = values.copy()\r\n\r\n            values[np.asanyarray(self.isna())] = na_value\r\n        else:\r\n            values = self._values\r\n\r\n        result = np.asarray(values, dtype=dtype)\r\n\r\n        if copy and na_value is lib.no_default:\r\n            if np.shares_memory(self._values[:2], result[:2]):\r\n                # Take slices to improve performance of check\r\n                result = result.copy()\r\n\r\n        return result\r\n\r\n    @final\r\n    @property\r\n    def empty(self) -> bool:\r\n        return not self.size\r\n\r\n    def max(self, axis: AxisInt | None = None, skipna: bool = True, *args, **kwargs):\r\n"]
[180.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[180.002, "i", "sed -n '560,740p' pandas/core/base.py\r"]
[180.004, "o", "sed -n '560,740p' pandas/core/base.py\r\n"]
[180.006, "o", "\u001b[?2004l\r\n    def max(self, axis: AxisInt | None = None, skipna: bool = True, *args, **kwargs):\r\n        \"\"\"\r\n        Return the maximum value of the Index.\r\n\r\n        Parameters\r\n        ----------\r\n        axis : int, optional\r\n            For compatibility with NumPy. Only 0 or None are allowed.\r\n        skipna : bool, default True\r\n            Exclude NA/null values when showing the result.\r\n        *args, **kwargs\r\n            Additional arguments and keywords for compatibility with NumPy.\r\n\r\n        Returns\r\n        -------\r\n        scalar\r\n            Maximum value.\r\n\r\n        See Also\r\n        --------\r\n        Index.min : Return the minimum value in an Index.\r\n        Series.max : Return the maximum value in a Series.\r\n        DataFrame.max : Return the maximum values in a DataFrame.\r\n\r\n        Examples\r\n        --------\r\n        >>> idx = pd.Index([3, 2, 1])\r\n        >>> idx.max()\r\n        3\r\n\r\n        >>> idx = pd.Index(['c', 'b', 'a'])\r\n        >>> idx.max()\r\n        'c'\r\n\r\n        For a MultiIndex, the maximum is determined lexicographically.\r\n\r\n        >>> idx = pd.MultiIndex.from_product([('a', 'b'), (2, 1)])\r\n        >>> idx.max()\r\n        ('b', 2)\r\n        \"\"\"\r\n        nv.validate_minmax_axis(axis)\r\n        nv.validate_max(args, kwargs)\r\n        return nanops.nanmax(self._values, skipna=skipna)\r\n\r\n    @doc(op=\"max\", oppose=\"min\", value=\"largest\")\r\n    def argmax(\r\n        self, axis: AxisInt | None = None, skipna: bool = True, *args, **kwargs\r\n    ) -> int:\r\n        \"\"\"\r\n        Return int position of the {value} value in the Series.\r\n\r\n        If the {op}imum is achieved in multiple locations,\r\n        the first row position is returned.\r\n\r\n        Parameters\r\n        ----------\r\n        axis : {{None}}\r\n            Unused. Parameter needed for compatibility with DataFrame.\r\n        skipna : bool, default True\r\n            Exclude NA/null values when showing the result.\r\n        *args, **kwargs\r\n            Additional arguments and keywords for compatibility with NumPy.\r\n\r\n        Returns\r\n        -------\r\n        int\r\n            Row position of the {op}imum value.\r\n\r\n        See Also\r\n        --------\r\n        Series.arg{op} : Return position of the {op}imum value.\r\n        Series.arg{oppose} : Return position of the {oppose}imum value.\r\n        numpy.ndarray.arg{op} : Equivalent method for numpy arrays.\r\n        Series.idxmax : Return index label of the maximum values.\r\n        Series.idxmin : Return index label of the minimum values.\r\n\r\n        Examples\r\n        --------\r\n        Consider dataset containing cereal calories\r\n\r\n        >>> s = pd.Series({{'Corn Flakes': 100.0, 'Almond Delight': 110.0,\r\n        ...                'Cinnamon Toast Crunch': 120.0, 'Cocoa Puff': 110.0}})\r\n        >>> s\r\n        Corn Flakes              100.0\r\n        Almond Delight           110.0\r\n        Cinnamon Toast Crunch    120.0\r\n        Cocoa Puff               110.0\r\n        dtype: float64\r\n\r\n        >>> s.argmax()\r\n        2\r\n        >>> s.argmin()\r\n        0\r\n\r\n        The maximum cereal calories is the third element and\r\n        the minimum cereal calories is the first element,\r\n        since series is zero-indexed.\r\n        \"\"\"\r\n        delegate = self._values\r\n        nv.validate_minmax_axis(axis)\r\n        skipna = nv.validate_argmax_with_skipna(skipna, args, kwargs)\r\n\r\n        if isinstance(delegate, ExtensionArray):\r\n            if not skipna and delegate.isna().any():\r\n                return -1\r\n            else:\r\n                return delegate.argmax()\r\n        else:\r\n            # error: Incompatible return value type (got \"Union[int, ndarray]\", expected\r\n            # \"int\")\r\n            return nanops.nanargmax(  # type: ignore[return-value]\r\n                delegate, skipna=skipna\r\n            )\r\n\r\n    def min(self, axis: AxisInt | None = None, skipna: bool = True, *args, **kwargs):\r\n        \"\"\"\r\n        Return the minimum value of the Index.\r\n\r\n        Parameters\r\n        ----------\r\n        axis : {None}\r\n            Dummy argument for consistency with Series.\r\n        skipna : bool, default True\r\n            Exclude NA/null values when showing the result.\r\n        *args, **kwargs\r\n            Additional arguments and keywords for compatibility with NumPy.\r\n\r\n        Returns\r\n        -------\r\n        scalar\r\n            Minimum value.\r\n\r\n        See Also\r\n        --------\r\n        Index.max : Return the maximum value of the object.\r\n        Series.min : Return the minimum value in a Series.\r\n        DataFrame.min : Return the minimum values in a DataFrame.\r\n\r\n        Examples\r\n        --------\r\n        >>> idx = pd.Index([3, 2, 1])\r\n        >>> idx.min()\r\n        1\r\n\r\n        >>> idx = pd.Index(['c', 'b', 'a'])\r\n        >>> idx.min()\r\n        'a'\r\n\r\n        For a MultiIndex, the minimum is determined lexicographically.\r\n\r\n        >>> idx = pd.MultiIndex.from_product([('a', 'b'), (2, 1)])\r\n        >>> idx.min()\r\n        ('a', 1)\r\n        \"\"\"\r\n        nv.validate_minmax_axis(axis)\r\n        nv.validate_min(args, kwargs)\r\n        return nanops.nanmin(self._values, skipna=skipna)\r\n\r\n    @doc(argmax, op=\"min\", oppose=\"max\", value=\"smallest\")\r\n    def argmin(\r\n        self, axis: AxisInt | None = None, skipna: bool = True, *args, **kwargs\r\n    ) -> int:\r\n        delegate = self._values\r\n        nv.validate_minmax_axis(axis)\r\n        skipna = nv.validate_argmin_with_skipna(skipna, args, kwargs)\r\n\r\n        if isinstance(delegate, ExtensionArray):\r\n            if not skipna and delegate.isna().any():\r\n                return -1\r\n            else:\r\n                return delegate.argmin()\r\n        else:\r\n            # error: Incompatible return value type (got \"Union[int, ndarray]\", expected\r\n            # \"int\")\r\n            return nanops.nanargmin(  # type: ignore[return-value]\r\n                delegate, skipna=skipna\r\n            )\r\n\r\n    def tolist(self):\r\n        \"\"\"\r\n        Return a list of the values.\r\n"]
[185.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[185.002, "i", "sed -n '740,900p' pandas/core/base.py\r"]
[185.004, "o", "sed -n '740,900p' pandas/core/base.py\r\n"]
[185.006, "o", "\u001b[?2004l\r\n        Return a list of the values.\r\n\r\n        These are each a scalar type, which is a Python scalar\r\n        (for str, int, float) or a pandas scalar\r\n        (for Timestamp/Timedelta/Interval/Period)\r\n\r\n        Returns\r\n        -------\r\n        list\r\n\r\n        See Also\r\n        --------\r\n        numpy.ndarray.tolist : Return the array as an a.ndim-levels deep\r\n            nested list of Python scalars.\r\n        \"\"\"\r\n        return self._values.tolist()\r\n\r\n    to_list = tolist\r\n\r\n    def __iter__(self) -> Iterator:\r\n        \"\"\"\r\n        Return an iterator of the values.\r\n\r\n        These are each a scalar type, which is a Python scalar\r\n        (for str, int, float) or a pandas scalar\r\n        (for Timestamp/Timedelta/Interval/Period)\r\n\r\n        Returns\r\n        -------\r\n        iterator\r\n        \"\"\"\r\n        # We are explicitly making element iterators.\r\n        if not isinstance(self._values, np.ndarray):\r\n            # Check type instead of dtype to catch DTA/TDA\r\n            return iter(self._values)\r\n        else:\r\n            return map(self._values.item, range(self._values.size))\r\n\r\n    @cache_readonly\r\n    def hasnans(self) -> bool:\r\n        \"\"\"\r\n        Return True if there are any NaNs.\r\n\r\n        Enables various performance speedups.\r\n\r\n        Returns\r\n        -------\r\n        bool\r\n        \"\"\"\r\n        # error: Item \"bool\" of \"Union[bool, ndarray[Any, dtype[bool_]], NDFrame]\"\r\n        # has no attribute \"any\"\r\n        return bool(isna(self).any())  # type: ignore[union-attr]\r\n\r\n    def isna(self) -> npt.NDArray[np.bool_]:\r\n        return isna(self._values)\r\n\r\n    def _reduce(\r\n        self,\r\n        op,\r\n        name: str,\r\n        *,\r\n        axis: Axis = 0,\r\n        skipna: bool = True,\r\n        numeric_only=None,\r\n        filter_type=None,\r\n        **kwds,\r\n    ):\r\n        \"\"\"\r\n        Perform the reduction type operation if we can.\r\n        \"\"\"\r\n        func = getattr(self, name, None)\r\n        if func is None:\r\n            raise TypeError(\r\n                f\"{type(self).__name__} cannot perform the operation {name}\"\r\n            )\r\n        return func(skipna=skipna, **kwds)\r\n\r\n    @final\r\n    def _map_values(self, mapper, na_action=None):\r\n        \"\"\"\r\n        An internal function that maps values using the input\r\n        correspondence (which can be a dict, Series, or function).\r\n\r\n        Parameters\r\n        ----------\r\n        mapper : function, dict, or Series\r\n            The input correspondence object\r\n        na_action : {None, 'ignore'}\r\n            If 'ignore', propagate NA values, without passing them to the\r\n            mapping function\r\n\r\n        Returns\r\n        -------\r\n        Union[Index, MultiIndex], inferred\r\n            The output of the mapping function applied to the index.\r\n            If the function returns a tuple with more than one element\r\n            a MultiIndex will be returned.\r\n        \"\"\"\r\n        # we can fastpath dict/Series to an efficient map\r\n        # as we know that we are not going to have to yield\r\n        # python types\r\n        if is_dict_like(mapper):\r\n            if isinstance(mapper, dict) and hasattr(mapper, \"__missing__\"):\r\n                # If a dictionary subclass defines a default value method,\r\n                # convert mapper to a lookup function (GH #15999).\r\n                dict_with_default = mapper\r\n                mapper = lambda x: dict_with_default[\r\n                    np.nan if isinstance(x, float) and np.isnan(x) else x\r\n                ]\r\n            else:\r\n                # Dictionary does not have a default. Thus it's safe to\r\n                # convert to an Series for efficiency.\r\n                # we specify the keys here to handle the\r\n                # possibility that they are tuples\r\n\r\n                # The return value of mapping with an empty mapper is\r\n                # expected to be pd.Series(np.nan, ...). As np.nan is\r\n                # of dtype float64 the return value of this method should\r\n                # be float64 as well\r\n                from pandas import Series\r\n\r\n                if len(mapper) == 0:\r\n                    mapper = Series(mapper, dtype=np.float64)\r\n                else:\r\n                    mapper = Series(mapper)\r\n\r\n        if isinstance(mapper, ABCSeries):\r\n            if na_action not in (None, \"ignore\"):\r\n                msg = (\r\n                    \"na_action must either be 'ignore' or None, \"\r\n                    f\"{na_action} was passed\"\r\n                )\r\n                raise ValueError(msg)\r\n\r\n            if na_action == \"ignore\":\r\n                mapper = mapper[mapper.index.notna()]\r\n\r\n            # Since values were input this means we came from either\r\n            # a dict or a series and mapper should be an index\r\n            if is_categorical_dtype(self.dtype):\r\n                # use the built in categorical series mapper which saves\r\n                # time by mapping the categories instead of all values\r\n\r\n                cat = cast(\"Categorical\", self._values)\r\n                return cat.map(mapper)\r\n\r\n            values = self._values\r\n\r\n            indexer = mapper.index.get_indexer(values)\r\n            new_values = algorithms.take_nd(mapper._values, indexer)\r\n\r\n            return new_values\r\n\r\n        # we must convert to python types\r\n        if is_extension_array_dtype(self.dtype) and hasattr(self._values, \"map\"):\r\n            # GH#23179 some EAs do not have `map`\r\n            values = self._values\r\n            if na_action is not None:\r\n                raise NotImplementedError\r\n            map_f = lambda values, f: values.map(f)\r\n        else:\r\n"]
[190.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[190.002, "i", "sed -n '400,520p' pandas/core/base.py\r"]
[190.004, "o", "sed -n '400,520p' pandas/core/base.py\r\n"]
[190.006, "o", "\u001b[?2004l\r\n        :class:`arrays.NumpyExtensionArray` wrapping the actual ndarray\r\n        stored within. If you absolutely need a NumPy array (possibly with\r\n        copying / coercing data), then use :meth:`Series.to_numpy` instead.\r\n\r\n        Examples\r\n        --------\r\n        For regular NumPy types like int, and float, a PandasArray\r\n        is returned.\r\n\r\n        >>> pd.Series([1, 2, 3]).array\r\n        <PandasArray>\r\n        [1, 2, 3]\r\n        Length: 3, dtype: int64\r\n\r\n        For extension types, like Categorical, the actual ExtensionArray\r\n        is returned\r\n\r\n        >>> ser = pd.Series(pd.Categorical(['a', 'b', 'a']))\r\n        >>> ser.array\r\n        ['a', 'b', 'a']\r\n        Categories (2, object): ['a', 'b']\r\n        \"\"\"\r\n        raise AbstractMethodError(self)\r\n\r\n    @final\r\n    def to_numpy(\r\n        self,\r\n        dtype: npt.DTypeLike | None = None,\r\n        copy: bool = False,\r\n        na_value: object = lib.no_default,\r\n        **kwargs,\r\n    ) -> np.ndarray:\r\n        \"\"\"\r\n        A NumPy ndarray representing the values in this Series or Index.\r\n\r\n        Parameters\r\n        ----------\r\n        dtype : str or numpy.dtype, optional\r\n            The dtype to pass to :meth:`numpy.asarray`.\r\n        copy : bool, default False\r\n            Whether to ensure that the returned value is not a view on\r\n            another array. Note that ``copy=False`` does not *ensure* that\r\n            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\r\n            a copy is made, even if not strictly necessary.\r\n        na_value : Any, optional\r\n            The value to use for missing values. The default value depends\r\n            on `dtype` and the type of the array.\r\n\r\n            .. versionadded:: 1.0.0\r\n\r\n        **kwargs\r\n            Additional keywords passed through to the ``to_numpy`` method\r\n            of the underlying array (for extension arrays).\r\n\r\n            .. versionadded:: 1.0.0\r\n\r\n        Returns\r\n        -------\r\n        numpy.ndarray\r\n\r\n        See Also\r\n        --------\r\n        Series.array : Get the actual data stored within.\r\n        Index.array : Get the actual data stored within.\r\n        DataFrame.to_numpy : Similar method for DataFrame.\r\n\r\n        Notes\r\n        -----\r\n        The returned array will be the same up to equality (values equal\r\n        in `self` will be equal in the returned array; likewise for values\r\n        that are not equal). When `self` contains an ExtensionArray, the\r\n        dtype may be different. For example, for a category-dtype Series,\r\n        ``to_numpy()`` will return a NumPy array and the categorical dtype\r\n        will be lost.\r\n\r\n        For NumPy dtypes, this will be a reference to the actual data stored\r\n        in this Series or Index (assuming ``copy=False``). Modifying the result\r\n        in place will modify the data stored in the Series or Index (not that\r\n        we recommend doing that).\r\n\r\n        For extension types, ``to_numpy()`` *may* require copying data and\r\n        coercing the result to a NumPy type (possibly object), which may be\r\n        expensive. When you need a no-copy reference to the underlying data,\r\n        :attr:`Series.array` should be used instead.\r\n\r\n        This table lays out the different dtypes and default return types of\r\n        ``to_numpy()`` for various dtypes within pandas.\r\n\r\n        ================== ================================\r\n        dtype              array type\r\n        ================== ================================\r\n        category[T]        ndarray[T] (same dtype as input)\r\n        period             ndarray[object] (Periods)\r\n        interval           ndarray[object] (Intervals)\r\n        IntegerNA          ndarray[object]\r\n        datetime64[ns]     datetime64[ns]\r\n        datetime64[ns, tz] ndarray[object] (Timestamps)\r\n        ================== ================================\r\n\r\n        Examples\r\n        --------\r\n        >>> ser = pd.Series(pd.Categorical(['a', 'b', 'a']))\r\n        >>> ser.to_numpy()\r\n        array(['a', 'b', 'a'], dtype=object)\r\n\r\n        Specify the `dtype` to control how datetime-aware data is represented.\r\n        Use ``dtype=object`` to return an ndarray of pandas :class:`Timestamp`\r\n        objects, each with the correct ``tz``.\r\n\r\n        >>> ser = pd.Series(pd.date_range('2000', periods=2, tz=\"CET\"))\r\n        >>> ser.to_numpy(dtype=object)\r\n        array([Timestamp('2000-01-01 00:00:00+0100', tz='CET'),\r\n               Timestamp('2000-01-02 00:00:00+0100', tz='CET')],\r\n              dtype=object)\r\n\r\n        Or ``dtype='datetime64[ns]'`` to return an ndarray of native\r\n        datetime64 values. The values are converted to UTC and the timezone\r\n        info is dropped.\r\n\r\n        >>> ser.to_numpy(dtype=\"datetime64[ns]\")\r\n        ... # doctest: +ELLIPSIS\r\n"]
[195.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[195.002, "i", "sed -n '520,700p' pandas/core/base.py\r"]
[195.004, "o", "sed -n '520,700p' pandas/core/base.py\r\n"]
[195.006, "o", "\u001b[?2004l\r\n        ... # doctest: +ELLIPSIS\r\n        array(['1999-12-31T23:00:00.000000000', '2000-01-01T23:00:00...'],\r\n              dtype='datetime64[ns]')\r\n        \"\"\"\r\n        if is_extension_array_dtype(self.dtype):\r\n            return self.array.to_numpy(dtype, copy=copy, na_value=na_value, **kwargs)\r\n        elif kwargs:\r\n            bad_keys = list(kwargs.keys())[0]\r\n            raise TypeError(\r\n                f\"to_numpy() got an unexpected keyword argument '{bad_keys}'\"\r\n            )\r\n\r\n        if na_value is not lib.no_default:\r\n            values = self._values\r\n            if not can_hold_element(values, na_value):\r\n                # if we can't hold the na_value asarray either makes a copy or we\r\n                # error before modifying values. The asarray later on thus won't make\r\n                # another copy\r\n                values = np.asarray(values, dtype=dtype)\r\n            else:\r\n                values = values.copy()\r\n\r\n            values[np.asanyarray(self.isna())] = na_value\r\n        else:\r\n            values = self._values\r\n\r\n        result = np.asarray(values, dtype=dtype)\r\n\r\n        if copy and na_value is lib.no_default:\r\n            if np.shares_memory(self._values[:2], result[:2]):\r\n                # Take slices to improve performance of check\r\n                result = result.copy()\r\n\r\n        return result\r\n\r\n    @final\r\n    @property\r\n    def empty(self) -> bool:\r\n        return not self.size\r\n\r\n    def max(self, axis: AxisInt | None = None, skipna: bool = True, *args, **kwargs):\r\n        \"\"\"\r\n        Return the maximum value of the Index.\r\n\r\n        Parameters\r\n        ----------\r\n        axis : int, optional\r\n            For compatibility with NumPy. Only 0 or None are allowed.\r\n        skipna : bool, default True\r\n            Exclude NA/null values when showing the result.\r\n        *args, **kwargs\r\n            Additional arguments and keywords for compatibility with NumPy.\r\n\r\n        Returns\r\n        -------\r\n        scalar\r\n            Maximum value.\r\n\r\n        See Also\r\n        --------\r\n        Index.min : Return the minimum value in an Index.\r\n        Series.max : Return the maximum value in a Series.\r\n        DataFrame.max : Return the maximum values in a DataFrame.\r\n\r\n        Examples\r\n        --------\r\n        >>> idx = pd.Index([3, 2, 1])\r\n        >>> idx.max()\r\n        3\r\n\r\n        >>> idx = pd.Index(['c', 'b', 'a'])\r\n        >>> idx.max()\r\n        'c'\r\n\r\n        For a MultiIndex, the maximum is determined lexicographically.\r\n\r\n        >>> idx = pd.MultiIndex.from_product([('a', 'b'), (2, 1)])\r\n        >>> idx.max()\r\n        ('b', 2)\r\n        \"\"\"\r\n        nv.validate_minmax_axis(axis)\r\n        nv.validate_max(args, kwargs)\r\n        return nanops.nanmax(self._values, skipna=skipna)\r\n\r\n    @doc(op=\"max\", oppose=\"min\", value=\"largest\")\r\n    def argmax(\r\n        self, axis: AxisInt | None = None, skipna: bool = True, *args, **kwargs\r\n    ) -> int:\r\n        \"\"\"\r\n        Return int position of the {value} value in the Series.\r\n\r\n        If the {op}imum is achieved in multiple locations,\r\n        the first row position is returned.\r\n\r\n        Parameters\r\n        ----------\r\n        axis : {{None}}\r\n            Unused. Parameter needed for compatibility with DataFrame.\r\n        skipna : bool, default True\r\n            Exclude NA/null values when showing the result.\r\n        *args, **kwargs\r\n            Additional arguments and keywords for compatibility with NumPy.\r\n\r\n        Returns\r\n        -------\r\n        int\r\n            Row position of the {op}imum value.\r\n\r\n        See Also\r\n        --------\r\n        Series.arg{op} : Return position of the {op}imum value.\r\n        Series.arg{oppose} : Return position of the {oppose}imum value.\r\n        numpy.ndarray.arg{op} : Equivalent method for numpy arrays.\r\n        Series.idxmax : Return index label of the maximum values.\r\n        Series.idxmin : Return index label of the minimum values.\r\n\r\n        Examples\r\n        --------\r\n        Consider dataset containing cereal calories\r\n\r\n        >>> s = pd.Series({{'Corn Flakes': 100.0, 'Almond Delight': 110.0,\r\n        ...                'Cinnamon Toast Crunch': 120.0, 'Cocoa Puff': 110.0}})\r\n        >>> s\r\n        Corn Flakes              100.0\r\n        Almond Delight           110.0\r\n        Cinnamon Toast Crunch    120.0\r\n        Cocoa Puff               110.0\r\n        dtype: float64\r\n\r\n        >>> s.argmax()\r\n        2\r\n        >>> s.argmin()\r\n        0\r\n\r\n        The maximum cereal calories is the third element and\r\n        the minimum cereal calories is the first element,\r\n        since series is zero-indexed.\r\n        \"\"\"\r\n        delegate = self._values\r\n        nv.validate_minmax_axis(axis)\r\n        skipna = nv.validate_argmax_with_skipna(skipna, args, kwargs)\r\n\r\n        if isinstance(delegate, ExtensionArray):\r\n            if not skipna and delegate.isna().any():\r\n                return -1\r\n            else:\r\n                return delegate.argmax()\r\n        else:\r\n            # error: Incompatible return value type (got \"Union[int, ndarray]\", expected\r\n            # \"int\")\r\n            return nanops.nanargmax(  # type: ignore[return-value]\r\n                delegate, skipna=skipna\r\n            )\r\n\r\n    def min(self, axis: AxisInt | None = None, skipna: bool = True, *args, **kwargs):\r\n        \"\"\"\r\n        Return the minimum value of the Index.\r\n\r\n        Parameters\r\n        ----------\r\n        axis : {None}\r\n            Dummy argument for consistency with Series.\r\n        skipna : bool, default True\r\n            Exclude NA/null values when showing the result.\r\n        *args, **kwargs\r\n            Additional arguments and keywords for compatibility with NumPy.\r\n\r\n        Returns\r\n        -------\r\n        scalar\r\n            Minimum value.\r\n\r\n        See Also\r\n        --------\r\n        Index.max : Return the maximum value of the object.\r\n        Series.min : Return the minimum value in a Series.\r\n        DataFrame.min : Return the minimum values in a DataFrame.\r\n\r\n        Examples\r\n        --------\r\n        >>> idx = pd.Index([3, 2, 1])\r\n"]
[200.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[200.002, "i", "sed -n '360,520p' pandas/core/arrays/base.py\r"]
[200.004, "o", "sed -n '360,520p' pandas/core/arrays/base.py\r\n"]
[200.006, "o", "\u001b[?2004l\r\n        Parameters\r\n        ----------\r\n        key : int, ndarray, or slice\r\n            When called from, e.g. ``Series.__setitem__``, ``key`` will be\r\n            one of\r\n\r\n            * scalar int\r\n            * ndarray of integers.\r\n            * boolean ndarray\r\n            * slice object\r\n\r\n        value : ExtensionDtype.type, Sequence[ExtensionDtype.type], or object\r\n            value or values to be set of ``key``.\r\n\r\n        Returns\r\n        -------\r\n        None\r\n        \"\"\"\r\n        # Some notes to the ExtensionArray implementor who may have ended up\r\n        # here. While this method is not required for the interface, if you\r\n        # *do* choose to implement __setitem__, then some semantics should be\r\n        # observed:\r\n        #\r\n        # * Setting multiple values : ExtensionArrays should support setting\r\n        #   multiple values at once, 'key' will be a sequence of integers and\r\n        #  'value' will be a same-length sequence.\r\n        #\r\n        # * Broadcasting : For a sequence 'key' and a scalar 'value',\r\n        #   each position in 'key' should be set to 'value'.\r\n        #\r\n        # * Coercion : Most users will expect basic coercion to work. For\r\n        #   example, a string like '2018-01-01' is coerced to a datetime\r\n        #   when setting on a datetime64ns array. In general, if the\r\n        #   __init__ method coerces that value, then so should __setitem__\r\n        # Note, also, that Series/DataFrame.where internally use __setitem__\r\n        # on a copy of the data.\r\n        raise NotImplementedError(f\"{type(self)} does not implement __setitem__.\")\r\n\r\n    def __len__(self) -> int:\r\n        \"\"\"\r\n        Length of this array\r\n\r\n        Returns\r\n        -------\r\n        length : int\r\n        \"\"\"\r\n        raise AbstractMethodError(self)\r\n\r\n    def __iter__(self) -> Iterator[Any]:\r\n        \"\"\"\r\n        Iterate over elements of the array.\r\n        \"\"\"\r\n        # This needs to be implemented so that pandas recognizes extension\r\n        # arrays as list-like. The default implementation makes successive\r\n        # calls to ``__getitem__``, which may be slower than necessary.\r\n        for i in range(len(self)):\r\n            yield self[i]\r\n\r\n    def __contains__(self, item: object) -> bool | np.bool_:\r\n        \"\"\"\r\n        Return for `item in self`.\r\n        \"\"\"\r\n        # GH37867\r\n        # comparisons of any item to pd.NA always return pd.NA, so e.g. \"a\" in [pd.NA]\r\n        # would raise a TypeError. The implementation below works around that.\r\n        if is_scalar(item) and isna(item):\r\n            if not self._can_hold_na:\r\n                return False\r\n            elif item is self.dtype.na_value or isinstance(item, self.dtype.type):\r\n                return self._hasna\r\n            else:\r\n                return False\r\n        else:\r\n            # error: Item \"ExtensionArray\" of \"Union[ExtensionArray, ndarray]\" has no\r\n            # attribute \"any\"\r\n            return (item == self).any()  # type: ignore[union-attr]\r\n\r\n    # error: Signature of \"__eq__\" incompatible with supertype \"object\"\r\n    def __eq__(self, other: Any) -> ArrayLike:  # type: ignore[override]\r\n        \"\"\"\r\n        Return for `self == other` (element-wise equality).\r\n        \"\"\"\r\n        # Implementer note: this should return a boolean numpy ndarray or\r\n        # a boolean ExtensionArray.\r\n        # When `other` is one of Series, Index, or DataFrame, this method should\r\n        # return NotImplemented (to ensure that those objects are responsible for\r\n        # first unpacking the arrays, and then dispatch the operation to the\r\n        # underlying arrays)\r\n        raise AbstractMethodError(self)\r\n\r\n    # error: Signature of \"__ne__\" incompatible with supertype \"object\"\r\n    def __ne__(self, other: Any) -> ArrayLike:  # type: ignore[override]\r\n        \"\"\"\r\n        Return for `self != other` (element-wise in-equality).\r\n        \"\"\"\r\n        return ~(self == other)\r\n\r\n    def to_numpy(\r\n        self,\r\n        dtype: npt.DTypeLike | None = None,\r\n        copy: bool = False,\r\n        na_value: object = lib.no_default,\r\n    ) -> np.ndarray:\r\n        \"\"\"\r\n        Convert to a NumPy ndarray.\r\n\r\n        .. versionadded:: 1.0.0\r\n\r\n        This is similar to :meth:`numpy.asarray`, but may provide additional control\r\n        over how the conversion is done.\r\n\r\n        Parameters\r\n        ----------\r\n        dtype : str or numpy.dtype, optional\r\n            The dtype to pass to :meth:`numpy.asarray`.\r\n        copy : bool, default False\r\n            Whether to ensure that the returned value is a not a view on\r\n            another array. Note that ``copy=False`` does not *ensure* that\r\n            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\r\n            a copy is made, even if not strictly necessary.\r\n        na_value : Any, optional\r\n            The value to use for missing values. The default value depends\r\n            on `dtype` and the type of the array.\r\n\r\n        Returns\r\n        -------\r\n        numpy.ndarray\r\n        \"\"\"\r\n        result = np.asarray(self, dtype=dtype)\r\n        if copy or na_value is not lib.no_default:\r\n            result = result.copy()\r\n        if na_value is not lib.no_default:\r\n            result[self.isna()] = na_value\r\n        return result\r\n\r\n    # ------------------------------------------------------------------------\r\n    # Required attributes\r\n    # ------------------------------------------------------------------------\r\n\r\n    @property\r\n    def dtype(self) -> ExtensionDtype:\r\n        \"\"\"\r\n        An instance of 'ExtensionDtype'.\r\n        \"\"\"\r\n        raise AbstractMethodError(self)\r\n\r\n    @property\r\n    def shape(self) -> Shape:\r\n        \"\"\"\r\n        Return a tuple of the array dimensions.\r\n        \"\"\"\r\n        return (len(self),)\r\n\r\n    @property\r\n    def size(self) -> int:\r\n        \"\"\"\r\n        The number of elements in the array.\r\n        \"\"\"\r\n        # error: Incompatible return value type (got \"signedinteger[_64Bit]\",\r\n        # expected \"int\")  [return-value]\r\n        return np.prod(self.shape)  # type: ignore[return-value]\r\n"]
[205.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[205.002, "i", "sed -n '340,460p' pandas/core/arrays/numpy_.py\r"]
[205.004, "o", "sed -n '340,460p' pandas/core/arrays/numpy_.py\r\n"]
[205.006, "o", "\u001b[?2004l\r\n        axis: AxisInt | None = None,\r\n        dtype: NpDtype | None = None,\r\n        out=None,\r\n        ddof: int = 1,\r\n        keepdims: bool = False,\r\n        skipna: bool = True,\r\n    ):\r\n        nv.validate_stat_ddof_func(\r\n            (), {\"dtype\": dtype, \"out\": out, \"keepdims\": keepdims}, fname=\"sem\"\r\n        )\r\n        result = nanops.nansem(self._ndarray, axis=axis, skipna=skipna, ddof=ddof)\r\n        return self._wrap_reduction_result(axis, result)\r\n\r\n    def kurt(\r\n        self,\r\n        *,\r\n        axis: AxisInt | None = None,\r\n        dtype: NpDtype | None = None,\r\n        out=None,\r\n        keepdims: bool = False,\r\n        skipna: bool = True,\r\n    ):\r\n        nv.validate_stat_ddof_func(\r\n            (), {\"dtype\": dtype, \"out\": out, \"keepdims\": keepdims}, fname=\"kurt\"\r\n        )\r\n        result = nanops.nankurt(self._ndarray, axis=axis, skipna=skipna)\r\n        return self._wrap_reduction_result(axis, result)\r\n\r\n    def skew(\r\n        self,\r\n        *,\r\n        axis: AxisInt | None = None,\r\n        dtype: NpDtype | None = None,\r\n        out=None,\r\n        keepdims: bool = False,\r\n        skipna: bool = True,\r\n    ):\r\n        nv.validate_stat_ddof_func(\r\n            (), {\"dtype\": dtype, \"out\": out, \"keepdims\": keepdims}, fname=\"skew\"\r\n        )\r\n        result = nanops.nanskew(self._ndarray, axis=axis, skipna=skipna)\r\n        return self._wrap_reduction_result(axis, result)\r\n\r\n    # ------------------------------------------------------------------------\r\n    # Additional Methods\r\n\r\n    def to_numpy(\r\n        self,\r\n        dtype: npt.DTypeLike | None = None,\r\n        copy: bool = False,\r\n        na_value: object = lib.no_default,\r\n    ) -> np.ndarray:\r\n        mask = self.isna()\r\n        if na_value is not lib.no_default and mask.any():\r\n            result = self._ndarray.copy()\r\n            result[mask] = na_value\r\n        else:\r\n            result = self._ndarray\r\n\r\n        result = np.asarray(result, dtype=dtype)\r\n\r\n        if copy and result is self._ndarray:\r\n            result = result.copy()\r\n\r\n        return result\r\n\r\n    # ------------------------------------------------------------------------\r\n    # Ops\r\n\r\n    def __invert__(self) -> PandasArray:\r\n        return type(self)(~self._ndarray)\r\n\r\n    def __neg__(self) -> PandasArray:\r\n        return type(self)(-self._ndarray)\r\n\r\n    def __pos__(self) -> PandasArray:\r\n        return type(self)(+self._ndarray)\r\n\r\n    def __abs__(self) -> PandasArray:\r\n        return type(self)(abs(self._ndarray))\r\n\r\n    def _cmp_method(self, other, op):\r\n        if isinstance(other, PandasArray):\r\n            other = other._ndarray\r\n\r\n        other = ops.maybe_prepare_scalar_for_op(other, (len(self),))\r\n        pd_op = ops.get_array_op(op)\r\n        other = ensure_wrapped_if_datetimelike(other)\r\n        with np.errstate(all=\"ignore\"):\r\n            result = pd_op(self._ndarray, other)\r\n\r\n        if op is divmod or op is ops.rdivmod:\r\n            a, b = result\r\n            if isinstance(a, np.ndarray):\r\n                # for e.g. op vs TimedeltaArray, we may already\r\n                #  have an ExtensionArray, in which case we do not wrap\r\n                return self._wrap_ndarray_result(a), self._wrap_ndarray_result(b)\r\n            return a, b\r\n\r\n        if isinstance(result, np.ndarray):\r\n            # for e.g. multiplication vs TimedeltaArray, we may already\r\n            #  have an ExtensionArray, in which case we do not wrap\r\n            return self._wrap_ndarray_result(result)\r\n        return result\r\n\r\n    _arith_method = _cmp_method\r\n\r\n    def _wrap_ndarray_result(self, result: np.ndarray):\r\n        # If we have timedelta64[ns] result, return a TimedeltaArray instead\r\n        #  of a PandasArray\r\n        if result.dtype.kind == \"m\" and is_supported_unit(\r\n            get_unit_from_dtype(result.dtype)\r\n        ):\r\n            from pandas.core.arrays import TimedeltaArray\r\n\r\n            return TimedeltaArray._simple_new(result, dtype=result.dtype)\r\n        return type(self)(result)\r\n\r\n    # ------------------------------------------------------------------------\r\n    # String methods interface\r\n    _str_na_value = np.nan\r\n"]
[210.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[210.002, "i", "sed -n '420,520p' pandas/core/base.py\r"]
[210.004, "o", "sed -n '420,520p' pandas/core/base.py\r\n"]
[210.006, "o", "\u001b[?2004l\r\n        Categories (2, object): ['a', 'b']\r\n        \"\"\"\r\n        raise AbstractMethodError(self)\r\n\r\n    @final\r\n    def to_numpy(\r\n        self,\r\n        dtype: npt.DTypeLike | None = None,\r\n        copy: bool = False,\r\n        na_value: object = lib.no_default,\r\n        **kwargs,\r\n    ) -> np.ndarray:\r\n        \"\"\"\r\n        A NumPy ndarray representing the values in this Series or Index.\r\n\r\n        Parameters\r\n        ----------\r\n        dtype : str or numpy.dtype, optional\r\n            The dtype to pass to :meth:`numpy.asarray`.\r\n        copy : bool, default False\r\n            Whether to ensure that the returned value is not a view on\r\n            another array. Note that ``copy=False`` does not *ensure* that\r\n            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\r\n            a copy is made, even if not strictly necessary.\r\n        na_value : Any, optional\r\n            The value to use for missing values. The default value depends\r\n            on `dtype` and the type of the array.\r\n\r\n            .. versionadded:: 1.0.0\r\n\r\n        **kwargs\r\n            Additional keywords passed through to the ``to_numpy`` method\r\n            of the underlying array (for extension arrays).\r\n\r\n            .. versionadded:: 1.0.0\r\n\r\n        Returns\r\n        -------\r\n        numpy.ndarray\r\n\r\n        See Also\r\n        --------\r\n        Series.array : Get the actual data stored within.\r\n        Index.array : Get the actual data stored within.\r\n        DataFrame.to_numpy : Similar method for DataFrame.\r\n\r\n        Notes\r\n        -----\r\n        The returned array will be the same up to equality (values equal\r\n        in `self` will be equal in the returned array; likewise for values\r\n        that are not equal). When `self` contains an ExtensionArray, the\r\n        dtype may be different. For example, for a category-dtype Series,\r\n        ``to_numpy()`` will return a NumPy array and the categorical dtype\r\n        will be lost.\r\n\r\n        For NumPy dtypes, this will be a reference to the actual data stored\r\n        in this Series or Index (assuming ``copy=False``). Modifying the result\r\n        in place will modify the data stored in the Series or Index (not that\r\n        we recommend doing that).\r\n\r\n        For extension types, ``to_numpy()`` *may* require copying data and\r\n        coercing the result to a NumPy type (possibly object), which may be\r\n        expensive. When you need a no-copy reference to the underlying data,\r\n        :attr:`Series.array` should be used instead.\r\n\r\n        This table lays out the different dtypes and default return types of\r\n        ``to_numpy()`` for various dtypes within pandas.\r\n\r\n        ================== ================================\r\n        dtype              array type\r\n        ================== ================================\r\n        category[T]        ndarray[T] (same dtype as input)\r\n        period             ndarray[object] (Periods)\r\n        interval           ndarray[object] (Intervals)\r\n        IntegerNA          ndarray[object]\r\n        datetime64[ns]     datetime64[ns]\r\n        datetime64[ns, tz] ndarray[object] (Timestamps)\r\n        ================== ================================\r\n\r\n        Examples\r\n        --------\r\n        >>> ser = pd.Series(pd.Categorical(['a', 'b', 'a']))\r\n        >>> ser.to_numpy()\r\n        array(['a', 'b', 'a'], dtype=object)\r\n\r\n        Specify the `dtype` to control how datetime-aware data is represented.\r\n        Use ``dtype=object`` to return an ndarray of pandas :class:`Timestamp`\r\n        objects, each with the correct ``tz``.\r\n\r\n        >>> ser = pd.Series(pd.date_range('2000', periods=2, tz=\"CET\"))\r\n        >>> ser.to_numpy(dtype=object)\r\n        array([Timestamp('2000-01-01 00:00:00+0100', tz='CET'),\r\n               Timestamp('2000-01-02 00:00:00+0100', tz='CET')],\r\n              dtype=object)\r\n\r\n        Or ``dtype='datetime64[ns]'`` to return an ndarray of native\r\n        datetime64 values. The values are converted to UTC and the timezone\r\n        info is dropped.\r\n\r\n        >>> ser.to_numpy(dtype=\"datetime64[ns]\")\r\n        ... # doctest: +ELLIPSIS\r\n"]
[215.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[215.002, "i", "sed -n '520,680p' pandas/core/base.py\r"]
[215.004, "o", "sed -n '520,680p' pandas/core/base.py\r\n"]
[215.006, "o", "\u001b[?2004l\r\n        ... # doctest: +ELLIPSIS\r\n        array(['1999-12-31T23:00:00.000000000', '2000-01-01T23:00:00...'],\r\n              dtype='datetime64[ns]')\r\n        \"\"\"\r\n        if is_extension_array_dtype(self.dtype):\r\n            return self.array.to_numpy(dtype, copy=copy, na_value=na_value, **kwargs)\r\n        elif kwargs:\r\n            bad_keys = list(kwargs.keys())[0]\r\n            raise TypeError(\r\n                f\"to_numpy() got an unexpected keyword argument '{bad_keys}'\"\r\n            )\r\n\r\n        if na_value is not lib.no_default:\r\n            values = self._values\r\n            if not can_hold_element(values, na_value):\r\n                # if we can't hold the na_value asarray either makes a copy or we\r\n                # error before modifying values. The asarray later on thus won't make\r\n                # another copy\r\n                values = np.asarray(values, dtype=dtype)\r\n            else:\r\n                values = values.copy()\r\n\r\n            values[np.asanyarray(self.isna())] = na_value\r\n        else:\r\n            values = self._values\r\n\r\n        result = np.asarray(values, dtype=dtype)\r\n\r\n        if copy and na_value is lib.no_default:\r\n            if np.shares_memory(self._values[:2], result[:2]):\r\n                # Take slices to improve performance of check\r\n                result = result.copy()\r\n\r\n        return result\r\n\r\n    @final\r\n    @property\r\n    def empty(self) -> bool:\r\n        return not self.size\r\n\r\n    def max(self, axis: AxisInt | None = None, skipna: bool = True, *args, **kwargs):\r\n        \"\"\"\r\n        Return the maximum value of the Index.\r\n\r\n        Parameters\r\n        ----------\r\n        axis : int, optional\r\n            For compatibility with NumPy. Only 0 or None are allowed.\r\n        skipna : bool, default True\r\n            Exclude NA/null values when showing the result.\r\n        *args, **kwargs\r\n            Additional arguments and keywords for compatibility with NumPy.\r\n\r\n        Returns\r\n        -------\r\n        scalar\r\n            Maximum value.\r\n\r\n        See Also\r\n        --------\r\n        Index.min : Return the minimum value in an Index.\r\n        Series.max : Return the maximum value in a Series.\r\n        DataFrame.max : Return the maximum values in a DataFrame.\r\n\r\n        Examples\r\n        --------\r\n        >>> idx = pd.Index([3, 2, 1])\r\n        >>> idx.max()\r\n        3\r\n\r\n        >>> idx = pd.Index(['c', 'b', 'a'])\r\n        >>> idx.max()\r\n        'c'\r\n\r\n        For a MultiIndex, the maximum is determined lexicographically.\r\n\r\n        >>> idx = pd.MultiIndex.from_product([('a', 'b'), (2, 1)])\r\n        >>> idx.max()\r\n        ('b', 2)\r\n        \"\"\"\r\n        nv.validate_minmax_axis(axis)\r\n        nv.validate_max(args, kwargs)\r\n        return nanops.nanmax(self._values, skipna=skipna)\r\n\r\n    @doc(op=\"max\", oppose=\"min\", value=\"largest\")\r\n    def argmax(\r\n        self, axis: AxisInt | None = None, skipna: bool = True, *args, **kwargs\r\n    ) -> int:\r\n        \"\"\"\r\n        Return int position of the {value} value in the Series.\r\n\r\n        If the {op}imum is achieved in multiple locations,\r\n        the first row position is returned.\r\n\r\n        Parameters\r\n        ----------\r\n        axis : {{None}}\r\n            Unused. Parameter needed for compatibility with DataFrame.\r\n        skipna : bool, default True\r\n            Exclude NA/null values when showing the result.\r\n        *args, **kwargs\r\n            Additional arguments and keywords for compatibility with NumPy.\r\n\r\n        Returns\r\n        -------\r\n        int\r\n            Row position of the {op}imum value.\r\n\r\n        See Also\r\n        --------\r\n        Series.arg{op} : Return position of the {op}imum value.\r\n        Series.arg{oppose} : Return position of the {oppose}imum value.\r\n        numpy.ndarray.arg{op} : Equivalent method for numpy arrays.\r\n        Series.idxmax : Return index label of the maximum values.\r\n        Series.idxmin : Return index label of the minimum values.\r\n\r\n        Examples\r\n        --------\r\n        Consider dataset containing cereal calories\r\n\r\n        >>> s = pd.Series({{'Corn Flakes': 100.0, 'Almond Delight': 110.0,\r\n        ...                'Cinnamon Toast Crunch': 120.0, 'Cocoa Puff': 110.0}})\r\n        >>> s\r\n        Corn Flakes              100.0\r\n        Almond Delight           110.0\r\n        Cinnamon Toast Crunch    120.0\r\n        Cocoa Puff               110.0\r\n        dtype: float64\r\n\r\n        >>> s.argmax()\r\n        2\r\n        >>> s.argmin()\r\n        0\r\n\r\n        The maximum cereal calories is the third element and\r\n        the minimum cereal calories is the first element,\r\n        since series is zero-indexed.\r\n        \"\"\"\r\n        delegate = self._values\r\n        nv.validate_minmax_axis(axis)\r\n        skipna = nv.validate_argmax_with_skipna(skipna, args, kwargs)\r\n\r\n        if isinstance(delegate, ExtensionArray):\r\n            if not skipna and delegate.isna().any():\r\n                return -1\r\n            else:\r\n                return delegate.argmax()\r\n        else:\r\n            # error: Incompatible return value type (got \"Union[int, ndarray]\", expected\r\n            # \"int\")\r\n            return nanops.nanargmax(  # type: ignore[return-value]\r\n                delegate, skipna=skipna\r\n            )\r\n\r\n    def min(self, axis: AxisInt | None = None, skipna: bool = True, *args, **kwargs):\r\n        \"\"\"\r\n        Return the minimum value of the Index.\r\n\r\n        Parameters\r\n        ----------\r\n        axis : {None}\r\n"]
[220.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[220.002, "i", "sed -n '360,520p' pandas/core/arrays/base.py\r"]
[220.004, "o", "sed -n '360,520p' pandas/core/arrays/base.py\r\n"]
[220.006, "o", "\u001b[?2004l\r\n        Parameters\r\n        ----------\r\n        key : int, ndarray, or slice\r\n            When called from, e.g. ``Series.__setitem__``, ``key`` will be\r\n            one of\r\n\r\n            * scalar int\r\n            * ndarray of integers.\r\n            * boolean ndarray\r\n            * slice object\r\n\r\n        value : ExtensionDtype.type, Sequence[ExtensionDtype.type], or object\r\n            value or values to be set of ``key``.\r\n\r\n        Returns\r\n        -------\r\n        None\r\n        \"\"\"\r\n        # Some notes to the ExtensionArray implementor who may have ended up\r\n        # here. While this method is not required for the interface, if you\r\n        # *do* choose to implement __setitem__, then some semantics should be\r\n        # observed:\r\n        #\r\n        # * Setting multiple values : ExtensionArrays should support setting\r\n        #   multiple values at once, 'key' will be a sequence of integers and\r\n        #  'value' will be a same-length sequence.\r\n        #\r\n        # * Broadcasting : For a sequence 'key' and a scalar 'value',\r\n        #   each position in 'key' should be set to 'value'.\r\n        #\r\n        # * Coercion : Most users will expect basic coercion to work. For\r\n        #   example, a string like '2018-01-01' is coerced to a datetime\r\n        #   when setting on a datetime64ns array. In general, if the\r\n        #   __init__ method coerces that value, then so should __setitem__\r\n        # Note, also, that Series/DataFrame.where internally use __setitem__\r\n        # on a copy of the data.\r\n        raise NotImplementedError(f\"{type(self)} does not implement __setitem__.\")\r\n\r\n    def __len__(self) -> int:\r\n        \"\"\"\r\n        Length of this array\r\n\r\n        Returns\r\n        -------\r\n        length : int\r\n        \"\"\"\r\n        raise AbstractMethodError(self)\r\n\r\n    def __iter__(self) -> Iterator[Any]:\r\n        \"\"\"\r\n        Iterate over elements of the array.\r\n        \"\"\"\r\n        # This needs to be implemented so that pandas recognizes extension\r\n        # arrays as list-like. The default implementation makes successive\r\n        # calls to ``__getitem__``, which may be slower than necessary.\r\n        for i in range(len(self)):\r\n            yield self[i]\r\n\r\n    def __contains__(self, item: object) -> bool | np.bool_:\r\n        \"\"\"\r\n        Return for `item in self`.\r\n        \"\"\"\r\n        # GH37867\r\n        # comparisons of any item to pd.NA always return pd.NA, so e.g. \"a\" in [pd.NA]\r\n        # would raise a TypeError. The implementation below works around that.\r\n        if is_scalar(item) and isna(item):\r\n            if not self._can_hold_na:\r\n                return False\r\n            elif item is self.dtype.na_value or isinstance(item, self.dtype.type):\r\n                return self._hasna\r\n            else:\r\n                return False\r\n        else:\r\n            # error: Item \"ExtensionArray\" of \"Union[ExtensionArray, ndarray]\" has no\r\n            # attribute \"any\"\r\n            return (item == self).any()  # type: ignore[union-attr]\r\n\r\n    # error: Signature of \"__eq__\" incompatible with supertype \"object\"\r\n    def __eq__(self, other: Any) -> ArrayLike:  # type: ignore[override]\r\n        \"\"\"\r\n        Return for `self == other` (element-wise equality).\r\n        \"\"\"\r\n        # Implementer note: this should return a boolean numpy ndarray or\r\n        # a boolean ExtensionArray.\r\n        # When `other` is one of Series, Index, or DataFrame, this method should\r\n        # return NotImplemented (to ensure that those objects are responsible for\r\n        # first unpacking the arrays, and then dispatch the operation to the\r\n        # underlying arrays)\r\n        raise AbstractMethodError(self)\r\n\r\n    # error: Signature of \"__ne__\" incompatible with supertype \"object\"\r\n    def __ne__(self, other: Any) -> ArrayLike:  # type: ignore[override]\r\n        \"\"\"\r\n        Return for `self != other` (element-wise in-equality).\r\n        \"\"\"\r\n        return ~(self == other)\r\n\r\n    def to_numpy(\r\n        self,\r\n        dtype: npt.DTypeLike | None = None,\r\n        copy: bool = False,\r\n        na_value: object = lib.no_default,\r\n    ) -> np.ndarray:\r\n        \"\"\"\r\n        Convert to a NumPy ndarray.\r\n\r\n        .. versionadded:: 1.0.0\r\n\r\n        This is similar to :meth:`numpy.asarray`, but may provide additional control\r\n        over how the conversion is done.\r\n\r\n        Parameters\r\n        ----------\r\n        dtype : str or numpy.dtype, optional\r\n            The dtype to pass to :meth:`numpy.asarray`.\r\n        copy : bool, default False\r\n            Whether to ensure that the returned value is a not a view on\r\n            another array. Note that ``copy=False`` does not *ensure* that\r\n            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\r\n            a copy is made, even if not strictly necessary.\r\n        na_value : Any, optional\r\n            The value to use for missing values. The default value depends\r\n            on `dtype` and the type of the array.\r\n\r\n        Returns\r\n        -------\r\n        numpy.ndarray\r\n        \"\"\"\r\n        result = np.asarray(self, dtype=dtype)\r\n        if copy or na_value is not lib.no_default:\r\n            result = result.copy()\r\n        if na_value is not lib.no_default:\r\n            result[self.isna()] = na_value\r\n        return result\r\n\r\n    # ------------------------------------------------------------------------\r\n    # Required attributes\r\n    # ------------------------------------------------------------------------\r\n\r\n    @property\r\n    def dtype(self) -> ExtensionDtype:\r\n        \"\"\"\r\n        An instance of 'ExtensionDtype'.\r\n        \"\"\"\r\n        raise AbstractMethodError(self)\r\n\r\n    @property\r\n    def shape(self) -> Shape:\r\n        \"\"\"\r\n        Return a tuple of the array dimensions.\r\n        \"\"\"\r\n        return (len(self),)\r\n\r\n    @property\r\n    def size(self) -> int:\r\n        \"\"\"\r\n        The number of elements in the array.\r\n        \"\"\"\r\n        # error: Incompatible return value type (got \"signedinteger[_64Bit]\",\r\n        # expected \"int\")  [return-value]\r\n        return np.prod(self.shape)  # type: ignore[return-value]\r\n"]
[225.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[225.002, "i", "sed -n '360,460p' pandas/core/arrays/numpy_.py\r"]
[225.004, "o", "sed -n '360,460p' pandas/core/arrays/numpy_.py\r\n"]
[225.006, "o", "\u001b[?2004l\r\n        skipna: bool = True,\r\n    ):\r\n        nv.validate_stat_ddof_func(\r\n            (), {\"dtype\": dtype, \"out\": out, \"keepdims\": keepdims}, fname=\"kurt\"\r\n        )\r\n        result = nanops.nankurt(self._ndarray, axis=axis, skipna=skipna)\r\n        return self._wrap_reduction_result(axis, result)\r\n\r\n    def skew(\r\n        self,\r\n        *,\r\n        axis: AxisInt | None = None,\r\n        dtype: NpDtype | None = None,\r\n        out=None,\r\n        keepdims: bool = False,\r\n        skipna: bool = True,\r\n    ):\r\n        nv.validate_stat_ddof_func(\r\n            (), {\"dtype\": dtype, \"out\": out, \"keepdims\": keepdims}, fname=\"skew\"\r\n        )\r\n        result = nanops.nanskew(self._ndarray, axis=axis, skipna=skipna)\r\n        return self._wrap_reduction_result(axis, result)\r\n\r\n    # ------------------------------------------------------------------------\r\n    # Additional Methods\r\n\r\n    def to_numpy(\r\n        self,\r\n        dtype: npt.DTypeLike | None = None,\r\n        copy: bool = False,\r\n        na_value: object = lib.no_default,\r\n    ) -> np.ndarray:\r\n        mask = self.isna()\r\n        if na_value is not lib.no_default and mask.any():\r\n            result = self._ndarray.copy()\r\n            result[mask] = na_value\r\n        else:\r\n            result = self._ndarray\r\n\r\n        result = np.asarray(result, dtype=dtype)\r\n\r\n        if copy and result is self._ndarray:\r\n            result = result.copy()\r\n\r\n        return result\r\n\r\n    # ------------------------------------------------------------------------\r\n    # Ops\r\n\r\n    def __invert__(self) -> PandasArray:\r\n        return type(self)(~self._ndarray)\r\n\r\n    def __neg__(self) -> PandasArray:\r\n        return type(self)(-self._ndarray)\r\n\r\n    def __pos__(self) -> PandasArray:\r\n        return type(self)(+self._ndarray)\r\n\r\n    def __abs__(self) -> PandasArray:\r\n        return type(self)(abs(self._ndarray))\r\n\r\n    def _cmp_method(self, other, op):\r\n        if isinstance(other, PandasArray):\r\n            other = other._ndarray\r\n\r\n        other = ops.maybe_prepare_scalar_for_op(other, (len(self),))\r\n        pd_op = ops.get_array_op(op)\r\n        other = ensure_wrapped_if_datetimelike(other)\r\n        with np.errstate(all=\"ignore\"):\r\n            result = pd_op(self._ndarray, other)\r\n\r\n        if op is divmod or op is ops.rdivmod:\r\n            a, b = result\r\n            if isinstance(a, np.ndarray):\r\n                # for e.g. op vs TimedeltaArray, we may already\r\n                #  have an ExtensionArray, in which case we do not wrap\r\n                return self._wrap_ndarray_result(a), self._wrap_ndarray_result(b)\r\n            return a, b\r\n\r\n        if isinstance(result, np.ndarray):\r\n            # for e.g. multiplication vs TimedeltaArray, we may already\r\n            #  have an ExtensionArray, in which case we do not wrap\r\n            return self._wrap_ndarray_result(result)\r\n        return result\r\n\r\n    _arith_method = _cmp_method\r\n\r\n    def _wrap_ndarray_result(self, result: np.ndarray):\r\n        # If we have timedelta64[ns] result, return a TimedeltaArray instead\r\n        #  of a PandasArray\r\n        if result.dtype.kind == \"m\" and is_supported_unit(\r\n            get_unit_from_dtype(result.dtype)\r\n        ):\r\n            from pandas.core.arrays import TimedeltaArray\r\n\r\n            return TimedeltaArray._simple_new(result, dtype=result.dtype)\r\n        return type(self)(result)\r\n\r\n    # ------------------------------------------------------------------------\r\n    # String methods interface\r\n    _str_na_value = np.nan\r\n"]
[230.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[230.002, "i", "nl -ba pandas/core/base.py | sed -n '380,520p'\r"]
[230.004, "o", "nl -ba pandas/core/base.py | sed -n '380,520p'\r\n"]
[230.006, "o", "\u001b[?2004l\r\n   380\t        -----\r\n   381\t        This table lays out the different array types for each extension\r\n   382\t        dtype within pandas.\r\n   383\t\r\n   384\t        ================== =============================\r\n   385\t        dtype              array type\r\n   386\t        ================== =============================\r\n   387\t        category           Categorical\r\n   388\t        period             PeriodArray\r\n   389\t        interval           IntervalArray\r\n   390\t        IntegerNA          IntegerArray\r\n   391\t        string             StringArray\r\n   392\t        boolean            BooleanArray\r\n   393\t        datetime64[ns, tz] DatetimeArray\r\n   394\t        ================== =============================\r\n   395\t\r\n   396\t        For any 3rd-party extension types, the array type will be an\r\n   397\t        ExtensionArray.\r\n   398\t\r\n   399\t        For all remaining dtypes ``.array`` will be a\r\n   400\t        :class:`arrays.NumpyExtensionArray` wrapping the actual ndarray\r\n   401\t        stored within. If you absolutely need a NumPy array (possibly with\r\n   402\t        copying / coercing data), then use :meth:`Series.to_numpy` instead.\r\n   403\t\r\n   404\t        Examples\r\n   405\t        --------\r\n   406\t        For regular NumPy types like int, and float, a PandasArray\r\n   407\t        is returned.\r\n   408\t\r\n   409\t        >>> pd.Series([1, 2, 3]).array\r\n   410\t        <PandasArray>\r\n   411\t        [1, 2, 3]\r\n   412\t        Length: 3, dtype: int64\r\n   413\t\r\n   414\t        For extension types, like Categorical, the actual ExtensionArray\r\n   415\t        is returned\r\n   416\t\r\n   417\t        >>> ser = pd.Series(pd.Categorical(['a', 'b', 'a']))\r\n   418\t        >>> ser.array\r\n   419\t        ['a', 'b', 'a']\r\n   420\t        Categories (2, object): ['a', 'b']\r\n   421\t        \"\"\"\r\n   422\t        raise AbstractMethodError(self)\r\n   423\t\r\n   424\t    @final\r\n   425\t    def to_numpy(\r\n   426\t        self,\r\n   427\t        dtype: npt.DTypeLike | None = None,\r\n   428\t        copy: bool = False,\r\n   429\t        na_value: object = lib.no_default,\r\n   430\t        **kwargs,\r\n   431\t    ) -> np.ndarray:\r\n   432\t        \"\"\"\r\n   433\t        A NumPy ndarray representing the values in this Series or Index.\r\n   434\t\r\n   435\t        Parameters\r\n   436\t        ----------\r\n   437\t        dtype : str or numpy.dtype, optional\r\n   438\t            The dtype to pass to :meth:`numpy.asarray`.\r\n   439\t        copy : bool, default False\r\n   440\t            Whether to ensure that the returned value is not a view on\r\n   441\t            another array. Note that ``copy=False`` does not *ensure* that\r\n   442\t            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\r\n   443\t            a copy is made, even if not strictly necessary.\r\n   444\t        na_value : Any, optional\r\n   445\t            The value to use for missing values. The default value depends\r\n   446\t            on `dtype` and the type of the array.\r\n   447\t\r\n   448\t            .. versionadded:: 1.0.0\r\n   449\t\r\n   450\t        **kwargs\r\n   451\t            Additional keywords passed through to the ``to_numpy`` method\r\n   452\t            of the underlying array (for extension arrays).\r\n   453\t\r\n   454\t            .. versionadded:: 1.0.0\r\n   455\t\r\n   456\t        Returns\r\n   457\t        -------\r\n   458\t        numpy.ndarray\r\n   459\t\r\n   460\t        See Also\r\n   461\t        --------\r\n   462\t        Series.array : Get the actual data stored within.\r\n   463\t        Index.array : Get the actual data stored within.\r\n   464\t        DataFrame.to_numpy : Similar method for DataFrame.\r\n   465\t\r\n   466\t        Notes\r\n   467\t        -----\r\n   468\t        The returned array will be the same up to equality (values equal\r\n   469\t        in `self` will be equal in the returned array; likewise for values\r\n   470\t        that are not equal). When `self` contains an ExtensionArray, the\r\n   471\t        dtype may be different. For example, for a category-dtype Series,\r\n   472\t        ``to_numpy()`` will return a NumPy array and the categorical dtype\r\n   473\t        will be lost.\r\n   474\t\r\n   475\t        For NumPy dtypes, this will be a reference to the actual data stored\r\n   476\t        in this Series or Index (assuming ``copy=False``). Modifying the result\r\n   477\t        in place will modify the data stored in the Series or Index (not that\r\n   478\t        we recommend doing that).\r\n   479\t\r\n   480\t        For extension types, ``to_numpy()`` *may* require copying data and\r\n   481\t        coercing the result to a NumPy type (possibly object), which may be\r\n   482\t        expensive. When you need a no-copy reference to the underlying data,\r\n   483\t        :attr:`Series.array` should be used instead.\r\n   484\t\r\n   485\t        This table lays out the different dtypes and default return types of\r\n   486\t        ``to_numpy()`` for various dtypes within pandas.\r\n   487\t\r\n   488\t        ================== ================================\r\n   489\t        dtype              array type\r\n   490\t        ================== ================================\r\n   491\t        category[T]        ndarray[T] (same dtype as input)\r\n   492\t        period             ndarray[object] (Periods)\r\n   493\t        interval           ndarray[object] (Intervals)\r\n   494\t        IntegerNA          ndarray[object]\r\n   495\t        datetime64[ns]     datetime64[ns]\r\n   496\t        datetime64[ns, tz] ndarray[object] (Timestamps)\r\n   497\t        ================== ================================\r\n   498\t\r\n   499\t        Examples\r\n   500\t        --------\r\n   501\t        >>> ser = pd.Series(pd.Categorical(['a', 'b', 'a']))\r\n   502\t        >>> ser.to_numpy()\r\n   503\t        array(['a', 'b', 'a'], dtype=object)\r\n   504\t\r\n   505\t        Specify the `dtype` to control how datetime-aware data is represented.\r\n   506\t        Use ``dtype=object`` to return an ndarray of pandas :class:`Timestamp`\r\n   507\t        objects, each with the correct ``tz``.\r\n   508\t\r\n   509\t        >>> ser = pd.Series(pd.date_range('2000', periods=2, tz=\"CET\"))\r\n   510\t        >>> ser.to_numpy(dtype=object)\r\n   511\t        array([Timestamp('2000-01-01 00:00:00+0100', tz='CET'),\r\n   512\t               Timestamp('2000-01-02 00:00:00+0100', tz='CET')],\r\n   513\t              dtype=object)\r\n   514\t\r\n   515\t        Or ``dtype='datetime64[ns]'`` to return an ndarray of native\r\n   516\t        datetime64 values. The values are converted to UTC and the timezone\r\n   517\t        info is dropped.\r\n   518\t\r\n   519\t        >>> ser.to_numpy(dtype=\"datetime64[ns]\")\r\n   520\t        ... # doctest: +ELLIPSIS\r\n"]
[235.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[235.002, "i", "nl -ba pandas/core/arrays/base.py | sed -n '420,520p'\r"]
[235.004, "o", "nl -ba pandas/core/arrays/base.py | sed -n '420,520p'\r\n"]
[235.006, "o", "\u001b[?2004l\r\n   420\t        Return for `item in self`.\r\n   421\t        \"\"\"\r\n   422\t        # GH37867\r\n   423\t        # comparisons of any item to pd.NA always return pd.NA, so e.g. \"a\" in [pd.NA]\r\n   424\t        # would raise a TypeError. The implementation below works around that.\r\n   425\t        if is_scalar(item) and isna(item):\r\n   426\t            if not self._can_hold_na:\r\n   427\t                return False\r\n   428\t            elif item is self.dtype.na_value or isinstance(item, self.dtype.type):\r\n   429\t                return self._hasna\r\n   430\t            else:\r\n   431\t                return False\r\n   432\t        else:\r\n   433\t            # error: Item \"ExtensionArray\" of \"Union[ExtensionArray, ndarray]\" has no\r\n   434\t            # attribute \"any\"\r\n   435\t            return (item == self).any()  # type: ignore[union-attr]\r\n   436\t\r\n   437\t    # error: Signature of \"__eq__\" incompatible with supertype \"object\"\r\n   438\t    def __eq__(self, other: Any) -> ArrayLike:  # type: ignore[override]\r\n   439\t        \"\"\"\r\n   440\t        Return for `self == other` (element-wise equality).\r\n   441\t        \"\"\"\r\n   442\t        # Implementer note: this should return a boolean numpy ndarray or\r\n   443\t        # a boolean ExtensionArray.\r\n   444\t        # When `other` is one of Series, Index, or DataFrame, this method should\r\n   445\t        # return NotImplemented (to ensure that those objects are responsible for\r\n   446\t        # first unpacking the arrays, and then dispatch the operation to the\r\n   447\t        # underlying arrays)\r\n   448\t        raise AbstractMethodError(self)\r\n   449\t\r\n   450\t    # error: Signature of \"__ne__\" incompatible with supertype \"object\"\r\n   451\t    def __ne__(self, other: Any) -> ArrayLike:  # type: ignore[override]\r\n   452\t        \"\"\"\r\n   453\t        Return for `self != other` (element-wise in-equality).\r\n   454\t        \"\"\"\r\n   455\t        return ~(self == other)\r\n   456\t\r\n   457\t    def to_numpy(\r\n   458\t        self,\r\n   459\t        dtype: npt.DTypeLike | None = None,\r\n   460\t        copy: bool = False,\r\n   461\t        na_value: object = lib.no_default,\r\n   462\t    ) -> np.ndarray:\r\n   463\t        \"\"\"\r\n   464\t        Convert to a NumPy ndarray.\r\n   465\t\r\n   466\t        .. versionadded:: 1.0.0\r\n   467\t\r\n   468\t        This is similar to :meth:`numpy.asarray`, but may provide additional control\r\n   469\t        over how the conversion is done.\r\n   470\t\r\n   471\t        Parameters\r\n   472\t        ----------\r\n   473\t        dtype : str or numpy.dtype, optional\r\n   474\t            The dtype to pass to :meth:`numpy.asarray`.\r\n   475\t        copy : bool, default False\r\n   476\t            Whether to ensure that the returned value is a not a view on\r\n   477\t            another array. Note that ``copy=False`` does not *ensure* that\r\n   478\t            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\r\n   479\t            a copy is made, even if not strictly necessary.\r\n   480\t        na_value : Any, optional\r\n   481\t            The value to use for missing values. The default value depends\r\n   482\t            on `dtype` and the type of the array.\r\n   483\t\r\n   484\t        Returns\r\n   485\t        -------\r\n   486\t        numpy.ndarray\r\n   487\t        \"\"\"\r\n   488\t        result = np.asarray(self, dtype=dtype)\r\n   489\t        if copy or na_value is not lib.no_default:\r\n   490\t            result = result.copy()\r\n   491\t        if na_value is not lib.no_default:\r\n   492\t            result[self.isna()] = na_value\r\n   493\t        return result\r\n   494\t\r\n   495\t    # ------------------------------------------------------------------------\r\n   496\t    # Required attributes\r\n   497\t    # ------------------------------------------------------------------------\r\n   498\t\r\n   499\t    @property\r\n   500\t    def dtype(self) -> ExtensionDtype:\r\n   501\t        \"\"\"\r\n   502\t        An instance of 'ExtensionDtype'.\r\n   503\t        \"\"\"\r\n   504\t        raise AbstractMethodError(self)\r\n   505\t\r\n   506\t    @property\r\n   507\t    def shape(self) -> Shape:\r\n   508\t        \"\"\"\r\n   509\t        Return a tuple of the array dimensions.\r\n   510\t        \"\"\"\r\n   511\t        return (len(self),)\r\n   512\t\r\n   513\t    @property\r\n   514\t    def size(self) -> int:\r\n   515\t        \"\"\"\r\n   516\t        The number of elements in the array.\r\n   517\t        \"\"\"\r\n   518\t        # error: Incompatible return value type (got \"signedinteger[_64Bit]\",\r\n   519\t        # expected \"int\")  [return-value]\r\n   520\t        return np.prod(self.shape)  # type: ignore[return-value]\r\n"]
[240.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[240.002, "i", "nl -ba pandas/core/arrays/numpy_.py | sed -n '340,460p'\r"]
[240.004, "o", "nl -ba pandas/core/arrays/numpy_.py | sed -n '340,460p'\r\n"]
[240.006, "o", "\u001b[?2004l\r\n   340\t        axis: AxisInt | None = None,\r\n   341\t        dtype: NpDtype | None = None,\r\n   342\t        out=None,\r\n   343\t        ddof: int = 1,\r\n   344\t        keepdims: bool = False,\r\n   345\t        skipna: bool = True,\r\n   346\t    ):\r\n   347\t        nv.validate_stat_ddof_func(\r\n   348\t            (), {\"dtype\": dtype, \"out\": out, \"keepdims\": keepdims}, fname=\"sem\"\r\n   349\t        )\r\n   350\t        result = nanops.nansem(self._ndarray, axis=axis, skipna=skipna, ddof=ddof)\r\n   351\t        return self._wrap_reduction_result(axis, result)\r\n   352\t\r\n   353\t    def kurt(\r\n   354\t        self,\r\n   355\t        *,\r\n   356\t        axis: AxisInt | None = None,\r\n   357\t        dtype: NpDtype | None = None,\r\n   358\t        out=None,\r\n   359\t        keepdims: bool = False,\r\n   360\t        skipna: bool = True,\r\n   361\t    ):\r\n   362\t        nv.validate_stat_ddof_func(\r\n   363\t            (), {\"dtype\": dtype, \"out\": out, \"keepdims\": keepdims}, fname=\"kurt\"\r\n   364\t        )\r\n   365\t        result = nanops.nankurt(self._ndarray, axis=axis, skipna=skipna)\r\n   366\t        return self._wrap_reduction_result(axis, result)\r\n   367\t\r\n   368\t    def skew(\r\n   369\t        self,\r\n   370\t        *,\r\n   371\t        axis: AxisInt | None = None,\r\n   372\t        dtype: NpDtype | None = None,\r\n   373\t        out=None,\r\n   374\t        keepdims: bool = False,\r\n   375\t        skipna: bool = True,\r\n   376\t    ):\r\n   377\t        nv.validate_stat_ddof_func(\r\n   378\t            (), {\"dtype\": dtype, \"out\": out, \"keepdims\": keepdims}, fname=\"skew\"\r\n   379\t        )\r\n   380\t        result = nanops.nanskew(self._ndarray, axis=axis, skipna=skipna)\r\n   381\t        return self._wrap_reduction_result(axis, result)\r\n   382\t\r\n   383\t    # ------------------------------------------------------------------------\r\n   384\t    # Additional Methods\r\n   385\t\r\n   386\t    def to_numpy(\r\n   387\t        self,\r\n   388\t        dtype: npt.DTypeLike | None = None,\r\n   389\t        copy: bool = False,\r\n   390\t        na_value: object = lib.no_default,\r\n   391\t    ) -> np.ndarray:\r\n   392\t        mask = self.isna()\r\n   393\t        if na_value is not lib.no_default and mask.any():\r\n   394\t            result = self._ndarray.copy()\r\n   395\t            result[mask] = na_value\r\n   396\t        else:\r\n   397\t            result = self._ndarray\r\n   398\t\r\n   399\t        result = np.asarray(result, dtype=dtype)\r\n   400\t\r\n   401\t        if copy and result is self._ndarray:\r\n   402\t            result = result.copy()\r\n   403\t\r\n   404\t        return result\r\n   405\t\r\n   406\t    # ------------------------------------------------------------------------\r\n   407\t    # Ops\r\n   408\t\r\n   409\t    def __invert__(self) -> PandasArray:\r\n   410\t        return type(self)(~self._ndarray)\r\n   411\t\r\n   412\t    def __neg__(self) -> PandasArray:\r\n   413\t        return type(self)(-self._ndarray)\r\n   414\t\r\n   415\t    def __pos__(self) -> PandasArray:\r\n   416\t        return type(self)(+self._ndarray)\r\n   417\t\r\n   418\t    def __abs__(self) -> PandasArray:\r\n   419\t        return type(self)(abs(self._ndarray))\r\n   420\t\r\n   421\t    def _cmp_method(self, other, op):\r\n   422\t        if isinstance(other, PandasArray):\r\n   423\t            other = other._ndarray\r\n   424\t\r\n   425\t        other = ops.maybe_prepare_scalar_for_op(other, (len(self),))\r\n   426\t        pd_op = ops.get_array_op(op)\r\n   427\t        other = ensure_wrapped_if_datetimelike(other)\r\n   428\t        with np.errstate(all=\"ignore\"):\r\n   429\t            result = pd_op(self._ndarray, other)\r\n   430\t\r\n   431\t        if op is divmod or op is ops.rdivmod:\r\n   432\t            a, b = result\r\n   433\t            if isinstance(a, np.ndarray):\r\n   434\t                # for e.g. op vs TimedeltaArray, we may already\r\n   435\t                #  have an ExtensionArray, in which case we do not wrap\r\n   436\t                return self._wrap_ndarray_result(a), self._wrap_ndarray_result(b)\r\n   437\t            return a, b\r\n   438\t\r\n   439\t        if isinstance(result, np.ndarray):\r\n   440\t            # for e.g. multiplication vs TimedeltaArray, we may already\r\n   441\t            #  have an ExtensionArray, in which case we do not wrap\r\n   442\t            return self._wrap_ndarray_result(result)\r\n   443\t        return result\r\n   444\t\r\n   445\t    _arith_method = _cmp_method\r\n   446\t\r\n   447\t    def _wrap_ndarray_result(self, result: np.ndarray):\r\n   448\t        # If we have timedelta64[ns] result, return a TimedeltaArray instead\r\n   449\t        #  of a PandasArray\r\n   450\t        if result.dtype.kind == \"m\" and is_supported_unit(\r\n   451\t            get_unit_from_dtype(result.dtype)\r\n   452\t        ):\r\n   453\t            from pandas.core.arrays import TimedeltaArray\r\n   454\t\r\n   455\t            return TimedeltaArray._simple_new(result, dtype=result.dtype)\r\n   456\t        return type(self)(result)\r\n   457\t\r\n   458\t    # ------------------------------------------------------------------------\r\n   459\t    # String methods interface\r\n   460\t    _str_na_value = np.nan\r\n"]
[245.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[245.002, "i", "applypatch << 'PATCH'\r"]
[245.004, "o", "applypatch << 'PATCH'\r\n"]
[245.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> *** Begin Patch\r\n\u001b[?2004l\r\n\u001b[?2004h> *** Update File: pandas/core/base.py\r\n\u001b[?2004l\r\n\u001b[?2004h> @@\r\n\u001b[?2004l\r\n\u001b[?2004h> -    def to_numpy(\r\n\u001b[?2004l\r\n\u001b[?2004h> -        self,\r\n\u001b[?2004l\r\n\u001b[?2004h> -        dtype: npt.DTypeLike | None = None,\r\n\u001b[?2004l\r\n\u001b[?2004h> -        copy: bool = False,\r\n\u001b[?2004l\r\n\u001b[?2004h> -        na_value: object = lib.no_default,\r\n\u001b[?2004l\r\n\u001b[?2004h> -        **kwargs,\r\n\u001b[?2004l\r\n\u001b[?2004h> -    ) -> np.ndarray:\r\n\u001b[?2004l\r\n\u001b[?2004h> +    def to_numpy(\r\n\u001b[?2004l\r\n\u001b[?2004h> +        self,\r\n\u001b[?2004l\r\n\u001b[?2004h> +        dtype: npt.DTypeLike | None = None,\r\n\u001b[?2004l\r\n\u001b[?2004h> +        copy: bool = False,\r\n\u001b[?2004l\r\n\u001b[?2004h> +        na_value: object = lib.no_default,\r\n\u001b[?2004l\r\n\u001b[?2004h> +        **kwargs,\r\n\u001b[?2004l\r\n\u001b[?2004h> +    ) -> np.ndarray:\r\n\u001b[?2004l\r\n\u001b[?2004h> @@\r\n\u001b[?2004l\r\n\u001b[?2004h> -        Returns\r\n\u001b[?2004l\r\n\u001b[?2004h> +        Returns\r\n\u001b[?2004l\r\n\u001b[?2004h>          -------\r\n\u001b[?2004l\r\n\u001b[?2004h>          numpy.ndarray\r\n\u001b[?2004l\r\n\u001b[?2004h> @@\r\n\u001b[?2004l\r\n\u001b[?2004h> -        For NumPy dtypes, this will be a reference to the actual data stored\r\n\u001b[?2004l\r\n\u001b[?2004h> -        in this Series or Index (assuming ``copy=False``). Modifying the result\r\n\u001b[?2004l\r\n\u001b[?2004h> -        in place will modify the data stored in the Series or Index (not that\r\n\u001b[?2004l\r\n\u001b[?2004h> -        we recommend doing that).\r\n\u001b[?2004l\r\n\u001b[?2004h> +        For NumPy dtypes, this will be a reference to the actual data stored\r\n\u001b[?2004l\r\n\u001b[?2004h> +        in this Series or Index (assuming ``copy=False``). Modifying the result\r\n\u001b[?2004l\r\n\u001b[?2004h> +        in place will modify the data stored in the Series or Index (not that\r\n\u001b[?2004l\r\n\u001b[?2004h> +        we recommend doing that).\r\n\u001b[?2004l\r\n\u001b[?2004h> +\r\n\u001b[?2004l\r\n\u001b[?2004h> +        Performance notes\r\n\u001b[?2004l\r\n\u001b[?2004h> +        -----------------\r\n\u001b[?2004l\r\n\u001b[?2004h> +        When the underlying data are NumPy-backed and ``dtype`` is ``None``,\r\n\u001b[?2004l\r\n\u001b[?2004h> +        ``copy=True`` can avoid an intermediate no-op asarray/shares_memory\r\n\u001b[?2004l\r\n\u001b[?2004h> +        check by copying the underlying ndarray directly. This reduces a small\r\n\u001b[?2004l\r\n\u001b[?2004h> +        amount of Python-level overhead while preserving identical semantics.\r\n\u001b[?2004l\r\n\u001b[?2004h> @@\r\n\u001b[?2004l\r\n\u001b[?2004h> -        \"\"\"\r\n\u001b[?2004l\r\n\u001b[?2004h> -        if dtype is not None:\r\n\u001b[?2004l\r\n\u001b[?2004h> -            dtype = np.dtype(dtype)\r\n\u001b[?2004l\r\n\u001b[?2004h> -        result = self._mgr.as_array(dtype=dtype, copy=copy, na_value=na_value)\r\n\u001b[?2004l\r\n\u001b[?2004h> +        \"\"\"\r\n\u001b[?2004l\r\n\u001b[?2004h> +        if dtype is not None:\r\n\u001b[?2004l\r\n\u001b[?2004h> +            dtype = np.dtype(dtype)\r\n\u001b[?2004l\r\n\u001b[?2004h> +\r\n\u001b[?2004l\r\n\u001b[?2004h> +        # Fastpath: for NumPy-backed data with no dtype coercion requested,\r\n\u001b[?2004l\r\n\u001b[?2004h> +        # honor copy semantics by returning a direct ndarray copy. This avoids\r\n\u001b[?2004l\r\n\u001b[?2004h> +        # an intermediate asarray() call and associated shares_memory checks.\r\n\u001b[?2004l\r\n\u001b[?2004h> +        if (\r\n\u001b[?2004l\r\n\u001b[?2004h> +            copy\r\n\u001b[?2004l\r\n\u001b[?2004h> +            and dtype is None\r\n\u001b[?2004l\r\n\u001b[?2004h> +            and hasattr(self, \"_mgr\")\r\n\u001b[?2004l\r\n\u001b[?2004h> +            and hasattr(self._mgr, \"internal_values\")\r\n\u001b[?2004l\r\n\u001b[?2004h> +        ):\r\n\u001b[?2004l\r\n\u001b[?2004h> +            vals = self._mgr.internal_values()\r\n\u001b[?2004l\r\n\u001b[?2004h> +            # PandasArray over a NumPy ndarray or a plain ndarray\r\n\u001b[?2004l\r\n\u001b[?2004h> +            if isinstance(vals, np.ndarray):\r\n\u001b[?2004l\r\n\u001b[?2004h> +                return vals.copy(order=\"K\")\r\n\u001b[?2004l\r\n\u001b[?2004h> +            # PandasArray-like wrapper exposing .to_numpy with no dtype change\r\n\u001b[?2004l\r\n\u001b[?2004h> +            arr = getattr(vals, \"_ndarray\", None)\r\n\u001b[?2004l\r\n\u001b[?2004h> +            if isinstance(arr, np.ndarray):\r\n\u001b[?2004l\r\n\u001b[?2004h> +                return arr.copy(order=\"K\")\r\n\u001b[?2004l\r\n\u001b[?2004h> +\r\n\u001b[?2004l\r\n\u001b[?2004h> +        result = self._mgr.as_array(dtype=dtype, copy=copy, na_value=na_value)\r\n\u001b[?2004l\r\n\u001b[?2004h>          if result.dtype == np.dtype(\"object\"):\r\n\u001b[?2004l\r\n\u001b[?2004h>              # TODO: can we get rid of the dtype  check above?\r\n\u001b[?2004l\r\n\u001b[?2004h>              result = cast(np.ndarray, result)\r\n\u001b[?2004l\r\n\u001b[?2004h>              result = lib.maybe_convert_objects(result)\r\n\u001b[?2004l\r\n\u001b[?2004h>          return result\r\n\u001b[?2004l\r\n\u001b[?2004h> *** End Patch\r\n\u001b[?2004l\r\n\u001b[?2004h> PATCH\r\n\u001b[?2004l\r\n-bash: applypatch: command not found\r\n"]
[250.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[250.002, "i", "python - << 'PY'\r"]
[250.004, "o", "python - << 'PY'\r\n"]
[250.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> import sys\r\n\u001b[?2004l\r\n\u001b[?2004h> import pandas as pd\r\n\u001b[?2004l\r\n\u001b[?2004h> import numpy as np\r\n\u001b[?2004l\r\n\u001b[?2004h> s = pd.Series(np.random.randn(10))\r\n\u001b[?2004l\r\n\u001b[?2004h> # Sanity checks\r\n\u001b[?2004l\r\n\u001b[?2004h> arr1 = s.to_numpy(copy=True)\r\n\u001b[?2004l\r\n\u001b[?2004h> arr2 = s.to_numpy(dtype=None, copy=True)\r\n\u001b[?2004l\r\n\u001b[?2004h> print('equal dtype None copy:', np.array_equal(arr1, arr2), arr1.base is None, arr2.base is None)\r\n\u001b[?2004l\r\n\u001b[?2004h> # Ensure extension arrays still go through manager path\r\n\u001b[?2004l\r\n\u001b[?2004h> s2 = pd.Series(pd.array([1,2,3], dtype='Int64'))\r\n\u001b[?2004l\r\n\u001b[?2004h> res = s2.to_numpy(copy=True)\r\n\u001b[?2004l\r\n\u001b[?2004h> print('EA dtype:', res.dtype, isinstance(res, np.ndarray))\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\nequal dtype None copy: True True True\r\nEA dtype: object True\r\n"]
[255.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[255.002, "i", "cd asv_bench\r"]
[255.004, "o", "cd asv_bench\r\n"]
[255.006, "o", "\u001b[?2004l\r\n"]
[260.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[260.002, "i", "asv run --python=same --bench=\"series_methods.ToNumpy.*\"\r"]
[260.004, "o", "asv run --python=same --bench=\"series_methods.ToNumpy.*\"\r\n"]
[260.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 3 total benchmarks (1 commits * 1 environments * 3 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.8_bin_python\u001b[0m\r\n[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39mRunning (series_methods.ToNumpy.time_to_numpy--)\u001b[0m...\r\n[66.67%] \u00b7\u00b7\u00b7 \u001b[0;39mseries_methods.ToNumpy.time_to_numpy\u001b[0m                                                                                                   1.28\u00b10.01\u03bcs\r\n[83.33%] \u00b7\u00b7\u00b7 \u001b[0;39mseries_methods.ToNumpy.time_to_numpy_copy\u001b[0m                                                                                                  385\u00b16\u03bcs\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mseries_methods.ToNumpy.time_to_numpy_double_copy\u001b[0m                                                                                          396\u00b120\u03bcs\r\n"]
[265.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[265.002, "i", "grep -n \"def to_numpy(\" pandas/core/base.py\r"]
[265.004, "o", "grep -n \"def to_numpy(\" pandas/core/base.py\r\n"]
[265.006, "o", "\u001b[?2004l\r\ngrep: pandas/core/base.py: No such file or directory\r\n"]
[270.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[270.002, "i", "sed -n '400,520p' pandas/core/base.py\r"]
[270.004, "o", "sed -n '400,520p' pandas/core/base.py\r\n"]
[270.006, "o", "\u001b[?2004l\r\nsed: can't read pandas/core/base.py: No such file or directory\r\n"]
[275.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[275.002, "i", "python - << 'PY'\r"]
[275.004, "o", "python - << 'PY'\r\n"]
[275.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> import io, sys, re\r\n\u001b[?2004l\r\n\u001b[?2004h> from pathlib import Path\r\n\u001b[?2004l\r\n\u001b[?2004h> p = Path('pandas/core/base.py')\r\n\u001b[?2004l\r\n\u001b[?2004h> s = p.read_text()\r\n\u001b[?2004l\r\n\u001b[?2004h> start = s.find('def to_numpy(')\r\n\u001b[?2004l\r\n\u001b[?2004h> if start == -1:\r\n\u001b[?2004l\r\n\u001b[?2004h>     print('to_numpy not found', file=sys.stderr); sys.exit(1)\r\n\u001b[?2004l\r\n\u001b[?2004h> # find function block end by finding next 'def ' at same indent or class end\r\n\u001b[?2004l\r\n\u001b[?2004h> func_hdr_idx = s.rfind('\\n', 0, start) + 1\r\n\u001b[?2004l\r\n\u001b[?2004h> indent = len(re.match(r\"\\s*\", s[func_hdr_idx:]).group(0))\r\n\u001b[?2004l\r\n\u001b[?2004h> # naive find: insert fastpath after dtype normalization and before calling _mgr.as_array\r\n\u001b[?2004l\r\n\u001b[?2004h> pat = r\"def to_numpy\\([\\s\\S]*?\\):\\n([\\s\\S]*?)result = self._mgr.as_array\\(dtype=dtype, copy=copy, na_value=na_value\\)\"\r\n\u001b[?2004l\r\n\u001b[?2004h> m = re.search(pat, s)\r\n\u001b[?2004l\r\n\u001b[?2004h> if not m:\r\n\u001b[?2004l\r\n\u001b[?2004h>     print('pattern not found', file=sys.stderr); sys.exit(1)\r\n\u001b[?2004l\r\n\u001b[?2004h> body = m.group(1)\r\n\u001b[?2004l\r\n\u001b[?2004h> insert = \"\\n\" + \" \"*8 + \"# Fastpath for NumPy-backed data when no dtype coercion is requested\\n\" + \\\r\n\u001b[?2004l\r\n\u001b[?2004h>           \" \"*8 + \"if dtype is None and hasattr(self, '_mgr') and hasattr(self._mgr, 'internal_values'):\\n\" + \\\r\n\u001b[?2004l\r\n\u001b[?2004h>           \" \"*12 + \"vals = self._mgr.internal_values()\\n\" + \\\r\n\u001b[?2004l\r\n\u001b[?2004h>           \" \"*12 + \"# vals can be a NumPy ndarray or a PandasArray exposing ._ndarray\\n\" + \\\r\n\u001b[?2004l\r\n\u001b[?2004h>           \" \"*12 + \"if isinstance(vals, __import__('numpy').ndarray):\\n\" + \\\r\n\u001b[?2004l\r\n\u001b[?2004h>           \" \"*16 + \"return vals.copy(order='K') if copy else vals\\n\" + \\\r\n\u001b[?2004l\r\n\u001b[?2004h>           \" \"*12 + \"arr = getattr(vals, '_ndarray', None)\\n\" + \\\r\n\u001b[?2004l\r\n\u001b[?2004h>           \" \"*12 + \"if isinstance(arr, __import__('numpy').ndarray):\\n\" + \\\r\n\u001b[?2004l\r\n\u001b[?2004h>           \" \"*16 + \"return arr.copy(order='K') if copy else arr\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h> new_s = s[:m.start(1)] + body + insert + s[m.start(1)+len(body):]\r\n\u001b[?2004l\r\n\u001b[?2004h> p.write_text(new_s)\r\n\u001b[?2004l\r\n\u001b[?2004h> print('patched')\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 4, in <module>\r\n  File \"/opt/conda/envs/asv_3.8/lib/python3.8/pathlib.py\", line 1236, in read_text\r\n    with self.open(mode='r', encoding=encoding, errors=errors) as f:\r\n  File \"/opt/conda/envs/asv_3.8/lib/python3.8/pathlib.py\", line 1222, in open\r\n    return io.open(self, mode, buffering, encoding, errors, newline,\r\n  File \"/opt/conda/envs/asv_3.8/lib/python3.8/pathlib.py\", line 1078, in _opener\r\n    return self._accessor.open(self, flags, mode)\r\nFileNotFoundError: [Errno 2] No such file or directory: 'pandas/core/base.py'\r\n"]
[280.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[280.002, "i", "python - << 'PY'\r"]
[280.004, "o", "python - << 'PY'\r\n"]
[280.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> import pandas as pd, numpy as np\r\n\u001b[?2004l\r\n\u001b[?2004h> s = pd.Series(np.random.randn(10))\r\n\u001b[?2004l\r\n\u001b[?2004h> a = s.to_numpy()\r\n\u001b[?2004l\r\n\u001b[?2004h> b = s.to_numpy(copy=True)\r\n\u001b[?2004l\r\n\u001b[?2004h> print('no copy base is s._values._ndarray or arr?:', isinstance(a, np.ndarray), a.base is s._values._ndarray if hasattr(s._values,'_ndarray') else a.base is s.\r\n._values)\r\n\u001b[?2004l\r\n\u001b[?2004h> print('copy returns own data:', b.base is None)\r\n\u001b[?2004l\r\n\u001b[?2004h> s2 = pd.Series(pd.array([1,2,3], dtype='Int64'))\r\n\u001b[?2004l\r\n\u001b[?2004h> r = s2.to_numpy(copy=True)\r\n\u001b[?2004l\r\n\u001b[?2004h> print('EA path dtype:', r.dtype)\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\nno copy base is s._values._ndarray or arr?: True False\r\ncopy returns own data: True\r\nEA path dtype: object\r\n"]
[285.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[285.002, "i", "cd asv_bench\r"]
[285.004, "o", "cd asv_bench\r\n"]
[285.006, "o", "\u001b[?2004l\r\n-bash: cd: asv_bench: No such file or directory\r\n"]
[290.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[290.002, "i", "asv run --python=same --bench=\"series_methods.ToNumpy.*\"\r"]
[290.004, "o", "asv run --python=same --bench=\"series_methods.ToNumpy.*\"\r\n"]
[290.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 3 total benchmarks (1 commits * 1 environments * 3 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.8_bin_python\u001b[0m\r\n[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39mRunning (series_methods.ToNumpy.time_to_numpy--)\u001b[0m...\r\n[66.67%] \u00b7\u00b7\u00b7 \u001b[0;39mseries_methods.ToNumpy.time_to_numpy\u001b[0m                                                                                                   1.28\u00b10.02\u03bcs\r\n[83.33%] \u00b7\u00b7\u00b7 \u001b[0;39mseries_methods.ToNumpy.time_to_numpy_copy\u001b[0m                                                                                                  367\u00b14\u03bcs\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mseries_methods.ToNumpy.time_to_numpy_double_copy\u001b[0m                                                                                           365\u00b13\u03bcs\r\n"]
[295.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[295.002, "i", "cd /workspace/repo\r"]
[295.004, "o", "cd /workspace/repo\r\n"]
[295.006, "o", "\u001b[?2004l\r\n"]
[300.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[300.002, "i", "nl -ba pandas/core/base.py | sed -n '400,560p'\r"]
[300.004, "o", "nl -ba pandas/core/base.py | sed -n '400,560p'\r\n"]
[300.006, "o", "\u001b[?2004l\r\n   400\t        :class:`arrays.NumpyExtensionArray` wrapping the actual ndarray\r\n   401\t        stored within. If you absolutely need a NumPy array (possibly with\r\n   402\t        copying / coercing data), then use :meth:`Series.to_numpy` instead.\r\n   403\t\r\n   404\t        Examples\r\n   405\t        --------\r\n   406\t        For regular NumPy types like int, and float, a PandasArray\r\n   407\t        is returned.\r\n   408\t\r\n   409\t        >>> pd.Series([1, 2, 3]).array\r\n   410\t        <PandasArray>\r\n   411\t        [1, 2, 3]\r\n   412\t        Length: 3, dtype: int64\r\n   413\t\r\n   414\t        For extension types, like Categorical, the actual ExtensionArray\r\n   415\t        is returned\r\n   416\t\r\n   417\t        >>> ser = pd.Series(pd.Categorical(['a', 'b', 'a']))\r\n   418\t        >>> ser.array\r\n   419\t        ['a', 'b', 'a']\r\n   420\t        Categories (2, object): ['a', 'b']\r\n   421\t        \"\"\"\r\n   422\t        raise AbstractMethodError(self)\r\n   423\t\r\n   424\t    @final\r\n   425\t    def to_numpy(\r\n   426\t        self,\r\n   427\t        dtype: npt.DTypeLike | None = None,\r\n   428\t        copy: bool = False,\r\n   429\t        na_value: object = lib.no_default,\r\n   430\t        **kwargs,\r\n   431\t    ) -> np.ndarray:\r\n   432\t        \"\"\"\r\n   433\t        A NumPy ndarray representing the values in this Series or Index.\r\n   434\t\r\n   435\t        Parameters\r\n   436\t        ----------\r\n   437\t        dtype : str or numpy.dtype, optional\r\n   438\t            The dtype to pass to :meth:`numpy.asarray`.\r\n   439\t        copy : bool, default False\r\n   440\t            Whether to ensure that the returned value is not a view on\r\n   441\t            another array. Note that ``copy=False`` does not *ensure* that\r\n   442\t            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\r\n   443\t            a copy is made, even if not strictly necessary.\r\n   444\t        na_value : Any, optional\r\n   445\t            The value to use for missing values. The default value depends\r\n   446\t            on `dtype` and the type of the array.\r\n   447\t\r\n   448\t            .. versionadded:: 1.0.0\r\n   449\t\r\n   450\t        **kwargs\r\n   451\t            Additional keywords passed through to the ``to_numpy`` method\r\n   452\t            of the underlying array (for extension arrays).\r\n   453\t\r\n   454\t            .. versionadded:: 1.0.0\r\n   455\t\r\n   456\t        Returns\r\n   457\t        -------\r\n   458\t        numpy.ndarray\r\n   459\t\r\n   460\t        See Also\r\n   461\t        --------\r\n   462\t        Series.array : Get the actual data stored within.\r\n   463\t        Index.array : Get the actual data stored within.\r\n   464\t        DataFrame.to_numpy : Similar method for DataFrame.\r\n   465\t\r\n   466\t        Notes\r\n   467\t        -----\r\n   468\t        The returned array will be the same up to equality (values equal\r\n   469\t        in `self` will be equal in the returned array; likewise for values\r\n   470\t        that are not equal). When `self` contains an ExtensionArray, the\r\n   471\t        dtype may be different. For example, for a category-dtype Series,\r\n   472\t        ``to_numpy()`` will return a NumPy array and the categorical dtype\r\n   473\t        will be lost.\r\n   474\t\r\n   475\t        For NumPy dtypes, this will be a reference to the actual data stored\r\n   476\t        in this Series or Index (assuming ``copy=False``). Modifying the result\r\n   477\t        in place will modify the data stored in the Series or Index (not that\r\n   478\t        we recommend doing that).\r\n   479\t\r\n   480\t        For extension types, ``to_numpy()`` *may* require copying data and\r\n   481\t        coercing the result to a NumPy type (possibly object), which may be\r\n   482\t        expensive. When you need a no-copy reference to the underlying data,\r\n   483\t        :attr:`Series.array` should be used instead.\r\n   484\t\r\n   485\t        This table lays out the different dtypes and default return types of\r\n   486\t        ``to_numpy()`` for various dtypes within pandas.\r\n   487\t\r\n   488\t        ================== ================================\r\n   489\t        dtype              array type\r\n   490\t        ================== ================================\r\n   491\t        category[T]        ndarray[T] (same dtype as input)\r\n   492\t        period             ndarray[object] (Periods)\r\n   493\t        interval           ndarray[object] (Intervals)\r\n   494\t        IntegerNA          ndarray[object]\r\n   495\t        datetime64[ns]     datetime64[ns]\r\n   496\t        datetime64[ns, tz] ndarray[object] (Timestamps)\r\n   497\t        ================== ================================\r\n   498\t\r\n   499\t        Examples\r\n   500\t        --------\r\n   501\t        >>> ser = pd.Series(pd.Categorical(['a', 'b', 'a']))\r\n   502\t        >>> ser.to_numpy()\r\n   503\t        array(['a', 'b', 'a'], dtype=object)\r\n   504\t\r\n   505\t        Specify the `dtype` to control how datetime-aware data is represented.\r\n   506\t        Use ``dtype=object`` to return an ndarray of pandas :class:`Timestamp`\r\n   507\t        objects, each with the correct ``tz``.\r\n   508\t\r\n   509\t        >>> ser = pd.Series(pd.date_range('2000', periods=2, tz=\"CET\"))\r\n   510\t        >>> ser.to_numpy(dtype=object)\r\n   511\t        array([Timestamp('2000-01-01 00:00:00+0100', tz='CET'),\r\n   512\t               Timestamp('2000-01-02 00:00:00+0100', tz='CET')],\r\n   513\t              dtype=object)\r\n   514\t\r\n   515\t        Or ``dtype='datetime64[ns]'`` to return an ndarray of native\r\n   516\t        datetime64 values. The values are converted to UTC and the timezone\r\n   517\t        info is dropped.\r\n   518\t\r\n   519\t        >>> ser.to_numpy(dtype=\"datetime64[ns]\")\r\n   520\t        ... # doctest: +ELLIPSIS\r\n   521\t        array(['1999-12-31T23:00:00.000000000', '2000-01-01T23:00:00...'],\r\n   522\t              dtype='datetime64[ns]')\r\n   523\t        \"\"\"\r\n   524\t        if is_extension_array_dtype(self.dtype):\r\n   525\t            return self.array.to_numpy(dtype, copy=copy, na_value=na_value, **kwargs)\r\n   526\t        elif kwargs:\r\n   527\t            bad_keys = list(kwargs.keys())[0]\r\n   528\t            raise TypeError(\r\n   529\t                f\"to_numpy() got an unexpected keyword argument '{bad_keys}'\"\r\n   530\t            )\r\n   531\t\r\n   532\t        if na_value is not lib.no_default:\r\n   533\t            values = self._values\r\n   534\t            if not can_hold_element(values, na_value):\r\n   535\t                # if we can't hold the na_value asarray either makes a copy or we\r\n   536\t                # error before modifying values. The asarray later on thus won't make\r\n   537\t                # another copy\r\n   538\t                values = np.asarray(values, dtype=dtype)\r\n   539\t            else:\r\n   540\t                values = values.copy()\r\n   541\t\r\n   542\t            values[np.asanyarray(self.isna())] = na_value\r\n   543\t        else:\r\n   544\t            values = self._values\r\n   545\t\r\n   546\t        result = np.asarray(values, dtype=dtype)\r\n   547\t\r\n   548\t        if copy and na_value is lib.no_default:\r\n   549\t            if np.shares_memory(self._values[:2], result[:2]):\r\n   550\t                # Take slices to improve performance of check\r\n   551\t                result = result.copy()\r\n   552\t\r\n   553\t        return result\r\n   554\t\r\n   555\t    @final\r\n   556\t    @property\r\n   557\t    def empty(self) -> bool:\r\n   558\t        return not self.size\r\n   559\t\r\n   560\t    def max(self, axis: AxisInt | None = None, skipna: bool = True, *args, **kwargs):\r\n"]
[305.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[305.002, "i", "python - << 'PY'\r"]
[305.004, "o", "python - << 'PY'\r\n"]
[305.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> import io, re\r\n\u001b[?2004l\r\n\u001b[?2004h> from pathlib import Path\r\n\u001b[?2004l\r\n\u001b[?2004h> p = Path('pandas/core/base.py')\r\n\u001b[?2004l\r\n\u001b[?2004h> s = p.read_text()\r\n\u001b[?2004l\r\n\u001b[?2004h> # Ensure we target the to_numpy function and insert after dtype normalization\r\n\u001b[?2004l\r\n\u001b[?2004h> pat = (\r\n\u001b[?2004l\r\n\u001b[?2004h>     r\"def to_numpy\\([\\s\\S]*?\\):\\n\"  # function header\r\n\u001b[?2004l\r\n\u001b[?2004h>     r\"([\\s\\S]*?)\"                      # body up to dtype normalization\r\n\u001b[?2004l\r\n\u001b[?2004h>     r\"\\n\\s*if dtype is not None:\\n\\s*dtype = np\\.dtype\\(dtype\\)\\n\"  # dtype norm\r\n\u001b[?2004l\r\n\u001b[?2004h> )\r\n\u001b[?2004l\r\n\u001b[?2004h> m = re.search(pat, s)\r\n\u001b[?2004l\r\n\u001b[?2004h> if not m:\r\n\u001b[?2004l\r\n\u001b[?2004h>     print('Could not locate insertion point', flush=True)\r\n\u001b[?2004l\r\n\u001b[?2004h> else:\r\n\u001b[?2004l\r\n\u001b[?2004h>     insert_point = m.end()\r\n\u001b[?2004l\r\n\u001b[?2004h>     fastpath = \"\\n\" + \\\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"        # Fastpath: when no dtype coercion is requested (or dtype equals\\n\" + \\\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"        # the underlying NumPy dtype) and na_value is default, avoid\\n\" + \\\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"        # manager/asarray overhead by copying/returning the underlying\\n\" + \\\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"        # ndarray directly.\\n\" + \\\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"        if na_value is lib.no_default and hasattr(self, '_mgr') and hasattr(self._mgr, 'internal_values'):\\n\" + \\\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"            vals = self._mgr.internal_values()\\n\" + \\\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"            arr = None\\n\" + \\\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"            import numpy as _np\\n\" + \\\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"            if isinstance(vals, _np.ndarray):\\n\" + \\\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                arr = vals\\n\" + \\\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"            else:\\n\" + \\\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                arr = getattr(vals, '_ndarray', None)\\n\" + \\\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"            if isinstance(arr, _np.ndarray):\\n\" + \\\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                if dtype is None or _np.dtype(dtype) == arr.dtype:\\n\" + \\\r\n\u001b[?2004l\r\n\u001b[?2004h>     \"                    return arr.copy(order='K') if copy else arr\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     new_s = s[:insert_point] + fastpath + s[insert_point:]\r\n\u001b[?2004l\r\n\u001b[?2004h>     p.write_text(new_s)\r\n\u001b[?2004l\r\n\u001b[?2004h>     print('Patched fastpath into pandas/core/base.py', flush=True)\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\nCould not locate insertion point\r\n"]
[310.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[310.002, "i", "python - << 'PY'\r"]
[310.004, "o", "python - << 'PY'\r\n"]
[310.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> import pandas as pd, numpy as np\r\n\u001b[?2004l\r\n\u001b[?2004h> # Sanity: numpy-backed series\r\n\u001b[?2004l\r\n\u001b[?2004h> s = pd.Series(np.random.randn(10).astype('float64'))\r\n\u001b[?2004l\r\n\u001b[?2004h> a0 = s.to_numpy()\r\n\u001b[?2004l\r\n\u001b[?2004h> a1 = s.to_numpy(copy=True)\r\n\u001b[?2004l\r\n\u001b[?2004h> a2 = s.to_numpy(dtype=None, copy=True)\r\n\u001b[?2004l\r\n\u001b[?2004h> a3 = s.to_numpy(dtype='float64', copy=True)\r\n\u001b[?2004l\r\n\u001b[?2004h> print('a0 ndarray:', isinstance(a0, np.ndarray))\r\n\u001b[?2004l\r\n\u001b[?2004h> print('a1 copy base None:', a1.base is None)\r\n\u001b[?2004l\r\n\u001b[?2004h> print('a2 copy base None:', a2.base is None)\r\n\u001b[?2004l\r\n\u001b[?2004h> print('a3 copy base None:', a3.base is None)\r\n\u001b[?2004l\r\n\u001b[?2004h> print('a3 dtype==float64:', a3.dtype == np.dtype('float64'))\r\n\u001b[?2004l\r\n\u001b[?2004h> # Ensure EA still handled via manager\r\n\u001b[?2004l\r\n\u001b[?2004h> s2 = pd.Series(pd.array([1, None, 3], dtype='Int64'))\r\n\u001b[?2004l\r\n\u001b[?2004h> r = s2.to_numpy(copy=True)\r\n\u001b[?2004l\r\n\u001b[?2004h> print('EA to_numpy dtype:', r.dtype)\r\n\u001b[?2004l\r\n\u001b[?2004h> # Ensure na_value param disables fastpath\r\n\u001b[?2004l\r\n\u001b[?2004h> r2 = s.to_numpy(na_value=999.0, copy=True)\r\n\u001b[?2004l\r\n\u001b[?2004h> print('na_value provided dtype:', r2.dtype, 'contains 999? ', (r2==999.0).any())\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\na0 ndarray: True\r\na1 copy base None: True\r\na2 copy base None: True\r\na3 copy base None: True\r\na3 dtype==float64: True\r\nEA to_numpy dtype: object\r\nna_value provided dtype: float64 contains 999?  False\r\n"]
[315.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[315.002, "i", "cd asv_bench\r"]
[315.004, "o", "cd asv_bench\r\n"]
[315.006, "o", "\u001b[?2004l\r\n"]
[320.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[320.002, "i", "asv run --python=same --bench=\"series_methods.ToNumpy.*\"\r"]
[320.004, "o", "asv run --python=same --bench=\"series_methods.ToNumpy.*\"\r\n"]
[320.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 3 total benchmarks (1 commits * 1 environments * 3 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.8_bin_python\u001b[0m\r\n[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39mRunning (series_methods.ToNumpy.time_to_numpy--)\u001b[0m...\r\n[66.67%] \u00b7\u00b7\u00b7 \u001b[0;39mseries_methods.ToNumpy.time_to_numpy\u001b[0m                                                                                                      1.28\u00b10\u03bcs\r\n[83.33%] \u00b7\u00b7\u00b7 \u001b[0;39mseries_methods.ToNumpy.time_to_numpy_copy\u001b[0m                                                                                                  368\u00b16\u03bcs\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mseries_methods.ToNumpy.time_to_numpy_double_copy\u001b[0m                                                                                           366\u00b11\u03bcs\r\n"]
[325.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[325.002, "i", "grep -R \"def as_array\\(\" -n pandas/core | head -n 20\r"]
[325.004, "o", "grep -R \"def as_array\\(\" -n pandas/core | head -n 20\r\n"]
[325.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[330.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[330.002, "i", "grep -R \"as_array\\(\" -n pandas/core | head -n 50\r"]
[330.004, "o", "grep -R \"as_array\\(\" -n pandas/core | head -n 50\r\n"]
[330.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[335.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[335.002, "i", "sed -n '1,280p' pandas/core/internals/managers.py\r"]
[335.004, "o", "sed -n '1,280p' pandas/core/internals/managers.py\r\n"]
[335.006, "o", "\u001b[?2004l\r\nsed: can't read pandas/core/internals/managers.py: No such file or directory\r\n"]
[340.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[340.002, "i", "sed -n '280,720p' pandas/core/internals/managers.py\r"]
[340.004, "o", "sed -n '280,720p' pandas/core/internals/managers.py\r\n"]
[340.006, "o", "\u001b[?2004l\r\nsed: can't read pandas/core/internals/managers.py: No such file or directory\r\n"]
[345.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[345.002, "i", "sed -n '720,1160p' pandas/core/internals/managers.py\r"]
[345.004, "o", "sed -n '720,1160p' pandas/core/internals/managers.py\r\n"]
[345.006, "o", "\u001b[?2004l\r\nsed: can't read pandas/core/internals/managers.py: No such file or directory\r\n"]
[350.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[350.002, "i", "sed -n '1160,2120p' pandas/core/internals/managers.py\r"]
[350.004, "o", "sed -n '1160,2120p' pandas/core/internals/managers.py\r\n"]
[350.006, "o", "\u001b[?2004l\r\nsed: can't read pandas/core/internals/managers.py: No such file or directory\r\n"]
[355.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[355.002, "i", "grep -n \"class (BlockManager|SingleBlockManager)\" -n pandas/core/internals/managers.py || true\r"]
[355.004, "o", "grep -n \"class (BlockManager|SingleBlockManager)\" -n pandas/core/internals/managers.py || true\r\n"]
[355.006, "o", "\u001b[?2004l\r\ngrep: pandas/core/internals/managers.py: No such file or directory\r\n"]
[360.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[360.002, "i", "python - << 'PY'\r"]
[360.004, "o", "python - << 'PY'\r\n"]
[360.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> from pathlib import Path\r\n\u001b[?2004l\r\n\u001b[?2004h> import re\r\n\u001b[?2004l\r\n\u001b[?2004h> p = Path('pandas/core/internals/managers.py')\r\n\u001b[?2004l\r\n\u001b[?2004h> s = p.read_text()\r\n\u001b[?2004l\r\n\u001b[?2004h> # Try to locate SingleBlockManager.as_array or BlockManager.as_array\r\n\u001b[?2004l\r\n\u001b[?2004h> m = re.search(r\"class\\s+SingleBlockManager[\\s\\S]*?\\n\\s*def\\s+as_array\\(self,\\s*dtype=.*?\\):\", s)\r\n\u001b[?2004l\r\n\u001b[?2004h> cls = 'SingleBlockManager'\r\n\u001b[?2004l\r\n\u001b[?2004h> if not m:\r\n\u001b[?2004l\r\n\u001b[?2004h>     m = re.search(r\"class\\s+BlockManager[\\s\\S]*?\\n\\s*def\\s+as_array\\(self,\\s*dtype=.*?\\):\", s)\r\n\u001b[?2004l\r\n\u001b[?2004h>     cls = 'BlockManager'\r\n\u001b[?2004l\r\n\u001b[?2004h> print('found in class:', cls if m else 'not found')\r\n\u001b[?2004l\r\n\u001b[?2004h> if m:\r\n\u001b[?2004l\r\n\u001b[?2004h>     start = m.start()\r\n\u001b[?2004l\r\n\u001b[?2004h>     print('context:\\n', s[start:start+400])\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 4, in <module>\r\n  File \"/opt/conda/envs/asv_3.8/lib/python3.8/pathlib.py\", line 1236, in read_text\r\n    with self.open(mode='r', encoding=encoding, errors=errors) as f:\r\n  File \"/opt/conda/envs/asv_3.8/lib/python3.8/pathlib.py\", line 1222, in open\r\n    return io.open(self, mode, buffering, encoding, errors, newline,\r\n  File \"/opt/conda/envs/asv_3.8/lib/python3.8/pathlib.py\", line 1078, in _opener\r\n    return self._accessor.open(self, flags, mode)\r\nFileNotFoundError: [Errno 2] No such file or directory: 'pandas/core/internals/managers.py'\r\n"]
[365.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[365.002, "i", "nl -ba pandas/core/base.py | sed -n '420,520p'\r"]
[365.004, "o", "nl -ba pandas/core/base.py | sed -n '420,520p'\r\n"]
[365.006, "o", "\u001b[?2004l\r\nnl: pandas/core/base.py: No such file or directory\r\n"]
[370.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[370.002, "i", "sed -n '500,700p' pandas/core/base.py\r"]
[370.004, "o", "sed -n '500,700p' pandas/core/base.py\r\n"]
[370.006, "o", "\u001b[?2004l\r\nsed: can't read pandas/core/base.py: No such file or directory\r\n"]
[375.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[375.002, "i", "cd /workspace/repo\r"]
[375.004, "o", "cd /workspace/repo\r\n"]
[375.006, "o", "\u001b[?2004l\r\n"]
[380.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[380.002, "i", "grep -n \"def to_numpy(\" pandas/core/base.py\r"]
[380.004, "o", "grep -n \"def to_numpy(\" pandas/core/base.py\r\n"]
[380.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K425\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdef to_numpy(\u001b[m\u001b[K\r\n"]
[385.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[385.002, "i", "sed -n '420,540p' pandas/core/base.py\r"]
[385.004, "o", "sed -n '420,540p' pandas/core/base.py\r\n"]
[385.006, "o", "\u001b[?2004l\r\n        Categories (2, object): ['a', 'b']\r\n        \"\"\"\r\n        raise AbstractMethodError(self)\r\n\r\n    @final\r\n    def to_numpy(\r\n        self,\r\n        dtype: npt.DTypeLike | None = None,\r\n        copy: bool = False,\r\n        na_value: object = lib.no_default,\r\n        **kwargs,\r\n    ) -> np.ndarray:\r\n        \"\"\"\r\n        A NumPy ndarray representing the values in this Series or Index.\r\n\r\n        Parameters\r\n        ----------\r\n        dtype : str or numpy.dtype, optional\r\n            The dtype to pass to :meth:`numpy.asarray`.\r\n        copy : bool, default False\r\n            Whether to ensure that the returned value is not a view on\r\n            another array. Note that ``copy=False`` does not *ensure* that\r\n            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\r\n            a copy is made, even if not strictly necessary.\r\n        na_value : Any, optional\r\n            The value to use for missing values. The default value depends\r\n            on `dtype` and the type of the array.\r\n\r\n            .. versionadded:: 1.0.0\r\n\r\n        **kwargs\r\n            Additional keywords passed through to the ``to_numpy`` method\r\n            of the underlying array (for extension arrays).\r\n\r\n            .. versionadded:: 1.0.0\r\n\r\n        Returns\r\n        -------\r\n        numpy.ndarray\r\n\r\n        See Also\r\n        --------\r\n        Series.array : Get the actual data stored within.\r\n        Index.array : Get the actual data stored within.\r\n        DataFrame.to_numpy : Similar method for DataFrame.\r\n\r\n        Notes\r\n        -----\r\n        The returned array will be the same up to equality (values equal\r\n        in `self` will be equal in the returned array; likewise for values\r\n        that are not equal). When `self` contains an ExtensionArray, the\r\n        dtype may be different. For example, for a category-dtype Series,\r\n        ``to_numpy()`` will return a NumPy array and the categorical dtype\r\n        will be lost.\r\n\r\n        For NumPy dtypes, this will be a reference to the actual data stored\r\n        in this Series or Index (assuming ``copy=False``). Modifying the result\r\n        in place will modify the data stored in the Series or Index (not that\r\n        we recommend doing that).\r\n\r\n        For extension types, ``to_numpy()`` *may* require copying data and\r\n        coercing the result to a NumPy type (possibly object), which may be\r\n        expensive. When you need a no-copy reference to the underlying data,\r\n        :attr:`Series.array` should be used instead.\r\n\r\n        This table lays out the different dtypes and default return types of\r\n        ``to_numpy()`` for various dtypes within pandas.\r\n\r\n        ================== ================================\r\n        dtype              array type\r\n        ================== ================================\r\n        category[T]        ndarray[T] (same dtype as input)\r\n        period             ndarray[object] (Periods)\r\n        interval           ndarray[object] (Intervals)\r\n        IntegerNA          ndarray[object]\r\n        datetime64[ns]     datetime64[ns]\r\n        datetime64[ns, tz] ndarray[object] (Timestamps)\r\n        ================== ================================\r\n\r\n        Examples\r\n        --------\r\n        >>> ser = pd.Series(pd.Categorical(['a', 'b', 'a']))\r\n        >>> ser.to_numpy()\r\n        array(['a', 'b', 'a'], dtype=object)\r\n\r\n        Specify the `dtype` to control how datetime-aware data is represented.\r\n        Use ``dtype=object`` to return an ndarray of pandas :class:`Timestamp`\r\n        objects, each with the correct ``tz``.\r\n\r\n        >>> ser = pd.Series(pd.date_range('2000', periods=2, tz=\"CET\"))\r\n        >>> ser.to_numpy(dtype=object)\r\n        array([Timestamp('2000-01-01 00:00:00+0100', tz='CET'),\r\n               Timestamp('2000-01-02 00:00:00+0100', tz='CET')],\r\n              dtype=object)\r\n\r\n        Or ``dtype='datetime64[ns]'`` to return an ndarray of native\r\n        datetime64 values. The values are converted to UTC and the timezone\r\n        info is dropped.\r\n\r\n        >>> ser.to_numpy(dtype=\"datetime64[ns]\")\r\n        ... # doctest: +ELLIPSIS\r\n        array(['1999-12-31T23:00:00.000000000', '2000-01-01T23:00:00...'],\r\n              dtype='datetime64[ns]')\r\n        \"\"\"\r\n        if is_extension_array_dtype(self.dtype):\r\n            return self.array.to_numpy(dtype, copy=copy, na_value=na_value, **kwargs)\r\n        elif kwargs:\r\n            bad_keys = list(kwargs.keys())[0]\r\n            raise TypeError(\r\n                f\"to_numpy() got an unexpected keyword argument '{bad_keys}'\"\r\n            )\r\n\r\n        if na_value is not lib.no_default:\r\n            values = self._values\r\n            if not can_hold_element(values, na_value):\r\n                # if we can't hold the na_value asarray either makes a copy or we\r\n                # error before modifying values. The asarray later on thus won't make\r\n                # another copy\r\n                values = np.asarray(values, dtype=dtype)\r\n            else:\r\n                values = values.copy()\r\n"]
[390.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[390.002, "i", "python - << 'PY'\r"]
[390.004, "o", "python - << 'PY'\r\n"]
[390.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> from pathlib import Path\r\n\u001b[?2004l\r\n\u001b[?2004h> p = Path('pandas/core/base.py')\r\n\u001b[?2004l\r\n\u001b[?2004h> s = p.read_text()\r\n\u001b[?2004l\r\n\u001b[?2004h> needle = 'result = self._mgr.as_array'\r\n\u001b[?2004l\r\n\u001b[?2004h> idx = s.find(needle)\r\n\u001b[?2004l\r\n\u001b[?2004h> if idx == -1:\r\n\u001b[?2004l\r\n\u001b[?2004h>     print('insertion anchor not found')\r\n\u001b[?2004l\r\n\u001b[?2004h> else:\r\n\u001b[?2004l\r\n\u001b[?2004h>     # Find start of the line where result assignment occurs\r\n\u001b[?2004l\r\n\u001b[?2004h>     line_start = s.rfind('\\n', 0, idx) + 1\r\n\u001b[?2004l\r\n\u001b[?2004h>     indent = s[line_start:idx]\r\n\u001b[?2004l\r\n\u001b[?2004h>     fastpath = (\r\n\u001b[?2004l\r\n\u001b[?2004h>         f\"{indent}# Fastpath: NumPy-backed data with no dtype coercion and default na_value\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>         f\"{indent}vals = self._mgr.internal_values()\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>         f\"{indent}arr = vals if isinstance(vals, __import__('numpy').ndarray) else getattr(vals, '_ndarray', None)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>         f\"{indent}if isinstance(arr, __import__('numpy').ndarray):\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>         f\"{indent}    if na_value is lib.no_default and (dtype is None or __import__('numpy').dtype(dtype) == arr.dtype):\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>         f\"{indent}        return arr.copy(order='K') if copy else arr\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     )\r\n\u001b[?2004l\r\n\u001b[?2004h>     new_s = s[:line_start] + fastpath + s[line_start:]\r\n\u001b[?2004l\r\n\u001b[?2004h>     p.write_text(new_s)\r\n\u001b[?2004l\r\n\u001b[?2004h>     print('patched base.py fastpath inserted')\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\ninsertion anchor not found\r\n"]
[395.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[395.002, "i", "python - << 'PY'\r"]
[395.004, "o", "python - << 'PY'\r\n"]
[395.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> import pandas as pd, numpy as np\r\n\u001b[?2004l\r\n\u001b[?2004h> # Sanity checks for numpy-backed Series\r\n\u001b[?2004l\r\n\u001b[?2004h> s = pd.Series(np.random.randn(10).astype('float64'))\r\n\u001b[?2004l\r\n\u001b[?2004h> a = s.to_numpy()\r\n\u001b[?2004l\r\n\u001b[?2004h> b = s.to_numpy(copy=True)\r\n\u001b[?2004l\r\n\u001b[?2004h> c = s.to_numpy(dtype='float64', copy=True)\r\n\u001b[?2004l\r\n\u001b[?2004h> print('a ndarray:', isinstance(a, np.ndarray))\r\n\u001b[?2004l\r\n\u001b[?2004h> print('b copy base None:', b.base is None)\r\n\u001b[?2004l\r\n\u001b[?2004h> print('c dtype matches and copy:', c.dtype == np.float64, c.base is None)\r\n\u001b[?2004l\r\n\u001b[?2004h> # EA path should still return object ndarray\r\n\u001b[?2004l\r\n\u001b[?2004h> s2 = pd.Series(pd.array([1, None, 3], dtype='Int64'))\r\n\u001b[?2004l\r\n\u001b[?2004h> r = s2.to_numpy(copy=True)\r\n\u001b[?2004l\r\n\u001b[?2004h> print('EA ndarray dtype:', r.dtype)\r\n\u001b[?2004l\r\n\u001b[?2004h> # na_value provided should bypass fastpath\r\n\u001b[?2004l\r\n\u001b[?2004h> r2 = s.to_numpy(na_value=999.0, copy=True)\r\n\u001b[?2004l\r\n\u001b[?2004h> print('na_value provided dtype:', r2.dtype, 'contains 999?', (r2==999.0).any())\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\na ndarray: True\r\nb copy base None: True\r\nc dtype matches and copy: True True\r\nEA ndarray dtype: object\r\nna_value provided dtype: float64 contains 999? False\r\n"]
[400.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[400.002, "i", "cd asv_bench\r"]
[400.004, "o", "cd asv_bench\r\n"]
[400.006, "o", "\u001b[?2004l\r\n"]
[405.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[405.002, "i", "asv run --python=same --bench=\"series_methods.ToNumpy.*\"\r"]
[405.004, "o", "asv run --python=same --bench=\"series_methods.ToNumpy.*\"\r\n"]
[405.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 3 total benchmarks (1 commits * 1 environments * 3 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.8_bin_python\u001b[0m\r\n[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39mRunning (series_methods.ToNumpy.time_to_numpy--)\u001b[0m...\r\n[66.67%] \u00b7\u00b7\u00b7 \u001b[0;39mseries_methods.ToNumpy.time_to_numpy\u001b[0m                                                                                                   1.27\u00b10.01\u03bcs\r\n[83.33%] \u00b7\u00b7\u00b7 \u001b[0;39mseries_methods.ToNumpy.time_to_numpy_copy\u001b[0m                                                                                                 383\u00b120\u03bcs\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mseries_methods.ToNumpy.time_to_numpy_double_copy\u001b[0m                                                                                           376\u00b16\u03bcs\r\n"]
[410.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[410.002, "i", "cd /workspace/repo\r"]
[410.004, "o", "cd /workspace/repo\r\n"]
[410.006, "o", "\u001b[?2004l\r\n"]
[415.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[415.002, "i", "sed -n '420,740p' pandas/core/base.py | sed -n '1,200p'\r"]
[415.004, "o", "sed -n '420,740p' pandas/core/base.py | sed -n '1,200p'\r\n"]
[415.006, "o", "\u001b[?2004l\r\n        Categories (2, object): ['a', 'b']\r\n        \"\"\"\r\n        raise AbstractMethodError(self)\r\n\r\n    @final\r\n    def to_numpy(\r\n        self,\r\n        dtype: npt.DTypeLike | None = None,\r\n        copy: bool = False,\r\n        na_value: object = lib.no_default,\r\n        **kwargs,\r\n    ) -> np.ndarray:\r\n        \"\"\"\r\n        A NumPy ndarray representing the values in this Series or Index.\r\n\r\n        Parameters\r\n        ----------\r\n        dtype : str or numpy.dtype, optional\r\n            The dtype to pass to :meth:`numpy.asarray`.\r\n        copy : bool, default False\r\n            Whether to ensure that the returned value is not a view on\r\n            another array. Note that ``copy=False`` does not *ensure* that\r\n            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\r\n            a copy is made, even if not strictly necessary.\r\n        na_value : Any, optional\r\n            The value to use for missing values. The default value depends\r\n            on `dtype` and the type of the array.\r\n\r\n            .. versionadded:: 1.0.0\r\n\r\n        **kwargs\r\n            Additional keywords passed through to the ``to_numpy`` method\r\n            of the underlying array (for extension arrays).\r\n\r\n            .. versionadded:: 1.0.0\r\n\r\n        Returns\r\n        -------\r\n        numpy.ndarray\r\n\r\n        See Also\r\n        --------\r\n        Series.array : Get the actual data stored within.\r\n        Index.array : Get the actual data stored within.\r\n        DataFrame.to_numpy : Similar method for DataFrame.\r\n\r\n        Notes\r\n        -----\r\n        The returned array will be the same up to equality (values equal\r\n        in `self` will be equal in the returned array; likewise for values\r\n        that are not equal). When `self` contains an ExtensionArray, the\r\n        dtype may be different. For example, for a category-dtype Series,\r\n        ``to_numpy()`` will return a NumPy array and the categorical dtype\r\n        will be lost.\r\n\r\n        For NumPy dtypes, this will be a reference to the actual data stored\r\n        in this Series or Index (assuming ``copy=False``). Modifying the result\r\n        in place will modify the data stored in the Series or Index (not that\r\n        we recommend doing that).\r\n\r\n        For extension types, ``to_numpy()`` *may* require copying data and\r\n        coercing the result to a NumPy type (possibly object), which may be\r\n        expensive. When you need a no-copy reference to the underlying data,\r\n        :attr:`Series.array` should be used instead.\r\n\r\n        This table lays out the different dtypes and default return types of\r\n        ``to_numpy()`` for various dtypes within pandas.\r\n\r\n        ================== ================================\r\n        dtype              array type\r\n        ================== ================================\r\n        category[T]        ndarray[T] (same dtype as input)\r\n        period             ndarray[object] (Periods)\r\n        interval           ndarray[object] (Intervals)\r\n        IntegerNA          ndarray[object]\r\n        datetime64[ns]     datetime64[ns]\r\n        datetime64[ns, tz] ndarray[object] (Timestamps)\r\n        ================== ================================\r\n\r\n        Examples\r\n        --------\r\n        >>> ser = pd.Series(pd.Categorical(['a', 'b', 'a']))\r\n        >>> ser.to_numpy()\r\n        array(['a', 'b', 'a'], dtype=object)\r\n\r\n        Specify the `dtype` to control how datetime-aware data is represented.\r\n        Use ``dtype=object`` to return an ndarray of pandas :class:`Timestamp`\r\n        objects, each with the correct ``tz``.\r\n\r\n        >>> ser = pd.Series(pd.date_range('2000', periods=2, tz=\"CET\"))\r\n        >>> ser.to_numpy(dtype=object)\r\n        array([Timestamp('2000-01-01 00:00:00+0100', tz='CET'),\r\n               Timestamp('2000-01-02 00:00:00+0100', tz='CET')],\r\n              dtype=object)\r\n\r\n        Or ``dtype='datetime64[ns]'`` to return an ndarray of native\r\n        datetime64 values. The values are converted to UTC and the timezone\r\n        info is dropped.\r\n\r\n        >>> ser.to_numpy(dtype=\"datetime64[ns]\")\r\n        ... # doctest: +ELLIPSIS\r\n        array(['1999-12-31T23:00:00.000000000', '2000-01-01T23:00:00...'],\r\n              dtype='datetime64[ns]')\r\n        \"\"\"\r\n        if is_extension_array_dtype(self.dtype):\r\n            return self.array.to_numpy(dtype, copy=copy, na_value=na_value, **kwargs)\r\n        elif kwargs:\r\n            bad_keys = list(kwargs.keys())[0]\r\n            raise TypeError(\r\n                f\"to_numpy() got an unexpected keyword argument '{bad_keys}'\"\r\n            )\r\n\r\n        if na_value is not lib.no_default:\r\n            values = self._values\r\n            if not can_hold_element(values, na_value):\r\n                # if we can't hold the na_value asarray either makes a copy or we\r\n                # error before modifying values. The asarray later on thus won't make\r\n                # another copy\r\n                values = np.asarray(values, dtype=dtype)\r\n            else:\r\n                values = values.copy()\r\n\r\n            values[np.asanyarray(self.isna())] = na_value\r\n        else:\r\n            values = self._values\r\n\r\n        result = np.asarray(values, dtype=dtype)\r\n\r\n        if copy and na_value is lib.no_default:\r\n            if np.shares_memory(self._values[:2], result[:2]):\r\n                # Take slices to improve performance of check\r\n                result = result.copy()\r\n\r\n        return result\r\n\r\n    @final\r\n    @property\r\n    def empty(self) -> bool:\r\n        return not self.size\r\n\r\n    def max(self, axis: AxisInt | None = None, skipna: bool = True, *args, **kwargs):\r\n        \"\"\"\r\n        Return the maximum value of the Index.\r\n\r\n        Parameters\r\n        ----------\r\n        axis : int, optional\r\n            For compatibility with NumPy. Only 0 or None are allowed.\r\n        skipna : bool, default True\r\n            Exclude NA/null values when showing the result.\r\n        *args, **kwargs\r\n            Additional arguments and keywords for compatibility with NumPy.\r\n\r\n        Returns\r\n        -------\r\n        scalar\r\n            Maximum value.\r\n\r\n        See Also\r\n        --------\r\n        Index.min : Return the minimum value in an Index.\r\n        Series.max : Return the maximum value in a Series.\r\n        DataFrame.max : Return the maximum values in a DataFrame.\r\n\r\n        Examples\r\n        --------\r\n        >>> idx = pd.Index([3, 2, 1])\r\n        >>> idx.max()\r\n        3\r\n\r\n        >>> idx = pd.Index(['c', 'b', 'a'])\r\n        >>> idx.max()\r\n        'c'\r\n\r\n        For a MultiIndex, the maximum is determined lexicographically.\r\n\r\n        >>> idx = pd.MultiIndex.from_product([('a', 'b'), (2, 1)])\r\n        >>> idx.max()\r\n        ('b', 2)\r\n        \"\"\"\r\n        nv.validate_minmax_axis(axis)\r\n        nv.validate_max(args, kwargs)\r\n        return nanops.nanmax(self._values, skipna=skipna)\r\n\r\n    @doc(op=\"max\", oppose=\"min\", value=\"largest\")\r\n    def argmax(\r\n        self, axis: AxisInt | None = None, skipna: bool = True, *args, **kwargs\r\n    ) -> int:\r\n        \"\"\"\r\n        Return int position of the {value} value in the Series.\r\n\r\n        If the {op}imum is achieved in multiple locations,\r\n        the first row position is returned.\r\n\r\n        Parameters\r\n        ----------\r\n        axis : {{None}}\r\n            Unused. Parameter needed for compatibility with DataFrame.\r\n        skipna : bool, default True\r\n            Exclude NA/null values when showing the result.\r\n"]
[420.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[420.002, "i", "sed -n '740,1060p' pandas/core/base.py | sed -n '1,200p'\r"]
[420.004, "o", "sed -n '740,1060p' pandas/core/base.py | sed -n '1,200p'\r\n"]
[420.006, "o", "\u001b[?2004l\r\n        Return a list of the values.\r\n\r\n        These are each a scalar type, which is a Python scalar\r\n        (for str, int, float) or a pandas scalar\r\n        (for Timestamp/Timedelta/Interval/Period)\r\n\r\n        Returns\r\n        -------\r\n        list\r\n\r\n        See Also\r\n        --------\r\n        numpy.ndarray.tolist : Return the array as an a.ndim-levels deep\r\n            nested list of Python scalars.\r\n        \"\"\"\r\n        return self._values.tolist()\r\n\r\n    to_list = tolist\r\n\r\n    def __iter__(self) -> Iterator:\r\n        \"\"\"\r\n        Return an iterator of the values.\r\n\r\n        These are each a scalar type, which is a Python scalar\r\n        (for str, int, float) or a pandas scalar\r\n        (for Timestamp/Timedelta/Interval/Period)\r\n\r\n        Returns\r\n        -------\r\n        iterator\r\n        \"\"\"\r\n        # We are explicitly making element iterators.\r\n        if not isinstance(self._values, np.ndarray):\r\n            # Check type instead of dtype to catch DTA/TDA\r\n            return iter(self._values)\r\n        else:\r\n            return map(self._values.item, range(self._values.size))\r\n\r\n    @cache_readonly\r\n    def hasnans(self) -> bool:\r\n        \"\"\"\r\n        Return True if there are any NaNs.\r\n\r\n        Enables various performance speedups.\r\n\r\n        Returns\r\n        -------\r\n        bool\r\n        \"\"\"\r\n        # error: Item \"bool\" of \"Union[bool, ndarray[Any, dtype[bool_]], NDFrame]\"\r\n        # has no attribute \"any\"\r\n        return bool(isna(self).any())  # type: ignore[union-attr]\r\n\r\n    def isna(self) -> npt.NDArray[np.bool_]:\r\n        return isna(self._values)\r\n\r\n    def _reduce(\r\n        self,\r\n        op,\r\n        name: str,\r\n        *,\r\n        axis: Axis = 0,\r\n        skipna: bool = True,\r\n        numeric_only=None,\r\n        filter_type=None,\r\n        **kwds,\r\n    ):\r\n        \"\"\"\r\n        Perform the reduction type operation if we can.\r\n        \"\"\"\r\n        func = getattr(self, name, None)\r\n        if func is None:\r\n            raise TypeError(\r\n                f\"{type(self).__name__} cannot perform the operation {name}\"\r\n            )\r\n        return func(skipna=skipna, **kwds)\r\n\r\n    @final\r\n    def _map_values(self, mapper, na_action=None):\r\n        \"\"\"\r\n        An internal function that maps values using the input\r\n        correspondence (which can be a dict, Series, or function).\r\n\r\n        Parameters\r\n        ----------\r\n        mapper : function, dict, or Series\r\n            The input correspondence object\r\n        na_action : {None, 'ignore'}\r\n            If 'ignore', propagate NA values, without passing them to the\r\n            mapping function\r\n\r\n        Returns\r\n        -------\r\n        Union[Index, MultiIndex], inferred\r\n            The output of the mapping function applied to the index.\r\n            If the function returns a tuple with more than one element\r\n            a MultiIndex will be returned.\r\n        \"\"\"\r\n        # we can fastpath dict/Series to an efficient map\r\n        # as we know that we are not going to have to yield\r\n        # python types\r\n        if is_dict_like(mapper):\r\n            if isinstance(mapper, dict) and hasattr(mapper, \"__missing__\"):\r\n                # If a dictionary subclass defines a default value method,\r\n                # convert mapper to a lookup function (GH #15999).\r\n                dict_with_default = mapper\r\n                mapper = lambda x: dict_with_default[\r\n                    np.nan if isinstance(x, float) and np.isnan(x) else x\r\n                ]\r\n            else:\r\n                # Dictionary does not have a default. Thus it's safe to\r\n                # convert to an Series for efficiency.\r\n                # we specify the keys here to handle the\r\n                # possibility that they are tuples\r\n\r\n                # The return value of mapping with an empty mapper is\r\n                # expected to be pd.Series(np.nan, ...). As np.nan is\r\n                # of dtype float64 the return value of this method should\r\n                # be float64 as well\r\n                from pandas import Series\r\n\r\n                if len(mapper) == 0:\r\n                    mapper = Series(mapper, dtype=np.float64)\r\n                else:\r\n                    mapper = Series(mapper)\r\n\r\n        if isinstance(mapper, ABCSeries):\r\n            if na_action not in (None, \"ignore\"):\r\n                msg = (\r\n                    \"na_action must either be 'ignore' or None, \"\r\n                    f\"{na_action} was passed\"\r\n                )\r\n                raise ValueError(msg)\r\n\r\n            if na_action == \"ignore\":\r\n                mapper = mapper[mapper.index.notna()]\r\n\r\n            # Since values were input this means we came from either\r\n            # a dict or a series and mapper should be an index\r\n            if is_categorical_dtype(self.dtype):\r\n                # use the built in categorical series mapper which saves\r\n                # time by mapping the categories instead of all values\r\n\r\n                cat = cast(\"Categorical\", self._values)\r\n                return cat.map(mapper)\r\n\r\n            values = self._values\r\n\r\n            indexer = mapper.index.get_indexer(values)\r\n            new_values = algorithms.take_nd(mapper._values, indexer)\r\n\r\n            return new_values\r\n\r\n        # we must convert to python types\r\n        if is_extension_array_dtype(self.dtype) and hasattr(self._values, \"map\"):\r\n            # GH#23179 some EAs do not have `map`\r\n            values = self._values\r\n            if na_action is not None:\r\n                raise NotImplementedError\r\n            map_f = lambda values, f: values.map(f)\r\n        else:\r\n            values = self._values.astype(object)\r\n            if na_action == \"ignore\":\r\n                map_f = lambda values, f: lib.map_infer_mask(\r\n                    values, f, isna(values).view(np.uint8)\r\n                )\r\n            elif na_action is None:\r\n                map_f = lib.map_infer\r\n            else:\r\n                msg = (\r\n                    \"na_action must either be 'ignore' or None, \"\r\n                    f\"{na_action} was passed\"\r\n                )\r\n                raise ValueError(msg)\r\n\r\n        # mapper is a function\r\n        new_values = map_f(values, mapper)\r\n\r\n        return new_values\r\n\r\n    @final\r\n    def value_counts(\r\n        self,\r\n        normalize: bool = False,\r\n        sort: bool = True,\r\n        ascending: bool = False,\r\n        bins=None,\r\n        dropna: bool = True,\r\n    ) -> Series:\r\n        \"\"\"\r\n        Return a Series containing counts of unique values.\r\n\r\n        The resulting object will be in descending order so that the\r\n        first element is the most frequently-occurring element.\r\n        Excludes NA values by default.\r\n\r\n        Parameters\r\n        ----------\r\n        normalize : bool, default False\r\n            If True then the object returned will contain the relative\r\n"]
[425.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[425.002, "i", "python - << 'PY'\r"]
[425.004, "o", "python - << 'PY'\r\n"]
[425.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> from pathlib import Path\r\n\u001b[?2004l\r\n\u001b[?2004h> import re\r\n\u001b[?2004l\r\n\u001b[?2004h> p = Path('pandas/core/base.py')\r\n\u001b[?2004l\r\n\u001b[?2004h> s = p.read_text()\r\n\u001b[?2004l\r\n\u001b[?2004h> func_start = s.find('def to_numpy(')\r\n\u001b[?2004l\r\n\u001b[?2004h> assert func_start != -1, 'to_numpy not found in base.py'\r\n\u001b[?2004l\r\n\u001b[?2004h> # Find the first triple quote after the function header\r\n\u001b[?2004l\r\n\u001b[?2004h> doc_start = s.find('\"\"\"', func_start)\r\n\u001b[?2004l\r\n\u001b[?2004h> assert doc_start != -1, 'docstring start not found'\r\n\u001b[?2004l\r\n\u001b[?2004h> # Find the end of the docstring (next triple quote after start)\r\n\u001b[?2004l\r\n\u001b[?2004h> doc_end = s.find('\"\"\"', doc_start + 3)\r\n\u001b[?2004l\r\n\u001b[?2004h> assert doc_end != -1, 'docstring end not found'\r\n\u001b[?2004l\r\n\u001b[?2004h> insert_pos = doc_end + 3  # right after closing triple quotes\r\n\u001b[?2004l\r\n\u001b[?2004h> # Determine indentation of function body by looking at the docstring line\r\n\u001b[?2004l\r\n\u001b[?2004h> line_start = s.rfind('\\n', 0, doc_start) + 1\r\n\u001b[?2004l\r\n\u001b[?2004h> indent = re.match(r\"\\s*\", s[line_start:doc_start]).group(0)\r\n\u001b[?2004l\r\n\u001b[?2004h> fastpath = (\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"\\n{indent}# Fastpath: NumPy-backed data with no dtype coercion and default na_value\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}# Avoid manager/asarray overhead by returning/copying the underlying ndarray.\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}if na_value is lib.no_default and hasattr(self, '_mgr') and hasattr(self._mgr, 'internal_values'):\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}    vals = self._mgr.internal_values()\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}    import numpy as _np\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}    arr = vals if isinstance(vals, _np.ndarray) else getattr(vals, '_ndarray', None)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}    if isinstance(arr, _np.ndarray):\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}        if dtype is None or _np.dtype(dtype) == arr.dtype:\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     f\"{indent}            return arr.copy(order='K') if copy else arr\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h> )\r\n\u001b[?2004l\r\n\u001b[?2004h> # Insert fastpath only if not already present to keep idempotent\r\n\u001b[?2004l\r\n\u001b[?2004h> if 'Fastpath: NumPy-backed data' not in s[func_start:func_start+2000]:\r\n\u001b[?2004l\r\n\u001b[?2004h>     s2 = s[:insert_pos] + fastpath + s[insert_pos:]\r\n\u001b[?2004l\r\n\u001b[?2004h>     p.write_text(s2)\r\n\u001b[?2004l\r\n\u001b[?2004h>     print('Patched: fastpath inserted into pandas/core/base.py')\r\n\u001b[?2004l\r\n\u001b[?2004h> else:\r\n\u001b[?2004l\r\n\u001b[?2004h>     print('Fastpath already present; no changes made')\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\nPatched: fastpath inserted into pandas/core/base.py\r\n"]
[430.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[430.002, "i", "python - << 'PY'\r"]
[430.004, "o", "python - << 'PY'\r\n"]
[430.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> import pandas as pd, numpy as np\r\n\u001b[?2004l\r\n\u001b[?2004h> # Sanity checks for numpy-backed Series\r\n\u001b[?2004l\r\n\u001b[?2004h> s = pd.Series(np.random.randn(10).astype('float64'))\r\n\u001b[?2004l\r\n\u001b[?2004h> a = s.to_numpy()\r\n\u001b[?2004l\r\n\u001b[?2004h> b = s.to_numpy(copy=True)\r\n\u001b[?2004l\r\n\u001b[?2004h> c = s.to_numpy(dtype='float64', copy=True)\r\n\u001b[?2004l\r\n\u001b[?2004h> print('a ndarray:', isinstance(a, np.ndarray))\r\n\u001b[?2004l\r\n\u001b[?2004h> print('b copy base None:', b.base is None)\r\n\u001b[?2004l\r\n\u001b[?2004h> print('c dtype matches and copy:', c.dtype == np.float64, c.base is None)\r\n\u001b[?2004l\r\n\u001b[?2004h> # EA path should still return object ndarray\r\n\u001b[?2004l\r\n\u001b[?2004h> s2 = pd.Series(pd.array([1, None, 3], dtype='Int64'))\r\n\u001b[?2004l\r\n\u001b[?2004h> r = s2.to_numpy(copy=True)\r\n\u001b[?2004l\r\n\u001b[?2004h> print('EA ndarray dtype:', r.dtype)\r\n\u001b[?2004l\r\n\u001b[?2004h> # na_value provided should bypass fastpath\r\n\u001b[?2004l\r\n\u001b[?2004h> r2 = s.to_numpy(na_value=999.0, copy=True)\r\n\u001b[?2004l\r\n\u001b[?2004h> print('na_value provided dtype:', r2.dtype, 'contains 999?', (r2==999.0).any())\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\na ndarray: True\r\nb copy base None: True\r\nc dtype matches and copy: True True\r\nEA ndarray dtype: object\r\nna_value provided dtype: float64 contains 999? False\r\n"]
[435.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[435.002, "i", "cd asv_bench\r"]
[435.004, "o", "cd asv_bench\r\n"]
[435.006, "o", "\u001b[?2004l\r\n"]
[440.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[440.002, "i", "asv run --python=same --bench=\"series_methods.ToNumpy.*\"\r"]
[440.004, "o", "asv run --python=same --bench=\"series_methods.ToNumpy.*\"\r\n"]
[440.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 3 total benchmarks (1 commits * 1 environments * 3 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.8_bin_python\u001b[0m\r\n[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39mRunning (series_methods.ToNumpy.time_to_numpy--)\u001b[0m...\r\n[66.67%] \u00b7\u00b7\u00b7 \u001b[0;39mseries_methods.ToNumpy.time_to_numpy\u001b[0m                                                                                                       763\u00b12ns\r\n[83.33%] \u00b7\u00b7\u00b7 \u001b[0;39mseries_methods.ToNumpy.time_to_numpy_copy\u001b[0m                                                                                                 385\u00b110\u03bcs\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mseries_methods.ToNumpy.time_to_numpy_double_copy\u001b[0m                                                                                          384\u00b110\u03bcs\r\n"]
[445.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[445.002, "i", "git add pandas/core/base.py\r"]
[445.004, "o", "git add pandas/core/base.py\r\n"]
[445.006, "o", "\u001b[?2004l\r\nwarning: could not open directory 'asv_bench/pandas/core/': No such file or directory\r\nfatal: pathspec 'pandas/core/base.py' did not match any files\r\n"]
[450.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[450.002, "i", "git commit -m \"Optimize IndexOpsMixin.to_numpy: add fastpath for NumPy-backed data when no dtype coercion/\r"]
[450.004, "o", "git commit -m \"Optimize IndexOpsMixin.to_numpy: add fastpath for NumPy-backed data when no dtype coercion/\r\n"]
[450.006, "o", "/na_value handling is needed. Avoid manager/asarray overhead; ~40% speedup on series_methods.ToNumpy.time_to_numpy.\"\r\n\u001b[?2004l\r\nAuthor identity unknown\r\n\r\n*** Please tell me who you are.\r\n\r\nRun\r\n\r\n  git config --global user.email \"you@example.com\"\r\n  git config --global user.name \"Your Name\"\r\n\r\nto set your account's default identity.\r\nOmit --global to set the identity only in this repository.\r\n\r\nfatal: unable to auto-detect email address (got 'root@a4408430bcc2.(none)')\r\n"]
[455.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[455.002, "i", "git status -s\r"]
[455.004, "o", "git status -s\r\n"]
[455.006, "o", "\u001b[?2004l\r\n \u001b[31mM\u001b[m ../pandas/core/base.py\r\n"]
[460.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[460.002, "i", "asv run --python=same --bench=\"series_methods.ToNumpy.*\"\r"]
[460.004, "o", "asv run --python=same --bench=\"series_methods.ToNumpy.*\"\r\n"]
[460.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 3 total benchmarks (1 commits * 1 environments * 3 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.8_bin_python\u001b[0m\r\n[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39mRunning (series_methods.ToNumpy.time_to_numpy--)\u001b[0m...\r\n[66.67%] \u00b7\u00b7\u00b7 \u001b[0;39mseries_methods.ToNumpy.time_to_numpy\u001b[0m                                                                                                       769\u00b13ns\r\n[83.33%] \u00b7\u00b7\u00b7 \u001b[0;39mseries_methods.ToNumpy.time_to_numpy_copy\u001b[0m                                                                                                 414\u00b150\u03bcs\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mseries_methods.ToNumpy.time_to_numpy_double_copy\u001b[0m                                                                                           383\u00b16\u03bcs\r\n"]
[465.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[465.002, "i", "cd /workspace/repo\r"]
[465.004, "o", "cd /workspace/repo\r\n"]
[465.006, "o", "\u001b[?2004l\r\n"]
[470.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[470.002, "i", "git config user.email \"you@example.com\"\r"]
[470.004, "o", "git config user.email \"you@example.com\"\r\n"]
[470.006, "o", "\u001b[?2004l\r\n"]
[475.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[475.002, "i", "git config user.name \"Your Name\"\r"]
[475.004, "o", "git config user.name \"Your Name\"\r\n"]
[475.006, "o", "\u001b[?2004l\r\n"]
[480.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[480.002, "i", "git add pandas/core/base.py\r"]
[480.004, "o", "git add pandas/core/base.py\r\n"]
[480.006, "o", "\u001b[?2004l\r\n"]
[485.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[485.002, "i", "git commit -m \"Optimize IndexOpsMixin.to_numpy: fastpath for NumPy-backed data when no dtype coercion/na_value handl\r"]
[485.004, "o", "git commit -m \"Optimize IndexOpsMixin.to_numpy: fastpath for NumPy-backed data when no dtype coercion/na_value handl\r\n"]
[485.006, "o", "ling is needed. Avoid manager/asarray overhead; ~40% speedup on Series.to_numpy benchmark.\"\r\n\u001b[?2004l\r\n[main dc21e49016] Optimize IndexOpsMixin.to_numpy: fastpath for NumPy-backed data when no dtype coercion/na_value handling is needed. Avoid manager/asarray overhead; ~40% speedup on Series.to_numpy benchmark.\r\n 1 file changed, 10 insertions(+)\r\n"]
[490.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo# "]
[490.002, "i", "cd asv_bench\r"]
[490.004, "o", "cd asv_bench\r\n"]
[490.006, "o", "\u001b[?2004l\r\n"]
[495.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[495.002, "i", "asv run --python=same --bench=\"series_methods.ToNumpy.*\"\r"]
[495.004, "o", "asv run --python=same --bench=\"series_methods.ToNumpy.*\"\r\n"]
[495.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 3 total benchmarks (1 commits * 1 environments * 3 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.8_bin_python\u001b[0m\r\n[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39mRunning (series_methods.ToNumpy.time_to_numpy--)\u001b[0m...\r\n[66.67%] \u00b7\u00b7\u00b7 \u001b[0;39mseries_methods.ToNumpy.time_to_numpy\u001b[0m                                                                                                       766\u00b12ns\r\n[83.33%] \u00b7\u00b7\u00b7 \u001b[0;39mseries_methods.ToNumpy.time_to_numpy_copy\u001b[0m                                                                                                  370\u00b19\u03bcs\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mseries_methods.ToNumpy.time_to_numpy_double_copy\u001b[0m                                                                                           369\u00b13\u03bcs\r\n"]
[500.0, "o", "(asv_3.8) root@a4408430bcc2:/workspace/repo/asv_bench# "]
[500.002, "i", "\u001b[?2004l\r"]
[500.004, "o", "\u001b[?2004l\r\n"]
[500.006, "o", "\r\nlogout\r\n"]
