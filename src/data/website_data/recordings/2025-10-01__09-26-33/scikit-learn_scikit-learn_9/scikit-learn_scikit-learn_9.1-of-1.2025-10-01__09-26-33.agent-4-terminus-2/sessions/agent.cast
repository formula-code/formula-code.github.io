{"version": 2, "width": 160, "height": 40, "timestamp": 1762995346, "env": {"SHELL": "/bin/bash", "TERM": "screen"}}
[0.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[0.002, "i", "asciinema rec --stdin /logs/agent.cast\r"]
[0.004, "o", "asciinema rec --stdin /logs/agent.cast\r\n"]
[1.242, "o", "\u001b[?2004l\r\n"]
[2.478, "o", "\u001b[0;31masciinema: /logs/agent.cast already exists, aborting\u001b[0m\r\n"]
[3.714, "o", "\u001b[0;31masciinema: use --overwrite option if you want to overwrite existing recording\u001b[0m\r\n"]
[5.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[5.002, "i", "clear\r"]
[5.004, "o", "clear\r\n"]
[7.478, "o", "\u001b[?2004l\r\n"]
[10.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[10.002, "i", "ls -la\r"]
[10.004, "o", "ls -la\r\n"]
[10.143333, "o", "\u001b[?2004l\r\n"]
[10.280667, "o", "total 176\r\n"]
[10.418, "o", "drwxr-xr-x 1 root root  4096 Oct  1 14:47 \u001b[0m\u001b[01;34m.\u001b[0m\r\n"]
[10.555333, "o", "drwxr-xr-x 1 root root  4096 Sep 15 17:25 \u001b[01;34m..\u001b[0m\r\n"]
[10.692667, "o", "drwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34m.binder\u001b[0m\r\n"]
[10.83, "o", "drwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34m.circleci\u001b[0m\r\n"]
[10.967333, "o", "-rw-r--r-- 1 root root  1392 Sep 13 19:09 .cirrus.star\r\n"]
[11.104667, "o", "-rw-r--r-- 1 root root   999 Sep 13 19:09 .codecov.yml\r\n"]
[11.242, "o", "-rw-r--r-- 1 root root   150 Sep 13 19:09 .coveragerc\r\n"]
[11.379333, "o", "drwxr-xr-x 1 root root  4096 Sep 15 17:27 \u001b[01;34m.git\u001b[0m\r\n"]
[11.516667, "o", "-rw-r--r-- 1 root root  1000 Sep 13 19:09 .git-blame-ignore-revs\r\n"]
[11.654, "o", "drwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34m.github\u001b[0m\r\n"]
[11.791333, "o", "-rw-r--r-- 1 root root  2004 Sep 13 19:09 .gitignore\r\n"]
[11.928667, "o", "-rw-r--r-- 1 root root  7263 Sep 13 19:08 .mailmap\r\n"]
[12.066, "o", "-rw-r--r-- 1 root root   872 Sep 13 19:09 .pre-commit-config.yaml\r\n"]
[12.203333, "o", "-rw-r--r-- 1 root root   645 Sep 13 19:09 CODE_OF_CONDUCT.md\r\n"]
[12.340667, "o", "-rw-r--r-- 1 root root  2109 Sep 13 19:09 CONTRIBUTING.md\r\n"]
[12.478, "o", "-rw-r--r-- 1 root root  1532 Sep 13 19:09 COPYING\r\n"]
[12.615333, "o", "-rw-r--r-- 1 root root   971 Sep 13 19:09 MANIFEST.in\r\n"]
[12.752667, "o", "-rw-r--r-- 1 root root  1491 Sep 13 19:09 Makefile\r\n"]
[12.89, "o", "-rw-r--r-- 1 root root  7604 Sep 13 19:09 README.rst\r\n"]
[13.027333, "o", "-rw-r--r-- 1 root root   692 Sep 13 19:09 SECURITY.md\r\n"]
[13.164667, "o", "drwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34masv_benchmarks\u001b[0m\r\n"]
[13.302, "o", "-rw-r--r-- 1 root root 11978 Sep 13 19:09 azure-pipelines.yml\r\n"]
[13.439333, "o", "drwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34mbenchmarks\u001b[0m\r\n"]
[13.576667, "o", "drwxr-xr-x 4 root root  4096 Sep 15 14:32 \u001b[01;34mbuild\u001b[0m\r\n"]
[13.714, "o", "drwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34mbuild_tools\u001b[0m\r\n"]
[13.851333, "o", "-rw-r--r-- 1 root root   388 Sep 13 19:09 conftest.py\r\n"]
[13.988667, "o", "drwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34mdoc\u001b[0m\r\n"]
[14.126, "o", "drwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34mexamples\u001b[0m\r\n"]
[14.263333, "o", "drwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34mmaint_tools\u001b[0m\r\n"]
[14.400667, "o", "-rw-r--r-- 1 root root  3838 Sep 13 19:09 pyproject.toml\r\n"]
[14.538, "o", "drwxr-xr-x 1 root root  4096 Sep 15 14:32 \u001b[01;34mscikit_learn.egg-info\u001b[0m\r\n"]
[14.675333, "o", "-rw-r--r-- 1 root root  2113 Sep 13 19:09 setup.cfg\r\n"]
[14.812667, "o", "-rwxr-xr-x 1 root root 22566 Sep 13 19:09 \u001b[01;32msetup.py\u001b[0m\r\n"]
[15.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[15.002, "i", "git status -sb\r"]
[15.004, "o", "git status -sb\r\n"]
[17.478, "o", "\u001b[?2004l\r\n"]
[20.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[20.002, "i", "ls -la benchmarks || ls -la asv_bench || ls -la asv\r"]
[20.004, "o", "ls -la benchmarks || ls -la asv_bench || ls -la asv\r\n"]
[20.113478, "o", "\u001b[?2004l\r\n"]
[20.220957, "o", "total 312\r\n"]
[20.328435, "o", "drwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[0m\u001b[01;34m.\u001b[0m\r\n"]
[20.435913, "o", "drwxr-xr-x 1 root root  4096 Oct  1 14:47 \u001b[01;34m..\u001b[0m\r\n"]
[20.543391, "o", "-rw-r--r-- 1 root root    41 Sep 13 19:08 .gitignore\r\n"]
[20.65087, "o", "-rw-r--r-- 1 root root  3121 Sep 13 19:08 bench_20newsgroups.py\r\n"]
[20.758348, "o", "-rw-r--r-- 1 root root  7338 Sep 13 19:09 bench_covertype.py\r\n"]
[20.865826, "o", "-rw-r--r-- 1 root root  1774 Sep 13 19:08 bench_feature_expansions.py\r\n"]
[20.973304, "o", "-rw-r--r-- 1 root root  1478 Sep 13 19:09 bench_glm.py\r\n"]
[21.080783, "o", "-rw-r--r-- 1 root root  3967 Sep 13 19:09 bench_glmnet.py\r\n"]
[21.188261, "o", "-rw-r--r-- 1 root root  9516 Sep 13 19:08 bench_hist_gradient_boosting.py\r\n"]
[21.295739, "o", "-rw-r--r-- 1 root root  3525 Sep 13 19:09 bench_hist_gradient_boosting_adult.py\r\n"]
[21.403217, "o", "-rw-r--r-- 1 root root  2622 Sep 13 19:08 bench_hist_gradient_boosting_categorical_only.py\r\n"]
[21.510696, "o", "-rw-r--r-- 1 root root  4120 Sep 13 19:09 bench_hist_gradient_boosting_higgsboson.py\r\n"]
[21.618174, "o", "-rw-r--r-- 1 root root 11031 Sep 13 19:08 bench_hist_gradient_boosting_threading.py\r\n"]
[21.725652, "o", "-rw-r--r-- 1 root root  5513 Sep 13 19:09 bench_isolation_forest.py\r\n"]
[21.83313, "o", "-rw-r--r-- 1 root root  3274 Sep 13 19:09 bench_isotonic.py\r\n"]
[21.940609, "o", "-rw-r--r-- 1 root root  5461 Sep 13 19:09 bench_kernel_pca_solvers_time_vs_n_components.py\r\n"]
[22.048087, "o", "-rw-r--r-- 1 root root  5729 Sep 13 19:09 bench_kernel_pca_solvers_time_vs_n_samples.py\r\n"]
[22.155565, "o", "-rw-r--r-- 1 root root  3097 Sep 13 19:09 bench_lasso.py\r\n"]
[22.263043, "o", "-rw-r--r-- 1 root root  3543 Sep 13 19:09 bench_lof.py\r\n"]
[22.370522, "o", "-rw-r--r-- 1 root root  6994 Sep 13 19:09 bench_mnist.py\r\n"]
[22.478, "o", "-rwxr-xr-x 1 root root  6769 Sep 13 19:08 \u001b[01;32mbench_multilabel_metrics.py\u001b[0m\r\n"]
[22.585478, "o", "-rw-r--r-- 1 root root  9419 Sep 13 19:08 bench_online_ocsvm.py\r\n"]
[22.692957, "o", "-rw-r--r-- 1 root root  4419 Sep 13 19:09 bench_plot_fastkmeans.py\r\n"]
[22.800435, "o", "-rw-r--r-- 1 root root  2555 Sep 13 19:08 bench_plot_hierarchical.py\r\n"]
[22.907913, "o", "-rw-r--r-- 1 root root  5561 Sep 13 19:08 bench_plot_incremental_pca.py\r\n"]
[23.015391, "o", "-rw-r--r-- 1 root root  3909 Sep 13 19:09 bench_plot_lasso_path.py\r\n"]
[23.12287, "o", "-rw-r--r-- 1 root root  5719 Sep 13 19:09 bench_plot_neighbors.py\r\n"]
[23.230348, "o", "-rw-r--r-- 1 root root 15557 Sep 13 19:09 bench_plot_nmf.py\r\n"]
[23.337826, "o", "-rw-r--r-- 1 root root  4415 Sep 13 19:09 bench_plot_omp_lars.py\r\n"]
[23.445304, "o", "-rw-r--r-- 1 root root  1236 Sep 13 19:09 bench_plot_parallel_pairwise.py\r\n"]
[23.552783, "o", "-rw-r--r-- 1 root root  5995 Sep 13 19:09 bench_plot_polynomial_kernel_approximation.py\r\n"]
[23.660261, "o", "-rw-r--r-- 1 root root 18086 Sep 13 19:09 bench_plot_randomized_svd.py\r\n"]
[23.767739, "o", "-rw-r--r-- 1 root root  2749 Sep 13 19:09 bench_plot_svd.py\r\n"]
[23.875217, "o", "-rw-r--r-- 1 root root  1270 Sep 13 19:08 bench_plot_ward.py\r\n"]
[23.982696, "o", "-rw-r--r-- 1 root root  8566 Sep 13 19:09 bench_random_projections.py\r\n"]
[24.090174, "o", "-rw-r--r-- 1 root root  7802 Sep 13 19:09 bench_rcv1_logreg_convergence.py\r\n"]
[24.197652, "o", "-rw-r--r-- 1 root root 10868 Sep 13 19:09 bench_saga.py\r\n"]
[24.30513, "o", "-rw-r--r-- 1 root root  7430 Sep 13 19:09 bench_sample_without_replacement.py\r\n"]
[24.412609, "o", "-rw-r--r-- 1 root root  5339 Sep 13 19:09 bench_sgd_regression.py\r\n"]
[24.520087, "o", "-rw-r--r-- 1 root root  3357 Sep 13 19:08 bench_sparsify.py\r\n"]
[24.627565, "o", "-rw-r--r-- 1 root root  1919 Sep 13 19:09 bench_text_vectorizers.py\r\n"]
[24.735043, "o", "-rw-r--r-- 1 root root  3623 Sep 13 19:09 bench_tree.py\r\n"]
[24.842522, "o", "-rw-r--r-- 1 root root  6387 Sep 13 19:09 bench_tsne_mnist.py\r\n"]
[25.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[25.002, "i", "grep -R \"MiniBatchDictionaryLearning\" -n . || true\r"]
[25.004, "o", "grep -R \"MiniBatchDictionaryLearning\" -n . || true\r\n"]
[25.072811, "o", "\u001b[?2004l\r\n"]
[25.139622, "o", "\u001b[35m\u001b[K./examples/decomposition/plot_image_denoising.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K118\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kfrom sklearn.decomposition import \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K\r\n"]
[25.206432, "o", "\u001b[35m\u001b[K./examples/decomposition/plot_image_denoising.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K122\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdico = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n"]
[25.273243, "o", "\u001b[35m\u001b[K./examples/decomposition/plot_faces_decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K167\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K# By default, :class:`\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K` divides the data into\r\n"]
[25.340054, "o", "\u001b[35m\u001b[K./examples/decomposition/plot_faces_decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K172\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kbatch_dict_estimator = decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n"]
[25.406865, "o", "\u001b[35m\u001b[K./examples/decomposition/plot_faces_decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K242\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K# :class:`\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K` implements a faster, but less accurate\r\n"]
[25.473676, "o", "\u001b[35m\u001b[K./examples/decomposition/plot_faces_decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K244\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K# datasets. Read more in the :ref:`User Guide <\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K>`.\r\n"]
[25.540486, "o", "\u001b[35m\u001b[K./examples/decomposition/plot_faces_decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K255\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K# `\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K` estimator on all images. Generally,\r\n"]
[25.607297, "o", "\u001b[35m\u001b[K./examples/decomposition/plot_faces_decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K272\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdict_pos_dict_estimator = decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n"]
[25.674108, "o", "\u001b[35m\u001b[K./examples/decomposition/plot_faces_decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K294\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdict_pos_code_estimator = decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n"]
[25.740919, "o", "\u001b[35m\u001b[K./examples/decomposition/plot_faces_decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K318\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdict_pos_estimator = decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n"]
[25.80773, "o", "\u001b[35m\u001b[K./doc/modules/decomposition.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K592\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K.. _\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K:\r\n"]
[25.874541, "o", "\u001b[35m\u001b[K./doc/modules/decomposition.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K597\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K:class:`\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K` implements a faster, but less accurate\r\n"]
[25.941351, "o", "\u001b[35m\u001b[K./doc/modules/decomposition.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K601\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[KBy default, :class:`\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K` divides the data into\r\n"]
[26.008162, "o", "\u001b[35m\u001b[K./doc/modules/classes.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K324\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K   decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K\r\n"]
[26.074973, "o", "\u001b[35m\u001b[K./doc/whats_new/v1.1.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K542\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K  :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K`\r\n"]
[26.141784, "o", "\u001b[35m\u001b[K./doc/whats_new/v1.1.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K551\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K- |Enhancement| The :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K` and\r\n"]
[26.208595, "o", "\u001b[35m\u001b[K./doc/whats_new/v1.1.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K562\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K` serve internal purpose and are\r\n"]
[26.275405, "o", "\u001b[35m\u001b[K./doc/whats_new/v1.1.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K585\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K  :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K`,\r\n"]
[26.342216, "o", "\u001b[35m\u001b[K./doc/whats_new/v1.0.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K41\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K  :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K`, :class:`decomposition.SparsePCA`\r\n"]
[26.409027, "o", "\u001b[35m\u001b[K./doc/whats_new/v1.0.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K595\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K- |Fix| Fixed a bug in :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K`,\r\n"]
[26.475838, "o", "\u001b[35m\u001b[K./doc/whats_new/v1.0.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K602\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K  :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K`,\r\n"]
[26.542649, "o", "\u001b[35m\u001b[K./doc/whats_new/v1.0.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K610\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K  :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K`,\r\n"]
[26.609459, "o", "\u001b[35m\u001b[K./doc/whats_new/v0.23.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K55\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K  :func:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K.partial_fit` which should\r\n"]
[26.67627, "o", "\u001b[35m\u001b[K./doc/whats_new/v1.2.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K121\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K- |Fix| The fitted components in :class:`\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K` might differ. The\r\n"]
[26.743081, "o", "\u001b[35m\u001b[K./doc/whats_new/v1.2.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K175\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K- |Fix| Fixed a bug in :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K` where the\r\n"]
[26.809892, "o", "\u001b[35m\u001b[K./doc/whats_new/v1.2.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K533\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K  :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K`. `n_iter` will be removed\r\n"]
[26.876703, "o", "\u001b[35m\u001b[K./doc/whats_new/v1.3.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K311\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K- |Efficiency| :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K` and\r\n"]
[26.943514, "o", "\u001b[35m\u001b[K./doc/whats_new/v0.22.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K241\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K  :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K` |Fix|\r\n"]
[27.010324, "o", "\u001b[35m\u001b[K./doc/whats_new/v0.22.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K404\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K  :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K` now take a\r\n"]
[27.077135, "o", "\u001b[35m\u001b[K./doc/whats_new/v0.15.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K333\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K- Fixed bug in :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K` :\r\n"]
[27.143946, "o", "\u001b[35m\u001b[K./doc/whats_new/v0.13.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K220\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K  :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K`,\r\n"]
[27.210757, "o", "\u001b[35m\u001b[K./doc/whats_new/v0.13.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K294\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K  :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K` and\r\n"]
[27.277568, "o", "\u001b[35m\u001b[K./doc/computing/scaling_strategies.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K80\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K      + :class:`sklearn.decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K`\r\n"]
[27.344378, "o", "\u001b[35m\u001b[K./sklearn/utils/estimator_checks.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K680\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        # \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K\r\n"]
[27.411189, "o", "\u001b[35m\u001b[K./sklearn/utils/estimator_checks.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K681\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        if name == \"\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K\":\r\n"]
[27.478, "o", "\u001b[35m\u001b[K./sklearn/decomposition/_nmf.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K1981\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K : Finds a dictionary that can best be used to represent\r\n"]
[27.544811, "o", "\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K12\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K,\r\n"]
[27.611622, "o", "\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K328\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    dico = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n"]
[27.678432, "o", "\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K354\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    dico = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n"]
[27.745243, "o", "\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K407\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        dico = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n"]
[27.812054, "o", "\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K411\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        dico = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n"]
[27.878865, "o", "\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K421\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        dico = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n"]
[27.945676, "o", "\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K451\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    dico = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n"]
[28.012486, "o", "\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K460\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    dico = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n"]
[28.079297, "o", "\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K470\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    dico = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n"]
[28.146108, "o", "\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K481\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n"]
[28.212919, "o", "\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K496\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    dict1 = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n"]
[28.27973, "o", "\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K507\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    dict2 = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n"]
[28.346541, "o", "\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K664\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    est = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n"]
[28.413351, "o", "\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K831\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    dict_learner = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n"]
[28.480162, "o", "\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K978\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(batch_size=4, max_iter=10),\r\n"]
[28.546973, "o", "\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K1025\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        model = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(batch_size=256, n_iter=2, max_iter=2).fit(X)\r\n"]
[28.613784, "o", "\u001b[35m\u001b[K./sklearn/decomposition/_sparse_pca.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K20\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kfrom ._dict_learning import \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K, dict_learning\r\n"]
[28.680595, "o", "\u001b[35m\u001b[K./sklearn/decomposition/_sparse_pca.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K532\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        est = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n"]
[28.747405, "o", "\u001b[35m\u001b[K./sklearn/decomposition/__init__.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K11\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K,\r\n"]
[28.814216, "o", "\u001b[35m\u001b[K./sklearn/decomposition/__init__.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K36\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \"\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K\",\r\n"]
[28.881027, "o", "\u001b[35m\u001b[K./sklearn/decomposition/_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K858\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K : A faster, less accurate, version of the dictionary\r\n"]
[28.947838, "o", "\u001b[35m\u001b[K./sklearn/decomposition/_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K884\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            \"'n_steps_' attributes of the \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K object instead.\"\r\n"]
[29.014649, "o", "\u001b[35m\u001b[K./sklearn/decomposition/_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K891\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        est = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n"]
[29.081459, "o", "\u001b[35m\u001b[K./sklearn/decomposition/_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K1209\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K : A faster, less accurate version\r\n"]
[29.14827, "o", "\u001b[35m\u001b[K./sklearn/decomposition/_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K1405\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K : A faster, less accurate, version of the\r\n"]
[29.215081, "o", "\u001b[35m\u001b[K./sklearn/decomposition/_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K1675\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K: A faster, less accurate, version of the\r\n"]
[29.281892, "o", "\u001b[35m\u001b[K./sklearn/decomposition/_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K1867\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kclass \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(_BaseSparseCoding, BaseEstimator):\r\n"]
[29.348703, "o", "\u001b[35m\u001b[K./sklearn/decomposition/_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K2062\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    >>> from sklearn.decomposition import \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K\r\n"]
[29.415514, "o", "\u001b[35m\u001b[K./sklearn/decomposition/_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K2066\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    >>> dict_learner = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n"]
[29.482324, "o", "grep: ./sklearn/decomposition/__pycache__/_sparse_pca.cpython-310.pyc: binary file matches\r\n"]
[29.549135, "o", "grep: ./sklearn/decomposition/__pycache__/__init__.cpython-310.pyc: binary file matches\r\n"]
[29.615946, "o", "grep: ./sklearn/decomposition/__pycache__/_nmf.cpython-310.pyc: binary file matches\r\n"]
[29.682757, "o", "grep: ./sklearn/decomposition/__pycache__/_dict_learning.cpython-310.pyc: binary file matches\r\n"]
[29.749568, "o", "\u001b[35m\u001b[K./asv_benchmarks/benchmarks/decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K1\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kfrom sklearn.decomposition import PCA, DictionaryLearning, \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K\r\n"]
[29.816378, "o", "\u001b[35m\u001b[K./asv_benchmarks/benchmarks/decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K67\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kclass \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[KBenchmark(Transformer, Estimator, Benchmark):\r\n"]
[29.883189, "o", "\u001b[35m\u001b[K./asv_benchmarks/benchmarks/decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K69\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    Benchmarks for \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K\r\n"]
[30.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[30.002, "i", "grep -R \"asv\" -n . | head -n 50\r"]
[30.004, "o", "grep -R \"asv\" -n . | head -n 50\r\n"]
[30.101077, "o", "\u001b[?2004l\r\n"]
[30.196154, "o", "grep: ./sklearn/metrics/cluster/_expected_mutual_info_fast.cpython-310-x86_64-linux-gnu.so: binary file matches\r\n"]
[30.291231, "o", "./doc/developers/contributing.rst:1051:`asv benchmarks <https://github.com/airspeed-velocity/asv>`_ to monitor the\r\n"]
[30.386308, "o", "./doc/developers/contributing.rst:1054:The corresponding benchmark suite can be found in the `scikit-learn/asv_benchmarks` directory.\r\n"]
[30.481385, "o", "./doc/developers/contributing.rst:1056:To use all features of asv, you will need either `conda` or `virtualenv`. For\r\n"]
[30.576462, "o", "./doc/developers/contributing.rst:1057:more details please check the `asv installation webpage\r\n"]
[30.671538, "o", "./doc/developers/contributing.rst:1058:<https://asv.readthedocs.io/en/latest/installing.html>`_.\r\n"]
[30.766615, "o", "./doc/developers/contributing.rst:1060:First of all you need to install the development version of asv:\r\n"]
[30.861692, "o", "./doc/developers/contributing.rst:1064:    pip install git+https://github.com/airspeed-velocity/asv\r\n"]
[30.956769, "o", "./doc/developers/contributing.rst:1066:and change your directory to `asv_benchmarks/`:\r\n"]
[31.051846, "o", "./doc/developers/contributing.rst:1070:  cd asv_benchmarks/\r\n"]
[31.146923, "o", "./doc/developers/contributing.rst:1085:  asv continuous -b LogisticRegression upstream/main HEAD\r\n"]
[31.242, "o", "./doc/developers/contributing.rst:1092:  asv continuous -E virtualenv -b LogisticRegression upstream/main HEAD\r\n"]
[31.337077, "o", "./doc/developers/contributing.rst:1098:  asv continuous -b linear_model upstream/main HEAD\r\n"]
[31.432154, "o", "./doc/developers/contributing.rst:1108:  asv continuous upstream/main HEAD\r\n"]
[31.527231, "o", "./doc/developers/contributing.rst:1118:  asv run -b linear_model HEAD^!\r\n"]
[31.622308, "o", "./doc/developers/contributing.rst:1125:  asv run --python=same\r\n"]
[31.717385, "o", "./doc/developers/contributing.rst:1134:  asv run --python=same --set-commit-hash=<commit hash>\r\n"]
[31.812462, "o", "./doc/developers/contributing.rst:1141:  asv show\r\n"]
[31.907538, "o", "./doc/developers/contributing.rst:1147:  asv show <commit hash>\r\n"]
[32.002615, "o", "./doc/developers/contributing.rst:1156:More information on how to write a benchmark and how to use asv can be found in\r\n"]
[32.097692, "o", "./doc/developers/contributing.rst:1157:the `asv documentation <https://asv.readthedocs.io/en/latest/index.html>`_.\r\n"]
[32.192769, "o", "./sklearn/metrics/cluster/_expected_mutual_info_fast.c:7:            \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/core/include/numpy/arrayobject.h\",\r\n"]
[32.287846, "o", "./sklearn/metrics/cluster/_expected_mutual_info_fast.c:8:            \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/core/include/numpy/arrayscalars.h\",\r\n"]
[32.382923, "o", "./sklearn/metrics/cluster/_expected_mutual_info_fast.c:9:            \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/core/include/numpy/ndarrayobject.h\",\r\n"]
[32.478, "o", "./sklearn/metrics/cluster/_expected_mutual_info_fast.c:10:            \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/core/include/numpy/ndarraytypes.h\",\r\n"]
[32.573077, "o", "./sklearn/metrics/cluster/_expected_mutual_info_fast.c:11:            \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/core/include/numpy/ufuncobject.h\"\r\n"]
[32.668154, "o", "./sklearn/metrics/cluster/_expected_mutual_info_fast.c:18:            \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/core/include\"\r\n"]
[32.763231, "o", "./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1134:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":688\r\n"]
[32.858308, "o", "./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1143:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":689\r\n"]
[32.953385, "o", "./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1152:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":690\r\n"]
[33.048462, "o", "./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1161:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":691\r\n"]
[33.143538, "o", "./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1170:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":695\r\n"]
[33.238615, "o", "./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1179:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":696\r\n"]
[33.333692, "o", "./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1188:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":697\r\n"]
[33.428769, "o", "./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1197:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":698\r\n"]
[33.523846, "o", "./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1206:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":702\r\n"]
[33.618923, "o", "./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1215:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":703\r\n"]
[33.714, "o", "./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1224:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":712\r\n"]
[33.809077, "o", "./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1233:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":713\r\n"]
[33.904154, "o", "./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1242:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":715\r\n"]
[33.999231, "o", "./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1251:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":716\r\n"]
[34.094308, "o", "./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1260:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":718\r\n"]
[34.189385, "o", "./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1269:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":719\r\n"]
[34.284462, "o", "./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1278:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":721\r\n"]
[34.379538, "o", "./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1287:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":722\r\n"]
[34.474615, "o", "./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1296:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":723\r\n"]
[34.569692, "o", "./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1335:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":725\r\n"]
[34.664769, "o", "./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1344:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":726\r\n"]
[34.759846, "o", "./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1353:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":727\r\n"]
[34.854923, "o", "./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1362:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":729\r\n"]
[35.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[35.002, "i", "fd -a asv.conf.json || find . -maxdepth 3 -name 'asv.conf.json'\r"]
[35.004, "o", "fd -a asv.conf.json || find . -maxdepth 3 -name 'asv.conf.json'\r\n"]
[36.654, "o", "\u001b[?2004l\r\n"]
[38.302, "o", "-bash: fd: command not found\r\n"]
[40.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[40.002, "i", "cat asv_benchmarks.txt 2>/dev/null || true\r"]
[40.004, "o", "cat asv_benchmarks.txt 2>/dev/null || true\r\n"]
[40.006, "o", "\u001b[?2004l\r\n"]
[45.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[45.002, "i", "rg -n \"class .*MiniBatchDictionaryLearningBenchmark|MiniBatchDictionaryLearningBenchmark\" -S benchmarks 2>/dev/null\r"]
[45.004, "o", "rg -n \"class .*MiniBatchDictionaryLearningBenchmark|MiniBatchDictionaryLearningBenchmark\" -S benchmarks 2>/dev/null\r\n"]
[46.654, "o", "l || rg -n \"MiniBatchDictionaryLearningBenchmark\" -S .\r\n"]
[48.302, "o", "\u001b[?2004l\r\n"]
[50.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[50.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\" --show-stderr\r"]
[50.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\" --show-stderr\r\n"]
[52.478, "o", "\u001b[?2004l\r\n"]
[55.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[55.002, "i", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r"]
[55.004, "o", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r\n"]
[57.478, "o", "\u001b[?2004l\r\n"]
[60.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[60.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[60.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[62.478, "o", "\u001b[?2004l\r\n"]
[65.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[65.002, "i", "ls -la asv_benchmarks\r"]
[65.004, "o", "ls -la asv_benchmarks\r\n"]
[65.712286, "o", "\u001b[?2004l\r\n"]
[66.418571, "o", "total 28\r\n"]
[67.124857, "o", "drwxr-xr-x 1 root root 4096 Sep 13 19:09 \u001b[0m\u001b[01;34m.\u001b[0m\r\n"]
[67.831143, "o", "drwxr-xr-x 1 root root 4096 Oct  1 14:47 \u001b[01;34m..\u001b[0m\r\n"]
[68.537429, "o", "-rw-r--r-- 1 root root   66 Sep 13 19:08 .gitignore\r\n"]
[69.243714, "o", "-rw-r--r-- 1 root root 6754 Sep 13 19:09 asv.conf.json\r\n"]
[70.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[70.002, "i", "ls -la asv_benchmarks/benchmarks\r"]
[70.004, "o", "ls -la asv_benchmarks/benchmarks\r\n"]
[70.280667, "o", "\u001b[?2004l\r\n"]
[70.555333, "o", "total 76\r\n"]
[70.83, "o", "drwxr-xr-x 1 root root 4096 Sep 13 19:09 \u001b[0m\u001b[01;34m.\u001b[0m\r\n"]
[71.104667, "o", "drwxr-xr-x 1 root root 4096 Sep 13 19:09 \u001b[01;34m..\u001b[0m\r\n"]
[71.379333, "o", "-rw-r--r-- 1 root root   49 Sep 13 19:08 __init__.py\r\n"]
[71.654, "o", "-rw-r--r-- 1 root root 2925 Sep 13 19:08 cluster.py\r\n"]
[71.928667, "o", "-rw-r--r-- 1 root root 7341 Sep 13 19:09 common.py\r\n"]
[72.203333, "o", "-rw-r--r-- 1 root root 1544 Sep 13 19:09 config.json\r\n"]
[72.478, "o", "-rw-r--r-- 1 root root 5176 Sep 13 19:09 datasets.py\r\n"]
[72.752667, "o", "-rw-r--r-- 1 root root 2406 Sep 13 19:08 decomposition.py\r\n"]
[73.027333, "o", "-rw-r--r-- 1 root root 2995 Sep 13 19:08 ensemble.py\r\n"]
[73.302, "o", "-rw-r--r-- 1 root root 6648 Sep 13 19:09 linear_model.py\r\n"]
[73.576667, "o", "-rw-r--r-- 1 root root  820 Sep 13 19:08 manifold.py\r\n"]
[73.851333, "o", "-rw-r--r-- 1 root root 1363 Sep 13 19:08 metrics.py\r\n"]
[74.126, "o", "-rw-r--r-- 1 root root 2371 Sep 13 19:08 model_selection.py\r\n"]
[74.400667, "o", "-rw-r--r-- 1 root root 1140 Sep 13 19:08 neighbors.py\r\n"]
[74.675333, "o", "-rw-r--r-- 1 root root  762 Sep 13 19:08 svm.py\r\n"]
[75.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[75.002, "i", "grep -R \"MiniBatchDictionaryLearningBenchmark\" -n asv_benchmarks/benchmarks || true\r"]
[75.004, "o", "grep -R \"MiniBatchDictionaryLearningBenchmark\" -n asv_benchmarks/benchmarks || true\r\n"]
[77.478, "o", "\u001b[?2004l\r\n"]
[80.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[80.002, "i", "grep -R \"DictionaryLearningBenchmark\\|MiniBatchDictionaryLearning\" -n asv_benchmarks/benchmarks || true\r"]
[80.004, "o", "grep -R \"DictionaryLearningBenchmark\\|MiniBatchDictionaryLearning\" -n asv_benchmarks/benchmarks || true\r\n"]
[80.83, "o", "\u001b[?2004l\r\n"]
[81.654, "o", "\u001b[35m\u001b[Kasv_benchmarks/benchmarks/decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K1\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kfrom sklearn.decomposition import PCA, DictionaryLearning, \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K\r\n"]
[82.478, "o", "\u001b[35m\u001b[Kasv_benchmarks/benchmarks/decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K33\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kclass \u001b[01;31m\u001b[KDictionaryLearningBenchmark\u001b[m\u001b[K(Transformer, Estimator, Benchmark):\r\n"]
[83.302, "o", "\u001b[35m\u001b[Kasv_benchmarks/benchmarks/decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K67\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kclass \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[KBenchmark(Transformer, Estimator, Benchmark):\r\n"]
[84.126, "o", "\u001b[35m\u001b[Kasv_benchmarks/benchmarks/decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K69\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    Benchmarks for \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K\r\n"]
[85.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[85.002, "i", "asv run --config asv_benchmarks/asv.conf.json --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBench\r"]
[85.004, "o", "asv run --config asv_benchmarks/asv.conf.json --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBench\r\n"]
[86.654, "o", "hmark.*\" --show-stderr\r\n"]
[88.302, "o", "\u001b[?2004l\r\n"]
[90.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[90.002, "i", "asv show --config asv_benchmarks/asv.conf.json --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenc\r"]
[90.004, "o", "asv show --config asv_benchmarks/asv.conf.json --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenc\r\n"]
[91.654, "o", "chmark.*\"\r\n"]
[93.302, "o", "\u001b[?2004l\r\n"]
[95.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[95.002, "i", "asv profile --config asv_benchmarks/asv.conf.json --python=same decomposition.MiniBatchDictionaryLearningBenchmark.\r"]
[95.004, "o", "asv profile --config asv_benchmarks/asv.conf.json --python=same decomposition.MiniBatchDictionaryLearningBenchmark.\r\n"]
[96.654, "o", ".time_fit\r\n"]
[98.302, "o", "\u001b[?2004l\r\n"]
[100.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[100.002, "i", "grep -R \"class MiniBatchDictionaryLearning\\|def .*MiniBatchDictionaryLearning\" -n sklearn/decomposition || true\r"]
[100.004, "o", "grep -R \"class MiniBatchDictionaryLearning\\|def .*MiniBatchDictionaryLearning\" -n sklearn/decomposition || true\r\n"]
[102.478, "o", "\u001b[?2004l\r\n"]
[105.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[105.002, "i", "sed -n '1,220p' sklearn/decomposition/_dict_learning.py\r"]
[105.004, "o", "sed -n '1,220p' sklearn/decomposition/_dict_learning.py\r\n"]
[105.028371, "o", "\u001b[?2004l\r\n"]
[105.050742, "o", "\"\"\" Dictionary learning.\r\n"]
[105.073113, "o", "\"\"\"\r\n"]
[105.095484, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[105.117855, "o", "# License: BSD 3 clause\r\n"]
[105.140226, "o", "\r\n"]
[105.162597, "o", "import itertools\r\n"]
[105.184968, "o", "import sys\r\n"]
[105.207339, "o", "import time\r\n"]
[105.22971, "o", "import warnings\r\n"]
[105.252081, "o", "from math import ceil\r\n"]
[105.274452, "o", "from numbers import Integral, Real\r\n"]
[105.296824, "o", "\r\n"]
[105.319195, "o", "import numpy as np\r\n"]
[105.341566, "o", "from joblib import effective_n_jobs\r\n"]
[105.363937, "o", "from scipy import linalg\r\n"]
[105.386308, "o", "\r\n"]
[105.408679, "o", "from ..base import (\r\n"]
[105.43105, "o", "    BaseEstimator,\r\n"]
[105.453421, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[105.475792, "o", "    TransformerMixin,\r\n"]
[105.498163, "o", "    _fit_context,\r\n"]
[105.520534, "o", ")\r\n"]
[105.542905, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[105.565276, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[105.587647, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[105.610018, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[105.632389, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[105.65476, "o", "from ..utils.validation import check_is_fitted\r\n"]
[105.677131, "o", "\r\n"]
[105.699502, "o", "\r\n"]
[105.721873, "o", "def _check_positive_coding(method, positive):\r\n"]
[105.744244, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[105.766615, "o", "        raise ValueError(\r\n"]
[105.788986, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[105.811357, "o", "        )\r\n"]
[105.833729, "o", "\r\n"]
[105.8561, "o", "\r\n"]
[105.878471, "o", "def _sparse_encode_precomputed(\r\n"]
[105.900842, "o", "    X,\r\n"]
[105.923213, "o", "    dictionary,\r\n"]
[105.945584, "o", "    *,\r\n"]
[105.967955, "o", "    gram=None,\r\n"]
[105.990326, "o", "    cov=None,\r\n"]
[106.012697, "o", "    algorithm=\"lasso_lars\",\r\n"]
[106.035068, "o", "    regularization=None,\r\n"]
[106.057439, "o", "    copy_cov=True,\r\n"]
[106.07981, "o", "    init=None,\r\n"]
[106.102181, "o", "    max_iter=1000,\r\n"]
[106.124552, "o", "    verbose=0,\r\n"]
[106.146923, "o", "    positive=False,\r\n"]
[106.169294, "o", "):\r\n"]
[106.191665, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[106.214036, "o", "\r\n"]
[106.236407, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[106.258778, "o", "\r\n"]
[106.281149, "o", "    Parameters\r\n"]
[106.30352, "o", "    ----------\r\n"]
[106.325891, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[106.348262, "o", "        Data matrix.\r\n"]
[106.370633, "o", "\r\n"]
[106.393005, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[106.415376, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[106.437747, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[106.460118, "o", "\r\n"]
[106.482489, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[106.50486, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[106.527231, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[106.549602, "o", "\r\n"]
[106.571973, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[106.594344, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[106.616715, "o", "\r\n"]
[106.639086, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[106.661457, "o", "            default='lasso_lars'\r\n"]
[106.683828, "o", "        The algorithm used:\r\n"]
[106.706199, "o", "\r\n"]
[106.72857, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[106.750941, "o", "          (`linear_model.lars_path`);\r\n"]
[106.773312, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[106.795683, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[106.818054, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[106.840425, "o", "          the estimated components are sparse;\r\n"]
[106.862796, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[106.885167, "o", "          solution;\r\n"]
[106.907538, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[106.92991, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[106.952281, "o", "\r\n"]
[106.974652, "o", "    regularization : int or float, default=None\r\n"]
[106.997023, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[107.019394, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[107.041765, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[107.064136, "o", "\r\n"]
[107.086507, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[107.108878, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[107.131249, "o", "        `algorithm='lasso_cd'`.\r\n"]
[107.15362, "o", "\r\n"]
[107.175991, "o", "    max_iter : int, default=1000\r\n"]
[107.198362, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[107.220733, "o", "        `'lasso_lars'`.\r\n"]
[107.243104, "o", "\r\n"]
[107.265475, "o", "    copy_cov : bool, default=True\r\n"]
[107.287846, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[107.310217, "o", "        be overwritten.\r\n"]
[107.332588, "o", "\r\n"]
[107.354959, "o", "    verbose : int, default=0\r\n"]
[107.37733, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[107.399701, "o", "\r\n"]
[107.422072, "o", "    positive: bool, default=False\r\n"]
[107.444443, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[107.466814, "o", "\r\n"]
[107.489186, "o", "        .. versionadded:: 0.20\r\n"]
[107.511557, "o", "\r\n"]
[107.533928, "o", "    Returns\r\n"]
[107.556299, "o", "    -------\r\n"]
[107.57867, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[107.601041, "o", "        The sparse codes.\r\n"]
[107.623412, "o", "    \"\"\"\r\n"]
[107.645783, "o", "    n_samples, n_features = X.shape\r\n"]
[107.668154, "o", "    n_components = dictionary.shape[0]\r\n"]
[107.690525, "o", "\r\n"]
[107.712896, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[107.735267, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[107.757638, "o", "        try:\r\n"]
[107.780009, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[107.80238, "o", "\r\n"]
[107.824751, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[107.847122, "o", "            # corrects the verbosity level.\r\n"]
[107.869493, "o", "            lasso_lars = LassoLars(\r\n"]
[107.891864, "o", "                alpha=alpha,\r\n"]
[107.914235, "o", "                fit_intercept=False,\r\n"]
[107.936606, "o", "                verbose=verbose,\r\n"]
[107.958977, "o", "                precompute=gram,\r\n"]
[107.981348, "o", "                fit_path=False,\r\n"]
[108.003719, "o", "                positive=positive,\r\n"]
[108.02609, "o", "                max_iter=max_iter,\r\n"]
[108.048462, "o", "            )\r\n"]
[108.070833, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[108.093204, "o", "            new_code = lasso_lars.coef_\r\n"]
[108.115575, "o", "        finally:\r\n"]
[108.137946, "o", "            np.seterr(**err_mgt)\r\n"]
[108.160317, "o", "\r\n"]
[108.182688, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[108.205059, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[108.22743, "o", "\r\n"]
[108.249801, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[108.272172, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[108.294543, "o", "        # argument that we could pass in from Lasso.\r\n"]
[108.316914, "o", "        clf = Lasso(\r\n"]
[108.339285, "o", "            alpha=alpha,\r\n"]
[108.361656, "o", "            fit_intercept=False,\r\n"]
[108.384027, "o", "            precompute=gram,\r\n"]
[108.406398, "o", "            max_iter=max_iter,\r\n"]
[108.428769, "o", "            warm_start=True,\r\n"]
[108.45114, "o", "            positive=positive,\r\n"]
[108.473511, "o", "        )\r\n"]
[108.495882, "o", "\r\n"]
[108.518253, "o", "        if init is not None:\r\n"]
[108.540624, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[108.562995, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[108.585367, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[108.607738, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[108.630109, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[108.65248, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[108.674851, "o", "                init = np.array(init)\r\n"]
[108.697222, "o", "            clf.coef_ = init\r\n"]
[108.719593, "o", "\r\n"]
[108.741964, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[108.764335, "o", "        new_code = clf.coef_\r\n"]
[108.786706, "o", "\r\n"]
[108.809077, "o", "    elif algorithm == \"lars\":\r\n"]
[108.831448, "o", "        try:\r\n"]
[108.853819, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[108.87619, "o", "\r\n"]
[108.898561, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[108.920932, "o", "            # corrects the verbosity level.\r\n"]
[108.943303, "o", "            lars = Lars(\r\n"]
[108.965674, "o", "                fit_intercept=False,\r\n"]
[108.988045, "o", "                verbose=verbose,\r\n"]
[109.010416, "o", "                precompute=gram,\r\n"]
[109.032787, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[109.055158, "o", "                fit_path=False,\r\n"]
[109.077529, "o", "            )\r\n"]
[109.0999, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[109.122271, "o", "            new_code = lars.coef_\r\n"]
[109.144643, "o", "        finally:\r\n"]
[109.167014, "o", "            np.seterr(**err_mgt)\r\n"]
[109.189385, "o", "\r\n"]
[109.211756, "o", "    elif algorithm == \"threshold\":\r\n"]
[109.234127, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[109.256498, "o", "        if positive:\r\n"]
[109.278869, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[109.30124, "o", "\r\n"]
[109.323611, "o", "    elif algorithm == \"omp\":\r\n"]
[109.345982, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[109.368353, "o", "            Gram=gram,\r\n"]
[109.390724, "o", "            Xy=cov,\r\n"]
[109.413095, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[109.435466, "o", "            tol=None,\r\n"]
[109.457837, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[109.480208, "o", "            copy_Xy=copy_cov,\r\n"]
[109.502579, "o", "        ).T\r\n"]
[109.52495, "o", "\r\n"]
[109.547321, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[109.569692, "o", "\r\n"]
[109.592063, "o", "\r\n"]
[109.614434, "o", "@validate_params(\r\n"]
[109.636805, "o", "    {\r\n"]
[109.659176, "o", "        \"X\": [\"array-like\"],\r\n"]
[109.681548, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[109.703919, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[109.72629, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[109.748661, "o", "        \"algorithm\": [\r\n"]
[109.771032, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[109.793403, "o", "        ],\r\n"]
[109.815774, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[109.838145, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[109.860516, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[109.882887, "o", "        \"init\": [\"array-like\", None],\r\n"]
[109.905258, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[109.927629, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[110.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[110.002, "i", "sed -n '1,200p' asv_benchmarks/asv.conf.json\r"]
[110.004, "o", "sed -n '1,200p' asv_benchmarks/asv.conf.json\r\n"]
[110.036146, "o", "\u001b[?2004l\r\n"]
[110.066293, "o", "{\r\n"]
[110.096439, "o", "    // The version of the config file format.  Do not change, unless\r\n"]
[110.126585, "o", "    // you know what you are doing.\r\n"]
[110.156732, "o", "    \"version\": 1,\r\n"]
[110.186878, "o", "\r\n"]
[110.217024, "o", "    // The name of the project being benchmarked\r\n"]
[110.247171, "o", "    \"project\": \"scikit-learn\",\r\n"]
[110.277317, "o", "\r\n"]
[110.307463, "o", "    // The project's homepage\r\n"]
[110.33761, "o", "    \"project_url\": \"scikit-learn.org/\",\r\n"]
[110.367756, "o", "\r\n"]
[110.397902, "o", "    // The URL or local path of the source code repository for the\r\n"]
[110.428049, "o", "    // project being benchmarked\r\n"]
[110.458195, "o", "    \"repo\": \"..\",\r\n"]
[110.488341, "o", "\r\n"]
[110.518488, "o", "    // The Python project's subdirectory in your repo.  If missing or\r\n"]
[110.548634, "o", "    // the empty string, the project is assumed to be located at the root\r\n"]
[110.57878, "o", "    // of the repository.\r\n"]
[110.608927, "o", "    // \"repo_subdir\": \"\",\r\n"]
[110.639073, "o", "\r\n"]
[110.66922, "o", "    // Customizable commands for building, installing, and\r\n"]
[110.699366, "o", "    // uninstalling the project. See asv.conf.json documentation.\r\n"]
[110.729512, "o", "    //\r\n"]
[110.759659, "o", "    // \"install_command\": [\"python -mpip install {wheel_file}\"],\r\n"]
[110.789805, "o", "    // \"uninstall_command\": [\"return-code=any python -mpip uninstall -y {project}\"],\r\n"]
[110.819951, "o", "    // \"build_command\": [\r\n"]
[110.850098, "o", "    //     \"python setup.py build\",\r\n"]
[110.880244, "o", "    //     \"PIP_NO_BUILD_ISOLATION=false python -mpip wheel --no-deps --no-index -w {build_cache_dir} {build_dir}\"\r\n"]
[110.91039, "o", "    // ],\r\n"]
[110.940537, "o", "\r\n"]
[110.970683, "o", "    // List of branches to benchmark. If not provided, defaults to \"master\r\n"]
[111.000829, "o", "    // (for git) or \"default\" (for mercurial).\r\n"]
[111.030976, "o", "    \"branches\": [\"main\"],\r\n"]
[111.061122, "o", "    // \"branches\": [\"default\"],    // for mercurial\r\n"]
[111.091268, "o", "\r\n"]
[111.121415, "o", "    // The DVCS being used.  If not set, it will be automatically\r\n"]
[111.151561, "o", "    // determined from \"repo\" by looking at the protocol in the URL\r\n"]
[111.181707, "o", "    // (if remote), or by looking for special directories, such as\r\n"]
[111.211854, "o", "    // \".git\" (if local).\r\n"]
[111.242, "o", "    // \"dvcs\": \"git\",\r\n"]
[111.272146, "o", "\r\n"]
[111.302293, "o", "    // The tool to use to create environments.  May be \"conda\",\r\n"]
[111.332439, "o", "    // \"virtualenv\" or other value depending on the plugins in use.\r\n"]
[111.362585, "o", "    // If missing or the empty string, the tool will be automatically\r\n"]
[111.392732, "o", "    // determined by looking for tools on the PATH environment\r\n"]
[111.422878, "o", "    // variable.\r\n"]
[111.453024, "o", "    \"environment_type\": \"conda\",\r\n"]
[111.483171, "o", "\r\n"]
[111.513317, "o", "    // timeout in seconds for installing any dependencies in environment\r\n"]
[111.543463, "o", "    // defaults to 10 min\r\n"]
[111.57361, "o", "    //\"install_timeout\": 600,\r\n"]
[111.603756, "o", "\r\n"]
[111.633902, "o", "    // the base URL to show a commit for the project.\r\n"]
[111.664049, "o", "    \"show_commit_url\": \"https://github.com/scikit-learn/scikit-learn/commit/\",\r\n"]
[111.694195, "o", "\r\n"]
[111.724341, "o", "    // The Pythons you'd like to test against. If not provided, defaults\r\n"]
[111.754488, "o", "    // to the current version of Python used to run `asv`.\r\n"]
[111.784634, "o", "    // \"pythons\": [\"3.6\"],\r\n"]
[111.81478, "o", "\r\n"]
[111.844927, "o", "    // The list of conda channel names to be searched for benchmark\r\n"]
[111.875073, "o", "    // dependency packages in the specified order\r\n"]
[111.90522, "o", "    // \"conda_channels\": [\"conda-forge\", \"defaults\"]\r\n"]
[111.935366, "o", "\r\n"]
[111.965512, "o", "    // The matrix of dependencies to test. Each key is the name of a\r\n"]
[111.995659, "o", "    // package (in PyPI) and the values are version numbers. An empty\r\n"]
[112.025805, "o", "    // list or empty string indicates to just test against the default\r\n"]
[112.055951, "o", "    // (latest) version. null indicates that the package is to not be\r\n"]
[112.086098, "o", "    // installed. If the package to be tested is only available from\r\n"]
[112.116244, "o", "    // PyPi, and the 'environment_type' is conda, then you can preface\r\n"]
[112.14639, "o", "    // the package name by 'pip+', and the package will be installed via\r\n"]
[112.176537, "o", "    // pip (with all the conda available packages installed first,\r\n"]
[112.206683, "o", "    // followed by the pip installed packages).\r\n"]
[112.236829, "o", "    //\r\n"]
[112.266976, "o", "    \"matrix\": {\r\n"]
[112.297122, "o", "        \"numpy\": [],\r\n"]
[112.327268, "o", "        \"scipy\": [],\r\n"]
[112.357415, "o", "        \"cython\": [],\r\n"]
[112.387561, "o", "        \"joblib\": [],\r\n"]
[112.417707, "o", "        \"threadpoolctl\": [],\r\n"]
[112.447854, "o", "        \"pandas\": []\r\n"]
[112.478, "o", "    },\r\n"]
[112.508146, "o", "\r\n"]
[112.538293, "o", "    // Combinations of libraries/python versions can be excluded/included\r\n"]
[112.568439, "o", "    // from the set to test. Each entry is a dictionary containing additional\r\n"]
[112.598585, "o", "    // key-value pairs to include/exclude.\r\n"]
[112.628732, "o", "    //\r\n"]
[112.658878, "o", "    // An exclude entry excludes entries where all values match. The\r\n"]
[112.689024, "o", "    // values are regexps that should match the whole string.\r\n"]
[112.719171, "o", "    //\r\n"]
[112.749317, "o", "    // An include entry adds an environment. Only the packages listed\r\n"]
[112.779463, "o", "    // are installed. The 'python' key is required. The exclude rules\r\n"]
[112.80961, "o", "    // do not apply to includes.\r\n"]
[112.839756, "o", "    //\r\n"]
[112.869902, "o", "    // In addition to package names, the following keys are available:\r\n"]
[112.900049, "o", "    //\r\n"]
[112.930195, "o", "    // - python\r\n"]
[112.960341, "o", "    //     Python version, as in the *pythons* variable above.\r\n"]
[112.990488, "o", "    // - environment_type\r\n"]
[113.020634, "o", "    //     Environment type, as above.\r\n"]
[113.05078, "o", "    // - sys_platform\r\n"]
[113.080927, "o", "    //     Platform, as in sys.platform. Possible values for the common\r\n"]
[113.111073, "o", "    //     cases: 'linux2', 'win32', 'cygwin', 'darwin'.\r\n"]
[113.14122, "o", "    //\r\n"]
[113.171366, "o", "    // \"exclude\": [\r\n"]
[113.201512, "o", "    //     {\"python\": \"3.2\", \"sys_platform\": \"win32\"}, // skip py3.2 on windows\r\n"]
[113.231659, "o", "    //     {\"environment_type\": \"conda\", \"six\": null}, // don't run without six on conda\r\n"]
[113.261805, "o", "    // ],\r\n"]
[113.291951, "o", "    //\r\n"]
[113.322098, "o", "    // \"include\": [\r\n"]
[113.352244, "o", "    //     // additional env for python2.7\r\n"]
[113.38239, "o", "    //     {\"python\": \"2.7\", \"numpy\": \"1.8\"},\r\n"]
[113.412537, "o", "    //     // additional env if run on windows+conda\r\n"]
[113.442683, "o", "    //     {\"platform\": \"win32\", \"environment_type\": \"conda\", \"python\": \"2.7\", \"libpython\": \"\"},\r\n"]
[113.472829, "o", "    // ],\r\n"]
[113.502976, "o", "\r\n"]
[113.533122, "o", "    // The directory (relative to the current directory) that benchmarks are\r\n"]
[113.563268, "o", "    // stored in.  If not provided, defaults to \"benchmarks\"\r\n"]
[113.593415, "o", "    // \"benchmark_dir\": \"benchmarks\",\r\n"]
[113.623561, "o", "\r\n"]
[113.653707, "o", "    // The directory (relative to the current directory) to cache the Python\r\n"]
[113.683854, "o", "    // environments in.  If not provided, defaults to \"env\"\r\n"]
[113.714, "o", "    // \"env_dir\": \"env\",\r\n"]
[113.744146, "o", "\r\n"]
[113.774293, "o", "    // The directory (relative to the current directory) that raw benchmark\r\n"]
[113.804439, "o", "    // results are stored in.  If not provided, defaults to \"results\".\r\n"]
[113.834585, "o", "    // \"results_dir\": \"results\",\r\n"]
[113.864732, "o", "\r\n"]
[113.894878, "o", "    // The directory (relative to the current directory) that the html tree\r\n"]
[113.925024, "o", "    // should be written to.  If not provided, defaults to \"html\".\r\n"]
[113.955171, "o", "    // \"html_dir\": \"html\",\r\n"]
[113.985317, "o", "\r\n"]
[114.015463, "o", "    // The number of characters to retain in the commit hashes.\r\n"]
[114.04561, "o", "    // \"hash_length\": 8,\r\n"]
[114.075756, "o", "\r\n"]
[114.105902, "o", "    // `asv` will cache results of the recent builds in each\r\n"]
[114.136049, "o", "    // environment, making them faster to install next time.  This is\r\n"]
[114.166195, "o", "    // the number of builds to keep, per environment.\r\n"]
[114.196341, "o", "    // \"build_cache_size\": 2,\r\n"]
[114.226488, "o", "\r\n"]
[114.256634, "o", "    // The commits after which the regression search in `asv publish`\r\n"]
[114.28678, "o", "    // should start looking for regressions. Dictionary whose keys are\r\n"]
[114.316927, "o", "    // regexps matching to benchmark names, and values corresponding to\r\n"]
[114.347073, "o", "    // the commit (exclusive) after which to start looking for\r\n"]
[114.37722, "o", "    // regressions.  The default is to start from the first commit\r\n"]
[114.407366, "o", "    // with results. If the commit is `null`, regression detection is\r\n"]
[114.437512, "o", "    // skipped for the matching benchmark.\r\n"]
[114.467659, "o", "    //\r\n"]
[114.497805, "o", "    // \"regressions_first_commits\": {\r\n"]
[114.527951, "o", "    //    \"some_benchmark\": \"352cdf\",  // Consider regressions only after this commit\r\n"]
[114.558098, "o", "    //    \"another_benchmark\": null,   // Skip regression detection altogether\r\n"]
[114.588244, "o", "    // },\r\n"]
[114.61839, "o", "\r\n"]
[114.648537, "o", "    // The thresholds for relative change in results, after which `asv\r\n"]
[114.678683, "o", "    // publish` starts reporting regressions. Dictionary of the same\r\n"]
[114.708829, "o", "    // form as in ``regressions_first_commits``, with values\r\n"]
[114.738976, "o", "    // indicating the thresholds.  If multiple entries match, the\r\n"]
[114.769122, "o", "    // maximum is taken. If no entry matches, the default is 5%.\r\n"]
[114.799268, "o", "    //\r\n"]
[114.829415, "o", "    // \"regressions_thresholds\": {\r\n"]
[114.859561, "o", "    //    \"some_benchmark\": 0.01,     // Threshold of 1%\r\n"]
[114.889707, "o", "    //    \"another_benchmark\": 0.5,   // Threshold of 50%\r\n"]
[114.919854, "o", "    // },\r\n"]
[115.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[115.002, "i", "sed -n '1,200p' asv_benchmarks/benchmarks/decomposition.py\r"]
[115.004, "o", "sed -n '1,200p' asv_benchmarks/benchmarks/decomposition.py\r\n"]
[115.056969, "o", "\u001b[?2004l\r\n"]
[115.107938, "o", "from sklearn.decomposition import PCA, DictionaryLearning, MiniBatchDictionaryLearning\r\n"]
[115.158907, "o", "\r\n"]
[115.209876, "o", "from .common import Benchmark, Estimator, Transformer\r\n"]
[115.260845, "o", "from .datasets import _mnist_dataset, _olivetti_faces_dataset\r\n"]
[115.311814, "o", "from .utils import make_dict_learning_scorers, make_pca_scorers\r\n"]
[115.362784, "o", "\r\n"]
[115.413753, "o", "\r\n"]
[115.464722, "o", "class PCABenchmark(Transformer, Estimator, Benchmark):\r\n"]
[115.515691, "o", "    \"\"\"\r\n"]
[115.56666, "o", "    Benchmarks for PCA.\r\n"]
[115.617629, "o", "    \"\"\"\r\n"]
[115.668598, "o", "\r\n"]
[115.719567, "o", "    param_names = [\"svd_solver\"]\r\n"]
[115.770536, "o", "    params = ([\"full\", \"arpack\", \"randomized\"],)\r\n"]
[115.821505, "o", "\r\n"]
[115.872474, "o", "    def setup_cache(self):\r\n"]
[115.923443, "o", "        super().setup_cache()\r\n"]
[115.974412, "o", "\r\n"]
[116.025381, "o", "    def make_data(self, params):\r\n"]
[116.076351, "o", "        return _mnist_dataset()\r\n"]
[116.12732, "o", "\r\n"]
[116.178289, "o", "    def make_estimator(self, params):\r\n"]
[116.229258, "o", "        (svd_solver,) = params\r\n"]
[116.280227, "o", "\r\n"]
[116.331196, "o", "        estimator = PCA(n_components=32, svd_solver=svd_solver, random_state=0)\r\n"]
[116.382165, "o", "\r\n"]
[116.433134, "o", "        return estimator\r\n"]
[116.484103, "o", "\r\n"]
[116.535072, "o", "    def make_scorers(self):\r\n"]
[116.586041, "o", "        make_pca_scorers(self)\r\n"]
[116.63701, "o", "\r\n"]
[116.687979, "o", "\r\n"]
[116.738948, "o", "class DictionaryLearningBenchmark(Transformer, Estimator, Benchmark):\r\n"]
[116.789918, "o", "    \"\"\"\r\n"]
[116.840887, "o", "    Benchmarks for DictionaryLearning.\r\n"]
[116.891856, "o", "    \"\"\"\r\n"]
[116.942825, "o", "\r\n"]
[116.993794, "o", "    param_names = [\"fit_algorithm\", \"n_jobs\"]\r\n"]
[117.044763, "o", "    params = ([\"lars\", \"cd\"], Benchmark.n_jobs_vals)\r\n"]
[117.095732, "o", "\r\n"]
[117.146701, "o", "    def setup_cache(self):\r\n"]
[117.19767, "o", "        super().setup_cache()\r\n"]
[117.248639, "o", "\r\n"]
[117.299608, "o", "    def make_data(self, params):\r\n"]
[117.350577, "o", "        return _olivetti_faces_dataset()\r\n"]
[117.401546, "o", "\r\n"]
[117.452515, "o", "    def make_estimator(self, params):\r\n"]
[117.503485, "o", "        fit_algorithm, n_jobs = params\r\n"]
[117.554454, "o", "\r\n"]
[117.605423, "o", "        estimator = DictionaryLearning(\r\n"]
[117.656392, "o", "            n_components=15,\r\n"]
[117.707361, "o", "            fit_algorithm=fit_algorithm,\r\n"]
[117.75833, "o", "            alpha=0.1,\r\n"]
[117.809299, "o", "            transform_alpha=1,\r\n"]
[117.860268, "o", "            max_iter=20,\r\n"]
[117.911237, "o", "            tol=1e-16,\r\n"]
[117.962206, "o", "            random_state=0,\r\n"]
[118.013175, "o", "            n_jobs=n_jobs,\r\n"]
[118.064144, "o", "        )\r\n"]
[118.115113, "o", "\r\n"]
[118.166082, "o", "        return estimator\r\n"]
[118.217052, "o", "\r\n"]
[118.268021, "o", "    def make_scorers(self):\r\n"]
[118.31899, "o", "        make_dict_learning_scorers(self)\r\n"]
[118.369959, "o", "\r\n"]
[118.420928, "o", "\r\n"]
[118.471897, "o", "class MiniBatchDictionaryLearningBenchmark(Transformer, Estimator, Benchmark):\r\n"]
[118.522866, "o", "    \"\"\"\r\n"]
[118.573835, "o", "    Benchmarks for MiniBatchDictionaryLearning\r\n"]
[118.624804, "o", "    \"\"\"\r\n"]
[118.675773, "o", "\r\n"]
[118.726742, "o", "    param_names = [\"fit_algorithm\", \"n_jobs\"]\r\n"]
[118.777711, "o", "    params = ([\"lars\", \"cd\"], Benchmark.n_jobs_vals)\r\n"]
[118.82868, "o", "\r\n"]
[118.879649, "o", "    def setup_cache(self):\r\n"]
[118.930619, "o", "        super().setup_cache()\r\n"]
[118.981588, "o", "\r\n"]
[119.032557, "o", "    def make_data(self, params):\r\n"]
[119.083526, "o", "        return _olivetti_faces_dataset()\r\n"]
[119.134495, "o", "\r\n"]
[119.185464, "o", "    def make_estimator(self, params):\r\n"]
[119.236433, "o", "        fit_algorithm, n_jobs = params\r\n"]
[119.287402, "o", "\r\n"]
[119.338371, "o", "        estimator = MiniBatchDictionaryLearning(\r\n"]
[119.38934, "o", "            n_components=15,\r\n"]
[119.440309, "o", "            fit_algorithm=fit_algorithm,\r\n"]
[119.491278, "o", "            alpha=0.1,\r\n"]
[119.542247, "o", "            batch_size=3,\r\n"]
[119.593216, "o", "            random_state=0,\r\n"]
[119.644186, "o", "            n_jobs=n_jobs,\r\n"]
[119.695155, "o", "        )\r\n"]
[119.746124, "o", "\r\n"]
[119.797093, "o", "        return estimator\r\n"]
[119.848062, "o", "\r\n"]
[119.899031, "o", "    def make_scorers(self):\r\n"]
[120.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[120.002, "i", "pwd\r"]
[120.004, "o", "pwd\r\n"]
[122.478, "o", "\u001b[?2004l\r\n"]
[125.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[125.002, "i", "ls -l asv_benchmarks/asv.conf.json\r"]
[125.004, "o", "ls -l asv_benchmarks/asv.conf.json\r\n"]
[127.478, "o", "\u001b[?2004l\r\n"]
[130.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[130.002, "i", "cd asv_benchmarks\r"]
[130.004, "o", "cd asv_benchmarks\r\n"]
[130.006, "o", "\u001b[?2004l\r\n"]
[135.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[135.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\" --show-stderr\r"]
[135.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\" --show-stderr\r\n"]
[135.022874, "o", "\u001b[?2004l\r\n"]
[135.039747, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[135.056621, "o", "\u00b7 \u001b[0;32mRunning 6 total benchmarks (1 commits * 1 environments * 6 benchmarks)\u001b[0m\r\n"]
[135.073495, "o", "[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n"]
[135.090369, "o", "[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n"]
[135.107242, "o", "[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.peakmem_fit\u001b[0m                                                                                  ok\r\n"]
[135.124116, "o", "[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =======\u001b[0m\r\n"]
[135.14099, "o", "             --               n_jobs\r\n"]
[135.157863, "o", "             --------------- -------\r\n"]
[135.174737, "o", "              fit_algorithm     1   \r\n"]
[135.191611, "o", "             =============== =======\r\n"]
[135.208485, "o", "                   lars        94M  \r\n"]
[135.225358, "o", "                    cd        93.4M \r\n"]
[135.242232, "o", "             =============== =======\r\n"]
[135.259106, "o", "\r\n"]
[135.27598, "o", "[33.33%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.peakmem_transform\u001b[0m                                                                            ok\r\n"]
[135.292853, "o", "[33.33%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =======\u001b[0m\r\n"]
[135.309727, "o", "             --               n_jobs\r\n"]
[135.326601, "o", "             --------------- -------\r\n"]
[135.343474, "o", "              fit_algorithm     1   \r\n"]
[135.360348, "o", "             =============== =======\r\n"]
[135.377222, "o", "                   lars       84.5M \r\n"]
[135.394096, "o", "                    cd        84.3M \r\n"]
[135.410969, "o", "             =============== =======\r\n"]
[135.427843, "o", "\r\n"]
[135.444717, "o", "[33.33%] \u00b7\u00b7\u00b7\u00b7 \u001b[0;31mFor parameters: 'lars', 1\u001b[0m\r\n"]
[135.46159, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[135.478464, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[135.495338, "o", "\r\n"]
[135.512212, "o", "[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0masv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r\n"]
[135.529085, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[135.545959, "o", "cd ..\r\n"]
[135.562833, "o", "sed -n '1800,2100p' sklearn/decomposition/_dict_learning.py\r\n"]
[135.579706, "o", "                                                                                     ok\r\n"]
[135.59658, "o", "[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ============\u001b[0m\r\n"]
[135.613454, "o", "             --                 n_jobs   \r\n"]
[135.630328, "o", "             --------------- ------------\r\n"]
[135.647201, "o", "              fit_algorithm       1      \r\n"]
[135.664075, "o", "             =============== ============\r\n"]
[135.680949, "o", "                   lars       6.73\u00b10.04s \r\n"]
[135.697823, "o", "                    cd        1.64\u00b10.03s \r\n"]
[135.714696, "o", "             =============== ============\r\n"]
[135.73157, "o", "\r\n"]
[135.748444, "o", "[66.67%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_transform\u001b[0m                                                                               ok\r\n"]
[135.765317, "o", "[66.67%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =========\u001b[0m\r\n"]
[135.782191, "o", "             --                n_jobs \r\n"]
[135.799065, "o", "             --------------- ---------\r\n"]
[135.815939, "o", "              fit_algorithm      1    \r\n"]
[135.832812, "o", "             =============== =========\r\n"]
[135.849686, "o", "                   lars       161\u00b11ms \r\n"]
[135.86656, "o", "                    cd        166\u00b12ms \r\n"]
[135.883433, "o", "             =============== =========\r\n"]
[135.900307, "o", "\r\n"]
[135.917181, "o", "[66.67%] \u00b7\u00b7\u00b7\u00b7 \u001b[0;31mFor parameters: 'lars', 1\u001b[0m\r\n"]
[135.934055, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[135.950928, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[135.967802, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[135.984676, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[136.001549, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[136.018423, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[136.035297, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[136.052171, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[136.069044, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[136.085918, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[136.102792, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[136.119666, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[136.136539, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[136.153413, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[136.170287, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[136.18716, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[136.204034, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[136.220908, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[136.237782, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[136.254655, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[136.271529, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[136.288403, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[136.305276, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[136.32215, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[136.339024, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[136.355898, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[136.372771, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[136.389645, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[136.406519, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[136.423392, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[136.440266, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[136.45714, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[136.474014, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[136.490887, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[136.507761, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[136.524635, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[136.541509, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[136.558382, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[136.575256, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[136.59213, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[136.609003, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[136.625877, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[136.642751, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[136.659625, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[136.676498, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[136.693372, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[136.710246, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[136.727119, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[136.743993, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[136.760867, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[136.777741, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[136.794614, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[136.811488, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[136.828362, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[136.845235, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[136.862109, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[136.878983, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[136.895857, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[136.91273, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[136.929604, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[136.946478, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[136.963352, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[136.980225, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[136.997099, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[137.013973, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[137.030846, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[137.04772, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[137.064594, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[137.081468, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[137.098341, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[137.115215, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[137.132089, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[137.148962, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[137.165836, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[137.18271, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[137.199584, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[137.216457, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[137.233331, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[137.250205, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[137.267078, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[137.283952, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[137.300826, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[137.3177, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[137.334573, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[137.351447, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[137.368321, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[137.385195, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[137.402068, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[137.418942, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[137.435816, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[137.452689, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[137.469563, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[137.486437, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[137.503311, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[137.520184, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[137.537058, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[137.553932, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[137.570805, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[137.587679, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[137.604553, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[137.621427, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[137.6383, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[137.655174, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[137.672048, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[137.688922, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[137.705795, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[137.722669, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[137.739543, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[137.756416, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[137.77329, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[137.790164, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[137.807038, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[137.823911, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[137.840785, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[137.857659, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[137.874532, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[137.891406, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[137.90828, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[137.925154, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[137.942027, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[137.958901, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[137.975775, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[137.992648, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[138.009522, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[138.026396, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[138.04327, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[138.060143, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[138.077017, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[138.093891, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[138.110765, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[138.127638, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[138.144512, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[138.161386, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[138.178259, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[138.195133, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[138.212007, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[138.228881, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[138.245754, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[138.262628, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[138.279502, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[138.296375, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[138.313249, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[138.330123, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[138.346997, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[138.36387, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[138.380744, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[138.397618, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[138.414491, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[138.431365, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[138.448239, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[138.465113, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[138.481986, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[138.49886, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[138.515734, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[138.532608, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[138.549481, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[138.566355, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[138.583229, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[138.600102, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[138.616976, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[138.63385, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[138.650724, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[138.667597, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[138.684471, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[138.701345, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[138.718218, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[138.735092, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[138.751966, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[138.76884, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[138.785713, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[138.802587, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[138.819461, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[138.836334, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[138.853208, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[138.870082, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[138.886956, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[138.903829, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[138.920703, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[138.937577, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[138.954451, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[138.971324, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[138.988198, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[139.005072, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[139.021945, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[139.038819, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[139.055693, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[139.072567, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[139.08944, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[139.106314, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[139.123188, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[139.140061, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[139.156935, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[139.173809, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[139.190683, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[139.207556, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[139.22443, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[139.241304, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[139.258177, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[139.275051, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[139.291925, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[139.308799, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[139.325672, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[139.342546, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[139.35942, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[139.376294, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[139.393167, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[139.410041, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[139.426915, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[139.443788, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[139.460662, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[139.477536, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[139.49441, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[139.511283, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[139.528157, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[139.545031, "o", "\r\n"]
[139.561904, "o", "[83.33%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.track_test_score\u001b[0m                                                                             ok\r\n"]
[139.578778, "o", "[83.33%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ======================\u001b[0m\r\n"]
[139.595652, "o", "             --                      n_jobs        \r\n"]
[139.612526, "o", "             --------------- ----------------------\r\n"]
[139.629399, "o", "              fit_algorithm            1           \r\n"]
[139.646273, "o", "             =============== ======================\r\n"]
[139.663147, "o", "                   lars       -0.07507291436195374 \r\n"]
[139.68002, "o", "                    cd        -0.07509346306324005 \r\n"]
[139.696894, "o", "             =============== ======================\r\n"]
[139.713768, "o", "\r\n"]
[139.730642, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.track_train_score\u001b[0m                                                                            ok\r\n"]
[139.747515, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ======================\u001b[0m\r\n"]
[139.764389, "o", "              --                      n_jobs        \r\n"]
[139.781263, "o", "              --------------- ----------------------\r\n"]
[139.798137, "o", "               fit_algorithm            1           \r\n"]
[139.81501, "o", "              =============== ======================\r\n"]
[139.831884, "o", "                    lars       -0.07244297116994858 \r\n"]
[139.848758, "o", "                     cd        -0.07244537025690079 \r\n"]
[139.865631, "o", "              =============== ======================\r\n"]
[139.882505, "o", "\r\n"]
[139.899379, "o", "[100.00%] \u00b7\u00b7\u00b7\u00b7 \u001b[0;31mFor parameters: 'lars', 1\u001b[0m\r\n"]
[139.916253, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[139.933126, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[140.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[140.002, "i", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r"]
[140.004, "o", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r\n"]
[141.654, "o", "\u001b[?2004l\r\n"]
[143.302, "o", "Commits with results:\r\n"]
[145.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[145.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[145.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[146.654, "o", "\u001b[?2004l\r\n"]
[148.302, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[150.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[150.002, "i", "cd ..\r"]
[150.004, "o", "cd ..\r\n"]
[150.006, "o", "\u001b[?2004l\r\n"]
[155.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[155.002, "i", "sed -n '1800,2100p' sklearn/decomposition/_dict_learning.py\r"]
[155.004, "o", "sed -n '1800,2100p' sklearn/decomposition/_dict_learning.py\r\n"]
[155.022371, "o", "\u001b[?2004l\r\n"]
[155.038742, "o", "        self.fit_transform(X)\r\n"]
[155.055113, "o", "        return self\r\n"]
[155.071483, "o", "\r\n"]
[155.087854, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[155.104225, "o", "    def fit_transform(self, X, y=None):\r\n"]
[155.120596, "o", "        \"\"\"Fit the model from data in X and return the transformed data.\r\n"]
[155.136967, "o", "\r\n"]
[155.153338, "o", "        Parameters\r\n"]
[155.169709, "o", "        ----------\r\n"]
[155.186079, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[155.20245, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[155.218821, "o", "            and `n_features` is the number of features.\r\n"]
[155.235192, "o", "\r\n"]
[155.251563, "o", "        y : Ignored\r\n"]
[155.267934, "o", "            Not used, present for API consistency by convention.\r\n"]
[155.284305, "o", "\r\n"]
[155.300675, "o", "        Returns\r\n"]
[155.317046, "o", "        -------\r\n"]
[155.333417, "o", "        V : ndarray of shape (n_samples, n_components)\r\n"]
[155.349788, "o", "            Transformed data.\r\n"]
[155.366159, "o", "        \"\"\"\r\n"]
[155.38253, "o", "        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n"]
[155.398901, "o", "\r\n"]
[155.415272, "o", "        method = \"lasso_\" + self.fit_algorithm\r\n"]
[155.431642, "o", "\r\n"]
[155.448013, "o", "        random_state = check_random_state(self.random_state)\r\n"]
[155.464384, "o", "        X = self._validate_data(X)\r\n"]
[155.480755, "o", "\r\n"]
[155.497126, "o", "        if self.n_components is None:\r\n"]
[155.513497, "o", "            n_components = X.shape[1]\r\n"]
[155.529868, "o", "        else:\r\n"]
[155.546238, "o", "            n_components = self.n_components\r\n"]
[155.562609, "o", "\r\n"]
[155.57898, "o", "        V, U, E, self.n_iter_ = _dict_learning(\r\n"]
[155.595351, "o", "            X,\r\n"]
[155.611722, "o", "            n_components,\r\n"]
[155.628093, "o", "            alpha=self.alpha,\r\n"]
[155.644464, "o", "            tol=self.tol,\r\n"]
[155.660834, "o", "            max_iter=self.max_iter,\r\n"]
[155.677205, "o", "            method=method,\r\n"]
[155.693576, "o", "            method_max_iter=self.transform_max_iter,\r\n"]
[155.709947, "o", "            n_jobs=self.n_jobs,\r\n"]
[155.726318, "o", "            code_init=self.code_init,\r\n"]
[155.742689, "o", "            dict_init=self.dict_init,\r\n"]
[155.75906, "o", "            callback=self.callback,\r\n"]
[155.77543, "o", "            verbose=self.verbose,\r\n"]
[155.791801, "o", "            random_state=random_state,\r\n"]
[155.808172, "o", "            return_n_iter=True,\r\n"]
[155.824543, "o", "            positive_dict=self.positive_dict,\r\n"]
[155.840914, "o", "            positive_code=self.positive_code,\r\n"]
[155.857285, "o", "        )\r\n"]
[155.873656, "o", "        self.components_ = U\r\n"]
[155.890026, "o", "        self.error_ = E\r\n"]
[155.906397, "o", "\r\n"]
[155.922768, "o", "        return V\r\n"]
[155.939139, "o", "\r\n"]
[155.95551, "o", "    @property\r\n"]
[155.971881, "o", "    def _n_features_out(self):\r\n"]
[155.988252, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[156.004623, "o", "        return self.components_.shape[0]\r\n"]
[156.020993, "o", "\r\n"]
[156.037364, "o", "    def _more_tags(self):\r\n"]
[156.053735, "o", "        return {\r\n"]
[156.070106, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[156.086477, "o", "        }\r\n"]
[156.102848, "o", "\r\n"]
[156.119219, "o", "\r\n"]
[156.135589, "o", "class MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[156.15196, "o", "    \"\"\"Mini-batch dictionary learning.\r\n"]
[156.168331, "o", "\r\n"]
[156.184702, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[156.201073, "o", "    encoding the fitted data.\r\n"]
[156.217444, "o", "\r\n"]
[156.233815, "o", "    Solves the optimization problem::\r\n"]
[156.250185, "o", "\r\n"]
[156.266556, "o", "       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[156.282927, "o", "                    (U,V)\r\n"]
[156.299298, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[156.315669, "o", "\r\n"]
[156.33204, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[156.348411, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[156.364781, "o", "    of all the entries in the matrix.\r\n"]
[156.381152, "o", "\r\n"]
[156.397523, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[156.413894, "o", "\r\n"]
[156.430265, "o", "    Parameters\r\n"]
[156.446636, "o", "    ----------\r\n"]
[156.463007, "o", "    n_components : int, default=None\r\n"]
[156.479377, "o", "        Number of dictionary elements to extract.\r\n"]
[156.495748, "o", "\r\n"]
[156.512119, "o", "    alpha : float, default=1\r\n"]
[156.52849, "o", "        Sparsity controlling parameter.\r\n"]
[156.544861, "o", "\r\n"]
[156.561232, "o", "    n_iter : int, default=1000\r\n"]
[156.577603, "o", "        Total number of iterations over data batches to perform.\r\n"]
[156.593974, "o", "\r\n"]
[156.610344, "o", "        .. deprecated:: 1.1\r\n"]
[156.626715, "o", "           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[156.643086, "o", "           ``max_iter`` instead.\r\n"]
[156.659457, "o", "\r\n"]
[156.675828, "o", "    max_iter : int, default=None\r\n"]
[156.692199, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[156.70857, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[156.72494, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[156.741311, "o", "\r\n"]
[156.757682, "o", "        .. versionadded:: 1.1\r\n"]
[156.774053, "o", "\r\n"]
[156.790424, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[156.806795, "o", "        The algorithm used:\r\n"]
[156.823166, "o", "\r\n"]
[156.839536, "o", "        - `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[156.855907, "o", "          problem (`linear_model.lars_path`)\r\n"]
[156.872278, "o", "        - `'cd'`: uses the coordinate descent method to compute the\r\n"]
[156.888649, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[156.90502, "o", "          the estimated components are sparse.\r\n"]
[156.921391, "o", "\r\n"]
[156.937762, "o", "    n_jobs : int, default=None\r\n"]
[156.954132, "o", "        Number of parallel jobs to run.\r\n"]
[156.970503, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[156.986874, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[157.003245, "o", "        for more details.\r\n"]
[157.019616, "o", "\r\n"]
[157.035987, "o", "    batch_size : int, default=256\r\n"]
[157.052358, "o", "        Number of samples in each mini-batch.\r\n"]
[157.068728, "o", "\r\n"]
[157.085099, "o", "        .. versionchanged:: 1.3\r\n"]
[157.10147, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[157.117841, "o", "\r\n"]
[157.134212, "o", "    shuffle : bool, default=True\r\n"]
[157.150583, "o", "        Whether to shuffle the samples before forming batches.\r\n"]
[157.166954, "o", "\r\n"]
[157.183325, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[157.199695, "o", "        Initial value of the dictionary for warm restart scenarios.\r\n"]
[157.216066, "o", "\r\n"]
[157.232437, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[157.248808, "o", "            'threshold'}, default='omp'\r\n"]
[157.265179, "o", "        Algorithm used to transform the data:\r\n"]
[157.28155, "o", "\r\n"]
[157.297921, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[157.314291, "o", "          (`linear_model.lars_path`);\r\n"]
[157.330662, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[157.347033, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[157.363404, "o", "          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n"]
[157.379775, "o", "          if the estimated components are sparse.\r\n"]
[157.396146, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[157.412517, "o", "          solution.\r\n"]
[157.428887, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[157.445258, "o", "          the projection ``dictionary * X'``.\r\n"]
[157.461629, "o", "\r\n"]
[157.478, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[157.494371, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[157.510742, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[157.527113, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[157.543483, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[157.559854, "o", "\r\n"]
[157.576225, "o", "    transform_alpha : float, default=None\r\n"]
[157.592596, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[157.608967, "o", "        penalty applied to the L1 norm.\r\n"]
[157.625338, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[157.641709, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[157.658079, "o", "        If `None`, defaults to `alpha`.\r\n"]
[157.67445, "o", "\r\n"]
[157.690821, "o", "        .. versionchanged:: 1.2\r\n"]
[157.707192, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[157.723563, "o", "\r\n"]
[157.739934, "o", "    verbose : bool or int, default=False\r\n"]
[157.756305, "o", "        To control the verbosity of the procedure.\r\n"]
[157.772675, "o", "\r\n"]
[157.789046, "o", "    split_sign : bool, default=False\r\n"]
[157.805417, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[157.821788, "o", "        its negative part and its positive part. This can improve the\r\n"]
[157.838159, "o", "        performance of downstream classifiers.\r\n"]
[157.85453, "o", "\r\n"]
[157.870901, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[157.887272, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[157.903642, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[157.920013, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[157.936384, "o", "        results across multiple function calls.\r\n"]
[157.952755, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[157.969126, "o", "\r\n"]
[157.985497, "o", "    positive_code : bool, default=False\r\n"]
[158.001868, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[158.018238, "o", "\r\n"]
[158.034609, "o", "        .. versionadded:: 0.20\r\n"]
[158.05098, "o", "\r\n"]
[158.067351, "o", "    positive_dict : bool, default=False\r\n"]
[158.083722, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[158.100093, "o", "\r\n"]
[158.116464, "o", "        .. versionadded:: 0.20\r\n"]
[158.132834, "o", "\r\n"]
[158.149205, "o", "    transform_max_iter : int, default=1000\r\n"]
[158.165576, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[158.181947, "o", "        `'lasso_lars'`.\r\n"]
[158.198318, "o", "\r\n"]
[158.214689, "o", "        .. versionadded:: 0.22\r\n"]
[158.23106, "o", "\r\n"]
[158.24743, "o", "    callback : callable, default=None\r\n"]
[158.263801, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[158.280172, "o", "\r\n"]
[158.296543, "o", "        .. versionadded:: 1.1\r\n"]
[158.312914, "o", "\r\n"]
[158.329285, "o", "    tol : float, default=1e-3\r\n"]
[158.345656, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[158.362026, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[158.378397, "o", "\r\n"]
[158.394768, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[158.411139, "o", "        `tol` to 0.0.\r\n"]
[158.42751, "o", "\r\n"]
[158.443881, "o", "        .. versionadded:: 1.1\r\n"]
[158.460252, "o", "\r\n"]
[158.476623, "o", "    max_no_improvement : int, default=10\r\n"]
[158.492993, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[158.509364, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[158.525735, "o", "        `max_iter` is not None.\r\n"]
[158.542106, "o", "\r\n"]
[158.558477, "o", "        To disable convergence detection based on cost function, set\r\n"]
[158.574848, "o", "        `max_no_improvement` to None.\r\n"]
[158.591219, "o", "\r\n"]
[158.607589, "o", "        .. versionadded:: 1.1\r\n"]
[158.62396, "o", "\r\n"]
[158.640331, "o", "    Attributes\r\n"]
[158.656702, "o", "    ----------\r\n"]
[158.673073, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[158.689444, "o", "        Components extracted from the data.\r\n"]
[158.705815, "o", "\r\n"]
[158.722185, "o", "    n_features_in_ : int\r\n"]
[158.738556, "o", "        Number of features seen during :term:`fit`.\r\n"]
[158.754927, "o", "\r\n"]
[158.771298, "o", "        .. versionadded:: 0.24\r\n"]
[158.787669, "o", "\r\n"]
[158.80404, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[158.820411, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[158.836781, "o", "        has feature names that are all strings.\r\n"]
[158.853152, "o", "\r\n"]
[158.869523, "o", "        .. versionadded:: 1.0\r\n"]
[158.885894, "o", "\r\n"]
[158.902265, "o", "    n_iter_ : int\r\n"]
[158.918636, "o", "        Number of iterations over the full dataset.\r\n"]
[158.935007, "o", "\r\n"]
[158.951377, "o", "    n_steps_ : int\r\n"]
[158.967748, "o", "        Number of mini-batches processed.\r\n"]
[158.984119, "o", "\r\n"]
[159.00049, "o", "        .. versionadded:: 1.1\r\n"]
[159.016861, "o", "\r\n"]
[159.033232, "o", "    See Also\r\n"]
[159.049603, "o", "    --------\r\n"]
[159.065974, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[159.082344, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[159.098715, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[159.115086, "o", "        precomputed dictionary.\r\n"]
[159.131457, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[159.147828, "o", "\r\n"]
[159.164199, "o", "    References\r\n"]
[159.18057, "o", "    ----------\r\n"]
[159.19694, "o", "\r\n"]
[159.213311, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[159.229682, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[159.246053, "o", "\r\n"]
[159.262424, "o", "    Examples\r\n"]
[159.278795, "o", "    --------\r\n"]
[159.295166, "o", "    >>> import numpy as np\r\n"]
[159.311536, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[159.327907, "o", "    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n"]
[159.344278, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[159.360649, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[159.37702, "o", "    ...     random_state=42)\r\n"]
[159.393391, "o", "    >>> dict_learner = MiniBatchDictionaryLearning(\r\n"]
[159.409762, "o", "    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n"]
[159.426132, "o", "    ...     transform_alpha=0.1, random_state=42)\r\n"]
[159.442503, "o", "    >>> X_transformed = dict_learner.fit_transform(X)\r\n"]
[159.458874, "o", "\r\n"]
[159.475245, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[159.491616, "o", "\r\n"]
[159.507987, "o", "    >>> np.mean(X_transformed == 0) < 0.5\r\n"]
[159.524358, "o", "    True\r\n"]
[159.540728, "o", "\r\n"]
[159.557099, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[159.57347, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[159.589841, "o", "    the original signal:\r\n"]
[159.606212, "o", "\r\n"]
[159.622583, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[159.638954, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[159.655325, "o", "    0.057...\r\n"]
[159.671695, "o", "    \"\"\"\r\n"]
[159.688066, "o", "\r\n"]
[159.704437, "o", "    _parameter_constraints: dict = {\r\n"]
[159.720808, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[159.737179, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[159.75355, "o", "        \"n_iter\": [\r\n"]
[159.769921, "o", "            Interval(Integral, 0, None, closed=\"left\"),\r\n"]
[159.786291, "o", "            Hidden(StrOptions({\"deprecated\"})),\r\n"]
[159.802662, "o", "        ],\r\n"]
[159.819033, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n"]
[159.835404, "o", "        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n"]
[159.851775, "o", "        \"n_jobs\": [None, Integral],\r\n"]
[159.868146, "o", "        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n"]
[159.884517, "o", "        \"shuffle\": [\"boolean\"],\r\n"]
[159.900887, "o", "        \"dict_init\": [None, np.ndarray],\r\n"]
[159.917258, "o", "        \"transform_algorithm\": [\r\n"]
[159.933629, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[160.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[160.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\" --show-stderr\r"]
[160.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\" --show-stderr\r\n"]
[162.478, "o", "\u001b[?2004l\r\n"]
[165.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[165.002, "i", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r"]
[165.004, "o", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r\n"]
[167.478, "o", "\u001b[?2004l\r\n"]
[170.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[170.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[170.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[172.478, "o", "\u001b[?2004l\r\n"]
[175.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[175.002, "i", "cd ..\r"]
[175.004, "o", "cd ..\r\n"]
[175.006, "o", "\u001b[?2004l\r\n"]
[180.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace# "]
[180.002, "i", "grep -n \"def dict_learning_online\\|class MiniBatchDictionaryLearning\\|def _mini_batch_dictionary_learning\" -n sklearn/de\r"]
[180.004, "o", "grep -n \"def dict_learning_online\\|class MiniBatchDictionaryLearning\\|def _mini_batch_dictionary_learning\" -n sklearn/de\r\n"]
[181.654, "o", "ecomposition/_dict_learning.py || true\r\n"]
[183.302, "o", "\u001b[?2004l\r\n"]
[185.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace# "]
[185.002, "i", "sed -n '1500,2200p' sklearn/decomposition/_dict_learning.py\r"]
[185.004, "o", "sed -n '1500,2200p' sklearn/decomposition/_dict_learning.py\r\n"]
[187.478, "o", "\u001b[?2004l\r\n"]
[190.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace# "]
[190.002, "i", "cd /workspace/repo\r"]
[190.004, "o", "cd /workspace/repo\r\n"]
[190.006, "o", "\u001b[?2004l\r\n"]
[195.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[195.002, "i", "cd asv_benchmarks\r"]
[195.004, "o", "cd asv_benchmarks\r\n"]
[195.006, "o", "\u001b[?2004l\r\n"]
[200.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[200.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[200.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[201.654, "o", "\u001b[?2004l\r\n"]
[203.302, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[205.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[205.002, "i", "cd ..\r"]
[205.004, "o", "cd ..\r\n"]
[205.006, "o", "\u001b[?2004l\r\n"]
[210.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[210.002, "i", "grep -n \"def dict_learning_online\\|def _mini_batch_dictionary_learning\\|class MiniBatchDictionaryLearning\" sklearn/\r"]
[210.004, "o", "grep -n \"def dict_learning_online\\|def _mini_batch_dictionary_learning\\|class MiniBatchDictionaryLearning\" sklearn/\r\n"]
[211.242, "o", "/decomposition/_dict_learning.py\r\n"]
[212.478, "o", "\u001b[?2004l\r\n"]
[213.714, "o", "\u001b[32m\u001b[K664\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kdef dict_learning_online\u001b[m\u001b[K(\r\n"]
[215.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[215.002, "i", "sed -n '1,320p' sklearn/decomposition/_dict_learning.py\r"]
[215.004, "o", "sed -n '1,320p' sklearn/decomposition/_dict_learning.py\r\n"]
[215.021402, "o", "\u001b[?2004l\r\n"]
[215.036804, "o", "\"\"\" Dictionary learning.\r\n"]
[215.052206, "o", "\"\"\"\r\n"]
[215.067607, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[215.083009, "o", "# License: BSD 3 clause\r\n"]
[215.098411, "o", "\r\n"]
[215.113813, "o", "import itertools\r\n"]
[215.129215, "o", "import sys\r\n"]
[215.144617, "o", "import time\r\n"]
[215.160019, "o", "import warnings\r\n"]
[215.175421, "o", "from math import ceil\r\n"]
[215.190822, "o", "from numbers import Integral, Real\r\n"]
[215.206224, "o", "\r\n"]
[215.221626, "o", "import numpy as np\r\n"]
[215.237028, "o", "from joblib import effective_n_jobs\r\n"]
[215.25243, "o", "from scipy import linalg\r\n"]
[215.267832, "o", "\r\n"]
[215.283234, "o", "from ..base import (\r\n"]
[215.298636, "o", "    BaseEstimator,\r\n"]
[215.314037, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[215.329439, "o", "    TransformerMixin,\r\n"]
[215.344841, "o", "    _fit_context,\r\n"]
[215.360243, "o", ")\r\n"]
[215.375645, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[215.391047, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[215.406449, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[215.42185, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[215.437252, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[215.452654, "o", "from ..utils.validation import check_is_fitted\r\n"]
[215.468056, "o", "\r\n"]
[215.483458, "o", "\r\n"]
[215.49886, "o", "def _check_positive_coding(method, positive):\r\n"]
[215.514262, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[215.529664, "o", "        raise ValueError(\r\n"]
[215.545065, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[215.560467, "o", "        )\r\n"]
[215.575869, "o", "\r\n"]
[215.591271, "o", "\r\n"]
[215.606673, "o", "def _sparse_encode_precomputed(\r\n"]
[215.622075, "o", "    X,\r\n"]
[215.637477, "o", "    dictionary,\r\n"]
[215.652879, "o", "    *,\r\n"]
[215.66828, "o", "    gram=None,\r\n"]
[215.683682, "o", "    cov=None,\r\n"]
[215.699084, "o", "    algorithm=\"lasso_lars\",\r\n"]
[215.714486, "o", "    regularization=None,\r\n"]
[215.729888, "o", "    copy_cov=True,\r\n"]
[215.74529, "o", "    init=None,\r\n"]
[215.760692, "o", "    max_iter=1000,\r\n"]
[215.776093, "o", "    verbose=0,\r\n"]
[215.791495, "o", "    positive=False,\r\n"]
[215.806897, "o", "):\r\n"]
[215.822299, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[215.837701, "o", "\r\n"]
[215.853103, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[215.868505, "o", "\r\n"]
[215.883907, "o", "    Parameters\r\n"]
[215.899308, "o", "    ----------\r\n"]
[215.91471, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[215.930112, "o", "        Data matrix.\r\n"]
[215.945514, "o", "\r\n"]
[215.960916, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[215.976318, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[215.99172, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[216.007121, "o", "\r\n"]
[216.022523, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[216.037925, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[216.053327, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[216.068729, "o", "\r\n"]
[216.084131, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[216.099533, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[216.114935, "o", "\r\n"]
[216.130336, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[216.145738, "o", "            default='lasso_lars'\r\n"]
[216.16114, "o", "        The algorithm used:\r\n"]
[216.176542, "o", "\r\n"]
[216.191944, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[216.207346, "o", "          (`linear_model.lars_path`);\r\n"]
[216.222748, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[216.23815, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[216.253551, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[216.268953, "o", "          the estimated components are sparse;\r\n"]
[216.284355, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[216.299757, "o", "          solution;\r\n"]
[216.315159, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[216.330561, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[216.345963, "o", "\r\n"]
[216.361364, "o", "    regularization : int or float, default=None\r\n"]
[216.376766, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[216.392168, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[216.40757, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[216.422972, "o", "\r\n"]
[216.438374, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[216.453776, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[216.469178, "o", "        `algorithm='lasso_cd'`.\r\n"]
[216.484579, "o", "\r\n"]
[216.499981, "o", "    max_iter : int, default=1000\r\n"]
[216.515383, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[216.530785, "o", "        `'lasso_lars'`.\r\n"]
[216.546187, "o", "\r\n"]
[216.561589, "o", "    copy_cov : bool, default=True\r\n"]
[216.576991, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[216.592393, "o", "        be overwritten.\r\n"]
[216.607794, "o", "\r\n"]
[216.623196, "o", "    verbose : int, default=0\r\n"]
[216.638598, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[216.654, "o", "\r\n"]
[216.669402, "o", "    positive: bool, default=False\r\n"]
[216.684804, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[216.700206, "o", "\r\n"]
[216.715607, "o", "        .. versionadded:: 0.20\r\n"]
[216.731009, "o", "\r\n"]
[216.746411, "o", "    Returns\r\n"]
[216.761813, "o", "    -------\r\n"]
[216.777215, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[216.792617, "o", "        The sparse codes.\r\n"]
[216.808019, "o", "    \"\"\"\r\n"]
[216.823421, "o", "    n_samples, n_features = X.shape\r\n"]
[216.838822, "o", "    n_components = dictionary.shape[0]\r\n"]
[216.854224, "o", "\r\n"]
[216.869626, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[216.885028, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[216.90043, "o", "        try:\r\n"]
[216.915832, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[216.931234, "o", "\r\n"]
[216.946636, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[216.962037, "o", "            # corrects the verbosity level.\r\n"]
[216.977439, "o", "            lasso_lars = LassoLars(\r\n"]
[216.992841, "o", "                alpha=alpha,\r\n"]
[217.008243, "o", "                fit_intercept=False,\r\n"]
[217.023645, "o", "                verbose=verbose,\r\n"]
[217.039047, "o", "                precompute=gram,\r\n"]
[217.054449, "o", "                fit_path=False,\r\n"]
[217.06985, "o", "                positive=positive,\r\n"]
[217.085252, "o", "                max_iter=max_iter,\r\n"]
[217.100654, "o", "            )\r\n"]
[217.116056, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[217.131458, "o", "            new_code = lasso_lars.coef_\r\n"]
[217.14686, "o", "        finally:\r\n"]
[217.162262, "o", "            np.seterr(**err_mgt)\r\n"]
[217.177664, "o", "\r\n"]
[217.193065, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[217.208467, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[217.223869, "o", "\r\n"]
[217.239271, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[217.254673, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[217.270075, "o", "        # argument that we could pass in from Lasso.\r\n"]
[217.285477, "o", "        clf = Lasso(\r\n"]
[217.300879, "o", "            alpha=alpha,\r\n"]
[217.31628, "o", "            fit_intercept=False,\r\n"]
[217.331682, "o", "            precompute=gram,\r\n"]
[217.347084, "o", "            max_iter=max_iter,\r\n"]
[217.362486, "o", "            warm_start=True,\r\n"]
[217.377888, "o", "            positive=positive,\r\n"]
[217.39329, "o", "        )\r\n"]
[217.408692, "o", "\r\n"]
[217.424093, "o", "        if init is not None:\r\n"]
[217.439495, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[217.454897, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[217.470299, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[217.485701, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[217.501103, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[217.516505, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[217.531907, "o", "                init = np.array(init)\r\n"]
[217.547308, "o", "            clf.coef_ = init\r\n"]
[217.56271, "o", "\r\n"]
[217.578112, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[217.593514, "o", "        new_code = clf.coef_\r\n"]
[217.608916, "o", "\r\n"]
[217.624318, "o", "    elif algorithm == \"lars\":\r\n"]
[217.63972, "o", "        try:\r\n"]
[217.655121, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[217.670523, "o", "\r\n"]
[217.685925, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[217.701327, "o", "            # corrects the verbosity level.\r\n"]
[217.716729, "o", "            lars = Lars(\r\n"]
[217.732131, "o", "                fit_intercept=False,\r\n"]
[217.747533, "o", "                verbose=verbose,\r\n"]
[217.762935, "o", "                precompute=gram,\r\n"]
[217.778336, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[217.793738, "o", "                fit_path=False,\r\n"]
[217.80914, "o", "            )\r\n"]
[217.824542, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[217.839944, "o", "            new_code = lars.coef_\r\n"]
[217.855346, "o", "        finally:\r\n"]
[217.870748, "o", "            np.seterr(**err_mgt)\r\n"]
[217.88615, "o", "\r\n"]
[217.901551, "o", "    elif algorithm == \"threshold\":\r\n"]
[217.916953, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[217.932355, "o", "        if positive:\r\n"]
[217.947757, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[217.963159, "o", "\r\n"]
[217.978561, "o", "    elif algorithm == \"omp\":\r\n"]
[217.993963, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[218.009364, "o", "            Gram=gram,\r\n"]
[218.024766, "o", "            Xy=cov,\r\n"]
[218.040168, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[218.05557, "o", "            tol=None,\r\n"]
[218.070972, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[218.086374, "o", "            copy_Xy=copy_cov,\r\n"]
[218.101776, "o", "        ).T\r\n"]
[218.117178, "o", "\r\n"]
[218.132579, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[218.147981, "o", "\r\n"]
[218.163383, "o", "\r\n"]
[218.178785, "o", "@validate_params(\r\n"]
[218.194187, "o", "    {\r\n"]
[218.209589, "o", "        \"X\": [\"array-like\"],\r\n"]
[218.224991, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[218.240393, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[218.255794, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[218.271196, "o", "        \"algorithm\": [\r\n"]
[218.286598, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[218.302, "o", "        ],\r\n"]
[218.317402, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[218.332804, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[218.348206, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[218.363607, "o", "        \"init\": [\"array-like\", None],\r\n"]
[218.379009, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[218.394411, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[218.409813, "o", "        \"check_input\": [\"boolean\"],\r\n"]
[218.425215, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[218.440617, "o", "        \"positive\": [\"boolean\"],\r\n"]
[218.456019, "o", "    },\r\n"]
[218.471421, "o", "    prefer_skip_nested_validation=True,\r\n"]
[218.486822, "o", ")\r\n"]
[218.502224, "o", "# XXX : could be moved to the linear_model module\r\n"]
[218.517626, "o", "def sparse_encode(\r\n"]
[218.533028, "o", "    X,\r\n"]
[218.54843, "o", "    dictionary,\r\n"]
[218.563832, "o", "    *,\r\n"]
[218.579234, "o", "    gram=None,\r\n"]
[218.594636, "o", "    cov=None,\r\n"]
[218.610037, "o", "    algorithm=\"lasso_lars\",\r\n"]
[218.625439, "o", "    n_nonzero_coefs=None,\r\n"]
[218.640841, "o", "    alpha=None,\r\n"]
[218.656243, "o", "    copy_cov=True,\r\n"]
[218.671645, "o", "    init=None,\r\n"]
[218.687047, "o", "    max_iter=1000,\r\n"]
[218.702449, "o", "    n_jobs=None,\r\n"]
[218.71785, "o", "    check_input=True,\r\n"]
[218.733252, "o", "    verbose=0,\r\n"]
[218.748654, "o", "    positive=False,\r\n"]
[218.764056, "o", "):\r\n"]
[218.779458, "o", "    \"\"\"Sparse coding.\r\n"]
[218.79486, "o", "\r\n"]
[218.810262, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[218.825664, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[218.841065, "o", "\r\n"]
[218.856467, "o", "        X ~= code * dictionary\r\n"]
[218.871869, "o", "\r\n"]
[218.887271, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[218.902673, "o", "\r\n"]
[218.918075, "o", "    Parameters\r\n"]
[218.933477, "o", "    ----------\r\n"]
[218.948879, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[218.96428, "o", "        Data matrix.\r\n"]
[218.979682, "o", "\r\n"]
[218.995084, "o", "    dictionary : array-like of shape (n_components, n_features)\r\n"]
[219.010486, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[219.025888, "o", "        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[219.04129, "o", "        output.\r\n"]
[219.056692, "o", "\r\n"]
[219.072093, "o", "    gram : array-like of shape (n_components, n_components), default=None\r\n"]
[219.087495, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n"]
[219.102897, "o", "\r\n"]
[219.118299, "o", "    cov : array-like of shape (n_components, n_samples), default=None\r\n"]
[219.133701, "o", "        Precomputed covariance, `dictionary' * X`.\r\n"]
[219.149103, "o", "\r\n"]
[219.164505, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[219.179907, "o", "            default='lasso_lars'\r\n"]
[219.195308, "o", "        The algorithm used:\r\n"]
[219.21071, "o", "\r\n"]
[219.226112, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[219.241514, "o", "          (`linear_model.lars_path`);\r\n"]
[219.256916, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[219.272318, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[219.28772, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[219.303121, "o", "          the estimated components are sparse;\r\n"]
[219.318523, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[219.333925, "o", "          solution;\r\n"]
[219.349327, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[219.364729, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[219.380131, "o", "\r\n"]
[219.395533, "o", "    n_nonzero_coefs : int, default=None\r\n"]
[219.410935, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[219.426336, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[219.441738, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[219.45714, "o", "        `n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[219.472542, "o", "\r\n"]
[219.487944, "o", "    alpha : float, default=None\r\n"]
[219.503346, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[219.518748, "o", "        penalty applied to the L1 norm.\r\n"]
[219.53415, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[219.549551, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[219.564953, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[219.580355, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[219.595757, "o", "        `n_nonzero_coefs`.\r\n"]
[219.611159, "o", "        If `None`, default to 1.\r\n"]
[219.626561, "o", "\r\n"]
[219.641963, "o", "    copy_cov : bool, default=True\r\n"]
[219.657364, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[219.672766, "o", "        be overwritten.\r\n"]
[219.688168, "o", "\r\n"]
[219.70357, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[219.718972, "o", "        Initialization value of the sparse codes. Only used if\r\n"]
[219.734374, "o", "        `algorithm='lasso_cd'`.\r\n"]
[219.749776, "o", "\r\n"]
[219.765178, "o", "    max_iter : int, default=1000\r\n"]
[219.780579, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[219.795981, "o", "        `'lasso_lars'`.\r\n"]
[219.811383, "o", "\r\n"]
[219.826785, "o", "    n_jobs : int, default=None\r\n"]
[219.842187, "o", "        Number of parallel jobs to run.\r\n"]
[219.857589, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[219.872991, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[219.888393, "o", "        for more details.\r\n"]
[219.903794, "o", "\r\n"]
[219.919196, "o", "    check_input : bool, default=True\r\n"]
[219.934598, "o", "        If `False`, the input arrays X and dictionary will not be checked.\r\n"]
[220.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[220.002, "i", "sed -n '320,900p' sklearn/decomposition/_dict_learning.py\r"]
[220.004, "o", "sed -n '320,900p' sklearn/decomposition/_dict_learning.py\r\n"]
[220.016, "o", "\u001b[?2004l\r\n"]
[220.026, "o", "\r\n"]
[220.036, "o", "    verbose : int, default=0\r\n"]
[220.046, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[220.056, "o", "\r\n"]
[220.066, "o", "    positive : bool, default=False\r\n"]
[220.076, "o", "        Whether to enforce positivity when finding the encoding.\r\n"]
[220.086, "o", "\r\n"]
[220.096, "o", "        .. versionadded:: 0.20\r\n"]
[220.106, "o", "\r\n"]
[220.116, "o", "    Returns\r\n"]
[220.126, "o", "    -------\r\n"]
[220.136, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[220.146, "o", "        The sparse codes.\r\n"]
[220.156, "o", "\r\n"]
[220.166, "o", "    See Also\r\n"]
[220.176, "o", "    --------\r\n"]
[220.186, "o", "    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n"]
[220.196, "o", "        path using LARS algorithm.\r\n"]
[220.206, "o", "    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n"]
[220.216, "o", "    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n"]
[220.226, "o", "    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n"]
[220.236, "o", "        dictionary.\r\n"]
[220.246, "o", "    \"\"\"\r\n"]
[220.256, "o", "    if check_input:\r\n"]
[220.266, "o", "        if algorithm == \"lasso_cd\":\r\n"]
[220.276, "o", "            dictionary = check_array(\r\n"]
[220.286, "o", "                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n"]
[220.296, "o", "            )\r\n"]
[220.306, "o", "            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n"]
[220.316, "o", "        else:\r\n"]
[220.326, "o", "            dictionary = check_array(dictionary)\r\n"]
[220.336, "o", "            X = check_array(X)\r\n"]
[220.346, "o", "\r\n"]
[220.356, "o", "    if dictionary.shape[1] != X.shape[1]:\r\n"]
[220.366, "o", "        raise ValueError(\r\n"]
[220.376, "o", "            \"Dictionary and X have different numbers of features:\"\r\n"]
[220.386, "o", "            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n"]
[220.396, "o", "        )\r\n"]
[220.406, "o", "\r\n"]
[220.416, "o", "    _check_positive_coding(algorithm, positive)\r\n"]
[220.426, "o", "\r\n"]
[220.436, "o", "    return _sparse_encode(\r\n"]
[220.446, "o", "        X,\r\n"]
[220.456, "o", "        dictionary,\r\n"]
[220.466, "o", "        gram=gram,\r\n"]
[220.476, "o", "        cov=cov,\r\n"]
[220.486, "o", "        algorithm=algorithm,\r\n"]
[220.496, "o", "        n_nonzero_coefs=n_nonzero_coefs,\r\n"]
[220.506, "o", "        alpha=alpha,\r\n"]
[220.516, "o", "        copy_cov=copy_cov,\r\n"]
[220.526, "o", "        init=init,\r\n"]
[220.536, "o", "        max_iter=max_iter,\r\n"]
[220.546, "o", "        n_jobs=n_jobs,\r\n"]
[220.556, "o", "        verbose=verbose,\r\n"]
[220.566, "o", "        positive=positive,\r\n"]
[220.576, "o", "    )\r\n"]
[220.586, "o", "\r\n"]
[220.596, "o", "\r\n"]
[220.606, "o", "def _sparse_encode(\r\n"]
[220.616, "o", "    X,\r\n"]
[220.626, "o", "    dictionary,\r\n"]
[220.636, "o", "    *,\r\n"]
[220.646, "o", "    gram=None,\r\n"]
[220.656, "o", "    cov=None,\r\n"]
[220.666, "o", "    algorithm=\"lasso_lars\",\r\n"]
[220.676, "o", "    n_nonzero_coefs=None,\r\n"]
[220.686, "o", "    alpha=None,\r\n"]
[220.696, "o", "    copy_cov=True,\r\n"]
[220.706, "o", "    init=None,\r\n"]
[220.716, "o", "    max_iter=1000,\r\n"]
[220.726, "o", "    n_jobs=None,\r\n"]
[220.736, "o", "    verbose=0,\r\n"]
[220.746, "o", "    positive=False,\r\n"]
[220.756, "o", "):\r\n"]
[220.766, "o", "    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[220.776, "o", "\r\n"]
[220.786, "o", "    n_samples, n_features = X.shape\r\n"]
[220.796, "o", "    n_components = dictionary.shape[0]\r\n"]
[220.806, "o", "\r\n"]
[220.816, "o", "    if algorithm in (\"lars\", \"omp\"):\r\n"]
[220.826, "o", "        regularization = n_nonzero_coefs\r\n"]
[220.836, "o", "        if regularization is None:\r\n"]
[220.846, "o", "            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[220.856, "o", "    else:\r\n"]
[220.866, "o", "        regularization = alpha\r\n"]
[220.876, "o", "        if regularization is None:\r\n"]
[220.886, "o", "            regularization = 1.0\r\n"]
[220.896, "o", "\r\n"]
[220.906, "o", "    if gram is None and algorithm != \"threshold\":\r\n"]
[220.916, "o", "        gram = np.dot(dictionary, dictionary.T)\r\n"]
[220.926, "o", "\r\n"]
[220.936, "o", "    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[220.946, "o", "        copy_cov = False\r\n"]
[220.956, "o", "        cov = np.dot(dictionary, X.T)\r\n"]
[220.966, "o", "\r\n"]
[220.976, "o", "    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[220.986, "o", "        code = _sparse_encode_precomputed(\r\n"]
[220.996, "o", "            X,\r\n"]
[221.006, "o", "            dictionary,\r\n"]
[221.016, "o", "            gram=gram,\r\n"]
[221.026, "o", "            cov=cov,\r\n"]
[221.036, "o", "            algorithm=algorithm,\r\n"]
[221.046, "o", "            regularization=regularization,\r\n"]
[221.056, "o", "            copy_cov=copy_cov,\r\n"]
[221.066, "o", "            init=init,\r\n"]
[221.076, "o", "            max_iter=max_iter,\r\n"]
[221.086, "o", "            verbose=verbose,\r\n"]
[221.096, "o", "            positive=positive,\r\n"]
[221.106, "o", "        )\r\n"]
[221.116, "o", "        return code\r\n"]
[221.126, "o", "\r\n"]
[221.136, "o", "    # Enter parallel code block\r\n"]
[221.146, "o", "    n_samples = X.shape[0]\r\n"]
[221.156, "o", "    n_components = dictionary.shape[0]\r\n"]
[221.166, "o", "    code = np.empty((n_samples, n_components))\r\n"]
[221.176, "o", "    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[221.186, "o", "\r\n"]
[221.196, "o", "    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[221.206, "o", "        delayed(_sparse_encode_precomputed)(\r\n"]
[221.216, "o", "            X[this_slice],\r\n"]
[221.226, "o", "            dictionary,\r\n"]
[221.236, "o", "            gram=gram,\r\n"]
[221.246, "o", "            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[221.256, "o", "            algorithm=algorithm,\r\n"]
[221.266, "o", "            regularization=regularization,\r\n"]
[221.276, "o", "            copy_cov=copy_cov,\r\n"]
[221.286, "o", "            init=init[this_slice] if init is not None else None,\r\n"]
[221.296, "o", "            max_iter=max_iter,\r\n"]
[221.306, "o", "            verbose=verbose,\r\n"]
[221.316, "o", "            positive=positive,\r\n"]
[221.326, "o", "        )\r\n"]
[221.336, "o", "        for this_slice in slices\r\n"]
[221.346, "o", "    )\r\n"]
[221.356, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[221.366, "o", "        code[this_slice] = this_view\r\n"]
[221.376, "o", "    return code\r\n"]
[221.386, "o", "\r\n"]
[221.396, "o", "\r\n"]
[221.406, "o", "def _update_dict(\r\n"]
[221.416, "o", "    dictionary,\r\n"]
[221.426, "o", "    Y,\r\n"]
[221.436, "o", "    code,\r\n"]
[221.446, "o", "    A=None,\r\n"]
[221.456, "o", "    B=None,\r\n"]
[221.466, "o", "    verbose=False,\r\n"]
[221.476, "o", "    random_state=None,\r\n"]
[221.486, "o", "    positive=False,\r\n"]
[221.496, "o", "):\r\n"]
[221.506, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[221.516, "o", "\r\n"]
[221.526, "o", "    Parameters\r\n"]
[221.536, "o", "    ----------\r\n"]
[221.546, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[221.556, "o", "        Value of the dictionary at the previous iteration.\r\n"]
[221.566, "o", "\r\n"]
[221.576, "o", "    Y : ndarray of shape (n_samples, n_features)\r\n"]
[221.586, "o", "        Data matrix.\r\n"]
[221.596, "o", "\r\n"]
[221.606, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[221.616, "o", "        Sparse coding of the data against which to optimize the dictionary.\r\n"]
[221.626, "o", "\r\n"]
[221.636, "o", "    A : ndarray of shape (n_components, n_components), default=None\r\n"]
[221.646, "o", "        Together with `B`, sufficient stats of the online model to update the\r\n"]
[221.656, "o", "        dictionary.\r\n"]
[221.666, "o", "\r\n"]
[221.676, "o", "    B : ndarray of shape (n_features, n_components), default=None\r\n"]
[221.686, "o", "        Together with `A`, sufficient stats of the online model to update the\r\n"]
[221.696, "o", "        dictionary.\r\n"]
[221.706, "o", "\r\n"]
[221.716, "o", "    verbose: bool, default=False\r\n"]
[221.726, "o", "        Degree of output the procedure will print.\r\n"]
[221.736, "o", "\r\n"]
[221.746, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[221.756, "o", "        Used for randomly initializing the dictionary. Pass an int for\r\n"]
[221.766, "o", "        reproducible results across multiple function calls.\r\n"]
[221.776, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[221.786, "o", "\r\n"]
[221.796, "o", "    positive : bool, default=False\r\n"]
[221.806, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[221.816, "o", "\r\n"]
[221.826, "o", "        .. versionadded:: 0.20\r\n"]
[221.836, "o", "    \"\"\"\r\n"]
[221.846, "o", "    n_samples, n_components = code.shape\r\n"]
[221.856, "o", "    random_state = check_random_state(random_state)\r\n"]
[221.866, "o", "\r\n"]
[221.876, "o", "    if A is None:\r\n"]
[221.886, "o", "        A = code.T @ code\r\n"]
[221.896, "o", "    if B is None:\r\n"]
[221.906, "o", "        B = Y.T @ code\r\n"]
[221.916, "o", "\r\n"]
[221.926, "o", "    n_unused = 0\r\n"]
[221.936, "o", "\r\n"]
[221.946, "o", "    for k in range(n_components):\r\n"]
[221.956, "o", "        if A[k, k] > 1e-6:\r\n"]
[221.966, "o", "            # 1e-6 is arbitrary but consistent with the spams implementation\r\n"]
[221.976, "o", "            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n"]
[221.986, "o", "        else:\r\n"]
[221.996, "o", "            # kth atom is almost never used -> sample a new one from the data\r\n"]
[222.006, "o", "            newd = Y[random_state.choice(n_samples)]\r\n"]
[222.016, "o", "\r\n"]
[222.026, "o", "            # add small noise to avoid making the sparse coding ill conditioned\r\n"]
[222.036, "o", "            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n"]
[222.046, "o", "            noise = random_state.normal(0, noise_level, size=len(newd))\r\n"]
[222.056, "o", "\r\n"]
[222.066, "o", "            dictionary[k] = newd + noise\r\n"]
[222.076, "o", "            code[:, k] = 0\r\n"]
[222.086, "o", "            n_unused += 1\r\n"]
[222.096, "o", "\r\n"]
[222.106, "o", "        if positive:\r\n"]
[222.116, "o", "            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n"]
[222.126, "o", "\r\n"]
[222.136, "o", "        # Projection on the constraint set ||V_k|| <= 1\r\n"]
[222.146, "o", "        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n"]
[222.156, "o", "\r\n"]
[222.166, "o", "    if verbose and n_unused > 0:\r\n"]
[222.176, "o", "        print(f\"{n_unused} unused atoms resampled.\")\r\n"]
[222.186, "o", "\r\n"]
[222.196, "o", "\r\n"]
[222.206, "o", "def _dict_learning(\r\n"]
[222.216, "o", "    X,\r\n"]
[222.226, "o", "    n_components,\r\n"]
[222.236, "o", "    *,\r\n"]
[222.246, "o", "    alpha,\r\n"]
[222.256, "o", "    max_iter,\r\n"]
[222.266, "o", "    tol,\r\n"]
[222.276, "o", "    method,\r\n"]
[222.286, "o", "    n_jobs,\r\n"]
[222.296, "o", "    dict_init,\r\n"]
[222.306, "o", "    code_init,\r\n"]
[222.316, "o", "    callback,\r\n"]
[222.326, "o", "    verbose,\r\n"]
[222.336, "o", "    random_state,\r\n"]
[222.346, "o", "    return_n_iter,\r\n"]
[222.356, "o", "    positive_dict,\r\n"]
[222.366, "o", "    positive_code,\r\n"]
[222.376, "o", "    method_max_iter,\r\n"]
[222.386, "o", "):\r\n"]
[222.396, "o", "    \"\"\"Main dictionary learning algorithm\"\"\"\r\n"]
[222.406, "o", "    t0 = time.time()\r\n"]
[222.416, "o", "    # Init the code and the dictionary with SVD of Y\r\n"]
[222.426, "o", "    if code_init is not None and dict_init is not None:\r\n"]
[222.436, "o", "        code = np.array(code_init, order=\"F\")\r\n"]
[222.446, "o", "        # Don't copy V, it will happen below\r\n"]
[222.456, "o", "        dictionary = dict_init\r\n"]
[222.466, "o", "    else:\r\n"]
[222.476, "o", "        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n"]
[222.486, "o", "        # flip the initial code's sign to enforce deterministic output\r\n"]
[222.496, "o", "        code, dictionary = svd_flip(code, dictionary)\r\n"]
[222.506, "o", "        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[222.516, "o", "    r = len(dictionary)\r\n"]
[222.526, "o", "    if n_components <= r:  # True even if n_components=None\r\n"]
[222.536, "o", "        code = code[:, :n_components]\r\n"]
[222.546, "o", "        dictionary = dictionary[:n_components, :]\r\n"]
[222.556, "o", "    else:\r\n"]
[222.566, "o", "        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n"]
[222.576, "o", "        dictionary = np.r_[\r\n"]
[222.586, "o", "            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n"]
[222.596, "o", "        ]\r\n"]
[222.606, "o", "\r\n"]
[222.616, "o", "    # Fortran-order dict better suited for the sparse coding which is the\r\n"]
[222.626, "o", "    # bottleneck of this algorithm.\r\n"]
[222.636, "o", "    dictionary = np.asfortranarray(dictionary)\r\n"]
[222.646, "o", "\r\n"]
[222.656, "o", "    errors = []\r\n"]
[222.666, "o", "    current_cost = np.nan\r\n"]
[222.676, "o", "\r\n"]
[222.686, "o", "    if verbose == 1:\r\n"]
[222.696, "o", "        print(\"[dict_learning]\", end=\" \")\r\n"]
[222.706, "o", "\r\n"]
[222.716, "o", "    # If max_iter is 0, number of iterations returned should be zero\r\n"]
[222.726, "o", "    ii = -1\r\n"]
[222.736, "o", "\r\n"]
[222.746, "o", "    for ii in range(max_iter):\r\n"]
[222.756, "o", "        dt = time.time() - t0\r\n"]
[222.766, "o", "        if verbose == 1:\r\n"]
[222.776, "o", "            sys.stdout.write(\".\")\r\n"]
[222.786, "o", "            sys.stdout.flush()\r\n"]
[222.796, "o", "        elif verbose:\r\n"]
[222.806, "o", "            print(\r\n"]
[222.816, "o", "                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n"]
[222.826, "o", "                % (ii, dt, dt / 60, current_cost)\r\n"]
[222.836, "o", "            )\r\n"]
[222.846, "o", "\r\n"]
[222.856, "o", "        # Update code\r\n"]
[222.866, "o", "        code = sparse_encode(\r\n"]
[222.876, "o", "            X,\r\n"]
[222.886, "o", "            dictionary,\r\n"]
[222.896, "o", "            algorithm=method,\r\n"]
[222.906, "o", "            alpha=alpha,\r\n"]
[222.916, "o", "            init=code,\r\n"]
[222.926, "o", "            n_jobs=n_jobs,\r\n"]
[222.936, "o", "            positive=positive_code,\r\n"]
[222.946, "o", "            max_iter=method_max_iter,\r\n"]
[222.956, "o", "            verbose=verbose,\r\n"]
[222.966, "o", "        )\r\n"]
[222.976, "o", "\r\n"]
[222.986, "o", "        # Update dictionary in place\r\n"]
[222.996, "o", "        _update_dict(\r\n"]
[223.006, "o", "            dictionary,\r\n"]
[223.016, "o", "            X,\r\n"]
[223.026, "o", "            code,\r\n"]
[223.036, "o", "            verbose=verbose,\r\n"]
[223.046, "o", "            random_state=random_state,\r\n"]
[223.056, "o", "            positive=positive_dict,\r\n"]
[223.066, "o", "        )\r\n"]
[223.076, "o", "\r\n"]
[223.086, "o", "        # Cost function\r\n"]
[223.096, "o", "        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n"]
[223.106, "o", "            np.abs(code)\r\n"]
[223.116, "o", "        )\r\n"]
[223.126, "o", "        errors.append(current_cost)\r\n"]
[223.136, "o", "\r\n"]
[223.146, "o", "        if ii > 0:\r\n"]
[223.156, "o", "            dE = errors[-2] - errors[-1]\r\n"]
[223.166, "o", "            # assert(dE >= -tol * errors[-1])\r\n"]
[223.176, "o", "            if dE < tol * errors[-1]:\r\n"]
[223.186, "o", "                if verbose == 1:\r\n"]
[223.196, "o", "                    # A line return\r\n"]
[223.206, "o", "                    print(\"\")\r\n"]
[223.216, "o", "                elif verbose:\r\n"]
[223.226, "o", "                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n"]
[223.236, "o", "                break\r\n"]
[223.246, "o", "        if ii % 5 == 0 and callback is not None:\r\n"]
[223.256, "o", "            callback(locals())\r\n"]
[223.266, "o", "\r\n"]
[223.276, "o", "    if return_n_iter:\r\n"]
[223.286, "o", "        return code, dictionary, errors, ii + 1\r\n"]
[223.296, "o", "    else:\r\n"]
[223.306, "o", "        return code, dictionary, errors\r\n"]
[223.316, "o", "\r\n"]
[223.326, "o", "\r\n"]
[223.336, "o", "def _check_warn_deprecated(param, name, default, additional_message=None):\r\n"]
[223.346, "o", "    if param != \"deprecated\":\r\n"]
[223.356, "o", "        msg = (\r\n"]
[223.366, "o", "            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n"]
[223.376, "o", "        )\r\n"]
[223.386, "o", "        if additional_message:\r\n"]
[223.396, "o", "            msg += f\" {additional_message}\"\r\n"]
[223.406, "o", "        warnings.warn(msg, FutureWarning)\r\n"]
[223.416, "o", "        return param\r\n"]
[223.426, "o", "    else:\r\n"]
[223.436, "o", "        return default\r\n"]
[223.446, "o", "\r\n"]
[223.456, "o", "\r\n"]
[223.466, "o", "def dict_learning_online(\r\n"]
[223.476, "o", "    X,\r\n"]
[223.486, "o", "    n_components=2,\r\n"]
[223.496, "o", "    *,\r\n"]
[223.506, "o", "    alpha=1,\r\n"]
[223.516, "o", "    n_iter=\"deprecated\",\r\n"]
[223.526, "o", "    max_iter=None,\r\n"]
[223.536, "o", "    return_code=True,\r\n"]
[223.546, "o", "    dict_init=None,\r\n"]
[223.556, "o", "    callback=None,\r\n"]
[223.566, "o", "    batch_size=256,\r\n"]
[223.576, "o", "    verbose=False,\r\n"]
[223.586, "o", "    shuffle=True,\r\n"]
[223.596, "o", "    n_jobs=None,\r\n"]
[223.606, "o", "    method=\"lars\",\r\n"]
[223.616, "o", "    iter_offset=\"deprecated\",\r\n"]
[223.626, "o", "    random_state=None,\r\n"]
[223.636, "o", "    return_inner_stats=\"deprecated\",\r\n"]
[223.646, "o", "    inner_stats=\"deprecated\",\r\n"]
[223.656, "o", "    return_n_iter=\"deprecated\",\r\n"]
[223.666, "o", "    positive_dict=False,\r\n"]
[223.676, "o", "    positive_code=False,\r\n"]
[223.686, "o", "    method_max_iter=1000,\r\n"]
[223.696, "o", "    tol=1e-3,\r\n"]
[223.706, "o", "    max_no_improvement=10,\r\n"]
[223.716, "o", "):\r\n"]
[223.726, "o", "    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n"]
[223.736, "o", "\r\n"]
[223.746, "o", "    Finds the best dictionary and the corresponding sparse code for\r\n"]
[223.756, "o", "    approximating the data matrix X by solving::\r\n"]
[223.766, "o", "\r\n"]
[223.776, "o", "        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[223.786, "o", "                     (U,V)\r\n"]
[223.796, "o", "                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n"]
[223.806, "o", "\r\n"]
[223.816, "o", "    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n"]
[223.826, "o", "    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n"]
[223.836, "o", "    which is the sum of the absolute values of all the entries in the matrix.\r\n"]
[223.846, "o", "    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n"]
[223.856, "o", "    the input data.\r\n"]
[223.866, "o", "\r\n"]
[223.876, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[223.886, "o", "\r\n"]
[223.896, "o", "    Parameters\r\n"]
[223.906, "o", "    ----------\r\n"]
[223.916, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[223.926, "o", "        Data matrix.\r\n"]
[223.936, "o", "\r\n"]
[223.946, "o", "    n_components : int or None, default=2\r\n"]
[223.956, "o", "        Number of dictionary atoms to extract. If None, then ``n_components``\r\n"]
[223.966, "o", "        is set to ``n_features``.\r\n"]
[223.976, "o", "\r\n"]
[223.986, "o", "    alpha : float, default=1\r\n"]
[223.996, "o", "        Sparsity controlling parameter.\r\n"]
[224.006, "o", "\r\n"]
[224.016, "o", "    n_iter : int, default=100\r\n"]
[224.026, "o", "        Number of mini-batch iterations to perform.\r\n"]
[224.036, "o", "\r\n"]
[224.046, "o", "        .. deprecated:: 1.1\r\n"]
[224.056, "o", "           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[224.066, "o", "           `max_iter` instead.\r\n"]
[224.076, "o", "\r\n"]
[224.086, "o", "    max_iter : int, default=None\r\n"]
[224.096, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[224.106, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[224.116, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[224.126, "o", "\r\n"]
[224.136, "o", "        .. versionadded:: 1.1\r\n"]
[224.146, "o", "\r\n"]
[224.156, "o", "    return_code : bool, default=True\r\n"]
[224.166, "o", "        Whether to also return the code U or just the dictionary `V`.\r\n"]
[224.176, "o", "\r\n"]
[224.186, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[224.196, "o", "        Initial values for the dictionary for warm restart scenarios.\r\n"]
[224.206, "o", "        If `None`, the initial values for the dictionary are created\r\n"]
[224.216, "o", "        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n"]
[224.226, "o", "\r\n"]
[224.236, "o", "    callback : callable, default=None\r\n"]
[224.246, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[224.256, "o", "\r\n"]
[224.266, "o", "    batch_size : int, default=256\r\n"]
[224.276, "o", "        The number of samples to take in each batch.\r\n"]
[224.286, "o", "\r\n"]
[224.296, "o", "        .. versionchanged:: 1.3\r\n"]
[224.306, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[224.316, "o", "\r\n"]
[224.326, "o", "    verbose : bool, default=False\r\n"]
[224.336, "o", "        To control the verbosity of the procedure.\r\n"]
[224.346, "o", "\r\n"]
[224.356, "o", "    shuffle : bool, default=True\r\n"]
[224.366, "o", "        Whether to shuffle the data before splitting it in batches.\r\n"]
[224.376, "o", "\r\n"]
[224.386, "o", "    n_jobs : int, default=None\r\n"]
[224.396, "o", "        Number of parallel jobs to run.\r\n"]
[224.406, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[224.416, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[224.426, "o", "        for more details.\r\n"]
[224.436, "o", "\r\n"]
[224.446, "o", "    method : {'lars', 'cd'}, default='lars'\r\n"]
[224.456, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[224.466, "o", "          problem (`linear_model.lars_path`);\r\n"]
[224.476, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[224.486, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[224.496, "o", "          the estimated components are sparse.\r\n"]
[224.506, "o", "\r\n"]
[224.516, "o", "    iter_offset : int, default=0\r\n"]
[224.526, "o", "        Number of previous iterations completed on the dictionary used for\r\n"]
[224.536, "o", "        initialization.\r\n"]
[224.546, "o", "\r\n"]
[224.556, "o", "        .. deprecated:: 1.1\r\n"]
[224.566, "o", "           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n"]
[224.576, "o", "\r\n"]
[224.586, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[224.596, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[224.606, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[224.616, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[224.626, "o", "        results across multiple function calls.\r\n"]
[224.636, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[224.646, "o", "\r\n"]
[224.656, "o", "    return_inner_stats : bool, default=False\r\n"]
[224.666, "o", "        Return the inner statistics A (dictionary covariance) and B\r\n"]
[224.676, "o", "        (data approximation). Useful to restart the algorithm in an\r\n"]
[224.686, "o", "        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n"]
[224.696, "o", "        ignored.\r\n"]
[224.706, "o", "\r\n"]
[224.716, "o", "        .. deprecated:: 1.1\r\n"]
[224.726, "o", "           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[224.736, "o", "\r\n"]
[224.746, "o", "    inner_stats : tuple of (A, B) ndarrays, default=None\r\n"]
[224.756, "o", "        Inner sufficient statistics that are kept by the algorithm.\r\n"]
[224.766, "o", "        Passing them at initialization is useful in online settings, to\r\n"]
[224.776, "o", "        avoid losing the history of the evolution.\r\n"]
[224.786, "o", "        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n"]
[224.796, "o", "        `B` `(n_features, n_components)` is the data approximation matrix.\r\n"]
[224.806, "o", "\r\n"]
[224.816, "o", "        .. deprecated:: 1.1\r\n"]
[224.826, "o", "           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[224.836, "o", "\r\n"]
[224.846, "o", "    return_n_iter : bool, default=False\r\n"]
[224.856, "o", "        Whether or not to return the number of iterations.\r\n"]
[224.866, "o", "\r\n"]
[224.876, "o", "        .. deprecated:: 1.1\r\n"]
[224.886, "o", "           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n"]
[224.896, "o", "\r\n"]
[224.906, "o", "    positive_dict : bool, default=False\r\n"]
[224.916, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[224.926, "o", "\r\n"]
[224.936, "o", "        .. versionadded:: 0.20\r\n"]
[225.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[225.002, "i", "sed -n '900,1600p' sklearn/decomposition/_dict_learning.py\r"]
[225.004, "o", "sed -n '900,1600p' sklearn/decomposition/_dict_learning.py\r\n"]
[225.016, "o", "\u001b[?2004l\r\n"]
[225.026, "o", "            random_state=random_state,\r\n"]
[225.036, "o", "            transform_algorithm=transform_algorithm,\r\n"]
[225.046, "o", "            transform_alpha=alpha,\r\n"]
[225.056, "o", "            positive_code=positive_code,\r\n"]
[225.066, "o", "            positive_dict=positive_dict,\r\n"]
[225.076, "o", "            transform_max_iter=method_max_iter,\r\n"]
[225.086, "o", "            verbose=verbose,\r\n"]
[225.096, "o", "            callback=callback,\r\n"]
[225.106, "o", "            tol=tol,\r\n"]
[225.116, "o", "            max_no_improvement=max_no_improvement,\r\n"]
[225.126, "o", "        ).fit(X)\r\n"]
[225.136, "o", "\r\n"]
[225.146, "o", "        if not return_code:\r\n"]
[225.156, "o", "            return est.components_\r\n"]
[225.166, "o", "        else:\r\n"]
[225.176, "o", "            code = est.transform(X)\r\n"]
[225.186, "o", "            return code, est.components_\r\n"]
[225.196, "o", "\r\n"]
[225.206, "o", "    # TODO(1.4) remove the whole old behavior\r\n"]
[225.216, "o", "    # Fallback to old behavior\r\n"]
[225.226, "o", "\r\n"]
[225.236, "o", "    n_iter = _check_warn_deprecated(\r\n"]
[225.246, "o", "        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n"]
[225.256, "o", "    )\r\n"]
[225.266, "o", "\r\n"]
[225.276, "o", "    if n_components is None:\r\n"]
[225.286, "o", "        n_components = X.shape[1]\r\n"]
[225.296, "o", "\r\n"]
[225.306, "o", "    if method not in (\"lars\", \"cd\"):\r\n"]
[225.316, "o", "        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n"]
[225.326, "o", "\r\n"]
[225.336, "o", "    _check_positive_coding(method, positive_code)\r\n"]
[225.346, "o", "\r\n"]
[225.356, "o", "    method = \"lasso_\" + method\r\n"]
[225.366, "o", "\r\n"]
[225.376, "o", "    t0 = time.time()\r\n"]
[225.386, "o", "    n_samples, n_features = X.shape\r\n"]
[225.396, "o", "    # Avoid integer division problems\r\n"]
[225.406, "o", "    alpha = float(alpha)\r\n"]
[225.416, "o", "    random_state = check_random_state(random_state)\r\n"]
[225.426, "o", "\r\n"]
[225.436, "o", "    # Init V with SVD of X\r\n"]
[225.446, "o", "    if dict_init is not None:\r\n"]
[225.456, "o", "        dictionary = dict_init\r\n"]
[225.466, "o", "    else:\r\n"]
[225.476, "o", "        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n"]
[225.486, "o", "        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[225.496, "o", "    r = len(dictionary)\r\n"]
[225.506, "o", "    if n_components <= r:\r\n"]
[225.516, "o", "        dictionary = dictionary[:n_components, :]\r\n"]
[225.526, "o", "    else:\r\n"]
[225.536, "o", "        dictionary = np.r_[\r\n"]
[225.546, "o", "            dictionary,\r\n"]
[225.556, "o", "            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n"]
[225.566, "o", "        ]\r\n"]
[225.576, "o", "\r\n"]
[225.586, "o", "    if verbose == 1:\r\n"]
[225.596, "o", "        print(\"[dict_learning]\", end=\" \")\r\n"]
[225.606, "o", "\r\n"]
[225.616, "o", "    if shuffle:\r\n"]
[225.626, "o", "        X_train = X.copy()\r\n"]
[225.636, "o", "        random_state.shuffle(X_train)\r\n"]
[225.646, "o", "    else:\r\n"]
[225.656, "o", "        X_train = X\r\n"]
[225.666, "o", "\r\n"]
[225.676, "o", "    X_train = check_array(\r\n"]
[225.686, "o", "        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n"]
[225.696, "o", "    )\r\n"]
[225.706, "o", "\r\n"]
[225.716, "o", "    # Fortran-order dict better suited for the sparse coding which is the\r\n"]
[225.726, "o", "    # bottleneck of this algorithm.\r\n"]
[225.736, "o", "    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n"]
[225.746, "o", "    dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[225.756, "o", "\r\n"]
[225.766, "o", "    batches = gen_batches(n_samples, batch_size)\r\n"]
[225.776, "o", "    batches = itertools.cycle(batches)\r\n"]
[225.786, "o", "\r\n"]
[225.796, "o", "    # The covariance of the dictionary\r\n"]
[225.806, "o", "    if inner_stats is None:\r\n"]
[225.816, "o", "        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n"]
[225.826, "o", "        # The data approximation\r\n"]
[225.836, "o", "        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n"]
[225.846, "o", "    else:\r\n"]
[225.856, "o", "        A = inner_stats[0].copy()\r\n"]
[225.866, "o", "        B = inner_stats[1].copy()\r\n"]
[225.876, "o", "\r\n"]
[225.886, "o", "    # If n_iter is zero, we need to return zero.\r\n"]
[225.896, "o", "    ii = iter_offset - 1\r\n"]
[225.906, "o", "\r\n"]
[225.916, "o", "    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n"]
[225.926, "o", "        this_X = X_train[batch]\r\n"]
[225.936, "o", "        dt = time.time() - t0\r\n"]
[225.946, "o", "        if verbose == 1:\r\n"]
[225.956, "o", "            sys.stdout.write(\".\")\r\n"]
[225.966, "o", "            sys.stdout.flush()\r\n"]
[225.976, "o", "        elif verbose:\r\n"]
[225.986, "o", "            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n"]
[225.996, "o", "                print(\r\n"]
[226.006, "o", "                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n"]
[226.016, "o", "                )\r\n"]
[226.026, "o", "\r\n"]
[226.036, "o", "        this_code = sparse_encode(\r\n"]
[226.046, "o", "            this_X,\r\n"]
[226.056, "o", "            dictionary,\r\n"]
[226.066, "o", "            algorithm=method,\r\n"]
[226.076, "o", "            alpha=alpha,\r\n"]
[226.086, "o", "            n_jobs=n_jobs,\r\n"]
[226.096, "o", "            check_input=False,\r\n"]
[226.106, "o", "            positive=positive_code,\r\n"]
[226.116, "o", "            max_iter=method_max_iter,\r\n"]
[226.126, "o", "            verbose=verbose,\r\n"]
[226.136, "o", "        )\r\n"]
[226.146, "o", "\r\n"]
[226.156, "o", "        # Update the auxiliary variables\r\n"]
[226.166, "o", "        if ii < batch_size - 1:\r\n"]
[226.176, "o", "            theta = float((ii + 1) * batch_size)\r\n"]
[226.186, "o", "        else:\r\n"]
[226.196, "o", "            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n"]
[226.206, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[226.216, "o", "\r\n"]
[226.226, "o", "        A *= beta\r\n"]
[226.236, "o", "        A += np.dot(this_code.T, this_code)\r\n"]
[226.246, "o", "        B *= beta\r\n"]
[226.256, "o", "        B += np.dot(this_X.T, this_code)\r\n"]
[226.266, "o", "\r\n"]
[226.276, "o", "        # Update dictionary in place\r\n"]
[226.286, "o", "        _update_dict(\r\n"]
[226.296, "o", "            dictionary,\r\n"]
[226.306, "o", "            this_X,\r\n"]
[226.316, "o", "            this_code,\r\n"]
[226.326, "o", "            A,\r\n"]
[226.336, "o", "            B,\r\n"]
[226.346, "o", "            verbose=verbose,\r\n"]
[226.356, "o", "            random_state=random_state,\r\n"]
[226.366, "o", "            positive=positive_dict,\r\n"]
[226.376, "o", "        )\r\n"]
[226.386, "o", "\r\n"]
[226.396, "o", "        # Maybe we need a stopping criteria based on the amount of\r\n"]
[226.406, "o", "        # modification in the dictionary\r\n"]
[226.416, "o", "        if callback is not None:\r\n"]
[226.426, "o", "            callback(locals())\r\n"]
[226.436, "o", "\r\n"]
[226.446, "o", "    if return_inner_stats:\r\n"]
[226.456, "o", "        if return_n_iter:\r\n"]
[226.466, "o", "            return dictionary, (A, B), ii - iter_offset + 1\r\n"]
[226.476, "o", "        else:\r\n"]
[226.486, "o", "            return dictionary, (A, B)\r\n"]
[226.496, "o", "    if return_code:\r\n"]
[226.506, "o", "        if verbose > 1:\r\n"]
[226.516, "o", "            print(\"Learning code...\", end=\" \")\r\n"]
[226.526, "o", "        elif verbose == 1:\r\n"]
[226.536, "o", "            print(\"|\", end=\" \")\r\n"]
[226.546, "o", "        code = sparse_encode(\r\n"]
[226.556, "o", "            X,\r\n"]
[226.566, "o", "            dictionary,\r\n"]
[226.576, "o", "            algorithm=method,\r\n"]
[226.586, "o", "            alpha=alpha,\r\n"]
[226.596, "o", "            n_jobs=n_jobs,\r\n"]
[226.606, "o", "            check_input=False,\r\n"]
[226.616, "o", "            positive=positive_code,\r\n"]
[226.626, "o", "            max_iter=method_max_iter,\r\n"]
[226.636, "o", "            verbose=verbose,\r\n"]
[226.646, "o", "        )\r\n"]
[226.656, "o", "        if verbose > 1:\r\n"]
[226.666, "o", "            dt = time.time() - t0\r\n"]
[226.676, "o", "            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n"]
[226.686, "o", "        if return_n_iter:\r\n"]
[226.696, "o", "            return code, dictionary, ii - iter_offset + 1\r\n"]
[226.706, "o", "        else:\r\n"]
[226.716, "o", "            return code, dictionary\r\n"]
[226.726, "o", "\r\n"]
[226.736, "o", "    if return_n_iter:\r\n"]
[226.746, "o", "        return dictionary, ii - iter_offset + 1\r\n"]
[226.756, "o", "    else:\r\n"]
[226.766, "o", "        return dictionary\r\n"]
[226.776, "o", "\r\n"]
[226.786, "o", "\r\n"]
[226.796, "o", "@validate_params(\r\n"]
[226.806, "o", "    {\r\n"]
[226.816, "o", "        \"X\": [\"array-like\"],\r\n"]
[226.826, "o", "        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[226.836, "o", "        \"return_n_iter\": [\"boolean\"],\r\n"]
[226.846, "o", "        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[226.856, "o", "    },\r\n"]
[226.866, "o", "    prefer_skip_nested_validation=False,\r\n"]
[226.876, "o", ")\r\n"]
[226.886, "o", "def dict_learning(\r\n"]
[226.896, "o", "    X,\r\n"]
[226.906, "o", "    n_components,\r\n"]
[226.916, "o", "    *,\r\n"]
[226.926, "o", "    alpha,\r\n"]
[226.936, "o", "    max_iter=100,\r\n"]
[226.946, "o", "    tol=1e-8,\r\n"]
[226.956, "o", "    method=\"lars\",\r\n"]
[226.966, "o", "    n_jobs=None,\r\n"]
[226.976, "o", "    dict_init=None,\r\n"]
[226.986, "o", "    code_init=None,\r\n"]
[226.996, "o", "    callback=None,\r\n"]
[227.006, "o", "    verbose=False,\r\n"]
[227.016, "o", "    random_state=None,\r\n"]
[227.026, "o", "    return_n_iter=False,\r\n"]
[227.036, "o", "    positive_dict=False,\r\n"]
[227.046, "o", "    positive_code=False,\r\n"]
[227.056, "o", "    method_max_iter=1000,\r\n"]
[227.066, "o", "):\r\n"]
[227.076, "o", "    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n"]
[227.086, "o", "\r\n"]
[227.096, "o", "    Finds the best dictionary and the corresponding sparse code for\r\n"]
[227.106, "o", "    approximating the data matrix X by solving::\r\n"]
[227.116, "o", "\r\n"]
[227.126, "o", "        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[227.136, "o", "                     (U,V)\r\n"]
[227.146, "o", "                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n"]
[227.156, "o", "\r\n"]
[227.166, "o", "    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n"]
[227.176, "o", "    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n"]
[227.186, "o", "    which is the sum of the absolute values of all the entries in the matrix.\r\n"]
[227.196, "o", "\r\n"]
[227.206, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[227.216, "o", "\r\n"]
[227.226, "o", "    Parameters\r\n"]
[227.236, "o", "    ----------\r\n"]
[227.246, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[227.256, "o", "        Data matrix.\r\n"]
[227.266, "o", "\r\n"]
[227.276, "o", "    n_components : int\r\n"]
[227.286, "o", "        Number of dictionary atoms to extract.\r\n"]
[227.296, "o", "\r\n"]
[227.306, "o", "    alpha : int or float\r\n"]
[227.316, "o", "        Sparsity controlling parameter.\r\n"]
[227.326, "o", "\r\n"]
[227.336, "o", "    max_iter : int, default=100\r\n"]
[227.346, "o", "        Maximum number of iterations to perform.\r\n"]
[227.356, "o", "\r\n"]
[227.366, "o", "    tol : float, default=1e-8\r\n"]
[227.376, "o", "        Tolerance for the stopping condition.\r\n"]
[227.386, "o", "\r\n"]
[227.396, "o", "    method : {'lars', 'cd'}, default='lars'\r\n"]
[227.406, "o", "        The method used:\r\n"]
[227.416, "o", "\r\n"]
[227.426, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[227.436, "o", "           problem (`linear_model.lars_path`);\r\n"]
[227.446, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[227.456, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[227.466, "o", "          the estimated components are sparse.\r\n"]
[227.476, "o", "\r\n"]
[227.486, "o", "    n_jobs : int, default=None\r\n"]
[227.496, "o", "        Number of parallel jobs to run.\r\n"]
[227.506, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[227.516, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[227.526, "o", "        for more details.\r\n"]
[227.536, "o", "\r\n"]
[227.546, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[227.556, "o", "        Initial value for the dictionary for warm restart scenarios. Only used\r\n"]
[227.566, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[227.576, "o", "\r\n"]
[227.586, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[227.596, "o", "        Initial value for the sparse code for warm restart scenarios. Only used\r\n"]
[227.606, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[227.616, "o", "\r\n"]
[227.626, "o", "    callback : callable, default=None\r\n"]
[227.636, "o", "        Callable that gets invoked every five iterations.\r\n"]
[227.646, "o", "\r\n"]
[227.656, "o", "    verbose : bool, default=False\r\n"]
[227.666, "o", "        To control the verbosity of the procedure.\r\n"]
[227.676, "o", "\r\n"]
[227.686, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[227.696, "o", "        Used for randomly initializing the dictionary. Pass an int for\r\n"]
[227.706, "o", "        reproducible results across multiple function calls.\r\n"]
[227.716, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[227.726, "o", "\r\n"]
[227.736, "o", "    return_n_iter : bool, default=False\r\n"]
[227.746, "o", "        Whether or not to return the number of iterations.\r\n"]
[227.756, "o", "\r\n"]
[227.766, "o", "    positive_dict : bool, default=False\r\n"]
[227.776, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[227.786, "o", "\r\n"]
[227.796, "o", "        .. versionadded:: 0.20\r\n"]
[227.806, "o", "\r\n"]
[227.816, "o", "    positive_code : bool, default=False\r\n"]
[227.826, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[227.836, "o", "\r\n"]
[227.846, "o", "        .. versionadded:: 0.20\r\n"]
[227.856, "o", "\r\n"]
[227.866, "o", "    method_max_iter : int, default=1000\r\n"]
[227.876, "o", "        Maximum number of iterations to perform.\r\n"]
[227.886, "o", "\r\n"]
[227.896, "o", "        .. versionadded:: 0.22\r\n"]
[227.906, "o", "\r\n"]
[227.916, "o", "    Returns\r\n"]
[227.926, "o", "    -------\r\n"]
[227.936, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[227.946, "o", "        The sparse code factor in the matrix factorization.\r\n"]
[227.956, "o", "\r\n"]
[227.966, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[227.976, "o", "        The dictionary factor in the matrix factorization.\r\n"]
[227.986, "o", "\r\n"]
[227.996, "o", "    errors : array\r\n"]
[228.006, "o", "        Vector of errors at each iteration.\r\n"]
[228.016, "o", "\r\n"]
[228.026, "o", "    n_iter : int\r\n"]
[228.036, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[228.046, "o", "        set to True.\r\n"]
[228.056, "o", "\r\n"]
[228.066, "o", "    See Also\r\n"]
[228.076, "o", "    --------\r\n"]
[228.086, "o", "    dict_learning_online : Solve a dictionary learning matrix factorization\r\n"]
[228.096, "o", "        problem online.\r\n"]
[228.106, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[228.116, "o", "    MiniBatchDictionaryLearning : A faster, less accurate version\r\n"]
[228.126, "o", "        of the dictionary learning algorithm.\r\n"]
[228.136, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[228.146, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[228.156, "o", "    \"\"\"\r\n"]
[228.166, "o", "    estimator = DictionaryLearning(\r\n"]
[228.176, "o", "        n_components=n_components,\r\n"]
[228.186, "o", "        alpha=alpha,\r\n"]
[228.196, "o", "        max_iter=max_iter,\r\n"]
[228.206, "o", "        tol=tol,\r\n"]
[228.216, "o", "        fit_algorithm=method,\r\n"]
[228.226, "o", "        n_jobs=n_jobs,\r\n"]
[228.236, "o", "        dict_init=dict_init,\r\n"]
[228.246, "o", "        callback=callback,\r\n"]
[228.256, "o", "        code_init=code_init,\r\n"]
[228.266, "o", "        verbose=verbose,\r\n"]
[228.276, "o", "        random_state=random_state,\r\n"]
[228.286, "o", "        positive_code=positive_code,\r\n"]
[228.296, "o", "        positive_dict=positive_dict,\r\n"]
[228.306, "o", "        transform_max_iter=method_max_iter,\r\n"]
[228.316, "o", "    )\r\n"]
[228.326, "o", "    code = estimator.fit_transform(X)\r\n"]
[228.336, "o", "    if return_n_iter:\r\n"]
[228.346, "o", "        return (\r\n"]
[228.356, "o", "            code,\r\n"]
[228.366, "o", "            estimator.components_,\r\n"]
[228.376, "o", "            estimator.error_,\r\n"]
[228.386, "o", "            estimator.n_iter_,\r\n"]
[228.396, "o", "        )\r\n"]
[228.406, "o", "    return code, estimator.components_, estimator.error_\r\n"]
[228.416, "o", "\r\n"]
[228.426, "o", "\r\n"]
[228.436, "o", "class _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n"]
[228.446, "o", "    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n"]
[228.456, "o", "\r\n"]
[228.466, "o", "    def __init__(\r\n"]
[228.476, "o", "        self,\r\n"]
[228.486, "o", "        transform_algorithm,\r\n"]
[228.496, "o", "        transform_n_nonzero_coefs,\r\n"]
[228.506, "o", "        transform_alpha,\r\n"]
[228.516, "o", "        split_sign,\r\n"]
[228.526, "o", "        n_jobs,\r\n"]
[228.536, "o", "        positive_code,\r\n"]
[228.546, "o", "        transform_max_iter,\r\n"]
[228.556, "o", "    ):\r\n"]
[228.566, "o", "        self.transform_algorithm = transform_algorithm\r\n"]
[228.576, "o", "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n"]
[228.586, "o", "        self.transform_alpha = transform_alpha\r\n"]
[228.596, "o", "        self.transform_max_iter = transform_max_iter\r\n"]
[228.606, "o", "        self.split_sign = split_sign\r\n"]
[228.616, "o", "        self.n_jobs = n_jobs\r\n"]
[228.626, "o", "        self.positive_code = positive_code\r\n"]
[228.636, "o", "\r\n"]
[228.646, "o", "    def _transform(self, X, dictionary):\r\n"]
[228.656, "o", "        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n"]
[228.666, "o", "        SparseCoder.\"\"\"\r\n"]
[228.676, "o", "        X = self._validate_data(X, reset=False)\r\n"]
[228.686, "o", "\r\n"]
[228.696, "o", "        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n"]
[228.706, "o", "            transform_alpha = self.alpha\r\n"]
[228.716, "o", "        else:\r\n"]
[228.726, "o", "            transform_alpha = self.transform_alpha\r\n"]
[228.736, "o", "\r\n"]
[228.746, "o", "        code = sparse_encode(\r\n"]
[228.756, "o", "            X,\r\n"]
[228.766, "o", "            dictionary,\r\n"]
[228.776, "o", "            algorithm=self.transform_algorithm,\r\n"]
[228.786, "o", "            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n"]
[228.796, "o", "            alpha=transform_alpha,\r\n"]
[228.806, "o", "            max_iter=self.transform_max_iter,\r\n"]
[228.816, "o", "            n_jobs=self.n_jobs,\r\n"]
[228.826, "o", "            positive=self.positive_code,\r\n"]
[228.836, "o", "        )\r\n"]
[228.846, "o", "\r\n"]
[228.856, "o", "        if self.split_sign:\r\n"]
[228.866, "o", "            # feature vector is split into a positive and negative side\r\n"]
[228.876, "o", "            n_samples, n_features = code.shape\r\n"]
[228.886, "o", "            split_code = np.empty((n_samples, 2 * n_features))\r\n"]
[228.896, "o", "            split_code[:, :n_features] = np.maximum(code, 0)\r\n"]
[228.906, "o", "            split_code[:, n_features:] = -np.minimum(code, 0)\r\n"]
[228.916, "o", "            code = split_code\r\n"]
[228.926, "o", "\r\n"]
[228.936, "o", "        return code\r\n"]
[228.946, "o", "\r\n"]
[228.956, "o", "    def transform(self, X):\r\n"]
[228.966, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[228.976, "o", "\r\n"]
[228.986, "o", "        Coding method is determined by the object parameter\r\n"]
[228.996, "o", "        `transform_algorithm`.\r\n"]
[229.006, "o", "\r\n"]
[229.016, "o", "        Parameters\r\n"]
[229.026, "o", "        ----------\r\n"]
[229.036, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[229.046, "o", "            Test data to be transformed, must have the same number of\r\n"]
[229.056, "o", "            features as the data used to train the model.\r\n"]
[229.066, "o", "\r\n"]
[229.076, "o", "        Returns\r\n"]
[229.086, "o", "        -------\r\n"]
[229.096, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[229.106, "o", "            Transformed data.\r\n"]
[229.116, "o", "        \"\"\"\r\n"]
[229.126, "o", "        check_is_fitted(self)\r\n"]
[229.136, "o", "        return self._transform(X, self.components_)\r\n"]
[229.146, "o", "\r\n"]
[229.156, "o", "\r\n"]
[229.166, "o", "class SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n"]
[229.176, "o", "    \"\"\"Sparse coding.\r\n"]
[229.186, "o", "\r\n"]
[229.196, "o", "    Finds a sparse representation of data against a fixed, precomputed\r\n"]
[229.206, "o", "    dictionary.\r\n"]
[229.216, "o", "\r\n"]
[229.226, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[229.236, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[229.246, "o", "\r\n"]
[229.256, "o", "        X ~= code * dictionary\r\n"]
[229.266, "o", "\r\n"]
[229.276, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[229.286, "o", "\r\n"]
[229.296, "o", "    Parameters\r\n"]
[229.306, "o", "    ----------\r\n"]
[229.316, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[229.326, "o", "        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n"]
[229.336, "o", "        normalized to unit norm.\r\n"]
[229.346, "o", "\r\n"]
[229.356, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[229.366, "o", "            'threshold'}, default='omp'\r\n"]
[229.376, "o", "        Algorithm used to transform the data:\r\n"]
[229.386, "o", "\r\n"]
[229.396, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[229.406, "o", "          (`linear_model.lars_path`);\r\n"]
[229.416, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[229.426, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[229.436, "o", "          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n"]
[229.446, "o", "          the estimated components are sparse;\r\n"]
[229.456, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[229.466, "o", "          solution;\r\n"]
[229.476, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[229.486, "o", "          the projection ``dictionary * X'``.\r\n"]
[229.496, "o", "\r\n"]
[229.506, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[229.516, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[229.526, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[229.536, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[229.546, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[229.556, "o", "\r\n"]
[229.566, "o", "    transform_alpha : float, default=None\r\n"]
[229.576, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[229.586, "o", "        penalty applied to the L1 norm.\r\n"]
[229.596, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[229.606, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[229.616, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[229.626, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[229.636, "o", "        `n_nonzero_coefs`.\r\n"]
[229.646, "o", "        If `None`, default to 1.\r\n"]
[229.656, "o", "\r\n"]
[229.666, "o", "    split_sign : bool, default=False\r\n"]
[229.676, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[229.686, "o", "        its negative part and its positive part. This can improve the\r\n"]
[229.696, "o", "        performance of downstream classifiers.\r\n"]
[229.706, "o", "\r\n"]
[229.716, "o", "    n_jobs : int, default=None\r\n"]
[229.726, "o", "        Number of parallel jobs to run.\r\n"]
[229.736, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[229.746, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[229.756, "o", "        for more details.\r\n"]
[229.766, "o", "\r\n"]
[229.776, "o", "    positive_code : bool, default=False\r\n"]
[229.786, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[229.796, "o", "\r\n"]
[229.806, "o", "        .. versionadded:: 0.20\r\n"]
[229.816, "o", "\r\n"]
[229.826, "o", "    transform_max_iter : int, default=1000\r\n"]
[229.836, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[229.846, "o", "        `lasso_lars`.\r\n"]
[229.856, "o", "\r\n"]
[229.866, "o", "        .. versionadded:: 0.22\r\n"]
[229.876, "o", "\r\n"]
[229.886, "o", "    Attributes\r\n"]
[229.896, "o", "    ----------\r\n"]
[229.906, "o", "    n_components_ : int\r\n"]
[229.916, "o", "        Number of atoms.\r\n"]
[229.926, "o", "\r\n"]
[229.936, "o", "    n_features_in_ : int\r\n"]
[230.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[230.002, "i", "sed -n '1600,2200p' sklearn/decomposition/_dict_learning.py\r"]
[230.004, "o", "sed -n '1600,2200p' sklearn/decomposition/_dict_learning.py\r\n"]
[230.016, "o", "\u001b[?2004l\r\n"]
[230.026, "o", "\r\n"]
[230.036, "o", "    n_jobs : int or None, default=None\r\n"]
[230.046, "o", "        Number of parallel jobs to run.\r\n"]
[230.056, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[230.066, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[230.076, "o", "        for more details.\r\n"]
[230.086, "o", "\r\n"]
[230.096, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[230.106, "o", "        Initial value for the code, for warm restart. Only used if `code_init`\r\n"]
[230.116, "o", "        and `dict_init` are not None.\r\n"]
[230.126, "o", "\r\n"]
[230.136, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[230.146, "o", "        Initial values for the dictionary, for warm restart. Only used if\r\n"]
[230.156, "o", "        `code_init` and `dict_init` are not None.\r\n"]
[230.166, "o", "\r\n"]
[230.176, "o", "    callback : callable, default=None\r\n"]
[230.186, "o", "        Callable that gets invoked every five iterations.\r\n"]
[230.196, "o", "\r\n"]
[230.206, "o", "        .. versionadded:: 1.3\r\n"]
[230.216, "o", "\r\n"]
[230.226, "o", "    verbose : bool, default=False\r\n"]
[230.236, "o", "        To control the verbosity of the procedure.\r\n"]
[230.246, "o", "\r\n"]
[230.256, "o", "    split_sign : bool, default=False\r\n"]
[230.266, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[230.276, "o", "        its negative part and its positive part. This can improve the\r\n"]
[230.286, "o", "        performance of downstream classifiers.\r\n"]
[230.296, "o", "\r\n"]
[230.306, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[230.316, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[230.326, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[230.336, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[230.346, "o", "        results across multiple function calls.\r\n"]
[230.356, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[230.366, "o", "\r\n"]
[230.376, "o", "    positive_code : bool, default=False\r\n"]
[230.386, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[230.396, "o", "\r\n"]
[230.406, "o", "        .. versionadded:: 0.20\r\n"]
[230.416, "o", "\r\n"]
[230.426, "o", "    positive_dict : bool, default=False\r\n"]
[230.436, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[230.446, "o", "\r\n"]
[230.456, "o", "        .. versionadded:: 0.20\r\n"]
[230.466, "o", "\r\n"]
[230.476, "o", "    transform_max_iter : int, default=1000\r\n"]
[230.486, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[230.496, "o", "        `'lasso_lars'`.\r\n"]
[230.506, "o", "\r\n"]
[230.516, "o", "        .. versionadded:: 0.22\r\n"]
[230.526, "o", "\r\n"]
[230.536, "o", "    Attributes\r\n"]
[230.546, "o", "    ----------\r\n"]
[230.556, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[230.566, "o", "        dictionary atoms extracted from the data\r\n"]
[230.576, "o", "\r\n"]
[230.586, "o", "    error_ : array\r\n"]
[230.596, "o", "        vector of errors at each iteration\r\n"]
[230.606, "o", "\r\n"]
[230.616, "o", "    n_features_in_ : int\r\n"]
[230.626, "o", "        Number of features seen during :term:`fit`.\r\n"]
[230.636, "o", "\r\n"]
[230.646, "o", "        .. versionadded:: 0.24\r\n"]
[230.656, "o", "\r\n"]
[230.666, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[230.676, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[230.686, "o", "        has feature names that are all strings.\r\n"]
[230.696, "o", "\r\n"]
[230.706, "o", "        .. versionadded:: 1.0\r\n"]
[230.716, "o", "\r\n"]
[230.726, "o", "    n_iter_ : int\r\n"]
[230.736, "o", "        Number of iterations run.\r\n"]
[230.746, "o", "\r\n"]
[230.756, "o", "    See Also\r\n"]
[230.766, "o", "    --------\r\n"]
[230.776, "o", "    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n"]
[230.786, "o", "        dictionary learning algorithm.\r\n"]
[230.796, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[230.806, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[230.816, "o", "        precomputed dictionary.\r\n"]
[230.826, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[230.836, "o", "\r\n"]
[230.846, "o", "    References\r\n"]
[230.856, "o", "    ----------\r\n"]
[230.866, "o", "\r\n"]
[230.876, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[230.886, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[230.896, "o", "\r\n"]
[230.906, "o", "    Examples\r\n"]
[230.916, "o", "    --------\r\n"]
[230.926, "o", "    >>> import numpy as np\r\n"]
[230.936, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[230.946, "o", "    >>> from sklearn.decomposition import DictionaryLearning\r\n"]
[230.956, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[230.966, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[230.976, "o", "    ...     random_state=42,\r\n"]
[230.986, "o", "    ... )\r\n"]
[230.996, "o", "    >>> dict_learner = DictionaryLearning(\r\n"]
[231.006, "o", "    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n"]
[231.016, "o", "    ...     random_state=42,\r\n"]
[231.026, "o", "    ... )\r\n"]
[231.036, "o", "    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n"]
[231.046, "o", "\r\n"]
[231.056, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[231.066, "o", "\r\n"]
[231.076, "o", "    >>> np.mean(X_transformed == 0)\r\n"]
[231.086, "o", "    0.41...\r\n"]
[231.096, "o", "\r\n"]
[231.106, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[231.116, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[231.126, "o", "    the original signal:\r\n"]
[231.136, "o", "\r\n"]
[231.146, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[231.156, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[231.166, "o", "    0.07...\r\n"]
[231.176, "o", "    \"\"\"\r\n"]
[231.186, "o", "\r\n"]
[231.196, "o", "    _parameter_constraints: dict = {\r\n"]
[231.206, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[231.216, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[231.226, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[231.236, "o", "        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[231.246, "o", "        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[231.256, "o", "        \"transform_algorithm\": [\r\n"]
[231.266, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[231.276, "o", "        ],\r\n"]
[231.286, "o", "        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[231.296, "o", "        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[231.306, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[231.316, "o", "        \"code_init\": [np.ndarray, None],\r\n"]
[231.326, "o", "        \"dict_init\": [np.ndarray, None],\r\n"]
[231.336, "o", "        \"callback\": [callable, None],\r\n"]
[231.346, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[231.356, "o", "        \"split_sign\": [\"boolean\"],\r\n"]
[231.366, "o", "        \"random_state\": [\"random_state\"],\r\n"]
[231.376, "o", "        \"positive_code\": [\"boolean\"],\r\n"]
[231.386, "o", "        \"positive_dict\": [\"boolean\"],\r\n"]
[231.396, "o", "        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[231.406, "o", "    }\r\n"]
[231.416, "o", "\r\n"]
[231.426, "o", "    def __init__(\r\n"]
[231.436, "o", "        self,\r\n"]
[231.446, "o", "        n_components=None,\r\n"]
[231.456, "o", "        *,\r\n"]
[231.466, "o", "        alpha=1,\r\n"]
[231.476, "o", "        max_iter=1000,\r\n"]
[231.486, "o", "        tol=1e-8,\r\n"]
[231.496, "o", "        fit_algorithm=\"lars\",\r\n"]
[231.506, "o", "        transform_algorithm=\"omp\",\r\n"]
[231.516, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[231.526, "o", "        transform_alpha=None,\r\n"]
[231.536, "o", "        n_jobs=None,\r\n"]
[231.546, "o", "        code_init=None,\r\n"]
[231.556, "o", "        dict_init=None,\r\n"]
[231.566, "o", "        callback=None,\r\n"]
[231.576, "o", "        verbose=False,\r\n"]
[231.586, "o", "        split_sign=False,\r\n"]
[231.596, "o", "        random_state=None,\r\n"]
[231.606, "o", "        positive_code=False,\r\n"]
[231.616, "o", "        positive_dict=False,\r\n"]
[231.626, "o", "        transform_max_iter=1000,\r\n"]
[231.636, "o", "    ):\r\n"]
[231.646, "o", "        super().__init__(\r\n"]
[231.656, "o", "            transform_algorithm,\r\n"]
[231.666, "o", "            transform_n_nonzero_coefs,\r\n"]
[231.676, "o", "            transform_alpha,\r\n"]
[231.686, "o", "            split_sign,\r\n"]
[231.696, "o", "            n_jobs,\r\n"]
[231.706, "o", "            positive_code,\r\n"]
[231.716, "o", "            transform_max_iter,\r\n"]
[231.726, "o", "        )\r\n"]
[231.736, "o", "        self.n_components = n_components\r\n"]
[231.746, "o", "        self.alpha = alpha\r\n"]
[231.756, "o", "        self.max_iter = max_iter\r\n"]
[231.766, "o", "        self.tol = tol\r\n"]
[231.776, "o", "        self.fit_algorithm = fit_algorithm\r\n"]
[231.786, "o", "        self.code_init = code_init\r\n"]
[231.796, "o", "        self.dict_init = dict_init\r\n"]
[231.806, "o", "        self.callback = callback\r\n"]
[231.816, "o", "        self.verbose = verbose\r\n"]
[231.826, "o", "        self.random_state = random_state\r\n"]
[231.836, "o", "        self.positive_dict = positive_dict\r\n"]
[231.846, "o", "\r\n"]
[231.856, "o", "    def fit(self, X, y=None):\r\n"]
[231.866, "o", "        \"\"\"Fit the model from data in X.\r\n"]
[231.876, "o", "\r\n"]
[231.886, "o", "        Parameters\r\n"]
[231.896, "o", "        ----------\r\n"]
[231.906, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[231.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[231.926, "o", "            and `n_features` is the number of features.\r\n"]
[231.936, "o", "\r\n"]
[231.946, "o", "        y : Ignored\r\n"]
[231.956, "o", "            Not used, present for API consistency by convention.\r\n"]
[231.966, "o", "\r\n"]
[231.976, "o", "        Returns\r\n"]
[231.986, "o", "        -------\r\n"]
[231.996, "o", "        self : object\r\n"]
[232.006, "o", "            Returns the instance itself.\r\n"]
[232.016, "o", "        \"\"\"\r\n"]
[232.026, "o", "        self.fit_transform(X)\r\n"]
[232.036, "o", "        return self\r\n"]
[232.046, "o", "\r\n"]
[232.056, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[232.066, "o", "    def fit_transform(self, X, y=None):\r\n"]
[232.076, "o", "        \"\"\"Fit the model from data in X and return the transformed data.\r\n"]
[232.086, "o", "\r\n"]
[232.096, "o", "        Parameters\r\n"]
[232.106, "o", "        ----------\r\n"]
[232.116, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[232.126, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[232.136, "o", "            and `n_features` is the number of features.\r\n"]
[232.146, "o", "\r\n"]
[232.156, "o", "        y : Ignored\r\n"]
[232.166, "o", "            Not used, present for API consistency by convention.\r\n"]
[232.176, "o", "\r\n"]
[232.186, "o", "        Returns\r\n"]
[232.196, "o", "        -------\r\n"]
[232.206, "o", "        V : ndarray of shape (n_samples, n_components)\r\n"]
[232.216, "o", "            Transformed data.\r\n"]
[232.226, "o", "        \"\"\"\r\n"]
[232.236, "o", "        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n"]
[232.246, "o", "\r\n"]
[232.256, "o", "        method = \"lasso_\" + self.fit_algorithm\r\n"]
[232.266, "o", "\r\n"]
[232.276, "o", "        random_state = check_random_state(self.random_state)\r\n"]
[232.286, "o", "        X = self._validate_data(X)\r\n"]
[232.296, "o", "\r\n"]
[232.306, "o", "        if self.n_components is None:\r\n"]
[232.316, "o", "            n_components = X.shape[1]\r\n"]
[232.326, "o", "        else:\r\n"]
[232.336, "o", "            n_components = self.n_components\r\n"]
[232.346, "o", "\r\n"]
[232.356, "o", "        V, U, E, self.n_iter_ = _dict_learning(\r\n"]
[232.366, "o", "            X,\r\n"]
[232.376, "o", "            n_components,\r\n"]
[232.386, "o", "            alpha=self.alpha,\r\n"]
[232.396, "o", "            tol=self.tol,\r\n"]
[232.406, "o", "            max_iter=self.max_iter,\r\n"]
[232.416, "o", "            method=method,\r\n"]
[232.426, "o", "            method_max_iter=self.transform_max_iter,\r\n"]
[232.436, "o", "            n_jobs=self.n_jobs,\r\n"]
[232.446, "o", "            code_init=self.code_init,\r\n"]
[232.456, "o", "            dict_init=self.dict_init,\r\n"]
[232.466, "o", "            callback=self.callback,\r\n"]
[232.476, "o", "            verbose=self.verbose,\r\n"]
[232.486, "o", "            random_state=random_state,\r\n"]
[232.496, "o", "            return_n_iter=True,\r\n"]
[232.506, "o", "            positive_dict=self.positive_dict,\r\n"]
[232.516, "o", "            positive_code=self.positive_code,\r\n"]
[232.526, "o", "        )\r\n"]
[232.536, "o", "        self.components_ = U\r\n"]
[232.546, "o", "        self.error_ = E\r\n"]
[232.556, "o", "\r\n"]
[232.566, "o", "        return V\r\n"]
[232.576, "o", "\r\n"]
[232.586, "o", "    @property\r\n"]
[232.596, "o", "    def _n_features_out(self):\r\n"]
[232.606, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[232.616, "o", "        return self.components_.shape[0]\r\n"]
[232.626, "o", "\r\n"]
[232.636, "o", "    def _more_tags(self):\r\n"]
[232.646, "o", "        return {\r\n"]
[232.656, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[232.666, "o", "        }\r\n"]
[232.676, "o", "\r\n"]
[232.686, "o", "\r\n"]
[232.696, "o", "class MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[232.706, "o", "    \"\"\"Mini-batch dictionary learning.\r\n"]
[232.716, "o", "\r\n"]
[232.726, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[232.736, "o", "    encoding the fitted data.\r\n"]
[232.746, "o", "\r\n"]
[232.756, "o", "    Solves the optimization problem::\r\n"]
[232.766, "o", "\r\n"]
[232.776, "o", "       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[232.786, "o", "                    (U,V)\r\n"]
[232.796, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[232.806, "o", "\r\n"]
[232.816, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[232.826, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[232.836, "o", "    of all the entries in the matrix.\r\n"]
[232.846, "o", "\r\n"]
[232.856, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[232.866, "o", "\r\n"]
[232.876, "o", "    Parameters\r\n"]
[232.886, "o", "    ----------\r\n"]
[232.896, "o", "    n_components : int, default=None\r\n"]
[232.906, "o", "        Number of dictionary elements to extract.\r\n"]
[232.916, "o", "\r\n"]
[232.926, "o", "    alpha : float, default=1\r\n"]
[232.936, "o", "        Sparsity controlling parameter.\r\n"]
[232.946, "o", "\r\n"]
[232.956, "o", "    n_iter : int, default=1000\r\n"]
[232.966, "o", "        Total number of iterations over data batches to perform.\r\n"]
[232.976, "o", "\r\n"]
[232.986, "o", "        .. deprecated:: 1.1\r\n"]
[232.996, "o", "           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[233.006, "o", "           ``max_iter`` instead.\r\n"]
[233.016, "o", "\r\n"]
[233.026, "o", "    max_iter : int, default=None\r\n"]
[233.036, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[233.046, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[233.056, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[233.066, "o", "\r\n"]
[233.076, "o", "        .. versionadded:: 1.1\r\n"]
[233.086, "o", "\r\n"]
[233.096, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[233.106, "o", "        The algorithm used:\r\n"]
[233.116, "o", "\r\n"]
[233.126, "o", "        - `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[233.136, "o", "          problem (`linear_model.lars_path`)\r\n"]
[233.146, "o", "        - `'cd'`: uses the coordinate descent method to compute the\r\n"]
[233.156, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[233.166, "o", "          the estimated components are sparse.\r\n"]
[233.176, "o", "\r\n"]
[233.186, "o", "    n_jobs : int, default=None\r\n"]
[233.196, "o", "        Number of parallel jobs to run.\r\n"]
[233.206, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[233.216, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[233.226, "o", "        for more details.\r\n"]
[233.236, "o", "\r\n"]
[233.246, "o", "    batch_size : int, default=256\r\n"]
[233.256, "o", "        Number of samples in each mini-batch.\r\n"]
[233.266, "o", "\r\n"]
[233.276, "o", "        .. versionchanged:: 1.3\r\n"]
[233.286, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[233.296, "o", "\r\n"]
[233.306, "o", "    shuffle : bool, default=True\r\n"]
[233.316, "o", "        Whether to shuffle the samples before forming batches.\r\n"]
[233.326, "o", "\r\n"]
[233.336, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[233.346, "o", "        Initial value of the dictionary for warm restart scenarios.\r\n"]
[233.356, "o", "\r\n"]
[233.366, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[233.376, "o", "            'threshold'}, default='omp'\r\n"]
[233.386, "o", "        Algorithm used to transform the data:\r\n"]
[233.396, "o", "\r\n"]
[233.406, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[233.416, "o", "          (`linear_model.lars_path`);\r\n"]
[233.426, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[233.436, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[233.446, "o", "          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n"]
[233.456, "o", "          if the estimated components are sparse.\r\n"]
[233.466, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[233.476, "o", "          solution.\r\n"]
[233.486, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[233.496, "o", "          the projection ``dictionary * X'``.\r\n"]
[233.506, "o", "\r\n"]
[233.516, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[233.526, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[233.536, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[233.546, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[233.556, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[233.566, "o", "\r\n"]
[233.576, "o", "    transform_alpha : float, default=None\r\n"]
[233.586, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[233.596, "o", "        penalty applied to the L1 norm.\r\n"]
[233.606, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[233.616, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[233.626, "o", "        If `None`, defaults to `alpha`.\r\n"]
[233.636, "o", "\r\n"]
[233.646, "o", "        .. versionchanged:: 1.2\r\n"]
[233.656, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[233.666, "o", "\r\n"]
[233.676, "o", "    verbose : bool or int, default=False\r\n"]
[233.686, "o", "        To control the verbosity of the procedure.\r\n"]
[233.696, "o", "\r\n"]
[233.706, "o", "    split_sign : bool, default=False\r\n"]
[233.716, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[233.726, "o", "        its negative part and its positive part. This can improve the\r\n"]
[233.736, "o", "        performance of downstream classifiers.\r\n"]
[233.746, "o", "\r\n"]
[233.756, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[233.766, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[233.776, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[233.786, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[233.796, "o", "        results across multiple function calls.\r\n"]
[233.806, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[233.816, "o", "\r\n"]
[233.826, "o", "    positive_code : bool, default=False\r\n"]
[233.836, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[233.846, "o", "\r\n"]
[233.856, "o", "        .. versionadded:: 0.20\r\n"]
[233.866, "o", "\r\n"]
[233.876, "o", "    positive_dict : bool, default=False\r\n"]
[233.886, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[233.896, "o", "\r\n"]
[233.906, "o", "        .. versionadded:: 0.20\r\n"]
[233.916, "o", "\r\n"]
[233.926, "o", "    transform_max_iter : int, default=1000\r\n"]
[233.936, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[233.946, "o", "        `'lasso_lars'`.\r\n"]
[233.956, "o", "\r\n"]
[233.966, "o", "        .. versionadded:: 0.22\r\n"]
[233.976, "o", "\r\n"]
[233.986, "o", "    callback : callable, default=None\r\n"]
[233.996, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[234.006, "o", "\r\n"]
[234.016, "o", "        .. versionadded:: 1.1\r\n"]
[234.026, "o", "\r\n"]
[234.036, "o", "    tol : float, default=1e-3\r\n"]
[234.046, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[234.056, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[234.066, "o", "\r\n"]
[234.076, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[234.086, "o", "        `tol` to 0.0.\r\n"]
[234.096, "o", "\r\n"]
[234.106, "o", "        .. versionadded:: 1.1\r\n"]
[234.116, "o", "\r\n"]
[234.126, "o", "    max_no_improvement : int, default=10\r\n"]
[234.136, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[234.146, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[234.156, "o", "        `max_iter` is not None.\r\n"]
[234.166, "o", "\r\n"]
[234.176, "o", "        To disable convergence detection based on cost function, set\r\n"]
[234.186, "o", "        `max_no_improvement` to None.\r\n"]
[234.196, "o", "\r\n"]
[234.206, "o", "        .. versionadded:: 1.1\r\n"]
[234.216, "o", "\r\n"]
[234.226, "o", "    Attributes\r\n"]
[234.236, "o", "    ----------\r\n"]
[234.246, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[234.256, "o", "        Components extracted from the data.\r\n"]
[234.266, "o", "\r\n"]
[234.276, "o", "    n_features_in_ : int\r\n"]
[234.286, "o", "        Number of features seen during :term:`fit`.\r\n"]
[234.296, "o", "\r\n"]
[234.306, "o", "        .. versionadded:: 0.24\r\n"]
[234.316, "o", "\r\n"]
[234.326, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[234.336, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[234.346, "o", "        has feature names that are all strings.\r\n"]
[234.356, "o", "\r\n"]
[234.366, "o", "        .. versionadded:: 1.0\r\n"]
[234.376, "o", "\r\n"]
[234.386, "o", "    n_iter_ : int\r\n"]
[234.396, "o", "        Number of iterations over the full dataset.\r\n"]
[234.406, "o", "\r\n"]
[234.416, "o", "    n_steps_ : int\r\n"]
[234.426, "o", "        Number of mini-batches processed.\r\n"]
[234.436, "o", "\r\n"]
[234.446, "o", "        .. versionadded:: 1.1\r\n"]
[234.456, "o", "\r\n"]
[234.466, "o", "    See Also\r\n"]
[234.476, "o", "    --------\r\n"]
[234.486, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[234.496, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[234.506, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[234.516, "o", "        precomputed dictionary.\r\n"]
[234.526, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[234.536, "o", "\r\n"]
[234.546, "o", "    References\r\n"]
[234.556, "o", "    ----------\r\n"]
[234.566, "o", "\r\n"]
[234.576, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[234.586, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[234.596, "o", "\r\n"]
[234.606, "o", "    Examples\r\n"]
[234.616, "o", "    --------\r\n"]
[234.626, "o", "    >>> import numpy as np\r\n"]
[234.636, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[234.646, "o", "    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n"]
[234.656, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[234.666, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[234.676, "o", "    ...     random_state=42)\r\n"]
[234.686, "o", "    >>> dict_learner = MiniBatchDictionaryLearning(\r\n"]
[234.696, "o", "    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n"]
[234.706, "o", "    ...     transform_alpha=0.1, random_state=42)\r\n"]
[234.716, "o", "    >>> X_transformed = dict_learner.fit_transform(X)\r\n"]
[234.726, "o", "\r\n"]
[234.736, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[234.746, "o", "\r\n"]
[234.756, "o", "    >>> np.mean(X_transformed == 0) < 0.5\r\n"]
[234.766, "o", "    True\r\n"]
[234.776, "o", "\r\n"]
[234.786, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[234.796, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[234.806, "o", "    the original signal:\r\n"]
[234.816, "o", "\r\n"]
[234.826, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[234.836, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[234.846, "o", "    0.057...\r\n"]
[234.856, "o", "    \"\"\"\r\n"]
[234.866, "o", "\r\n"]
[234.876, "o", "    _parameter_constraints: dict = {\r\n"]
[234.886, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[234.896, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[234.906, "o", "        \"n_iter\": [\r\n"]
[234.916, "o", "            Interval(Integral, 0, None, closed=\"left\"),\r\n"]
[234.926, "o", "            Hidden(StrOptions({\"deprecated\"})),\r\n"]
[234.936, "o", "        ],\r\n"]
[235.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[235.002, "i", "cd asv_benchmarks\r"]
[235.004, "o", "cd asv_benchmarks\r\n"]
[235.006, "o", "\u001b[?2004l\r\n"]
[240.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[240.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[240.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[241.654, "o", "\u001b[?2004l\r\n"]
[243.302, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[245.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[245.002, "i", "cd ..\r"]
[245.004, "o", "cd ..\r\n"]
[245.006, "o", "\u001b[?2004l\r\n"]
[250.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[250.002, "i", "grep -n \"def dict_learning_online\\|def _update_dict\\|def _mini_batch_dictionary_learning\\|class MiniBatchDictionary\r"]
[250.004, "o", "grep -n \"def dict_learning_online\\|def _update_dict\\|def _mini_batch_dictionary_learning\\|class MiniBatchDictionary\r\n"]
[250.9948, "o", "yLearning\" sklearn/decomposition/_dict_learning.py\r\n"]
[251.9836, "o", "\u001b[?2004l\r\n"]
[252.9724, "o", "\u001b[32m\u001b[K458\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kdef _update_dict\u001b[m\u001b[K(\r\n"]
[253.9612, "o", "\u001b[32m\u001b[K664\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kdef dict_learning_online\u001b[m\u001b[K(\r\n"]
[255.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[255.002, "i", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r"]
[255.004, "o", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r\n"]
[255.016, "o", "\u001b[?2004l\r\n"]
[255.026, "o", "    n_iter : int\r\n"]
[255.036, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[255.046, "o", "        set to True.\r\n"]
[255.056, "o", "\r\n"]
[255.066, "o", "    See Also\r\n"]
[255.076, "o", "    --------\r\n"]
[255.086, "o", "    dict_learning_online : Solve a dictionary learning matrix factorization\r\n"]
[255.096, "o", "        problem online.\r\n"]
[255.106, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[255.116, "o", "    MiniBatchDictionaryLearning : A faster, less accurate version\r\n"]
[255.126, "o", "        of the dictionary learning algorithm.\r\n"]
[255.136, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[255.146, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[255.156, "o", "    \"\"\"\r\n"]
[255.166, "o", "    estimator = DictionaryLearning(\r\n"]
[255.176, "o", "        n_components=n_components,\r\n"]
[255.186, "o", "        alpha=alpha,\r\n"]
[255.196, "o", "        max_iter=max_iter,\r\n"]
[255.206, "o", "        tol=tol,\r\n"]
[255.216, "o", "        fit_algorithm=method,\r\n"]
[255.226, "o", "        n_jobs=n_jobs,\r\n"]
[255.236, "o", "        dict_init=dict_init,\r\n"]
[255.246, "o", "        callback=callback,\r\n"]
[255.256, "o", "        code_init=code_init,\r\n"]
[255.266, "o", "        verbose=verbose,\r\n"]
[255.276, "o", "        random_state=random_state,\r\n"]
[255.286, "o", "        positive_code=positive_code,\r\n"]
[255.296, "o", "        positive_dict=positive_dict,\r\n"]
[255.306, "o", "        transform_max_iter=method_max_iter,\r\n"]
[255.316, "o", "    )\r\n"]
[255.326, "o", "    code = estimator.fit_transform(X)\r\n"]
[255.336, "o", "    if return_n_iter:\r\n"]
[255.346, "o", "        return (\r\n"]
[255.356, "o", "            code,\r\n"]
[255.366, "o", "            estimator.components_,\r\n"]
[255.376, "o", "            estimator.error_,\r\n"]
[255.386, "o", "            estimator.n_iter_,\r\n"]
[255.396, "o", "        )\r\n"]
[255.406, "o", "    return code, estimator.components_, estimator.error_\r\n"]
[255.416, "o", "\r\n"]
[255.426, "o", "\r\n"]
[255.436, "o", "class _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n"]
[255.446, "o", "    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n"]
[255.456, "o", "\r\n"]
[255.466, "o", "    def __init__(\r\n"]
[255.476, "o", "        self,\r\n"]
[255.486, "o", "        transform_algorithm,\r\n"]
[255.496, "o", "        transform_n_nonzero_coefs,\r\n"]
[255.506, "o", "        transform_alpha,\r\n"]
[255.516, "o", "        split_sign,\r\n"]
[255.526, "o", "        n_jobs,\r\n"]
[255.536, "o", "        positive_code,\r\n"]
[255.546, "o", "        transform_max_iter,\r\n"]
[255.556, "o", "    ):\r\n"]
[255.566, "o", "        self.transform_algorithm = transform_algorithm\r\n"]
[255.576, "o", "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n"]
[255.586, "o", "        self.transform_alpha = transform_alpha\r\n"]
[255.596, "o", "        self.transform_max_iter = transform_max_iter\r\n"]
[255.606, "o", "        self.split_sign = split_sign\r\n"]
[255.616, "o", "        self.n_jobs = n_jobs\r\n"]
[255.626, "o", "        self.positive_code = positive_code\r\n"]
[255.636, "o", "\r\n"]
[255.646, "o", "    def _transform(self, X, dictionary):\r\n"]
[255.656, "o", "        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n"]
[255.666, "o", "        SparseCoder.\"\"\"\r\n"]
[255.676, "o", "        X = self._validate_data(X, reset=False)\r\n"]
[255.686, "o", "\r\n"]
[255.696, "o", "        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n"]
[255.706, "o", "            transform_alpha = self.alpha\r\n"]
[255.716, "o", "        else:\r\n"]
[255.726, "o", "            transform_alpha = self.transform_alpha\r\n"]
[255.736, "o", "\r\n"]
[255.746, "o", "        code = sparse_encode(\r\n"]
[255.756, "o", "            X,\r\n"]
[255.766, "o", "            dictionary,\r\n"]
[255.776, "o", "            algorithm=self.transform_algorithm,\r\n"]
[255.786, "o", "            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n"]
[255.796, "o", "            alpha=transform_alpha,\r\n"]
[255.806, "o", "            max_iter=self.transform_max_iter,\r\n"]
[255.816, "o", "            n_jobs=self.n_jobs,\r\n"]
[255.826, "o", "            positive=self.positive_code,\r\n"]
[255.836, "o", "        )\r\n"]
[255.846, "o", "\r\n"]
[255.856, "o", "        if self.split_sign:\r\n"]
[255.866, "o", "            # feature vector is split into a positive and negative side\r\n"]
[255.876, "o", "            n_samples, n_features = code.shape\r\n"]
[255.886, "o", "            split_code = np.empty((n_samples, 2 * n_features))\r\n"]
[255.896, "o", "            split_code[:, :n_features] = np.maximum(code, 0)\r\n"]
[255.906, "o", "            split_code[:, n_features:] = -np.minimum(code, 0)\r\n"]
[255.916, "o", "            code = split_code\r\n"]
[255.926, "o", "\r\n"]
[255.936, "o", "        return code\r\n"]
[255.946, "o", "\r\n"]
[255.956, "o", "    def transform(self, X):\r\n"]
[255.966, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[255.976, "o", "\r\n"]
[255.986, "o", "        Coding method is determined by the object parameter\r\n"]
[255.996, "o", "        `transform_algorithm`.\r\n"]
[256.006, "o", "\r\n"]
[256.016, "o", "        Parameters\r\n"]
[256.026, "o", "        ----------\r\n"]
[256.036, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[256.046, "o", "            Test data to be transformed, must have the same number of\r\n"]
[256.056, "o", "            features as the data used to train the model.\r\n"]
[256.066, "o", "\r\n"]
[256.076, "o", "        Returns\r\n"]
[256.086, "o", "        -------\r\n"]
[256.096, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[256.106, "o", "            Transformed data.\r\n"]
[256.116, "o", "        \"\"\"\r\n"]
[256.126, "o", "        check_is_fitted(self)\r\n"]
[256.136, "o", "        return self._transform(X, self.components_)\r\n"]
[256.146, "o", "\r\n"]
[256.156, "o", "\r\n"]
[256.166, "o", "class SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n"]
[256.176, "o", "    \"\"\"Sparse coding.\r\n"]
[256.186, "o", "\r\n"]
[256.196, "o", "    Finds a sparse representation of data against a fixed, precomputed\r\n"]
[256.206, "o", "    dictionary.\r\n"]
[256.216, "o", "\r\n"]
[256.226, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[256.236, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[256.246, "o", "\r\n"]
[256.256, "o", "        X ~= code * dictionary\r\n"]
[256.266, "o", "\r\n"]
[256.276, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[256.286, "o", "\r\n"]
[256.296, "o", "    Parameters\r\n"]
[256.306, "o", "    ----------\r\n"]
[256.316, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[256.326, "o", "        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n"]
[256.336, "o", "        normalized to unit norm.\r\n"]
[256.346, "o", "\r\n"]
[256.356, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[256.366, "o", "            'threshold'}, default='omp'\r\n"]
[256.376, "o", "        Algorithm used to transform the data:\r\n"]
[256.386, "o", "\r\n"]
[256.396, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[256.406, "o", "          (`linear_model.lars_path`);\r\n"]
[256.416, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[256.426, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[256.436, "o", "          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n"]
[256.446, "o", "          the estimated components are sparse;\r\n"]
[256.456, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[256.466, "o", "          solution;\r\n"]
[256.476, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[256.486, "o", "          the projection ``dictionary * X'``.\r\n"]
[256.496, "o", "\r\n"]
[256.506, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[256.516, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[256.526, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[256.536, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[256.546, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[256.556, "o", "\r\n"]
[256.566, "o", "    transform_alpha : float, default=None\r\n"]
[256.576, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[256.586, "o", "        penalty applied to the L1 norm.\r\n"]
[256.596, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[256.606, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[256.616, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[256.626, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[256.636, "o", "        `n_nonzero_coefs`.\r\n"]
[256.646, "o", "        If `None`, default to 1.\r\n"]
[256.656, "o", "\r\n"]
[256.666, "o", "    split_sign : bool, default=False\r\n"]
[256.676, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[256.686, "o", "        its negative part and its positive part. This can improve the\r\n"]
[256.696, "o", "        performance of downstream classifiers.\r\n"]
[256.706, "o", "\r\n"]
[256.716, "o", "    n_jobs : int, default=None\r\n"]
[256.726, "o", "        Number of parallel jobs to run.\r\n"]
[256.736, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[256.746, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[256.756, "o", "        for more details.\r\n"]
[256.766, "o", "\r\n"]
[256.776, "o", "    positive_code : bool, default=False\r\n"]
[256.786, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[256.796, "o", "\r\n"]
[256.806, "o", "        .. versionadded:: 0.20\r\n"]
[256.816, "o", "\r\n"]
[256.826, "o", "    transform_max_iter : int, default=1000\r\n"]
[256.836, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[256.846, "o", "        `lasso_lars`.\r\n"]
[256.856, "o", "\r\n"]
[256.866, "o", "        .. versionadded:: 0.22\r\n"]
[256.876, "o", "\r\n"]
[256.886, "o", "    Attributes\r\n"]
[256.896, "o", "    ----------\r\n"]
[256.906, "o", "    n_components_ : int\r\n"]
[256.916, "o", "        Number of atoms.\r\n"]
[256.926, "o", "\r\n"]
[256.936, "o", "    n_features_in_ : int\r\n"]
[256.946, "o", "        Number of features seen during :term:`fit`.\r\n"]
[256.956, "o", "\r\n"]
[256.966, "o", "        .. versionadded:: 0.24\r\n"]
[256.976, "o", "\r\n"]
[256.986, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[256.996, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[257.006, "o", "        has feature names that are all strings.\r\n"]
[257.016, "o", "\r\n"]
[257.026, "o", "        .. versionadded:: 1.0\r\n"]
[257.036, "o", "\r\n"]
[257.046, "o", "    See Also\r\n"]
[257.056, "o", "    --------\r\n"]
[257.066, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[257.076, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n"]
[257.086, "o", "        dictionary learning algorithm.\r\n"]
[257.096, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[257.106, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[257.116, "o", "    sparse_encode : Sparse coding where each row of the result is the solution\r\n"]
[257.126, "o", "        to a sparse coding problem.\r\n"]
[257.136, "o", "\r\n"]
[257.146, "o", "    Examples\r\n"]
[257.156, "o", "    --------\r\n"]
[257.166, "o", "    >>> import numpy as np\r\n"]
[257.176, "o", "    >>> from sklearn.decomposition import SparseCoder\r\n"]
[257.186, "o", "    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n"]
[257.196, "o", "    >>> dictionary = np.array(\r\n"]
[257.206, "o", "    ...     [[0, 1, 0],\r\n"]
[257.216, "o", "    ...      [-1, -1, 2],\r\n"]
[257.226, "o", "    ...      [1, 1, 1],\r\n"]
[257.236, "o", "    ...      [0, 1, 1],\r\n"]
[257.246, "o", "    ...      [0, 2, 1]],\r\n"]
[257.256, "o", "    ...    dtype=np.float64\r\n"]
[257.266, "o", "    ... )\r\n"]
[257.276, "o", "    >>> coder = SparseCoder(\r\n"]
[257.286, "o", "    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n"]
[257.296, "o", "    ...     transform_alpha=1e-10,\r\n"]
[257.306, "o", "    ... )\r\n"]
[257.316, "o", "    >>> coder.transform(X)\r\n"]
[257.326, "o", "    array([[ 0.,  0., -1.,  0.,  0.],\r\n"]
[257.336, "o", "           [ 0.,  1.,  1.,  0.,  0.]])\r\n"]
[257.346, "o", "    \"\"\"\r\n"]
[257.356, "o", "\r\n"]
[257.366, "o", "    _required_parameters = [\"dictionary\"]\r\n"]
[257.376, "o", "\r\n"]
[257.386, "o", "    def __init__(\r\n"]
[257.396, "o", "        self,\r\n"]
[257.406, "o", "        dictionary,\r\n"]
[257.416, "o", "        *,\r\n"]
[257.426, "o", "        transform_algorithm=\"omp\",\r\n"]
[257.436, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[257.446, "o", "        transform_alpha=None,\r\n"]
[257.456, "o", "        split_sign=False,\r\n"]
[257.466, "o", "        n_jobs=None,\r\n"]
[257.476, "o", "        positive_code=False,\r\n"]
[257.486, "o", "        transform_max_iter=1000,\r\n"]
[257.496, "o", "    ):\r\n"]
[257.506, "o", "        super().__init__(\r\n"]
[257.516, "o", "            transform_algorithm,\r\n"]
[257.526, "o", "            transform_n_nonzero_coefs,\r\n"]
[257.536, "o", "            transform_alpha,\r\n"]
[257.546, "o", "            split_sign,\r\n"]
[257.556, "o", "            n_jobs,\r\n"]
[257.566, "o", "            positive_code,\r\n"]
[257.576, "o", "            transform_max_iter,\r\n"]
[257.586, "o", "        )\r\n"]
[257.596, "o", "        self.dictionary = dictionary\r\n"]
[257.606, "o", "\r\n"]
[257.616, "o", "    def fit(self, X, y=None):\r\n"]
[257.626, "o", "        \"\"\"Do nothing and return the estimator unchanged.\r\n"]
[257.636, "o", "\r\n"]
[257.646, "o", "        This method is just there to implement the usual API and hence\r\n"]
[257.656, "o", "        work in pipelines.\r\n"]
[257.666, "o", "\r\n"]
[257.676, "o", "        Parameters\r\n"]
[257.686, "o", "        ----------\r\n"]
[257.696, "o", "        X : Ignored\r\n"]
[257.706, "o", "            Not used, present for API consistency by convention.\r\n"]
[257.716, "o", "\r\n"]
[257.726, "o", "        y : Ignored\r\n"]
[257.736, "o", "            Not used, present for API consistency by convention.\r\n"]
[257.746, "o", "\r\n"]
[257.756, "o", "        Returns\r\n"]
[257.766, "o", "        -------\r\n"]
[257.776, "o", "        self : object\r\n"]
[257.786, "o", "            Returns the instance itself.\r\n"]
[257.796, "o", "        \"\"\"\r\n"]
[257.806, "o", "        return self\r\n"]
[257.816, "o", "\r\n"]
[257.826, "o", "    def transform(self, X, y=None):\r\n"]
[257.836, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[257.846, "o", "\r\n"]
[257.856, "o", "        Coding method is determined by the object parameter\r\n"]
[257.866, "o", "        `transform_algorithm`.\r\n"]
[257.876, "o", "\r\n"]
[257.886, "o", "        Parameters\r\n"]
[257.896, "o", "        ----------\r\n"]
[257.906, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[257.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[257.926, "o", "            and `n_features` is the number of features.\r\n"]
[257.936, "o", "\r\n"]
[257.946, "o", "        y : Ignored\r\n"]
[257.956, "o", "            Not used, present for API consistency by convention.\r\n"]
[257.966, "o", "\r\n"]
[257.976, "o", "        Returns\r\n"]
[257.986, "o", "        -------\r\n"]
[257.996, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[258.006, "o", "            Transformed data.\r\n"]
[258.016, "o", "        \"\"\"\r\n"]
[258.026, "o", "        return super()._transform(X, self.dictionary)\r\n"]
[258.036, "o", "\r\n"]
[258.046, "o", "    def _more_tags(self):\r\n"]
[258.056, "o", "        return {\r\n"]
[258.066, "o", "            \"requires_fit\": False,\r\n"]
[258.076, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[258.086, "o", "        }\r\n"]
[258.096, "o", "\r\n"]
[258.106, "o", "    @property\r\n"]
[258.116, "o", "    def n_components_(self):\r\n"]
[258.126, "o", "        \"\"\"Number of atoms.\"\"\"\r\n"]
[258.136, "o", "        return self.dictionary.shape[0]\r\n"]
[258.146, "o", "\r\n"]
[258.156, "o", "    @property\r\n"]
[258.166, "o", "    def n_features_in_(self):\r\n"]
[258.176, "o", "        \"\"\"Number of features seen during `fit`.\"\"\"\r\n"]
[258.186, "o", "        return self.dictionary.shape[1]\r\n"]
[258.196, "o", "\r\n"]
[258.206, "o", "    @property\r\n"]
[258.216, "o", "    def _n_features_out(self):\r\n"]
[258.226, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[258.236, "o", "        return self.n_components_\r\n"]
[258.246, "o", "\r\n"]
[258.256, "o", "\r\n"]
[258.266, "o", "class DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[258.276, "o", "    \"\"\"Dictionary learning.\r\n"]
[258.286, "o", "\r\n"]
[258.296, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[258.306, "o", "    encoding the fitted data.\r\n"]
[258.316, "o", "\r\n"]
[258.326, "o", "    Solves the optimization problem::\r\n"]
[258.336, "o", "\r\n"]
[258.346, "o", "        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[258.356, "o", "                    (U,V)\r\n"]
[258.366, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[258.376, "o", "\r\n"]
[258.386, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[258.396, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[258.406, "o", "    of all the entries in the matrix.\r\n"]
[258.416, "o", "\r\n"]
[258.426, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[258.436, "o", "\r\n"]
[258.446, "o", "    Parameters\r\n"]
[258.456, "o", "    ----------\r\n"]
[258.466, "o", "    n_components : int, default=None\r\n"]
[258.476, "o", "        Number of dictionary elements to extract. If None, then ``n_components``\r\n"]
[258.486, "o", "        is set to ``n_features``.\r\n"]
[258.496, "o", "\r\n"]
[258.506, "o", "    alpha : float, default=1.0\r\n"]
[258.516, "o", "        Sparsity controlling parameter.\r\n"]
[258.526, "o", "\r\n"]
[258.536, "o", "    max_iter : int, default=1000\r\n"]
[258.546, "o", "        Maximum number of iterations to perform.\r\n"]
[258.556, "o", "\r\n"]
[258.566, "o", "    tol : float, default=1e-8\r\n"]
[258.576, "o", "        Tolerance for numerical error.\r\n"]
[258.586, "o", "\r\n"]
[258.596, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[258.606, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[258.616, "o", "          problem (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[258.626, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[258.636, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n"]
[258.646, "o", "          faster if the estimated components are sparse.\r\n"]
[258.656, "o", "\r\n"]
[258.666, "o", "        .. versionadded:: 0.17\r\n"]
[258.676, "o", "           *cd* coordinate descent method to improve speed.\r\n"]
[258.686, "o", "\r\n"]
[258.696, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[258.706, "o", "            'threshold'}, default='omp'\r\n"]
[258.716, "o", "        Algorithm used to transform the data:\r\n"]
[258.726, "o", "\r\n"]
[258.736, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[258.746, "o", "          (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[258.756, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[258.766, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[258.776, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n"]
[258.786, "o", "          will be faster if the estimated components are sparse.\r\n"]
[258.796, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[258.806, "o", "          solution.\r\n"]
[258.816, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[258.826, "o", "          the projection ``dictionary * X'``.\r\n"]
[258.836, "o", "\r\n"]
[258.846, "o", "        .. versionadded:: 0.17\r\n"]
[258.856, "o", "           *lasso_cd* coordinate descent method to improve speed.\r\n"]
[258.866, "o", "\r\n"]
[258.876, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[258.886, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[258.896, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[258.906, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[258.916, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[258.926, "o", "\r\n"]
[258.936, "o", "    transform_alpha : float, default=None\r\n"]
[258.946, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[258.956, "o", "        penalty applied to the L1 norm.\r\n"]
[258.966, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[258.976, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[258.986, "o", "        If `None`, defaults to `alpha`.\r\n"]
[258.996, "o", "\r\n"]
[259.006, "o", "        .. versionchanged:: 1.2\r\n"]
[259.016, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[259.026, "o", "\r\n"]
[259.036, "o", "    n_jobs : int or None, default=None\r\n"]
[259.046, "o", "        Number of parallel jobs to run.\r\n"]
[259.056, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[259.066, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[259.076, "o", "        for more details.\r\n"]
[259.086, "o", "\r\n"]
[259.096, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[259.106, "o", "        Initial value for the code, for warm restart. Only used if `code_init`\r\n"]
[259.116, "o", "        and `dict_init` are not None.\r\n"]
[259.126, "o", "\r\n"]
[259.136, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[259.146, "o", "        Initial values for the dictionary, for warm restart. Only used if\r\n"]
[259.156, "o", "        `code_init` and `dict_init` are not None.\r\n"]
[259.166, "o", "\r\n"]
[259.176, "o", "    callback : callable, default=None\r\n"]
[259.186, "o", "        Callable that gets invoked every five iterations.\r\n"]
[259.196, "o", "\r\n"]
[259.206, "o", "        .. versionadded:: 1.3\r\n"]
[259.216, "o", "\r\n"]
[259.226, "o", "    verbose : bool, default=False\r\n"]
[259.236, "o", "        To control the verbosity of the procedure.\r\n"]
[259.246, "o", "\r\n"]
[259.256, "o", "    split_sign : bool, default=False\r\n"]
[259.266, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[259.276, "o", "        its negative part and its positive part. This can improve the\r\n"]
[259.286, "o", "        performance of downstream classifiers.\r\n"]
[259.296, "o", "\r\n"]
[259.306, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[259.316, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[259.326, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[259.336, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[259.346, "o", "        results across multiple function calls.\r\n"]
[259.356, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[259.366, "o", "\r\n"]
[259.376, "o", "    positive_code : bool, default=False\r\n"]
[259.386, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[259.396, "o", "\r\n"]
[259.406, "o", "        .. versionadded:: 0.20\r\n"]
[259.416, "o", "\r\n"]
[259.426, "o", "    positive_dict : bool, default=False\r\n"]
[259.436, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[259.446, "o", "\r\n"]
[259.456, "o", "        .. versionadded:: 0.20\r\n"]
[259.466, "o", "\r\n"]
[259.476, "o", "    transform_max_iter : int, default=1000\r\n"]
[259.486, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[259.496, "o", "        `'lasso_lars'`.\r\n"]
[259.506, "o", "\r\n"]
[259.516, "o", "        .. versionadded:: 0.22\r\n"]
[259.526, "o", "\r\n"]
[259.536, "o", "    Attributes\r\n"]
[259.546, "o", "    ----------\r\n"]
[259.556, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[259.566, "o", "        dictionary atoms extracted from the data\r\n"]
[259.576, "o", "\r\n"]
[259.586, "o", "    error_ : array\r\n"]
[259.596, "o", "        vector of errors at each iteration\r\n"]
[259.606, "o", "\r\n"]
[259.616, "o", "    n_features_in_ : int\r\n"]
[259.626, "o", "        Number of features seen during :term:`fit`.\r\n"]
[259.636, "o", "\r\n"]
[259.646, "o", "        .. versionadded:: 0.24\r\n"]
[259.656, "o", "\r\n"]
[259.666, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[259.676, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[259.686, "o", "        has feature names that are all strings.\r\n"]
[259.696, "o", "\r\n"]
[259.706, "o", "        .. versionadded:: 1.0\r\n"]
[259.716, "o", "\r\n"]
[259.726, "o", "    n_iter_ : int\r\n"]
[259.736, "o", "        Number of iterations run.\r\n"]
[259.746, "o", "\r\n"]
[259.756, "o", "    See Also\r\n"]
[259.766, "o", "    --------\r\n"]
[259.776, "o", "    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n"]
[259.786, "o", "        dictionary learning algorithm.\r\n"]
[259.796, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[259.806, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[259.816, "o", "        precomputed dictionary.\r\n"]
[259.826, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[259.836, "o", "\r\n"]
[259.846, "o", "    References\r\n"]
[259.856, "o", "    ----------\r\n"]
[259.866, "o", "\r\n"]
[259.876, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[259.886, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[259.896, "o", "\r\n"]
[259.906, "o", "    Examples\r\n"]
[259.916, "o", "    --------\r\n"]
[259.926, "o", "    >>> import numpy as np\r\n"]
[259.936, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[260.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[260.002, "i", "sed -n '2000,2600p' sklearn/decomposition/_dict_learning.py\r"]
[260.004, "o", "sed -n '2000,2600p' sklearn/decomposition/_dict_learning.py\r\n"]
[260.016, "o", "\u001b[?2004l\r\n"]
[260.026, "o", "\r\n"]
[260.036, "o", "    tol : float, default=1e-3\r\n"]
[260.046, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[260.056, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[260.066, "o", "\r\n"]
[260.076, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[260.086, "o", "        `tol` to 0.0.\r\n"]
[260.096, "o", "\r\n"]
[260.106, "o", "        .. versionadded:: 1.1\r\n"]
[260.116, "o", "\r\n"]
[260.126, "o", "    max_no_improvement : int, default=10\r\n"]
[260.136, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[260.146, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[260.156, "o", "        `max_iter` is not None.\r\n"]
[260.166, "o", "\r\n"]
[260.176, "o", "        To disable convergence detection based on cost function, set\r\n"]
[260.186, "o", "        `max_no_improvement` to None.\r\n"]
[260.196, "o", "\r\n"]
[260.206, "o", "        .. versionadded:: 1.1\r\n"]
[260.216, "o", "\r\n"]
[260.226, "o", "    Attributes\r\n"]
[260.236, "o", "    ----------\r\n"]
[260.246, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[260.256, "o", "        Components extracted from the data.\r\n"]
[260.266, "o", "\r\n"]
[260.276, "o", "    n_features_in_ : int\r\n"]
[260.286, "o", "        Number of features seen during :term:`fit`.\r\n"]
[260.296, "o", "\r\n"]
[260.306, "o", "        .. versionadded:: 0.24\r\n"]
[260.316, "o", "\r\n"]
[260.326, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[260.336, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[260.346, "o", "        has feature names that are all strings.\r\n"]
[260.356, "o", "\r\n"]
[260.366, "o", "        .. versionadded:: 1.0\r\n"]
[260.376, "o", "\r\n"]
[260.386, "o", "    n_iter_ : int\r\n"]
[260.396, "o", "        Number of iterations over the full dataset.\r\n"]
[260.406, "o", "\r\n"]
[260.416, "o", "    n_steps_ : int\r\n"]
[260.426, "o", "        Number of mini-batches processed.\r\n"]
[260.436, "o", "\r\n"]
[260.446, "o", "        .. versionadded:: 1.1\r\n"]
[260.456, "o", "\r\n"]
[260.466, "o", "    See Also\r\n"]
[260.476, "o", "    --------\r\n"]
[260.486, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[260.496, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[260.506, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[260.516, "o", "        precomputed dictionary.\r\n"]
[260.526, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[260.536, "o", "\r\n"]
[260.546, "o", "    References\r\n"]
[260.556, "o", "    ----------\r\n"]
[260.566, "o", "\r\n"]
[260.576, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[260.586, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[260.596, "o", "\r\n"]
[260.606, "o", "    Examples\r\n"]
[260.616, "o", "    --------\r\n"]
[260.626, "o", "    >>> import numpy as np\r\n"]
[260.636, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[260.646, "o", "    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n"]
[260.656, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[260.666, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[260.676, "o", "    ...     random_state=42)\r\n"]
[260.686, "o", "    >>> dict_learner = MiniBatchDictionaryLearning(\r\n"]
[260.696, "o", "    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n"]
[260.706, "o", "    ...     transform_alpha=0.1, random_state=42)\r\n"]
[260.716, "o", "    >>> X_transformed = dict_learner.fit_transform(X)\r\n"]
[260.726, "o", "\r\n"]
[260.736, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[260.746, "o", "\r\n"]
[260.756, "o", "    >>> np.mean(X_transformed == 0) < 0.5\r\n"]
[260.766, "o", "    True\r\n"]
[260.776, "o", "\r\n"]
[260.786, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[260.796, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[260.806, "o", "    the original signal:\r\n"]
[260.816, "o", "\r\n"]
[260.826, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[260.836, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[260.846, "o", "    0.057...\r\n"]
[260.856, "o", "    \"\"\"\r\n"]
[260.866, "o", "\r\n"]
[260.876, "o", "    _parameter_constraints: dict = {\r\n"]
[260.886, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[260.896, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[260.906, "o", "        \"n_iter\": [\r\n"]
[260.916, "o", "            Interval(Integral, 0, None, closed=\"left\"),\r\n"]
[260.926, "o", "            Hidden(StrOptions({\"deprecated\"})),\r\n"]
[260.936, "o", "        ],\r\n"]
[260.946, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n"]
[260.956, "o", "        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n"]
[260.966, "o", "        \"n_jobs\": [None, Integral],\r\n"]
[260.976, "o", "        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n"]
[260.986, "o", "        \"shuffle\": [\"boolean\"],\r\n"]
[260.996, "o", "        \"dict_init\": [None, np.ndarray],\r\n"]
[261.006, "o", "        \"transform_algorithm\": [\r\n"]
[261.016, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[261.026, "o", "        ],\r\n"]
[261.036, "o", "        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[261.046, "o", "        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[261.056, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[261.066, "o", "        \"split_sign\": [\"boolean\"],\r\n"]
[261.076, "o", "        \"random_state\": [\"random_state\"],\r\n"]
[261.086, "o", "        \"positive_code\": [\"boolean\"],\r\n"]
[261.096, "o", "        \"positive_dict\": [\"boolean\"],\r\n"]
[261.106, "o", "        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[261.116, "o", "        \"callback\": [None, callable],\r\n"]
[261.126, "o", "        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[261.136, "o", "        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n"]
[261.146, "o", "    }\r\n"]
[261.156, "o", "\r\n"]
[261.166, "o", "    def __init__(\r\n"]
[261.176, "o", "        self,\r\n"]
[261.186, "o", "        n_components=None,\r\n"]
[261.196, "o", "        *,\r\n"]
[261.206, "o", "        alpha=1,\r\n"]
[261.216, "o", "        n_iter=\"deprecated\",\r\n"]
[261.226, "o", "        max_iter=None,\r\n"]
[261.236, "o", "        fit_algorithm=\"lars\",\r\n"]
[261.246, "o", "        n_jobs=None,\r\n"]
[261.256, "o", "        batch_size=256,\r\n"]
[261.266, "o", "        shuffle=True,\r\n"]
[261.276, "o", "        dict_init=None,\r\n"]
[261.286, "o", "        transform_algorithm=\"omp\",\r\n"]
[261.296, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[261.306, "o", "        transform_alpha=None,\r\n"]
[261.316, "o", "        verbose=False,\r\n"]
[261.326, "o", "        split_sign=False,\r\n"]
[261.336, "o", "        random_state=None,\r\n"]
[261.346, "o", "        positive_code=False,\r\n"]
[261.356, "o", "        positive_dict=False,\r\n"]
[261.366, "o", "        transform_max_iter=1000,\r\n"]
[261.376, "o", "        callback=None,\r\n"]
[261.386, "o", "        tol=1e-3,\r\n"]
[261.396, "o", "        max_no_improvement=10,\r\n"]
[261.406, "o", "    ):\r\n"]
[261.416, "o", "        super().__init__(\r\n"]
[261.426, "o", "            transform_algorithm,\r\n"]
[261.436, "o", "            transform_n_nonzero_coefs,\r\n"]
[261.446, "o", "            transform_alpha,\r\n"]
[261.456, "o", "            split_sign,\r\n"]
[261.466, "o", "            n_jobs,\r\n"]
[261.476, "o", "            positive_code,\r\n"]
[261.486, "o", "            transform_max_iter,\r\n"]
[261.496, "o", "        )\r\n"]
[261.506, "o", "        self.n_components = n_components\r\n"]
[261.516, "o", "        self.alpha = alpha\r\n"]
[261.526, "o", "        self.n_iter = n_iter\r\n"]
[261.536, "o", "        self.max_iter = max_iter\r\n"]
[261.546, "o", "        self.fit_algorithm = fit_algorithm\r\n"]
[261.556, "o", "        self.dict_init = dict_init\r\n"]
[261.566, "o", "        self.verbose = verbose\r\n"]
[261.576, "o", "        self.shuffle = shuffle\r\n"]
[261.586, "o", "        self.batch_size = batch_size\r\n"]
[261.596, "o", "        self.split_sign = split_sign\r\n"]
[261.606, "o", "        self.random_state = random_state\r\n"]
[261.616, "o", "        self.positive_dict = positive_dict\r\n"]
[261.626, "o", "        self.callback = callback\r\n"]
[261.636, "o", "        self.max_no_improvement = max_no_improvement\r\n"]
[261.646, "o", "        self.tol = tol\r\n"]
[261.656, "o", "\r\n"]
[261.666, "o", "    def _check_params(self, X):\r\n"]
[261.676, "o", "        # n_components\r\n"]
[261.686, "o", "        self._n_components = self.n_components\r\n"]
[261.696, "o", "        if self._n_components is None:\r\n"]
[261.706, "o", "            self._n_components = X.shape[1]\r\n"]
[261.716, "o", "\r\n"]
[261.726, "o", "        # fit_algorithm\r\n"]
[261.736, "o", "        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n"]
[261.746, "o", "        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n"]
[261.756, "o", "\r\n"]
[261.766, "o", "        # batch_size\r\n"]
[261.776, "o", "        self._batch_size = min(self.batch_size, X.shape[0])\r\n"]
[261.786, "o", "\r\n"]
[261.796, "o", "    def _initialize_dict(self, X, random_state):\r\n"]
[261.806, "o", "        \"\"\"Initialization of the dictionary.\"\"\"\r\n"]
[261.816, "o", "        if self.dict_init is not None:\r\n"]
[261.826, "o", "            dictionary = self.dict_init\r\n"]
[261.836, "o", "        else:\r\n"]
[261.846, "o", "            # Init V with SVD of X\r\n"]
[261.856, "o", "            _, S, dictionary = randomized_svd(\r\n"]
[261.866, "o", "                X, self._n_components, random_state=random_state\r\n"]
[261.876, "o", "            )\r\n"]
[261.886, "o", "            dictionary = S[:, np.newaxis] * dictionary\r\n"]
[261.896, "o", "\r\n"]
[261.906, "o", "        if self._n_components <= len(dictionary):\r\n"]
[261.916, "o", "            dictionary = dictionary[: self._n_components, :]\r\n"]
[261.926, "o", "        else:\r\n"]
[261.936, "o", "            dictionary = np.concatenate(\r\n"]
[261.946, "o", "                (\r\n"]
[261.956, "o", "                    dictionary,\r\n"]
[261.966, "o", "                    np.zeros(\r\n"]
[261.976, "o", "                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n"]
[261.986, "o", "                        dtype=dictionary.dtype,\r\n"]
[261.996, "o", "                    ),\r\n"]
[262.006, "o", "                )\r\n"]
[262.016, "o", "            )\r\n"]
[262.026, "o", "\r\n"]
[262.036, "o", "        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n"]
[262.046, "o", "        dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[262.056, "o", "\r\n"]
[262.066, "o", "        return dictionary\r\n"]
[262.076, "o", "\r\n"]
[262.086, "o", "    def _update_inner_stats(self, X, code, batch_size, step):\r\n"]
[262.096, "o", "        \"\"\"Update the inner stats inplace.\"\"\"\r\n"]
[262.106, "o", "        if step < batch_size - 1:\r\n"]
[262.116, "o", "            theta = (step + 1) * batch_size\r\n"]
[262.126, "o", "        else:\r\n"]
[262.136, "o", "            theta = batch_size**2 + step + 1 - batch_size\r\n"]
[262.146, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[262.156, "o", "\r\n"]
[262.166, "o", "        self._A *= beta\r\n"]
[262.176, "o", "        self._A += code.T @ code / batch_size\r\n"]
[262.186, "o", "        self._B *= beta\r\n"]
[262.196, "o", "        self._B += X.T @ code / batch_size\r\n"]
[262.206, "o", "\r\n"]
[262.216, "o", "    def _minibatch_step(self, X, dictionary, random_state, step):\r\n"]
[262.226, "o", "        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n"]
[262.236, "o", "        batch_size = X.shape[0]\r\n"]
[262.246, "o", "\r\n"]
[262.256, "o", "        # Compute code for this batch\r\n"]
[262.266, "o", "        code = _sparse_encode(\r\n"]
[262.276, "o", "            X,\r\n"]
[262.286, "o", "            dictionary,\r\n"]
[262.296, "o", "            algorithm=self._fit_algorithm,\r\n"]
[262.306, "o", "            alpha=self.alpha,\r\n"]
[262.316, "o", "            n_jobs=self.n_jobs,\r\n"]
[262.326, "o", "            positive=self.positive_code,\r\n"]
[262.336, "o", "            max_iter=self.transform_max_iter,\r\n"]
[262.346, "o", "            verbose=self.verbose,\r\n"]
[262.356, "o", "        )\r\n"]
[262.366, "o", "\r\n"]
[262.376, "o", "        batch_cost = (\r\n"]
[262.386, "o", "            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n"]
[262.396, "o", "            + self.alpha * np.sum(np.abs(code))\r\n"]
[262.406, "o", "        ) / batch_size\r\n"]
[262.416, "o", "\r\n"]
[262.426, "o", "        # Update inner stats\r\n"]
[262.436, "o", "        self._update_inner_stats(X, code, batch_size, step)\r\n"]
[262.446, "o", "\r\n"]
[262.456, "o", "        # Update dictionary\r\n"]
[262.466, "o", "        _update_dict(\r\n"]
[262.476, "o", "            dictionary,\r\n"]
[262.486, "o", "            X,\r\n"]
[262.496, "o", "            code,\r\n"]
[262.506, "o", "            self._A,\r\n"]
[262.516, "o", "            self._B,\r\n"]
[262.526, "o", "            verbose=self.verbose,\r\n"]
[262.536, "o", "            random_state=random_state,\r\n"]
[262.546, "o", "            positive=self.positive_dict,\r\n"]
[262.556, "o", "        )\r\n"]
[262.566, "o", "\r\n"]
[262.576, "o", "        return batch_cost\r\n"]
[262.586, "o", "\r\n"]
[262.596, "o", "    def _check_convergence(\r\n"]
[262.606, "o", "        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n"]
[262.616, "o", "    ):\r\n"]
[262.626, "o", "        \"\"\"Helper function to encapsulate the early stopping logic.\r\n"]
[262.636, "o", "\r\n"]
[262.646, "o", "        Early stopping is based on two factors:\r\n"]
[262.656, "o", "        - A small change of the dictionary between two minibatch updates. This is\r\n"]
[262.666, "o", "          controlled by the tol parameter.\r\n"]
[262.676, "o", "        - No more improvement on a smoothed estimate of the objective function for a\r\n"]
[262.686, "o", "          a certain number of consecutive minibatch updates. This is controlled by\r\n"]
[262.696, "o", "          the max_no_improvement parameter.\r\n"]
[262.706, "o", "        \"\"\"\r\n"]
[262.716, "o", "        batch_size = X.shape[0]\r\n"]
[262.726, "o", "\r\n"]
[262.736, "o", "        # counts steps starting from 1 for user friendly verbose mode.\r\n"]
[262.746, "o", "        step = step + 1\r\n"]
[262.756, "o", "\r\n"]
[262.766, "o", "        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n"]
[262.776, "o", "        # too bad value\r\n"]
[262.786, "o", "        if step <= min(100, n_samples / batch_size):\r\n"]
[262.796, "o", "            if self.verbose:\r\n"]
[262.806, "o", "                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n"]
[262.816, "o", "            return False\r\n"]
[262.826, "o", "\r\n"]
[262.836, "o", "        # Compute an Exponentially Weighted Average of the cost function to\r\n"]
[262.846, "o", "        # monitor the convergence while discarding minibatch-local stochastic\r\n"]
[262.856, "o", "        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n"]
[262.866, "o", "        if self._ewa_cost is None:\r\n"]
[262.876, "o", "            self._ewa_cost = batch_cost\r\n"]
[262.886, "o", "        else:\r\n"]
[262.896, "o", "            alpha = batch_size / (n_samples + 1)\r\n"]
[262.906, "o", "            alpha = min(alpha, 1)\r\n"]
[262.916, "o", "            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n"]
[262.926, "o", "\r\n"]
[262.936, "o", "        if self.verbose:\r\n"]
[262.946, "o", "            print(\r\n"]
[262.956, "o", "                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\r\n"]
[262.966, "o", "                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\r\n"]
[262.976, "o", "            )\r\n"]
[262.986, "o", "\r\n"]
[262.996, "o", "        # Early stopping based on change of dictionary\r\n"]
[263.006, "o", "        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\r\n"]
[263.016, "o", "        if self.tol > 0 and dict_diff <= self.tol:\r\n"]
[263.026, "o", "            if self.verbose:\r\n"]
[263.036, "o", "                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\r\n"]
[263.046, "o", "            return True\r\n"]
[263.056, "o", "\r\n"]
[263.066, "o", "        # Early stopping heuristic due to lack of improvement on smoothed\r\n"]
[263.076, "o", "        # cost function\r\n"]
[263.086, "o", "        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\r\n"]
[263.096, "o", "            self._no_improvement = 0\r\n"]
[263.106, "o", "            self._ewa_cost_min = self._ewa_cost\r\n"]
[263.116, "o", "        else:\r\n"]
[263.126, "o", "            self._no_improvement += 1\r\n"]
[263.136, "o", "\r\n"]
[263.146, "o", "        if (\r\n"]
[263.156, "o", "            self.max_no_improvement is not None\r\n"]
[263.166, "o", "            and self._no_improvement >= self.max_no_improvement\r\n"]
[263.176, "o", "        ):\r\n"]
[263.186, "o", "            if self.verbose:\r\n"]
[263.196, "o", "                print(\r\n"]
[263.206, "o", "                    \"Converged (lack of improvement in objective function) \"\r\n"]
[263.216, "o", "                    f\"at step {step}/{n_steps}\"\r\n"]
[263.226, "o", "                )\r\n"]
[263.236, "o", "            return True\r\n"]
[263.246, "o", "\r\n"]
[263.256, "o", "        return False\r\n"]
[263.266, "o", "\r\n"]
[263.276, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[263.286, "o", "    def fit(self, X, y=None):\r\n"]
[263.296, "o", "        \"\"\"Fit the model from data in X.\r\n"]
[263.306, "o", "\r\n"]
[263.316, "o", "        Parameters\r\n"]
[263.326, "o", "        ----------\r\n"]
[263.336, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[263.346, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[263.356, "o", "            and `n_features` is the number of features.\r\n"]
[263.366, "o", "\r\n"]
[263.376, "o", "        y : Ignored\r\n"]
[263.386, "o", "            Not used, present for API consistency by convention.\r\n"]
[263.396, "o", "\r\n"]
[263.406, "o", "        Returns\r\n"]
[263.416, "o", "        -------\r\n"]
[263.426, "o", "        self : object\r\n"]
[263.436, "o", "            Returns the instance itself.\r\n"]
[263.446, "o", "        \"\"\"\r\n"]
[263.456, "o", "        X = self._validate_data(\r\n"]
[263.466, "o", "            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n"]
[263.476, "o", "        )\r\n"]
[263.486, "o", "\r\n"]
[263.496, "o", "        self._check_params(X)\r\n"]
[263.506, "o", "\r\n"]
[263.516, "o", "        if self.n_iter != \"deprecated\":\r\n"]
[263.526, "o", "            warnings.warn(\r\n"]
[263.536, "o", "                (\r\n"]
[263.546, "o", "                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n"]
[263.556, "o", "                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n"]
[263.566, "o", "                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n"]
[263.576, "o", "                    \"specified.\"\r\n"]
[263.586, "o", "                ),\r\n"]
[263.596, "o", "                FutureWarning,\r\n"]
[263.606, "o", "            )\r\n"]
[263.616, "o", "            n_iter = self.n_iter\r\n"]
[263.626, "o", "\r\n"]
[263.636, "o", "        self._random_state = check_random_state(self.random_state)\r\n"]
[263.646, "o", "\r\n"]
[263.656, "o", "        dictionary = self._initialize_dict(X, self._random_state)\r\n"]
[263.666, "o", "        old_dict = dictionary.copy()\r\n"]
[263.676, "o", "\r\n"]
[263.686, "o", "        if self.shuffle:\r\n"]
[263.696, "o", "            X_train = X.copy()\r\n"]
[263.706, "o", "            self._random_state.shuffle(X_train)\r\n"]
[263.716, "o", "        else:\r\n"]
[263.726, "o", "            X_train = X\r\n"]
[263.736, "o", "\r\n"]
[263.746, "o", "        n_samples, n_features = X_train.shape\r\n"]
[263.756, "o", "\r\n"]
[263.766, "o", "        if self.verbose:\r\n"]
[263.776, "o", "            print(\"[dict_learning]\")\r\n"]
[263.786, "o", "\r\n"]
[263.796, "o", "        # Inner stats\r\n"]
[263.806, "o", "        self._A = np.zeros(\r\n"]
[263.816, "o", "            (self._n_components, self._n_components), dtype=X_train.dtype\r\n"]
[263.826, "o", "        )\r\n"]
[263.836, "o", "        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n"]
[263.846, "o", "\r\n"]
[263.856, "o", "        if self.max_iter is not None:\r\n"]
[263.866, "o", "            # Attributes to monitor the convergence\r\n"]
[263.876, "o", "            self._ewa_cost = None\r\n"]
[263.886, "o", "            self._ewa_cost_min = None\r\n"]
[263.896, "o", "            self._no_improvement = 0\r\n"]
[263.906, "o", "\r\n"]
[263.916, "o", "            batches = gen_batches(n_samples, self._batch_size)\r\n"]
[263.926, "o", "            batches = itertools.cycle(batches)\r\n"]
[263.936, "o", "            n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\r\n"]
[263.946, "o", "            n_steps = self.max_iter * n_steps_per_iter\r\n"]
[263.956, "o", "\r\n"]
[263.966, "o", "            i = -1  # to allow max_iter = 0\r\n"]
[263.976, "o", "\r\n"]
[263.986, "o", "            for i, batch in zip(range(n_steps), batches):\r\n"]
[263.996, "o", "                X_batch = X_train[batch]\r\n"]
[264.006, "o", "\r\n"]
[264.016, "o", "                batch_cost = self._minibatch_step(\r\n"]
[264.026, "o", "                    X_batch, dictionary, self._random_state, i\r\n"]
[264.036, "o", "                )\r\n"]
[264.046, "o", "\r\n"]
[264.056, "o", "                if self._check_convergence(\r\n"]
[264.066, "o", "                    X_batch, batch_cost, dictionary, old_dict, n_samples, i, n_steps\r\n"]
[264.076, "o", "                ):\r\n"]
[264.086, "o", "                    break\r\n"]
[264.096, "o", "\r\n"]
[264.106, "o", "                # XXX callback param added for backward compat in #18975 but a common\r\n"]
[264.116, "o", "                # unified callback API should be preferred\r\n"]
[264.126, "o", "                if self.callback is not None:\r\n"]
[264.136, "o", "                    self.callback(locals())\r\n"]
[264.146, "o", "\r\n"]
[264.156, "o", "                old_dict[:] = dictionary\r\n"]
[264.166, "o", "\r\n"]
[264.176, "o", "            self.n_steps_ = i + 1\r\n"]
[264.186, "o", "            self.n_iter_ = np.ceil(self.n_steps_ / n_steps_per_iter)\r\n"]
[264.196, "o", "        else:\r\n"]
[264.206, "o", "            # TODO remove this branch in 1.4\r\n"]
[264.216, "o", "            n_iter = 1000 if self.n_iter == \"deprecated\" else self.n_iter\r\n"]
[264.226, "o", "\r\n"]
[264.236, "o", "            batches = gen_batches(n_samples, self._batch_size)\r\n"]
[264.246, "o", "            batches = itertools.cycle(batches)\r\n"]
[264.256, "o", "\r\n"]
[264.266, "o", "            for i, batch in zip(range(n_iter), batches):\r\n"]
[264.276, "o", "                self._minibatch_step(X_train[batch], dictionary, self._random_state, i)\r\n"]
[264.286, "o", "\r\n"]
[264.296, "o", "                trigger_verbose = self.verbose and i % ceil(100.0 / self.verbose) == 0\r\n"]
[264.306, "o", "                if self.verbose > 10 or trigger_verbose:\r\n"]
[264.316, "o", "                    print(f\"{i} batches processed.\")\r\n"]
[264.326, "o", "\r\n"]
[264.336, "o", "                if self.callback is not None:\r\n"]
[264.346, "o", "                    self.callback(locals())\r\n"]
[264.356, "o", "\r\n"]
[264.366, "o", "            self.n_steps_ = n_iter\r\n"]
[264.376, "o", "            self.n_iter_ = np.ceil(n_iter / int(np.ceil(n_samples / self._batch_size)))\r\n"]
[264.386, "o", "\r\n"]
[264.396, "o", "        self.components_ = dictionary\r\n"]
[264.406, "o", "\r\n"]
[264.416, "o", "        return self\r\n"]
[264.426, "o", "\r\n"]
[264.436, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[264.446, "o", "    def partial_fit(self, X, y=None):\r\n"]
[264.456, "o", "        \"\"\"Update the model using the data in X as a mini-batch.\r\n"]
[264.466, "o", "\r\n"]
[264.476, "o", "        Parameters\r\n"]
[264.486, "o", "        ----------\r\n"]
[264.496, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[264.506, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[264.516, "o", "            and `n_features` is the number of features.\r\n"]
[264.526, "o", "\r\n"]
[264.536, "o", "        y : Ignored\r\n"]
[264.546, "o", "            Not used, present for API consistency by convention.\r\n"]
[264.556, "o", "\r\n"]
[264.566, "o", "        Returns\r\n"]
[264.576, "o", "        -------\r\n"]
[264.586, "o", "        self : object\r\n"]
[264.596, "o", "            Return the instance itself.\r\n"]
[264.606, "o", "        \"\"\"\r\n"]
[264.616, "o", "        has_components = hasattr(self, \"components_\")\r\n"]
[264.626, "o", "\r\n"]
[264.636, "o", "        X = self._validate_data(\r\n"]
[264.646, "o", "            X, dtype=[np.float64, np.float32], order=\"C\", reset=not has_components\r\n"]
[264.656, "o", "        )\r\n"]
[264.666, "o", "\r\n"]
[264.676, "o", "        if not has_components:\r\n"]
[264.686, "o", "            # This instance has not been fitted yet (fit or partial_fit)\r\n"]
[264.696, "o", "            self._check_params(X)\r\n"]
[264.706, "o", "            self._random_state = check_random_state(self.random_state)\r\n"]
[264.716, "o", "\r\n"]
[264.726, "o", "            dictionary = self._initialize_dict(X, self._random_state)\r\n"]
[264.736, "o", "\r\n"]
[264.746, "o", "            self.n_steps_ = 0\r\n"]
[264.756, "o", "\r\n"]
[264.766, "o", "            self._A = np.zeros((self._n_components, self._n_components), dtype=X.dtype)\r\n"]
[264.776, "o", "            self._B = np.zeros((X.shape[1], self._n_components), dtype=X.dtype)\r\n"]
[264.786, "o", "        else:\r\n"]
[264.796, "o", "            dictionary = self.components_\r\n"]
[264.806, "o", "\r\n"]
[264.816, "o", "        self._minibatch_step(X, dictionary, self._random_state, self.n_steps_)\r\n"]
[264.826, "o", "\r\n"]
[264.836, "o", "        self.components_ = dictionary\r\n"]
[264.846, "o", "        self.n_steps_ += 1\r\n"]
[264.856, "o", "\r\n"]
[264.866, "o", "        return self\r\n"]
[264.876, "o", "\r\n"]
[264.886, "o", "    @property\r\n"]
[264.896, "o", "    def _n_features_out(self):\r\n"]
[264.906, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[264.916, "o", "        return self.components_.shape[0]\r\n"]
[264.926, "o", "\r\n"]
[264.936, "o", "    def _more_tags(self):\r\n"]
[265.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[265.002, "i", "cd asv_benchmarks\r"]
[265.004, "o", "cd asv_benchmarks\r\n"]
[265.006, "o", "\u001b[?2004l\r\n"]
[270.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[270.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[270.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[271.654, "o", "\u001b[?2004l\r\n"]
[273.302, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[275.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[275.002, "i", "cd ..\r"]
[275.004, "o", "cd ..\r\n"]
[275.006, "o", "\u001b[?2004l\r\n"]
[280.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[280.002, "i", "grep -n \"def _minibatch_step\\|def dict_learning_online\\|def _mini_batch_dictionary_learning\\|class MiniBatchDiction\r"]
[280.004, "o", "grep -n \"def _minibatch_step\\|def dict_learning_online\\|def _mini_batch_dictionary_learning\\|class MiniBatchDiction\r\n"]
[280.9948, "o", "naryLearning\" sklearn/decomposition/_dict_learning.py\r\n"]
[281.9836, "o", "\u001b[?2004l\r\n"]
[282.9724, "o", "\u001b[32m\u001b[K664\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kdef dict_learning_online\u001b[m\u001b[K(\r\n"]
[283.9612, "o", "\u001b[32m\u001b[K1867\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kclass MiniBatchDictionaryLearning\u001b[m\u001b[K(_BaseSparseCoding, BaseEstimator):\r\n"]
[285.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[285.002, "i", "sed -n '1100,1800p' sklearn/decomposition/_dict_learning.py\r"]
[285.004, "o", "sed -n '1100,1800p' sklearn/decomposition/_dict_learning.py\r\n"]
[285.016, "o", "\u001b[?2004l\r\n"]
[285.026, "o", "    return_n_iter=False,\r\n"]
[285.036, "o", "    positive_dict=False,\r\n"]
[285.046, "o", "    positive_code=False,\r\n"]
[285.056, "o", "    method_max_iter=1000,\r\n"]
[285.066, "o", "):\r\n"]
[285.076, "o", "    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n"]
[285.086, "o", "\r\n"]
[285.096, "o", "    Finds the best dictionary and the corresponding sparse code for\r\n"]
[285.106, "o", "    approximating the data matrix X by solving::\r\n"]
[285.116, "o", "\r\n"]
[285.126, "o", "        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[285.136, "o", "                     (U,V)\r\n"]
[285.146, "o", "                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n"]
[285.156, "o", "\r\n"]
[285.166, "o", "    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n"]
[285.176, "o", "    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n"]
[285.186, "o", "    which is the sum of the absolute values of all the entries in the matrix.\r\n"]
[285.196, "o", "\r\n"]
[285.206, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[285.216, "o", "\r\n"]
[285.226, "o", "    Parameters\r\n"]
[285.236, "o", "    ----------\r\n"]
[285.246, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[285.256, "o", "        Data matrix.\r\n"]
[285.266, "o", "\r\n"]
[285.276, "o", "    n_components : int\r\n"]
[285.286, "o", "        Number of dictionary atoms to extract.\r\n"]
[285.296, "o", "\r\n"]
[285.306, "o", "    alpha : int or float\r\n"]
[285.316, "o", "        Sparsity controlling parameter.\r\n"]
[285.326, "o", "\r\n"]
[285.336, "o", "    max_iter : int, default=100\r\n"]
[285.346, "o", "        Maximum number of iterations to perform.\r\n"]
[285.356, "o", "\r\n"]
[285.366, "o", "    tol : float, default=1e-8\r\n"]
[285.376, "o", "        Tolerance for the stopping condition.\r\n"]
[285.386, "o", "\r\n"]
[285.396, "o", "    method : {'lars', 'cd'}, default='lars'\r\n"]
[285.406, "o", "        The method used:\r\n"]
[285.416, "o", "\r\n"]
[285.426, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[285.436, "o", "           problem (`linear_model.lars_path`);\r\n"]
[285.446, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[285.456, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[285.466, "o", "          the estimated components are sparse.\r\n"]
[285.476, "o", "\r\n"]
[285.486, "o", "    n_jobs : int, default=None\r\n"]
[285.496, "o", "        Number of parallel jobs to run.\r\n"]
[285.506, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[285.516, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[285.526, "o", "        for more details.\r\n"]
[285.536, "o", "\r\n"]
[285.546, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[285.556, "o", "        Initial value for the dictionary for warm restart scenarios. Only used\r\n"]
[285.566, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[285.576, "o", "\r\n"]
[285.586, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[285.596, "o", "        Initial value for the sparse code for warm restart scenarios. Only used\r\n"]
[285.606, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[285.616, "o", "\r\n"]
[285.626, "o", "    callback : callable, default=None\r\n"]
[285.636, "o", "        Callable that gets invoked every five iterations.\r\n"]
[285.646, "o", "\r\n"]
[285.656, "o", "    verbose : bool, default=False\r\n"]
[285.666, "o", "        To control the verbosity of the procedure.\r\n"]
[285.676, "o", "\r\n"]
[285.686, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[285.696, "o", "        Used for randomly initializing the dictionary. Pass an int for\r\n"]
[285.706, "o", "        reproducible results across multiple function calls.\r\n"]
[285.716, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[285.726, "o", "\r\n"]
[285.736, "o", "    return_n_iter : bool, default=False\r\n"]
[285.746, "o", "        Whether or not to return the number of iterations.\r\n"]
[285.756, "o", "\r\n"]
[285.766, "o", "    positive_dict : bool, default=False\r\n"]
[285.776, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[285.786, "o", "\r\n"]
[285.796, "o", "        .. versionadded:: 0.20\r\n"]
[285.806, "o", "\r\n"]
[285.816, "o", "    positive_code : bool, default=False\r\n"]
[285.826, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[285.836, "o", "\r\n"]
[285.846, "o", "        .. versionadded:: 0.20\r\n"]
[285.856, "o", "\r\n"]
[285.866, "o", "    method_max_iter : int, default=1000\r\n"]
[285.876, "o", "        Maximum number of iterations to perform.\r\n"]
[285.886, "o", "\r\n"]
[285.896, "o", "        .. versionadded:: 0.22\r\n"]
[285.906, "o", "\r\n"]
[285.916, "o", "    Returns\r\n"]
[285.926, "o", "    -------\r\n"]
[285.936, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[285.946, "o", "        The sparse code factor in the matrix factorization.\r\n"]
[285.956, "o", "\r\n"]
[285.966, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[285.976, "o", "        The dictionary factor in the matrix factorization.\r\n"]
[285.986, "o", "\r\n"]
[285.996, "o", "    errors : array\r\n"]
[286.006, "o", "        Vector of errors at each iteration.\r\n"]
[286.016, "o", "\r\n"]
[286.026, "o", "    n_iter : int\r\n"]
[286.036, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[286.046, "o", "        set to True.\r\n"]
[286.056, "o", "\r\n"]
[286.066, "o", "    See Also\r\n"]
[286.076, "o", "    --------\r\n"]
[286.086, "o", "    dict_learning_online : Solve a dictionary learning matrix factorization\r\n"]
[286.096, "o", "        problem online.\r\n"]
[286.106, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[286.116, "o", "    MiniBatchDictionaryLearning : A faster, less accurate version\r\n"]
[286.126, "o", "        of the dictionary learning algorithm.\r\n"]
[286.136, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[286.146, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[286.156, "o", "    \"\"\"\r\n"]
[286.166, "o", "    estimator = DictionaryLearning(\r\n"]
[286.176, "o", "        n_components=n_components,\r\n"]
[286.186, "o", "        alpha=alpha,\r\n"]
[286.196, "o", "        max_iter=max_iter,\r\n"]
[286.206, "o", "        tol=tol,\r\n"]
[286.216, "o", "        fit_algorithm=method,\r\n"]
[286.226, "o", "        n_jobs=n_jobs,\r\n"]
[286.236, "o", "        dict_init=dict_init,\r\n"]
[286.246, "o", "        callback=callback,\r\n"]
[286.256, "o", "        code_init=code_init,\r\n"]
[286.266, "o", "        verbose=verbose,\r\n"]
[286.276, "o", "        random_state=random_state,\r\n"]
[286.286, "o", "        positive_code=positive_code,\r\n"]
[286.296, "o", "        positive_dict=positive_dict,\r\n"]
[286.306, "o", "        transform_max_iter=method_max_iter,\r\n"]
[286.316, "o", "    )\r\n"]
[286.326, "o", "    code = estimator.fit_transform(X)\r\n"]
[286.336, "o", "    if return_n_iter:\r\n"]
[286.346, "o", "        return (\r\n"]
[286.356, "o", "            code,\r\n"]
[286.366, "o", "            estimator.components_,\r\n"]
[286.376, "o", "            estimator.error_,\r\n"]
[286.386, "o", "            estimator.n_iter_,\r\n"]
[286.396, "o", "        )\r\n"]
[286.406, "o", "    return code, estimator.components_, estimator.error_\r\n"]
[286.416, "o", "\r\n"]
[286.426, "o", "\r\n"]
[286.436, "o", "class _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n"]
[286.446, "o", "    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n"]
[286.456, "o", "\r\n"]
[286.466, "o", "    def __init__(\r\n"]
[286.476, "o", "        self,\r\n"]
[286.486, "o", "        transform_algorithm,\r\n"]
[286.496, "o", "        transform_n_nonzero_coefs,\r\n"]
[286.506, "o", "        transform_alpha,\r\n"]
[286.516, "o", "        split_sign,\r\n"]
[286.526, "o", "        n_jobs,\r\n"]
[286.536, "o", "        positive_code,\r\n"]
[286.546, "o", "        transform_max_iter,\r\n"]
[286.556, "o", "    ):\r\n"]
[286.566, "o", "        self.transform_algorithm = transform_algorithm\r\n"]
[286.576, "o", "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n"]
[286.586, "o", "        self.transform_alpha = transform_alpha\r\n"]
[286.596, "o", "        self.transform_max_iter = transform_max_iter\r\n"]
[286.606, "o", "        self.split_sign = split_sign\r\n"]
[286.616, "o", "        self.n_jobs = n_jobs\r\n"]
[286.626, "o", "        self.positive_code = positive_code\r\n"]
[286.636, "o", "\r\n"]
[286.646, "o", "    def _transform(self, X, dictionary):\r\n"]
[286.656, "o", "        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n"]
[286.666, "o", "        SparseCoder.\"\"\"\r\n"]
[286.676, "o", "        X = self._validate_data(X, reset=False)\r\n"]
[286.686, "o", "\r\n"]
[286.696, "o", "        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n"]
[286.706, "o", "            transform_alpha = self.alpha\r\n"]
[286.716, "o", "        else:\r\n"]
[286.726, "o", "            transform_alpha = self.transform_alpha\r\n"]
[286.736, "o", "\r\n"]
[286.746, "o", "        code = sparse_encode(\r\n"]
[286.756, "o", "            X,\r\n"]
[286.766, "o", "            dictionary,\r\n"]
[286.776, "o", "            algorithm=self.transform_algorithm,\r\n"]
[286.786, "o", "            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n"]
[286.796, "o", "            alpha=transform_alpha,\r\n"]
[286.806, "o", "            max_iter=self.transform_max_iter,\r\n"]
[286.816, "o", "            n_jobs=self.n_jobs,\r\n"]
[286.826, "o", "            positive=self.positive_code,\r\n"]
[286.836, "o", "        )\r\n"]
[286.846, "o", "\r\n"]
[286.856, "o", "        if self.split_sign:\r\n"]
[286.866, "o", "            # feature vector is split into a positive and negative side\r\n"]
[286.876, "o", "            n_samples, n_features = code.shape\r\n"]
[286.886, "o", "            split_code = np.empty((n_samples, 2 * n_features))\r\n"]
[286.896, "o", "            split_code[:, :n_features] = np.maximum(code, 0)\r\n"]
[286.906, "o", "            split_code[:, n_features:] = -np.minimum(code, 0)\r\n"]
[286.916, "o", "            code = split_code\r\n"]
[286.926, "o", "\r\n"]
[286.936, "o", "        return code\r\n"]
[286.946, "o", "\r\n"]
[286.956, "o", "    def transform(self, X):\r\n"]
[286.966, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[286.976, "o", "\r\n"]
[286.986, "o", "        Coding method is determined by the object parameter\r\n"]
[286.996, "o", "        `transform_algorithm`.\r\n"]
[287.006, "o", "\r\n"]
[287.016, "o", "        Parameters\r\n"]
[287.026, "o", "        ----------\r\n"]
[287.036, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[287.046, "o", "            Test data to be transformed, must have the same number of\r\n"]
[287.056, "o", "            features as the data used to train the model.\r\n"]
[287.066, "o", "\r\n"]
[287.076, "o", "        Returns\r\n"]
[287.086, "o", "        -------\r\n"]
[287.096, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[287.106, "o", "            Transformed data.\r\n"]
[287.116, "o", "        \"\"\"\r\n"]
[287.126, "o", "        check_is_fitted(self)\r\n"]
[287.136, "o", "        return self._transform(X, self.components_)\r\n"]
[287.146, "o", "\r\n"]
[287.156, "o", "\r\n"]
[287.166, "o", "class SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n"]
[287.176, "o", "    \"\"\"Sparse coding.\r\n"]
[287.186, "o", "\r\n"]
[287.196, "o", "    Finds a sparse representation of data against a fixed, precomputed\r\n"]
[287.206, "o", "    dictionary.\r\n"]
[287.216, "o", "\r\n"]
[287.226, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[287.236, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[287.246, "o", "\r\n"]
[287.256, "o", "        X ~= code * dictionary\r\n"]
[287.266, "o", "\r\n"]
[287.276, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[287.286, "o", "\r\n"]
[287.296, "o", "    Parameters\r\n"]
[287.306, "o", "    ----------\r\n"]
[287.316, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[287.326, "o", "        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n"]
[287.336, "o", "        normalized to unit norm.\r\n"]
[287.346, "o", "\r\n"]
[287.356, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[287.366, "o", "            'threshold'}, default='omp'\r\n"]
[287.376, "o", "        Algorithm used to transform the data:\r\n"]
[287.386, "o", "\r\n"]
[287.396, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[287.406, "o", "          (`linear_model.lars_path`);\r\n"]
[287.416, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[287.426, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[287.436, "o", "          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n"]
[287.446, "o", "          the estimated components are sparse;\r\n"]
[287.456, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[287.466, "o", "          solution;\r\n"]
[287.476, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[287.486, "o", "          the projection ``dictionary * X'``.\r\n"]
[287.496, "o", "\r\n"]
[287.506, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[287.516, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[287.526, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[287.536, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[287.546, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[287.556, "o", "\r\n"]
[287.566, "o", "    transform_alpha : float, default=None\r\n"]
[287.576, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[287.586, "o", "        penalty applied to the L1 norm.\r\n"]
[287.596, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[287.606, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[287.616, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[287.626, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[287.636, "o", "        `n_nonzero_coefs`.\r\n"]
[287.646, "o", "        If `None`, default to 1.\r\n"]
[287.656, "o", "\r\n"]
[287.666, "o", "    split_sign : bool, default=False\r\n"]
[287.676, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[287.686, "o", "        its negative part and its positive part. This can improve the\r\n"]
[287.696, "o", "        performance of downstream classifiers.\r\n"]
[287.706, "o", "\r\n"]
[287.716, "o", "    n_jobs : int, default=None\r\n"]
[287.726, "o", "        Number of parallel jobs to run.\r\n"]
[287.736, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[287.746, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[287.756, "o", "        for more details.\r\n"]
[287.766, "o", "\r\n"]
[287.776, "o", "    positive_code : bool, default=False\r\n"]
[287.786, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[287.796, "o", "\r\n"]
[287.806, "o", "        .. versionadded:: 0.20\r\n"]
[287.816, "o", "\r\n"]
[287.826, "o", "    transform_max_iter : int, default=1000\r\n"]
[287.836, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[287.846, "o", "        `lasso_lars`.\r\n"]
[287.856, "o", "\r\n"]
[287.866, "o", "        .. versionadded:: 0.22\r\n"]
[287.876, "o", "\r\n"]
[287.886, "o", "    Attributes\r\n"]
[287.896, "o", "    ----------\r\n"]
[287.906, "o", "    n_components_ : int\r\n"]
[287.916, "o", "        Number of atoms.\r\n"]
[287.926, "o", "\r\n"]
[287.936, "o", "    n_features_in_ : int\r\n"]
[287.946, "o", "        Number of features seen during :term:`fit`.\r\n"]
[287.956, "o", "\r\n"]
[287.966, "o", "        .. versionadded:: 0.24\r\n"]
[287.976, "o", "\r\n"]
[287.986, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[287.996, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[288.006, "o", "        has feature names that are all strings.\r\n"]
[288.016, "o", "\r\n"]
[288.026, "o", "        .. versionadded:: 1.0\r\n"]
[288.036, "o", "\r\n"]
[288.046, "o", "    See Also\r\n"]
[288.056, "o", "    --------\r\n"]
[288.066, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[288.076, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n"]
[288.086, "o", "        dictionary learning algorithm.\r\n"]
[288.096, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[288.106, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[288.116, "o", "    sparse_encode : Sparse coding where each row of the result is the solution\r\n"]
[288.126, "o", "        to a sparse coding problem.\r\n"]
[288.136, "o", "\r\n"]
[288.146, "o", "    Examples\r\n"]
[288.156, "o", "    --------\r\n"]
[288.166, "o", "    >>> import numpy as np\r\n"]
[288.176, "o", "    >>> from sklearn.decomposition import SparseCoder\r\n"]
[288.186, "o", "    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n"]
[288.196, "o", "    >>> dictionary = np.array(\r\n"]
[288.206, "o", "    ...     [[0, 1, 0],\r\n"]
[288.216, "o", "    ...      [-1, -1, 2],\r\n"]
[288.226, "o", "    ...      [1, 1, 1],\r\n"]
[288.236, "o", "    ...      [0, 1, 1],\r\n"]
[288.246, "o", "    ...      [0, 2, 1]],\r\n"]
[288.256, "o", "    ...    dtype=np.float64\r\n"]
[288.266, "o", "    ... )\r\n"]
[288.276, "o", "    >>> coder = SparseCoder(\r\n"]
[288.286, "o", "    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n"]
[288.296, "o", "    ...     transform_alpha=1e-10,\r\n"]
[288.306, "o", "    ... )\r\n"]
[288.316, "o", "    >>> coder.transform(X)\r\n"]
[288.326, "o", "    array([[ 0.,  0., -1.,  0.,  0.],\r\n"]
[288.336, "o", "           [ 0.,  1.,  1.,  0.,  0.]])\r\n"]
[288.346, "o", "    \"\"\"\r\n"]
[288.356, "o", "\r\n"]
[288.366, "o", "    _required_parameters = [\"dictionary\"]\r\n"]
[288.376, "o", "\r\n"]
[288.386, "o", "    def __init__(\r\n"]
[288.396, "o", "        self,\r\n"]
[288.406, "o", "        dictionary,\r\n"]
[288.416, "o", "        *,\r\n"]
[288.426, "o", "        transform_algorithm=\"omp\",\r\n"]
[288.436, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[288.446, "o", "        transform_alpha=None,\r\n"]
[288.456, "o", "        split_sign=False,\r\n"]
[288.466, "o", "        n_jobs=None,\r\n"]
[288.476, "o", "        positive_code=False,\r\n"]
[288.486, "o", "        transform_max_iter=1000,\r\n"]
[288.496, "o", "    ):\r\n"]
[288.506, "o", "        super().__init__(\r\n"]
[288.516, "o", "            transform_algorithm,\r\n"]
[288.526, "o", "            transform_n_nonzero_coefs,\r\n"]
[288.536, "o", "            transform_alpha,\r\n"]
[288.546, "o", "            split_sign,\r\n"]
[288.556, "o", "            n_jobs,\r\n"]
[288.566, "o", "            positive_code,\r\n"]
[288.576, "o", "            transform_max_iter,\r\n"]
[288.586, "o", "        )\r\n"]
[288.596, "o", "        self.dictionary = dictionary\r\n"]
[288.606, "o", "\r\n"]
[288.616, "o", "    def fit(self, X, y=None):\r\n"]
[288.626, "o", "        \"\"\"Do nothing and return the estimator unchanged.\r\n"]
[288.636, "o", "\r\n"]
[288.646, "o", "        This method is just there to implement the usual API and hence\r\n"]
[288.656, "o", "        work in pipelines.\r\n"]
[288.666, "o", "\r\n"]
[288.676, "o", "        Parameters\r\n"]
[288.686, "o", "        ----------\r\n"]
[288.696, "o", "        X : Ignored\r\n"]
[288.706, "o", "            Not used, present for API consistency by convention.\r\n"]
[288.716, "o", "\r\n"]
[288.726, "o", "        y : Ignored\r\n"]
[288.736, "o", "            Not used, present for API consistency by convention.\r\n"]
[288.746, "o", "\r\n"]
[288.756, "o", "        Returns\r\n"]
[288.766, "o", "        -------\r\n"]
[288.776, "o", "        self : object\r\n"]
[288.786, "o", "            Returns the instance itself.\r\n"]
[288.796, "o", "        \"\"\"\r\n"]
[288.806, "o", "        return self\r\n"]
[288.816, "o", "\r\n"]
[288.826, "o", "    def transform(self, X, y=None):\r\n"]
[288.836, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[288.846, "o", "\r\n"]
[288.856, "o", "        Coding method is determined by the object parameter\r\n"]
[288.866, "o", "        `transform_algorithm`.\r\n"]
[288.876, "o", "\r\n"]
[288.886, "o", "        Parameters\r\n"]
[288.896, "o", "        ----------\r\n"]
[288.906, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[288.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[288.926, "o", "            and `n_features` is the number of features.\r\n"]
[288.936, "o", "\r\n"]
[288.946, "o", "        y : Ignored\r\n"]
[288.956, "o", "            Not used, present for API consistency by convention.\r\n"]
[288.966, "o", "\r\n"]
[288.976, "o", "        Returns\r\n"]
[288.986, "o", "        -------\r\n"]
[288.996, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[289.006, "o", "            Transformed data.\r\n"]
[289.016, "o", "        \"\"\"\r\n"]
[289.026, "o", "        return super()._transform(X, self.dictionary)\r\n"]
[289.036, "o", "\r\n"]
[289.046, "o", "    def _more_tags(self):\r\n"]
[289.056, "o", "        return {\r\n"]
[289.066, "o", "            \"requires_fit\": False,\r\n"]
[289.076, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[289.086, "o", "        }\r\n"]
[289.096, "o", "\r\n"]
[289.106, "o", "    @property\r\n"]
[289.116, "o", "    def n_components_(self):\r\n"]
[289.126, "o", "        \"\"\"Number of atoms.\"\"\"\r\n"]
[289.136, "o", "        return self.dictionary.shape[0]\r\n"]
[289.146, "o", "\r\n"]
[289.156, "o", "    @property\r\n"]
[289.166, "o", "    def n_features_in_(self):\r\n"]
[289.176, "o", "        \"\"\"Number of features seen during `fit`.\"\"\"\r\n"]
[289.186, "o", "        return self.dictionary.shape[1]\r\n"]
[289.196, "o", "\r\n"]
[289.206, "o", "    @property\r\n"]
[289.216, "o", "    def _n_features_out(self):\r\n"]
[289.226, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[289.236, "o", "        return self.n_components_\r\n"]
[289.246, "o", "\r\n"]
[289.256, "o", "\r\n"]
[289.266, "o", "class DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[289.276, "o", "    \"\"\"Dictionary learning.\r\n"]
[289.286, "o", "\r\n"]
[289.296, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[289.306, "o", "    encoding the fitted data.\r\n"]
[289.316, "o", "\r\n"]
[289.326, "o", "    Solves the optimization problem::\r\n"]
[289.336, "o", "\r\n"]
[289.346, "o", "        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[289.356, "o", "                    (U,V)\r\n"]
[289.366, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[289.376, "o", "\r\n"]
[289.386, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[289.396, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[289.406, "o", "    of all the entries in the matrix.\r\n"]
[289.416, "o", "\r\n"]
[289.426, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[289.436, "o", "\r\n"]
[289.446, "o", "    Parameters\r\n"]
[289.456, "o", "    ----------\r\n"]
[289.466, "o", "    n_components : int, default=None\r\n"]
[289.476, "o", "        Number of dictionary elements to extract. If None, then ``n_components``\r\n"]
[289.486, "o", "        is set to ``n_features``.\r\n"]
[289.496, "o", "\r\n"]
[289.506, "o", "    alpha : float, default=1.0\r\n"]
[289.516, "o", "        Sparsity controlling parameter.\r\n"]
[289.526, "o", "\r\n"]
[289.536, "o", "    max_iter : int, default=1000\r\n"]
[289.546, "o", "        Maximum number of iterations to perform.\r\n"]
[289.556, "o", "\r\n"]
[289.566, "o", "    tol : float, default=1e-8\r\n"]
[289.576, "o", "        Tolerance for numerical error.\r\n"]
[289.586, "o", "\r\n"]
[289.596, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[289.606, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[289.616, "o", "          problem (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[289.626, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[289.636, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n"]
[289.646, "o", "          faster if the estimated components are sparse.\r\n"]
[289.656, "o", "\r\n"]
[289.666, "o", "        .. versionadded:: 0.17\r\n"]
[289.676, "o", "           *cd* coordinate descent method to improve speed.\r\n"]
[289.686, "o", "\r\n"]
[289.696, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[289.706, "o", "            'threshold'}, default='omp'\r\n"]
[289.716, "o", "        Algorithm used to transform the data:\r\n"]
[289.726, "o", "\r\n"]
[289.736, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[289.746, "o", "          (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[289.756, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[289.766, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[289.776, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n"]
[289.786, "o", "          will be faster if the estimated components are sparse.\r\n"]
[289.796, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[289.806, "o", "          solution.\r\n"]
[289.816, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[289.826, "o", "          the projection ``dictionary * X'``.\r\n"]
[289.836, "o", "\r\n"]
[289.846, "o", "        .. versionadded:: 0.17\r\n"]
[289.856, "o", "           *lasso_cd* coordinate descent method to improve speed.\r\n"]
[289.866, "o", "\r\n"]
[289.876, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[289.886, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[289.896, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[289.906, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[289.916, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[289.926, "o", "\r\n"]
[289.936, "o", "    transform_alpha : float, default=None\r\n"]
[290.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[290.002, "i", "sed -n '1800,2400p' sklearn/decomposition/_dict_learning.py\r"]
[290.004, "o", "sed -n '1800,2400p' sklearn/decomposition/_dict_learning.py\r\n"]
[290.016, "o", "\u001b[?2004l\r\n"]
[290.026, "o", "        self.fit_transform(X)\r\n"]
[290.036, "o", "        return self\r\n"]
[290.046, "o", "\r\n"]
[290.056, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[290.066, "o", "    def fit_transform(self, X, y=None):\r\n"]
[290.076, "o", "        \"\"\"Fit the model from data in X and return the transformed data.\r\n"]
[290.086, "o", "\r\n"]
[290.096, "o", "        Parameters\r\n"]
[290.106, "o", "        ----------\r\n"]
[290.116, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[290.126, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[290.136, "o", "            and `n_features` is the number of features.\r\n"]
[290.146, "o", "\r\n"]
[290.156, "o", "        y : Ignored\r\n"]
[290.166, "o", "            Not used, present for API consistency by convention.\r\n"]
[290.176, "o", "\r\n"]
[290.186, "o", "        Returns\r\n"]
[290.196, "o", "        -------\r\n"]
[290.206, "o", "        V : ndarray of shape (n_samples, n_components)\r\n"]
[290.216, "o", "            Transformed data.\r\n"]
[290.226, "o", "        \"\"\"\r\n"]
[290.236, "o", "        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n"]
[290.246, "o", "\r\n"]
[290.256, "o", "        method = \"lasso_\" + self.fit_algorithm\r\n"]
[290.266, "o", "\r\n"]
[290.276, "o", "        random_state = check_random_state(self.random_state)\r\n"]
[290.286, "o", "        X = self._validate_data(X)\r\n"]
[290.296, "o", "\r\n"]
[290.306, "o", "        if self.n_components is None:\r\n"]
[290.316, "o", "            n_components = X.shape[1]\r\n"]
[290.326, "o", "        else:\r\n"]
[290.336, "o", "            n_components = self.n_components\r\n"]
[290.346, "o", "\r\n"]
[290.356, "o", "        V, U, E, self.n_iter_ = _dict_learning(\r\n"]
[290.366, "o", "            X,\r\n"]
[290.376, "o", "            n_components,\r\n"]
[290.386, "o", "            alpha=self.alpha,\r\n"]
[290.396, "o", "            tol=self.tol,\r\n"]
[290.406, "o", "            max_iter=self.max_iter,\r\n"]
[290.416, "o", "            method=method,\r\n"]
[290.426, "o", "            method_max_iter=self.transform_max_iter,\r\n"]
[290.436, "o", "            n_jobs=self.n_jobs,\r\n"]
[290.446, "o", "            code_init=self.code_init,\r\n"]
[290.456, "o", "            dict_init=self.dict_init,\r\n"]
[290.466, "o", "            callback=self.callback,\r\n"]
[290.476, "o", "            verbose=self.verbose,\r\n"]
[290.486, "o", "            random_state=random_state,\r\n"]
[290.496, "o", "            return_n_iter=True,\r\n"]
[290.506, "o", "            positive_dict=self.positive_dict,\r\n"]
[290.516, "o", "            positive_code=self.positive_code,\r\n"]
[290.526, "o", "        )\r\n"]
[290.536, "o", "        self.components_ = U\r\n"]
[290.546, "o", "        self.error_ = E\r\n"]
[290.556, "o", "\r\n"]
[290.566, "o", "        return V\r\n"]
[290.576, "o", "\r\n"]
[290.586, "o", "    @property\r\n"]
[290.596, "o", "    def _n_features_out(self):\r\n"]
[290.606, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[290.616, "o", "        return self.components_.shape[0]\r\n"]
[290.626, "o", "\r\n"]
[290.636, "o", "    def _more_tags(self):\r\n"]
[290.646, "o", "        return {\r\n"]
[290.656, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[290.666, "o", "        }\r\n"]
[290.676, "o", "\r\n"]
[290.686, "o", "\r\n"]
[290.696, "o", "class MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[290.706, "o", "    \"\"\"Mini-batch dictionary learning.\r\n"]
[290.716, "o", "\r\n"]
[290.726, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[290.736, "o", "    encoding the fitted data.\r\n"]
[290.746, "o", "\r\n"]
[290.756, "o", "    Solves the optimization problem::\r\n"]
[290.766, "o", "\r\n"]
[290.776, "o", "       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[290.786, "o", "                    (U,V)\r\n"]
[290.796, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[290.806, "o", "\r\n"]
[290.816, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[290.826, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[290.836, "o", "    of all the entries in the matrix.\r\n"]
[290.846, "o", "\r\n"]
[290.856, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[290.866, "o", "\r\n"]
[290.876, "o", "    Parameters\r\n"]
[290.886, "o", "    ----------\r\n"]
[290.896, "o", "    n_components : int, default=None\r\n"]
[290.906, "o", "        Number of dictionary elements to extract.\r\n"]
[290.916, "o", "\r\n"]
[290.926, "o", "    alpha : float, default=1\r\n"]
[290.936, "o", "        Sparsity controlling parameter.\r\n"]
[290.946, "o", "\r\n"]
[290.956, "o", "    n_iter : int, default=1000\r\n"]
[290.966, "o", "        Total number of iterations over data batches to perform.\r\n"]
[290.976, "o", "\r\n"]
[290.986, "o", "        .. deprecated:: 1.1\r\n"]
[290.996, "o", "           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[291.006, "o", "           ``max_iter`` instead.\r\n"]
[291.016, "o", "\r\n"]
[291.026, "o", "    max_iter : int, default=None\r\n"]
[291.036, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[291.046, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[291.056, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[291.066, "o", "\r\n"]
[291.076, "o", "        .. versionadded:: 1.1\r\n"]
[291.086, "o", "\r\n"]
[291.096, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[291.106, "o", "        The algorithm used:\r\n"]
[291.116, "o", "\r\n"]
[291.126, "o", "        - `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[291.136, "o", "          problem (`linear_model.lars_path`)\r\n"]
[291.146, "o", "        - `'cd'`: uses the coordinate descent method to compute the\r\n"]
[291.156, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[291.166, "o", "          the estimated components are sparse.\r\n"]
[291.176, "o", "\r\n"]
[291.186, "o", "    n_jobs : int, default=None\r\n"]
[291.196, "o", "        Number of parallel jobs to run.\r\n"]
[291.206, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[291.216, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[291.226, "o", "        for more details.\r\n"]
[291.236, "o", "\r\n"]
[291.246, "o", "    batch_size : int, default=256\r\n"]
[291.256, "o", "        Number of samples in each mini-batch.\r\n"]
[291.266, "o", "\r\n"]
[291.276, "o", "        .. versionchanged:: 1.3\r\n"]
[291.286, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[291.296, "o", "\r\n"]
[291.306, "o", "    shuffle : bool, default=True\r\n"]
[291.316, "o", "        Whether to shuffle the samples before forming batches.\r\n"]
[291.326, "o", "\r\n"]
[291.336, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[291.346, "o", "        Initial value of the dictionary for warm restart scenarios.\r\n"]
[291.356, "o", "\r\n"]
[291.366, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[291.376, "o", "            'threshold'}, default='omp'\r\n"]
[291.386, "o", "        Algorithm used to transform the data:\r\n"]
[291.396, "o", "\r\n"]
[291.406, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[291.416, "o", "          (`linear_model.lars_path`);\r\n"]
[291.426, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[291.436, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[291.446, "o", "          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n"]
[291.456, "o", "          if the estimated components are sparse.\r\n"]
[291.466, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[291.476, "o", "          solution.\r\n"]
[291.486, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[291.496, "o", "          the projection ``dictionary * X'``.\r\n"]
[291.506, "o", "\r\n"]
[291.516, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[291.526, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[291.536, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[291.546, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[291.556, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[291.566, "o", "\r\n"]
[291.576, "o", "    transform_alpha : float, default=None\r\n"]
[291.586, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[291.596, "o", "        penalty applied to the L1 norm.\r\n"]
[291.606, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[291.616, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[291.626, "o", "        If `None`, defaults to `alpha`.\r\n"]
[291.636, "o", "\r\n"]
[291.646, "o", "        .. versionchanged:: 1.2\r\n"]
[291.656, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[291.666, "o", "\r\n"]
[291.676, "o", "    verbose : bool or int, default=False\r\n"]
[291.686, "o", "        To control the verbosity of the procedure.\r\n"]
[291.696, "o", "\r\n"]
[291.706, "o", "    split_sign : bool, default=False\r\n"]
[291.716, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[291.726, "o", "        its negative part and its positive part. This can improve the\r\n"]
[291.736, "o", "        performance of downstream classifiers.\r\n"]
[291.746, "o", "\r\n"]
[291.756, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[291.766, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[291.776, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[291.786, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[291.796, "o", "        results across multiple function calls.\r\n"]
[291.806, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[291.816, "o", "\r\n"]
[291.826, "o", "    positive_code : bool, default=False\r\n"]
[291.836, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[291.846, "o", "\r\n"]
[291.856, "o", "        .. versionadded:: 0.20\r\n"]
[291.866, "o", "\r\n"]
[291.876, "o", "    positive_dict : bool, default=False\r\n"]
[291.886, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[291.896, "o", "\r\n"]
[291.906, "o", "        .. versionadded:: 0.20\r\n"]
[291.916, "o", "\r\n"]
[291.926, "o", "    transform_max_iter : int, default=1000\r\n"]
[291.936, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[291.946, "o", "        `'lasso_lars'`.\r\n"]
[291.956, "o", "\r\n"]
[291.966, "o", "        .. versionadded:: 0.22\r\n"]
[291.976, "o", "\r\n"]
[291.986, "o", "    callback : callable, default=None\r\n"]
[291.996, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[292.006, "o", "\r\n"]
[292.016, "o", "        .. versionadded:: 1.1\r\n"]
[292.026, "o", "\r\n"]
[292.036, "o", "    tol : float, default=1e-3\r\n"]
[292.046, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[292.056, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[292.066, "o", "\r\n"]
[292.076, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[292.086, "o", "        `tol` to 0.0.\r\n"]
[292.096, "o", "\r\n"]
[292.106, "o", "        .. versionadded:: 1.1\r\n"]
[292.116, "o", "\r\n"]
[292.126, "o", "    max_no_improvement : int, default=10\r\n"]
[292.136, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[292.146, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[292.156, "o", "        `max_iter` is not None.\r\n"]
[292.166, "o", "\r\n"]
[292.176, "o", "        To disable convergence detection based on cost function, set\r\n"]
[292.186, "o", "        `max_no_improvement` to None.\r\n"]
[292.196, "o", "\r\n"]
[292.206, "o", "        .. versionadded:: 1.1\r\n"]
[292.216, "o", "\r\n"]
[292.226, "o", "    Attributes\r\n"]
[292.236, "o", "    ----------\r\n"]
[292.246, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[292.256, "o", "        Components extracted from the data.\r\n"]
[292.266, "o", "\r\n"]
[292.276, "o", "    n_features_in_ : int\r\n"]
[292.286, "o", "        Number of features seen during :term:`fit`.\r\n"]
[292.296, "o", "\r\n"]
[292.306, "o", "        .. versionadded:: 0.24\r\n"]
[292.316, "o", "\r\n"]
[292.326, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[292.336, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[292.346, "o", "        has feature names that are all strings.\r\n"]
[292.356, "o", "\r\n"]
[292.366, "o", "        .. versionadded:: 1.0\r\n"]
[292.376, "o", "\r\n"]
[292.386, "o", "    n_iter_ : int\r\n"]
[292.396, "o", "        Number of iterations over the full dataset.\r\n"]
[292.406, "o", "\r\n"]
[292.416, "o", "    n_steps_ : int\r\n"]
[292.426, "o", "        Number of mini-batches processed.\r\n"]
[292.436, "o", "\r\n"]
[292.446, "o", "        .. versionadded:: 1.1\r\n"]
[292.456, "o", "\r\n"]
[292.466, "o", "    See Also\r\n"]
[292.476, "o", "    --------\r\n"]
[292.486, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[292.496, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[292.506, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[292.516, "o", "        precomputed dictionary.\r\n"]
[292.526, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[292.536, "o", "\r\n"]
[292.546, "o", "    References\r\n"]
[292.556, "o", "    ----------\r\n"]
[292.566, "o", "\r\n"]
[292.576, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[292.586, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[292.596, "o", "\r\n"]
[292.606, "o", "    Examples\r\n"]
[292.616, "o", "    --------\r\n"]
[292.626, "o", "    >>> import numpy as np\r\n"]
[292.636, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[292.646, "o", "    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n"]
[292.656, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[292.666, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[292.676, "o", "    ...     random_state=42)\r\n"]
[292.686, "o", "    >>> dict_learner = MiniBatchDictionaryLearning(\r\n"]
[292.696, "o", "    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n"]
[292.706, "o", "    ...     transform_alpha=0.1, random_state=42)\r\n"]
[292.716, "o", "    >>> X_transformed = dict_learner.fit_transform(X)\r\n"]
[292.726, "o", "\r\n"]
[292.736, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[292.746, "o", "\r\n"]
[292.756, "o", "    >>> np.mean(X_transformed == 0) < 0.5\r\n"]
[292.766, "o", "    True\r\n"]
[292.776, "o", "\r\n"]
[292.786, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[292.796, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[292.806, "o", "    the original signal:\r\n"]
[292.816, "o", "\r\n"]
[292.826, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[292.836, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[292.846, "o", "    0.057...\r\n"]
[292.856, "o", "    \"\"\"\r\n"]
[292.866, "o", "\r\n"]
[292.876, "o", "    _parameter_constraints: dict = {\r\n"]
[292.886, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[292.896, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[292.906, "o", "        \"n_iter\": [\r\n"]
[292.916, "o", "            Interval(Integral, 0, None, closed=\"left\"),\r\n"]
[292.926, "o", "            Hidden(StrOptions({\"deprecated\"})),\r\n"]
[292.936, "o", "        ],\r\n"]
[292.946, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n"]
[292.956, "o", "        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n"]
[292.966, "o", "        \"n_jobs\": [None, Integral],\r\n"]
[292.976, "o", "        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n"]
[292.986, "o", "        \"shuffle\": [\"boolean\"],\r\n"]
[292.996, "o", "        \"dict_init\": [None, np.ndarray],\r\n"]
[293.006, "o", "        \"transform_algorithm\": [\r\n"]
[293.016, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[293.026, "o", "        ],\r\n"]
[293.036, "o", "        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[293.046, "o", "        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[293.056, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[293.066, "o", "        \"split_sign\": [\"boolean\"],\r\n"]
[293.076, "o", "        \"random_state\": [\"random_state\"],\r\n"]
[293.086, "o", "        \"positive_code\": [\"boolean\"],\r\n"]
[293.096, "o", "        \"positive_dict\": [\"boolean\"],\r\n"]
[293.106, "o", "        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[293.116, "o", "        \"callback\": [None, callable],\r\n"]
[293.126, "o", "        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[293.136, "o", "        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n"]
[293.146, "o", "    }\r\n"]
[293.156, "o", "\r\n"]
[293.166, "o", "    def __init__(\r\n"]
[293.176, "o", "        self,\r\n"]
[293.186, "o", "        n_components=None,\r\n"]
[293.196, "o", "        *,\r\n"]
[293.206, "o", "        alpha=1,\r\n"]
[293.216, "o", "        n_iter=\"deprecated\",\r\n"]
[293.226, "o", "        max_iter=None,\r\n"]
[293.236, "o", "        fit_algorithm=\"lars\",\r\n"]
[293.246, "o", "        n_jobs=None,\r\n"]
[293.256, "o", "        batch_size=256,\r\n"]
[293.266, "o", "        shuffle=True,\r\n"]
[293.276, "o", "        dict_init=None,\r\n"]
[293.286, "o", "        transform_algorithm=\"omp\",\r\n"]
[293.296, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[293.306, "o", "        transform_alpha=None,\r\n"]
[293.316, "o", "        verbose=False,\r\n"]
[293.326, "o", "        split_sign=False,\r\n"]
[293.336, "o", "        random_state=None,\r\n"]
[293.346, "o", "        positive_code=False,\r\n"]
[293.356, "o", "        positive_dict=False,\r\n"]
[293.366, "o", "        transform_max_iter=1000,\r\n"]
[293.376, "o", "        callback=None,\r\n"]
[293.386, "o", "        tol=1e-3,\r\n"]
[293.396, "o", "        max_no_improvement=10,\r\n"]
[293.406, "o", "    ):\r\n"]
[293.416, "o", "        super().__init__(\r\n"]
[293.426, "o", "            transform_algorithm,\r\n"]
[293.436, "o", "            transform_n_nonzero_coefs,\r\n"]
[293.446, "o", "            transform_alpha,\r\n"]
[293.456, "o", "            split_sign,\r\n"]
[293.466, "o", "            n_jobs,\r\n"]
[293.476, "o", "            positive_code,\r\n"]
[293.486, "o", "            transform_max_iter,\r\n"]
[293.496, "o", "        )\r\n"]
[293.506, "o", "        self.n_components = n_components\r\n"]
[293.516, "o", "        self.alpha = alpha\r\n"]
[293.526, "o", "        self.n_iter = n_iter\r\n"]
[293.536, "o", "        self.max_iter = max_iter\r\n"]
[293.546, "o", "        self.fit_algorithm = fit_algorithm\r\n"]
[293.556, "o", "        self.dict_init = dict_init\r\n"]
[293.566, "o", "        self.verbose = verbose\r\n"]
[293.576, "o", "        self.shuffle = shuffle\r\n"]
[293.586, "o", "        self.batch_size = batch_size\r\n"]
[293.596, "o", "        self.split_sign = split_sign\r\n"]
[293.606, "o", "        self.random_state = random_state\r\n"]
[293.616, "o", "        self.positive_dict = positive_dict\r\n"]
[293.626, "o", "        self.callback = callback\r\n"]
[293.636, "o", "        self.max_no_improvement = max_no_improvement\r\n"]
[293.646, "o", "        self.tol = tol\r\n"]
[293.656, "o", "\r\n"]
[293.666, "o", "    def _check_params(self, X):\r\n"]
[293.676, "o", "        # n_components\r\n"]
[293.686, "o", "        self._n_components = self.n_components\r\n"]
[293.696, "o", "        if self._n_components is None:\r\n"]
[293.706, "o", "            self._n_components = X.shape[1]\r\n"]
[293.716, "o", "\r\n"]
[293.726, "o", "        # fit_algorithm\r\n"]
[293.736, "o", "        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n"]
[293.746, "o", "        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n"]
[293.756, "o", "\r\n"]
[293.766, "o", "        # batch_size\r\n"]
[293.776, "o", "        self._batch_size = min(self.batch_size, X.shape[0])\r\n"]
[293.786, "o", "\r\n"]
[293.796, "o", "    def _initialize_dict(self, X, random_state):\r\n"]
[293.806, "o", "        \"\"\"Initialization of the dictionary.\"\"\"\r\n"]
[293.816, "o", "        if self.dict_init is not None:\r\n"]
[293.826, "o", "            dictionary = self.dict_init\r\n"]
[293.836, "o", "        else:\r\n"]
[293.846, "o", "            # Init V with SVD of X\r\n"]
[293.856, "o", "            _, S, dictionary = randomized_svd(\r\n"]
[293.866, "o", "                X, self._n_components, random_state=random_state\r\n"]
[293.876, "o", "            )\r\n"]
[293.886, "o", "            dictionary = S[:, np.newaxis] * dictionary\r\n"]
[293.896, "o", "\r\n"]
[293.906, "o", "        if self._n_components <= len(dictionary):\r\n"]
[293.916, "o", "            dictionary = dictionary[: self._n_components, :]\r\n"]
[293.926, "o", "        else:\r\n"]
[293.936, "o", "            dictionary = np.concatenate(\r\n"]
[293.946, "o", "                (\r\n"]
[293.956, "o", "                    dictionary,\r\n"]
[293.966, "o", "                    np.zeros(\r\n"]
[293.976, "o", "                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n"]
[293.986, "o", "                        dtype=dictionary.dtype,\r\n"]
[293.996, "o", "                    ),\r\n"]
[294.006, "o", "                )\r\n"]
[294.016, "o", "            )\r\n"]
[294.026, "o", "\r\n"]
[294.036, "o", "        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n"]
[294.046, "o", "        dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[294.056, "o", "\r\n"]
[294.066, "o", "        return dictionary\r\n"]
[294.076, "o", "\r\n"]
[294.086, "o", "    def _update_inner_stats(self, X, code, batch_size, step):\r\n"]
[294.096, "o", "        \"\"\"Update the inner stats inplace.\"\"\"\r\n"]
[294.106, "o", "        if step < batch_size - 1:\r\n"]
[294.116, "o", "            theta = (step + 1) * batch_size\r\n"]
[294.126, "o", "        else:\r\n"]
[294.136, "o", "            theta = batch_size**2 + step + 1 - batch_size\r\n"]
[294.146, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[294.156, "o", "\r\n"]
[294.166, "o", "        self._A *= beta\r\n"]
[294.176, "o", "        self._A += code.T @ code / batch_size\r\n"]
[294.186, "o", "        self._B *= beta\r\n"]
[294.196, "o", "        self._B += X.T @ code / batch_size\r\n"]
[294.206, "o", "\r\n"]
[294.216, "o", "    def _minibatch_step(self, X, dictionary, random_state, step):\r\n"]
[294.226, "o", "        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n"]
[294.236, "o", "        batch_size = X.shape[0]\r\n"]
[294.246, "o", "\r\n"]
[294.256, "o", "        # Compute code for this batch\r\n"]
[294.266, "o", "        code = _sparse_encode(\r\n"]
[294.276, "o", "            X,\r\n"]
[294.286, "o", "            dictionary,\r\n"]
[294.296, "o", "            algorithm=self._fit_algorithm,\r\n"]
[294.306, "o", "            alpha=self.alpha,\r\n"]
[294.316, "o", "            n_jobs=self.n_jobs,\r\n"]
[294.326, "o", "            positive=self.positive_code,\r\n"]
[294.336, "o", "            max_iter=self.transform_max_iter,\r\n"]
[294.346, "o", "            verbose=self.verbose,\r\n"]
[294.356, "o", "        )\r\n"]
[294.366, "o", "\r\n"]
[294.376, "o", "        batch_cost = (\r\n"]
[294.386, "o", "            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n"]
[294.396, "o", "            + self.alpha * np.sum(np.abs(code))\r\n"]
[294.406, "o", "        ) / batch_size\r\n"]
[294.416, "o", "\r\n"]
[294.426, "o", "        # Update inner stats\r\n"]
[294.436, "o", "        self._update_inner_stats(X, code, batch_size, step)\r\n"]
[294.446, "o", "\r\n"]
[294.456, "o", "        # Update dictionary\r\n"]
[294.466, "o", "        _update_dict(\r\n"]
[294.476, "o", "            dictionary,\r\n"]
[294.486, "o", "            X,\r\n"]
[294.496, "o", "            code,\r\n"]
[294.506, "o", "            self._A,\r\n"]
[294.516, "o", "            self._B,\r\n"]
[294.526, "o", "            verbose=self.verbose,\r\n"]
[294.536, "o", "            random_state=random_state,\r\n"]
[294.546, "o", "            positive=self.positive_dict,\r\n"]
[294.556, "o", "        )\r\n"]
[294.566, "o", "\r\n"]
[294.576, "o", "        return batch_cost\r\n"]
[294.586, "o", "\r\n"]
[294.596, "o", "    def _check_convergence(\r\n"]
[294.606, "o", "        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n"]
[294.616, "o", "    ):\r\n"]
[294.626, "o", "        \"\"\"Helper function to encapsulate the early stopping logic.\r\n"]
[294.636, "o", "\r\n"]
[294.646, "o", "        Early stopping is based on two factors:\r\n"]
[294.656, "o", "        - A small change of the dictionary between two minibatch updates. This is\r\n"]
[294.666, "o", "          controlled by the tol parameter.\r\n"]
[294.676, "o", "        - No more improvement on a smoothed estimate of the objective function for a\r\n"]
[294.686, "o", "          a certain number of consecutive minibatch updates. This is controlled by\r\n"]
[294.696, "o", "          the max_no_improvement parameter.\r\n"]
[294.706, "o", "        \"\"\"\r\n"]
[294.716, "o", "        batch_size = X.shape[0]\r\n"]
[294.726, "o", "\r\n"]
[294.736, "o", "        # counts steps starting from 1 for user friendly verbose mode.\r\n"]
[294.746, "o", "        step = step + 1\r\n"]
[294.756, "o", "\r\n"]
[294.766, "o", "        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n"]
[294.776, "o", "        # too bad value\r\n"]
[294.786, "o", "        if step <= min(100, n_samples / batch_size):\r\n"]
[294.796, "o", "            if self.verbose:\r\n"]
[294.806, "o", "                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n"]
[294.816, "o", "            return False\r\n"]
[294.826, "o", "\r\n"]
[294.836, "o", "        # Compute an Exponentially Weighted Average of the cost function to\r\n"]
[294.846, "o", "        # monitor the convergence while discarding minibatch-local stochastic\r\n"]
[294.856, "o", "        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n"]
[294.866, "o", "        if self._ewa_cost is None:\r\n"]
[294.876, "o", "            self._ewa_cost = batch_cost\r\n"]
[294.886, "o", "        else:\r\n"]
[294.896, "o", "            alpha = batch_size / (n_samples + 1)\r\n"]
[294.906, "o", "            alpha = min(alpha, 1)\r\n"]
[294.916, "o", "            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n"]
[294.926, "o", "\r\n"]
[294.936, "o", "        if self.verbose:\r\n"]
[295.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[295.002, "i", "cd asv_benchmarks\r"]
[295.004, "o", "cd asv_benchmarks\r\n"]
[295.006, "o", "\u001b[?2004l\r\n"]
[300.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[300.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[300.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[301.654, "o", "\u001b[?2004l\r\n"]
[303.302, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[305.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[305.002, "i", "cd ..\r"]
[305.004, "o", "cd ..\r\n"]
[305.006, "o", "\u001b[?2004l\r\n"]
[310.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[310.002, "i", "grep -n \"def _minibatch_step\\|def dict_learning_online\\|def _mini_batch_dictionary_learning\\|class MiniBatchDiction\r"]
[310.004, "o", "grep -n \"def _minibatch_step\\|def dict_learning_online\\|def _mini_batch_dictionary_learning\\|class MiniBatchDiction\r\n"]
[310.9948, "o", "naryLearning\" sklearn/decomposition/_dict_learning.py\r\n"]
[311.9836, "o", "\u001b[?2004l\r\n"]
[312.9724, "o", "\u001b[32m\u001b[K664\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kdef dict_learning_online\u001b[m\u001b[K(\r\n"]
[313.9612, "o", "\u001b[32m\u001b[K1867\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kclass MiniBatchDictionaryLearning\u001b[m\u001b[K(_BaseSparseCoding, BaseEstimator):\r\n"]
[315.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[315.002, "i", "sed -n '1000,1800p' sklearn/decomposition/_dict_learning.py\r"]
[315.004, "o", "sed -n '1000,1800p' sklearn/decomposition/_dict_learning.py\r\n"]
[315.016, "o", "\u001b[?2004l\r\n"]
[315.026, "o", "\r\n"]
[315.036, "o", "        this_code = sparse_encode(\r\n"]
[315.046, "o", "            this_X,\r\n"]
[315.056, "o", "            dictionary,\r\n"]
[315.066, "o", "            algorithm=method,\r\n"]
[315.076, "o", "            alpha=alpha,\r\n"]
[315.086, "o", "            n_jobs=n_jobs,\r\n"]
[315.096, "o", "            check_input=False,\r\n"]
[315.106, "o", "            positive=positive_code,\r\n"]
[315.116, "o", "            max_iter=method_max_iter,\r\n"]
[315.126, "o", "            verbose=verbose,\r\n"]
[315.136, "o", "        )\r\n"]
[315.146, "o", "\r\n"]
[315.156, "o", "        # Update the auxiliary variables\r\n"]
[315.166, "o", "        if ii < batch_size - 1:\r\n"]
[315.176, "o", "            theta = float((ii + 1) * batch_size)\r\n"]
[315.186, "o", "        else:\r\n"]
[315.196, "o", "            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n"]
[315.206, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[315.216, "o", "\r\n"]
[315.226, "o", "        A *= beta\r\n"]
[315.236, "o", "        A += np.dot(this_code.T, this_code)\r\n"]
[315.246, "o", "        B *= beta\r\n"]
[315.256, "o", "        B += np.dot(this_X.T, this_code)\r\n"]
[315.266, "o", "\r\n"]
[315.276, "o", "        # Update dictionary in place\r\n"]
[315.286, "o", "        _update_dict(\r\n"]
[315.296, "o", "            dictionary,\r\n"]
[315.306, "o", "            this_X,\r\n"]
[315.316, "o", "            this_code,\r\n"]
[315.326, "o", "            A,\r\n"]
[315.336, "o", "            B,\r\n"]
[315.346, "o", "            verbose=verbose,\r\n"]
[315.356, "o", "            random_state=random_state,\r\n"]
[315.366, "o", "            positive=positive_dict,\r\n"]
[315.376, "o", "        )\r\n"]
[315.386, "o", "\r\n"]
[315.396, "o", "        # Maybe we need a stopping criteria based on the amount of\r\n"]
[315.406, "o", "        # modification in the dictionary\r\n"]
[315.416, "o", "        if callback is not None:\r\n"]
[315.426, "o", "            callback(locals())\r\n"]
[315.436, "o", "\r\n"]
[315.446, "o", "    if return_inner_stats:\r\n"]
[315.456, "o", "        if return_n_iter:\r\n"]
[315.466, "o", "            return dictionary, (A, B), ii - iter_offset + 1\r\n"]
[315.476, "o", "        else:\r\n"]
[315.486, "o", "            return dictionary, (A, B)\r\n"]
[315.496, "o", "    if return_code:\r\n"]
[315.506, "o", "        if verbose > 1:\r\n"]
[315.516, "o", "            print(\"Learning code...\", end=\" \")\r\n"]
[315.526, "o", "        elif verbose == 1:\r\n"]
[315.536, "o", "            print(\"|\", end=\" \")\r\n"]
[315.546, "o", "        code = sparse_encode(\r\n"]
[315.556, "o", "            X,\r\n"]
[315.566, "o", "            dictionary,\r\n"]
[315.576, "o", "            algorithm=method,\r\n"]
[315.586, "o", "            alpha=alpha,\r\n"]
[315.596, "o", "            n_jobs=n_jobs,\r\n"]
[315.606, "o", "            check_input=False,\r\n"]
[315.616, "o", "            positive=positive_code,\r\n"]
[315.626, "o", "            max_iter=method_max_iter,\r\n"]
[315.636, "o", "            verbose=verbose,\r\n"]
[315.646, "o", "        )\r\n"]
[315.656, "o", "        if verbose > 1:\r\n"]
[315.666, "o", "            dt = time.time() - t0\r\n"]
[315.676, "o", "            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n"]
[315.686, "o", "        if return_n_iter:\r\n"]
[315.696, "o", "            return code, dictionary, ii - iter_offset + 1\r\n"]
[315.706, "o", "        else:\r\n"]
[315.716, "o", "            return code, dictionary\r\n"]
[315.726, "o", "\r\n"]
[315.736, "o", "    if return_n_iter:\r\n"]
[315.746, "o", "        return dictionary, ii - iter_offset + 1\r\n"]
[315.756, "o", "    else:\r\n"]
[315.766, "o", "        return dictionary\r\n"]
[315.776, "o", "\r\n"]
[315.786, "o", "\r\n"]
[315.796, "o", "@validate_params(\r\n"]
[315.806, "o", "    {\r\n"]
[315.816, "o", "        \"X\": [\"array-like\"],\r\n"]
[315.826, "o", "        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[315.836, "o", "        \"return_n_iter\": [\"boolean\"],\r\n"]
[315.846, "o", "        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[315.856, "o", "    },\r\n"]
[315.866, "o", "    prefer_skip_nested_validation=False,\r\n"]
[315.876, "o", ")\r\n"]
[315.886, "o", "def dict_learning(\r\n"]
[315.896, "o", "    X,\r\n"]
[315.906, "o", "    n_components,\r\n"]
[315.916, "o", "    *,\r\n"]
[315.926, "o", "    alpha,\r\n"]
[315.936, "o", "    max_iter=100,\r\n"]
[315.946, "o", "    tol=1e-8,\r\n"]
[315.956, "o", "    method=\"lars\",\r\n"]
[315.966, "o", "    n_jobs=None,\r\n"]
[315.976, "o", "    dict_init=None,\r\n"]
[315.986, "o", "    code_init=None,\r\n"]
[315.996, "o", "    callback=None,\r\n"]
[316.006, "o", "    verbose=False,\r\n"]
[316.016, "o", "    random_state=None,\r\n"]
[316.026, "o", "    return_n_iter=False,\r\n"]
[316.036, "o", "    positive_dict=False,\r\n"]
[316.046, "o", "    positive_code=False,\r\n"]
[316.056, "o", "    method_max_iter=1000,\r\n"]
[316.066, "o", "):\r\n"]
[316.076, "o", "    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n"]
[316.086, "o", "\r\n"]
[316.096, "o", "    Finds the best dictionary and the corresponding sparse code for\r\n"]
[316.106, "o", "    approximating the data matrix X by solving::\r\n"]
[316.116, "o", "\r\n"]
[316.126, "o", "        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[316.136, "o", "                     (U,V)\r\n"]
[316.146, "o", "                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n"]
[316.156, "o", "\r\n"]
[316.166, "o", "    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n"]
[316.176, "o", "    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n"]
[316.186, "o", "    which is the sum of the absolute values of all the entries in the matrix.\r\n"]
[316.196, "o", "\r\n"]
[316.206, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[316.216, "o", "\r\n"]
[316.226, "o", "    Parameters\r\n"]
[316.236, "o", "    ----------\r\n"]
[316.246, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[316.256, "o", "        Data matrix.\r\n"]
[316.266, "o", "\r\n"]
[316.276, "o", "    n_components : int\r\n"]
[316.286, "o", "        Number of dictionary atoms to extract.\r\n"]
[316.296, "o", "\r\n"]
[316.306, "o", "    alpha : int or float\r\n"]
[316.316, "o", "        Sparsity controlling parameter.\r\n"]
[316.326, "o", "\r\n"]
[316.336, "o", "    max_iter : int, default=100\r\n"]
[316.346, "o", "        Maximum number of iterations to perform.\r\n"]
[316.356, "o", "\r\n"]
[316.366, "o", "    tol : float, default=1e-8\r\n"]
[316.376, "o", "        Tolerance for the stopping condition.\r\n"]
[316.386, "o", "\r\n"]
[316.396, "o", "    method : {'lars', 'cd'}, default='lars'\r\n"]
[316.406, "o", "        The method used:\r\n"]
[316.416, "o", "\r\n"]
[316.426, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[316.436, "o", "           problem (`linear_model.lars_path`);\r\n"]
[316.446, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[316.456, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[316.466, "o", "          the estimated components are sparse.\r\n"]
[316.476, "o", "\r\n"]
[316.486, "o", "    n_jobs : int, default=None\r\n"]
[316.496, "o", "        Number of parallel jobs to run.\r\n"]
[316.506, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[316.516, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[316.526, "o", "        for more details.\r\n"]
[316.536, "o", "\r\n"]
[316.546, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[316.556, "o", "        Initial value for the dictionary for warm restart scenarios. Only used\r\n"]
[316.566, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[316.576, "o", "\r\n"]
[316.586, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[316.596, "o", "        Initial value for the sparse code for warm restart scenarios. Only used\r\n"]
[316.606, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[316.616, "o", "\r\n"]
[316.626, "o", "    callback : callable, default=None\r\n"]
[316.636, "o", "        Callable that gets invoked every five iterations.\r\n"]
[316.646, "o", "\r\n"]
[316.656, "o", "    verbose : bool, default=False\r\n"]
[316.666, "o", "        To control the verbosity of the procedure.\r\n"]
[316.676, "o", "\r\n"]
[316.686, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[316.696, "o", "        Used for randomly initializing the dictionary. Pass an int for\r\n"]
[316.706, "o", "        reproducible results across multiple function calls.\r\n"]
[316.716, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[316.726, "o", "\r\n"]
[316.736, "o", "    return_n_iter : bool, default=False\r\n"]
[316.746, "o", "        Whether or not to return the number of iterations.\r\n"]
[316.756, "o", "\r\n"]
[316.766, "o", "    positive_dict : bool, default=False\r\n"]
[316.776, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[316.786, "o", "\r\n"]
[316.796, "o", "        .. versionadded:: 0.20\r\n"]
[316.806, "o", "\r\n"]
[316.816, "o", "    positive_code : bool, default=False\r\n"]
[316.826, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[316.836, "o", "\r\n"]
[316.846, "o", "        .. versionadded:: 0.20\r\n"]
[316.856, "o", "\r\n"]
[316.866, "o", "    method_max_iter : int, default=1000\r\n"]
[316.876, "o", "        Maximum number of iterations to perform.\r\n"]
[316.886, "o", "\r\n"]
[316.896, "o", "        .. versionadded:: 0.22\r\n"]
[316.906, "o", "\r\n"]
[316.916, "o", "    Returns\r\n"]
[316.926, "o", "    -------\r\n"]
[316.936, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[316.946, "o", "        The sparse code factor in the matrix factorization.\r\n"]
[316.956, "o", "\r\n"]
[316.966, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[316.976, "o", "        The dictionary factor in the matrix factorization.\r\n"]
[316.986, "o", "\r\n"]
[316.996, "o", "    errors : array\r\n"]
[317.006, "o", "        Vector of errors at each iteration.\r\n"]
[317.016, "o", "\r\n"]
[317.026, "o", "    n_iter : int\r\n"]
[317.036, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[317.046, "o", "        set to True.\r\n"]
[317.056, "o", "\r\n"]
[317.066, "o", "    See Also\r\n"]
[317.076, "o", "    --------\r\n"]
[317.086, "o", "    dict_learning_online : Solve a dictionary learning matrix factorization\r\n"]
[317.096, "o", "        problem online.\r\n"]
[317.106, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[317.116, "o", "    MiniBatchDictionaryLearning : A faster, less accurate version\r\n"]
[317.126, "o", "        of the dictionary learning algorithm.\r\n"]
[317.136, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[317.146, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[317.156, "o", "    \"\"\"\r\n"]
[317.166, "o", "    estimator = DictionaryLearning(\r\n"]
[317.176, "o", "        n_components=n_components,\r\n"]
[317.186, "o", "        alpha=alpha,\r\n"]
[317.196, "o", "        max_iter=max_iter,\r\n"]
[317.206, "o", "        tol=tol,\r\n"]
[317.216, "o", "        fit_algorithm=method,\r\n"]
[317.226, "o", "        n_jobs=n_jobs,\r\n"]
[317.236, "o", "        dict_init=dict_init,\r\n"]
[317.246, "o", "        callback=callback,\r\n"]
[317.256, "o", "        code_init=code_init,\r\n"]
[317.266, "o", "        verbose=verbose,\r\n"]
[317.276, "o", "        random_state=random_state,\r\n"]
[317.286, "o", "        positive_code=positive_code,\r\n"]
[317.296, "o", "        positive_dict=positive_dict,\r\n"]
[317.306, "o", "        transform_max_iter=method_max_iter,\r\n"]
[317.316, "o", "    )\r\n"]
[317.326, "o", "    code = estimator.fit_transform(X)\r\n"]
[317.336, "o", "    if return_n_iter:\r\n"]
[317.346, "o", "        return (\r\n"]
[317.356, "o", "            code,\r\n"]
[317.366, "o", "            estimator.components_,\r\n"]
[317.376, "o", "            estimator.error_,\r\n"]
[317.386, "o", "            estimator.n_iter_,\r\n"]
[317.396, "o", "        )\r\n"]
[317.406, "o", "    return code, estimator.components_, estimator.error_\r\n"]
[317.416, "o", "\r\n"]
[317.426, "o", "\r\n"]
[317.436, "o", "class _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n"]
[317.446, "o", "    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n"]
[317.456, "o", "\r\n"]
[317.466, "o", "    def __init__(\r\n"]
[317.476, "o", "        self,\r\n"]
[317.486, "o", "        transform_algorithm,\r\n"]
[317.496, "o", "        transform_n_nonzero_coefs,\r\n"]
[317.506, "o", "        transform_alpha,\r\n"]
[317.516, "o", "        split_sign,\r\n"]
[317.526, "o", "        n_jobs,\r\n"]
[317.536, "o", "        positive_code,\r\n"]
[317.546, "o", "        transform_max_iter,\r\n"]
[317.556, "o", "    ):\r\n"]
[317.566, "o", "        self.transform_algorithm = transform_algorithm\r\n"]
[317.576, "o", "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n"]
[317.586, "o", "        self.transform_alpha = transform_alpha\r\n"]
[317.596, "o", "        self.transform_max_iter = transform_max_iter\r\n"]
[317.606, "o", "        self.split_sign = split_sign\r\n"]
[317.616, "o", "        self.n_jobs = n_jobs\r\n"]
[317.626, "o", "        self.positive_code = positive_code\r\n"]
[317.636, "o", "\r\n"]
[317.646, "o", "    def _transform(self, X, dictionary):\r\n"]
[317.656, "o", "        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n"]
[317.666, "o", "        SparseCoder.\"\"\"\r\n"]
[317.676, "o", "        X = self._validate_data(X, reset=False)\r\n"]
[317.686, "o", "\r\n"]
[317.696, "o", "        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n"]
[317.706, "o", "            transform_alpha = self.alpha\r\n"]
[317.716, "o", "        else:\r\n"]
[317.726, "o", "            transform_alpha = self.transform_alpha\r\n"]
[317.736, "o", "\r\n"]
[317.746, "o", "        code = sparse_encode(\r\n"]
[317.756, "o", "            X,\r\n"]
[317.766, "o", "            dictionary,\r\n"]
[317.776, "o", "            algorithm=self.transform_algorithm,\r\n"]
[317.786, "o", "            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n"]
[317.796, "o", "            alpha=transform_alpha,\r\n"]
[317.806, "o", "            max_iter=self.transform_max_iter,\r\n"]
[317.816, "o", "            n_jobs=self.n_jobs,\r\n"]
[317.826, "o", "            positive=self.positive_code,\r\n"]
[317.836, "o", "        )\r\n"]
[317.846, "o", "\r\n"]
[317.856, "o", "        if self.split_sign:\r\n"]
[317.866, "o", "            # feature vector is split into a positive and negative side\r\n"]
[317.876, "o", "            n_samples, n_features = code.shape\r\n"]
[317.886, "o", "            split_code = np.empty((n_samples, 2 * n_features))\r\n"]
[317.896, "o", "            split_code[:, :n_features] = np.maximum(code, 0)\r\n"]
[317.906, "o", "            split_code[:, n_features:] = -np.minimum(code, 0)\r\n"]
[317.916, "o", "            code = split_code\r\n"]
[317.926, "o", "\r\n"]
[317.936, "o", "        return code\r\n"]
[317.946, "o", "\r\n"]
[317.956, "o", "    def transform(self, X):\r\n"]
[317.966, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[317.976, "o", "\r\n"]
[317.986, "o", "        Coding method is determined by the object parameter\r\n"]
[317.996, "o", "        `transform_algorithm`.\r\n"]
[318.006, "o", "\r\n"]
[318.016, "o", "        Parameters\r\n"]
[318.026, "o", "        ----------\r\n"]
[318.036, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[318.046, "o", "            Test data to be transformed, must have the same number of\r\n"]
[318.056, "o", "            features as the data used to train the model.\r\n"]
[318.066, "o", "\r\n"]
[318.076, "o", "        Returns\r\n"]
[318.086, "o", "        -------\r\n"]
[318.096, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[318.106, "o", "            Transformed data.\r\n"]
[318.116, "o", "        \"\"\"\r\n"]
[318.126, "o", "        check_is_fitted(self)\r\n"]
[318.136, "o", "        return self._transform(X, self.components_)\r\n"]
[318.146, "o", "\r\n"]
[318.156, "o", "\r\n"]
[318.166, "o", "class SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n"]
[318.176, "o", "    \"\"\"Sparse coding.\r\n"]
[318.186, "o", "\r\n"]
[318.196, "o", "    Finds a sparse representation of data against a fixed, precomputed\r\n"]
[318.206, "o", "    dictionary.\r\n"]
[318.216, "o", "\r\n"]
[318.226, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[318.236, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[318.246, "o", "\r\n"]
[318.256, "o", "        X ~= code * dictionary\r\n"]
[318.266, "o", "\r\n"]
[318.276, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[318.286, "o", "\r\n"]
[318.296, "o", "    Parameters\r\n"]
[318.306, "o", "    ----------\r\n"]
[318.316, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[318.326, "o", "        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n"]
[318.336, "o", "        normalized to unit norm.\r\n"]
[318.346, "o", "\r\n"]
[318.356, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[318.366, "o", "            'threshold'}, default='omp'\r\n"]
[318.376, "o", "        Algorithm used to transform the data:\r\n"]
[318.386, "o", "\r\n"]
[318.396, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[318.406, "o", "          (`linear_model.lars_path`);\r\n"]
[318.416, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[318.426, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[318.436, "o", "          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n"]
[318.446, "o", "          the estimated components are sparse;\r\n"]
[318.456, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[318.466, "o", "          solution;\r\n"]
[318.476, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[318.486, "o", "          the projection ``dictionary * X'``.\r\n"]
[318.496, "o", "\r\n"]
[318.506, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[318.516, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[318.526, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[318.536, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[318.546, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[318.556, "o", "\r\n"]
[318.566, "o", "    transform_alpha : float, default=None\r\n"]
[318.576, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[318.586, "o", "        penalty applied to the L1 norm.\r\n"]
[318.596, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[318.606, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[318.616, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[318.626, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[318.636, "o", "        `n_nonzero_coefs`.\r\n"]
[318.646, "o", "        If `None`, default to 1.\r\n"]
[318.656, "o", "\r\n"]
[318.666, "o", "    split_sign : bool, default=False\r\n"]
[318.676, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[318.686, "o", "        its negative part and its positive part. This can improve the\r\n"]
[318.696, "o", "        performance of downstream classifiers.\r\n"]
[318.706, "o", "\r\n"]
[318.716, "o", "    n_jobs : int, default=None\r\n"]
[318.726, "o", "        Number of parallel jobs to run.\r\n"]
[318.736, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[318.746, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[318.756, "o", "        for more details.\r\n"]
[318.766, "o", "\r\n"]
[318.776, "o", "    positive_code : bool, default=False\r\n"]
[318.786, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[318.796, "o", "\r\n"]
[318.806, "o", "        .. versionadded:: 0.20\r\n"]
[318.816, "o", "\r\n"]
[318.826, "o", "    transform_max_iter : int, default=1000\r\n"]
[318.836, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[318.846, "o", "        `lasso_lars`.\r\n"]
[318.856, "o", "\r\n"]
[318.866, "o", "        .. versionadded:: 0.22\r\n"]
[318.876, "o", "\r\n"]
[318.886, "o", "    Attributes\r\n"]
[318.896, "o", "    ----------\r\n"]
[318.906, "o", "    n_components_ : int\r\n"]
[318.916, "o", "        Number of atoms.\r\n"]
[318.926, "o", "\r\n"]
[318.936, "o", "    n_features_in_ : int\r\n"]
[318.946, "o", "        Number of features seen during :term:`fit`.\r\n"]
[318.956, "o", "\r\n"]
[318.966, "o", "        .. versionadded:: 0.24\r\n"]
[318.976, "o", "\r\n"]
[318.986, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[318.996, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[319.006, "o", "        has feature names that are all strings.\r\n"]
[319.016, "o", "\r\n"]
[319.026, "o", "        .. versionadded:: 1.0\r\n"]
[319.036, "o", "\r\n"]
[319.046, "o", "    See Also\r\n"]
[319.056, "o", "    --------\r\n"]
[319.066, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[319.076, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n"]
[319.086, "o", "        dictionary learning algorithm.\r\n"]
[319.096, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[319.106, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[319.116, "o", "    sparse_encode : Sparse coding where each row of the result is the solution\r\n"]
[319.126, "o", "        to a sparse coding problem.\r\n"]
[319.136, "o", "\r\n"]
[319.146, "o", "    Examples\r\n"]
[319.156, "o", "    --------\r\n"]
[319.166, "o", "    >>> import numpy as np\r\n"]
[319.176, "o", "    >>> from sklearn.decomposition import SparseCoder\r\n"]
[319.186, "o", "    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n"]
[319.196, "o", "    >>> dictionary = np.array(\r\n"]
[319.206, "o", "    ...     [[0, 1, 0],\r\n"]
[319.216, "o", "    ...      [-1, -1, 2],\r\n"]
[319.226, "o", "    ...      [1, 1, 1],\r\n"]
[319.236, "o", "    ...      [0, 1, 1],\r\n"]
[319.246, "o", "    ...      [0, 2, 1]],\r\n"]
[319.256, "o", "    ...    dtype=np.float64\r\n"]
[319.266, "o", "    ... )\r\n"]
[319.276, "o", "    >>> coder = SparseCoder(\r\n"]
[319.286, "o", "    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n"]
[319.296, "o", "    ...     transform_alpha=1e-10,\r\n"]
[319.306, "o", "    ... )\r\n"]
[319.316, "o", "    >>> coder.transform(X)\r\n"]
[319.326, "o", "    array([[ 0.,  0., -1.,  0.,  0.],\r\n"]
[319.336, "o", "           [ 0.,  1.,  1.,  0.,  0.]])\r\n"]
[319.346, "o", "    \"\"\"\r\n"]
[319.356, "o", "\r\n"]
[319.366, "o", "    _required_parameters = [\"dictionary\"]\r\n"]
[319.376, "o", "\r\n"]
[319.386, "o", "    def __init__(\r\n"]
[319.396, "o", "        self,\r\n"]
[319.406, "o", "        dictionary,\r\n"]
[319.416, "o", "        *,\r\n"]
[319.426, "o", "        transform_algorithm=\"omp\",\r\n"]
[319.436, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[319.446, "o", "        transform_alpha=None,\r\n"]
[319.456, "o", "        split_sign=False,\r\n"]
[319.466, "o", "        n_jobs=None,\r\n"]
[319.476, "o", "        positive_code=False,\r\n"]
[319.486, "o", "        transform_max_iter=1000,\r\n"]
[319.496, "o", "    ):\r\n"]
[319.506, "o", "        super().__init__(\r\n"]
[319.516, "o", "            transform_algorithm,\r\n"]
[319.526, "o", "            transform_n_nonzero_coefs,\r\n"]
[319.536, "o", "            transform_alpha,\r\n"]
[319.546, "o", "            split_sign,\r\n"]
[319.556, "o", "            n_jobs,\r\n"]
[319.566, "o", "            positive_code,\r\n"]
[319.576, "o", "            transform_max_iter,\r\n"]
[319.586, "o", "        )\r\n"]
[319.596, "o", "        self.dictionary = dictionary\r\n"]
[319.606, "o", "\r\n"]
[319.616, "o", "    def fit(self, X, y=None):\r\n"]
[319.626, "o", "        \"\"\"Do nothing and return the estimator unchanged.\r\n"]
[319.636, "o", "\r\n"]
[319.646, "o", "        This method is just there to implement the usual API and hence\r\n"]
[319.656, "o", "        work in pipelines.\r\n"]
[319.666, "o", "\r\n"]
[319.676, "o", "        Parameters\r\n"]
[319.686, "o", "        ----------\r\n"]
[319.696, "o", "        X : Ignored\r\n"]
[319.706, "o", "            Not used, present for API consistency by convention.\r\n"]
[319.716, "o", "\r\n"]
[319.726, "o", "        y : Ignored\r\n"]
[319.736, "o", "            Not used, present for API consistency by convention.\r\n"]
[319.746, "o", "\r\n"]
[319.756, "o", "        Returns\r\n"]
[319.766, "o", "        -------\r\n"]
[319.776, "o", "        self : object\r\n"]
[319.786, "o", "            Returns the instance itself.\r\n"]
[319.796, "o", "        \"\"\"\r\n"]
[319.806, "o", "        return self\r\n"]
[319.816, "o", "\r\n"]
[319.826, "o", "    def transform(self, X, y=None):\r\n"]
[319.836, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[319.846, "o", "\r\n"]
[319.856, "o", "        Coding method is determined by the object parameter\r\n"]
[319.866, "o", "        `transform_algorithm`.\r\n"]
[319.876, "o", "\r\n"]
[319.886, "o", "        Parameters\r\n"]
[319.896, "o", "        ----------\r\n"]
[319.906, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[319.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[319.926, "o", "            and `n_features` is the number of features.\r\n"]
[319.936, "o", "\r\n"]
[320.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[320.002, "i", "sed -n '1800,2600p' sklearn/decomposition/_dict_learning.py\r"]
[320.004, "o", "sed -n '1800,2600p' sklearn/decomposition/_dict_learning.py\r\n"]
[320.016, "o", "\u001b[?2004l\r\n"]
[320.026, "o", "        self.fit_transform(X)\r\n"]
[320.036, "o", "        return self\r\n"]
[320.046, "o", "\r\n"]
[320.056, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[320.066, "o", "    def fit_transform(self, X, y=None):\r\n"]
[320.076, "o", "        \"\"\"Fit the model from data in X and return the transformed data.\r\n"]
[320.086, "o", "\r\n"]
[320.096, "o", "        Parameters\r\n"]
[320.106, "o", "        ----------\r\n"]
[320.116, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[320.126, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[320.136, "o", "            and `n_features` is the number of features.\r\n"]
[320.146, "o", "\r\n"]
[320.156, "o", "        y : Ignored\r\n"]
[320.166, "o", "            Not used, present for API consistency by convention.\r\n"]
[320.176, "o", "\r\n"]
[320.186, "o", "        Returns\r\n"]
[320.196, "o", "        -------\r\n"]
[320.206, "o", "        V : ndarray of shape (n_samples, n_components)\r\n"]
[320.216, "o", "            Transformed data.\r\n"]
[320.226, "o", "        \"\"\"\r\n"]
[320.236, "o", "        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n"]
[320.246, "o", "\r\n"]
[320.256, "o", "        method = \"lasso_\" + self.fit_algorithm\r\n"]
[320.266, "o", "\r\n"]
[320.276, "o", "        random_state = check_random_state(self.random_state)\r\n"]
[320.286, "o", "        X = self._validate_data(X)\r\n"]
[320.296, "o", "\r\n"]
[320.306, "o", "        if self.n_components is None:\r\n"]
[320.316, "o", "            n_components = X.shape[1]\r\n"]
[320.326, "o", "        else:\r\n"]
[320.336, "o", "            n_components = self.n_components\r\n"]
[320.346, "o", "\r\n"]
[320.356, "o", "        V, U, E, self.n_iter_ = _dict_learning(\r\n"]
[320.366, "o", "            X,\r\n"]
[320.376, "o", "            n_components,\r\n"]
[320.386, "o", "            alpha=self.alpha,\r\n"]
[320.396, "o", "            tol=self.tol,\r\n"]
[320.406, "o", "            max_iter=self.max_iter,\r\n"]
[320.416, "o", "            method=method,\r\n"]
[320.426, "o", "            method_max_iter=self.transform_max_iter,\r\n"]
[320.436, "o", "            n_jobs=self.n_jobs,\r\n"]
[320.446, "o", "            code_init=self.code_init,\r\n"]
[320.456, "o", "            dict_init=self.dict_init,\r\n"]
[320.466, "o", "            callback=self.callback,\r\n"]
[320.476, "o", "            verbose=self.verbose,\r\n"]
[320.486, "o", "            random_state=random_state,\r\n"]
[320.496, "o", "            return_n_iter=True,\r\n"]
[320.506, "o", "            positive_dict=self.positive_dict,\r\n"]
[320.516, "o", "            positive_code=self.positive_code,\r\n"]
[320.526, "o", "        )\r\n"]
[320.536, "o", "        self.components_ = U\r\n"]
[320.546, "o", "        self.error_ = E\r\n"]
[320.556, "o", "\r\n"]
[320.566, "o", "        return V\r\n"]
[320.576, "o", "\r\n"]
[320.586, "o", "    @property\r\n"]
[320.596, "o", "    def _n_features_out(self):\r\n"]
[320.606, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[320.616, "o", "        return self.components_.shape[0]\r\n"]
[320.626, "o", "\r\n"]
[320.636, "o", "    def _more_tags(self):\r\n"]
[320.646, "o", "        return {\r\n"]
[320.656, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[320.666, "o", "        }\r\n"]
[320.676, "o", "\r\n"]
[320.686, "o", "\r\n"]
[320.696, "o", "class MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[320.706, "o", "    \"\"\"Mini-batch dictionary learning.\r\n"]
[320.716, "o", "\r\n"]
[320.726, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[320.736, "o", "    encoding the fitted data.\r\n"]
[320.746, "o", "\r\n"]
[320.756, "o", "    Solves the optimization problem::\r\n"]
[320.766, "o", "\r\n"]
[320.776, "o", "       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[320.786, "o", "                    (U,V)\r\n"]
[320.796, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[320.806, "o", "\r\n"]
[320.816, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[320.826, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[320.836, "o", "    of all the entries in the matrix.\r\n"]
[320.846, "o", "\r\n"]
[320.856, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[320.866, "o", "\r\n"]
[320.876, "o", "    Parameters\r\n"]
[320.886, "o", "    ----------\r\n"]
[320.896, "o", "    n_components : int, default=None\r\n"]
[320.906, "o", "        Number of dictionary elements to extract.\r\n"]
[320.916, "o", "\r\n"]
[320.926, "o", "    alpha : float, default=1\r\n"]
[320.936, "o", "        Sparsity controlling parameter.\r\n"]
[320.946, "o", "\r\n"]
[320.956, "o", "    n_iter : int, default=1000\r\n"]
[320.966, "o", "        Total number of iterations over data batches to perform.\r\n"]
[320.976, "o", "\r\n"]
[320.986, "o", "        .. deprecated:: 1.1\r\n"]
[320.996, "o", "           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[321.006, "o", "           ``max_iter`` instead.\r\n"]
[321.016, "o", "\r\n"]
[321.026, "o", "    max_iter : int, default=None\r\n"]
[321.036, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[321.046, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[321.056, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[321.066, "o", "\r\n"]
[321.076, "o", "        .. versionadded:: 1.1\r\n"]
[321.086, "o", "\r\n"]
[321.096, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[321.106, "o", "        The algorithm used:\r\n"]
[321.116, "o", "\r\n"]
[321.126, "o", "        - `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[321.136, "o", "          problem (`linear_model.lars_path`)\r\n"]
[321.146, "o", "        - `'cd'`: uses the coordinate descent method to compute the\r\n"]
[321.156, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[321.166, "o", "          the estimated components are sparse.\r\n"]
[321.176, "o", "\r\n"]
[321.186, "o", "    n_jobs : int, default=None\r\n"]
[321.196, "o", "        Number of parallel jobs to run.\r\n"]
[321.206, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[321.216, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[321.226, "o", "        for more details.\r\n"]
[321.236, "o", "\r\n"]
[321.246, "o", "    batch_size : int, default=256\r\n"]
[321.256, "o", "        Number of samples in each mini-batch.\r\n"]
[321.266, "o", "\r\n"]
[321.276, "o", "        .. versionchanged:: 1.3\r\n"]
[321.286, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[321.296, "o", "\r\n"]
[321.306, "o", "    shuffle : bool, default=True\r\n"]
[321.316, "o", "        Whether to shuffle the samples before forming batches.\r\n"]
[321.326, "o", "\r\n"]
[321.336, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[321.346, "o", "        Initial value of the dictionary for warm restart scenarios.\r\n"]
[321.356, "o", "\r\n"]
[321.366, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[321.376, "o", "            'threshold'}, default='omp'\r\n"]
[321.386, "o", "        Algorithm used to transform the data:\r\n"]
[321.396, "o", "\r\n"]
[321.406, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[321.416, "o", "          (`linear_model.lars_path`);\r\n"]
[321.426, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[321.436, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[321.446, "o", "          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n"]
[321.456, "o", "          if the estimated components are sparse.\r\n"]
[321.466, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[321.476, "o", "          solution.\r\n"]
[321.486, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[321.496, "o", "          the projection ``dictionary * X'``.\r\n"]
[321.506, "o", "\r\n"]
[321.516, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[321.526, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[321.536, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[321.546, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[321.556, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[321.566, "o", "\r\n"]
[321.576, "o", "    transform_alpha : float, default=None\r\n"]
[321.586, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[321.596, "o", "        penalty applied to the L1 norm.\r\n"]
[321.606, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[321.616, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[321.626, "o", "        If `None`, defaults to `alpha`.\r\n"]
[321.636, "o", "\r\n"]
[321.646, "o", "        .. versionchanged:: 1.2\r\n"]
[321.656, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[321.666, "o", "\r\n"]
[321.676, "o", "    verbose : bool or int, default=False\r\n"]
[321.686, "o", "        To control the verbosity of the procedure.\r\n"]
[321.696, "o", "\r\n"]
[321.706, "o", "    split_sign : bool, default=False\r\n"]
[321.716, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[321.726, "o", "        its negative part and its positive part. This can improve the\r\n"]
[321.736, "o", "        performance of downstream classifiers.\r\n"]
[321.746, "o", "\r\n"]
[321.756, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[321.766, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[321.776, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[321.786, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[321.796, "o", "        results across multiple function calls.\r\n"]
[321.806, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[321.816, "o", "\r\n"]
[321.826, "o", "    positive_code : bool, default=False\r\n"]
[321.836, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[321.846, "o", "\r\n"]
[321.856, "o", "        .. versionadded:: 0.20\r\n"]
[321.866, "o", "\r\n"]
[321.876, "o", "    positive_dict : bool, default=False\r\n"]
[321.886, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[321.896, "o", "\r\n"]
[321.906, "o", "        .. versionadded:: 0.20\r\n"]
[321.916, "o", "\r\n"]
[321.926, "o", "    transform_max_iter : int, default=1000\r\n"]
[321.936, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[321.946, "o", "        `'lasso_lars'`.\r\n"]
[321.956, "o", "\r\n"]
[321.966, "o", "        .. versionadded:: 0.22\r\n"]
[321.976, "o", "\r\n"]
[321.986, "o", "    callback : callable, default=None\r\n"]
[321.996, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[322.006, "o", "\r\n"]
[322.016, "o", "        .. versionadded:: 1.1\r\n"]
[322.026, "o", "\r\n"]
[322.036, "o", "    tol : float, default=1e-3\r\n"]
[322.046, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[322.056, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[322.066, "o", "\r\n"]
[322.076, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[322.086, "o", "        `tol` to 0.0.\r\n"]
[322.096, "o", "\r\n"]
[322.106, "o", "        .. versionadded:: 1.1\r\n"]
[322.116, "o", "\r\n"]
[322.126, "o", "    max_no_improvement : int, default=10\r\n"]
[322.136, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[322.146, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[322.156, "o", "        `max_iter` is not None.\r\n"]
[322.166, "o", "\r\n"]
[322.176, "o", "        To disable convergence detection based on cost function, set\r\n"]
[322.186, "o", "        `max_no_improvement` to None.\r\n"]
[322.196, "o", "\r\n"]
[322.206, "o", "        .. versionadded:: 1.1\r\n"]
[322.216, "o", "\r\n"]
[322.226, "o", "    Attributes\r\n"]
[322.236, "o", "    ----------\r\n"]
[322.246, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[322.256, "o", "        Components extracted from the data.\r\n"]
[322.266, "o", "\r\n"]
[322.276, "o", "    n_features_in_ : int\r\n"]
[322.286, "o", "        Number of features seen during :term:`fit`.\r\n"]
[322.296, "o", "\r\n"]
[322.306, "o", "        .. versionadded:: 0.24\r\n"]
[322.316, "o", "\r\n"]
[322.326, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[322.336, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[322.346, "o", "        has feature names that are all strings.\r\n"]
[322.356, "o", "\r\n"]
[322.366, "o", "        .. versionadded:: 1.0\r\n"]
[322.376, "o", "\r\n"]
[322.386, "o", "    n_iter_ : int\r\n"]
[322.396, "o", "        Number of iterations over the full dataset.\r\n"]
[322.406, "o", "\r\n"]
[322.416, "o", "    n_steps_ : int\r\n"]
[322.426, "o", "        Number of mini-batches processed.\r\n"]
[322.436, "o", "\r\n"]
[322.446, "o", "        .. versionadded:: 1.1\r\n"]
[322.456, "o", "\r\n"]
[322.466, "o", "    See Also\r\n"]
[322.476, "o", "    --------\r\n"]
[322.486, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[322.496, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[322.506, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[322.516, "o", "        precomputed dictionary.\r\n"]
[322.526, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[322.536, "o", "\r\n"]
[322.546, "o", "    References\r\n"]
[322.556, "o", "    ----------\r\n"]
[322.566, "o", "\r\n"]
[322.576, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[322.586, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[322.596, "o", "\r\n"]
[322.606, "o", "    Examples\r\n"]
[322.616, "o", "    --------\r\n"]
[322.626, "o", "    >>> import numpy as np\r\n"]
[322.636, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[322.646, "o", "    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n"]
[322.656, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[322.666, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[322.676, "o", "    ...     random_state=42)\r\n"]
[322.686, "o", "    >>> dict_learner = MiniBatchDictionaryLearning(\r\n"]
[322.696, "o", "    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n"]
[322.706, "o", "    ...     transform_alpha=0.1, random_state=42)\r\n"]
[322.716, "o", "    >>> X_transformed = dict_learner.fit_transform(X)\r\n"]
[322.726, "o", "\r\n"]
[322.736, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[322.746, "o", "\r\n"]
[322.756, "o", "    >>> np.mean(X_transformed == 0) < 0.5\r\n"]
[322.766, "o", "    True\r\n"]
[322.776, "o", "\r\n"]
[322.786, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[322.796, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[322.806, "o", "    the original signal:\r\n"]
[322.816, "o", "\r\n"]
[322.826, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[322.836, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[322.846, "o", "    0.057...\r\n"]
[322.856, "o", "    \"\"\"\r\n"]
[322.866, "o", "\r\n"]
[322.876, "o", "    _parameter_constraints: dict = {\r\n"]
[322.886, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[322.896, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[322.906, "o", "        \"n_iter\": [\r\n"]
[322.916, "o", "            Interval(Integral, 0, None, closed=\"left\"),\r\n"]
[322.926, "o", "            Hidden(StrOptions({\"deprecated\"})),\r\n"]
[322.936, "o", "        ],\r\n"]
[322.946, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n"]
[322.956, "o", "        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n"]
[322.966, "o", "        \"n_jobs\": [None, Integral],\r\n"]
[322.976, "o", "        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n"]
[322.986, "o", "        \"shuffle\": [\"boolean\"],\r\n"]
[322.996, "o", "        \"dict_init\": [None, np.ndarray],\r\n"]
[323.006, "o", "        \"transform_algorithm\": [\r\n"]
[323.016, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[323.026, "o", "        ],\r\n"]
[323.036, "o", "        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[323.046, "o", "        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[323.056, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[323.066, "o", "        \"split_sign\": [\"boolean\"],\r\n"]
[323.076, "o", "        \"random_state\": [\"random_state\"],\r\n"]
[323.086, "o", "        \"positive_code\": [\"boolean\"],\r\n"]
[323.096, "o", "        \"positive_dict\": [\"boolean\"],\r\n"]
[323.106, "o", "        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[323.116, "o", "        \"callback\": [None, callable],\r\n"]
[323.126, "o", "        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[323.136, "o", "        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n"]
[323.146, "o", "    }\r\n"]
[323.156, "o", "\r\n"]
[323.166, "o", "    def __init__(\r\n"]
[323.176, "o", "        self,\r\n"]
[323.186, "o", "        n_components=None,\r\n"]
[323.196, "o", "        *,\r\n"]
[323.206, "o", "        alpha=1,\r\n"]
[323.216, "o", "        n_iter=\"deprecated\",\r\n"]
[323.226, "o", "        max_iter=None,\r\n"]
[323.236, "o", "        fit_algorithm=\"lars\",\r\n"]
[323.246, "o", "        n_jobs=None,\r\n"]
[323.256, "o", "        batch_size=256,\r\n"]
[323.266, "o", "        shuffle=True,\r\n"]
[323.276, "o", "        dict_init=None,\r\n"]
[323.286, "o", "        transform_algorithm=\"omp\",\r\n"]
[323.296, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[323.306, "o", "        transform_alpha=None,\r\n"]
[323.316, "o", "        verbose=False,\r\n"]
[323.326, "o", "        split_sign=False,\r\n"]
[323.336, "o", "        random_state=None,\r\n"]
[323.346, "o", "        positive_code=False,\r\n"]
[323.356, "o", "        positive_dict=False,\r\n"]
[323.366, "o", "        transform_max_iter=1000,\r\n"]
[323.376, "o", "        callback=None,\r\n"]
[323.386, "o", "        tol=1e-3,\r\n"]
[323.396, "o", "        max_no_improvement=10,\r\n"]
[323.406, "o", "    ):\r\n"]
[323.416, "o", "        super().__init__(\r\n"]
[323.426, "o", "            transform_algorithm,\r\n"]
[323.436, "o", "            transform_n_nonzero_coefs,\r\n"]
[323.446, "o", "            transform_alpha,\r\n"]
[323.456, "o", "            split_sign,\r\n"]
[323.466, "o", "            n_jobs,\r\n"]
[323.476, "o", "            positive_code,\r\n"]
[323.486, "o", "            transform_max_iter,\r\n"]
[323.496, "o", "        )\r\n"]
[323.506, "o", "        self.n_components = n_components\r\n"]
[323.516, "o", "        self.alpha = alpha\r\n"]
[323.526, "o", "        self.n_iter = n_iter\r\n"]
[323.536, "o", "        self.max_iter = max_iter\r\n"]
[323.546, "o", "        self.fit_algorithm = fit_algorithm\r\n"]
[323.556, "o", "        self.dict_init = dict_init\r\n"]
[323.566, "o", "        self.verbose = verbose\r\n"]
[323.576, "o", "        self.shuffle = shuffle\r\n"]
[323.586, "o", "        self.batch_size = batch_size\r\n"]
[323.596, "o", "        self.split_sign = split_sign\r\n"]
[323.606, "o", "        self.random_state = random_state\r\n"]
[323.616, "o", "        self.positive_dict = positive_dict\r\n"]
[323.626, "o", "        self.callback = callback\r\n"]
[323.636, "o", "        self.max_no_improvement = max_no_improvement\r\n"]
[323.646, "o", "        self.tol = tol\r\n"]
[323.656, "o", "\r\n"]
[323.666, "o", "    def _check_params(self, X):\r\n"]
[323.676, "o", "        # n_components\r\n"]
[323.686, "o", "        self._n_components = self.n_components\r\n"]
[323.696, "o", "        if self._n_components is None:\r\n"]
[323.706, "o", "            self._n_components = X.shape[1]\r\n"]
[323.716, "o", "\r\n"]
[323.726, "o", "        # fit_algorithm\r\n"]
[323.736, "o", "        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n"]
[323.746, "o", "        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n"]
[323.756, "o", "\r\n"]
[323.766, "o", "        # batch_size\r\n"]
[323.776, "o", "        self._batch_size = min(self.batch_size, X.shape[0])\r\n"]
[323.786, "o", "\r\n"]
[323.796, "o", "    def _initialize_dict(self, X, random_state):\r\n"]
[323.806, "o", "        \"\"\"Initialization of the dictionary.\"\"\"\r\n"]
[323.816, "o", "        if self.dict_init is not None:\r\n"]
[323.826, "o", "            dictionary = self.dict_init\r\n"]
[323.836, "o", "        else:\r\n"]
[323.846, "o", "            # Init V with SVD of X\r\n"]
[323.856, "o", "            _, S, dictionary = randomized_svd(\r\n"]
[323.866, "o", "                X, self._n_components, random_state=random_state\r\n"]
[323.876, "o", "            )\r\n"]
[323.886, "o", "            dictionary = S[:, np.newaxis] * dictionary\r\n"]
[323.896, "o", "\r\n"]
[323.906, "o", "        if self._n_components <= len(dictionary):\r\n"]
[323.916, "o", "            dictionary = dictionary[: self._n_components, :]\r\n"]
[323.926, "o", "        else:\r\n"]
[323.936, "o", "            dictionary = np.concatenate(\r\n"]
[323.946, "o", "                (\r\n"]
[323.956, "o", "                    dictionary,\r\n"]
[323.966, "o", "                    np.zeros(\r\n"]
[323.976, "o", "                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n"]
[323.986, "o", "                        dtype=dictionary.dtype,\r\n"]
[323.996, "o", "                    ),\r\n"]
[324.006, "o", "                )\r\n"]
[324.016, "o", "            )\r\n"]
[324.026, "o", "\r\n"]
[324.036, "o", "        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n"]
[324.046, "o", "        dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[324.056, "o", "\r\n"]
[324.066, "o", "        return dictionary\r\n"]
[324.076, "o", "\r\n"]
[324.086, "o", "    def _update_inner_stats(self, X, code, batch_size, step):\r\n"]
[324.096, "o", "        \"\"\"Update the inner stats inplace.\"\"\"\r\n"]
[324.106, "o", "        if step < batch_size - 1:\r\n"]
[324.116, "o", "            theta = (step + 1) * batch_size\r\n"]
[324.126, "o", "        else:\r\n"]
[324.136, "o", "            theta = batch_size**2 + step + 1 - batch_size\r\n"]
[324.146, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[324.156, "o", "\r\n"]
[324.166, "o", "        self._A *= beta\r\n"]
[324.176, "o", "        self._A += code.T @ code / batch_size\r\n"]
[324.186, "o", "        self._B *= beta\r\n"]
[324.196, "o", "        self._B += X.T @ code / batch_size\r\n"]
[324.206, "o", "\r\n"]
[324.216, "o", "    def _minibatch_step(self, X, dictionary, random_state, step):\r\n"]
[324.226, "o", "        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n"]
[324.236, "o", "        batch_size = X.shape[0]\r\n"]
[324.246, "o", "\r\n"]
[324.256, "o", "        # Compute code for this batch\r\n"]
[324.266, "o", "        code = _sparse_encode(\r\n"]
[324.276, "o", "            X,\r\n"]
[324.286, "o", "            dictionary,\r\n"]
[324.296, "o", "            algorithm=self._fit_algorithm,\r\n"]
[324.306, "o", "            alpha=self.alpha,\r\n"]
[324.316, "o", "            n_jobs=self.n_jobs,\r\n"]
[324.326, "o", "            positive=self.positive_code,\r\n"]
[324.336, "o", "            max_iter=self.transform_max_iter,\r\n"]
[324.346, "o", "            verbose=self.verbose,\r\n"]
[324.356, "o", "        )\r\n"]
[324.366, "o", "\r\n"]
[324.376, "o", "        batch_cost = (\r\n"]
[324.386, "o", "            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n"]
[324.396, "o", "            + self.alpha * np.sum(np.abs(code))\r\n"]
[324.406, "o", "        ) / batch_size\r\n"]
[324.416, "o", "\r\n"]
[324.426, "o", "        # Update inner stats\r\n"]
[324.436, "o", "        self._update_inner_stats(X, code, batch_size, step)\r\n"]
[324.446, "o", "\r\n"]
[324.456, "o", "        # Update dictionary\r\n"]
[324.466, "o", "        _update_dict(\r\n"]
[324.476, "o", "            dictionary,\r\n"]
[324.486, "o", "            X,\r\n"]
[324.496, "o", "            code,\r\n"]
[324.506, "o", "            self._A,\r\n"]
[324.516, "o", "            self._B,\r\n"]
[324.526, "o", "            verbose=self.verbose,\r\n"]
[324.536, "o", "            random_state=random_state,\r\n"]
[324.546, "o", "            positive=self.positive_dict,\r\n"]
[324.556, "o", "        )\r\n"]
[324.566, "o", "\r\n"]
[324.576, "o", "        return batch_cost\r\n"]
[324.586, "o", "\r\n"]
[324.596, "o", "    def _check_convergence(\r\n"]
[324.606, "o", "        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n"]
[324.616, "o", "    ):\r\n"]
[324.626, "o", "        \"\"\"Helper function to encapsulate the early stopping logic.\r\n"]
[324.636, "o", "\r\n"]
[324.646, "o", "        Early stopping is based on two factors:\r\n"]
[324.656, "o", "        - A small change of the dictionary between two minibatch updates. This is\r\n"]
[324.666, "o", "          controlled by the tol parameter.\r\n"]
[324.676, "o", "        - No more improvement on a smoothed estimate of the objective function for a\r\n"]
[324.686, "o", "          a certain number of consecutive minibatch updates. This is controlled by\r\n"]
[324.696, "o", "          the max_no_improvement parameter.\r\n"]
[324.706, "o", "        \"\"\"\r\n"]
[324.716, "o", "        batch_size = X.shape[0]\r\n"]
[324.726, "o", "\r\n"]
[324.736, "o", "        # counts steps starting from 1 for user friendly verbose mode.\r\n"]
[324.746, "o", "        step = step + 1\r\n"]
[324.756, "o", "\r\n"]
[324.766, "o", "        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n"]
[324.776, "o", "        # too bad value\r\n"]
[324.786, "o", "        if step <= min(100, n_samples / batch_size):\r\n"]
[324.796, "o", "            if self.verbose:\r\n"]
[324.806, "o", "                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n"]
[324.816, "o", "            return False\r\n"]
[324.826, "o", "\r\n"]
[324.836, "o", "        # Compute an Exponentially Weighted Average of the cost function to\r\n"]
[324.846, "o", "        # monitor the convergence while discarding minibatch-local stochastic\r\n"]
[324.856, "o", "        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n"]
[324.866, "o", "        if self._ewa_cost is None:\r\n"]
[324.876, "o", "            self._ewa_cost = batch_cost\r\n"]
[324.886, "o", "        else:\r\n"]
[324.896, "o", "            alpha = batch_size / (n_samples + 1)\r\n"]
[324.906, "o", "            alpha = min(alpha, 1)\r\n"]
[324.916, "o", "            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n"]
[324.926, "o", "\r\n"]
[324.936, "o", "        if self.verbose:\r\n"]
[325.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[325.002, "i", "cd asv_benchmarks\r"]
[325.004, "o", "cd asv_benchmarks\r\n"]
[325.006, "o", "\u001b[?2004l\r\n"]
[330.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[330.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[330.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[331.654, "o", "\u001b[?2004l\r\n"]
[333.302, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[335.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[335.002, "i", "cd ..\r"]
[335.004, "o", "cd ..\r\n"]
[335.006, "o", "\u001b[?2004l\r\n"]
[340.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[340.002, "i", "grep -n \"def _minibatch_step\\|def dict_learning_online\\|def _mini_batch_dictionary_learning\" sklearn/decomposition/\r"]
[340.004, "o", "grep -n \"def _minibatch_step\\|def dict_learning_online\\|def _mini_batch_dictionary_learning\" sklearn/decomposition/\r\n"]
[341.242, "o", "/_dict_learning.py\r\n"]
[342.478, "o", "\u001b[?2004l\r\n"]
[343.714, "o", "\u001b[32m\u001b[K664\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kdef dict_learning_online\u001b[m\u001b[K(\r\n"]
[345.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[345.002, "i", "sed -n '1,1200p' sklearn/decomposition/_dict_learning.py\r"]
[345.004, "o", "sed -n '1,1200p' sklearn/decomposition/_dict_learning.py\r\n"]
[345.016, "o", "\u001b[?2004l\r\n"]
[345.026, "o", "\"\"\" Dictionary learning.\r\n"]
[345.036, "o", "\"\"\"\r\n"]
[345.046, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[345.056, "o", "# License: BSD 3 clause\r\n"]
[345.066, "o", "\r\n"]
[345.076, "o", "import itertools\r\n"]
[345.086, "o", "import sys\r\n"]
[345.096, "o", "import time\r\n"]
[345.106, "o", "import warnings\r\n"]
[345.116, "o", "from math import ceil\r\n"]
[345.126, "o", "from numbers import Integral, Real\r\n"]
[345.136, "o", "\r\n"]
[345.146, "o", "import numpy as np\r\n"]
[345.156, "o", "from joblib import effective_n_jobs\r\n"]
[345.166, "o", "from scipy import linalg\r\n"]
[345.176, "o", "\r\n"]
[345.186, "o", "from ..base import (\r\n"]
[345.196, "o", "    BaseEstimator,\r\n"]
[345.206, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[345.216, "o", "    TransformerMixin,\r\n"]
[345.226, "o", "    _fit_context,\r\n"]
[345.236, "o", ")\r\n"]
[345.246, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[345.256, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[345.266, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[345.276, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[345.286, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[345.296, "o", "from ..utils.validation import check_is_fitted\r\n"]
[345.306, "o", "\r\n"]
[345.316, "o", "\r\n"]
[345.326, "o", "def _check_positive_coding(method, positive):\r\n"]
[345.336, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[345.346, "o", "        raise ValueError(\r\n"]
[345.356, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[345.366, "o", "        )\r\n"]
[345.376, "o", "\r\n"]
[345.386, "o", "\r\n"]
[345.396, "o", "def _sparse_encode_precomputed(\r\n"]
[345.406, "o", "    X,\r\n"]
[345.416, "o", "    dictionary,\r\n"]
[345.426, "o", "    *,\r\n"]
[345.436, "o", "    gram=None,\r\n"]
[345.446, "o", "    cov=None,\r\n"]
[345.456, "o", "    algorithm=\"lasso_lars\",\r\n"]
[345.466, "o", "    regularization=None,\r\n"]
[345.476, "o", "    copy_cov=True,\r\n"]
[345.486, "o", "    init=None,\r\n"]
[345.496, "o", "    max_iter=1000,\r\n"]
[345.506, "o", "    verbose=0,\r\n"]
[345.516, "o", "    positive=False,\r\n"]
[345.526, "o", "):\r\n"]
[345.536, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[345.546, "o", "\r\n"]
[345.556, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[345.566, "o", "\r\n"]
[345.576, "o", "    Parameters\r\n"]
[345.586, "o", "    ----------\r\n"]
[345.596, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[345.606, "o", "        Data matrix.\r\n"]
[345.616, "o", "\r\n"]
[345.626, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[345.636, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[345.646, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[345.656, "o", "\r\n"]
[345.666, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[345.676, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[345.686, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[345.696, "o", "\r\n"]
[345.706, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[345.716, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[345.726, "o", "\r\n"]
[345.736, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[345.746, "o", "            default='lasso_lars'\r\n"]
[345.756, "o", "        The algorithm used:\r\n"]
[345.766, "o", "\r\n"]
[345.776, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[345.786, "o", "          (`linear_model.lars_path`);\r\n"]
[345.796, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[345.806, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[345.816, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[345.826, "o", "          the estimated components are sparse;\r\n"]
[345.836, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[345.846, "o", "          solution;\r\n"]
[345.856, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[345.866, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[345.876, "o", "\r\n"]
[345.886, "o", "    regularization : int or float, default=None\r\n"]
[345.896, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[345.906, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[345.916, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[345.926, "o", "\r\n"]
[345.936, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[345.946, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[345.956, "o", "        `algorithm='lasso_cd'`.\r\n"]
[345.966, "o", "\r\n"]
[345.976, "o", "    max_iter : int, default=1000\r\n"]
[345.986, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[345.996, "o", "        `'lasso_lars'`.\r\n"]
[346.006, "o", "\r\n"]
[346.016, "o", "    copy_cov : bool, default=True\r\n"]
[346.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[346.036, "o", "        be overwritten.\r\n"]
[346.046, "o", "\r\n"]
[346.056, "o", "    verbose : int, default=0\r\n"]
[346.066, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[346.076, "o", "\r\n"]
[346.086, "o", "    positive: bool, default=False\r\n"]
[346.096, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[346.106, "o", "\r\n"]
[346.116, "o", "        .. versionadded:: 0.20\r\n"]
[346.126, "o", "\r\n"]
[346.136, "o", "    Returns\r\n"]
[346.146, "o", "    -------\r\n"]
[346.156, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[346.166, "o", "        The sparse codes.\r\n"]
[346.176, "o", "    \"\"\"\r\n"]
[346.186, "o", "    n_samples, n_features = X.shape\r\n"]
[346.196, "o", "    n_components = dictionary.shape[0]\r\n"]
[346.206, "o", "\r\n"]
[346.216, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[346.226, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[346.236, "o", "        try:\r\n"]
[346.246, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[346.256, "o", "\r\n"]
[346.266, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[346.276, "o", "            # corrects the verbosity level.\r\n"]
[346.286, "o", "            lasso_lars = LassoLars(\r\n"]
[346.296, "o", "                alpha=alpha,\r\n"]
[346.306, "o", "                fit_intercept=False,\r\n"]
[346.316, "o", "                verbose=verbose,\r\n"]
[346.326, "o", "                precompute=gram,\r\n"]
[346.336, "o", "                fit_path=False,\r\n"]
[346.346, "o", "                positive=positive,\r\n"]
[346.356, "o", "                max_iter=max_iter,\r\n"]
[346.366, "o", "            )\r\n"]
[346.376, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[346.386, "o", "            new_code = lasso_lars.coef_\r\n"]
[346.396, "o", "        finally:\r\n"]
[346.406, "o", "            np.seterr(**err_mgt)\r\n"]
[346.416, "o", "\r\n"]
[346.426, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[346.436, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[346.446, "o", "\r\n"]
[346.456, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[346.466, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[346.476, "o", "        # argument that we could pass in from Lasso.\r\n"]
[346.486, "o", "        clf = Lasso(\r\n"]
[346.496, "o", "            alpha=alpha,\r\n"]
[346.506, "o", "            fit_intercept=False,\r\n"]
[346.516, "o", "            precompute=gram,\r\n"]
[346.526, "o", "            max_iter=max_iter,\r\n"]
[346.536, "o", "            warm_start=True,\r\n"]
[346.546, "o", "            positive=positive,\r\n"]
[346.556, "o", "        )\r\n"]
[346.566, "o", "\r\n"]
[346.576, "o", "        if init is not None:\r\n"]
[346.586, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[346.596, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[346.606, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[346.616, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[346.626, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[346.636, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[346.646, "o", "                init = np.array(init)\r\n"]
[346.656, "o", "            clf.coef_ = init\r\n"]
[346.666, "o", "\r\n"]
[346.676, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[346.686, "o", "        new_code = clf.coef_\r\n"]
[346.696, "o", "\r\n"]
[346.706, "o", "    elif algorithm == \"lars\":\r\n"]
[346.716, "o", "        try:\r\n"]
[346.726, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[346.736, "o", "\r\n"]
[346.746, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[346.756, "o", "            # corrects the verbosity level.\r\n"]
[346.766, "o", "            lars = Lars(\r\n"]
[346.776, "o", "                fit_intercept=False,\r\n"]
[346.786, "o", "                verbose=verbose,\r\n"]
[346.796, "o", "                precompute=gram,\r\n"]
[346.806, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[346.816, "o", "                fit_path=False,\r\n"]
[346.826, "o", "            )\r\n"]
[346.836, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[346.846, "o", "            new_code = lars.coef_\r\n"]
[346.856, "o", "        finally:\r\n"]
[346.866, "o", "            np.seterr(**err_mgt)\r\n"]
[346.876, "o", "\r\n"]
[346.886, "o", "    elif algorithm == \"threshold\":\r\n"]
[346.896, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[346.906, "o", "        if positive:\r\n"]
[346.916, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[346.926, "o", "\r\n"]
[346.936, "o", "    elif algorithm == \"omp\":\r\n"]
[346.946, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[346.956, "o", "            Gram=gram,\r\n"]
[346.966, "o", "            Xy=cov,\r\n"]
[346.976, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[346.986, "o", "            tol=None,\r\n"]
[346.996, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[347.006, "o", "            copy_Xy=copy_cov,\r\n"]
[347.016, "o", "        ).T\r\n"]
[347.026, "o", "\r\n"]
[347.036, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[347.046, "o", "\r\n"]
[347.056, "o", "\r\n"]
[347.066, "o", "@validate_params(\r\n"]
[347.076, "o", "    {\r\n"]
[347.086, "o", "        \"X\": [\"array-like\"],\r\n"]
[347.096, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[347.106, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[347.116, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[347.126, "o", "        \"algorithm\": [\r\n"]
[347.136, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[347.146, "o", "        ],\r\n"]
[347.156, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[347.166, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[347.176, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[347.186, "o", "        \"init\": [\"array-like\", None],\r\n"]
[347.196, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[347.206, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[347.216, "o", "        \"check_input\": [\"boolean\"],\r\n"]
[347.226, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[347.236, "o", "        \"positive\": [\"boolean\"],\r\n"]
[347.246, "o", "    },\r\n"]
[347.256, "o", "    prefer_skip_nested_validation=True,\r\n"]
[347.266, "o", ")\r\n"]
[347.276, "o", "# XXX : could be moved to the linear_model module\r\n"]
[347.286, "o", "def sparse_encode(\r\n"]
[347.296, "o", "    X,\r\n"]
[347.306, "o", "    dictionary,\r\n"]
[347.316, "o", "    *,\r\n"]
[347.326, "o", "    gram=None,\r\n"]
[347.336, "o", "    cov=None,\r\n"]
[347.346, "o", "    algorithm=\"lasso_lars\",\r\n"]
[347.356, "o", "    n_nonzero_coefs=None,\r\n"]
[347.366, "o", "    alpha=None,\r\n"]
[347.376, "o", "    copy_cov=True,\r\n"]
[347.386, "o", "    init=None,\r\n"]
[347.396, "o", "    max_iter=1000,\r\n"]
[347.406, "o", "    n_jobs=None,\r\n"]
[347.416, "o", "    check_input=True,\r\n"]
[347.426, "o", "    verbose=0,\r\n"]
[347.436, "o", "    positive=False,\r\n"]
[347.446, "o", "):\r\n"]
[347.456, "o", "    \"\"\"Sparse coding.\r\n"]
[347.466, "o", "\r\n"]
[347.476, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[347.486, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[347.496, "o", "\r\n"]
[347.506, "o", "        X ~= code * dictionary\r\n"]
[347.516, "o", "\r\n"]
[347.526, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[347.536, "o", "\r\n"]
[347.546, "o", "    Parameters\r\n"]
[347.556, "o", "    ----------\r\n"]
[347.566, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[347.576, "o", "        Data matrix.\r\n"]
[347.586, "o", "\r\n"]
[347.596, "o", "    dictionary : array-like of shape (n_components, n_features)\r\n"]
[347.606, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[347.616, "o", "        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[347.626, "o", "        output.\r\n"]
[347.636, "o", "\r\n"]
[347.646, "o", "    gram : array-like of shape (n_components, n_components), default=None\r\n"]
[347.656, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n"]
[347.666, "o", "\r\n"]
[347.676, "o", "    cov : array-like of shape (n_components, n_samples), default=None\r\n"]
[347.686, "o", "        Precomputed covariance, `dictionary' * X`.\r\n"]
[347.696, "o", "\r\n"]
[347.706, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[347.716, "o", "            default='lasso_lars'\r\n"]
[347.726, "o", "        The algorithm used:\r\n"]
[347.736, "o", "\r\n"]
[347.746, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[347.756, "o", "          (`linear_model.lars_path`);\r\n"]
[347.766, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[347.776, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[347.786, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[347.796, "o", "          the estimated components are sparse;\r\n"]
[347.806, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[347.816, "o", "          solution;\r\n"]
[347.826, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[347.836, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[347.846, "o", "\r\n"]
[347.856, "o", "    n_nonzero_coefs : int, default=None\r\n"]
[347.866, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[347.876, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[347.886, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[347.896, "o", "        `n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[347.906, "o", "\r\n"]
[347.916, "o", "    alpha : float, default=None\r\n"]
[347.926, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[347.936, "o", "        penalty applied to the L1 norm.\r\n"]
[347.946, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[347.956, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[347.966, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[347.976, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[347.986, "o", "        `n_nonzero_coefs`.\r\n"]
[347.996, "o", "        If `None`, default to 1.\r\n"]
[348.006, "o", "\r\n"]
[348.016, "o", "    copy_cov : bool, default=True\r\n"]
[348.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[348.036, "o", "        be overwritten.\r\n"]
[348.046, "o", "\r\n"]
[348.056, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[348.066, "o", "        Initialization value of the sparse codes. Only used if\r\n"]
[348.076, "o", "        `algorithm='lasso_cd'`.\r\n"]
[348.086, "o", "\r\n"]
[348.096, "o", "    max_iter : int, default=1000\r\n"]
[348.106, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[348.116, "o", "        `'lasso_lars'`.\r\n"]
[348.126, "o", "\r\n"]
[348.136, "o", "    n_jobs : int, default=None\r\n"]
[348.146, "o", "        Number of parallel jobs to run.\r\n"]
[348.156, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[348.166, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[348.176, "o", "        for more details.\r\n"]
[348.186, "o", "\r\n"]
[348.196, "o", "    check_input : bool, default=True\r\n"]
[348.206, "o", "        If `False`, the input arrays X and dictionary will not be checked.\r\n"]
[348.216, "o", "\r\n"]
[348.226, "o", "    verbose : int, default=0\r\n"]
[348.236, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[348.246, "o", "\r\n"]
[348.256, "o", "    positive : bool, default=False\r\n"]
[348.266, "o", "        Whether to enforce positivity when finding the encoding.\r\n"]
[348.276, "o", "\r\n"]
[348.286, "o", "        .. versionadded:: 0.20\r\n"]
[348.296, "o", "\r\n"]
[348.306, "o", "    Returns\r\n"]
[348.316, "o", "    -------\r\n"]
[348.326, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[348.336, "o", "        The sparse codes.\r\n"]
[348.346, "o", "\r\n"]
[348.356, "o", "    See Also\r\n"]
[348.366, "o", "    --------\r\n"]
[348.376, "o", "    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n"]
[348.386, "o", "        path using LARS algorithm.\r\n"]
[348.396, "o", "    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n"]
[348.406, "o", "    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n"]
[348.416, "o", "    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n"]
[348.426, "o", "        dictionary.\r\n"]
[348.436, "o", "    \"\"\"\r\n"]
[348.446, "o", "    if check_input:\r\n"]
[348.456, "o", "        if algorithm == \"lasso_cd\":\r\n"]
[348.466, "o", "            dictionary = check_array(\r\n"]
[348.476, "o", "                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n"]
[348.486, "o", "            )\r\n"]
[348.496, "o", "            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n"]
[348.506, "o", "        else:\r\n"]
[348.516, "o", "            dictionary = check_array(dictionary)\r\n"]
[348.526, "o", "            X = check_array(X)\r\n"]
[348.536, "o", "\r\n"]
[348.546, "o", "    if dictionary.shape[1] != X.shape[1]:\r\n"]
[348.556, "o", "        raise ValueError(\r\n"]
[348.566, "o", "            \"Dictionary and X have different numbers of features:\"\r\n"]
[348.576, "o", "            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n"]
[348.586, "o", "        )\r\n"]
[348.596, "o", "\r\n"]
[348.606, "o", "    _check_positive_coding(algorithm, positive)\r\n"]
[348.616, "o", "\r\n"]
[348.626, "o", "    return _sparse_encode(\r\n"]
[348.636, "o", "        X,\r\n"]
[348.646, "o", "        dictionary,\r\n"]
[348.656, "o", "        gram=gram,\r\n"]
[348.666, "o", "        cov=cov,\r\n"]
[348.676, "o", "        algorithm=algorithm,\r\n"]
[348.686, "o", "        n_nonzero_coefs=n_nonzero_coefs,\r\n"]
[348.696, "o", "        alpha=alpha,\r\n"]
[348.706, "o", "        copy_cov=copy_cov,\r\n"]
[348.716, "o", "        init=init,\r\n"]
[348.726, "o", "        max_iter=max_iter,\r\n"]
[348.736, "o", "        n_jobs=n_jobs,\r\n"]
[348.746, "o", "        verbose=verbose,\r\n"]
[348.756, "o", "        positive=positive,\r\n"]
[348.766, "o", "    )\r\n"]
[348.776, "o", "\r\n"]
[348.786, "o", "\r\n"]
[348.796, "o", "def _sparse_encode(\r\n"]
[348.806, "o", "    X,\r\n"]
[348.816, "o", "    dictionary,\r\n"]
[348.826, "o", "    *,\r\n"]
[348.836, "o", "    gram=None,\r\n"]
[348.846, "o", "    cov=None,\r\n"]
[348.856, "o", "    algorithm=\"lasso_lars\",\r\n"]
[348.866, "o", "    n_nonzero_coefs=None,\r\n"]
[348.876, "o", "    alpha=None,\r\n"]
[348.886, "o", "    copy_cov=True,\r\n"]
[348.896, "o", "    init=None,\r\n"]
[348.906, "o", "    max_iter=1000,\r\n"]
[348.916, "o", "    n_jobs=None,\r\n"]
[348.926, "o", "    verbose=0,\r\n"]
[348.936, "o", "    positive=False,\r\n"]
[348.946, "o", "):\r\n"]
[348.956, "o", "    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[348.966, "o", "\r\n"]
[348.976, "o", "    n_samples, n_features = X.shape\r\n"]
[348.986, "o", "    n_components = dictionary.shape[0]\r\n"]
[348.996, "o", "\r\n"]
[349.006, "o", "    if algorithm in (\"lars\", \"omp\"):\r\n"]
[349.016, "o", "        regularization = n_nonzero_coefs\r\n"]
[349.026, "o", "        if regularization is None:\r\n"]
[349.036, "o", "            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[349.046, "o", "    else:\r\n"]
[349.056, "o", "        regularization = alpha\r\n"]
[349.066, "o", "        if regularization is None:\r\n"]
[349.076, "o", "            regularization = 1.0\r\n"]
[349.086, "o", "\r\n"]
[349.096, "o", "    if gram is None and algorithm != \"threshold\":\r\n"]
[349.106, "o", "        gram = np.dot(dictionary, dictionary.T)\r\n"]
[349.116, "o", "\r\n"]
[349.126, "o", "    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[349.136, "o", "        copy_cov = False\r\n"]
[349.146, "o", "        cov = np.dot(dictionary, X.T)\r\n"]
[349.156, "o", "\r\n"]
[349.166, "o", "    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[349.176, "o", "        code = _sparse_encode_precomputed(\r\n"]
[349.186, "o", "            X,\r\n"]
[349.196, "o", "            dictionary,\r\n"]
[349.206, "o", "            gram=gram,\r\n"]
[349.216, "o", "            cov=cov,\r\n"]
[349.226, "o", "            algorithm=algorithm,\r\n"]
[349.236, "o", "            regularization=regularization,\r\n"]
[349.246, "o", "            copy_cov=copy_cov,\r\n"]
[349.256, "o", "            init=init,\r\n"]
[349.266, "o", "            max_iter=max_iter,\r\n"]
[349.276, "o", "            verbose=verbose,\r\n"]
[349.286, "o", "            positive=positive,\r\n"]
[349.296, "o", "        )\r\n"]
[349.306, "o", "        return code\r\n"]
[349.316, "o", "\r\n"]
[349.326, "o", "    # Enter parallel code block\r\n"]
[349.336, "o", "    n_samples = X.shape[0]\r\n"]
[349.346, "o", "    n_components = dictionary.shape[0]\r\n"]
[349.356, "o", "    code = np.empty((n_samples, n_components))\r\n"]
[349.366, "o", "    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[349.376, "o", "\r\n"]
[349.386, "o", "    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[349.396, "o", "        delayed(_sparse_encode_precomputed)(\r\n"]
[349.406, "o", "            X[this_slice],\r\n"]
[349.416, "o", "            dictionary,\r\n"]
[349.426, "o", "            gram=gram,\r\n"]
[349.436, "o", "            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[349.446, "o", "            algorithm=algorithm,\r\n"]
[349.456, "o", "            regularization=regularization,\r\n"]
[349.466, "o", "            copy_cov=copy_cov,\r\n"]
[349.476, "o", "            init=init[this_slice] if init is not None else None,\r\n"]
[349.486, "o", "            max_iter=max_iter,\r\n"]
[349.496, "o", "            verbose=verbose,\r\n"]
[349.506, "o", "            positive=positive,\r\n"]
[349.516, "o", "        )\r\n"]
[349.526, "o", "        for this_slice in slices\r\n"]
[349.536, "o", "    )\r\n"]
[349.546, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[349.556, "o", "        code[this_slice] = this_view\r\n"]
[349.566, "o", "    return code\r\n"]
[349.576, "o", "\r\n"]
[349.586, "o", "\r\n"]
[349.596, "o", "def _update_dict(\r\n"]
[349.606, "o", "    dictionary,\r\n"]
[349.616, "o", "    Y,\r\n"]
[349.626, "o", "    code,\r\n"]
[349.636, "o", "    A=None,\r\n"]
[349.646, "o", "    B=None,\r\n"]
[349.656, "o", "    verbose=False,\r\n"]
[349.666, "o", "    random_state=None,\r\n"]
[349.676, "o", "    positive=False,\r\n"]
[349.686, "o", "):\r\n"]
[349.696, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[349.706, "o", "\r\n"]
[349.716, "o", "    Parameters\r\n"]
[349.726, "o", "    ----------\r\n"]
[349.736, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[349.746, "o", "        Value of the dictionary at the previous iteration.\r\n"]
[349.756, "o", "\r\n"]
[349.766, "o", "    Y : ndarray of shape (n_samples, n_features)\r\n"]
[349.776, "o", "        Data matrix.\r\n"]
[349.786, "o", "\r\n"]
[349.796, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[349.806, "o", "        Sparse coding of the data against which to optimize the dictionary.\r\n"]
[349.816, "o", "\r\n"]
[349.826, "o", "    A : ndarray of shape (n_components, n_components), default=None\r\n"]
[349.836, "o", "        Together with `B`, sufficient stats of the online model to update the\r\n"]
[349.846, "o", "        dictionary.\r\n"]
[349.856, "o", "\r\n"]
[349.866, "o", "    B : ndarray of shape (n_features, n_components), default=None\r\n"]
[349.876, "o", "        Together with `A`, sufficient stats of the online model to update the\r\n"]
[349.886, "o", "        dictionary.\r\n"]
[349.896, "o", "\r\n"]
[349.906, "o", "    verbose: bool, default=False\r\n"]
[349.916, "o", "        Degree of output the procedure will print.\r\n"]
[349.926, "o", "\r\n"]
[349.936, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[350.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[350.002, "i", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r"]
[350.004, "o", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r\n"]
[350.016, "o", "\u001b[?2004l\r\n"]
[350.026, "o", "    n_iter : int\r\n"]
[350.036, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[350.046, "o", "        set to True.\r\n"]
[350.056, "o", "\r\n"]
[350.066, "o", "    See Also\r\n"]
[350.076, "o", "    --------\r\n"]
[350.086, "o", "    dict_learning_online : Solve a dictionary learning matrix factorization\r\n"]
[350.096, "o", "        problem online.\r\n"]
[350.106, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[350.116, "o", "    MiniBatchDictionaryLearning : A faster, less accurate version\r\n"]
[350.126, "o", "        of the dictionary learning algorithm.\r\n"]
[350.136, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[350.146, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[350.156, "o", "    \"\"\"\r\n"]
[350.166, "o", "    estimator = DictionaryLearning(\r\n"]
[350.176, "o", "        n_components=n_components,\r\n"]
[350.186, "o", "        alpha=alpha,\r\n"]
[350.196, "o", "        max_iter=max_iter,\r\n"]
[350.206, "o", "        tol=tol,\r\n"]
[350.216, "o", "        fit_algorithm=method,\r\n"]
[350.226, "o", "        n_jobs=n_jobs,\r\n"]
[350.236, "o", "        dict_init=dict_init,\r\n"]
[350.246, "o", "        callback=callback,\r\n"]
[350.256, "o", "        code_init=code_init,\r\n"]
[350.266, "o", "        verbose=verbose,\r\n"]
[350.276, "o", "        random_state=random_state,\r\n"]
[350.286, "o", "        positive_code=positive_code,\r\n"]
[350.296, "o", "        positive_dict=positive_dict,\r\n"]
[350.306, "o", "        transform_max_iter=method_max_iter,\r\n"]
[350.316, "o", "    )\r\n"]
[350.326, "o", "    code = estimator.fit_transform(X)\r\n"]
[350.336, "o", "    if return_n_iter:\r\n"]
[350.346, "o", "        return (\r\n"]
[350.356, "o", "            code,\r\n"]
[350.366, "o", "            estimator.components_,\r\n"]
[350.376, "o", "            estimator.error_,\r\n"]
[350.386, "o", "            estimator.n_iter_,\r\n"]
[350.396, "o", "        )\r\n"]
[350.406, "o", "    return code, estimator.components_, estimator.error_\r\n"]
[350.416, "o", "\r\n"]
[350.426, "o", "\r\n"]
[350.436, "o", "class _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n"]
[350.446, "o", "    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n"]
[350.456, "o", "\r\n"]
[350.466, "o", "    def __init__(\r\n"]
[350.476, "o", "        self,\r\n"]
[350.486, "o", "        transform_algorithm,\r\n"]
[350.496, "o", "        transform_n_nonzero_coefs,\r\n"]
[350.506, "o", "        transform_alpha,\r\n"]
[350.516, "o", "        split_sign,\r\n"]
[350.526, "o", "        n_jobs,\r\n"]
[350.536, "o", "        positive_code,\r\n"]
[350.546, "o", "        transform_max_iter,\r\n"]
[350.556, "o", "    ):\r\n"]
[350.566, "o", "        self.transform_algorithm = transform_algorithm\r\n"]
[350.576, "o", "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n"]
[350.586, "o", "        self.transform_alpha = transform_alpha\r\n"]
[350.596, "o", "        self.transform_max_iter = transform_max_iter\r\n"]
[350.606, "o", "        self.split_sign = split_sign\r\n"]
[350.616, "o", "        self.n_jobs = n_jobs\r\n"]
[350.626, "o", "        self.positive_code = positive_code\r\n"]
[350.636, "o", "\r\n"]
[350.646, "o", "    def _transform(self, X, dictionary):\r\n"]
[350.656, "o", "        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n"]
[350.666, "o", "        SparseCoder.\"\"\"\r\n"]
[350.676, "o", "        X = self._validate_data(X, reset=False)\r\n"]
[350.686, "o", "\r\n"]
[350.696, "o", "        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n"]
[350.706, "o", "            transform_alpha = self.alpha\r\n"]
[350.716, "o", "        else:\r\n"]
[350.726, "o", "            transform_alpha = self.transform_alpha\r\n"]
[350.736, "o", "\r\n"]
[350.746, "o", "        code = sparse_encode(\r\n"]
[350.756, "o", "            X,\r\n"]
[350.766, "o", "            dictionary,\r\n"]
[350.776, "o", "            algorithm=self.transform_algorithm,\r\n"]
[350.786, "o", "            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n"]
[350.796, "o", "            alpha=transform_alpha,\r\n"]
[350.806, "o", "            max_iter=self.transform_max_iter,\r\n"]
[350.816, "o", "            n_jobs=self.n_jobs,\r\n"]
[350.826, "o", "            positive=self.positive_code,\r\n"]
[350.836, "o", "        )\r\n"]
[350.846, "o", "\r\n"]
[350.856, "o", "        if self.split_sign:\r\n"]
[350.866, "o", "            # feature vector is split into a positive and negative side\r\n"]
[350.876, "o", "            n_samples, n_features = code.shape\r\n"]
[350.886, "o", "            split_code = np.empty((n_samples, 2 * n_features))\r\n"]
[350.896, "o", "            split_code[:, :n_features] = np.maximum(code, 0)\r\n"]
[350.906, "o", "            split_code[:, n_features:] = -np.minimum(code, 0)\r\n"]
[350.916, "o", "            code = split_code\r\n"]
[350.926, "o", "\r\n"]
[350.936, "o", "        return code\r\n"]
[350.946, "o", "\r\n"]
[350.956, "o", "    def transform(self, X):\r\n"]
[350.966, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[350.976, "o", "\r\n"]
[350.986, "o", "        Coding method is determined by the object parameter\r\n"]
[350.996, "o", "        `transform_algorithm`.\r\n"]
[351.006, "o", "\r\n"]
[351.016, "o", "        Parameters\r\n"]
[351.026, "o", "        ----------\r\n"]
[351.036, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[351.046, "o", "            Test data to be transformed, must have the same number of\r\n"]
[351.056, "o", "            features as the data used to train the model.\r\n"]
[351.066, "o", "\r\n"]
[351.076, "o", "        Returns\r\n"]
[351.086, "o", "        -------\r\n"]
[351.096, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[351.106, "o", "            Transformed data.\r\n"]
[351.116, "o", "        \"\"\"\r\n"]
[351.126, "o", "        check_is_fitted(self)\r\n"]
[351.136, "o", "        return self._transform(X, self.components_)\r\n"]
[351.146, "o", "\r\n"]
[351.156, "o", "\r\n"]
[351.166, "o", "class SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n"]
[351.176, "o", "    \"\"\"Sparse coding.\r\n"]
[351.186, "o", "\r\n"]
[351.196, "o", "    Finds a sparse representation of data against a fixed, precomputed\r\n"]
[351.206, "o", "    dictionary.\r\n"]
[351.216, "o", "\r\n"]
[351.226, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[351.236, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[351.246, "o", "\r\n"]
[351.256, "o", "        X ~= code * dictionary\r\n"]
[351.266, "o", "\r\n"]
[351.276, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[351.286, "o", "\r\n"]
[351.296, "o", "    Parameters\r\n"]
[351.306, "o", "    ----------\r\n"]
[351.316, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[351.326, "o", "        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n"]
[351.336, "o", "        normalized to unit norm.\r\n"]
[351.346, "o", "\r\n"]
[351.356, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[351.366, "o", "            'threshold'}, default='omp'\r\n"]
[351.376, "o", "        Algorithm used to transform the data:\r\n"]
[351.386, "o", "\r\n"]
[351.396, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[351.406, "o", "          (`linear_model.lars_path`);\r\n"]
[351.416, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[351.426, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[351.436, "o", "          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n"]
[351.446, "o", "          the estimated components are sparse;\r\n"]
[351.456, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[351.466, "o", "          solution;\r\n"]
[351.476, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[351.486, "o", "          the projection ``dictionary * X'``.\r\n"]
[351.496, "o", "\r\n"]
[351.506, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[351.516, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[351.526, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[351.536, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[351.546, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[351.556, "o", "\r\n"]
[351.566, "o", "    transform_alpha : float, default=None\r\n"]
[351.576, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[351.586, "o", "        penalty applied to the L1 norm.\r\n"]
[351.596, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[351.606, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[351.616, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[351.626, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[351.636, "o", "        `n_nonzero_coefs`.\r\n"]
[351.646, "o", "        If `None`, default to 1.\r\n"]
[351.656, "o", "\r\n"]
[351.666, "o", "    split_sign : bool, default=False\r\n"]
[351.676, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[351.686, "o", "        its negative part and its positive part. This can improve the\r\n"]
[351.696, "o", "        performance of downstream classifiers.\r\n"]
[351.706, "o", "\r\n"]
[351.716, "o", "    n_jobs : int, default=None\r\n"]
[351.726, "o", "        Number of parallel jobs to run.\r\n"]
[351.736, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[351.746, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[351.756, "o", "        for more details.\r\n"]
[351.766, "o", "\r\n"]
[351.776, "o", "    positive_code : bool, default=False\r\n"]
[351.786, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[351.796, "o", "\r\n"]
[351.806, "o", "        .. versionadded:: 0.20\r\n"]
[351.816, "o", "\r\n"]
[351.826, "o", "    transform_max_iter : int, default=1000\r\n"]
[351.836, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[351.846, "o", "        `lasso_lars`.\r\n"]
[351.856, "o", "\r\n"]
[351.866, "o", "        .. versionadded:: 0.22\r\n"]
[351.876, "o", "\r\n"]
[351.886, "o", "    Attributes\r\n"]
[351.896, "o", "    ----------\r\n"]
[351.906, "o", "    n_components_ : int\r\n"]
[351.916, "o", "        Number of atoms.\r\n"]
[351.926, "o", "\r\n"]
[351.936, "o", "    n_features_in_ : int\r\n"]
[351.946, "o", "        Number of features seen during :term:`fit`.\r\n"]
[351.956, "o", "\r\n"]
[351.966, "o", "        .. versionadded:: 0.24\r\n"]
[351.976, "o", "\r\n"]
[351.986, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[351.996, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[352.006, "o", "        has feature names that are all strings.\r\n"]
[352.016, "o", "\r\n"]
[352.026, "o", "        .. versionadded:: 1.0\r\n"]
[352.036, "o", "\r\n"]
[352.046, "o", "    See Also\r\n"]
[352.056, "o", "    --------\r\n"]
[352.066, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[352.076, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n"]
[352.086, "o", "        dictionary learning algorithm.\r\n"]
[352.096, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[352.106, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[352.116, "o", "    sparse_encode : Sparse coding where each row of the result is the solution\r\n"]
[352.126, "o", "        to a sparse coding problem.\r\n"]
[352.136, "o", "\r\n"]
[352.146, "o", "    Examples\r\n"]
[352.156, "o", "    --------\r\n"]
[352.166, "o", "    >>> import numpy as np\r\n"]
[352.176, "o", "    >>> from sklearn.decomposition import SparseCoder\r\n"]
[352.186, "o", "    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n"]
[352.196, "o", "    >>> dictionary = np.array(\r\n"]
[352.206, "o", "    ...     [[0, 1, 0],\r\n"]
[352.216, "o", "    ...      [-1, -1, 2],\r\n"]
[352.226, "o", "    ...      [1, 1, 1],\r\n"]
[352.236, "o", "    ...      [0, 1, 1],\r\n"]
[352.246, "o", "    ...      [0, 2, 1]],\r\n"]
[352.256, "o", "    ...    dtype=np.float64\r\n"]
[352.266, "o", "    ... )\r\n"]
[352.276, "o", "    >>> coder = SparseCoder(\r\n"]
[352.286, "o", "    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n"]
[352.296, "o", "    ...     transform_alpha=1e-10,\r\n"]
[352.306, "o", "    ... )\r\n"]
[352.316, "o", "    >>> coder.transform(X)\r\n"]
[352.326, "o", "    array([[ 0.,  0., -1.,  0.,  0.],\r\n"]
[352.336, "o", "           [ 0.,  1.,  1.,  0.,  0.]])\r\n"]
[352.346, "o", "    \"\"\"\r\n"]
[352.356, "o", "\r\n"]
[352.366, "o", "    _required_parameters = [\"dictionary\"]\r\n"]
[352.376, "o", "\r\n"]
[352.386, "o", "    def __init__(\r\n"]
[352.396, "o", "        self,\r\n"]
[352.406, "o", "        dictionary,\r\n"]
[352.416, "o", "        *,\r\n"]
[352.426, "o", "        transform_algorithm=\"omp\",\r\n"]
[352.436, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[352.446, "o", "        transform_alpha=None,\r\n"]
[352.456, "o", "        split_sign=False,\r\n"]
[352.466, "o", "        n_jobs=None,\r\n"]
[352.476, "o", "        positive_code=False,\r\n"]
[352.486, "o", "        transform_max_iter=1000,\r\n"]
[352.496, "o", "    ):\r\n"]
[352.506, "o", "        super().__init__(\r\n"]
[352.516, "o", "            transform_algorithm,\r\n"]
[352.526, "o", "            transform_n_nonzero_coefs,\r\n"]
[352.536, "o", "            transform_alpha,\r\n"]
[352.546, "o", "            split_sign,\r\n"]
[352.556, "o", "            n_jobs,\r\n"]
[352.566, "o", "            positive_code,\r\n"]
[352.576, "o", "            transform_max_iter,\r\n"]
[352.586, "o", "        )\r\n"]
[352.596, "o", "        self.dictionary = dictionary\r\n"]
[352.606, "o", "\r\n"]
[352.616, "o", "    def fit(self, X, y=None):\r\n"]
[352.626, "o", "        \"\"\"Do nothing and return the estimator unchanged.\r\n"]
[352.636, "o", "\r\n"]
[352.646, "o", "        This method is just there to implement the usual API and hence\r\n"]
[352.656, "o", "        work in pipelines.\r\n"]
[352.666, "o", "\r\n"]
[352.676, "o", "        Parameters\r\n"]
[352.686, "o", "        ----------\r\n"]
[352.696, "o", "        X : Ignored\r\n"]
[352.706, "o", "            Not used, present for API consistency by convention.\r\n"]
[352.716, "o", "\r\n"]
[352.726, "o", "        y : Ignored\r\n"]
[352.736, "o", "            Not used, present for API consistency by convention.\r\n"]
[352.746, "o", "\r\n"]
[352.756, "o", "        Returns\r\n"]
[352.766, "o", "        -------\r\n"]
[352.776, "o", "        self : object\r\n"]
[352.786, "o", "            Returns the instance itself.\r\n"]
[352.796, "o", "        \"\"\"\r\n"]
[352.806, "o", "        return self\r\n"]
[352.816, "o", "\r\n"]
[352.826, "o", "    def transform(self, X, y=None):\r\n"]
[352.836, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[352.846, "o", "\r\n"]
[352.856, "o", "        Coding method is determined by the object parameter\r\n"]
[352.866, "o", "        `transform_algorithm`.\r\n"]
[352.876, "o", "\r\n"]
[352.886, "o", "        Parameters\r\n"]
[352.896, "o", "        ----------\r\n"]
[352.906, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[352.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[352.926, "o", "            and `n_features` is the number of features.\r\n"]
[352.936, "o", "\r\n"]
[352.946, "o", "        y : Ignored\r\n"]
[352.956, "o", "            Not used, present for API consistency by convention.\r\n"]
[352.966, "o", "\r\n"]
[352.976, "o", "        Returns\r\n"]
[352.986, "o", "        -------\r\n"]
[352.996, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[353.006, "o", "            Transformed data.\r\n"]
[353.016, "o", "        \"\"\"\r\n"]
[353.026, "o", "        return super()._transform(X, self.dictionary)\r\n"]
[353.036, "o", "\r\n"]
[353.046, "o", "    def _more_tags(self):\r\n"]
[353.056, "o", "        return {\r\n"]
[353.066, "o", "            \"requires_fit\": False,\r\n"]
[353.076, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[353.086, "o", "        }\r\n"]
[353.096, "o", "\r\n"]
[353.106, "o", "    @property\r\n"]
[353.116, "o", "    def n_components_(self):\r\n"]
[353.126, "o", "        \"\"\"Number of atoms.\"\"\"\r\n"]
[353.136, "o", "        return self.dictionary.shape[0]\r\n"]
[353.146, "o", "\r\n"]
[353.156, "o", "    @property\r\n"]
[353.166, "o", "    def n_features_in_(self):\r\n"]
[353.176, "o", "        \"\"\"Number of features seen during `fit`.\"\"\"\r\n"]
[353.186, "o", "        return self.dictionary.shape[1]\r\n"]
[353.196, "o", "\r\n"]
[353.206, "o", "    @property\r\n"]
[353.216, "o", "    def _n_features_out(self):\r\n"]
[353.226, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[353.236, "o", "        return self.n_components_\r\n"]
[353.246, "o", "\r\n"]
[353.256, "o", "\r\n"]
[353.266, "o", "class DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[353.276, "o", "    \"\"\"Dictionary learning.\r\n"]
[353.286, "o", "\r\n"]
[353.296, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[353.306, "o", "    encoding the fitted data.\r\n"]
[353.316, "o", "\r\n"]
[353.326, "o", "    Solves the optimization problem::\r\n"]
[353.336, "o", "\r\n"]
[353.346, "o", "        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[353.356, "o", "                    (U,V)\r\n"]
[353.366, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[353.376, "o", "\r\n"]
[353.386, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[353.396, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[353.406, "o", "    of all the entries in the matrix.\r\n"]
[353.416, "o", "\r\n"]
[353.426, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[353.436, "o", "\r\n"]
[353.446, "o", "    Parameters\r\n"]
[353.456, "o", "    ----------\r\n"]
[353.466, "o", "    n_components : int, default=None\r\n"]
[353.476, "o", "        Number of dictionary elements to extract. If None, then ``n_components``\r\n"]
[353.486, "o", "        is set to ``n_features``.\r\n"]
[353.496, "o", "\r\n"]
[353.506, "o", "    alpha : float, default=1.0\r\n"]
[353.516, "o", "        Sparsity controlling parameter.\r\n"]
[353.526, "o", "\r\n"]
[353.536, "o", "    max_iter : int, default=1000\r\n"]
[353.546, "o", "        Maximum number of iterations to perform.\r\n"]
[353.556, "o", "\r\n"]
[353.566, "o", "    tol : float, default=1e-8\r\n"]
[353.576, "o", "        Tolerance for numerical error.\r\n"]
[353.586, "o", "\r\n"]
[353.596, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[353.606, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[353.616, "o", "          problem (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[353.626, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[353.636, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n"]
[353.646, "o", "          faster if the estimated components are sparse.\r\n"]
[353.656, "o", "\r\n"]
[353.666, "o", "        .. versionadded:: 0.17\r\n"]
[353.676, "o", "           *cd* coordinate descent method to improve speed.\r\n"]
[353.686, "o", "\r\n"]
[353.696, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[353.706, "o", "            'threshold'}, default='omp'\r\n"]
[353.716, "o", "        Algorithm used to transform the data:\r\n"]
[353.726, "o", "\r\n"]
[353.736, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[353.746, "o", "          (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[353.756, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[353.766, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[353.776, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n"]
[353.786, "o", "          will be faster if the estimated components are sparse.\r\n"]
[353.796, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[353.806, "o", "          solution.\r\n"]
[353.816, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[353.826, "o", "          the projection ``dictionary * X'``.\r\n"]
[353.836, "o", "\r\n"]
[353.846, "o", "        .. versionadded:: 0.17\r\n"]
[353.856, "o", "           *lasso_cd* coordinate descent method to improve speed.\r\n"]
[353.866, "o", "\r\n"]
[353.876, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[353.886, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[353.896, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[353.906, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[353.916, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[353.926, "o", "\r\n"]
[353.936, "o", "    transform_alpha : float, default=None\r\n"]
[353.946, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[353.956, "o", "        penalty applied to the L1 norm.\r\n"]
[353.966, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[353.976, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[353.986, "o", "        If `None`, defaults to `alpha`.\r\n"]
[353.996, "o", "\r\n"]
[354.006, "o", "        .. versionchanged:: 1.2\r\n"]
[354.016, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[354.026, "o", "\r\n"]
[354.036, "o", "    n_jobs : int or None, default=None\r\n"]
[354.046, "o", "        Number of parallel jobs to run.\r\n"]
[354.056, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[354.066, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[354.076, "o", "        for more details.\r\n"]
[354.086, "o", "\r\n"]
[354.096, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[354.106, "o", "        Initial value for the code, for warm restart. Only used if `code_init`\r\n"]
[354.116, "o", "        and `dict_init` are not None.\r\n"]
[354.126, "o", "\r\n"]
[354.136, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[354.146, "o", "        Initial values for the dictionary, for warm restart. Only used if\r\n"]
[354.156, "o", "        `code_init` and `dict_init` are not None.\r\n"]
[354.166, "o", "\r\n"]
[354.176, "o", "    callback : callable, default=None\r\n"]
[354.186, "o", "        Callable that gets invoked every five iterations.\r\n"]
[354.196, "o", "\r\n"]
[354.206, "o", "        .. versionadded:: 1.3\r\n"]
[354.216, "o", "\r\n"]
[354.226, "o", "    verbose : bool, default=False\r\n"]
[354.236, "o", "        To control the verbosity of the procedure.\r\n"]
[354.246, "o", "\r\n"]
[354.256, "o", "    split_sign : bool, default=False\r\n"]
[354.266, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[354.276, "o", "        its negative part and its positive part. This can improve the\r\n"]
[354.286, "o", "        performance of downstream classifiers.\r\n"]
[354.296, "o", "\r\n"]
[354.306, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[354.316, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[354.326, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[354.336, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[354.346, "o", "        results across multiple function calls.\r\n"]
[354.356, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[354.366, "o", "\r\n"]
[354.376, "o", "    positive_code : bool, default=False\r\n"]
[354.386, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[354.396, "o", "\r\n"]
[354.406, "o", "        .. versionadded:: 0.20\r\n"]
[354.416, "o", "\r\n"]
[354.426, "o", "    positive_dict : bool, default=False\r\n"]
[354.436, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[354.446, "o", "\r\n"]
[354.456, "o", "        .. versionadded:: 0.20\r\n"]
[354.466, "o", "\r\n"]
[354.476, "o", "    transform_max_iter : int, default=1000\r\n"]
[354.486, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[354.496, "o", "        `'lasso_lars'`.\r\n"]
[354.506, "o", "\r\n"]
[354.516, "o", "        .. versionadded:: 0.22\r\n"]
[354.526, "o", "\r\n"]
[354.536, "o", "    Attributes\r\n"]
[354.546, "o", "    ----------\r\n"]
[354.556, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[354.566, "o", "        dictionary atoms extracted from the data\r\n"]
[354.576, "o", "\r\n"]
[354.586, "o", "    error_ : array\r\n"]
[354.596, "o", "        vector of errors at each iteration\r\n"]
[354.606, "o", "\r\n"]
[354.616, "o", "    n_features_in_ : int\r\n"]
[354.626, "o", "        Number of features seen during :term:`fit`.\r\n"]
[354.636, "o", "\r\n"]
[354.646, "o", "        .. versionadded:: 0.24\r\n"]
[354.656, "o", "\r\n"]
[354.666, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[354.676, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[354.686, "o", "        has feature names that are all strings.\r\n"]
[354.696, "o", "\r\n"]
[354.706, "o", "        .. versionadded:: 1.0\r\n"]
[354.716, "o", "\r\n"]
[354.726, "o", "    n_iter_ : int\r\n"]
[354.736, "o", "        Number of iterations run.\r\n"]
[354.746, "o", "\r\n"]
[354.756, "o", "    See Also\r\n"]
[354.766, "o", "    --------\r\n"]
[354.776, "o", "    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n"]
[354.786, "o", "        dictionary learning algorithm.\r\n"]
[354.796, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[354.806, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[354.816, "o", "        precomputed dictionary.\r\n"]
[354.826, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[354.836, "o", "\r\n"]
[354.846, "o", "    References\r\n"]
[354.856, "o", "    ----------\r\n"]
[354.866, "o", "\r\n"]
[354.876, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[354.886, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[354.896, "o", "\r\n"]
[354.906, "o", "    Examples\r\n"]
[354.916, "o", "    --------\r\n"]
[354.926, "o", "    >>> import numpy as np\r\n"]
[354.936, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[355.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[355.002, "i", "sed -n '2000,2600p' sklearn/decomposition/_dict_learning.py\r"]
[355.004, "o", "sed -n '2000,2600p' sklearn/decomposition/_dict_learning.py\r\n"]
[355.016, "o", "\u001b[?2004l\r\n"]
[355.026, "o", "\r\n"]
[355.036, "o", "    tol : float, default=1e-3\r\n"]
[355.046, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[355.056, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[355.066, "o", "\r\n"]
[355.076, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[355.086, "o", "        `tol` to 0.0.\r\n"]
[355.096, "o", "\r\n"]
[355.106, "o", "        .. versionadded:: 1.1\r\n"]
[355.116, "o", "\r\n"]
[355.126, "o", "    max_no_improvement : int, default=10\r\n"]
[355.136, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[355.146, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[355.156, "o", "        `max_iter` is not None.\r\n"]
[355.166, "o", "\r\n"]
[355.176, "o", "        To disable convergence detection based on cost function, set\r\n"]
[355.186, "o", "        `max_no_improvement` to None.\r\n"]
[355.196, "o", "\r\n"]
[355.206, "o", "        .. versionadded:: 1.1\r\n"]
[355.216, "o", "\r\n"]
[355.226, "o", "    Attributes\r\n"]
[355.236, "o", "    ----------\r\n"]
[355.246, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[355.256, "o", "        Components extracted from the data.\r\n"]
[355.266, "o", "\r\n"]
[355.276, "o", "    n_features_in_ : int\r\n"]
[355.286, "o", "        Number of features seen during :term:`fit`.\r\n"]
[355.296, "o", "\r\n"]
[355.306, "o", "        .. versionadded:: 0.24\r\n"]
[355.316, "o", "\r\n"]
[355.326, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[355.336, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[355.346, "o", "        has feature names that are all strings.\r\n"]
[355.356, "o", "\r\n"]
[355.366, "o", "        .. versionadded:: 1.0\r\n"]
[355.376, "o", "\r\n"]
[355.386, "o", "    n_iter_ : int\r\n"]
[355.396, "o", "        Number of iterations over the full dataset.\r\n"]
[355.406, "o", "\r\n"]
[355.416, "o", "    n_steps_ : int\r\n"]
[355.426, "o", "        Number of mini-batches processed.\r\n"]
[355.436, "o", "\r\n"]
[355.446, "o", "        .. versionadded:: 1.1\r\n"]
[355.456, "o", "\r\n"]
[355.466, "o", "    See Also\r\n"]
[355.476, "o", "    --------\r\n"]
[355.486, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[355.496, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[355.506, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[355.516, "o", "        precomputed dictionary.\r\n"]
[355.526, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[355.536, "o", "\r\n"]
[355.546, "o", "    References\r\n"]
[355.556, "o", "    ----------\r\n"]
[355.566, "o", "\r\n"]
[355.576, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[355.586, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[355.596, "o", "\r\n"]
[355.606, "o", "    Examples\r\n"]
[355.616, "o", "    --------\r\n"]
[355.626, "o", "    >>> import numpy as np\r\n"]
[355.636, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[355.646, "o", "    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n"]
[355.656, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[355.666, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[355.676, "o", "    ...     random_state=42)\r\n"]
[355.686, "o", "    >>> dict_learner = MiniBatchDictionaryLearning(\r\n"]
[355.696, "o", "    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n"]
[355.706, "o", "    ...     transform_alpha=0.1, random_state=42)\r\n"]
[355.716, "o", "    >>> X_transformed = dict_learner.fit_transform(X)\r\n"]
[355.726, "o", "\r\n"]
[355.736, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[355.746, "o", "\r\n"]
[355.756, "o", "    >>> np.mean(X_transformed == 0) < 0.5\r\n"]
[355.766, "o", "    True\r\n"]
[355.776, "o", "\r\n"]
[355.786, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[355.796, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[355.806, "o", "    the original signal:\r\n"]
[355.816, "o", "\r\n"]
[355.826, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[355.836, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[355.846, "o", "    0.057...\r\n"]
[355.856, "o", "    \"\"\"\r\n"]
[355.866, "o", "\r\n"]
[355.876, "o", "    _parameter_constraints: dict = {\r\n"]
[355.886, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[355.896, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[355.906, "o", "        \"n_iter\": [\r\n"]
[355.916, "o", "            Interval(Integral, 0, None, closed=\"left\"),\r\n"]
[355.926, "o", "            Hidden(StrOptions({\"deprecated\"})),\r\n"]
[355.936, "o", "        ],\r\n"]
[355.946, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n"]
[355.956, "o", "        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n"]
[355.966, "o", "        \"n_jobs\": [None, Integral],\r\n"]
[355.976, "o", "        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n"]
[355.986, "o", "        \"shuffle\": [\"boolean\"],\r\n"]
[355.996, "o", "        \"dict_init\": [None, np.ndarray],\r\n"]
[356.006, "o", "        \"transform_algorithm\": [\r\n"]
[356.016, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[356.026, "o", "        ],\r\n"]
[356.036, "o", "        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[356.046, "o", "        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[356.056, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[356.066, "o", "        \"split_sign\": [\"boolean\"],\r\n"]
[356.076, "o", "        \"random_state\": [\"random_state\"],\r\n"]
[356.086, "o", "        \"positive_code\": [\"boolean\"],\r\n"]
[356.096, "o", "        \"positive_dict\": [\"boolean\"],\r\n"]
[356.106, "o", "        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[356.116, "o", "        \"callback\": [None, callable],\r\n"]
[356.126, "o", "        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[356.136, "o", "        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n"]
[356.146, "o", "    }\r\n"]
[356.156, "o", "\r\n"]
[356.166, "o", "    def __init__(\r\n"]
[356.176, "o", "        self,\r\n"]
[356.186, "o", "        n_components=None,\r\n"]
[356.196, "o", "        *,\r\n"]
[356.206, "o", "        alpha=1,\r\n"]
[356.216, "o", "        n_iter=\"deprecated\",\r\n"]
[356.226, "o", "        max_iter=None,\r\n"]
[356.236, "o", "        fit_algorithm=\"lars\",\r\n"]
[356.246, "o", "        n_jobs=None,\r\n"]
[356.256, "o", "        batch_size=256,\r\n"]
[356.266, "o", "        shuffle=True,\r\n"]
[356.276, "o", "        dict_init=None,\r\n"]
[356.286, "o", "        transform_algorithm=\"omp\",\r\n"]
[356.296, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[356.306, "o", "        transform_alpha=None,\r\n"]
[356.316, "o", "        verbose=False,\r\n"]
[356.326, "o", "        split_sign=False,\r\n"]
[356.336, "o", "        random_state=None,\r\n"]
[356.346, "o", "        positive_code=False,\r\n"]
[356.356, "o", "        positive_dict=False,\r\n"]
[356.366, "o", "        transform_max_iter=1000,\r\n"]
[356.376, "o", "        callback=None,\r\n"]
[356.386, "o", "        tol=1e-3,\r\n"]
[356.396, "o", "        max_no_improvement=10,\r\n"]
[356.406, "o", "    ):\r\n"]
[356.416, "o", "        super().__init__(\r\n"]
[356.426, "o", "            transform_algorithm,\r\n"]
[356.436, "o", "            transform_n_nonzero_coefs,\r\n"]
[356.446, "o", "            transform_alpha,\r\n"]
[356.456, "o", "            split_sign,\r\n"]
[356.466, "o", "            n_jobs,\r\n"]
[356.476, "o", "            positive_code,\r\n"]
[356.486, "o", "            transform_max_iter,\r\n"]
[356.496, "o", "        )\r\n"]
[356.506, "o", "        self.n_components = n_components\r\n"]
[356.516, "o", "        self.alpha = alpha\r\n"]
[356.526, "o", "        self.n_iter = n_iter\r\n"]
[356.536, "o", "        self.max_iter = max_iter\r\n"]
[356.546, "o", "        self.fit_algorithm = fit_algorithm\r\n"]
[356.556, "o", "        self.dict_init = dict_init\r\n"]
[356.566, "o", "        self.verbose = verbose\r\n"]
[356.576, "o", "        self.shuffle = shuffle\r\n"]
[356.586, "o", "        self.batch_size = batch_size\r\n"]
[356.596, "o", "        self.split_sign = split_sign\r\n"]
[356.606, "o", "        self.random_state = random_state\r\n"]
[356.616, "o", "        self.positive_dict = positive_dict\r\n"]
[356.626, "o", "        self.callback = callback\r\n"]
[356.636, "o", "        self.max_no_improvement = max_no_improvement\r\n"]
[356.646, "o", "        self.tol = tol\r\n"]
[356.656, "o", "\r\n"]
[356.666, "o", "    def _check_params(self, X):\r\n"]
[356.676, "o", "        # n_components\r\n"]
[356.686, "o", "        self._n_components = self.n_components\r\n"]
[356.696, "o", "        if self._n_components is None:\r\n"]
[356.706, "o", "            self._n_components = X.shape[1]\r\n"]
[356.716, "o", "\r\n"]
[356.726, "o", "        # fit_algorithm\r\n"]
[356.736, "o", "        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n"]
[356.746, "o", "        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n"]
[356.756, "o", "\r\n"]
[356.766, "o", "        # batch_size\r\n"]
[356.776, "o", "        self._batch_size = min(self.batch_size, X.shape[0])\r\n"]
[356.786, "o", "\r\n"]
[356.796, "o", "    def _initialize_dict(self, X, random_state):\r\n"]
[356.806, "o", "        \"\"\"Initialization of the dictionary.\"\"\"\r\n"]
[356.816, "o", "        if self.dict_init is not None:\r\n"]
[356.826, "o", "            dictionary = self.dict_init\r\n"]
[356.836, "o", "        else:\r\n"]
[356.846, "o", "            # Init V with SVD of X\r\n"]
[356.856, "o", "            _, S, dictionary = randomized_svd(\r\n"]
[356.866, "o", "                X, self._n_components, random_state=random_state\r\n"]
[356.876, "o", "            )\r\n"]
[356.886, "o", "            dictionary = S[:, np.newaxis] * dictionary\r\n"]
[356.896, "o", "\r\n"]
[356.906, "o", "        if self._n_components <= len(dictionary):\r\n"]
[356.916, "o", "            dictionary = dictionary[: self._n_components, :]\r\n"]
[356.926, "o", "        else:\r\n"]
[356.936, "o", "            dictionary = np.concatenate(\r\n"]
[356.946, "o", "                (\r\n"]
[356.956, "o", "                    dictionary,\r\n"]
[356.966, "o", "                    np.zeros(\r\n"]
[356.976, "o", "                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n"]
[356.986, "o", "                        dtype=dictionary.dtype,\r\n"]
[356.996, "o", "                    ),\r\n"]
[357.006, "o", "                )\r\n"]
[357.016, "o", "            )\r\n"]
[357.026, "o", "\r\n"]
[357.036, "o", "        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n"]
[357.046, "o", "        dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[357.056, "o", "\r\n"]
[357.066, "o", "        return dictionary\r\n"]
[357.076, "o", "\r\n"]
[357.086, "o", "    def _update_inner_stats(self, X, code, batch_size, step):\r\n"]
[357.096, "o", "        \"\"\"Update the inner stats inplace.\"\"\"\r\n"]
[357.106, "o", "        if step < batch_size - 1:\r\n"]
[357.116, "o", "            theta = (step + 1) * batch_size\r\n"]
[357.126, "o", "        else:\r\n"]
[357.136, "o", "            theta = batch_size**2 + step + 1 - batch_size\r\n"]
[357.146, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[357.156, "o", "\r\n"]
[357.166, "o", "        self._A *= beta\r\n"]
[357.176, "o", "        self._A += code.T @ code / batch_size\r\n"]
[357.186, "o", "        self._B *= beta\r\n"]
[357.196, "o", "        self._B += X.T @ code / batch_size\r\n"]
[357.206, "o", "\r\n"]
[357.216, "o", "    def _minibatch_step(self, X, dictionary, random_state, step):\r\n"]
[357.226, "o", "        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n"]
[357.236, "o", "        batch_size = X.shape[0]\r\n"]
[357.246, "o", "\r\n"]
[357.256, "o", "        # Compute code for this batch\r\n"]
[357.266, "o", "        code = _sparse_encode(\r\n"]
[357.276, "o", "            X,\r\n"]
[357.286, "o", "            dictionary,\r\n"]
[357.296, "o", "            algorithm=self._fit_algorithm,\r\n"]
[357.306, "o", "            alpha=self.alpha,\r\n"]
[357.316, "o", "            n_jobs=self.n_jobs,\r\n"]
[357.326, "o", "            positive=self.positive_code,\r\n"]
[357.336, "o", "            max_iter=self.transform_max_iter,\r\n"]
[357.346, "o", "            verbose=self.verbose,\r\n"]
[357.356, "o", "        )\r\n"]
[357.366, "o", "\r\n"]
[357.376, "o", "        batch_cost = (\r\n"]
[357.386, "o", "            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n"]
[357.396, "o", "            + self.alpha * np.sum(np.abs(code))\r\n"]
[357.406, "o", "        ) / batch_size\r\n"]
[357.416, "o", "\r\n"]
[357.426, "o", "        # Update inner stats\r\n"]
[357.436, "o", "        self._update_inner_stats(X, code, batch_size, step)\r\n"]
[357.446, "o", "\r\n"]
[357.456, "o", "        # Update dictionary\r\n"]
[357.466, "o", "        _update_dict(\r\n"]
[357.476, "o", "            dictionary,\r\n"]
[357.486, "o", "            X,\r\n"]
[357.496, "o", "            code,\r\n"]
[357.506, "o", "            self._A,\r\n"]
[357.516, "o", "            self._B,\r\n"]
[357.526, "o", "            verbose=self.verbose,\r\n"]
[357.536, "o", "            random_state=random_state,\r\n"]
[357.546, "o", "            positive=self.positive_dict,\r\n"]
[357.556, "o", "        )\r\n"]
[357.566, "o", "\r\n"]
[357.576, "o", "        return batch_cost\r\n"]
[357.586, "o", "\r\n"]
[357.596, "o", "    def _check_convergence(\r\n"]
[357.606, "o", "        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n"]
[357.616, "o", "    ):\r\n"]
[357.626, "o", "        \"\"\"Helper function to encapsulate the early stopping logic.\r\n"]
[357.636, "o", "\r\n"]
[357.646, "o", "        Early stopping is based on two factors:\r\n"]
[357.656, "o", "        - A small change of the dictionary between two minibatch updates. This is\r\n"]
[357.666, "o", "          controlled by the tol parameter.\r\n"]
[357.676, "o", "        - No more improvement on a smoothed estimate of the objective function for a\r\n"]
[357.686, "o", "          a certain number of consecutive minibatch updates. This is controlled by\r\n"]
[357.696, "o", "          the max_no_improvement parameter.\r\n"]
[357.706, "o", "        \"\"\"\r\n"]
[357.716, "o", "        batch_size = X.shape[0]\r\n"]
[357.726, "o", "\r\n"]
[357.736, "o", "        # counts steps starting from 1 for user friendly verbose mode.\r\n"]
[357.746, "o", "        step = step + 1\r\n"]
[357.756, "o", "\r\n"]
[357.766, "o", "        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n"]
[357.776, "o", "        # too bad value\r\n"]
[357.786, "o", "        if step <= min(100, n_samples / batch_size):\r\n"]
[357.796, "o", "            if self.verbose:\r\n"]
[357.806, "o", "                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n"]
[357.816, "o", "            return False\r\n"]
[357.826, "o", "\r\n"]
[357.836, "o", "        # Compute an Exponentially Weighted Average of the cost function to\r\n"]
[357.846, "o", "        # monitor the convergence while discarding minibatch-local stochastic\r\n"]
[357.856, "o", "        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n"]
[357.866, "o", "        if self._ewa_cost is None:\r\n"]
[357.876, "o", "            self._ewa_cost = batch_cost\r\n"]
[357.886, "o", "        else:\r\n"]
[357.896, "o", "            alpha = batch_size / (n_samples + 1)\r\n"]
[357.906, "o", "            alpha = min(alpha, 1)\r\n"]
[357.916, "o", "            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n"]
[357.926, "o", "\r\n"]
[357.936, "o", "        if self.verbose:\r\n"]
[357.946, "o", "            print(\r\n"]
[357.956, "o", "                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\r\n"]
[357.966, "o", "                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\r\n"]
[357.976, "o", "            )\r\n"]
[357.986, "o", "\r\n"]
[357.996, "o", "        # Early stopping based on change of dictionary\r\n"]
[358.006, "o", "        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\r\n"]
[358.016, "o", "        if self.tol > 0 and dict_diff <= self.tol:\r\n"]
[358.026, "o", "            if self.verbose:\r\n"]
[358.036, "o", "                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\r\n"]
[358.046, "o", "            return True\r\n"]
[358.056, "o", "\r\n"]
[358.066, "o", "        # Early stopping heuristic due to lack of improvement on smoothed\r\n"]
[358.076, "o", "        # cost function\r\n"]
[358.086, "o", "        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\r\n"]
[358.096, "o", "            self._no_improvement = 0\r\n"]
[358.106, "o", "            self._ewa_cost_min = self._ewa_cost\r\n"]
[358.116, "o", "        else:\r\n"]
[358.126, "o", "            self._no_improvement += 1\r\n"]
[358.136, "o", "\r\n"]
[358.146, "o", "        if (\r\n"]
[358.156, "o", "            self.max_no_improvement is not None\r\n"]
[358.166, "o", "            and self._no_improvement >= self.max_no_improvement\r\n"]
[358.176, "o", "        ):\r\n"]
[358.186, "o", "            if self.verbose:\r\n"]
[358.196, "o", "                print(\r\n"]
[358.206, "o", "                    \"Converged (lack of improvement in objective function) \"\r\n"]
[358.216, "o", "                    f\"at step {step}/{n_steps}\"\r\n"]
[358.226, "o", "                )\r\n"]
[358.236, "o", "            return True\r\n"]
[358.246, "o", "\r\n"]
[358.256, "o", "        return False\r\n"]
[358.266, "o", "\r\n"]
[358.276, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[358.286, "o", "    def fit(self, X, y=None):\r\n"]
[358.296, "o", "        \"\"\"Fit the model from data in X.\r\n"]
[358.306, "o", "\r\n"]
[358.316, "o", "        Parameters\r\n"]
[358.326, "o", "        ----------\r\n"]
[358.336, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[358.346, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[358.356, "o", "            and `n_features` is the number of features.\r\n"]
[358.366, "o", "\r\n"]
[358.376, "o", "        y : Ignored\r\n"]
[358.386, "o", "            Not used, present for API consistency by convention.\r\n"]
[358.396, "o", "\r\n"]
[358.406, "o", "        Returns\r\n"]
[358.416, "o", "        -------\r\n"]
[358.426, "o", "        self : object\r\n"]
[358.436, "o", "            Returns the instance itself.\r\n"]
[358.446, "o", "        \"\"\"\r\n"]
[358.456, "o", "        X = self._validate_data(\r\n"]
[358.466, "o", "            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n"]
[358.476, "o", "        )\r\n"]
[358.486, "o", "\r\n"]
[358.496, "o", "        self._check_params(X)\r\n"]
[358.506, "o", "\r\n"]
[358.516, "o", "        if self.n_iter != \"deprecated\":\r\n"]
[358.526, "o", "            warnings.warn(\r\n"]
[358.536, "o", "                (\r\n"]
[358.546, "o", "                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n"]
[358.556, "o", "                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n"]
[358.566, "o", "                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n"]
[358.576, "o", "                    \"specified.\"\r\n"]
[358.586, "o", "                ),\r\n"]
[358.596, "o", "                FutureWarning,\r\n"]
[358.606, "o", "            )\r\n"]
[358.616, "o", "            n_iter = self.n_iter\r\n"]
[358.626, "o", "\r\n"]
[358.636, "o", "        self._random_state = check_random_state(self.random_state)\r\n"]
[358.646, "o", "\r\n"]
[358.656, "o", "        dictionary = self._initialize_dict(X, self._random_state)\r\n"]
[358.666, "o", "        old_dict = dictionary.copy()\r\n"]
[358.676, "o", "\r\n"]
[358.686, "o", "        if self.shuffle:\r\n"]
[358.696, "o", "            X_train = X.copy()\r\n"]
[358.706, "o", "            self._random_state.shuffle(X_train)\r\n"]
[358.716, "o", "        else:\r\n"]
[358.726, "o", "            X_train = X\r\n"]
[358.736, "o", "\r\n"]
[358.746, "o", "        n_samples, n_features = X_train.shape\r\n"]
[358.756, "o", "\r\n"]
[358.766, "o", "        if self.verbose:\r\n"]
[358.776, "o", "            print(\"[dict_learning]\")\r\n"]
[358.786, "o", "\r\n"]
[358.796, "o", "        # Inner stats\r\n"]
[358.806, "o", "        self._A = np.zeros(\r\n"]
[358.816, "o", "            (self._n_components, self._n_components), dtype=X_train.dtype\r\n"]
[358.826, "o", "        )\r\n"]
[358.836, "o", "        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n"]
[358.846, "o", "\r\n"]
[358.856, "o", "        if self.max_iter is not None:\r\n"]
[358.866, "o", "            # Attributes to monitor the convergence\r\n"]
[358.876, "o", "            self._ewa_cost = None\r\n"]
[358.886, "o", "            self._ewa_cost_min = None\r\n"]
[358.896, "o", "            self._no_improvement = 0\r\n"]
[358.906, "o", "\r\n"]
[358.916, "o", "            batches = gen_batches(n_samples, self._batch_size)\r\n"]
[358.926, "o", "            batches = itertools.cycle(batches)\r\n"]
[358.936, "o", "            n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\r\n"]
[358.946, "o", "            n_steps = self.max_iter * n_steps_per_iter\r\n"]
[358.956, "o", "\r\n"]
[358.966, "o", "            i = -1  # to allow max_iter = 0\r\n"]
[358.976, "o", "\r\n"]
[358.986, "o", "            for i, batch in zip(range(n_steps), batches):\r\n"]
[358.996, "o", "                X_batch = X_train[batch]\r\n"]
[359.006, "o", "\r\n"]
[359.016, "o", "                batch_cost = self._minibatch_step(\r\n"]
[359.026, "o", "                    X_batch, dictionary, self._random_state, i\r\n"]
[359.036, "o", "                )\r\n"]
[359.046, "o", "\r\n"]
[359.056, "o", "                if self._check_convergence(\r\n"]
[359.066, "o", "                    X_batch, batch_cost, dictionary, old_dict, n_samples, i, n_steps\r\n"]
[359.076, "o", "                ):\r\n"]
[359.086, "o", "                    break\r\n"]
[359.096, "o", "\r\n"]
[359.106, "o", "                # XXX callback param added for backward compat in #18975 but a common\r\n"]
[359.116, "o", "                # unified callback API should be preferred\r\n"]
[359.126, "o", "                if self.callback is not None:\r\n"]
[359.136, "o", "                    self.callback(locals())\r\n"]
[359.146, "o", "\r\n"]
[359.156, "o", "                old_dict[:] = dictionary\r\n"]
[359.166, "o", "\r\n"]
[359.176, "o", "            self.n_steps_ = i + 1\r\n"]
[359.186, "o", "            self.n_iter_ = np.ceil(self.n_steps_ / n_steps_per_iter)\r\n"]
[359.196, "o", "        else:\r\n"]
[359.206, "o", "            # TODO remove this branch in 1.4\r\n"]
[359.216, "o", "            n_iter = 1000 if self.n_iter == \"deprecated\" else self.n_iter\r\n"]
[359.226, "o", "\r\n"]
[359.236, "o", "            batches = gen_batches(n_samples, self._batch_size)\r\n"]
[359.246, "o", "            batches = itertools.cycle(batches)\r\n"]
[359.256, "o", "\r\n"]
[359.266, "o", "            for i, batch in zip(range(n_iter), batches):\r\n"]
[359.276, "o", "                self._minibatch_step(X_train[batch], dictionary, self._random_state, i)\r\n"]
[359.286, "o", "\r\n"]
[359.296, "o", "                trigger_verbose = self.verbose and i % ceil(100.0 / self.verbose) == 0\r\n"]
[359.306, "o", "                if self.verbose > 10 or trigger_verbose:\r\n"]
[359.316, "o", "                    print(f\"{i} batches processed.\")\r\n"]
[359.326, "o", "\r\n"]
[359.336, "o", "                if self.callback is not None:\r\n"]
[359.346, "o", "                    self.callback(locals())\r\n"]
[359.356, "o", "\r\n"]
[359.366, "o", "            self.n_steps_ = n_iter\r\n"]
[359.376, "o", "            self.n_iter_ = np.ceil(n_iter / int(np.ceil(n_samples / self._batch_size)))\r\n"]
[359.386, "o", "\r\n"]
[359.396, "o", "        self.components_ = dictionary\r\n"]
[359.406, "o", "\r\n"]
[359.416, "o", "        return self\r\n"]
[359.426, "o", "\r\n"]
[359.436, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[359.446, "o", "    def partial_fit(self, X, y=None):\r\n"]
[359.456, "o", "        \"\"\"Update the model using the data in X as a mini-batch.\r\n"]
[359.466, "o", "\r\n"]
[359.476, "o", "        Parameters\r\n"]
[359.486, "o", "        ----------\r\n"]
[359.496, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[359.506, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[359.516, "o", "            and `n_features` is the number of features.\r\n"]
[359.526, "o", "\r\n"]
[359.536, "o", "        y : Ignored\r\n"]
[359.546, "o", "            Not used, present for API consistency by convention.\r\n"]
[359.556, "o", "\r\n"]
[359.566, "o", "        Returns\r\n"]
[359.576, "o", "        -------\r\n"]
[359.586, "o", "        self : object\r\n"]
[359.596, "o", "            Return the instance itself.\r\n"]
[359.606, "o", "        \"\"\"\r\n"]
[359.616, "o", "        has_components = hasattr(self, \"components_\")\r\n"]
[359.626, "o", "\r\n"]
[359.636, "o", "        X = self._validate_data(\r\n"]
[359.646, "o", "            X, dtype=[np.float64, np.float32], order=\"C\", reset=not has_components\r\n"]
[359.656, "o", "        )\r\n"]
[359.666, "o", "\r\n"]
[359.676, "o", "        if not has_components:\r\n"]
[359.686, "o", "            # This instance has not been fitted yet (fit or partial_fit)\r\n"]
[359.696, "o", "            self._check_params(X)\r\n"]
[359.706, "o", "            self._random_state = check_random_state(self.random_state)\r\n"]
[359.716, "o", "\r\n"]
[359.726, "o", "            dictionary = self._initialize_dict(X, self._random_state)\r\n"]
[359.736, "o", "\r\n"]
[359.746, "o", "            self.n_steps_ = 0\r\n"]
[359.756, "o", "\r\n"]
[359.766, "o", "            self._A = np.zeros((self._n_components, self._n_components), dtype=X.dtype)\r\n"]
[359.776, "o", "            self._B = np.zeros((X.shape[1], self._n_components), dtype=X.dtype)\r\n"]
[359.786, "o", "        else:\r\n"]
[359.796, "o", "            dictionary = self.components_\r\n"]
[359.806, "o", "\r\n"]
[359.816, "o", "        self._minibatch_step(X, dictionary, self._random_state, self.n_steps_)\r\n"]
[359.826, "o", "\r\n"]
[359.836, "o", "        self.components_ = dictionary\r\n"]
[359.846, "o", "        self.n_steps_ += 1\r\n"]
[359.856, "o", "\r\n"]
[359.866, "o", "        return self\r\n"]
[359.876, "o", "\r\n"]
[359.886, "o", "    @property\r\n"]
[359.896, "o", "    def _n_features_out(self):\r\n"]
[359.906, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[359.916, "o", "        return self.components_.shape[0]\r\n"]
[359.926, "o", "\r\n"]
[359.936, "o", "    def _more_tags(self):\r\n"]
[360.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[360.002, "i", "cd asv_benchmarks\r"]
[360.004, "o", "cd asv_benchmarks\r\n"]
[360.006, "o", "\u001b[?2004l\r\n"]
[365.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[365.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[365.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[366.654, "o", "\u001b[?2004l\r\n"]
[368.302, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[370.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[370.002, "i", "cd ..\r"]
[370.004, "o", "cd ..\r\n"]
[370.006, "o", "\u001b[?2004l\r\n"]
[375.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[375.002, "i", "grep -n \"def _minibatch_step\\|def dict_learning_online\\|def _mini_batch_dictionary_learning\\|def _block_coordinate_\r"]
[375.004, "o", "grep -n \"def _minibatch_step\\|def dict_learning_online\\|def _mini_batch_dictionary_learning\\|def _block_coordinate_\r\n"]
[375.9948, "o", "_descent\\|class MiniBatchDictionaryLearning\" sklearn/decomposition/_dict_learning.py\r\n"]
[376.9836, "o", "\u001b[?2004l\r\n"]
[377.9724, "o", "\u001b[32m\u001b[K664\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kdef dict_learning_online\u001b[m\u001b[K(\r\n"]
[378.9612, "o", "\u001b[32m\u001b[K1867\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kclass MiniBatchDictionaryLearning\u001b[m\u001b[K(_BaseSparseCoding, BaseEstimator):\r\n"]
[380.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[380.002, "i", "sed -n '1,1000p' sklearn/decomposition/_dict_learning.py\r"]
[380.004, "o", "sed -n '1,1000p' sklearn/decomposition/_dict_learning.py\r\n"]
[380.016, "o", "\u001b[?2004l\r\n"]
[380.026, "o", "\"\"\" Dictionary learning.\r\n"]
[380.036, "o", "\"\"\"\r\n"]
[380.046, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[380.056, "o", "# License: BSD 3 clause\r\n"]
[380.066, "o", "\r\n"]
[380.076, "o", "import itertools\r\n"]
[380.086, "o", "import sys\r\n"]
[380.096, "o", "import time\r\n"]
[380.106, "o", "import warnings\r\n"]
[380.116, "o", "from math import ceil\r\n"]
[380.126, "o", "from numbers import Integral, Real\r\n"]
[380.136, "o", "\r\n"]
[380.146, "o", "import numpy as np\r\n"]
[380.156, "o", "from joblib import effective_n_jobs\r\n"]
[380.166, "o", "from scipy import linalg\r\n"]
[380.176, "o", "\r\n"]
[380.186, "o", "from ..base import (\r\n"]
[380.196, "o", "    BaseEstimator,\r\n"]
[380.206, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[380.216, "o", "    TransformerMixin,\r\n"]
[380.226, "o", "    _fit_context,\r\n"]
[380.236, "o", ")\r\n"]
[380.246, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[380.256, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[380.266, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[380.276, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[380.286, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[380.296, "o", "from ..utils.validation import check_is_fitted\r\n"]
[380.306, "o", "\r\n"]
[380.316, "o", "\r\n"]
[380.326, "o", "def _check_positive_coding(method, positive):\r\n"]
[380.336, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[380.346, "o", "        raise ValueError(\r\n"]
[380.356, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[380.366, "o", "        )\r\n"]
[380.376, "o", "\r\n"]
[380.386, "o", "\r\n"]
[380.396, "o", "def _sparse_encode_precomputed(\r\n"]
[380.406, "o", "    X,\r\n"]
[380.416, "o", "    dictionary,\r\n"]
[380.426, "o", "    *,\r\n"]
[380.436, "o", "    gram=None,\r\n"]
[380.446, "o", "    cov=None,\r\n"]
[380.456, "o", "    algorithm=\"lasso_lars\",\r\n"]
[380.466, "o", "    regularization=None,\r\n"]
[380.476, "o", "    copy_cov=True,\r\n"]
[380.486, "o", "    init=None,\r\n"]
[380.496, "o", "    max_iter=1000,\r\n"]
[380.506, "o", "    verbose=0,\r\n"]
[380.516, "o", "    positive=False,\r\n"]
[380.526, "o", "):\r\n"]
[380.536, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[380.546, "o", "\r\n"]
[380.556, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[380.566, "o", "\r\n"]
[380.576, "o", "    Parameters\r\n"]
[380.586, "o", "    ----------\r\n"]
[380.596, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[380.606, "o", "        Data matrix.\r\n"]
[380.616, "o", "\r\n"]
[380.626, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[380.636, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[380.646, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[380.656, "o", "\r\n"]
[380.666, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[380.676, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[380.686, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[380.696, "o", "\r\n"]
[380.706, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[380.716, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[380.726, "o", "\r\n"]
[380.736, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[380.746, "o", "            default='lasso_lars'\r\n"]
[380.756, "o", "        The algorithm used:\r\n"]
[380.766, "o", "\r\n"]
[380.776, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[380.786, "o", "          (`linear_model.lars_path`);\r\n"]
[380.796, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[380.806, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[380.816, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[380.826, "o", "          the estimated components are sparse;\r\n"]
[380.836, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[380.846, "o", "          solution;\r\n"]
[380.856, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[380.866, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[380.876, "o", "\r\n"]
[380.886, "o", "    regularization : int or float, default=None\r\n"]
[380.896, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[380.906, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[380.916, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[380.926, "o", "\r\n"]
[380.936, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[380.946, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[380.956, "o", "        `algorithm='lasso_cd'`.\r\n"]
[380.966, "o", "\r\n"]
[380.976, "o", "    max_iter : int, default=1000\r\n"]
[380.986, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[380.996, "o", "        `'lasso_lars'`.\r\n"]
[381.006, "o", "\r\n"]
[381.016, "o", "    copy_cov : bool, default=True\r\n"]
[381.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[381.036, "o", "        be overwritten.\r\n"]
[381.046, "o", "\r\n"]
[381.056, "o", "    verbose : int, default=0\r\n"]
[381.066, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[381.076, "o", "\r\n"]
[381.086, "o", "    positive: bool, default=False\r\n"]
[381.096, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[381.106, "o", "\r\n"]
[381.116, "o", "        .. versionadded:: 0.20\r\n"]
[381.126, "o", "\r\n"]
[381.136, "o", "    Returns\r\n"]
[381.146, "o", "    -------\r\n"]
[381.156, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[381.166, "o", "        The sparse codes.\r\n"]
[381.176, "o", "    \"\"\"\r\n"]
[381.186, "o", "    n_samples, n_features = X.shape\r\n"]
[381.196, "o", "    n_components = dictionary.shape[0]\r\n"]
[381.206, "o", "\r\n"]
[381.216, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[381.226, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[381.236, "o", "        try:\r\n"]
[381.246, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[381.256, "o", "\r\n"]
[381.266, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[381.276, "o", "            # corrects the verbosity level.\r\n"]
[381.286, "o", "            lasso_lars = LassoLars(\r\n"]
[381.296, "o", "                alpha=alpha,\r\n"]
[381.306, "o", "                fit_intercept=False,\r\n"]
[381.316, "o", "                verbose=verbose,\r\n"]
[381.326, "o", "                precompute=gram,\r\n"]
[381.336, "o", "                fit_path=False,\r\n"]
[381.346, "o", "                positive=positive,\r\n"]
[381.356, "o", "                max_iter=max_iter,\r\n"]
[381.366, "o", "            )\r\n"]
[381.376, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[381.386, "o", "            new_code = lasso_lars.coef_\r\n"]
[381.396, "o", "        finally:\r\n"]
[381.406, "o", "            np.seterr(**err_mgt)\r\n"]
[381.416, "o", "\r\n"]
[381.426, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[381.436, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[381.446, "o", "\r\n"]
[381.456, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[381.466, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[381.476, "o", "        # argument that we could pass in from Lasso.\r\n"]
[381.486, "o", "        clf = Lasso(\r\n"]
[381.496, "o", "            alpha=alpha,\r\n"]
[381.506, "o", "            fit_intercept=False,\r\n"]
[381.516, "o", "            precompute=gram,\r\n"]
[381.526, "o", "            max_iter=max_iter,\r\n"]
[381.536, "o", "            warm_start=True,\r\n"]
[381.546, "o", "            positive=positive,\r\n"]
[381.556, "o", "        )\r\n"]
[381.566, "o", "\r\n"]
[381.576, "o", "        if init is not None:\r\n"]
[381.586, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[381.596, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[381.606, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[381.616, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[381.626, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[381.636, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[381.646, "o", "                init = np.array(init)\r\n"]
[381.656, "o", "            clf.coef_ = init\r\n"]
[381.666, "o", "\r\n"]
[381.676, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[381.686, "o", "        new_code = clf.coef_\r\n"]
[381.696, "o", "\r\n"]
[381.706, "o", "    elif algorithm == \"lars\":\r\n"]
[381.716, "o", "        try:\r\n"]
[381.726, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[381.736, "o", "\r\n"]
[381.746, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[381.756, "o", "            # corrects the verbosity level.\r\n"]
[381.766, "o", "            lars = Lars(\r\n"]
[381.776, "o", "                fit_intercept=False,\r\n"]
[381.786, "o", "                verbose=verbose,\r\n"]
[381.796, "o", "                precompute=gram,\r\n"]
[381.806, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[381.816, "o", "                fit_path=False,\r\n"]
[381.826, "o", "            )\r\n"]
[381.836, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[381.846, "o", "            new_code = lars.coef_\r\n"]
[381.856, "o", "        finally:\r\n"]
[381.866, "o", "            np.seterr(**err_mgt)\r\n"]
[381.876, "o", "\r\n"]
[381.886, "o", "    elif algorithm == \"threshold\":\r\n"]
[381.896, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[381.906, "o", "        if positive:\r\n"]
[381.916, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[381.926, "o", "\r\n"]
[381.936, "o", "    elif algorithm == \"omp\":\r\n"]
[381.946, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[381.956, "o", "            Gram=gram,\r\n"]
[381.966, "o", "            Xy=cov,\r\n"]
[381.976, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[381.986, "o", "            tol=None,\r\n"]
[381.996, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[382.006, "o", "            copy_Xy=copy_cov,\r\n"]
[382.016, "o", "        ).T\r\n"]
[382.026, "o", "\r\n"]
[382.036, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[382.046, "o", "\r\n"]
[382.056, "o", "\r\n"]
[382.066, "o", "@validate_params(\r\n"]
[382.076, "o", "    {\r\n"]
[382.086, "o", "        \"X\": [\"array-like\"],\r\n"]
[382.096, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[382.106, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[382.116, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[382.126, "o", "        \"algorithm\": [\r\n"]
[382.136, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[382.146, "o", "        ],\r\n"]
[382.156, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[382.166, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[382.176, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[382.186, "o", "        \"init\": [\"array-like\", None],\r\n"]
[382.196, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[382.206, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[382.216, "o", "        \"check_input\": [\"boolean\"],\r\n"]
[382.226, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[382.236, "o", "        \"positive\": [\"boolean\"],\r\n"]
[382.246, "o", "    },\r\n"]
[382.256, "o", "    prefer_skip_nested_validation=True,\r\n"]
[382.266, "o", ")\r\n"]
[382.276, "o", "# XXX : could be moved to the linear_model module\r\n"]
[382.286, "o", "def sparse_encode(\r\n"]
[382.296, "o", "    X,\r\n"]
[382.306, "o", "    dictionary,\r\n"]
[382.316, "o", "    *,\r\n"]
[382.326, "o", "    gram=None,\r\n"]
[382.336, "o", "    cov=None,\r\n"]
[382.346, "o", "    algorithm=\"lasso_lars\",\r\n"]
[382.356, "o", "    n_nonzero_coefs=None,\r\n"]
[382.366, "o", "    alpha=None,\r\n"]
[382.376, "o", "    copy_cov=True,\r\n"]
[382.386, "o", "    init=None,\r\n"]
[382.396, "o", "    max_iter=1000,\r\n"]
[382.406, "o", "    n_jobs=None,\r\n"]
[382.416, "o", "    check_input=True,\r\n"]
[382.426, "o", "    verbose=0,\r\n"]
[382.436, "o", "    positive=False,\r\n"]
[382.446, "o", "):\r\n"]
[382.456, "o", "    \"\"\"Sparse coding.\r\n"]
[382.466, "o", "\r\n"]
[382.476, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[382.486, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[382.496, "o", "\r\n"]
[382.506, "o", "        X ~= code * dictionary\r\n"]
[382.516, "o", "\r\n"]
[382.526, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[382.536, "o", "\r\n"]
[382.546, "o", "    Parameters\r\n"]
[382.556, "o", "    ----------\r\n"]
[382.566, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[382.576, "o", "        Data matrix.\r\n"]
[382.586, "o", "\r\n"]
[382.596, "o", "    dictionary : array-like of shape (n_components, n_features)\r\n"]
[382.606, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[382.616, "o", "        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[382.626, "o", "        output.\r\n"]
[382.636, "o", "\r\n"]
[382.646, "o", "    gram : array-like of shape (n_components, n_components), default=None\r\n"]
[382.656, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n"]
[382.666, "o", "\r\n"]
[382.676, "o", "    cov : array-like of shape (n_components, n_samples), default=None\r\n"]
[382.686, "o", "        Precomputed covariance, `dictionary' * X`.\r\n"]
[382.696, "o", "\r\n"]
[382.706, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[382.716, "o", "            default='lasso_lars'\r\n"]
[382.726, "o", "        The algorithm used:\r\n"]
[382.736, "o", "\r\n"]
[382.746, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[382.756, "o", "          (`linear_model.lars_path`);\r\n"]
[382.766, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[382.776, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[382.786, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[382.796, "o", "          the estimated components are sparse;\r\n"]
[382.806, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[382.816, "o", "          solution;\r\n"]
[382.826, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[382.836, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[382.846, "o", "\r\n"]
[382.856, "o", "    n_nonzero_coefs : int, default=None\r\n"]
[382.866, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[382.876, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[382.886, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[382.896, "o", "        `n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[382.906, "o", "\r\n"]
[382.916, "o", "    alpha : float, default=None\r\n"]
[382.926, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[382.936, "o", "        penalty applied to the L1 norm.\r\n"]
[382.946, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[382.956, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[382.966, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[382.976, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[382.986, "o", "        `n_nonzero_coefs`.\r\n"]
[382.996, "o", "        If `None`, default to 1.\r\n"]
[383.006, "o", "\r\n"]
[383.016, "o", "    copy_cov : bool, default=True\r\n"]
[383.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[383.036, "o", "        be overwritten.\r\n"]
[383.046, "o", "\r\n"]
[383.056, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[383.066, "o", "        Initialization value of the sparse codes. Only used if\r\n"]
[383.076, "o", "        `algorithm='lasso_cd'`.\r\n"]
[383.086, "o", "\r\n"]
[383.096, "o", "    max_iter : int, default=1000\r\n"]
[383.106, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[383.116, "o", "        `'lasso_lars'`.\r\n"]
[383.126, "o", "\r\n"]
[383.136, "o", "    n_jobs : int, default=None\r\n"]
[383.146, "o", "        Number of parallel jobs to run.\r\n"]
[383.156, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[383.166, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[383.176, "o", "        for more details.\r\n"]
[383.186, "o", "\r\n"]
[383.196, "o", "    check_input : bool, default=True\r\n"]
[383.206, "o", "        If `False`, the input arrays X and dictionary will not be checked.\r\n"]
[383.216, "o", "\r\n"]
[383.226, "o", "    verbose : int, default=0\r\n"]
[383.236, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[383.246, "o", "\r\n"]
[383.256, "o", "    positive : bool, default=False\r\n"]
[383.266, "o", "        Whether to enforce positivity when finding the encoding.\r\n"]
[383.276, "o", "\r\n"]
[383.286, "o", "        .. versionadded:: 0.20\r\n"]
[383.296, "o", "\r\n"]
[383.306, "o", "    Returns\r\n"]
[383.316, "o", "    -------\r\n"]
[383.326, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[383.336, "o", "        The sparse codes.\r\n"]
[383.346, "o", "\r\n"]
[383.356, "o", "    See Also\r\n"]
[383.366, "o", "    --------\r\n"]
[383.376, "o", "    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n"]
[383.386, "o", "        path using LARS algorithm.\r\n"]
[383.396, "o", "    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n"]
[383.406, "o", "    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n"]
[383.416, "o", "    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n"]
[383.426, "o", "        dictionary.\r\n"]
[383.436, "o", "    \"\"\"\r\n"]
[383.446, "o", "    if check_input:\r\n"]
[383.456, "o", "        if algorithm == \"lasso_cd\":\r\n"]
[383.466, "o", "            dictionary = check_array(\r\n"]
[383.476, "o", "                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n"]
[383.486, "o", "            )\r\n"]
[383.496, "o", "            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n"]
[383.506, "o", "        else:\r\n"]
[383.516, "o", "            dictionary = check_array(dictionary)\r\n"]
[383.526, "o", "            X = check_array(X)\r\n"]
[383.536, "o", "\r\n"]
[383.546, "o", "    if dictionary.shape[1] != X.shape[1]:\r\n"]
[383.556, "o", "        raise ValueError(\r\n"]
[383.566, "o", "            \"Dictionary and X have different numbers of features:\"\r\n"]
[383.576, "o", "            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n"]
[383.586, "o", "        )\r\n"]
[383.596, "o", "\r\n"]
[383.606, "o", "    _check_positive_coding(algorithm, positive)\r\n"]
[383.616, "o", "\r\n"]
[383.626, "o", "    return _sparse_encode(\r\n"]
[383.636, "o", "        X,\r\n"]
[383.646, "o", "        dictionary,\r\n"]
[383.656, "o", "        gram=gram,\r\n"]
[383.666, "o", "        cov=cov,\r\n"]
[383.676, "o", "        algorithm=algorithm,\r\n"]
[383.686, "o", "        n_nonzero_coefs=n_nonzero_coefs,\r\n"]
[383.696, "o", "        alpha=alpha,\r\n"]
[383.706, "o", "        copy_cov=copy_cov,\r\n"]
[383.716, "o", "        init=init,\r\n"]
[383.726, "o", "        max_iter=max_iter,\r\n"]
[383.736, "o", "        n_jobs=n_jobs,\r\n"]
[383.746, "o", "        verbose=verbose,\r\n"]
[383.756, "o", "        positive=positive,\r\n"]
[383.766, "o", "    )\r\n"]
[383.776, "o", "\r\n"]
[383.786, "o", "\r\n"]
[383.796, "o", "def _sparse_encode(\r\n"]
[383.806, "o", "    X,\r\n"]
[383.816, "o", "    dictionary,\r\n"]
[383.826, "o", "    *,\r\n"]
[383.836, "o", "    gram=None,\r\n"]
[383.846, "o", "    cov=None,\r\n"]
[383.856, "o", "    algorithm=\"lasso_lars\",\r\n"]
[383.866, "o", "    n_nonzero_coefs=None,\r\n"]
[383.876, "o", "    alpha=None,\r\n"]
[383.886, "o", "    copy_cov=True,\r\n"]
[383.896, "o", "    init=None,\r\n"]
[383.906, "o", "    max_iter=1000,\r\n"]
[383.916, "o", "    n_jobs=None,\r\n"]
[383.926, "o", "    verbose=0,\r\n"]
[383.936, "o", "    positive=False,\r\n"]
[383.946, "o", "):\r\n"]
[383.956, "o", "    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[383.966, "o", "\r\n"]
[383.976, "o", "    n_samples, n_features = X.shape\r\n"]
[383.986, "o", "    n_components = dictionary.shape[0]\r\n"]
[383.996, "o", "\r\n"]
[384.006, "o", "    if algorithm in (\"lars\", \"omp\"):\r\n"]
[384.016, "o", "        regularization = n_nonzero_coefs\r\n"]
[384.026, "o", "        if regularization is None:\r\n"]
[384.036, "o", "            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[384.046, "o", "    else:\r\n"]
[384.056, "o", "        regularization = alpha\r\n"]
[384.066, "o", "        if regularization is None:\r\n"]
[384.076, "o", "            regularization = 1.0\r\n"]
[384.086, "o", "\r\n"]
[384.096, "o", "    if gram is None and algorithm != \"threshold\":\r\n"]
[384.106, "o", "        gram = np.dot(dictionary, dictionary.T)\r\n"]
[384.116, "o", "\r\n"]
[384.126, "o", "    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[384.136, "o", "        copy_cov = False\r\n"]
[384.146, "o", "        cov = np.dot(dictionary, X.T)\r\n"]
[384.156, "o", "\r\n"]
[384.166, "o", "    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[384.176, "o", "        code = _sparse_encode_precomputed(\r\n"]
[384.186, "o", "            X,\r\n"]
[384.196, "o", "            dictionary,\r\n"]
[384.206, "o", "            gram=gram,\r\n"]
[384.216, "o", "            cov=cov,\r\n"]
[384.226, "o", "            algorithm=algorithm,\r\n"]
[384.236, "o", "            regularization=regularization,\r\n"]
[384.246, "o", "            copy_cov=copy_cov,\r\n"]
[384.256, "o", "            init=init,\r\n"]
[384.266, "o", "            max_iter=max_iter,\r\n"]
[384.276, "o", "            verbose=verbose,\r\n"]
[384.286, "o", "            positive=positive,\r\n"]
[384.296, "o", "        )\r\n"]
[384.306, "o", "        return code\r\n"]
[384.316, "o", "\r\n"]
[384.326, "o", "    # Enter parallel code block\r\n"]
[384.336, "o", "    n_samples = X.shape[0]\r\n"]
[384.346, "o", "    n_components = dictionary.shape[0]\r\n"]
[384.356, "o", "    code = np.empty((n_samples, n_components))\r\n"]
[384.366, "o", "    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[384.376, "o", "\r\n"]
[384.386, "o", "    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[384.396, "o", "        delayed(_sparse_encode_precomputed)(\r\n"]
[384.406, "o", "            X[this_slice],\r\n"]
[384.416, "o", "            dictionary,\r\n"]
[384.426, "o", "            gram=gram,\r\n"]
[384.436, "o", "            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[384.446, "o", "            algorithm=algorithm,\r\n"]
[384.456, "o", "            regularization=regularization,\r\n"]
[384.466, "o", "            copy_cov=copy_cov,\r\n"]
[384.476, "o", "            init=init[this_slice] if init is not None else None,\r\n"]
[384.486, "o", "            max_iter=max_iter,\r\n"]
[384.496, "o", "            verbose=verbose,\r\n"]
[384.506, "o", "            positive=positive,\r\n"]
[384.516, "o", "        )\r\n"]
[384.526, "o", "        for this_slice in slices\r\n"]
[384.536, "o", "    )\r\n"]
[384.546, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[384.556, "o", "        code[this_slice] = this_view\r\n"]
[384.566, "o", "    return code\r\n"]
[384.576, "o", "\r\n"]
[384.586, "o", "\r\n"]
[384.596, "o", "def _update_dict(\r\n"]
[384.606, "o", "    dictionary,\r\n"]
[384.616, "o", "    Y,\r\n"]
[384.626, "o", "    code,\r\n"]
[384.636, "o", "    A=None,\r\n"]
[384.646, "o", "    B=None,\r\n"]
[384.656, "o", "    verbose=False,\r\n"]
[384.666, "o", "    random_state=None,\r\n"]
[384.676, "o", "    positive=False,\r\n"]
[384.686, "o", "):\r\n"]
[384.696, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[384.706, "o", "\r\n"]
[384.716, "o", "    Parameters\r\n"]
[384.726, "o", "    ----------\r\n"]
[384.736, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[384.746, "o", "        Value of the dictionary at the previous iteration.\r\n"]
[384.756, "o", "\r\n"]
[384.766, "o", "    Y : ndarray of shape (n_samples, n_features)\r\n"]
[384.776, "o", "        Data matrix.\r\n"]
[384.786, "o", "\r\n"]
[384.796, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[384.806, "o", "        Sparse coding of the data against which to optimize the dictionary.\r\n"]
[384.816, "o", "\r\n"]
[384.826, "o", "    A : ndarray of shape (n_components, n_components), default=None\r\n"]
[384.836, "o", "        Together with `B`, sufficient stats of the online model to update the\r\n"]
[384.846, "o", "        dictionary.\r\n"]
[384.856, "o", "\r\n"]
[384.866, "o", "    B : ndarray of shape (n_features, n_components), default=None\r\n"]
[384.876, "o", "        Together with `A`, sufficient stats of the online model to update the\r\n"]
[384.886, "o", "        dictionary.\r\n"]
[384.896, "o", "\r\n"]
[384.906, "o", "    verbose: bool, default=False\r\n"]
[384.916, "o", "        Degree of output the procedure will print.\r\n"]
[384.926, "o", "\r\n"]
[384.936, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[385.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[385.002, "i", "sed -n '1000,1800p' sklearn/decomposition/_dict_learning.py\r"]
[385.004, "o", "sed -n '1000,1800p' sklearn/decomposition/_dict_learning.py\r\n"]
[385.016, "o", "\u001b[?2004l\r\n"]
[385.026, "o", "\r\n"]
[385.036, "o", "        this_code = sparse_encode(\r\n"]
[385.046, "o", "            this_X,\r\n"]
[385.056, "o", "            dictionary,\r\n"]
[385.066, "o", "            algorithm=method,\r\n"]
[385.076, "o", "            alpha=alpha,\r\n"]
[385.086, "o", "            n_jobs=n_jobs,\r\n"]
[385.096, "o", "            check_input=False,\r\n"]
[385.106, "o", "            positive=positive_code,\r\n"]
[385.116, "o", "            max_iter=method_max_iter,\r\n"]
[385.126, "o", "            verbose=verbose,\r\n"]
[385.136, "o", "        )\r\n"]
[385.146, "o", "\r\n"]
[385.156, "o", "        # Update the auxiliary variables\r\n"]
[385.166, "o", "        if ii < batch_size - 1:\r\n"]
[385.176, "o", "            theta = float((ii + 1) * batch_size)\r\n"]
[385.186, "o", "        else:\r\n"]
[385.196, "o", "            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n"]
[385.206, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[385.216, "o", "\r\n"]
[385.226, "o", "        A *= beta\r\n"]
[385.236, "o", "        A += np.dot(this_code.T, this_code)\r\n"]
[385.246, "o", "        B *= beta\r\n"]
[385.256, "o", "        B += np.dot(this_X.T, this_code)\r\n"]
[385.266, "o", "\r\n"]
[385.276, "o", "        # Update dictionary in place\r\n"]
[385.286, "o", "        _update_dict(\r\n"]
[385.296, "o", "            dictionary,\r\n"]
[385.306, "o", "            this_X,\r\n"]
[385.316, "o", "            this_code,\r\n"]
[385.326, "o", "            A,\r\n"]
[385.336, "o", "            B,\r\n"]
[385.346, "o", "            verbose=verbose,\r\n"]
[385.356, "o", "            random_state=random_state,\r\n"]
[385.366, "o", "            positive=positive_dict,\r\n"]
[385.376, "o", "        )\r\n"]
[385.386, "o", "\r\n"]
[385.396, "o", "        # Maybe we need a stopping criteria based on the amount of\r\n"]
[385.406, "o", "        # modification in the dictionary\r\n"]
[385.416, "o", "        if callback is not None:\r\n"]
[385.426, "o", "            callback(locals())\r\n"]
[385.436, "o", "\r\n"]
[385.446, "o", "    if return_inner_stats:\r\n"]
[385.456, "o", "        if return_n_iter:\r\n"]
[385.466, "o", "            return dictionary, (A, B), ii - iter_offset + 1\r\n"]
[385.476, "o", "        else:\r\n"]
[385.486, "o", "            return dictionary, (A, B)\r\n"]
[385.496, "o", "    if return_code:\r\n"]
[385.506, "o", "        if verbose > 1:\r\n"]
[385.516, "o", "            print(\"Learning code...\", end=\" \")\r\n"]
[385.526, "o", "        elif verbose == 1:\r\n"]
[385.536, "o", "            print(\"|\", end=\" \")\r\n"]
[385.546, "o", "        code = sparse_encode(\r\n"]
[385.556, "o", "            X,\r\n"]
[385.566, "o", "            dictionary,\r\n"]
[385.576, "o", "            algorithm=method,\r\n"]
[385.586, "o", "            alpha=alpha,\r\n"]
[385.596, "o", "            n_jobs=n_jobs,\r\n"]
[385.606, "o", "            check_input=False,\r\n"]
[385.616, "o", "            positive=positive_code,\r\n"]
[385.626, "o", "            max_iter=method_max_iter,\r\n"]
[385.636, "o", "            verbose=verbose,\r\n"]
[385.646, "o", "        )\r\n"]
[385.656, "o", "        if verbose > 1:\r\n"]
[385.666, "o", "            dt = time.time() - t0\r\n"]
[385.676, "o", "            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n"]
[385.686, "o", "        if return_n_iter:\r\n"]
[385.696, "o", "            return code, dictionary, ii - iter_offset + 1\r\n"]
[385.706, "o", "        else:\r\n"]
[385.716, "o", "            return code, dictionary\r\n"]
[385.726, "o", "\r\n"]
[385.736, "o", "    if return_n_iter:\r\n"]
[385.746, "o", "        return dictionary, ii - iter_offset + 1\r\n"]
[385.756, "o", "    else:\r\n"]
[385.766, "o", "        return dictionary\r\n"]
[385.776, "o", "\r\n"]
[385.786, "o", "\r\n"]
[385.796, "o", "@validate_params(\r\n"]
[385.806, "o", "    {\r\n"]
[385.816, "o", "        \"X\": [\"array-like\"],\r\n"]
[385.826, "o", "        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[385.836, "o", "        \"return_n_iter\": [\"boolean\"],\r\n"]
[385.846, "o", "        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[385.856, "o", "    },\r\n"]
[385.866, "o", "    prefer_skip_nested_validation=False,\r\n"]
[385.876, "o", ")\r\n"]
[385.886, "o", "def dict_learning(\r\n"]
[385.896, "o", "    X,\r\n"]
[385.906, "o", "    n_components,\r\n"]
[385.916, "o", "    *,\r\n"]
[385.926, "o", "    alpha,\r\n"]
[385.936, "o", "    max_iter=100,\r\n"]
[385.946, "o", "    tol=1e-8,\r\n"]
[385.956, "o", "    method=\"lars\",\r\n"]
[385.966, "o", "    n_jobs=None,\r\n"]
[385.976, "o", "    dict_init=None,\r\n"]
[385.986, "o", "    code_init=None,\r\n"]
[385.996, "o", "    callback=None,\r\n"]
[386.006, "o", "    verbose=False,\r\n"]
[386.016, "o", "    random_state=None,\r\n"]
[386.026, "o", "    return_n_iter=False,\r\n"]
[386.036, "o", "    positive_dict=False,\r\n"]
[386.046, "o", "    positive_code=False,\r\n"]
[386.056, "o", "    method_max_iter=1000,\r\n"]
[386.066, "o", "):\r\n"]
[386.076, "o", "    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n"]
[386.086, "o", "\r\n"]
[386.096, "o", "    Finds the best dictionary and the corresponding sparse code for\r\n"]
[386.106, "o", "    approximating the data matrix X by solving::\r\n"]
[386.116, "o", "\r\n"]
[386.126, "o", "        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[386.136, "o", "                     (U,V)\r\n"]
[386.146, "o", "                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n"]
[386.156, "o", "\r\n"]
[386.166, "o", "    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n"]
[386.176, "o", "    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n"]
[386.186, "o", "    which is the sum of the absolute values of all the entries in the matrix.\r\n"]
[386.196, "o", "\r\n"]
[386.206, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[386.216, "o", "\r\n"]
[386.226, "o", "    Parameters\r\n"]
[386.236, "o", "    ----------\r\n"]
[386.246, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[386.256, "o", "        Data matrix.\r\n"]
[386.266, "o", "\r\n"]
[386.276, "o", "    n_components : int\r\n"]
[386.286, "o", "        Number of dictionary atoms to extract.\r\n"]
[386.296, "o", "\r\n"]
[386.306, "o", "    alpha : int or float\r\n"]
[386.316, "o", "        Sparsity controlling parameter.\r\n"]
[386.326, "o", "\r\n"]
[386.336, "o", "    max_iter : int, default=100\r\n"]
[386.346, "o", "        Maximum number of iterations to perform.\r\n"]
[386.356, "o", "\r\n"]
[386.366, "o", "    tol : float, default=1e-8\r\n"]
[386.376, "o", "        Tolerance for the stopping condition.\r\n"]
[386.386, "o", "\r\n"]
[386.396, "o", "    method : {'lars', 'cd'}, default='lars'\r\n"]
[386.406, "o", "        The method used:\r\n"]
[386.416, "o", "\r\n"]
[386.426, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[386.436, "o", "           problem (`linear_model.lars_path`);\r\n"]
[386.446, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[386.456, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[386.466, "o", "          the estimated components are sparse.\r\n"]
[386.476, "o", "\r\n"]
[386.486, "o", "    n_jobs : int, default=None\r\n"]
[386.496, "o", "        Number of parallel jobs to run.\r\n"]
[386.506, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[386.516, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[386.526, "o", "        for more details.\r\n"]
[386.536, "o", "\r\n"]
[386.546, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[386.556, "o", "        Initial value for the dictionary for warm restart scenarios. Only used\r\n"]
[386.566, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[386.576, "o", "\r\n"]
[386.586, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[386.596, "o", "        Initial value for the sparse code for warm restart scenarios. Only used\r\n"]
[386.606, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[386.616, "o", "\r\n"]
[386.626, "o", "    callback : callable, default=None\r\n"]
[386.636, "o", "        Callable that gets invoked every five iterations.\r\n"]
[386.646, "o", "\r\n"]
[386.656, "o", "    verbose : bool, default=False\r\n"]
[386.666, "o", "        To control the verbosity of the procedure.\r\n"]
[386.676, "o", "\r\n"]
[386.686, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[386.696, "o", "        Used for randomly initializing the dictionary. Pass an int for\r\n"]
[386.706, "o", "        reproducible results across multiple function calls.\r\n"]
[386.716, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[386.726, "o", "\r\n"]
[386.736, "o", "    return_n_iter : bool, default=False\r\n"]
[386.746, "o", "        Whether or not to return the number of iterations.\r\n"]
[386.756, "o", "\r\n"]
[386.766, "o", "    positive_dict : bool, default=False\r\n"]
[386.776, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[386.786, "o", "\r\n"]
[386.796, "o", "        .. versionadded:: 0.20\r\n"]
[386.806, "o", "\r\n"]
[386.816, "o", "    positive_code : bool, default=False\r\n"]
[386.826, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[386.836, "o", "\r\n"]
[386.846, "o", "        .. versionadded:: 0.20\r\n"]
[386.856, "o", "\r\n"]
[386.866, "o", "    method_max_iter : int, default=1000\r\n"]
[386.876, "o", "        Maximum number of iterations to perform.\r\n"]
[386.886, "o", "\r\n"]
[386.896, "o", "        .. versionadded:: 0.22\r\n"]
[386.906, "o", "\r\n"]
[386.916, "o", "    Returns\r\n"]
[386.926, "o", "    -------\r\n"]
[386.936, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[386.946, "o", "        The sparse code factor in the matrix factorization.\r\n"]
[386.956, "o", "\r\n"]
[386.966, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[386.976, "o", "        The dictionary factor in the matrix factorization.\r\n"]
[386.986, "o", "\r\n"]
[386.996, "o", "    errors : array\r\n"]
[387.006, "o", "        Vector of errors at each iteration.\r\n"]
[387.016, "o", "\r\n"]
[387.026, "o", "    n_iter : int\r\n"]
[387.036, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[387.046, "o", "        set to True.\r\n"]
[387.056, "o", "\r\n"]
[387.066, "o", "    See Also\r\n"]
[387.076, "o", "    --------\r\n"]
[387.086, "o", "    dict_learning_online : Solve a dictionary learning matrix factorization\r\n"]
[387.096, "o", "        problem online.\r\n"]
[387.106, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[387.116, "o", "    MiniBatchDictionaryLearning : A faster, less accurate version\r\n"]
[387.126, "o", "        of the dictionary learning algorithm.\r\n"]
[387.136, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[387.146, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[387.156, "o", "    \"\"\"\r\n"]
[387.166, "o", "    estimator = DictionaryLearning(\r\n"]
[387.176, "o", "        n_components=n_components,\r\n"]
[387.186, "o", "        alpha=alpha,\r\n"]
[387.196, "o", "        max_iter=max_iter,\r\n"]
[387.206, "o", "        tol=tol,\r\n"]
[387.216, "o", "        fit_algorithm=method,\r\n"]
[387.226, "o", "        n_jobs=n_jobs,\r\n"]
[387.236, "o", "        dict_init=dict_init,\r\n"]
[387.246, "o", "        callback=callback,\r\n"]
[387.256, "o", "        code_init=code_init,\r\n"]
[387.266, "o", "        verbose=verbose,\r\n"]
[387.276, "o", "        random_state=random_state,\r\n"]
[387.286, "o", "        positive_code=positive_code,\r\n"]
[387.296, "o", "        positive_dict=positive_dict,\r\n"]
[387.306, "o", "        transform_max_iter=method_max_iter,\r\n"]
[387.316, "o", "    )\r\n"]
[387.326, "o", "    code = estimator.fit_transform(X)\r\n"]
[387.336, "o", "    if return_n_iter:\r\n"]
[387.346, "o", "        return (\r\n"]
[387.356, "o", "            code,\r\n"]
[387.366, "o", "            estimator.components_,\r\n"]
[387.376, "o", "            estimator.error_,\r\n"]
[387.386, "o", "            estimator.n_iter_,\r\n"]
[387.396, "o", "        )\r\n"]
[387.406, "o", "    return code, estimator.components_, estimator.error_\r\n"]
[387.416, "o", "\r\n"]
[387.426, "o", "\r\n"]
[387.436, "o", "class _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n"]
[387.446, "o", "    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n"]
[387.456, "o", "\r\n"]
[387.466, "o", "    def __init__(\r\n"]
[387.476, "o", "        self,\r\n"]
[387.486, "o", "        transform_algorithm,\r\n"]
[387.496, "o", "        transform_n_nonzero_coefs,\r\n"]
[387.506, "o", "        transform_alpha,\r\n"]
[387.516, "o", "        split_sign,\r\n"]
[387.526, "o", "        n_jobs,\r\n"]
[387.536, "o", "        positive_code,\r\n"]
[387.546, "o", "        transform_max_iter,\r\n"]
[387.556, "o", "    ):\r\n"]
[387.566, "o", "        self.transform_algorithm = transform_algorithm\r\n"]
[387.576, "o", "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n"]
[387.586, "o", "        self.transform_alpha = transform_alpha\r\n"]
[387.596, "o", "        self.transform_max_iter = transform_max_iter\r\n"]
[387.606, "o", "        self.split_sign = split_sign\r\n"]
[387.616, "o", "        self.n_jobs = n_jobs\r\n"]
[387.626, "o", "        self.positive_code = positive_code\r\n"]
[387.636, "o", "\r\n"]
[387.646, "o", "    def _transform(self, X, dictionary):\r\n"]
[387.656, "o", "        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n"]
[387.666, "o", "        SparseCoder.\"\"\"\r\n"]
[387.676, "o", "        X = self._validate_data(X, reset=False)\r\n"]
[387.686, "o", "\r\n"]
[387.696, "o", "        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n"]
[387.706, "o", "            transform_alpha = self.alpha\r\n"]
[387.716, "o", "        else:\r\n"]
[387.726, "o", "            transform_alpha = self.transform_alpha\r\n"]
[387.736, "o", "\r\n"]
[387.746, "o", "        code = sparse_encode(\r\n"]
[387.756, "o", "            X,\r\n"]
[387.766, "o", "            dictionary,\r\n"]
[387.776, "o", "            algorithm=self.transform_algorithm,\r\n"]
[387.786, "o", "            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n"]
[387.796, "o", "            alpha=transform_alpha,\r\n"]
[387.806, "o", "            max_iter=self.transform_max_iter,\r\n"]
[387.816, "o", "            n_jobs=self.n_jobs,\r\n"]
[387.826, "o", "            positive=self.positive_code,\r\n"]
[387.836, "o", "        )\r\n"]
[387.846, "o", "\r\n"]
[387.856, "o", "        if self.split_sign:\r\n"]
[387.866, "o", "            # feature vector is split into a positive and negative side\r\n"]
[387.876, "o", "            n_samples, n_features = code.shape\r\n"]
[387.886, "o", "            split_code = np.empty((n_samples, 2 * n_features))\r\n"]
[387.896, "o", "            split_code[:, :n_features] = np.maximum(code, 0)\r\n"]
[387.906, "o", "            split_code[:, n_features:] = -np.minimum(code, 0)\r\n"]
[387.916, "o", "            code = split_code\r\n"]
[387.926, "o", "\r\n"]
[387.936, "o", "        return code\r\n"]
[387.946, "o", "\r\n"]
[387.956, "o", "    def transform(self, X):\r\n"]
[387.966, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[387.976, "o", "\r\n"]
[387.986, "o", "        Coding method is determined by the object parameter\r\n"]
[387.996, "o", "        `transform_algorithm`.\r\n"]
[388.006, "o", "\r\n"]
[388.016, "o", "        Parameters\r\n"]
[388.026, "o", "        ----------\r\n"]
[388.036, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[388.046, "o", "            Test data to be transformed, must have the same number of\r\n"]
[388.056, "o", "            features as the data used to train the model.\r\n"]
[388.066, "o", "\r\n"]
[388.076, "o", "        Returns\r\n"]
[388.086, "o", "        -------\r\n"]
[388.096, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[388.106, "o", "            Transformed data.\r\n"]
[388.116, "o", "        \"\"\"\r\n"]
[388.126, "o", "        check_is_fitted(self)\r\n"]
[388.136, "o", "        return self._transform(X, self.components_)\r\n"]
[388.146, "o", "\r\n"]
[388.156, "o", "\r\n"]
[388.166, "o", "class SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n"]
[388.176, "o", "    \"\"\"Sparse coding.\r\n"]
[388.186, "o", "\r\n"]
[388.196, "o", "    Finds a sparse representation of data against a fixed, precomputed\r\n"]
[388.206, "o", "    dictionary.\r\n"]
[388.216, "o", "\r\n"]
[388.226, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[388.236, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[388.246, "o", "\r\n"]
[388.256, "o", "        X ~= code * dictionary\r\n"]
[388.266, "o", "\r\n"]
[388.276, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[388.286, "o", "\r\n"]
[388.296, "o", "    Parameters\r\n"]
[388.306, "o", "    ----------\r\n"]
[388.316, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[388.326, "o", "        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n"]
[388.336, "o", "        normalized to unit norm.\r\n"]
[388.346, "o", "\r\n"]
[388.356, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[388.366, "o", "            'threshold'}, default='omp'\r\n"]
[388.376, "o", "        Algorithm used to transform the data:\r\n"]
[388.386, "o", "\r\n"]
[388.396, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[388.406, "o", "          (`linear_model.lars_path`);\r\n"]
[388.416, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[388.426, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[388.436, "o", "          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n"]
[388.446, "o", "          the estimated components are sparse;\r\n"]
[388.456, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[388.466, "o", "          solution;\r\n"]
[388.476, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[388.486, "o", "          the projection ``dictionary * X'``.\r\n"]
[388.496, "o", "\r\n"]
[388.506, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[388.516, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[388.526, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[388.536, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[388.546, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[388.556, "o", "\r\n"]
[388.566, "o", "    transform_alpha : float, default=None\r\n"]
[388.576, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[388.586, "o", "        penalty applied to the L1 norm.\r\n"]
[388.596, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[388.606, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[388.616, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[388.626, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[388.636, "o", "        `n_nonzero_coefs`.\r\n"]
[388.646, "o", "        If `None`, default to 1.\r\n"]
[388.656, "o", "\r\n"]
[388.666, "o", "    split_sign : bool, default=False\r\n"]
[388.676, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[388.686, "o", "        its negative part and its positive part. This can improve the\r\n"]
[388.696, "o", "        performance of downstream classifiers.\r\n"]
[388.706, "o", "\r\n"]
[388.716, "o", "    n_jobs : int, default=None\r\n"]
[388.726, "o", "        Number of parallel jobs to run.\r\n"]
[388.736, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[388.746, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[388.756, "o", "        for more details.\r\n"]
[388.766, "o", "\r\n"]
[388.776, "o", "    positive_code : bool, default=False\r\n"]
[388.786, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[388.796, "o", "\r\n"]
[388.806, "o", "        .. versionadded:: 0.20\r\n"]
[388.816, "o", "\r\n"]
[388.826, "o", "    transform_max_iter : int, default=1000\r\n"]
[388.836, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[388.846, "o", "        `lasso_lars`.\r\n"]
[388.856, "o", "\r\n"]
[388.866, "o", "        .. versionadded:: 0.22\r\n"]
[388.876, "o", "\r\n"]
[388.886, "o", "    Attributes\r\n"]
[388.896, "o", "    ----------\r\n"]
[388.906, "o", "    n_components_ : int\r\n"]
[388.916, "o", "        Number of atoms.\r\n"]
[388.926, "o", "\r\n"]
[388.936, "o", "    n_features_in_ : int\r\n"]
[388.946, "o", "        Number of features seen during :term:`fit`.\r\n"]
[388.956, "o", "\r\n"]
[388.966, "o", "        .. versionadded:: 0.24\r\n"]
[388.976, "o", "\r\n"]
[388.986, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[388.996, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[389.006, "o", "        has feature names that are all strings.\r\n"]
[389.016, "o", "\r\n"]
[389.026, "o", "        .. versionadded:: 1.0\r\n"]
[389.036, "o", "\r\n"]
[389.046, "o", "    See Also\r\n"]
[389.056, "o", "    --------\r\n"]
[389.066, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[389.076, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n"]
[389.086, "o", "        dictionary learning algorithm.\r\n"]
[389.096, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[389.106, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[389.116, "o", "    sparse_encode : Sparse coding where each row of the result is the solution\r\n"]
[389.126, "o", "        to a sparse coding problem.\r\n"]
[389.136, "o", "\r\n"]
[389.146, "o", "    Examples\r\n"]
[389.156, "o", "    --------\r\n"]
[389.166, "o", "    >>> import numpy as np\r\n"]
[389.176, "o", "    >>> from sklearn.decomposition import SparseCoder\r\n"]
[389.186, "o", "    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n"]
[389.196, "o", "    >>> dictionary = np.array(\r\n"]
[389.206, "o", "    ...     [[0, 1, 0],\r\n"]
[389.216, "o", "    ...      [-1, -1, 2],\r\n"]
[389.226, "o", "    ...      [1, 1, 1],\r\n"]
[389.236, "o", "    ...      [0, 1, 1],\r\n"]
[389.246, "o", "    ...      [0, 2, 1]],\r\n"]
[389.256, "o", "    ...    dtype=np.float64\r\n"]
[389.266, "o", "    ... )\r\n"]
[389.276, "o", "    >>> coder = SparseCoder(\r\n"]
[389.286, "o", "    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n"]
[389.296, "o", "    ...     transform_alpha=1e-10,\r\n"]
[389.306, "o", "    ... )\r\n"]
[389.316, "o", "    >>> coder.transform(X)\r\n"]
[389.326, "o", "    array([[ 0.,  0., -1.,  0.,  0.],\r\n"]
[389.336, "o", "           [ 0.,  1.,  1.,  0.,  0.]])\r\n"]
[389.346, "o", "    \"\"\"\r\n"]
[389.356, "o", "\r\n"]
[389.366, "o", "    _required_parameters = [\"dictionary\"]\r\n"]
[389.376, "o", "\r\n"]
[389.386, "o", "    def __init__(\r\n"]
[389.396, "o", "        self,\r\n"]
[389.406, "o", "        dictionary,\r\n"]
[389.416, "o", "        *,\r\n"]
[389.426, "o", "        transform_algorithm=\"omp\",\r\n"]
[389.436, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[389.446, "o", "        transform_alpha=None,\r\n"]
[389.456, "o", "        split_sign=False,\r\n"]
[389.466, "o", "        n_jobs=None,\r\n"]
[389.476, "o", "        positive_code=False,\r\n"]
[389.486, "o", "        transform_max_iter=1000,\r\n"]
[389.496, "o", "    ):\r\n"]
[389.506, "o", "        super().__init__(\r\n"]
[389.516, "o", "            transform_algorithm,\r\n"]
[389.526, "o", "            transform_n_nonzero_coefs,\r\n"]
[389.536, "o", "            transform_alpha,\r\n"]
[389.546, "o", "            split_sign,\r\n"]
[389.556, "o", "            n_jobs,\r\n"]
[389.566, "o", "            positive_code,\r\n"]
[389.576, "o", "            transform_max_iter,\r\n"]
[389.586, "o", "        )\r\n"]
[389.596, "o", "        self.dictionary = dictionary\r\n"]
[389.606, "o", "\r\n"]
[389.616, "o", "    def fit(self, X, y=None):\r\n"]
[389.626, "o", "        \"\"\"Do nothing and return the estimator unchanged.\r\n"]
[389.636, "o", "\r\n"]
[389.646, "o", "        This method is just there to implement the usual API and hence\r\n"]
[389.656, "o", "        work in pipelines.\r\n"]
[389.666, "o", "\r\n"]
[389.676, "o", "        Parameters\r\n"]
[389.686, "o", "        ----------\r\n"]
[389.696, "o", "        X : Ignored\r\n"]
[389.706, "o", "            Not used, present for API consistency by convention.\r\n"]
[389.716, "o", "\r\n"]
[389.726, "o", "        y : Ignored\r\n"]
[389.736, "o", "            Not used, present for API consistency by convention.\r\n"]
[389.746, "o", "\r\n"]
[389.756, "o", "        Returns\r\n"]
[389.766, "o", "        -------\r\n"]
[389.776, "o", "        self : object\r\n"]
[389.786, "o", "            Returns the instance itself.\r\n"]
[389.796, "o", "        \"\"\"\r\n"]
[389.806, "o", "        return self\r\n"]
[389.816, "o", "\r\n"]
[389.826, "o", "    def transform(self, X, y=None):\r\n"]
[389.836, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[389.846, "o", "\r\n"]
[389.856, "o", "        Coding method is determined by the object parameter\r\n"]
[389.866, "o", "        `transform_algorithm`.\r\n"]
[389.876, "o", "\r\n"]
[389.886, "o", "        Parameters\r\n"]
[389.896, "o", "        ----------\r\n"]
[389.906, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[389.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[389.926, "o", "            and `n_features` is the number of features.\r\n"]
[389.936, "o", "\r\n"]
[390.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[390.002, "i", "sed -n '1800,2600p' sklearn/decomposition/_dict_learning.py\r"]
[390.004, "o", "sed -n '1800,2600p' sklearn/decomposition/_dict_learning.py\r\n"]
[390.016, "o", "\u001b[?2004l\r\n"]
[390.026, "o", "        self.fit_transform(X)\r\n"]
[390.036, "o", "        return self\r\n"]
[390.046, "o", "\r\n"]
[390.056, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[390.066, "o", "    def fit_transform(self, X, y=None):\r\n"]
[390.076, "o", "        \"\"\"Fit the model from data in X and return the transformed data.\r\n"]
[390.086, "o", "\r\n"]
[390.096, "o", "        Parameters\r\n"]
[390.106, "o", "        ----------\r\n"]
[390.116, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[390.126, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[390.136, "o", "            and `n_features` is the number of features.\r\n"]
[390.146, "o", "\r\n"]
[390.156, "o", "        y : Ignored\r\n"]
[390.166, "o", "            Not used, present for API consistency by convention.\r\n"]
[390.176, "o", "\r\n"]
[390.186, "o", "        Returns\r\n"]
[390.196, "o", "        -------\r\n"]
[390.206, "o", "        V : ndarray of shape (n_samples, n_components)\r\n"]
[390.216, "o", "            Transformed data.\r\n"]
[390.226, "o", "        \"\"\"\r\n"]
[390.236, "o", "        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n"]
[390.246, "o", "\r\n"]
[390.256, "o", "        method = \"lasso_\" + self.fit_algorithm\r\n"]
[390.266, "o", "\r\n"]
[390.276, "o", "        random_state = check_random_state(self.random_state)\r\n"]
[390.286, "o", "        X = self._validate_data(X)\r\n"]
[390.296, "o", "\r\n"]
[390.306, "o", "        if self.n_components is None:\r\n"]
[390.316, "o", "            n_components = X.shape[1]\r\n"]
[390.326, "o", "        else:\r\n"]
[390.336, "o", "            n_components = self.n_components\r\n"]
[390.346, "o", "\r\n"]
[390.356, "o", "        V, U, E, self.n_iter_ = _dict_learning(\r\n"]
[390.366, "o", "            X,\r\n"]
[390.376, "o", "            n_components,\r\n"]
[390.386, "o", "            alpha=self.alpha,\r\n"]
[390.396, "o", "            tol=self.tol,\r\n"]
[390.406, "o", "            max_iter=self.max_iter,\r\n"]
[390.416, "o", "            method=method,\r\n"]
[390.426, "o", "            method_max_iter=self.transform_max_iter,\r\n"]
[390.436, "o", "            n_jobs=self.n_jobs,\r\n"]
[390.446, "o", "            code_init=self.code_init,\r\n"]
[390.456, "o", "            dict_init=self.dict_init,\r\n"]
[390.466, "o", "            callback=self.callback,\r\n"]
[390.476, "o", "            verbose=self.verbose,\r\n"]
[390.486, "o", "            random_state=random_state,\r\n"]
[390.496, "o", "            return_n_iter=True,\r\n"]
[390.506, "o", "            positive_dict=self.positive_dict,\r\n"]
[390.516, "o", "            positive_code=self.positive_code,\r\n"]
[390.526, "o", "        )\r\n"]
[390.536, "o", "        self.components_ = U\r\n"]
[390.546, "o", "        self.error_ = E\r\n"]
[390.556, "o", "\r\n"]
[390.566, "o", "        return V\r\n"]
[390.576, "o", "\r\n"]
[390.586, "o", "    @property\r\n"]
[390.596, "o", "    def _n_features_out(self):\r\n"]
[390.606, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[390.616, "o", "        return self.components_.shape[0]\r\n"]
[390.626, "o", "\r\n"]
[390.636, "o", "    def _more_tags(self):\r\n"]
[390.646, "o", "        return {\r\n"]
[390.656, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[390.666, "o", "        }\r\n"]
[390.676, "o", "\r\n"]
[390.686, "o", "\r\n"]
[390.696, "o", "class MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[390.706, "o", "    \"\"\"Mini-batch dictionary learning.\r\n"]
[390.716, "o", "\r\n"]
[390.726, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[390.736, "o", "    encoding the fitted data.\r\n"]
[390.746, "o", "\r\n"]
[390.756, "o", "    Solves the optimization problem::\r\n"]
[390.766, "o", "\r\n"]
[390.776, "o", "       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[390.786, "o", "                    (U,V)\r\n"]
[390.796, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[390.806, "o", "\r\n"]
[390.816, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[390.826, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[390.836, "o", "    of all the entries in the matrix.\r\n"]
[390.846, "o", "\r\n"]
[390.856, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[390.866, "o", "\r\n"]
[390.876, "o", "    Parameters\r\n"]
[390.886, "o", "    ----------\r\n"]
[390.896, "o", "    n_components : int, default=None\r\n"]
[390.906, "o", "        Number of dictionary elements to extract.\r\n"]
[390.916, "o", "\r\n"]
[390.926, "o", "    alpha : float, default=1\r\n"]
[390.936, "o", "        Sparsity controlling parameter.\r\n"]
[390.946, "o", "\r\n"]
[390.956, "o", "    n_iter : int, default=1000\r\n"]
[390.966, "o", "        Total number of iterations over data batches to perform.\r\n"]
[390.976, "o", "\r\n"]
[390.986, "o", "        .. deprecated:: 1.1\r\n"]
[390.996, "o", "           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[391.006, "o", "           ``max_iter`` instead.\r\n"]
[391.016, "o", "\r\n"]
[391.026, "o", "    max_iter : int, default=None\r\n"]
[391.036, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[391.046, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[391.056, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[391.066, "o", "\r\n"]
[391.076, "o", "        .. versionadded:: 1.1\r\n"]
[391.086, "o", "\r\n"]
[391.096, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[391.106, "o", "        The algorithm used:\r\n"]
[391.116, "o", "\r\n"]
[391.126, "o", "        - `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[391.136, "o", "          problem (`linear_model.lars_path`)\r\n"]
[391.146, "o", "        - `'cd'`: uses the coordinate descent method to compute the\r\n"]
[391.156, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[391.166, "o", "          the estimated components are sparse.\r\n"]
[391.176, "o", "\r\n"]
[391.186, "o", "    n_jobs : int, default=None\r\n"]
[391.196, "o", "        Number of parallel jobs to run.\r\n"]
[391.206, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[391.216, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[391.226, "o", "        for more details.\r\n"]
[391.236, "o", "\r\n"]
[391.246, "o", "    batch_size : int, default=256\r\n"]
[391.256, "o", "        Number of samples in each mini-batch.\r\n"]
[391.266, "o", "\r\n"]
[391.276, "o", "        .. versionchanged:: 1.3\r\n"]
[391.286, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[391.296, "o", "\r\n"]
[391.306, "o", "    shuffle : bool, default=True\r\n"]
[391.316, "o", "        Whether to shuffle the samples before forming batches.\r\n"]
[391.326, "o", "\r\n"]
[391.336, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[391.346, "o", "        Initial value of the dictionary for warm restart scenarios.\r\n"]
[391.356, "o", "\r\n"]
[391.366, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[391.376, "o", "            'threshold'}, default='omp'\r\n"]
[391.386, "o", "        Algorithm used to transform the data:\r\n"]
[391.396, "o", "\r\n"]
[391.406, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[391.416, "o", "          (`linear_model.lars_path`);\r\n"]
[391.426, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[391.436, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[391.446, "o", "          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n"]
[391.456, "o", "          if the estimated components are sparse.\r\n"]
[391.466, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[391.476, "o", "          solution.\r\n"]
[391.486, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[391.496, "o", "          the projection ``dictionary * X'``.\r\n"]
[391.506, "o", "\r\n"]
[391.516, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[391.526, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[391.536, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[391.546, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[391.556, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[391.566, "o", "\r\n"]
[391.576, "o", "    transform_alpha : float, default=None\r\n"]
[391.586, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[391.596, "o", "        penalty applied to the L1 norm.\r\n"]
[391.606, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[391.616, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[391.626, "o", "        If `None`, defaults to `alpha`.\r\n"]
[391.636, "o", "\r\n"]
[391.646, "o", "        .. versionchanged:: 1.2\r\n"]
[391.656, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[391.666, "o", "\r\n"]
[391.676, "o", "    verbose : bool or int, default=False\r\n"]
[391.686, "o", "        To control the verbosity of the procedure.\r\n"]
[391.696, "o", "\r\n"]
[391.706, "o", "    split_sign : bool, default=False\r\n"]
[391.716, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[391.726, "o", "        its negative part and its positive part. This can improve the\r\n"]
[391.736, "o", "        performance of downstream classifiers.\r\n"]
[391.746, "o", "\r\n"]
[391.756, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[391.766, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[391.776, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[391.786, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[391.796, "o", "        results across multiple function calls.\r\n"]
[391.806, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[391.816, "o", "\r\n"]
[391.826, "o", "    positive_code : bool, default=False\r\n"]
[391.836, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[391.846, "o", "\r\n"]
[391.856, "o", "        .. versionadded:: 0.20\r\n"]
[391.866, "o", "\r\n"]
[391.876, "o", "    positive_dict : bool, default=False\r\n"]
[391.886, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[391.896, "o", "\r\n"]
[391.906, "o", "        .. versionadded:: 0.20\r\n"]
[391.916, "o", "\r\n"]
[391.926, "o", "    transform_max_iter : int, default=1000\r\n"]
[391.936, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[391.946, "o", "        `'lasso_lars'`.\r\n"]
[391.956, "o", "\r\n"]
[391.966, "o", "        .. versionadded:: 0.22\r\n"]
[391.976, "o", "\r\n"]
[391.986, "o", "    callback : callable, default=None\r\n"]
[391.996, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[392.006, "o", "\r\n"]
[392.016, "o", "        .. versionadded:: 1.1\r\n"]
[392.026, "o", "\r\n"]
[392.036, "o", "    tol : float, default=1e-3\r\n"]
[392.046, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[392.056, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[392.066, "o", "\r\n"]
[392.076, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[392.086, "o", "        `tol` to 0.0.\r\n"]
[392.096, "o", "\r\n"]
[392.106, "o", "        .. versionadded:: 1.1\r\n"]
[392.116, "o", "\r\n"]
[392.126, "o", "    max_no_improvement : int, default=10\r\n"]
[392.136, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[392.146, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[392.156, "o", "        `max_iter` is not None.\r\n"]
[392.166, "o", "\r\n"]
[392.176, "o", "        To disable convergence detection based on cost function, set\r\n"]
[392.186, "o", "        `max_no_improvement` to None.\r\n"]
[392.196, "o", "\r\n"]
[392.206, "o", "        .. versionadded:: 1.1\r\n"]
[392.216, "o", "\r\n"]
[392.226, "o", "    Attributes\r\n"]
[392.236, "o", "    ----------\r\n"]
[392.246, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[392.256, "o", "        Components extracted from the data.\r\n"]
[392.266, "o", "\r\n"]
[392.276, "o", "    n_features_in_ : int\r\n"]
[392.286, "o", "        Number of features seen during :term:`fit`.\r\n"]
[392.296, "o", "\r\n"]
[392.306, "o", "        .. versionadded:: 0.24\r\n"]
[392.316, "o", "\r\n"]
[392.326, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[392.336, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[392.346, "o", "        has feature names that are all strings.\r\n"]
[392.356, "o", "\r\n"]
[392.366, "o", "        .. versionadded:: 1.0\r\n"]
[392.376, "o", "\r\n"]
[392.386, "o", "    n_iter_ : int\r\n"]
[392.396, "o", "        Number of iterations over the full dataset.\r\n"]
[392.406, "o", "\r\n"]
[392.416, "o", "    n_steps_ : int\r\n"]
[392.426, "o", "        Number of mini-batches processed.\r\n"]
[392.436, "o", "\r\n"]
[392.446, "o", "        .. versionadded:: 1.1\r\n"]
[392.456, "o", "\r\n"]
[392.466, "o", "    See Also\r\n"]
[392.476, "o", "    --------\r\n"]
[392.486, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[392.496, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[392.506, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[392.516, "o", "        precomputed dictionary.\r\n"]
[392.526, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[392.536, "o", "\r\n"]
[392.546, "o", "    References\r\n"]
[392.556, "o", "    ----------\r\n"]
[392.566, "o", "\r\n"]
[392.576, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[392.586, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[392.596, "o", "\r\n"]
[392.606, "o", "    Examples\r\n"]
[392.616, "o", "    --------\r\n"]
[392.626, "o", "    >>> import numpy as np\r\n"]
[392.636, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[392.646, "o", "    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n"]
[392.656, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[392.666, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[392.676, "o", "    ...     random_state=42)\r\n"]
[392.686, "o", "    >>> dict_learner = MiniBatchDictionaryLearning(\r\n"]
[392.696, "o", "    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n"]
[392.706, "o", "    ...     transform_alpha=0.1, random_state=42)\r\n"]
[392.716, "o", "    >>> X_transformed = dict_learner.fit_transform(X)\r\n"]
[392.726, "o", "\r\n"]
[392.736, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[392.746, "o", "\r\n"]
[392.756, "o", "    >>> np.mean(X_transformed == 0) < 0.5\r\n"]
[392.766, "o", "    True\r\n"]
[392.776, "o", "\r\n"]
[392.786, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[392.796, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[392.806, "o", "    the original signal:\r\n"]
[392.816, "o", "\r\n"]
[392.826, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[392.836, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[392.846, "o", "    0.057...\r\n"]
[392.856, "o", "    \"\"\"\r\n"]
[392.866, "o", "\r\n"]
[392.876, "o", "    _parameter_constraints: dict = {\r\n"]
[392.886, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[392.896, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[392.906, "o", "        \"n_iter\": [\r\n"]
[392.916, "o", "            Interval(Integral, 0, None, closed=\"left\"),\r\n"]
[392.926, "o", "            Hidden(StrOptions({\"deprecated\"})),\r\n"]
[392.936, "o", "        ],\r\n"]
[392.946, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n"]
[392.956, "o", "        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n"]
[392.966, "o", "        \"n_jobs\": [None, Integral],\r\n"]
[392.976, "o", "        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n"]
[392.986, "o", "        \"shuffle\": [\"boolean\"],\r\n"]
[392.996, "o", "        \"dict_init\": [None, np.ndarray],\r\n"]
[393.006, "o", "        \"transform_algorithm\": [\r\n"]
[393.016, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[393.026, "o", "        ],\r\n"]
[393.036, "o", "        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[393.046, "o", "        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[393.056, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[393.066, "o", "        \"split_sign\": [\"boolean\"],\r\n"]
[393.076, "o", "        \"random_state\": [\"random_state\"],\r\n"]
[393.086, "o", "        \"positive_code\": [\"boolean\"],\r\n"]
[393.096, "o", "        \"positive_dict\": [\"boolean\"],\r\n"]
[393.106, "o", "        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[393.116, "o", "        \"callback\": [None, callable],\r\n"]
[393.126, "o", "        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[393.136, "o", "        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n"]
[393.146, "o", "    }\r\n"]
[393.156, "o", "\r\n"]
[393.166, "o", "    def __init__(\r\n"]
[393.176, "o", "        self,\r\n"]
[393.186, "o", "        n_components=None,\r\n"]
[393.196, "o", "        *,\r\n"]
[393.206, "o", "        alpha=1,\r\n"]
[393.216, "o", "        n_iter=\"deprecated\",\r\n"]
[393.226, "o", "        max_iter=None,\r\n"]
[393.236, "o", "        fit_algorithm=\"lars\",\r\n"]
[393.246, "o", "        n_jobs=None,\r\n"]
[393.256, "o", "        batch_size=256,\r\n"]
[393.266, "o", "        shuffle=True,\r\n"]
[393.276, "o", "        dict_init=None,\r\n"]
[393.286, "o", "        transform_algorithm=\"omp\",\r\n"]
[393.296, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[393.306, "o", "        transform_alpha=None,\r\n"]
[393.316, "o", "        verbose=False,\r\n"]
[393.326, "o", "        split_sign=False,\r\n"]
[393.336, "o", "        random_state=None,\r\n"]
[393.346, "o", "        positive_code=False,\r\n"]
[393.356, "o", "        positive_dict=False,\r\n"]
[393.366, "o", "        transform_max_iter=1000,\r\n"]
[393.376, "o", "        callback=None,\r\n"]
[393.386, "o", "        tol=1e-3,\r\n"]
[393.396, "o", "        max_no_improvement=10,\r\n"]
[393.406, "o", "    ):\r\n"]
[393.416, "o", "        super().__init__(\r\n"]
[393.426, "o", "            transform_algorithm,\r\n"]
[393.436, "o", "            transform_n_nonzero_coefs,\r\n"]
[393.446, "o", "            transform_alpha,\r\n"]
[393.456, "o", "            split_sign,\r\n"]
[393.466, "o", "            n_jobs,\r\n"]
[393.476, "o", "            positive_code,\r\n"]
[393.486, "o", "            transform_max_iter,\r\n"]
[393.496, "o", "        )\r\n"]
[393.506, "o", "        self.n_components = n_components\r\n"]
[393.516, "o", "        self.alpha = alpha\r\n"]
[393.526, "o", "        self.n_iter = n_iter\r\n"]
[393.536, "o", "        self.max_iter = max_iter\r\n"]
[393.546, "o", "        self.fit_algorithm = fit_algorithm\r\n"]
[393.556, "o", "        self.dict_init = dict_init\r\n"]
[393.566, "o", "        self.verbose = verbose\r\n"]
[393.576, "o", "        self.shuffle = shuffle\r\n"]
[393.586, "o", "        self.batch_size = batch_size\r\n"]
[393.596, "o", "        self.split_sign = split_sign\r\n"]
[393.606, "o", "        self.random_state = random_state\r\n"]
[393.616, "o", "        self.positive_dict = positive_dict\r\n"]
[393.626, "o", "        self.callback = callback\r\n"]
[393.636, "o", "        self.max_no_improvement = max_no_improvement\r\n"]
[393.646, "o", "        self.tol = tol\r\n"]
[393.656, "o", "\r\n"]
[393.666, "o", "    def _check_params(self, X):\r\n"]
[393.676, "o", "        # n_components\r\n"]
[393.686, "o", "        self._n_components = self.n_components\r\n"]
[393.696, "o", "        if self._n_components is None:\r\n"]
[393.706, "o", "            self._n_components = X.shape[1]\r\n"]
[393.716, "o", "\r\n"]
[393.726, "o", "        # fit_algorithm\r\n"]
[393.736, "o", "        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n"]
[393.746, "o", "        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n"]
[393.756, "o", "\r\n"]
[393.766, "o", "        # batch_size\r\n"]
[393.776, "o", "        self._batch_size = min(self.batch_size, X.shape[0])\r\n"]
[393.786, "o", "\r\n"]
[393.796, "o", "    def _initialize_dict(self, X, random_state):\r\n"]
[393.806, "o", "        \"\"\"Initialization of the dictionary.\"\"\"\r\n"]
[393.816, "o", "        if self.dict_init is not None:\r\n"]
[393.826, "o", "            dictionary = self.dict_init\r\n"]
[393.836, "o", "        else:\r\n"]
[393.846, "o", "            # Init V with SVD of X\r\n"]
[393.856, "o", "            _, S, dictionary = randomized_svd(\r\n"]
[393.866, "o", "                X, self._n_components, random_state=random_state\r\n"]
[393.876, "o", "            )\r\n"]
[393.886, "o", "            dictionary = S[:, np.newaxis] * dictionary\r\n"]
[393.896, "o", "\r\n"]
[393.906, "o", "        if self._n_components <= len(dictionary):\r\n"]
[393.916, "o", "            dictionary = dictionary[: self._n_components, :]\r\n"]
[393.926, "o", "        else:\r\n"]
[393.936, "o", "            dictionary = np.concatenate(\r\n"]
[393.946, "o", "                (\r\n"]
[393.956, "o", "                    dictionary,\r\n"]
[393.966, "o", "                    np.zeros(\r\n"]
[393.976, "o", "                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n"]
[393.986, "o", "                        dtype=dictionary.dtype,\r\n"]
[393.996, "o", "                    ),\r\n"]
[394.006, "o", "                )\r\n"]
[394.016, "o", "            )\r\n"]
[394.026, "o", "\r\n"]
[394.036, "o", "        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n"]
[394.046, "o", "        dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[394.056, "o", "\r\n"]
[394.066, "o", "        return dictionary\r\n"]
[394.076, "o", "\r\n"]
[394.086, "o", "    def _update_inner_stats(self, X, code, batch_size, step):\r\n"]
[394.096, "o", "        \"\"\"Update the inner stats inplace.\"\"\"\r\n"]
[394.106, "o", "        if step < batch_size - 1:\r\n"]
[394.116, "o", "            theta = (step + 1) * batch_size\r\n"]
[394.126, "o", "        else:\r\n"]
[394.136, "o", "            theta = batch_size**2 + step + 1 - batch_size\r\n"]
[394.146, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[394.156, "o", "\r\n"]
[394.166, "o", "        self._A *= beta\r\n"]
[394.176, "o", "        self._A += code.T @ code / batch_size\r\n"]
[394.186, "o", "        self._B *= beta\r\n"]
[394.196, "o", "        self._B += X.T @ code / batch_size\r\n"]
[394.206, "o", "\r\n"]
[394.216, "o", "    def _minibatch_step(self, X, dictionary, random_state, step):\r\n"]
[394.226, "o", "        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n"]
[394.236, "o", "        batch_size = X.shape[0]\r\n"]
[394.246, "o", "\r\n"]
[394.256, "o", "        # Compute code for this batch\r\n"]
[394.266, "o", "        code = _sparse_encode(\r\n"]
[394.276, "o", "            X,\r\n"]
[394.286, "o", "            dictionary,\r\n"]
[394.296, "o", "            algorithm=self._fit_algorithm,\r\n"]
[394.306, "o", "            alpha=self.alpha,\r\n"]
[394.316, "o", "            n_jobs=self.n_jobs,\r\n"]
[394.326, "o", "            positive=self.positive_code,\r\n"]
[394.336, "o", "            max_iter=self.transform_max_iter,\r\n"]
[394.346, "o", "            verbose=self.verbose,\r\n"]
[394.356, "o", "        )\r\n"]
[394.366, "o", "\r\n"]
[394.376, "o", "        batch_cost = (\r\n"]
[394.386, "o", "            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n"]
[394.396, "o", "            + self.alpha * np.sum(np.abs(code))\r\n"]
[394.406, "o", "        ) / batch_size\r\n"]
[394.416, "o", "\r\n"]
[394.426, "o", "        # Update inner stats\r\n"]
[394.436, "o", "        self._update_inner_stats(X, code, batch_size, step)\r\n"]
[394.446, "o", "\r\n"]
[394.456, "o", "        # Update dictionary\r\n"]
[394.466, "o", "        _update_dict(\r\n"]
[394.476, "o", "            dictionary,\r\n"]
[394.486, "o", "            X,\r\n"]
[394.496, "o", "            code,\r\n"]
[394.506, "o", "            self._A,\r\n"]
[394.516, "o", "            self._B,\r\n"]
[394.526, "o", "            verbose=self.verbose,\r\n"]
[394.536, "o", "            random_state=random_state,\r\n"]
[394.546, "o", "            positive=self.positive_dict,\r\n"]
[394.556, "o", "        )\r\n"]
[394.566, "o", "\r\n"]
[394.576, "o", "        return batch_cost\r\n"]
[394.586, "o", "\r\n"]
[394.596, "o", "    def _check_convergence(\r\n"]
[394.606, "o", "        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n"]
[394.616, "o", "    ):\r\n"]
[394.626, "o", "        \"\"\"Helper function to encapsulate the early stopping logic.\r\n"]
[394.636, "o", "\r\n"]
[394.646, "o", "        Early stopping is based on two factors:\r\n"]
[394.656, "o", "        - A small change of the dictionary between two minibatch updates. This is\r\n"]
[394.666, "o", "          controlled by the tol parameter.\r\n"]
[394.676, "o", "        - No more improvement on a smoothed estimate of the objective function for a\r\n"]
[394.686, "o", "          a certain number of consecutive minibatch updates. This is controlled by\r\n"]
[394.696, "o", "          the max_no_improvement parameter.\r\n"]
[394.706, "o", "        \"\"\"\r\n"]
[394.716, "o", "        batch_size = X.shape[0]\r\n"]
[394.726, "o", "\r\n"]
[394.736, "o", "        # counts steps starting from 1 for user friendly verbose mode.\r\n"]
[394.746, "o", "        step = step + 1\r\n"]
[394.756, "o", "\r\n"]
[394.766, "o", "        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n"]
[394.776, "o", "        # too bad value\r\n"]
[394.786, "o", "        if step <= min(100, n_samples / batch_size):\r\n"]
[394.796, "o", "            if self.verbose:\r\n"]
[394.806, "o", "                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n"]
[394.816, "o", "            return False\r\n"]
[394.826, "o", "\r\n"]
[394.836, "o", "        # Compute an Exponentially Weighted Average of the cost function to\r\n"]
[394.846, "o", "        # monitor the convergence while discarding minibatch-local stochastic\r\n"]
[394.856, "o", "        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n"]
[394.866, "o", "        if self._ewa_cost is None:\r\n"]
[394.876, "o", "            self._ewa_cost = batch_cost\r\n"]
[394.886, "o", "        else:\r\n"]
[394.896, "o", "            alpha = batch_size / (n_samples + 1)\r\n"]
[394.906, "o", "            alpha = min(alpha, 1)\r\n"]
[394.916, "o", "            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n"]
[394.926, "o", "\r\n"]
[394.936, "o", "        if self.verbose:\r\n"]
[395.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[395.002, "i", "cd asv_benchmarks\r"]
[395.004, "o", "cd asv_benchmarks\r\n"]
[395.006, "o", "\u001b[?2004l\r\n"]
[400.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[400.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[400.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[401.654, "o", "\u001b[?2004l\r\n"]
[403.302, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[405.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[405.002, "i", "cd ..\r"]
[405.004, "o", "cd ..\r\n"]
[405.006, "o", "\u001b[?2004l\r\n"]
[410.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[410.002, "i", "grep -n \"_minibatch_step\" sklearn/decomposition/_dict_learning.py\r"]
[410.004, "o", "grep -n \"_minibatch_step\" sklearn/decomposition/_dict_learning.py\r\n"]
[410.9948, "o", "\u001b[?2004l\r\n"]
[411.9836, "o", "\u001b[32m\u001b[K2219\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    def \u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(self, X, dictionary, random_state, step):\r\n"]
[412.9724, "o", "\u001b[32m\u001b[K2399\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                batch_cost = self.\u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(\r\n"]
[413.9612, "o", "\u001b[32m\u001b[K2425\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                self.\u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(X_train[batch], dictionary, self._random_state, i)\r\n"]
[415.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[415.002, "i", "grep -n \"def dict_learning_online\\|def _update_dict\\|def _mini_batch_dictionary_learning\" sklearn/decomposition/_di\r"]
[415.004, "o", "grep -n \"def dict_learning_online\\|def _update_dict\\|def _mini_batch_dictionary_learning\" sklearn/decomposition/_di\r\n"]
[416.242, "o", "ict_learning.py\r\n"]
[417.478, "o", "\u001b[?2004l\r\n"]
[418.714, "o", "\u001b[32m\u001b[K458\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kdef _update_dict\u001b[m\u001b[K(\r\n"]
[420.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[420.002, "i", "sed -n '1,1200p' sklearn/decomposition/_dict_learning.py\r"]
[420.004, "o", "sed -n '1,1200p' sklearn/decomposition/_dict_learning.py\r\n"]
[420.016, "o", "\u001b[?2004l\r\n"]
[420.026, "o", "\"\"\" Dictionary learning.\r\n"]
[420.036, "o", "\"\"\"\r\n"]
[420.046, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[420.056, "o", "# License: BSD 3 clause\r\n"]
[420.066, "o", "\r\n"]
[420.076, "o", "import itertools\r\n"]
[420.086, "o", "import sys\r\n"]
[420.096, "o", "import time\r\n"]
[420.106, "o", "import warnings\r\n"]
[420.116, "o", "from math import ceil\r\n"]
[420.126, "o", "from numbers import Integral, Real\r\n"]
[420.136, "o", "\r\n"]
[420.146, "o", "import numpy as np\r\n"]
[420.156, "o", "from joblib import effective_n_jobs\r\n"]
[420.166, "o", "from scipy import linalg\r\n"]
[420.176, "o", "\r\n"]
[420.186, "o", "from ..base import (\r\n"]
[420.196, "o", "    BaseEstimator,\r\n"]
[420.206, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[420.216, "o", "    TransformerMixin,\r\n"]
[420.226, "o", "    _fit_context,\r\n"]
[420.236, "o", ")\r\n"]
[420.246, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[420.256, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[420.266, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[420.276, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[420.286, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[420.296, "o", "from ..utils.validation import check_is_fitted\r\n"]
[420.306, "o", "\r\n"]
[420.316, "o", "\r\n"]
[420.326, "o", "def _check_positive_coding(method, positive):\r\n"]
[420.336, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[420.346, "o", "        raise ValueError(\r\n"]
[420.356, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[420.366, "o", "        )\r\n"]
[420.376, "o", "\r\n"]
[420.386, "o", "\r\n"]
[420.396, "o", "def _sparse_encode_precomputed(\r\n"]
[420.406, "o", "    X,\r\n"]
[420.416, "o", "    dictionary,\r\n"]
[420.426, "o", "    *,\r\n"]
[420.436, "o", "    gram=None,\r\n"]
[420.446, "o", "    cov=None,\r\n"]
[420.456, "o", "    algorithm=\"lasso_lars\",\r\n"]
[420.466, "o", "    regularization=None,\r\n"]
[420.476, "o", "    copy_cov=True,\r\n"]
[420.486, "o", "    init=None,\r\n"]
[420.496, "o", "    max_iter=1000,\r\n"]
[420.506, "o", "    verbose=0,\r\n"]
[420.516, "o", "    positive=False,\r\n"]
[420.526, "o", "):\r\n"]
[420.536, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[420.546, "o", "\r\n"]
[420.556, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[420.566, "o", "\r\n"]
[420.576, "o", "    Parameters\r\n"]
[420.586, "o", "    ----------\r\n"]
[420.596, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[420.606, "o", "        Data matrix.\r\n"]
[420.616, "o", "\r\n"]
[420.626, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[420.636, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[420.646, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[420.656, "o", "\r\n"]
[420.666, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[420.676, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[420.686, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[420.696, "o", "\r\n"]
[420.706, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[420.716, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[420.726, "o", "\r\n"]
[420.736, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[420.746, "o", "            default='lasso_lars'\r\n"]
[420.756, "o", "        The algorithm used:\r\n"]
[420.766, "o", "\r\n"]
[420.776, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[420.786, "o", "          (`linear_model.lars_path`);\r\n"]
[420.796, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[420.806, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[420.816, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[420.826, "o", "          the estimated components are sparse;\r\n"]
[420.836, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[420.846, "o", "          solution;\r\n"]
[420.856, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[420.866, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[420.876, "o", "\r\n"]
[420.886, "o", "    regularization : int or float, default=None\r\n"]
[420.896, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[420.906, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[420.916, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[420.926, "o", "\r\n"]
[420.936, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[420.946, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[420.956, "o", "        `algorithm='lasso_cd'`.\r\n"]
[420.966, "o", "\r\n"]
[420.976, "o", "    max_iter : int, default=1000\r\n"]
[420.986, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[420.996, "o", "        `'lasso_lars'`.\r\n"]
[421.006, "o", "\r\n"]
[421.016, "o", "    copy_cov : bool, default=True\r\n"]
[421.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[421.036, "o", "        be overwritten.\r\n"]
[421.046, "o", "\r\n"]
[421.056, "o", "    verbose : int, default=0\r\n"]
[421.066, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[421.076, "o", "\r\n"]
[421.086, "o", "    positive: bool, default=False\r\n"]
[421.096, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[421.106, "o", "\r\n"]
[421.116, "o", "        .. versionadded:: 0.20\r\n"]
[421.126, "o", "\r\n"]
[421.136, "o", "    Returns\r\n"]
[421.146, "o", "    -------\r\n"]
[421.156, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[421.166, "o", "        The sparse codes.\r\n"]
[421.176, "o", "    \"\"\"\r\n"]
[421.186, "o", "    n_samples, n_features = X.shape\r\n"]
[421.196, "o", "    n_components = dictionary.shape[0]\r\n"]
[421.206, "o", "\r\n"]
[421.216, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[421.226, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[421.236, "o", "        try:\r\n"]
[421.246, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[421.256, "o", "\r\n"]
[421.266, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[421.276, "o", "            # corrects the verbosity level.\r\n"]
[421.286, "o", "            lasso_lars = LassoLars(\r\n"]
[421.296, "o", "                alpha=alpha,\r\n"]
[421.306, "o", "                fit_intercept=False,\r\n"]
[421.316, "o", "                verbose=verbose,\r\n"]
[421.326, "o", "                precompute=gram,\r\n"]
[421.336, "o", "                fit_path=False,\r\n"]
[421.346, "o", "                positive=positive,\r\n"]
[421.356, "o", "                max_iter=max_iter,\r\n"]
[421.366, "o", "            )\r\n"]
[421.376, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[421.386, "o", "            new_code = lasso_lars.coef_\r\n"]
[421.396, "o", "        finally:\r\n"]
[421.406, "o", "            np.seterr(**err_mgt)\r\n"]
[421.416, "o", "\r\n"]
[421.426, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[421.436, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[421.446, "o", "\r\n"]
[421.456, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[421.466, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[421.476, "o", "        # argument that we could pass in from Lasso.\r\n"]
[421.486, "o", "        clf = Lasso(\r\n"]
[421.496, "o", "            alpha=alpha,\r\n"]
[421.506, "o", "            fit_intercept=False,\r\n"]
[421.516, "o", "            precompute=gram,\r\n"]
[421.526, "o", "            max_iter=max_iter,\r\n"]
[421.536, "o", "            warm_start=True,\r\n"]
[421.546, "o", "            positive=positive,\r\n"]
[421.556, "o", "        )\r\n"]
[421.566, "o", "\r\n"]
[421.576, "o", "        if init is not None:\r\n"]
[421.586, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[421.596, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[421.606, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[421.616, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[421.626, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[421.636, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[421.646, "o", "                init = np.array(init)\r\n"]
[421.656, "o", "            clf.coef_ = init\r\n"]
[421.666, "o", "\r\n"]
[421.676, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[421.686, "o", "        new_code = clf.coef_\r\n"]
[421.696, "o", "\r\n"]
[421.706, "o", "    elif algorithm == \"lars\":\r\n"]
[421.716, "o", "        try:\r\n"]
[421.726, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[421.736, "o", "\r\n"]
[421.746, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[421.756, "o", "            # corrects the verbosity level.\r\n"]
[421.766, "o", "            lars = Lars(\r\n"]
[421.776, "o", "                fit_intercept=False,\r\n"]
[421.786, "o", "                verbose=verbose,\r\n"]
[421.796, "o", "                precompute=gram,\r\n"]
[421.806, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[421.816, "o", "                fit_path=False,\r\n"]
[421.826, "o", "            )\r\n"]
[421.836, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[421.846, "o", "            new_code = lars.coef_\r\n"]
[421.856, "o", "        finally:\r\n"]
[421.866, "o", "            np.seterr(**err_mgt)\r\n"]
[421.876, "o", "\r\n"]
[421.886, "o", "    elif algorithm == \"threshold\":\r\n"]
[421.896, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[421.906, "o", "        if positive:\r\n"]
[421.916, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[421.926, "o", "\r\n"]
[421.936, "o", "    elif algorithm == \"omp\":\r\n"]
[421.946, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[421.956, "o", "            Gram=gram,\r\n"]
[421.966, "o", "            Xy=cov,\r\n"]
[421.976, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[421.986, "o", "            tol=None,\r\n"]
[421.996, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[422.006, "o", "            copy_Xy=copy_cov,\r\n"]
[422.016, "o", "        ).T\r\n"]
[422.026, "o", "\r\n"]
[422.036, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[422.046, "o", "\r\n"]
[422.056, "o", "\r\n"]
[422.066, "o", "@validate_params(\r\n"]
[422.076, "o", "    {\r\n"]
[422.086, "o", "        \"X\": [\"array-like\"],\r\n"]
[422.096, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[422.106, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[422.116, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[422.126, "o", "        \"algorithm\": [\r\n"]
[422.136, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[422.146, "o", "        ],\r\n"]
[422.156, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[422.166, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[422.176, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[422.186, "o", "        \"init\": [\"array-like\", None],\r\n"]
[422.196, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[422.206, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[422.216, "o", "        \"check_input\": [\"boolean\"],\r\n"]
[422.226, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[422.236, "o", "        \"positive\": [\"boolean\"],\r\n"]
[422.246, "o", "    },\r\n"]
[422.256, "o", "    prefer_skip_nested_validation=True,\r\n"]
[422.266, "o", ")\r\n"]
[422.276, "o", "# XXX : could be moved to the linear_model module\r\n"]
[422.286, "o", "def sparse_encode(\r\n"]
[422.296, "o", "    X,\r\n"]
[422.306, "o", "    dictionary,\r\n"]
[422.316, "o", "    *,\r\n"]
[422.326, "o", "    gram=None,\r\n"]
[422.336, "o", "    cov=None,\r\n"]
[422.346, "o", "    algorithm=\"lasso_lars\",\r\n"]
[422.356, "o", "    n_nonzero_coefs=None,\r\n"]
[422.366, "o", "    alpha=None,\r\n"]
[422.376, "o", "    copy_cov=True,\r\n"]
[422.386, "o", "    init=None,\r\n"]
[422.396, "o", "    max_iter=1000,\r\n"]
[422.406, "o", "    n_jobs=None,\r\n"]
[422.416, "o", "    check_input=True,\r\n"]
[422.426, "o", "    verbose=0,\r\n"]
[422.436, "o", "    positive=False,\r\n"]
[422.446, "o", "):\r\n"]
[422.456, "o", "    \"\"\"Sparse coding.\r\n"]
[422.466, "o", "\r\n"]
[422.476, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[422.486, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[422.496, "o", "\r\n"]
[422.506, "o", "        X ~= code * dictionary\r\n"]
[422.516, "o", "\r\n"]
[422.526, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[422.536, "o", "\r\n"]
[422.546, "o", "    Parameters\r\n"]
[422.556, "o", "    ----------\r\n"]
[422.566, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[422.576, "o", "        Data matrix.\r\n"]
[422.586, "o", "\r\n"]
[422.596, "o", "    dictionary : array-like of shape (n_components, n_features)\r\n"]
[422.606, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[422.616, "o", "        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[422.626, "o", "        output.\r\n"]
[422.636, "o", "\r\n"]
[422.646, "o", "    gram : array-like of shape (n_components, n_components), default=None\r\n"]
[422.656, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n"]
[422.666, "o", "\r\n"]
[422.676, "o", "    cov : array-like of shape (n_components, n_samples), default=None\r\n"]
[422.686, "o", "        Precomputed covariance, `dictionary' * X`.\r\n"]
[422.696, "o", "\r\n"]
[422.706, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[422.716, "o", "            default='lasso_lars'\r\n"]
[422.726, "o", "        The algorithm used:\r\n"]
[422.736, "o", "\r\n"]
[422.746, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[422.756, "o", "          (`linear_model.lars_path`);\r\n"]
[422.766, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[422.776, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[422.786, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[422.796, "o", "          the estimated components are sparse;\r\n"]
[422.806, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[422.816, "o", "          solution;\r\n"]
[422.826, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[422.836, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[422.846, "o", "\r\n"]
[422.856, "o", "    n_nonzero_coefs : int, default=None\r\n"]
[422.866, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[422.876, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[422.886, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[422.896, "o", "        `n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[422.906, "o", "\r\n"]
[422.916, "o", "    alpha : float, default=None\r\n"]
[422.926, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[422.936, "o", "        penalty applied to the L1 norm.\r\n"]
[422.946, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[422.956, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[422.966, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[422.976, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[422.986, "o", "        `n_nonzero_coefs`.\r\n"]
[422.996, "o", "        If `None`, default to 1.\r\n"]
[423.006, "o", "\r\n"]
[423.016, "o", "    copy_cov : bool, default=True\r\n"]
[423.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[423.036, "o", "        be overwritten.\r\n"]
[423.046, "o", "\r\n"]
[423.056, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[423.066, "o", "        Initialization value of the sparse codes. Only used if\r\n"]
[423.076, "o", "        `algorithm='lasso_cd'`.\r\n"]
[423.086, "o", "\r\n"]
[423.096, "o", "    max_iter : int, default=1000\r\n"]
[423.106, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[423.116, "o", "        `'lasso_lars'`.\r\n"]
[423.126, "o", "\r\n"]
[423.136, "o", "    n_jobs : int, default=None\r\n"]
[423.146, "o", "        Number of parallel jobs to run.\r\n"]
[423.156, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[423.166, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[423.176, "o", "        for more details.\r\n"]
[423.186, "o", "\r\n"]
[423.196, "o", "    check_input : bool, default=True\r\n"]
[423.206, "o", "        If `False`, the input arrays X and dictionary will not be checked.\r\n"]
[423.216, "o", "\r\n"]
[423.226, "o", "    verbose : int, default=0\r\n"]
[423.236, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[423.246, "o", "\r\n"]
[423.256, "o", "    positive : bool, default=False\r\n"]
[423.266, "o", "        Whether to enforce positivity when finding the encoding.\r\n"]
[423.276, "o", "\r\n"]
[423.286, "o", "        .. versionadded:: 0.20\r\n"]
[423.296, "o", "\r\n"]
[423.306, "o", "    Returns\r\n"]
[423.316, "o", "    -------\r\n"]
[423.326, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[423.336, "o", "        The sparse codes.\r\n"]
[423.346, "o", "\r\n"]
[423.356, "o", "    See Also\r\n"]
[423.366, "o", "    --------\r\n"]
[423.376, "o", "    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n"]
[423.386, "o", "        path using LARS algorithm.\r\n"]
[423.396, "o", "    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n"]
[423.406, "o", "    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n"]
[423.416, "o", "    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n"]
[423.426, "o", "        dictionary.\r\n"]
[423.436, "o", "    \"\"\"\r\n"]
[423.446, "o", "    if check_input:\r\n"]
[423.456, "o", "        if algorithm == \"lasso_cd\":\r\n"]
[423.466, "o", "            dictionary = check_array(\r\n"]
[423.476, "o", "                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n"]
[423.486, "o", "            )\r\n"]
[423.496, "o", "            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n"]
[423.506, "o", "        else:\r\n"]
[423.516, "o", "            dictionary = check_array(dictionary)\r\n"]
[423.526, "o", "            X = check_array(X)\r\n"]
[423.536, "o", "\r\n"]
[423.546, "o", "    if dictionary.shape[1] != X.shape[1]:\r\n"]
[423.556, "o", "        raise ValueError(\r\n"]
[423.566, "o", "            \"Dictionary and X have different numbers of features:\"\r\n"]
[423.576, "o", "            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n"]
[423.586, "o", "        )\r\n"]
[423.596, "o", "\r\n"]
[423.606, "o", "    _check_positive_coding(algorithm, positive)\r\n"]
[423.616, "o", "\r\n"]
[423.626, "o", "    return _sparse_encode(\r\n"]
[423.636, "o", "        X,\r\n"]
[423.646, "o", "        dictionary,\r\n"]
[423.656, "o", "        gram=gram,\r\n"]
[423.666, "o", "        cov=cov,\r\n"]
[423.676, "o", "        algorithm=algorithm,\r\n"]
[423.686, "o", "        n_nonzero_coefs=n_nonzero_coefs,\r\n"]
[423.696, "o", "        alpha=alpha,\r\n"]
[423.706, "o", "        copy_cov=copy_cov,\r\n"]
[423.716, "o", "        init=init,\r\n"]
[423.726, "o", "        max_iter=max_iter,\r\n"]
[423.736, "o", "        n_jobs=n_jobs,\r\n"]
[423.746, "o", "        verbose=verbose,\r\n"]
[423.756, "o", "        positive=positive,\r\n"]
[423.766, "o", "    )\r\n"]
[423.776, "o", "\r\n"]
[423.786, "o", "\r\n"]
[423.796, "o", "def _sparse_encode(\r\n"]
[423.806, "o", "    X,\r\n"]
[423.816, "o", "    dictionary,\r\n"]
[423.826, "o", "    *,\r\n"]
[423.836, "o", "    gram=None,\r\n"]
[423.846, "o", "    cov=None,\r\n"]
[423.856, "o", "    algorithm=\"lasso_lars\",\r\n"]
[423.866, "o", "    n_nonzero_coefs=None,\r\n"]
[423.876, "o", "    alpha=None,\r\n"]
[423.886, "o", "    copy_cov=True,\r\n"]
[423.896, "o", "    init=None,\r\n"]
[423.906, "o", "    max_iter=1000,\r\n"]
[423.916, "o", "    n_jobs=None,\r\n"]
[423.926, "o", "    verbose=0,\r\n"]
[423.936, "o", "    positive=False,\r\n"]
[423.946, "o", "):\r\n"]
[423.956, "o", "    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[423.966, "o", "\r\n"]
[423.976, "o", "    n_samples, n_features = X.shape\r\n"]
[423.986, "o", "    n_components = dictionary.shape[0]\r\n"]
[423.996, "o", "\r\n"]
[424.006, "o", "    if algorithm in (\"lars\", \"omp\"):\r\n"]
[424.016, "o", "        regularization = n_nonzero_coefs\r\n"]
[424.026, "o", "        if regularization is None:\r\n"]
[424.036, "o", "            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[424.046, "o", "    else:\r\n"]
[424.056, "o", "        regularization = alpha\r\n"]
[424.066, "o", "        if regularization is None:\r\n"]
[424.076, "o", "            regularization = 1.0\r\n"]
[424.086, "o", "\r\n"]
[424.096, "o", "    if gram is None and algorithm != \"threshold\":\r\n"]
[424.106, "o", "        gram = np.dot(dictionary, dictionary.T)\r\n"]
[424.116, "o", "\r\n"]
[424.126, "o", "    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[424.136, "o", "        copy_cov = False\r\n"]
[424.146, "o", "        cov = np.dot(dictionary, X.T)\r\n"]
[424.156, "o", "\r\n"]
[424.166, "o", "    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[424.176, "o", "        code = _sparse_encode_precomputed(\r\n"]
[424.186, "o", "            X,\r\n"]
[424.196, "o", "            dictionary,\r\n"]
[424.206, "o", "            gram=gram,\r\n"]
[424.216, "o", "            cov=cov,\r\n"]
[424.226, "o", "            algorithm=algorithm,\r\n"]
[424.236, "o", "            regularization=regularization,\r\n"]
[424.246, "o", "            copy_cov=copy_cov,\r\n"]
[424.256, "o", "            init=init,\r\n"]
[424.266, "o", "            max_iter=max_iter,\r\n"]
[424.276, "o", "            verbose=verbose,\r\n"]
[424.286, "o", "            positive=positive,\r\n"]
[424.296, "o", "        )\r\n"]
[424.306, "o", "        return code\r\n"]
[424.316, "o", "\r\n"]
[424.326, "o", "    # Enter parallel code block\r\n"]
[424.336, "o", "    n_samples = X.shape[0]\r\n"]
[424.346, "o", "    n_components = dictionary.shape[0]\r\n"]
[424.356, "o", "    code = np.empty((n_samples, n_components))\r\n"]
[424.366, "o", "    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[424.376, "o", "\r\n"]
[424.386, "o", "    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[424.396, "o", "        delayed(_sparse_encode_precomputed)(\r\n"]
[424.406, "o", "            X[this_slice],\r\n"]
[424.416, "o", "            dictionary,\r\n"]
[424.426, "o", "            gram=gram,\r\n"]
[424.436, "o", "            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[424.446, "o", "            algorithm=algorithm,\r\n"]
[424.456, "o", "            regularization=regularization,\r\n"]
[424.466, "o", "            copy_cov=copy_cov,\r\n"]
[424.476, "o", "            init=init[this_slice] if init is not None else None,\r\n"]
[424.486, "o", "            max_iter=max_iter,\r\n"]
[424.496, "o", "            verbose=verbose,\r\n"]
[424.506, "o", "            positive=positive,\r\n"]
[424.516, "o", "        )\r\n"]
[424.526, "o", "        for this_slice in slices\r\n"]
[424.536, "o", "    )\r\n"]
[424.546, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[424.556, "o", "        code[this_slice] = this_view\r\n"]
[424.566, "o", "    return code\r\n"]
[424.576, "o", "\r\n"]
[424.586, "o", "\r\n"]
[424.596, "o", "def _update_dict(\r\n"]
[424.606, "o", "    dictionary,\r\n"]
[424.616, "o", "    Y,\r\n"]
[424.626, "o", "    code,\r\n"]
[424.636, "o", "    A=None,\r\n"]
[424.646, "o", "    B=None,\r\n"]
[424.656, "o", "    verbose=False,\r\n"]
[424.666, "o", "    random_state=None,\r\n"]
[424.676, "o", "    positive=False,\r\n"]
[424.686, "o", "):\r\n"]
[424.696, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[424.706, "o", "\r\n"]
[424.716, "o", "    Parameters\r\n"]
[424.726, "o", "    ----------\r\n"]
[424.736, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[424.746, "o", "        Value of the dictionary at the previous iteration.\r\n"]
[424.756, "o", "\r\n"]
[424.766, "o", "    Y : ndarray of shape (n_samples, n_features)\r\n"]
[424.776, "o", "        Data matrix.\r\n"]
[424.786, "o", "\r\n"]
[424.796, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[424.806, "o", "        Sparse coding of the data against which to optimize the dictionary.\r\n"]
[424.816, "o", "\r\n"]
[424.826, "o", "    A : ndarray of shape (n_components, n_components), default=None\r\n"]
[424.836, "o", "        Together with `B`, sufficient stats of the online model to update the\r\n"]
[424.846, "o", "        dictionary.\r\n"]
[424.856, "o", "\r\n"]
[424.866, "o", "    B : ndarray of shape (n_features, n_components), default=None\r\n"]
[424.876, "o", "        Together with `A`, sufficient stats of the online model to update the\r\n"]
[424.886, "o", "        dictionary.\r\n"]
[424.896, "o", "\r\n"]
[424.906, "o", "    verbose: bool, default=False\r\n"]
[424.916, "o", "        Degree of output the procedure will print.\r\n"]
[424.926, "o", "\r\n"]
[424.936, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[425.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[425.002, "i", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r"]
[425.004, "o", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r\n"]
[425.016, "o", "\u001b[?2004l\r\n"]
[425.026, "o", "    n_iter : int\r\n"]
[425.036, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[425.046, "o", "        set to True.\r\n"]
[425.056, "o", "\r\n"]
[425.066, "o", "    See Also\r\n"]
[425.076, "o", "    --------\r\n"]
[425.086, "o", "    dict_learning_online : Solve a dictionary learning matrix factorization\r\n"]
[425.096, "o", "        problem online.\r\n"]
[425.106, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[425.116, "o", "    MiniBatchDictionaryLearning : A faster, less accurate version\r\n"]
[425.126, "o", "        of the dictionary learning algorithm.\r\n"]
[425.136, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[425.146, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[425.156, "o", "    \"\"\"\r\n"]
[425.166, "o", "    estimator = DictionaryLearning(\r\n"]
[425.176, "o", "        n_components=n_components,\r\n"]
[425.186, "o", "        alpha=alpha,\r\n"]
[425.196, "o", "        max_iter=max_iter,\r\n"]
[425.206, "o", "        tol=tol,\r\n"]
[425.216, "o", "        fit_algorithm=method,\r\n"]
[425.226, "o", "        n_jobs=n_jobs,\r\n"]
[425.236, "o", "        dict_init=dict_init,\r\n"]
[425.246, "o", "        callback=callback,\r\n"]
[425.256, "o", "        code_init=code_init,\r\n"]
[425.266, "o", "        verbose=verbose,\r\n"]
[425.276, "o", "        random_state=random_state,\r\n"]
[425.286, "o", "        positive_code=positive_code,\r\n"]
[425.296, "o", "        positive_dict=positive_dict,\r\n"]
[425.306, "o", "        transform_max_iter=method_max_iter,\r\n"]
[425.316, "o", "    )\r\n"]
[425.326, "o", "    code = estimator.fit_transform(X)\r\n"]
[425.336, "o", "    if return_n_iter:\r\n"]
[425.346, "o", "        return (\r\n"]
[425.356, "o", "            code,\r\n"]
[425.366, "o", "            estimator.components_,\r\n"]
[425.376, "o", "            estimator.error_,\r\n"]
[425.386, "o", "            estimator.n_iter_,\r\n"]
[425.396, "o", "        )\r\n"]
[425.406, "o", "    return code, estimator.components_, estimator.error_\r\n"]
[425.416, "o", "\r\n"]
[425.426, "o", "\r\n"]
[425.436, "o", "class _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n"]
[425.446, "o", "    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n"]
[425.456, "o", "\r\n"]
[425.466, "o", "    def __init__(\r\n"]
[425.476, "o", "        self,\r\n"]
[425.486, "o", "        transform_algorithm,\r\n"]
[425.496, "o", "        transform_n_nonzero_coefs,\r\n"]
[425.506, "o", "        transform_alpha,\r\n"]
[425.516, "o", "        split_sign,\r\n"]
[425.526, "o", "        n_jobs,\r\n"]
[425.536, "o", "        positive_code,\r\n"]
[425.546, "o", "        transform_max_iter,\r\n"]
[425.556, "o", "    ):\r\n"]
[425.566, "o", "        self.transform_algorithm = transform_algorithm\r\n"]
[425.576, "o", "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n"]
[425.586, "o", "        self.transform_alpha = transform_alpha\r\n"]
[425.596, "o", "        self.transform_max_iter = transform_max_iter\r\n"]
[425.606, "o", "        self.split_sign = split_sign\r\n"]
[425.616, "o", "        self.n_jobs = n_jobs\r\n"]
[425.626, "o", "        self.positive_code = positive_code\r\n"]
[425.636, "o", "\r\n"]
[425.646, "o", "    def _transform(self, X, dictionary):\r\n"]
[425.656, "o", "        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n"]
[425.666, "o", "        SparseCoder.\"\"\"\r\n"]
[425.676, "o", "        X = self._validate_data(X, reset=False)\r\n"]
[425.686, "o", "\r\n"]
[425.696, "o", "        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n"]
[425.706, "o", "            transform_alpha = self.alpha\r\n"]
[425.716, "o", "        else:\r\n"]
[425.726, "o", "            transform_alpha = self.transform_alpha\r\n"]
[425.736, "o", "\r\n"]
[425.746, "o", "        code = sparse_encode(\r\n"]
[425.756, "o", "            X,\r\n"]
[425.766, "o", "            dictionary,\r\n"]
[425.776, "o", "            algorithm=self.transform_algorithm,\r\n"]
[425.786, "o", "            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n"]
[425.796, "o", "            alpha=transform_alpha,\r\n"]
[425.806, "o", "            max_iter=self.transform_max_iter,\r\n"]
[425.816, "o", "            n_jobs=self.n_jobs,\r\n"]
[425.826, "o", "            positive=self.positive_code,\r\n"]
[425.836, "o", "        )\r\n"]
[425.846, "o", "\r\n"]
[425.856, "o", "        if self.split_sign:\r\n"]
[425.866, "o", "            # feature vector is split into a positive and negative side\r\n"]
[425.876, "o", "            n_samples, n_features = code.shape\r\n"]
[425.886, "o", "            split_code = np.empty((n_samples, 2 * n_features))\r\n"]
[425.896, "o", "            split_code[:, :n_features] = np.maximum(code, 0)\r\n"]
[425.906, "o", "            split_code[:, n_features:] = -np.minimum(code, 0)\r\n"]
[425.916, "o", "            code = split_code\r\n"]
[425.926, "o", "\r\n"]
[425.936, "o", "        return code\r\n"]
[425.946, "o", "\r\n"]
[425.956, "o", "    def transform(self, X):\r\n"]
[425.966, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[425.976, "o", "\r\n"]
[425.986, "o", "        Coding method is determined by the object parameter\r\n"]
[425.996, "o", "        `transform_algorithm`.\r\n"]
[426.006, "o", "\r\n"]
[426.016, "o", "        Parameters\r\n"]
[426.026, "o", "        ----------\r\n"]
[426.036, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[426.046, "o", "            Test data to be transformed, must have the same number of\r\n"]
[426.056, "o", "            features as the data used to train the model.\r\n"]
[426.066, "o", "\r\n"]
[426.076, "o", "        Returns\r\n"]
[426.086, "o", "        -------\r\n"]
[426.096, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[426.106, "o", "            Transformed data.\r\n"]
[426.116, "o", "        \"\"\"\r\n"]
[426.126, "o", "        check_is_fitted(self)\r\n"]
[426.136, "o", "        return self._transform(X, self.components_)\r\n"]
[426.146, "o", "\r\n"]
[426.156, "o", "\r\n"]
[426.166, "o", "class SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n"]
[426.176, "o", "    \"\"\"Sparse coding.\r\n"]
[426.186, "o", "\r\n"]
[426.196, "o", "    Finds a sparse representation of data against a fixed, precomputed\r\n"]
[426.206, "o", "    dictionary.\r\n"]
[426.216, "o", "\r\n"]
[426.226, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[426.236, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[426.246, "o", "\r\n"]
[426.256, "o", "        X ~= code * dictionary\r\n"]
[426.266, "o", "\r\n"]
[426.276, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[426.286, "o", "\r\n"]
[426.296, "o", "    Parameters\r\n"]
[426.306, "o", "    ----------\r\n"]
[426.316, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[426.326, "o", "        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n"]
[426.336, "o", "        normalized to unit norm.\r\n"]
[426.346, "o", "\r\n"]
[426.356, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[426.366, "o", "            'threshold'}, default='omp'\r\n"]
[426.376, "o", "        Algorithm used to transform the data:\r\n"]
[426.386, "o", "\r\n"]
[426.396, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[426.406, "o", "          (`linear_model.lars_path`);\r\n"]
[426.416, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[426.426, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[426.436, "o", "          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n"]
[426.446, "o", "          the estimated components are sparse;\r\n"]
[426.456, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[426.466, "o", "          solution;\r\n"]
[426.476, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[426.486, "o", "          the projection ``dictionary * X'``.\r\n"]
[426.496, "o", "\r\n"]
[426.506, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[426.516, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[426.526, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[426.536, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[426.546, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[426.556, "o", "\r\n"]
[426.566, "o", "    transform_alpha : float, default=None\r\n"]
[426.576, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[426.586, "o", "        penalty applied to the L1 norm.\r\n"]
[426.596, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[426.606, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[426.616, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[426.626, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[426.636, "o", "        `n_nonzero_coefs`.\r\n"]
[426.646, "o", "        If `None`, default to 1.\r\n"]
[426.656, "o", "\r\n"]
[426.666, "o", "    split_sign : bool, default=False\r\n"]
[426.676, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[426.686, "o", "        its negative part and its positive part. This can improve the\r\n"]
[426.696, "o", "        performance of downstream classifiers.\r\n"]
[426.706, "o", "\r\n"]
[426.716, "o", "    n_jobs : int, default=None\r\n"]
[426.726, "o", "        Number of parallel jobs to run.\r\n"]
[426.736, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[426.746, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[426.756, "o", "        for more details.\r\n"]
[426.766, "o", "\r\n"]
[426.776, "o", "    positive_code : bool, default=False\r\n"]
[426.786, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[426.796, "o", "\r\n"]
[426.806, "o", "        .. versionadded:: 0.20\r\n"]
[426.816, "o", "\r\n"]
[426.826, "o", "    transform_max_iter : int, default=1000\r\n"]
[426.836, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[426.846, "o", "        `lasso_lars`.\r\n"]
[426.856, "o", "\r\n"]
[426.866, "o", "        .. versionadded:: 0.22\r\n"]
[426.876, "o", "\r\n"]
[426.886, "o", "    Attributes\r\n"]
[426.896, "o", "    ----------\r\n"]
[426.906, "o", "    n_components_ : int\r\n"]
[426.916, "o", "        Number of atoms.\r\n"]
[426.926, "o", "\r\n"]
[426.936, "o", "    n_features_in_ : int\r\n"]
[426.946, "o", "        Number of features seen during :term:`fit`.\r\n"]
[426.956, "o", "\r\n"]
[426.966, "o", "        .. versionadded:: 0.24\r\n"]
[426.976, "o", "\r\n"]
[426.986, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[426.996, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[427.006, "o", "        has feature names that are all strings.\r\n"]
[427.016, "o", "\r\n"]
[427.026, "o", "        .. versionadded:: 1.0\r\n"]
[427.036, "o", "\r\n"]
[427.046, "o", "    See Also\r\n"]
[427.056, "o", "    --------\r\n"]
[427.066, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[427.076, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n"]
[427.086, "o", "        dictionary learning algorithm.\r\n"]
[427.096, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[427.106, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[427.116, "o", "    sparse_encode : Sparse coding where each row of the result is the solution\r\n"]
[427.126, "o", "        to a sparse coding problem.\r\n"]
[427.136, "o", "\r\n"]
[427.146, "o", "    Examples\r\n"]
[427.156, "o", "    --------\r\n"]
[427.166, "o", "    >>> import numpy as np\r\n"]
[427.176, "o", "    >>> from sklearn.decomposition import SparseCoder\r\n"]
[427.186, "o", "    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n"]
[427.196, "o", "    >>> dictionary = np.array(\r\n"]
[427.206, "o", "    ...     [[0, 1, 0],\r\n"]
[427.216, "o", "    ...      [-1, -1, 2],\r\n"]
[427.226, "o", "    ...      [1, 1, 1],\r\n"]
[427.236, "o", "    ...      [0, 1, 1],\r\n"]
[427.246, "o", "    ...      [0, 2, 1]],\r\n"]
[427.256, "o", "    ...    dtype=np.float64\r\n"]
[427.266, "o", "    ... )\r\n"]
[427.276, "o", "    >>> coder = SparseCoder(\r\n"]
[427.286, "o", "    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n"]
[427.296, "o", "    ...     transform_alpha=1e-10,\r\n"]
[427.306, "o", "    ... )\r\n"]
[427.316, "o", "    >>> coder.transform(X)\r\n"]
[427.326, "o", "    array([[ 0.,  0., -1.,  0.,  0.],\r\n"]
[427.336, "o", "           [ 0.,  1.,  1.,  0.,  0.]])\r\n"]
[427.346, "o", "    \"\"\"\r\n"]
[427.356, "o", "\r\n"]
[427.366, "o", "    _required_parameters = [\"dictionary\"]\r\n"]
[427.376, "o", "\r\n"]
[427.386, "o", "    def __init__(\r\n"]
[427.396, "o", "        self,\r\n"]
[427.406, "o", "        dictionary,\r\n"]
[427.416, "o", "        *,\r\n"]
[427.426, "o", "        transform_algorithm=\"omp\",\r\n"]
[427.436, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[427.446, "o", "        transform_alpha=None,\r\n"]
[427.456, "o", "        split_sign=False,\r\n"]
[427.466, "o", "        n_jobs=None,\r\n"]
[427.476, "o", "        positive_code=False,\r\n"]
[427.486, "o", "        transform_max_iter=1000,\r\n"]
[427.496, "o", "    ):\r\n"]
[427.506, "o", "        super().__init__(\r\n"]
[427.516, "o", "            transform_algorithm,\r\n"]
[427.526, "o", "            transform_n_nonzero_coefs,\r\n"]
[427.536, "o", "            transform_alpha,\r\n"]
[427.546, "o", "            split_sign,\r\n"]
[427.556, "o", "            n_jobs,\r\n"]
[427.566, "o", "            positive_code,\r\n"]
[427.576, "o", "            transform_max_iter,\r\n"]
[427.586, "o", "        )\r\n"]
[427.596, "o", "        self.dictionary = dictionary\r\n"]
[427.606, "o", "\r\n"]
[427.616, "o", "    def fit(self, X, y=None):\r\n"]
[427.626, "o", "        \"\"\"Do nothing and return the estimator unchanged.\r\n"]
[427.636, "o", "\r\n"]
[427.646, "o", "        This method is just there to implement the usual API and hence\r\n"]
[427.656, "o", "        work in pipelines.\r\n"]
[427.666, "o", "\r\n"]
[427.676, "o", "        Parameters\r\n"]
[427.686, "o", "        ----------\r\n"]
[427.696, "o", "        X : Ignored\r\n"]
[427.706, "o", "            Not used, present for API consistency by convention.\r\n"]
[427.716, "o", "\r\n"]
[427.726, "o", "        y : Ignored\r\n"]
[427.736, "o", "            Not used, present for API consistency by convention.\r\n"]
[427.746, "o", "\r\n"]
[427.756, "o", "        Returns\r\n"]
[427.766, "o", "        -------\r\n"]
[427.776, "o", "        self : object\r\n"]
[427.786, "o", "            Returns the instance itself.\r\n"]
[427.796, "o", "        \"\"\"\r\n"]
[427.806, "o", "        return self\r\n"]
[427.816, "o", "\r\n"]
[427.826, "o", "    def transform(self, X, y=None):\r\n"]
[427.836, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[427.846, "o", "\r\n"]
[427.856, "o", "        Coding method is determined by the object parameter\r\n"]
[427.866, "o", "        `transform_algorithm`.\r\n"]
[427.876, "o", "\r\n"]
[427.886, "o", "        Parameters\r\n"]
[427.896, "o", "        ----------\r\n"]
[427.906, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[427.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[427.926, "o", "            and `n_features` is the number of features.\r\n"]
[427.936, "o", "\r\n"]
[427.946, "o", "        y : Ignored\r\n"]
[427.956, "o", "            Not used, present for API consistency by convention.\r\n"]
[427.966, "o", "\r\n"]
[427.976, "o", "        Returns\r\n"]
[427.986, "o", "        -------\r\n"]
[427.996, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[428.006, "o", "            Transformed data.\r\n"]
[428.016, "o", "        \"\"\"\r\n"]
[428.026, "o", "        return super()._transform(X, self.dictionary)\r\n"]
[428.036, "o", "\r\n"]
[428.046, "o", "    def _more_tags(self):\r\n"]
[428.056, "o", "        return {\r\n"]
[428.066, "o", "            \"requires_fit\": False,\r\n"]
[428.076, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[428.086, "o", "        }\r\n"]
[428.096, "o", "\r\n"]
[428.106, "o", "    @property\r\n"]
[428.116, "o", "    def n_components_(self):\r\n"]
[428.126, "o", "        \"\"\"Number of atoms.\"\"\"\r\n"]
[428.136, "o", "        return self.dictionary.shape[0]\r\n"]
[428.146, "o", "\r\n"]
[428.156, "o", "    @property\r\n"]
[428.166, "o", "    def n_features_in_(self):\r\n"]
[428.176, "o", "        \"\"\"Number of features seen during `fit`.\"\"\"\r\n"]
[428.186, "o", "        return self.dictionary.shape[1]\r\n"]
[428.196, "o", "\r\n"]
[428.206, "o", "    @property\r\n"]
[428.216, "o", "    def _n_features_out(self):\r\n"]
[428.226, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[428.236, "o", "        return self.n_components_\r\n"]
[428.246, "o", "\r\n"]
[428.256, "o", "\r\n"]
[428.266, "o", "class DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[428.276, "o", "    \"\"\"Dictionary learning.\r\n"]
[428.286, "o", "\r\n"]
[428.296, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[428.306, "o", "    encoding the fitted data.\r\n"]
[428.316, "o", "\r\n"]
[428.326, "o", "    Solves the optimization problem::\r\n"]
[428.336, "o", "\r\n"]
[428.346, "o", "        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[428.356, "o", "                    (U,V)\r\n"]
[428.366, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[428.376, "o", "\r\n"]
[428.386, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[428.396, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[428.406, "o", "    of all the entries in the matrix.\r\n"]
[428.416, "o", "\r\n"]
[428.426, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[428.436, "o", "\r\n"]
[428.446, "o", "    Parameters\r\n"]
[428.456, "o", "    ----------\r\n"]
[428.466, "o", "    n_components : int, default=None\r\n"]
[428.476, "o", "        Number of dictionary elements to extract. If None, then ``n_components``\r\n"]
[428.486, "o", "        is set to ``n_features``.\r\n"]
[428.496, "o", "\r\n"]
[428.506, "o", "    alpha : float, default=1.0\r\n"]
[428.516, "o", "        Sparsity controlling parameter.\r\n"]
[428.526, "o", "\r\n"]
[428.536, "o", "    max_iter : int, default=1000\r\n"]
[428.546, "o", "        Maximum number of iterations to perform.\r\n"]
[428.556, "o", "\r\n"]
[428.566, "o", "    tol : float, default=1e-8\r\n"]
[428.576, "o", "        Tolerance for numerical error.\r\n"]
[428.586, "o", "\r\n"]
[428.596, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[428.606, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[428.616, "o", "          problem (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[428.626, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[428.636, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n"]
[428.646, "o", "          faster if the estimated components are sparse.\r\n"]
[428.656, "o", "\r\n"]
[428.666, "o", "        .. versionadded:: 0.17\r\n"]
[428.676, "o", "           *cd* coordinate descent method to improve speed.\r\n"]
[428.686, "o", "\r\n"]
[428.696, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[428.706, "o", "            'threshold'}, default='omp'\r\n"]
[428.716, "o", "        Algorithm used to transform the data:\r\n"]
[428.726, "o", "\r\n"]
[428.736, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[428.746, "o", "          (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[428.756, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[428.766, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[428.776, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n"]
[428.786, "o", "          will be faster if the estimated components are sparse.\r\n"]
[428.796, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[428.806, "o", "          solution.\r\n"]
[428.816, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[428.826, "o", "          the projection ``dictionary * X'``.\r\n"]
[428.836, "o", "\r\n"]
[428.846, "o", "        .. versionadded:: 0.17\r\n"]
[428.856, "o", "           *lasso_cd* coordinate descent method to improve speed.\r\n"]
[428.866, "o", "\r\n"]
[428.876, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[428.886, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[428.896, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[428.906, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[428.916, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[428.926, "o", "\r\n"]
[428.936, "o", "    transform_alpha : float, default=None\r\n"]
[428.946, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[428.956, "o", "        penalty applied to the L1 norm.\r\n"]
[428.966, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[428.976, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[428.986, "o", "        If `None`, defaults to `alpha`.\r\n"]
[428.996, "o", "\r\n"]
[429.006, "o", "        .. versionchanged:: 1.2\r\n"]
[429.016, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[429.026, "o", "\r\n"]
[429.036, "o", "    n_jobs : int or None, default=None\r\n"]
[429.046, "o", "        Number of parallel jobs to run.\r\n"]
[429.056, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[429.066, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[429.076, "o", "        for more details.\r\n"]
[429.086, "o", "\r\n"]
[429.096, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[429.106, "o", "        Initial value for the code, for warm restart. Only used if `code_init`\r\n"]
[429.116, "o", "        and `dict_init` are not None.\r\n"]
[429.126, "o", "\r\n"]
[429.136, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[429.146, "o", "        Initial values for the dictionary, for warm restart. Only used if\r\n"]
[429.156, "o", "        `code_init` and `dict_init` are not None.\r\n"]
[429.166, "o", "\r\n"]
[429.176, "o", "    callback : callable, default=None\r\n"]
[429.186, "o", "        Callable that gets invoked every five iterations.\r\n"]
[429.196, "o", "\r\n"]
[429.206, "o", "        .. versionadded:: 1.3\r\n"]
[429.216, "o", "\r\n"]
[429.226, "o", "    verbose : bool, default=False\r\n"]
[429.236, "o", "        To control the verbosity of the procedure.\r\n"]
[429.246, "o", "\r\n"]
[429.256, "o", "    split_sign : bool, default=False\r\n"]
[429.266, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[429.276, "o", "        its negative part and its positive part. This can improve the\r\n"]
[429.286, "o", "        performance of downstream classifiers.\r\n"]
[429.296, "o", "\r\n"]
[429.306, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[429.316, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[429.326, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[429.336, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[429.346, "o", "        results across multiple function calls.\r\n"]
[429.356, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[429.366, "o", "\r\n"]
[429.376, "o", "    positive_code : bool, default=False\r\n"]
[429.386, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[429.396, "o", "\r\n"]
[429.406, "o", "        .. versionadded:: 0.20\r\n"]
[429.416, "o", "\r\n"]
[429.426, "o", "    positive_dict : bool, default=False\r\n"]
[429.436, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[429.446, "o", "\r\n"]
[429.456, "o", "        .. versionadded:: 0.20\r\n"]
[429.466, "o", "\r\n"]
[429.476, "o", "    transform_max_iter : int, default=1000\r\n"]
[429.486, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[429.496, "o", "        `'lasso_lars'`.\r\n"]
[429.506, "o", "\r\n"]
[429.516, "o", "        .. versionadded:: 0.22\r\n"]
[429.526, "o", "\r\n"]
[429.536, "o", "    Attributes\r\n"]
[429.546, "o", "    ----------\r\n"]
[429.556, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[429.566, "o", "        dictionary atoms extracted from the data\r\n"]
[429.576, "o", "\r\n"]
[429.586, "o", "    error_ : array\r\n"]
[429.596, "o", "        vector of errors at each iteration\r\n"]
[429.606, "o", "\r\n"]
[429.616, "o", "    n_features_in_ : int\r\n"]
[429.626, "o", "        Number of features seen during :term:`fit`.\r\n"]
[429.636, "o", "\r\n"]
[429.646, "o", "        .. versionadded:: 0.24\r\n"]
[429.656, "o", "\r\n"]
[429.666, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[429.676, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[429.686, "o", "        has feature names that are all strings.\r\n"]
[429.696, "o", "\r\n"]
[429.706, "o", "        .. versionadded:: 1.0\r\n"]
[429.716, "o", "\r\n"]
[429.726, "o", "    n_iter_ : int\r\n"]
[429.736, "o", "        Number of iterations run.\r\n"]
[429.746, "o", "\r\n"]
[429.756, "o", "    See Also\r\n"]
[429.766, "o", "    --------\r\n"]
[429.776, "o", "    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n"]
[429.786, "o", "        dictionary learning algorithm.\r\n"]
[429.796, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[429.806, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[429.816, "o", "        precomputed dictionary.\r\n"]
[429.826, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[429.836, "o", "\r\n"]
[429.846, "o", "    References\r\n"]
[429.856, "o", "    ----------\r\n"]
[429.866, "o", "\r\n"]
[429.876, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[429.886, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[429.896, "o", "\r\n"]
[429.906, "o", "    Examples\r\n"]
[429.916, "o", "    --------\r\n"]
[429.926, "o", "    >>> import numpy as np\r\n"]
[429.936, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[430.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[430.002, "i", "sed -n '2000,3000p' sklearn/decomposition/_dict_learning.py\r"]
[430.004, "o", "sed -n '2000,3000p' sklearn/decomposition/_dict_learning.py\r\n"]
[430.016, "o", "\u001b[?2004l\r\n"]
[430.026, "o", "\r\n"]
[430.036, "o", "    tol : float, default=1e-3\r\n"]
[430.046, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[430.056, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[430.066, "o", "\r\n"]
[430.076, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[430.086, "o", "        `tol` to 0.0.\r\n"]
[430.096, "o", "\r\n"]
[430.106, "o", "        .. versionadded:: 1.1\r\n"]
[430.116, "o", "\r\n"]
[430.126, "o", "    max_no_improvement : int, default=10\r\n"]
[430.136, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[430.146, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[430.156, "o", "        `max_iter` is not None.\r\n"]
[430.166, "o", "\r\n"]
[430.176, "o", "        To disable convergence detection based on cost function, set\r\n"]
[430.186, "o", "        `max_no_improvement` to None.\r\n"]
[430.196, "o", "\r\n"]
[430.206, "o", "        .. versionadded:: 1.1\r\n"]
[430.216, "o", "\r\n"]
[430.226, "o", "    Attributes\r\n"]
[430.236, "o", "    ----------\r\n"]
[430.246, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[430.256, "o", "        Components extracted from the data.\r\n"]
[430.266, "o", "\r\n"]
[430.276, "o", "    n_features_in_ : int\r\n"]
[430.286, "o", "        Number of features seen during :term:`fit`.\r\n"]
[430.296, "o", "\r\n"]
[430.306, "o", "        .. versionadded:: 0.24\r\n"]
[430.316, "o", "\r\n"]
[430.326, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[430.336, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[430.346, "o", "        has feature names that are all strings.\r\n"]
[430.356, "o", "\r\n"]
[430.366, "o", "        .. versionadded:: 1.0\r\n"]
[430.376, "o", "\r\n"]
[430.386, "o", "    n_iter_ : int\r\n"]
[430.396, "o", "        Number of iterations over the full dataset.\r\n"]
[430.406, "o", "\r\n"]
[430.416, "o", "    n_steps_ : int\r\n"]
[430.426, "o", "        Number of mini-batches processed.\r\n"]
[430.436, "o", "\r\n"]
[430.446, "o", "        .. versionadded:: 1.1\r\n"]
[430.456, "o", "\r\n"]
[430.466, "o", "    See Also\r\n"]
[430.476, "o", "    --------\r\n"]
[430.486, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[430.496, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[430.506, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[430.516, "o", "        precomputed dictionary.\r\n"]
[430.526, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[430.536, "o", "\r\n"]
[430.546, "o", "    References\r\n"]
[430.556, "o", "    ----------\r\n"]
[430.566, "o", "\r\n"]
[430.576, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[430.586, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[430.596, "o", "\r\n"]
[430.606, "o", "    Examples\r\n"]
[430.616, "o", "    --------\r\n"]
[430.626, "o", "    >>> import numpy as np\r\n"]
[430.636, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[430.646, "o", "    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n"]
[430.656, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[430.666, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[430.676, "o", "    ...     random_state=42)\r\n"]
[430.686, "o", "    >>> dict_learner = MiniBatchDictionaryLearning(\r\n"]
[430.696, "o", "    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n"]
[430.706, "o", "    ...     transform_alpha=0.1, random_state=42)\r\n"]
[430.716, "o", "    >>> X_transformed = dict_learner.fit_transform(X)\r\n"]
[430.726, "o", "\r\n"]
[430.736, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[430.746, "o", "\r\n"]
[430.756, "o", "    >>> np.mean(X_transformed == 0) < 0.5\r\n"]
[430.766, "o", "    True\r\n"]
[430.776, "o", "\r\n"]
[430.786, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[430.796, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[430.806, "o", "    the original signal:\r\n"]
[430.816, "o", "\r\n"]
[430.826, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[430.836, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[430.846, "o", "    0.057...\r\n"]
[430.856, "o", "    \"\"\"\r\n"]
[430.866, "o", "\r\n"]
[430.876, "o", "    _parameter_constraints: dict = {\r\n"]
[430.886, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[430.896, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[430.906, "o", "        \"n_iter\": [\r\n"]
[430.916, "o", "            Interval(Integral, 0, None, closed=\"left\"),\r\n"]
[430.926, "o", "            Hidden(StrOptions({\"deprecated\"})),\r\n"]
[430.936, "o", "        ],\r\n"]
[430.946, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n"]
[430.956, "o", "        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n"]
[430.966, "o", "        \"n_jobs\": [None, Integral],\r\n"]
[430.976, "o", "        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n"]
[430.986, "o", "        \"shuffle\": [\"boolean\"],\r\n"]
[430.996, "o", "        \"dict_init\": [None, np.ndarray],\r\n"]
[431.006, "o", "        \"transform_algorithm\": [\r\n"]
[431.016, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[431.026, "o", "        ],\r\n"]
[431.036, "o", "        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[431.046, "o", "        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[431.056, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[431.066, "o", "        \"split_sign\": [\"boolean\"],\r\n"]
[431.076, "o", "        \"random_state\": [\"random_state\"],\r\n"]
[431.086, "o", "        \"positive_code\": [\"boolean\"],\r\n"]
[431.096, "o", "        \"positive_dict\": [\"boolean\"],\r\n"]
[431.106, "o", "        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[431.116, "o", "        \"callback\": [None, callable],\r\n"]
[431.126, "o", "        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[431.136, "o", "        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n"]
[431.146, "o", "    }\r\n"]
[431.156, "o", "\r\n"]
[431.166, "o", "    def __init__(\r\n"]
[431.176, "o", "        self,\r\n"]
[431.186, "o", "        n_components=None,\r\n"]
[431.196, "o", "        *,\r\n"]
[431.206, "o", "        alpha=1,\r\n"]
[431.216, "o", "        n_iter=\"deprecated\",\r\n"]
[431.226, "o", "        max_iter=None,\r\n"]
[431.236, "o", "        fit_algorithm=\"lars\",\r\n"]
[431.246, "o", "        n_jobs=None,\r\n"]
[431.256, "o", "        batch_size=256,\r\n"]
[431.266, "o", "        shuffle=True,\r\n"]
[431.276, "o", "        dict_init=None,\r\n"]
[431.286, "o", "        transform_algorithm=\"omp\",\r\n"]
[431.296, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[431.306, "o", "        transform_alpha=None,\r\n"]
[431.316, "o", "        verbose=False,\r\n"]
[431.326, "o", "        split_sign=False,\r\n"]
[431.336, "o", "        random_state=None,\r\n"]
[431.346, "o", "        positive_code=False,\r\n"]
[431.356, "o", "        positive_dict=False,\r\n"]
[431.366, "o", "        transform_max_iter=1000,\r\n"]
[431.376, "o", "        callback=None,\r\n"]
[431.386, "o", "        tol=1e-3,\r\n"]
[431.396, "o", "        max_no_improvement=10,\r\n"]
[431.406, "o", "    ):\r\n"]
[431.416, "o", "        super().__init__(\r\n"]
[431.426, "o", "            transform_algorithm,\r\n"]
[431.436, "o", "            transform_n_nonzero_coefs,\r\n"]
[431.446, "o", "            transform_alpha,\r\n"]
[431.456, "o", "            split_sign,\r\n"]
[431.466, "o", "            n_jobs,\r\n"]
[431.476, "o", "            positive_code,\r\n"]
[431.486, "o", "            transform_max_iter,\r\n"]
[431.496, "o", "        )\r\n"]
[431.506, "o", "        self.n_components = n_components\r\n"]
[431.516, "o", "        self.alpha = alpha\r\n"]
[431.526, "o", "        self.n_iter = n_iter\r\n"]
[431.536, "o", "        self.max_iter = max_iter\r\n"]
[431.546, "o", "        self.fit_algorithm = fit_algorithm\r\n"]
[431.556, "o", "        self.dict_init = dict_init\r\n"]
[431.566, "o", "        self.verbose = verbose\r\n"]
[431.576, "o", "        self.shuffle = shuffle\r\n"]
[431.586, "o", "        self.batch_size = batch_size\r\n"]
[431.596, "o", "        self.split_sign = split_sign\r\n"]
[431.606, "o", "        self.random_state = random_state\r\n"]
[431.616, "o", "        self.positive_dict = positive_dict\r\n"]
[431.626, "o", "        self.callback = callback\r\n"]
[431.636, "o", "        self.max_no_improvement = max_no_improvement\r\n"]
[431.646, "o", "        self.tol = tol\r\n"]
[431.656, "o", "\r\n"]
[431.666, "o", "    def _check_params(self, X):\r\n"]
[431.676, "o", "        # n_components\r\n"]
[431.686, "o", "        self._n_components = self.n_components\r\n"]
[431.696, "o", "        if self._n_components is None:\r\n"]
[431.706, "o", "            self._n_components = X.shape[1]\r\n"]
[431.716, "o", "\r\n"]
[431.726, "o", "        # fit_algorithm\r\n"]
[431.736, "o", "        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n"]
[431.746, "o", "        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n"]
[431.756, "o", "\r\n"]
[431.766, "o", "        # batch_size\r\n"]
[431.776, "o", "        self._batch_size = min(self.batch_size, X.shape[0])\r\n"]
[431.786, "o", "\r\n"]
[431.796, "o", "    def _initialize_dict(self, X, random_state):\r\n"]
[431.806, "o", "        \"\"\"Initialization of the dictionary.\"\"\"\r\n"]
[431.816, "o", "        if self.dict_init is not None:\r\n"]
[431.826, "o", "            dictionary = self.dict_init\r\n"]
[431.836, "o", "        else:\r\n"]
[431.846, "o", "            # Init V with SVD of X\r\n"]
[431.856, "o", "            _, S, dictionary = randomized_svd(\r\n"]
[431.866, "o", "                X, self._n_components, random_state=random_state\r\n"]
[431.876, "o", "            )\r\n"]
[431.886, "o", "            dictionary = S[:, np.newaxis] * dictionary\r\n"]
[431.896, "o", "\r\n"]
[431.906, "o", "        if self._n_components <= len(dictionary):\r\n"]
[431.916, "o", "            dictionary = dictionary[: self._n_components, :]\r\n"]
[431.926, "o", "        else:\r\n"]
[431.936, "o", "            dictionary = np.concatenate(\r\n"]
[431.946, "o", "                (\r\n"]
[431.956, "o", "                    dictionary,\r\n"]
[431.966, "o", "                    np.zeros(\r\n"]
[431.976, "o", "                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n"]
[431.986, "o", "                        dtype=dictionary.dtype,\r\n"]
[431.996, "o", "                    ),\r\n"]
[432.006, "o", "                )\r\n"]
[432.016, "o", "            )\r\n"]
[432.026, "o", "\r\n"]
[432.036, "o", "        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n"]
[432.046, "o", "        dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[432.056, "o", "\r\n"]
[432.066, "o", "        return dictionary\r\n"]
[432.076, "o", "\r\n"]
[432.086, "o", "    def _update_inner_stats(self, X, code, batch_size, step):\r\n"]
[432.096, "o", "        \"\"\"Update the inner stats inplace.\"\"\"\r\n"]
[432.106, "o", "        if step < batch_size - 1:\r\n"]
[432.116, "o", "            theta = (step + 1) * batch_size\r\n"]
[432.126, "o", "        else:\r\n"]
[432.136, "o", "            theta = batch_size**2 + step + 1 - batch_size\r\n"]
[432.146, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[432.156, "o", "\r\n"]
[432.166, "o", "        self._A *= beta\r\n"]
[432.176, "o", "        self._A += code.T @ code / batch_size\r\n"]
[432.186, "o", "        self._B *= beta\r\n"]
[432.196, "o", "        self._B += X.T @ code / batch_size\r\n"]
[432.206, "o", "\r\n"]
[432.216, "o", "    def _minibatch_step(self, X, dictionary, random_state, step):\r\n"]
[432.226, "o", "        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n"]
[432.236, "o", "        batch_size = X.shape[0]\r\n"]
[432.246, "o", "\r\n"]
[432.256, "o", "        # Compute code for this batch\r\n"]
[432.266, "o", "        code = _sparse_encode(\r\n"]
[432.276, "o", "            X,\r\n"]
[432.286, "o", "            dictionary,\r\n"]
[432.296, "o", "            algorithm=self._fit_algorithm,\r\n"]
[432.306, "o", "            alpha=self.alpha,\r\n"]
[432.316, "o", "            n_jobs=self.n_jobs,\r\n"]
[432.326, "o", "            positive=self.positive_code,\r\n"]
[432.336, "o", "            max_iter=self.transform_max_iter,\r\n"]
[432.346, "o", "            verbose=self.verbose,\r\n"]
[432.356, "o", "        )\r\n"]
[432.366, "o", "\r\n"]
[432.376, "o", "        batch_cost = (\r\n"]
[432.386, "o", "            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n"]
[432.396, "o", "            + self.alpha * np.sum(np.abs(code))\r\n"]
[432.406, "o", "        ) / batch_size\r\n"]
[432.416, "o", "\r\n"]
[432.426, "o", "        # Update inner stats\r\n"]
[432.436, "o", "        self._update_inner_stats(X, code, batch_size, step)\r\n"]
[432.446, "o", "\r\n"]
[432.456, "o", "        # Update dictionary\r\n"]
[432.466, "o", "        _update_dict(\r\n"]
[432.476, "o", "            dictionary,\r\n"]
[432.486, "o", "            X,\r\n"]
[432.496, "o", "            code,\r\n"]
[432.506, "o", "            self._A,\r\n"]
[432.516, "o", "            self._B,\r\n"]
[432.526, "o", "            verbose=self.verbose,\r\n"]
[432.536, "o", "            random_state=random_state,\r\n"]
[432.546, "o", "            positive=self.positive_dict,\r\n"]
[432.556, "o", "        )\r\n"]
[432.566, "o", "\r\n"]
[432.576, "o", "        return batch_cost\r\n"]
[432.586, "o", "\r\n"]
[432.596, "o", "    def _check_convergence(\r\n"]
[432.606, "o", "        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n"]
[432.616, "o", "    ):\r\n"]
[432.626, "o", "        \"\"\"Helper function to encapsulate the early stopping logic.\r\n"]
[432.636, "o", "\r\n"]
[432.646, "o", "        Early stopping is based on two factors:\r\n"]
[432.656, "o", "        - A small change of the dictionary between two minibatch updates. This is\r\n"]
[432.666, "o", "          controlled by the tol parameter.\r\n"]
[432.676, "o", "        - No more improvement on a smoothed estimate of the objective function for a\r\n"]
[432.686, "o", "          a certain number of consecutive minibatch updates. This is controlled by\r\n"]
[432.696, "o", "          the max_no_improvement parameter.\r\n"]
[432.706, "o", "        \"\"\"\r\n"]
[432.716, "o", "        batch_size = X.shape[0]\r\n"]
[432.726, "o", "\r\n"]
[432.736, "o", "        # counts steps starting from 1 for user friendly verbose mode.\r\n"]
[432.746, "o", "        step = step + 1\r\n"]
[432.756, "o", "\r\n"]
[432.766, "o", "        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n"]
[432.776, "o", "        # too bad value\r\n"]
[432.786, "o", "        if step <= min(100, n_samples / batch_size):\r\n"]
[432.796, "o", "            if self.verbose:\r\n"]
[432.806, "o", "                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n"]
[432.816, "o", "            return False\r\n"]
[432.826, "o", "\r\n"]
[432.836, "o", "        # Compute an Exponentially Weighted Average of the cost function to\r\n"]
[432.846, "o", "        # monitor the convergence while discarding minibatch-local stochastic\r\n"]
[432.856, "o", "        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n"]
[432.866, "o", "        if self._ewa_cost is None:\r\n"]
[432.876, "o", "            self._ewa_cost = batch_cost\r\n"]
[432.886, "o", "        else:\r\n"]
[432.896, "o", "            alpha = batch_size / (n_samples + 1)\r\n"]
[432.906, "o", "            alpha = min(alpha, 1)\r\n"]
[432.916, "o", "            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n"]
[432.926, "o", "\r\n"]
[432.936, "o", "        if self.verbose:\r\n"]
[432.946, "o", "            print(\r\n"]
[432.956, "o", "                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\r\n"]
[432.966, "o", "                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\r\n"]
[432.976, "o", "            )\r\n"]
[432.986, "o", "\r\n"]
[432.996, "o", "        # Early stopping based on change of dictionary\r\n"]
[433.006, "o", "        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\r\n"]
[433.016, "o", "        if self.tol > 0 and dict_diff <= self.tol:\r\n"]
[433.026, "o", "            if self.verbose:\r\n"]
[433.036, "o", "                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\r\n"]
[433.046, "o", "            return True\r\n"]
[433.056, "o", "\r\n"]
[433.066, "o", "        # Early stopping heuristic due to lack of improvement on smoothed\r\n"]
[433.076, "o", "        # cost function\r\n"]
[433.086, "o", "        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\r\n"]
[433.096, "o", "            self._no_improvement = 0\r\n"]
[433.106, "o", "            self._ewa_cost_min = self._ewa_cost\r\n"]
[433.116, "o", "        else:\r\n"]
[433.126, "o", "            self._no_improvement += 1\r\n"]
[433.136, "o", "\r\n"]
[433.146, "o", "        if (\r\n"]
[433.156, "o", "            self.max_no_improvement is not None\r\n"]
[433.166, "o", "            and self._no_improvement >= self.max_no_improvement\r\n"]
[433.176, "o", "        ):\r\n"]
[433.186, "o", "            if self.verbose:\r\n"]
[433.196, "o", "                print(\r\n"]
[433.206, "o", "                    \"Converged (lack of improvement in objective function) \"\r\n"]
[433.216, "o", "                    f\"at step {step}/{n_steps}\"\r\n"]
[433.226, "o", "                )\r\n"]
[433.236, "o", "            return True\r\n"]
[433.246, "o", "\r\n"]
[433.256, "o", "        return False\r\n"]
[433.266, "o", "\r\n"]
[433.276, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[433.286, "o", "    def fit(self, X, y=None):\r\n"]
[433.296, "o", "        \"\"\"Fit the model from data in X.\r\n"]
[433.306, "o", "\r\n"]
[433.316, "o", "        Parameters\r\n"]
[433.326, "o", "        ----------\r\n"]
[433.336, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[433.346, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[433.356, "o", "            and `n_features` is the number of features.\r\n"]
[433.366, "o", "\r\n"]
[433.376, "o", "        y : Ignored\r\n"]
[433.386, "o", "            Not used, present for API consistency by convention.\r\n"]
[433.396, "o", "\r\n"]
[433.406, "o", "        Returns\r\n"]
[433.416, "o", "        -------\r\n"]
[433.426, "o", "        self : object\r\n"]
[433.436, "o", "            Returns the instance itself.\r\n"]
[433.446, "o", "        \"\"\"\r\n"]
[433.456, "o", "        X = self._validate_data(\r\n"]
[433.466, "o", "            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n"]
[433.476, "o", "        )\r\n"]
[433.486, "o", "\r\n"]
[433.496, "o", "        self._check_params(X)\r\n"]
[433.506, "o", "\r\n"]
[433.516, "o", "        if self.n_iter != \"deprecated\":\r\n"]
[433.526, "o", "            warnings.warn(\r\n"]
[433.536, "o", "                (\r\n"]
[433.546, "o", "                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n"]
[433.556, "o", "                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n"]
[433.566, "o", "                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n"]
[433.576, "o", "                    \"specified.\"\r\n"]
[433.586, "o", "                ),\r\n"]
[433.596, "o", "                FutureWarning,\r\n"]
[433.606, "o", "            )\r\n"]
[433.616, "o", "            n_iter = self.n_iter\r\n"]
[433.626, "o", "\r\n"]
[433.636, "o", "        self._random_state = check_random_state(self.random_state)\r\n"]
[433.646, "o", "\r\n"]
[433.656, "o", "        dictionary = self._initialize_dict(X, self._random_state)\r\n"]
[433.666, "o", "        old_dict = dictionary.copy()\r\n"]
[433.676, "o", "\r\n"]
[433.686, "o", "        if self.shuffle:\r\n"]
[433.696, "o", "            X_train = X.copy()\r\n"]
[433.706, "o", "            self._random_state.shuffle(X_train)\r\n"]
[433.716, "o", "        else:\r\n"]
[433.726, "o", "            X_train = X\r\n"]
[433.736, "o", "\r\n"]
[433.746, "o", "        n_samples, n_features = X_train.shape\r\n"]
[433.756, "o", "\r\n"]
[433.766, "o", "        if self.verbose:\r\n"]
[433.776, "o", "            print(\"[dict_learning]\")\r\n"]
[433.786, "o", "\r\n"]
[433.796, "o", "        # Inner stats\r\n"]
[433.806, "o", "        self._A = np.zeros(\r\n"]
[433.816, "o", "            (self._n_components, self._n_components), dtype=X_train.dtype\r\n"]
[433.826, "o", "        )\r\n"]
[433.836, "o", "        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n"]
[433.846, "o", "\r\n"]
[433.856, "o", "        if self.max_iter is not None:\r\n"]
[433.866, "o", "            # Attributes to monitor the convergence\r\n"]
[433.876, "o", "            self._ewa_cost = None\r\n"]
[433.886, "o", "            self._ewa_cost_min = None\r\n"]
[433.896, "o", "            self._no_improvement = 0\r\n"]
[433.906, "o", "\r\n"]
[433.916, "o", "            batches = gen_batches(n_samples, self._batch_size)\r\n"]
[433.926, "o", "            batches = itertools.cycle(batches)\r\n"]
[433.936, "o", "            n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\r\n"]
[433.946, "o", "            n_steps = self.max_iter * n_steps_per_iter\r\n"]
[433.956, "o", "\r\n"]
[433.966, "o", "            i = -1  # to allow max_iter = 0\r\n"]
[433.976, "o", "\r\n"]
[433.986, "o", "            for i, batch in zip(range(n_steps), batches):\r\n"]
[433.996, "o", "                X_batch = X_train[batch]\r\n"]
[434.006, "o", "\r\n"]
[434.016, "o", "                batch_cost = self._minibatch_step(\r\n"]
[434.026, "o", "                    X_batch, dictionary, self._random_state, i\r\n"]
[434.036, "o", "                )\r\n"]
[434.046, "o", "\r\n"]
[434.056, "o", "                if self._check_convergence(\r\n"]
[434.066, "o", "                    X_batch, batch_cost, dictionary, old_dict, n_samples, i, n_steps\r\n"]
[434.076, "o", "                ):\r\n"]
[434.086, "o", "                    break\r\n"]
[434.096, "o", "\r\n"]
[434.106, "o", "                # XXX callback param added for backward compat in #18975 but a common\r\n"]
[434.116, "o", "                # unified callback API should be preferred\r\n"]
[434.126, "o", "                if self.callback is not None:\r\n"]
[434.136, "o", "                    self.callback(locals())\r\n"]
[434.146, "o", "\r\n"]
[434.156, "o", "                old_dict[:] = dictionary\r\n"]
[434.166, "o", "\r\n"]
[434.176, "o", "            self.n_steps_ = i + 1\r\n"]
[434.186, "o", "            self.n_iter_ = np.ceil(self.n_steps_ / n_steps_per_iter)\r\n"]
[434.196, "o", "        else:\r\n"]
[434.206, "o", "            # TODO remove this branch in 1.4\r\n"]
[434.216, "o", "            n_iter = 1000 if self.n_iter == \"deprecated\" else self.n_iter\r\n"]
[434.226, "o", "\r\n"]
[434.236, "o", "            batches = gen_batches(n_samples, self._batch_size)\r\n"]
[434.246, "o", "            batches = itertools.cycle(batches)\r\n"]
[434.256, "o", "\r\n"]
[434.266, "o", "            for i, batch in zip(range(n_iter), batches):\r\n"]
[434.276, "o", "                self._minibatch_step(X_train[batch], dictionary, self._random_state, i)\r\n"]
[434.286, "o", "\r\n"]
[434.296, "o", "                trigger_verbose = self.verbose and i % ceil(100.0 / self.verbose) == 0\r\n"]
[434.306, "o", "                if self.verbose > 10 or trigger_verbose:\r\n"]
[434.316, "o", "                    print(f\"{i} batches processed.\")\r\n"]
[434.326, "o", "\r\n"]
[434.336, "o", "                if self.callback is not None:\r\n"]
[434.346, "o", "                    self.callback(locals())\r\n"]
[434.356, "o", "\r\n"]
[434.366, "o", "            self.n_steps_ = n_iter\r\n"]
[434.376, "o", "            self.n_iter_ = np.ceil(n_iter / int(np.ceil(n_samples / self._batch_size)))\r\n"]
[434.386, "o", "\r\n"]
[434.396, "o", "        self.components_ = dictionary\r\n"]
[434.406, "o", "\r\n"]
[434.416, "o", "        return self\r\n"]
[434.426, "o", "\r\n"]
[434.436, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[434.446, "o", "    def partial_fit(self, X, y=None):\r\n"]
[434.456, "o", "        \"\"\"Update the model using the data in X as a mini-batch.\r\n"]
[434.466, "o", "\r\n"]
[434.476, "o", "        Parameters\r\n"]
[434.486, "o", "        ----------\r\n"]
[434.496, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[434.506, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[434.516, "o", "            and `n_features` is the number of features.\r\n"]
[434.526, "o", "\r\n"]
[434.536, "o", "        y : Ignored\r\n"]
[434.546, "o", "            Not used, present for API consistency by convention.\r\n"]
[434.556, "o", "\r\n"]
[434.566, "o", "        Returns\r\n"]
[434.576, "o", "        -------\r\n"]
[434.586, "o", "        self : object\r\n"]
[434.596, "o", "            Return the instance itself.\r\n"]
[434.606, "o", "        \"\"\"\r\n"]
[434.616, "o", "        has_components = hasattr(self, \"components_\")\r\n"]
[434.626, "o", "\r\n"]
[434.636, "o", "        X = self._validate_data(\r\n"]
[434.646, "o", "            X, dtype=[np.float64, np.float32], order=\"C\", reset=not has_components\r\n"]
[434.656, "o", "        )\r\n"]
[434.666, "o", "\r\n"]
[434.676, "o", "        if not has_components:\r\n"]
[434.686, "o", "            # This instance has not been fitted yet (fit or partial_fit)\r\n"]
[434.696, "o", "            self._check_params(X)\r\n"]
[434.706, "o", "            self._random_state = check_random_state(self.random_state)\r\n"]
[434.716, "o", "\r\n"]
[434.726, "o", "            dictionary = self._initialize_dict(X, self._random_state)\r\n"]
[434.736, "o", "\r\n"]
[434.746, "o", "            self.n_steps_ = 0\r\n"]
[434.756, "o", "\r\n"]
[434.766, "o", "            self._A = np.zeros((self._n_components, self._n_components), dtype=X.dtype)\r\n"]
[434.776, "o", "            self._B = np.zeros((X.shape[1], self._n_components), dtype=X.dtype)\r\n"]
[434.786, "o", "        else:\r\n"]
[434.796, "o", "            dictionary = self.components_\r\n"]
[434.806, "o", "\r\n"]
[434.816, "o", "        self._minibatch_step(X, dictionary, self._random_state, self.n_steps_)\r\n"]
[434.826, "o", "\r\n"]
[434.836, "o", "        self.components_ = dictionary\r\n"]
[434.846, "o", "        self.n_steps_ += 1\r\n"]
[434.856, "o", "\r\n"]
[434.866, "o", "        return self\r\n"]
[434.876, "o", "\r\n"]
[434.886, "o", "    @property\r\n"]
[434.896, "o", "    def _n_features_out(self):\r\n"]
[434.906, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[434.916, "o", "        return self.components_.shape[0]\r\n"]
[434.926, "o", "\r\n"]
[434.936, "o", "    def _more_tags(self):\r\n"]
[435.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[435.002, "i", "cd asv_benchmarks\r"]
[435.004, "o", "cd asv_benchmarks\r\n"]
[435.006, "o", "\u001b[?2004l\r\n"]
[440.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[440.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[440.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[441.654, "o", "\u001b[?2004l\r\n"]
[443.302, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[445.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[445.002, "i", "cd ..\r"]
[445.004, "o", "cd ..\r\n"]
[445.006, "o", "\u001b[?2004l\r\n"]
[450.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[450.002, "i", "grep -n \"def _minibatch_step\\|def dict_learning_online\\|def _mini_batch_dictionary_learning\\|def _sparse_encode_pre\r"]
[450.004, "o", "grep -n \"def _minibatch_step\\|def dict_learning_online\\|def _mini_batch_dictionary_learning\\|def _sparse_encode_pre\r\n"]
[450.455455, "o", "ecomputed\\|gram=\" sklearn/decomposition/_dict_learning.py\r\n"]
[450.904909, "o", "\u001b[?2004l\r\n"]
[451.354364, "o", "\u001b[32m\u001b[K38\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kdef _sparse_encode_precomputed\u001b[m\u001b[K(\r\n"]
[451.803818, "o", "\u001b[32m\u001b[K42\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kgram=\u001b[m\u001b[KNone,\r\n"]
[452.253273, "o", "\u001b[32m\u001b[K231\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kgram=\u001b[m\u001b[KNone,\r\n"]
[452.702727, "o", "\u001b[32m\u001b[K364\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kgram=\u001b[m\u001b[Kgram,\r\n"]
[453.152182, "o", "\u001b[32m\u001b[K382\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kgram=\u001b[m\u001b[KNone,\r\n"]
[453.601636, "o", "\u001b[32m\u001b[K419\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            \u001b[01;31m\u001b[Kgram=\u001b[m\u001b[Kgram,\r\n"]
[454.051091, "o", "\u001b[32m\u001b[K441\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            \u001b[01;31m\u001b[Kgram=\u001b[m\u001b[Kgram,\r\n"]
[454.500545, "o", "\u001b[32m\u001b[K664\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kdef dict_learning_online\u001b[m\u001b[K(\r\n"]
[455.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[455.002, "i", "sed -n '1,600p' sklearn/decomposition/_dict_learning.py\r"]
[455.004, "o", "sed -n '1,600p' sklearn/decomposition/_dict_learning.py\r\n"]
[455.016, "o", "\u001b[?2004l\r\n"]
[455.026, "o", "\"\"\" Dictionary learning.\r\n"]
[455.036, "o", "\"\"\"\r\n"]
[455.046, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[455.056, "o", "# License: BSD 3 clause\r\n"]
[455.066, "o", "\r\n"]
[455.076, "o", "import itertools\r\n"]
[455.086, "o", "import sys\r\n"]
[455.096, "o", "import time\r\n"]
[455.106, "o", "import warnings\r\n"]
[455.116, "o", "from math import ceil\r\n"]
[455.126, "o", "from numbers import Integral, Real\r\n"]
[455.136, "o", "\r\n"]
[455.146, "o", "import numpy as np\r\n"]
[455.156, "o", "from joblib import effective_n_jobs\r\n"]
[455.166, "o", "from scipy import linalg\r\n"]
[455.176, "o", "\r\n"]
[455.186, "o", "from ..base import (\r\n"]
[455.196, "o", "    BaseEstimator,\r\n"]
[455.206, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[455.216, "o", "    TransformerMixin,\r\n"]
[455.226, "o", "    _fit_context,\r\n"]
[455.236, "o", ")\r\n"]
[455.246, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[455.256, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[455.266, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[455.276, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[455.286, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[455.296, "o", "from ..utils.validation import check_is_fitted\r\n"]
[455.306, "o", "\r\n"]
[455.316, "o", "\r\n"]
[455.326, "o", "def _check_positive_coding(method, positive):\r\n"]
[455.336, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[455.346, "o", "        raise ValueError(\r\n"]
[455.356, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[455.366, "o", "        )\r\n"]
[455.376, "o", "\r\n"]
[455.386, "o", "\r\n"]
[455.396, "o", "def _sparse_encode_precomputed(\r\n"]
[455.406, "o", "    X,\r\n"]
[455.416, "o", "    dictionary,\r\n"]
[455.426, "o", "    *,\r\n"]
[455.436, "o", "    gram=None,\r\n"]
[455.446, "o", "    cov=None,\r\n"]
[455.456, "o", "    algorithm=\"lasso_lars\",\r\n"]
[455.466, "o", "    regularization=None,\r\n"]
[455.476, "o", "    copy_cov=True,\r\n"]
[455.486, "o", "    init=None,\r\n"]
[455.496, "o", "    max_iter=1000,\r\n"]
[455.506, "o", "    verbose=0,\r\n"]
[455.516, "o", "    positive=False,\r\n"]
[455.526, "o", "):\r\n"]
[455.536, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[455.546, "o", "\r\n"]
[455.556, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[455.566, "o", "\r\n"]
[455.576, "o", "    Parameters\r\n"]
[455.586, "o", "    ----------\r\n"]
[455.596, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[455.606, "o", "        Data matrix.\r\n"]
[455.616, "o", "\r\n"]
[455.626, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[455.636, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[455.646, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[455.656, "o", "\r\n"]
[455.666, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[455.676, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[455.686, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[455.696, "o", "\r\n"]
[455.706, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[455.716, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[455.726, "o", "\r\n"]
[455.736, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[455.746, "o", "            default='lasso_lars'\r\n"]
[455.756, "o", "        The algorithm used:\r\n"]
[455.766, "o", "\r\n"]
[455.776, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[455.786, "o", "          (`linear_model.lars_path`);\r\n"]
[455.796, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[455.806, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[455.816, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[455.826, "o", "          the estimated components are sparse;\r\n"]
[455.836, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[455.846, "o", "          solution;\r\n"]
[455.856, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[455.866, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[455.876, "o", "\r\n"]
[455.886, "o", "    regularization : int or float, default=None\r\n"]
[455.896, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[455.906, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[455.916, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[455.926, "o", "\r\n"]
[455.936, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[455.946, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[455.956, "o", "        `algorithm='lasso_cd'`.\r\n"]
[455.966, "o", "\r\n"]
[455.976, "o", "    max_iter : int, default=1000\r\n"]
[455.986, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[455.996, "o", "        `'lasso_lars'`.\r\n"]
[456.006, "o", "\r\n"]
[456.016, "o", "    copy_cov : bool, default=True\r\n"]
[456.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[456.036, "o", "        be overwritten.\r\n"]
[456.046, "o", "\r\n"]
[456.056, "o", "    verbose : int, default=0\r\n"]
[456.066, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[456.076, "o", "\r\n"]
[456.086, "o", "    positive: bool, default=False\r\n"]
[456.096, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[456.106, "o", "\r\n"]
[456.116, "o", "        .. versionadded:: 0.20\r\n"]
[456.126, "o", "\r\n"]
[456.136, "o", "    Returns\r\n"]
[456.146, "o", "    -------\r\n"]
[456.156, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[456.166, "o", "        The sparse codes.\r\n"]
[456.176, "o", "    \"\"\"\r\n"]
[456.186, "o", "    n_samples, n_features = X.shape\r\n"]
[456.196, "o", "    n_components = dictionary.shape[0]\r\n"]
[456.206, "o", "\r\n"]
[456.216, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[456.226, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[456.236, "o", "        try:\r\n"]
[456.246, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[456.256, "o", "\r\n"]
[456.266, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[456.276, "o", "            # corrects the verbosity level.\r\n"]
[456.286, "o", "            lasso_lars = LassoLars(\r\n"]
[456.296, "o", "                alpha=alpha,\r\n"]
[456.306, "o", "                fit_intercept=False,\r\n"]
[456.316, "o", "                verbose=verbose,\r\n"]
[456.326, "o", "                precompute=gram,\r\n"]
[456.336, "o", "                fit_path=False,\r\n"]
[456.346, "o", "                positive=positive,\r\n"]
[456.356, "o", "                max_iter=max_iter,\r\n"]
[456.366, "o", "            )\r\n"]
[456.376, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[456.386, "o", "            new_code = lasso_lars.coef_\r\n"]
[456.396, "o", "        finally:\r\n"]
[456.406, "o", "            np.seterr(**err_mgt)\r\n"]
[456.416, "o", "\r\n"]
[456.426, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[456.436, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[456.446, "o", "\r\n"]
[456.456, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[456.466, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[456.476, "o", "        # argument that we could pass in from Lasso.\r\n"]
[456.486, "o", "        clf = Lasso(\r\n"]
[456.496, "o", "            alpha=alpha,\r\n"]
[456.506, "o", "            fit_intercept=False,\r\n"]
[456.516, "o", "            precompute=gram,\r\n"]
[456.526, "o", "            max_iter=max_iter,\r\n"]
[456.536, "o", "            warm_start=True,\r\n"]
[456.546, "o", "            positive=positive,\r\n"]
[456.556, "o", "        )\r\n"]
[456.566, "o", "\r\n"]
[456.576, "o", "        if init is not None:\r\n"]
[456.586, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[456.596, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[456.606, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[456.616, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[456.626, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[456.636, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[456.646, "o", "                init = np.array(init)\r\n"]
[456.656, "o", "            clf.coef_ = init\r\n"]
[456.666, "o", "\r\n"]
[456.676, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[456.686, "o", "        new_code = clf.coef_\r\n"]
[456.696, "o", "\r\n"]
[456.706, "o", "    elif algorithm == \"lars\":\r\n"]
[456.716, "o", "        try:\r\n"]
[456.726, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[456.736, "o", "\r\n"]
[456.746, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[456.756, "o", "            # corrects the verbosity level.\r\n"]
[456.766, "o", "            lars = Lars(\r\n"]
[456.776, "o", "                fit_intercept=False,\r\n"]
[456.786, "o", "                verbose=verbose,\r\n"]
[456.796, "o", "                precompute=gram,\r\n"]
[456.806, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[456.816, "o", "                fit_path=False,\r\n"]
[456.826, "o", "            )\r\n"]
[456.836, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[456.846, "o", "            new_code = lars.coef_\r\n"]
[456.856, "o", "        finally:\r\n"]
[456.866, "o", "            np.seterr(**err_mgt)\r\n"]
[456.876, "o", "\r\n"]
[456.886, "o", "    elif algorithm == \"threshold\":\r\n"]
[456.896, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[456.906, "o", "        if positive:\r\n"]
[456.916, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[456.926, "o", "\r\n"]
[456.936, "o", "    elif algorithm == \"omp\":\r\n"]
[456.946, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[456.956, "o", "            Gram=gram,\r\n"]
[456.966, "o", "            Xy=cov,\r\n"]
[456.976, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[456.986, "o", "            tol=None,\r\n"]
[456.996, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[457.006, "o", "            copy_Xy=copy_cov,\r\n"]
[457.016, "o", "        ).T\r\n"]
[457.026, "o", "\r\n"]
[457.036, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[457.046, "o", "\r\n"]
[457.056, "o", "\r\n"]
[457.066, "o", "@validate_params(\r\n"]
[457.076, "o", "    {\r\n"]
[457.086, "o", "        \"X\": [\"array-like\"],\r\n"]
[457.096, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[457.106, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[457.116, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[457.126, "o", "        \"algorithm\": [\r\n"]
[457.136, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[457.146, "o", "        ],\r\n"]
[457.156, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[457.166, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[457.176, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[457.186, "o", "        \"init\": [\"array-like\", None],\r\n"]
[457.196, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[457.206, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[457.216, "o", "        \"check_input\": [\"boolean\"],\r\n"]
[457.226, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[457.236, "o", "        \"positive\": [\"boolean\"],\r\n"]
[457.246, "o", "    },\r\n"]
[457.256, "o", "    prefer_skip_nested_validation=True,\r\n"]
[457.266, "o", ")\r\n"]
[457.276, "o", "# XXX : could be moved to the linear_model module\r\n"]
[457.286, "o", "def sparse_encode(\r\n"]
[457.296, "o", "    X,\r\n"]
[457.306, "o", "    dictionary,\r\n"]
[457.316, "o", "    *,\r\n"]
[457.326, "o", "    gram=None,\r\n"]
[457.336, "o", "    cov=None,\r\n"]
[457.346, "o", "    algorithm=\"lasso_lars\",\r\n"]
[457.356, "o", "    n_nonzero_coefs=None,\r\n"]
[457.366, "o", "    alpha=None,\r\n"]
[457.376, "o", "    copy_cov=True,\r\n"]
[457.386, "o", "    init=None,\r\n"]
[457.396, "o", "    max_iter=1000,\r\n"]
[457.406, "o", "    n_jobs=None,\r\n"]
[457.416, "o", "    check_input=True,\r\n"]
[457.426, "o", "    verbose=0,\r\n"]
[457.436, "o", "    positive=False,\r\n"]
[457.446, "o", "):\r\n"]
[457.456, "o", "    \"\"\"Sparse coding.\r\n"]
[457.466, "o", "\r\n"]
[457.476, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[457.486, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[457.496, "o", "\r\n"]
[457.506, "o", "        X ~= code * dictionary\r\n"]
[457.516, "o", "\r\n"]
[457.526, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[457.536, "o", "\r\n"]
[457.546, "o", "    Parameters\r\n"]
[457.556, "o", "    ----------\r\n"]
[457.566, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[457.576, "o", "        Data matrix.\r\n"]
[457.586, "o", "\r\n"]
[457.596, "o", "    dictionary : array-like of shape (n_components, n_features)\r\n"]
[457.606, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[457.616, "o", "        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[457.626, "o", "        output.\r\n"]
[457.636, "o", "\r\n"]
[457.646, "o", "    gram : array-like of shape (n_components, n_components), default=None\r\n"]
[457.656, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n"]
[457.666, "o", "\r\n"]
[457.676, "o", "    cov : array-like of shape (n_components, n_samples), default=None\r\n"]
[457.686, "o", "        Precomputed covariance, `dictionary' * X`.\r\n"]
[457.696, "o", "\r\n"]
[457.706, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[457.716, "o", "            default='lasso_lars'\r\n"]
[457.726, "o", "        The algorithm used:\r\n"]
[457.736, "o", "\r\n"]
[457.746, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[457.756, "o", "          (`linear_model.lars_path`);\r\n"]
[457.766, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[457.776, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[457.786, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[457.796, "o", "          the estimated components are sparse;\r\n"]
[457.806, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[457.816, "o", "          solution;\r\n"]
[457.826, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[457.836, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[457.846, "o", "\r\n"]
[457.856, "o", "    n_nonzero_coefs : int, default=None\r\n"]
[457.866, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[457.876, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[457.886, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[457.896, "o", "        `n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[457.906, "o", "\r\n"]
[457.916, "o", "    alpha : float, default=None\r\n"]
[457.926, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[457.936, "o", "        penalty applied to the L1 norm.\r\n"]
[457.946, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[457.956, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[457.966, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[457.976, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[457.986, "o", "        `n_nonzero_coefs`.\r\n"]
[457.996, "o", "        If `None`, default to 1.\r\n"]
[458.006, "o", "\r\n"]
[458.016, "o", "    copy_cov : bool, default=True\r\n"]
[458.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[458.036, "o", "        be overwritten.\r\n"]
[458.046, "o", "\r\n"]
[458.056, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[458.066, "o", "        Initialization value of the sparse codes. Only used if\r\n"]
[458.076, "o", "        `algorithm='lasso_cd'`.\r\n"]
[458.086, "o", "\r\n"]
[458.096, "o", "    max_iter : int, default=1000\r\n"]
[458.106, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[458.116, "o", "        `'lasso_lars'`.\r\n"]
[458.126, "o", "\r\n"]
[458.136, "o", "    n_jobs : int, default=None\r\n"]
[458.146, "o", "        Number of parallel jobs to run.\r\n"]
[458.156, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[458.166, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[458.176, "o", "        for more details.\r\n"]
[458.186, "o", "\r\n"]
[458.196, "o", "    check_input : bool, default=True\r\n"]
[458.206, "o", "        If `False`, the input arrays X and dictionary will not be checked.\r\n"]
[458.216, "o", "\r\n"]
[458.226, "o", "    verbose : int, default=0\r\n"]
[458.236, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[458.246, "o", "\r\n"]
[458.256, "o", "    positive : bool, default=False\r\n"]
[458.266, "o", "        Whether to enforce positivity when finding the encoding.\r\n"]
[458.276, "o", "\r\n"]
[458.286, "o", "        .. versionadded:: 0.20\r\n"]
[458.296, "o", "\r\n"]
[458.306, "o", "    Returns\r\n"]
[458.316, "o", "    -------\r\n"]
[458.326, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[458.336, "o", "        The sparse codes.\r\n"]
[458.346, "o", "\r\n"]
[458.356, "o", "    See Also\r\n"]
[458.366, "o", "    --------\r\n"]
[458.376, "o", "    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n"]
[458.386, "o", "        path using LARS algorithm.\r\n"]
[458.396, "o", "    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n"]
[458.406, "o", "    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n"]
[458.416, "o", "    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n"]
[458.426, "o", "        dictionary.\r\n"]
[458.436, "o", "    \"\"\"\r\n"]
[458.446, "o", "    if check_input:\r\n"]
[458.456, "o", "        if algorithm == \"lasso_cd\":\r\n"]
[458.466, "o", "            dictionary = check_array(\r\n"]
[458.476, "o", "                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n"]
[458.486, "o", "            )\r\n"]
[458.496, "o", "            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n"]
[458.506, "o", "        else:\r\n"]
[458.516, "o", "            dictionary = check_array(dictionary)\r\n"]
[458.526, "o", "            X = check_array(X)\r\n"]
[458.536, "o", "\r\n"]
[458.546, "o", "    if dictionary.shape[1] != X.shape[1]:\r\n"]
[458.556, "o", "        raise ValueError(\r\n"]
[458.566, "o", "            \"Dictionary and X have different numbers of features:\"\r\n"]
[458.576, "o", "            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n"]
[458.586, "o", "        )\r\n"]
[458.596, "o", "\r\n"]
[458.606, "o", "    _check_positive_coding(algorithm, positive)\r\n"]
[458.616, "o", "\r\n"]
[458.626, "o", "    return _sparse_encode(\r\n"]
[458.636, "o", "        X,\r\n"]
[458.646, "o", "        dictionary,\r\n"]
[458.656, "o", "        gram=gram,\r\n"]
[458.666, "o", "        cov=cov,\r\n"]
[458.676, "o", "        algorithm=algorithm,\r\n"]
[458.686, "o", "        n_nonzero_coefs=n_nonzero_coefs,\r\n"]
[458.696, "o", "        alpha=alpha,\r\n"]
[458.706, "o", "        copy_cov=copy_cov,\r\n"]
[458.716, "o", "        init=init,\r\n"]
[458.726, "o", "        max_iter=max_iter,\r\n"]
[458.736, "o", "        n_jobs=n_jobs,\r\n"]
[458.746, "o", "        verbose=verbose,\r\n"]
[458.756, "o", "        positive=positive,\r\n"]
[458.766, "o", "    )\r\n"]
[458.776, "o", "\r\n"]
[458.786, "o", "\r\n"]
[458.796, "o", "def _sparse_encode(\r\n"]
[458.806, "o", "    X,\r\n"]
[458.816, "o", "    dictionary,\r\n"]
[458.826, "o", "    *,\r\n"]
[458.836, "o", "    gram=None,\r\n"]
[458.846, "o", "    cov=None,\r\n"]
[458.856, "o", "    algorithm=\"lasso_lars\",\r\n"]
[458.866, "o", "    n_nonzero_coefs=None,\r\n"]
[458.876, "o", "    alpha=None,\r\n"]
[458.886, "o", "    copy_cov=True,\r\n"]
[458.896, "o", "    init=None,\r\n"]
[458.906, "o", "    max_iter=1000,\r\n"]
[458.916, "o", "    n_jobs=None,\r\n"]
[458.926, "o", "    verbose=0,\r\n"]
[458.936, "o", "    positive=False,\r\n"]
[458.946, "o", "):\r\n"]
[458.956, "o", "    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[458.966, "o", "\r\n"]
[458.976, "o", "    n_samples, n_features = X.shape\r\n"]
[458.986, "o", "    n_components = dictionary.shape[0]\r\n"]
[458.996, "o", "\r\n"]
[459.006, "o", "    if algorithm in (\"lars\", \"omp\"):\r\n"]
[459.016, "o", "        regularization = n_nonzero_coefs\r\n"]
[459.026, "o", "        if regularization is None:\r\n"]
[459.036, "o", "            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[459.046, "o", "    else:\r\n"]
[459.056, "o", "        regularization = alpha\r\n"]
[459.066, "o", "        if regularization is None:\r\n"]
[459.076, "o", "            regularization = 1.0\r\n"]
[459.086, "o", "\r\n"]
[459.096, "o", "    if gram is None and algorithm != \"threshold\":\r\n"]
[459.106, "o", "        gram = np.dot(dictionary, dictionary.T)\r\n"]
[459.116, "o", "\r\n"]
[459.126, "o", "    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[459.136, "o", "        copy_cov = False\r\n"]
[459.146, "o", "        cov = np.dot(dictionary, X.T)\r\n"]
[459.156, "o", "\r\n"]
[459.166, "o", "    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[459.176, "o", "        code = _sparse_encode_precomputed(\r\n"]
[459.186, "o", "            X,\r\n"]
[459.196, "o", "            dictionary,\r\n"]
[459.206, "o", "            gram=gram,\r\n"]
[459.216, "o", "            cov=cov,\r\n"]
[459.226, "o", "            algorithm=algorithm,\r\n"]
[459.236, "o", "            regularization=regularization,\r\n"]
[459.246, "o", "            copy_cov=copy_cov,\r\n"]
[459.256, "o", "            init=init,\r\n"]
[459.266, "o", "            max_iter=max_iter,\r\n"]
[459.276, "o", "            verbose=verbose,\r\n"]
[459.286, "o", "            positive=positive,\r\n"]
[459.296, "o", "        )\r\n"]
[459.306, "o", "        return code\r\n"]
[459.316, "o", "\r\n"]
[459.326, "o", "    # Enter parallel code block\r\n"]
[459.336, "o", "    n_samples = X.shape[0]\r\n"]
[459.346, "o", "    n_components = dictionary.shape[0]\r\n"]
[459.356, "o", "    code = np.empty((n_samples, n_components))\r\n"]
[459.366, "o", "    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[459.376, "o", "\r\n"]
[459.386, "o", "    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[459.396, "o", "        delayed(_sparse_encode_precomputed)(\r\n"]
[459.406, "o", "            X[this_slice],\r\n"]
[459.416, "o", "            dictionary,\r\n"]
[459.426, "o", "            gram=gram,\r\n"]
[459.436, "o", "            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[459.446, "o", "            algorithm=algorithm,\r\n"]
[459.456, "o", "            regularization=regularization,\r\n"]
[459.466, "o", "            copy_cov=copy_cov,\r\n"]
[459.476, "o", "            init=init[this_slice] if init is not None else None,\r\n"]
[459.486, "o", "            max_iter=max_iter,\r\n"]
[459.496, "o", "            verbose=verbose,\r\n"]
[459.506, "o", "            positive=positive,\r\n"]
[459.516, "o", "        )\r\n"]
[459.526, "o", "        for this_slice in slices\r\n"]
[459.536, "o", "    )\r\n"]
[459.546, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[459.556, "o", "        code[this_slice] = this_view\r\n"]
[459.566, "o", "    return code\r\n"]
[459.576, "o", "\r\n"]
[459.586, "o", "\r\n"]
[459.596, "o", "def _update_dict(\r\n"]
[459.606, "o", "    dictionary,\r\n"]
[459.616, "o", "    Y,\r\n"]
[459.626, "o", "    code,\r\n"]
[459.636, "o", "    A=None,\r\n"]
[459.646, "o", "    B=None,\r\n"]
[459.656, "o", "    verbose=False,\r\n"]
[459.666, "o", "    random_state=None,\r\n"]
[459.676, "o", "    positive=False,\r\n"]
[459.686, "o", "):\r\n"]
[459.696, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[459.706, "o", "\r\n"]
[459.716, "o", "    Parameters\r\n"]
[459.726, "o", "    ----------\r\n"]
[459.736, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[459.746, "o", "        Value of the dictionary at the previous iteration.\r\n"]
[459.756, "o", "\r\n"]
[459.766, "o", "    Y : ndarray of shape (n_samples, n_features)\r\n"]
[459.776, "o", "        Data matrix.\r\n"]
[459.786, "o", "\r\n"]
[459.796, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[459.806, "o", "        Sparse coding of the data against which to optimize the dictionary.\r\n"]
[459.816, "o", "\r\n"]
[459.826, "o", "    A : ndarray of shape (n_components, n_components), default=None\r\n"]
[459.836, "o", "        Together with `B`, sufficient stats of the online model to update the\r\n"]
[459.846, "o", "        dictionary.\r\n"]
[459.856, "o", "\r\n"]
[459.866, "o", "    B : ndarray of shape (n_features, n_components), default=None\r\n"]
[459.876, "o", "        Together with `A`, sufficient stats of the online model to update the\r\n"]
[459.886, "o", "        dictionary.\r\n"]
[459.896, "o", "\r\n"]
[459.906, "o", "    verbose: bool, default=False\r\n"]
[459.916, "o", "        Degree of output the procedure will print.\r\n"]
[459.926, "o", "\r\n"]
[459.936, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[460.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[460.002, "i", "sed -n '600,1200p' sklearn/decomposition/_dict_learning.py\r"]
[460.004, "o", "sed -n '600,1200p' sklearn/decomposition/_dict_learning.py\r\n"]
[460.016, "o", "\u001b[?2004l\r\n"]
[460.026, "o", "                % (ii, dt, dt / 60, current_cost)\r\n"]
[460.036, "o", "            )\r\n"]
[460.046, "o", "\r\n"]
[460.056, "o", "        # Update code\r\n"]
[460.066, "o", "        code = sparse_encode(\r\n"]
[460.076, "o", "            X,\r\n"]
[460.086, "o", "            dictionary,\r\n"]
[460.096, "o", "            algorithm=method,\r\n"]
[460.106, "o", "            alpha=alpha,\r\n"]
[460.116, "o", "            init=code,\r\n"]
[460.126, "o", "            n_jobs=n_jobs,\r\n"]
[460.136, "o", "            positive=positive_code,\r\n"]
[460.146, "o", "            max_iter=method_max_iter,\r\n"]
[460.156, "o", "            verbose=verbose,\r\n"]
[460.166, "o", "        )\r\n"]
[460.176, "o", "\r\n"]
[460.186, "o", "        # Update dictionary in place\r\n"]
[460.196, "o", "        _update_dict(\r\n"]
[460.206, "o", "            dictionary,\r\n"]
[460.216, "o", "            X,\r\n"]
[460.226, "o", "            code,\r\n"]
[460.236, "o", "            verbose=verbose,\r\n"]
[460.246, "o", "            random_state=random_state,\r\n"]
[460.256, "o", "            positive=positive_dict,\r\n"]
[460.266, "o", "        )\r\n"]
[460.276, "o", "\r\n"]
[460.286, "o", "        # Cost function\r\n"]
[460.296, "o", "        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n"]
[460.306, "o", "            np.abs(code)\r\n"]
[460.316, "o", "        )\r\n"]
[460.326, "o", "        errors.append(current_cost)\r\n"]
[460.336, "o", "\r\n"]
[460.346, "o", "        if ii > 0:\r\n"]
[460.356, "o", "            dE = errors[-2] - errors[-1]\r\n"]
[460.366, "o", "            # assert(dE >= -tol * errors[-1])\r\n"]
[460.376, "o", "            if dE < tol * errors[-1]:\r\n"]
[460.386, "o", "                if verbose == 1:\r\n"]
[460.396, "o", "                    # A line return\r\n"]
[460.406, "o", "                    print(\"\")\r\n"]
[460.416, "o", "                elif verbose:\r\n"]
[460.426, "o", "                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n"]
[460.436, "o", "                break\r\n"]
[460.446, "o", "        if ii % 5 == 0 and callback is not None:\r\n"]
[460.456, "o", "            callback(locals())\r\n"]
[460.466, "o", "\r\n"]
[460.476, "o", "    if return_n_iter:\r\n"]
[460.486, "o", "        return code, dictionary, errors, ii + 1\r\n"]
[460.496, "o", "    else:\r\n"]
[460.506, "o", "        return code, dictionary, errors\r\n"]
[460.516, "o", "\r\n"]
[460.526, "o", "\r\n"]
[460.536, "o", "def _check_warn_deprecated(param, name, default, additional_message=None):\r\n"]
[460.546, "o", "    if param != \"deprecated\":\r\n"]
[460.556, "o", "        msg = (\r\n"]
[460.566, "o", "            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n"]
[460.576, "o", "        )\r\n"]
[460.586, "o", "        if additional_message:\r\n"]
[460.596, "o", "            msg += f\" {additional_message}\"\r\n"]
[460.606, "o", "        warnings.warn(msg, FutureWarning)\r\n"]
[460.616, "o", "        return param\r\n"]
[460.626, "o", "    else:\r\n"]
[460.636, "o", "        return default\r\n"]
[460.646, "o", "\r\n"]
[460.656, "o", "\r\n"]
[460.666, "o", "def dict_learning_online(\r\n"]
[460.676, "o", "    X,\r\n"]
[460.686, "o", "    n_components=2,\r\n"]
[460.696, "o", "    *,\r\n"]
[460.706, "o", "    alpha=1,\r\n"]
[460.716, "o", "    n_iter=\"deprecated\",\r\n"]
[460.726, "o", "    max_iter=None,\r\n"]
[460.736, "o", "    return_code=True,\r\n"]
[460.746, "o", "    dict_init=None,\r\n"]
[460.756, "o", "    callback=None,\r\n"]
[460.766, "o", "    batch_size=256,\r\n"]
[460.776, "o", "    verbose=False,\r\n"]
[460.786, "o", "    shuffle=True,\r\n"]
[460.796, "o", "    n_jobs=None,\r\n"]
[460.806, "o", "    method=\"lars\",\r\n"]
[460.816, "o", "    iter_offset=\"deprecated\",\r\n"]
[460.826, "o", "    random_state=None,\r\n"]
[460.836, "o", "    return_inner_stats=\"deprecated\",\r\n"]
[460.846, "o", "    inner_stats=\"deprecated\",\r\n"]
[460.856, "o", "    return_n_iter=\"deprecated\",\r\n"]
[460.866, "o", "    positive_dict=False,\r\n"]
[460.876, "o", "    positive_code=False,\r\n"]
[460.886, "o", "    method_max_iter=1000,\r\n"]
[460.896, "o", "    tol=1e-3,\r\n"]
[460.906, "o", "    max_no_improvement=10,\r\n"]
[460.916, "o", "):\r\n"]
[460.926, "o", "    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n"]
[460.936, "o", "\r\n"]
[460.946, "o", "    Finds the best dictionary and the corresponding sparse code for\r\n"]
[460.956, "o", "    approximating the data matrix X by solving::\r\n"]
[460.966, "o", "\r\n"]
[460.976, "o", "        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[460.986, "o", "                     (U,V)\r\n"]
[460.996, "o", "                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n"]
[461.006, "o", "\r\n"]
[461.016, "o", "    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n"]
[461.026, "o", "    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n"]
[461.036, "o", "    which is the sum of the absolute values of all the entries in the matrix.\r\n"]
[461.046, "o", "    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n"]
[461.056, "o", "    the input data.\r\n"]
[461.066, "o", "\r\n"]
[461.076, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[461.086, "o", "\r\n"]
[461.096, "o", "    Parameters\r\n"]
[461.106, "o", "    ----------\r\n"]
[461.116, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[461.126, "o", "        Data matrix.\r\n"]
[461.136, "o", "\r\n"]
[461.146, "o", "    n_components : int or None, default=2\r\n"]
[461.156, "o", "        Number of dictionary atoms to extract. If None, then ``n_components``\r\n"]
[461.166, "o", "        is set to ``n_features``.\r\n"]
[461.176, "o", "\r\n"]
[461.186, "o", "    alpha : float, default=1\r\n"]
[461.196, "o", "        Sparsity controlling parameter.\r\n"]
[461.206, "o", "\r\n"]
[461.216, "o", "    n_iter : int, default=100\r\n"]
[461.226, "o", "        Number of mini-batch iterations to perform.\r\n"]
[461.236, "o", "\r\n"]
[461.246, "o", "        .. deprecated:: 1.1\r\n"]
[461.256, "o", "           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[461.266, "o", "           `max_iter` instead.\r\n"]
[461.276, "o", "\r\n"]
[461.286, "o", "    max_iter : int, default=None\r\n"]
[461.296, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[461.306, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[461.316, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[461.326, "o", "\r\n"]
[461.336, "o", "        .. versionadded:: 1.1\r\n"]
[461.346, "o", "\r\n"]
[461.356, "o", "    return_code : bool, default=True\r\n"]
[461.366, "o", "        Whether to also return the code U or just the dictionary `V`.\r\n"]
[461.376, "o", "\r\n"]
[461.386, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[461.396, "o", "        Initial values for the dictionary for warm restart scenarios.\r\n"]
[461.406, "o", "        If `None`, the initial values for the dictionary are created\r\n"]
[461.416, "o", "        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n"]
[461.426, "o", "\r\n"]
[461.436, "o", "    callback : callable, default=None\r\n"]
[461.446, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[461.456, "o", "\r\n"]
[461.466, "o", "    batch_size : int, default=256\r\n"]
[461.476, "o", "        The number of samples to take in each batch.\r\n"]
[461.486, "o", "\r\n"]
[461.496, "o", "        .. versionchanged:: 1.3\r\n"]
[461.506, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[461.516, "o", "\r\n"]
[461.526, "o", "    verbose : bool, default=False\r\n"]
[461.536, "o", "        To control the verbosity of the procedure.\r\n"]
[461.546, "o", "\r\n"]
[461.556, "o", "    shuffle : bool, default=True\r\n"]
[461.566, "o", "        Whether to shuffle the data before splitting it in batches.\r\n"]
[461.576, "o", "\r\n"]
[461.586, "o", "    n_jobs : int, default=None\r\n"]
[461.596, "o", "        Number of parallel jobs to run.\r\n"]
[461.606, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[461.616, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[461.626, "o", "        for more details.\r\n"]
[461.636, "o", "\r\n"]
[461.646, "o", "    method : {'lars', 'cd'}, default='lars'\r\n"]
[461.656, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[461.666, "o", "          problem (`linear_model.lars_path`);\r\n"]
[461.676, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[461.686, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[461.696, "o", "          the estimated components are sparse.\r\n"]
[461.706, "o", "\r\n"]
[461.716, "o", "    iter_offset : int, default=0\r\n"]
[461.726, "o", "        Number of previous iterations completed on the dictionary used for\r\n"]
[461.736, "o", "        initialization.\r\n"]
[461.746, "o", "\r\n"]
[461.756, "o", "        .. deprecated:: 1.1\r\n"]
[461.766, "o", "           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n"]
[461.776, "o", "\r\n"]
[461.786, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[461.796, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[461.806, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[461.816, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[461.826, "o", "        results across multiple function calls.\r\n"]
[461.836, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[461.846, "o", "\r\n"]
[461.856, "o", "    return_inner_stats : bool, default=False\r\n"]
[461.866, "o", "        Return the inner statistics A (dictionary covariance) and B\r\n"]
[461.876, "o", "        (data approximation). Useful to restart the algorithm in an\r\n"]
[461.886, "o", "        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n"]
[461.896, "o", "        ignored.\r\n"]
[461.906, "o", "\r\n"]
[461.916, "o", "        .. deprecated:: 1.1\r\n"]
[461.926, "o", "           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[461.936, "o", "\r\n"]
[461.946, "o", "    inner_stats : tuple of (A, B) ndarrays, default=None\r\n"]
[461.956, "o", "        Inner sufficient statistics that are kept by the algorithm.\r\n"]
[461.966, "o", "        Passing them at initialization is useful in online settings, to\r\n"]
[461.976, "o", "        avoid losing the history of the evolution.\r\n"]
[461.986, "o", "        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n"]
[461.996, "o", "        `B` `(n_features, n_components)` is the data approximation matrix.\r\n"]
[462.006, "o", "\r\n"]
[462.016, "o", "        .. deprecated:: 1.1\r\n"]
[462.026, "o", "           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[462.036, "o", "\r\n"]
[462.046, "o", "    return_n_iter : bool, default=False\r\n"]
[462.056, "o", "        Whether or not to return the number of iterations.\r\n"]
[462.066, "o", "\r\n"]
[462.076, "o", "        .. deprecated:: 1.1\r\n"]
[462.086, "o", "           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n"]
[462.096, "o", "\r\n"]
[462.106, "o", "    positive_dict : bool, default=False\r\n"]
[462.116, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[462.126, "o", "\r\n"]
[462.136, "o", "        .. versionadded:: 0.20\r\n"]
[462.146, "o", "\r\n"]
[462.156, "o", "    positive_code : bool, default=False\r\n"]
[462.166, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[462.176, "o", "\r\n"]
[462.186, "o", "        .. versionadded:: 0.20\r\n"]
[462.196, "o", "\r\n"]
[462.206, "o", "    method_max_iter : int, default=1000\r\n"]
[462.216, "o", "        Maximum number of iterations to perform when solving the lasso problem.\r\n"]
[462.226, "o", "\r\n"]
[462.236, "o", "        .. versionadded:: 0.22\r\n"]
[462.246, "o", "\r\n"]
[462.256, "o", "    tol : float, default=1e-3\r\n"]
[462.266, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[462.276, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[462.286, "o", "\r\n"]
[462.296, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[462.306, "o", "        `tol` to 0.0.\r\n"]
[462.316, "o", "\r\n"]
[462.326, "o", "        .. versionadded:: 1.1\r\n"]
[462.336, "o", "\r\n"]
[462.346, "o", "    max_no_improvement : int, default=10\r\n"]
[462.356, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[462.366, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[462.376, "o", "        `max_iter` is not None.\r\n"]
[462.386, "o", "\r\n"]
[462.396, "o", "        To disable convergence detection based on cost function, set\r\n"]
[462.406, "o", "        `max_no_improvement` to None.\r\n"]
[462.416, "o", "\r\n"]
[462.426, "o", "        .. versionadded:: 1.1\r\n"]
[462.436, "o", "\r\n"]
[462.446, "o", "    Returns\r\n"]
[462.456, "o", "    -------\r\n"]
[462.466, "o", "    code : ndarray of shape (n_samples, n_components),\r\n"]
[462.476, "o", "        The sparse code (only returned if `return_code=True`).\r\n"]
[462.486, "o", "\r\n"]
[462.496, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[462.506, "o", "        The solutions to the dictionary learning problem.\r\n"]
[462.516, "o", "\r\n"]
[462.526, "o", "    n_iter : int\r\n"]
[462.536, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[462.546, "o", "        set to `True`.\r\n"]
[462.556, "o", "\r\n"]
[462.566, "o", "    See Also\r\n"]
[462.576, "o", "    --------\r\n"]
[462.586, "o", "    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n"]
[462.596, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[462.606, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n"]
[462.616, "o", "        learning algorithm.\r\n"]
[462.626, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[462.636, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[462.646, "o", "    \"\"\"\r\n"]
[462.656, "o", "    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n"]
[462.666, "o", "    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n"]
[462.676, "o", "        raise ValueError(\r\n"]
[462.686, "o", "            \"The following arguments are incompatible with 'max_iter': \"\r\n"]
[462.696, "o", "            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n"]
[462.706, "o", "        )\r\n"]
[462.716, "o", "\r\n"]
[462.726, "o", "    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n"]
[462.736, "o", "    return_inner_stats = _check_warn_deprecated(\r\n"]
[462.746, "o", "        return_inner_stats,\r\n"]
[462.756, "o", "        \"return_inner_stats\",\r\n"]
[462.766, "o", "        default=False,\r\n"]
[462.776, "o", "        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n"]
[462.786, "o", "    )\r\n"]
[462.796, "o", "    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n"]
[462.806, "o", "    return_n_iter = _check_warn_deprecated(\r\n"]
[462.816, "o", "        return_n_iter,\r\n"]
[462.826, "o", "        \"return_n_iter\",\r\n"]
[462.836, "o", "        default=False,\r\n"]
[462.846, "o", "        additional_message=(\r\n"]
[462.856, "o", "            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n"]
[462.866, "o", "            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n"]
[462.876, "o", "        ),\r\n"]
[462.886, "o", "    )\r\n"]
[462.896, "o", "\r\n"]
[462.906, "o", "    if max_iter is not None:\r\n"]
[462.916, "o", "        transform_algorithm = \"lasso_\" + method\r\n"]
[462.926, "o", "\r\n"]
[462.936, "o", "        est = MiniBatchDictionaryLearning(\r\n"]
[462.946, "o", "            n_components=n_components,\r\n"]
[462.956, "o", "            alpha=alpha,\r\n"]
[462.966, "o", "            n_iter=n_iter,\r\n"]
[462.976, "o", "            n_jobs=n_jobs,\r\n"]
[462.986, "o", "            fit_algorithm=method,\r\n"]
[462.996, "o", "            batch_size=batch_size,\r\n"]
[463.006, "o", "            shuffle=shuffle,\r\n"]
[463.016, "o", "            dict_init=dict_init,\r\n"]
[463.026, "o", "            random_state=random_state,\r\n"]
[463.036, "o", "            transform_algorithm=transform_algorithm,\r\n"]
[463.046, "o", "            transform_alpha=alpha,\r\n"]
[463.056, "o", "            positive_code=positive_code,\r\n"]
[463.066, "o", "            positive_dict=positive_dict,\r\n"]
[463.076, "o", "            transform_max_iter=method_max_iter,\r\n"]
[463.086, "o", "            verbose=verbose,\r\n"]
[463.096, "o", "            callback=callback,\r\n"]
[463.106, "o", "            tol=tol,\r\n"]
[463.116, "o", "            max_no_improvement=max_no_improvement,\r\n"]
[463.126, "o", "        ).fit(X)\r\n"]
[463.136, "o", "\r\n"]
[463.146, "o", "        if not return_code:\r\n"]
[463.156, "o", "            return est.components_\r\n"]
[463.166, "o", "        else:\r\n"]
[463.176, "o", "            code = est.transform(X)\r\n"]
[463.186, "o", "            return code, est.components_\r\n"]
[463.196, "o", "\r\n"]
[463.206, "o", "    # TODO(1.4) remove the whole old behavior\r\n"]
[463.216, "o", "    # Fallback to old behavior\r\n"]
[463.226, "o", "\r\n"]
[463.236, "o", "    n_iter = _check_warn_deprecated(\r\n"]
[463.246, "o", "        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n"]
[463.256, "o", "    )\r\n"]
[463.266, "o", "\r\n"]
[463.276, "o", "    if n_components is None:\r\n"]
[463.286, "o", "        n_components = X.shape[1]\r\n"]
[463.296, "o", "\r\n"]
[463.306, "o", "    if method not in (\"lars\", \"cd\"):\r\n"]
[463.316, "o", "        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n"]
[463.326, "o", "\r\n"]
[463.336, "o", "    _check_positive_coding(method, positive_code)\r\n"]
[463.346, "o", "\r\n"]
[463.356, "o", "    method = \"lasso_\" + method\r\n"]
[463.366, "o", "\r\n"]
[463.376, "o", "    t0 = time.time()\r\n"]
[463.386, "o", "    n_samples, n_features = X.shape\r\n"]
[463.396, "o", "    # Avoid integer division problems\r\n"]
[463.406, "o", "    alpha = float(alpha)\r\n"]
[463.416, "o", "    random_state = check_random_state(random_state)\r\n"]
[463.426, "o", "\r\n"]
[463.436, "o", "    # Init V with SVD of X\r\n"]
[463.446, "o", "    if dict_init is not None:\r\n"]
[463.456, "o", "        dictionary = dict_init\r\n"]
[463.466, "o", "    else:\r\n"]
[463.476, "o", "        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n"]
[463.486, "o", "        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[463.496, "o", "    r = len(dictionary)\r\n"]
[463.506, "o", "    if n_components <= r:\r\n"]
[463.516, "o", "        dictionary = dictionary[:n_components, :]\r\n"]
[463.526, "o", "    else:\r\n"]
[463.536, "o", "        dictionary = np.r_[\r\n"]
[463.546, "o", "            dictionary,\r\n"]
[463.556, "o", "            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n"]
[463.566, "o", "        ]\r\n"]
[463.576, "o", "\r\n"]
[463.586, "o", "    if verbose == 1:\r\n"]
[463.596, "o", "        print(\"[dict_learning]\", end=\" \")\r\n"]
[463.606, "o", "\r\n"]
[463.616, "o", "    if shuffle:\r\n"]
[463.626, "o", "        X_train = X.copy()\r\n"]
[463.636, "o", "        random_state.shuffle(X_train)\r\n"]
[463.646, "o", "    else:\r\n"]
[463.656, "o", "        X_train = X\r\n"]
[463.666, "o", "\r\n"]
[463.676, "o", "    X_train = check_array(\r\n"]
[463.686, "o", "        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n"]
[463.696, "o", "    )\r\n"]
[463.706, "o", "\r\n"]
[463.716, "o", "    # Fortran-order dict better suited for the sparse coding which is the\r\n"]
[463.726, "o", "    # bottleneck of this algorithm.\r\n"]
[463.736, "o", "    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n"]
[463.746, "o", "    dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[463.756, "o", "\r\n"]
[463.766, "o", "    batches = gen_batches(n_samples, batch_size)\r\n"]
[463.776, "o", "    batches = itertools.cycle(batches)\r\n"]
[463.786, "o", "\r\n"]
[463.796, "o", "    # The covariance of the dictionary\r\n"]
[463.806, "o", "    if inner_stats is None:\r\n"]
[463.816, "o", "        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n"]
[463.826, "o", "        # The data approximation\r\n"]
[463.836, "o", "        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n"]
[463.846, "o", "    else:\r\n"]
[463.856, "o", "        A = inner_stats[0].copy()\r\n"]
[463.866, "o", "        B = inner_stats[1].copy()\r\n"]
[463.876, "o", "\r\n"]
[463.886, "o", "    # If n_iter is zero, we need to return zero.\r\n"]
[463.896, "o", "    ii = iter_offset - 1\r\n"]
[463.906, "o", "\r\n"]
[463.916, "o", "    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n"]
[463.926, "o", "        this_X = X_train[batch]\r\n"]
[463.936, "o", "        dt = time.time() - t0\r\n"]
[463.946, "o", "        if verbose == 1:\r\n"]
[463.956, "o", "            sys.stdout.write(\".\")\r\n"]
[463.966, "o", "            sys.stdout.flush()\r\n"]
[463.976, "o", "        elif verbose:\r\n"]
[463.986, "o", "            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n"]
[463.996, "o", "                print(\r\n"]
[464.006, "o", "                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n"]
[464.016, "o", "                )\r\n"]
[464.026, "o", "\r\n"]
[464.036, "o", "        this_code = sparse_encode(\r\n"]
[464.046, "o", "            this_X,\r\n"]
[464.056, "o", "            dictionary,\r\n"]
[464.066, "o", "            algorithm=method,\r\n"]
[464.076, "o", "            alpha=alpha,\r\n"]
[464.086, "o", "            n_jobs=n_jobs,\r\n"]
[464.096, "o", "            check_input=False,\r\n"]
[464.106, "o", "            positive=positive_code,\r\n"]
[464.116, "o", "            max_iter=method_max_iter,\r\n"]
[464.126, "o", "            verbose=verbose,\r\n"]
[464.136, "o", "        )\r\n"]
[464.146, "o", "\r\n"]
[464.156, "o", "        # Update the auxiliary variables\r\n"]
[464.166, "o", "        if ii < batch_size - 1:\r\n"]
[464.176, "o", "            theta = float((ii + 1) * batch_size)\r\n"]
[464.186, "o", "        else:\r\n"]
[464.196, "o", "            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n"]
[464.206, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[464.216, "o", "\r\n"]
[464.226, "o", "        A *= beta\r\n"]
[464.236, "o", "        A += np.dot(this_code.T, this_code)\r\n"]
[464.246, "o", "        B *= beta\r\n"]
[464.256, "o", "        B += np.dot(this_X.T, this_code)\r\n"]
[464.266, "o", "\r\n"]
[464.276, "o", "        # Update dictionary in place\r\n"]
[464.286, "o", "        _update_dict(\r\n"]
[464.296, "o", "            dictionary,\r\n"]
[464.306, "o", "            this_X,\r\n"]
[464.316, "o", "            this_code,\r\n"]
[464.326, "o", "            A,\r\n"]
[464.336, "o", "            B,\r\n"]
[464.346, "o", "            verbose=verbose,\r\n"]
[464.356, "o", "            random_state=random_state,\r\n"]
[464.366, "o", "            positive=positive_dict,\r\n"]
[464.376, "o", "        )\r\n"]
[464.386, "o", "\r\n"]
[464.396, "o", "        # Maybe we need a stopping criteria based on the amount of\r\n"]
[464.406, "o", "        # modification in the dictionary\r\n"]
[464.416, "o", "        if callback is not None:\r\n"]
[464.426, "o", "            callback(locals())\r\n"]
[464.436, "o", "\r\n"]
[464.446, "o", "    if return_inner_stats:\r\n"]
[464.456, "o", "        if return_n_iter:\r\n"]
[464.466, "o", "            return dictionary, (A, B), ii - iter_offset + 1\r\n"]
[464.476, "o", "        else:\r\n"]
[464.486, "o", "            return dictionary, (A, B)\r\n"]
[464.496, "o", "    if return_code:\r\n"]
[464.506, "o", "        if verbose > 1:\r\n"]
[464.516, "o", "            print(\"Learning code...\", end=\" \")\r\n"]
[464.526, "o", "        elif verbose == 1:\r\n"]
[464.536, "o", "            print(\"|\", end=\" \")\r\n"]
[464.546, "o", "        code = sparse_encode(\r\n"]
[464.556, "o", "            X,\r\n"]
[464.566, "o", "            dictionary,\r\n"]
[464.576, "o", "            algorithm=method,\r\n"]
[464.586, "o", "            alpha=alpha,\r\n"]
[464.596, "o", "            n_jobs=n_jobs,\r\n"]
[464.606, "o", "            check_input=False,\r\n"]
[464.616, "o", "            positive=positive_code,\r\n"]
[464.626, "o", "            max_iter=method_max_iter,\r\n"]
[464.636, "o", "            verbose=verbose,\r\n"]
[464.646, "o", "        )\r\n"]
[464.656, "o", "        if verbose > 1:\r\n"]
[464.666, "o", "            dt = time.time() - t0\r\n"]
[464.676, "o", "            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n"]
[464.686, "o", "        if return_n_iter:\r\n"]
[464.696, "o", "            return code, dictionary, ii - iter_offset + 1\r\n"]
[464.706, "o", "        else:\r\n"]
[464.716, "o", "            return code, dictionary\r\n"]
[464.726, "o", "\r\n"]
[464.736, "o", "    if return_n_iter:\r\n"]
[464.746, "o", "        return dictionary, ii - iter_offset + 1\r\n"]
[464.756, "o", "    else:\r\n"]
[464.766, "o", "        return dictionary\r\n"]
[464.776, "o", "\r\n"]
[464.786, "o", "\r\n"]
[464.796, "o", "@validate_params(\r\n"]
[464.806, "o", "    {\r\n"]
[464.816, "o", "        \"X\": [\"array-like\"],\r\n"]
[464.826, "o", "        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[464.836, "o", "        \"return_n_iter\": [\"boolean\"],\r\n"]
[464.846, "o", "        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[464.856, "o", "    },\r\n"]
[464.866, "o", "    prefer_skip_nested_validation=False,\r\n"]
[464.876, "o", ")\r\n"]
[464.886, "o", "def dict_learning(\r\n"]
[464.896, "o", "    X,\r\n"]
[464.906, "o", "    n_components,\r\n"]
[464.916, "o", "    *,\r\n"]
[464.926, "o", "    alpha,\r\n"]
[464.936, "o", "    max_iter=100,\r\n"]
[465.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[465.002, "i", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r"]
[465.004, "o", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r\n"]
[465.016, "o", "\u001b[?2004l\r\n"]
[465.026, "o", "    n_iter : int\r\n"]
[465.036, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[465.046, "o", "        set to True.\r\n"]
[465.056, "o", "\r\n"]
[465.066, "o", "    See Also\r\n"]
[465.076, "o", "    --------\r\n"]
[465.086, "o", "    dict_learning_online : Solve a dictionary learning matrix factorization\r\n"]
[465.096, "o", "        problem online.\r\n"]
[465.106, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[465.116, "o", "    MiniBatchDictionaryLearning : A faster, less accurate version\r\n"]
[465.126, "o", "        of the dictionary learning algorithm.\r\n"]
[465.136, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[465.146, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[465.156, "o", "    \"\"\"\r\n"]
[465.166, "o", "    estimator = DictionaryLearning(\r\n"]
[465.176, "o", "        n_components=n_components,\r\n"]
[465.186, "o", "        alpha=alpha,\r\n"]
[465.196, "o", "        max_iter=max_iter,\r\n"]
[465.206, "o", "        tol=tol,\r\n"]
[465.216, "o", "        fit_algorithm=method,\r\n"]
[465.226, "o", "        n_jobs=n_jobs,\r\n"]
[465.236, "o", "        dict_init=dict_init,\r\n"]
[465.246, "o", "        callback=callback,\r\n"]
[465.256, "o", "        code_init=code_init,\r\n"]
[465.266, "o", "        verbose=verbose,\r\n"]
[465.276, "o", "        random_state=random_state,\r\n"]
[465.286, "o", "        positive_code=positive_code,\r\n"]
[465.296, "o", "        positive_dict=positive_dict,\r\n"]
[465.306, "o", "        transform_max_iter=method_max_iter,\r\n"]
[465.316, "o", "    )\r\n"]
[465.326, "o", "    code = estimator.fit_transform(X)\r\n"]
[465.336, "o", "    if return_n_iter:\r\n"]
[465.346, "o", "        return (\r\n"]
[465.356, "o", "            code,\r\n"]
[465.366, "o", "            estimator.components_,\r\n"]
[465.376, "o", "            estimator.error_,\r\n"]
[465.386, "o", "            estimator.n_iter_,\r\n"]
[465.396, "o", "        )\r\n"]
[465.406, "o", "    return code, estimator.components_, estimator.error_\r\n"]
[465.416, "o", "\r\n"]
[465.426, "o", "\r\n"]
[465.436, "o", "class _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n"]
[465.446, "o", "    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n"]
[465.456, "o", "\r\n"]
[465.466, "o", "    def __init__(\r\n"]
[465.476, "o", "        self,\r\n"]
[465.486, "o", "        transform_algorithm,\r\n"]
[465.496, "o", "        transform_n_nonzero_coefs,\r\n"]
[465.506, "o", "        transform_alpha,\r\n"]
[465.516, "o", "        split_sign,\r\n"]
[465.526, "o", "        n_jobs,\r\n"]
[465.536, "o", "        positive_code,\r\n"]
[465.546, "o", "        transform_max_iter,\r\n"]
[465.556, "o", "    ):\r\n"]
[465.566, "o", "        self.transform_algorithm = transform_algorithm\r\n"]
[465.576, "o", "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n"]
[465.586, "o", "        self.transform_alpha = transform_alpha\r\n"]
[465.596, "o", "        self.transform_max_iter = transform_max_iter\r\n"]
[465.606, "o", "        self.split_sign = split_sign\r\n"]
[465.616, "o", "        self.n_jobs = n_jobs\r\n"]
[465.626, "o", "        self.positive_code = positive_code\r\n"]
[465.636, "o", "\r\n"]
[465.646, "o", "    def _transform(self, X, dictionary):\r\n"]
[465.656, "o", "        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n"]
[465.666, "o", "        SparseCoder.\"\"\"\r\n"]
[465.676, "o", "        X = self._validate_data(X, reset=False)\r\n"]
[465.686, "o", "\r\n"]
[465.696, "o", "        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n"]
[465.706, "o", "            transform_alpha = self.alpha\r\n"]
[465.716, "o", "        else:\r\n"]
[465.726, "o", "            transform_alpha = self.transform_alpha\r\n"]
[465.736, "o", "\r\n"]
[465.746, "o", "        code = sparse_encode(\r\n"]
[465.756, "o", "            X,\r\n"]
[465.766, "o", "            dictionary,\r\n"]
[465.776, "o", "            algorithm=self.transform_algorithm,\r\n"]
[465.786, "o", "            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n"]
[465.796, "o", "            alpha=transform_alpha,\r\n"]
[465.806, "o", "            max_iter=self.transform_max_iter,\r\n"]
[465.816, "o", "            n_jobs=self.n_jobs,\r\n"]
[465.826, "o", "            positive=self.positive_code,\r\n"]
[465.836, "o", "        )\r\n"]
[465.846, "o", "\r\n"]
[465.856, "o", "        if self.split_sign:\r\n"]
[465.866, "o", "            # feature vector is split into a positive and negative side\r\n"]
[465.876, "o", "            n_samples, n_features = code.shape\r\n"]
[465.886, "o", "            split_code = np.empty((n_samples, 2 * n_features))\r\n"]
[465.896, "o", "            split_code[:, :n_features] = np.maximum(code, 0)\r\n"]
[465.906, "o", "            split_code[:, n_features:] = -np.minimum(code, 0)\r\n"]
[465.916, "o", "            code = split_code\r\n"]
[465.926, "o", "\r\n"]
[465.936, "o", "        return code\r\n"]
[465.946, "o", "\r\n"]
[465.956, "o", "    def transform(self, X):\r\n"]
[465.966, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[465.976, "o", "\r\n"]
[465.986, "o", "        Coding method is determined by the object parameter\r\n"]
[465.996, "o", "        `transform_algorithm`.\r\n"]
[466.006, "o", "\r\n"]
[466.016, "o", "        Parameters\r\n"]
[466.026, "o", "        ----------\r\n"]
[466.036, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[466.046, "o", "            Test data to be transformed, must have the same number of\r\n"]
[466.056, "o", "            features as the data used to train the model.\r\n"]
[466.066, "o", "\r\n"]
[466.076, "o", "        Returns\r\n"]
[466.086, "o", "        -------\r\n"]
[466.096, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[466.106, "o", "            Transformed data.\r\n"]
[466.116, "o", "        \"\"\"\r\n"]
[466.126, "o", "        check_is_fitted(self)\r\n"]
[466.136, "o", "        return self._transform(X, self.components_)\r\n"]
[466.146, "o", "\r\n"]
[466.156, "o", "\r\n"]
[466.166, "o", "class SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n"]
[466.176, "o", "    \"\"\"Sparse coding.\r\n"]
[466.186, "o", "\r\n"]
[466.196, "o", "    Finds a sparse representation of data against a fixed, precomputed\r\n"]
[466.206, "o", "    dictionary.\r\n"]
[466.216, "o", "\r\n"]
[466.226, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[466.236, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[466.246, "o", "\r\n"]
[466.256, "o", "        X ~= code * dictionary\r\n"]
[466.266, "o", "\r\n"]
[466.276, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[466.286, "o", "\r\n"]
[466.296, "o", "    Parameters\r\n"]
[466.306, "o", "    ----------\r\n"]
[466.316, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[466.326, "o", "        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n"]
[466.336, "o", "        normalized to unit norm.\r\n"]
[466.346, "o", "\r\n"]
[466.356, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[466.366, "o", "            'threshold'}, default='omp'\r\n"]
[466.376, "o", "        Algorithm used to transform the data:\r\n"]
[466.386, "o", "\r\n"]
[466.396, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[466.406, "o", "          (`linear_model.lars_path`);\r\n"]
[466.416, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[466.426, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[466.436, "o", "          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n"]
[466.446, "o", "          the estimated components are sparse;\r\n"]
[466.456, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[466.466, "o", "          solution;\r\n"]
[466.476, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[466.486, "o", "          the projection ``dictionary * X'``.\r\n"]
[466.496, "o", "\r\n"]
[466.506, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[466.516, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[466.526, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[466.536, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[466.546, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[466.556, "o", "\r\n"]
[466.566, "o", "    transform_alpha : float, default=None\r\n"]
[466.576, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[466.586, "o", "        penalty applied to the L1 norm.\r\n"]
[466.596, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[466.606, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[466.616, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[466.626, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[466.636, "o", "        `n_nonzero_coefs`.\r\n"]
[466.646, "o", "        If `None`, default to 1.\r\n"]
[466.656, "o", "\r\n"]
[466.666, "o", "    split_sign : bool, default=False\r\n"]
[466.676, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[466.686, "o", "        its negative part and its positive part. This can improve the\r\n"]
[466.696, "o", "        performance of downstream classifiers.\r\n"]
[466.706, "o", "\r\n"]
[466.716, "o", "    n_jobs : int, default=None\r\n"]
[466.726, "o", "        Number of parallel jobs to run.\r\n"]
[466.736, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[466.746, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[466.756, "o", "        for more details.\r\n"]
[466.766, "o", "\r\n"]
[466.776, "o", "    positive_code : bool, default=False\r\n"]
[466.786, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[466.796, "o", "\r\n"]
[466.806, "o", "        .. versionadded:: 0.20\r\n"]
[466.816, "o", "\r\n"]
[466.826, "o", "    transform_max_iter : int, default=1000\r\n"]
[466.836, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[466.846, "o", "        `lasso_lars`.\r\n"]
[466.856, "o", "\r\n"]
[466.866, "o", "        .. versionadded:: 0.22\r\n"]
[466.876, "o", "\r\n"]
[466.886, "o", "    Attributes\r\n"]
[466.896, "o", "    ----------\r\n"]
[466.906, "o", "    n_components_ : int\r\n"]
[466.916, "o", "        Number of atoms.\r\n"]
[466.926, "o", "\r\n"]
[466.936, "o", "    n_features_in_ : int\r\n"]
[466.946, "o", "        Number of features seen during :term:`fit`.\r\n"]
[466.956, "o", "\r\n"]
[466.966, "o", "        .. versionadded:: 0.24\r\n"]
[466.976, "o", "\r\n"]
[466.986, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[466.996, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[467.006, "o", "        has feature names that are all strings.\r\n"]
[467.016, "o", "\r\n"]
[467.026, "o", "        .. versionadded:: 1.0\r\n"]
[467.036, "o", "\r\n"]
[467.046, "o", "    See Also\r\n"]
[467.056, "o", "    --------\r\n"]
[467.066, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[467.076, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n"]
[467.086, "o", "        dictionary learning algorithm.\r\n"]
[467.096, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[467.106, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[467.116, "o", "    sparse_encode : Sparse coding where each row of the result is the solution\r\n"]
[467.126, "o", "        to a sparse coding problem.\r\n"]
[467.136, "o", "\r\n"]
[467.146, "o", "    Examples\r\n"]
[467.156, "o", "    --------\r\n"]
[467.166, "o", "    >>> import numpy as np\r\n"]
[467.176, "o", "    >>> from sklearn.decomposition import SparseCoder\r\n"]
[467.186, "o", "    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n"]
[467.196, "o", "    >>> dictionary = np.array(\r\n"]
[467.206, "o", "    ...     [[0, 1, 0],\r\n"]
[467.216, "o", "    ...      [-1, -1, 2],\r\n"]
[467.226, "o", "    ...      [1, 1, 1],\r\n"]
[467.236, "o", "    ...      [0, 1, 1],\r\n"]
[467.246, "o", "    ...      [0, 2, 1]],\r\n"]
[467.256, "o", "    ...    dtype=np.float64\r\n"]
[467.266, "o", "    ... )\r\n"]
[467.276, "o", "    >>> coder = SparseCoder(\r\n"]
[467.286, "o", "    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n"]
[467.296, "o", "    ...     transform_alpha=1e-10,\r\n"]
[467.306, "o", "    ... )\r\n"]
[467.316, "o", "    >>> coder.transform(X)\r\n"]
[467.326, "o", "    array([[ 0.,  0., -1.,  0.,  0.],\r\n"]
[467.336, "o", "           [ 0.,  1.,  1.,  0.,  0.]])\r\n"]
[467.346, "o", "    \"\"\"\r\n"]
[467.356, "o", "\r\n"]
[467.366, "o", "    _required_parameters = [\"dictionary\"]\r\n"]
[467.376, "o", "\r\n"]
[467.386, "o", "    def __init__(\r\n"]
[467.396, "o", "        self,\r\n"]
[467.406, "o", "        dictionary,\r\n"]
[467.416, "o", "        *,\r\n"]
[467.426, "o", "        transform_algorithm=\"omp\",\r\n"]
[467.436, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[467.446, "o", "        transform_alpha=None,\r\n"]
[467.456, "o", "        split_sign=False,\r\n"]
[467.466, "o", "        n_jobs=None,\r\n"]
[467.476, "o", "        positive_code=False,\r\n"]
[467.486, "o", "        transform_max_iter=1000,\r\n"]
[467.496, "o", "    ):\r\n"]
[467.506, "o", "        super().__init__(\r\n"]
[467.516, "o", "            transform_algorithm,\r\n"]
[467.526, "o", "            transform_n_nonzero_coefs,\r\n"]
[467.536, "o", "            transform_alpha,\r\n"]
[467.546, "o", "            split_sign,\r\n"]
[467.556, "o", "            n_jobs,\r\n"]
[467.566, "o", "            positive_code,\r\n"]
[467.576, "o", "            transform_max_iter,\r\n"]
[467.586, "o", "        )\r\n"]
[467.596, "o", "        self.dictionary = dictionary\r\n"]
[467.606, "o", "\r\n"]
[467.616, "o", "    def fit(self, X, y=None):\r\n"]
[467.626, "o", "        \"\"\"Do nothing and return the estimator unchanged.\r\n"]
[467.636, "o", "\r\n"]
[467.646, "o", "        This method is just there to implement the usual API and hence\r\n"]
[467.656, "o", "        work in pipelines.\r\n"]
[467.666, "o", "\r\n"]
[467.676, "o", "        Parameters\r\n"]
[467.686, "o", "        ----------\r\n"]
[467.696, "o", "        X : Ignored\r\n"]
[467.706, "o", "            Not used, present for API consistency by convention.\r\n"]
[467.716, "o", "\r\n"]
[467.726, "o", "        y : Ignored\r\n"]
[467.736, "o", "            Not used, present for API consistency by convention.\r\n"]
[467.746, "o", "\r\n"]
[467.756, "o", "        Returns\r\n"]
[467.766, "o", "        -------\r\n"]
[467.776, "o", "        self : object\r\n"]
[467.786, "o", "            Returns the instance itself.\r\n"]
[467.796, "o", "        \"\"\"\r\n"]
[467.806, "o", "        return self\r\n"]
[467.816, "o", "\r\n"]
[467.826, "o", "    def transform(self, X, y=None):\r\n"]
[467.836, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[467.846, "o", "\r\n"]
[467.856, "o", "        Coding method is determined by the object parameter\r\n"]
[467.866, "o", "        `transform_algorithm`.\r\n"]
[467.876, "o", "\r\n"]
[467.886, "o", "        Parameters\r\n"]
[467.896, "o", "        ----------\r\n"]
[467.906, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[467.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[467.926, "o", "            and `n_features` is the number of features.\r\n"]
[467.936, "o", "\r\n"]
[467.946, "o", "        y : Ignored\r\n"]
[467.956, "o", "            Not used, present for API consistency by convention.\r\n"]
[467.966, "o", "\r\n"]
[467.976, "o", "        Returns\r\n"]
[467.986, "o", "        -------\r\n"]
[467.996, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[468.006, "o", "            Transformed data.\r\n"]
[468.016, "o", "        \"\"\"\r\n"]
[468.026, "o", "        return super()._transform(X, self.dictionary)\r\n"]
[468.036, "o", "\r\n"]
[468.046, "o", "    def _more_tags(self):\r\n"]
[468.056, "o", "        return {\r\n"]
[468.066, "o", "            \"requires_fit\": False,\r\n"]
[468.076, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[468.086, "o", "        }\r\n"]
[468.096, "o", "\r\n"]
[468.106, "o", "    @property\r\n"]
[468.116, "o", "    def n_components_(self):\r\n"]
[468.126, "o", "        \"\"\"Number of atoms.\"\"\"\r\n"]
[468.136, "o", "        return self.dictionary.shape[0]\r\n"]
[468.146, "o", "\r\n"]
[468.156, "o", "    @property\r\n"]
[468.166, "o", "    def n_features_in_(self):\r\n"]
[468.176, "o", "        \"\"\"Number of features seen during `fit`.\"\"\"\r\n"]
[468.186, "o", "        return self.dictionary.shape[1]\r\n"]
[468.196, "o", "\r\n"]
[468.206, "o", "    @property\r\n"]
[468.216, "o", "    def _n_features_out(self):\r\n"]
[468.226, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[468.236, "o", "        return self.n_components_\r\n"]
[468.246, "o", "\r\n"]
[468.256, "o", "\r\n"]
[468.266, "o", "class DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[468.276, "o", "    \"\"\"Dictionary learning.\r\n"]
[468.286, "o", "\r\n"]
[468.296, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[468.306, "o", "    encoding the fitted data.\r\n"]
[468.316, "o", "\r\n"]
[468.326, "o", "    Solves the optimization problem::\r\n"]
[468.336, "o", "\r\n"]
[468.346, "o", "        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[468.356, "o", "                    (U,V)\r\n"]
[468.366, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[468.376, "o", "\r\n"]
[468.386, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[468.396, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[468.406, "o", "    of all the entries in the matrix.\r\n"]
[468.416, "o", "\r\n"]
[468.426, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[468.436, "o", "\r\n"]
[468.446, "o", "    Parameters\r\n"]
[468.456, "o", "    ----------\r\n"]
[468.466, "o", "    n_components : int, default=None\r\n"]
[468.476, "o", "        Number of dictionary elements to extract. If None, then ``n_components``\r\n"]
[468.486, "o", "        is set to ``n_features``.\r\n"]
[468.496, "o", "\r\n"]
[468.506, "o", "    alpha : float, default=1.0\r\n"]
[468.516, "o", "        Sparsity controlling parameter.\r\n"]
[468.526, "o", "\r\n"]
[468.536, "o", "    max_iter : int, default=1000\r\n"]
[468.546, "o", "        Maximum number of iterations to perform.\r\n"]
[468.556, "o", "\r\n"]
[468.566, "o", "    tol : float, default=1e-8\r\n"]
[468.576, "o", "        Tolerance for numerical error.\r\n"]
[468.586, "o", "\r\n"]
[468.596, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[468.606, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[468.616, "o", "          problem (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[468.626, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[468.636, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n"]
[468.646, "o", "          faster if the estimated components are sparse.\r\n"]
[468.656, "o", "\r\n"]
[468.666, "o", "        .. versionadded:: 0.17\r\n"]
[468.676, "o", "           *cd* coordinate descent method to improve speed.\r\n"]
[468.686, "o", "\r\n"]
[468.696, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[468.706, "o", "            'threshold'}, default='omp'\r\n"]
[468.716, "o", "        Algorithm used to transform the data:\r\n"]
[468.726, "o", "\r\n"]
[468.736, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[468.746, "o", "          (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[468.756, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[468.766, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[468.776, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n"]
[468.786, "o", "          will be faster if the estimated components are sparse.\r\n"]
[468.796, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[468.806, "o", "          solution.\r\n"]
[468.816, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[468.826, "o", "          the projection ``dictionary * X'``.\r\n"]
[468.836, "o", "\r\n"]
[468.846, "o", "        .. versionadded:: 0.17\r\n"]
[468.856, "o", "           *lasso_cd* coordinate descent method to improve speed.\r\n"]
[468.866, "o", "\r\n"]
[468.876, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[468.886, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[468.896, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[468.906, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[468.916, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[468.926, "o", "\r\n"]
[468.936, "o", "    transform_alpha : float, default=None\r\n"]
[468.946, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[468.956, "o", "        penalty applied to the L1 norm.\r\n"]
[468.966, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[468.976, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[468.986, "o", "        If `None`, defaults to `alpha`.\r\n"]
[468.996, "o", "\r\n"]
[469.006, "o", "        .. versionchanged:: 1.2\r\n"]
[469.016, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[469.026, "o", "\r\n"]
[469.036, "o", "    n_jobs : int or None, default=None\r\n"]
[469.046, "o", "        Number of parallel jobs to run.\r\n"]
[469.056, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[469.066, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[469.076, "o", "        for more details.\r\n"]
[469.086, "o", "\r\n"]
[469.096, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[469.106, "o", "        Initial value for the code, for warm restart. Only used if `code_init`\r\n"]
[469.116, "o", "        and `dict_init` are not None.\r\n"]
[469.126, "o", "\r\n"]
[469.136, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[469.146, "o", "        Initial values for the dictionary, for warm restart. Only used if\r\n"]
[469.156, "o", "        `code_init` and `dict_init` are not None.\r\n"]
[469.166, "o", "\r\n"]
[469.176, "o", "    callback : callable, default=None\r\n"]
[469.186, "o", "        Callable that gets invoked every five iterations.\r\n"]
[469.196, "o", "\r\n"]
[469.206, "o", "        .. versionadded:: 1.3\r\n"]
[469.216, "o", "\r\n"]
[469.226, "o", "    verbose : bool, default=False\r\n"]
[469.236, "o", "        To control the verbosity of the procedure.\r\n"]
[469.246, "o", "\r\n"]
[469.256, "o", "    split_sign : bool, default=False\r\n"]
[469.266, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[469.276, "o", "        its negative part and its positive part. This can improve the\r\n"]
[469.286, "o", "        performance of downstream classifiers.\r\n"]
[469.296, "o", "\r\n"]
[469.306, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[469.316, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[469.326, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[469.336, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[469.346, "o", "        results across multiple function calls.\r\n"]
[469.356, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[469.366, "o", "\r\n"]
[469.376, "o", "    positive_code : bool, default=False\r\n"]
[469.386, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[469.396, "o", "\r\n"]
[469.406, "o", "        .. versionadded:: 0.20\r\n"]
[469.416, "o", "\r\n"]
[469.426, "o", "    positive_dict : bool, default=False\r\n"]
[469.436, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[469.446, "o", "\r\n"]
[469.456, "o", "        .. versionadded:: 0.20\r\n"]
[469.466, "o", "\r\n"]
[469.476, "o", "    transform_max_iter : int, default=1000\r\n"]
[469.486, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[469.496, "o", "        `'lasso_lars'`.\r\n"]
[469.506, "o", "\r\n"]
[469.516, "o", "        .. versionadded:: 0.22\r\n"]
[469.526, "o", "\r\n"]
[469.536, "o", "    Attributes\r\n"]
[469.546, "o", "    ----------\r\n"]
[469.556, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[469.566, "o", "        dictionary atoms extracted from the data\r\n"]
[469.576, "o", "\r\n"]
[469.586, "o", "    error_ : array\r\n"]
[469.596, "o", "        vector of errors at each iteration\r\n"]
[469.606, "o", "\r\n"]
[469.616, "o", "    n_features_in_ : int\r\n"]
[469.626, "o", "        Number of features seen during :term:`fit`.\r\n"]
[469.636, "o", "\r\n"]
[469.646, "o", "        .. versionadded:: 0.24\r\n"]
[469.656, "o", "\r\n"]
[469.666, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[469.676, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[469.686, "o", "        has feature names that are all strings.\r\n"]
[469.696, "o", "\r\n"]
[469.706, "o", "        .. versionadded:: 1.0\r\n"]
[469.716, "o", "\r\n"]
[469.726, "o", "    n_iter_ : int\r\n"]
[469.736, "o", "        Number of iterations run.\r\n"]
[469.746, "o", "\r\n"]
[469.756, "o", "    See Also\r\n"]
[469.766, "o", "    --------\r\n"]
[469.776, "o", "    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n"]
[469.786, "o", "        dictionary learning algorithm.\r\n"]
[469.796, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[469.806, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[469.816, "o", "        precomputed dictionary.\r\n"]
[469.826, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[469.836, "o", "\r\n"]
[469.846, "o", "    References\r\n"]
[469.856, "o", "    ----------\r\n"]
[469.866, "o", "\r\n"]
[469.876, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[469.886, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[469.896, "o", "\r\n"]
[469.906, "o", "    Examples\r\n"]
[469.916, "o", "    --------\r\n"]
[469.926, "o", "    >>> import numpy as np\r\n"]
[469.936, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[470.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[470.002, "i", "cd asv_benchmarks\r"]
[470.004, "o", "cd asv_benchmarks\r\n"]
[470.006, "o", "\u001b[?2004l\r\n"]
[475.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[475.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[475.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[476.654, "o", "\u001b[?2004l\r\n"]
[478.302, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[480.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[480.002, "i", "cd ..\r"]
[480.004, "o", "cd ..\r\n"]
[480.006, "o", "\u001b[?2004l\r\n"]
[485.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[485.002, "i", "grep -n \"def _minibatch_step\\|def dict_learning_online\\|def _mini_batch_dictionary_learning\\|def _sparse_encode_pre\r"]
[485.004, "o", "grep -n \"def _minibatch_step\\|def dict_learning_online\\|def _mini_batch_dictionary_learning\\|def _sparse_encode_pre\r\n"]
[485.9948, "o", "ecomputed\" sklearn/decomposition/_dict_learning.py\r\n"]
[486.9836, "o", "\u001b[?2004l\r\n"]
[487.9724, "o", "\u001b[32m\u001b[K38\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kdef _sparse_encode_precomputed\u001b[m\u001b[K(\r\n"]
[488.9612, "o", "\u001b[32m\u001b[K664\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kdef dict_learning_online\u001b[m\u001b[K(\r\n"]
[490.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[490.002, "i", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r"]
[490.004, "o", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r\n"]
[490.016, "o", "\u001b[?2004l\r\n"]
[490.026, "o", "\"\"\" Dictionary learning.\r\n"]
[490.036, "o", "\"\"\"\r\n"]
[490.046, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[490.056, "o", "# License: BSD 3 clause\r\n"]
[490.066, "o", "\r\n"]
[490.076, "o", "import itertools\r\n"]
[490.086, "o", "import sys\r\n"]
[490.096, "o", "import time\r\n"]
[490.106, "o", "import warnings\r\n"]
[490.116, "o", "from math import ceil\r\n"]
[490.126, "o", "from numbers import Integral, Real\r\n"]
[490.136, "o", "\r\n"]
[490.146, "o", "import numpy as np\r\n"]
[490.156, "o", "from joblib import effective_n_jobs\r\n"]
[490.166, "o", "from scipy import linalg\r\n"]
[490.176, "o", "\r\n"]
[490.186, "o", "from ..base import (\r\n"]
[490.196, "o", "    BaseEstimator,\r\n"]
[490.206, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[490.216, "o", "    TransformerMixin,\r\n"]
[490.226, "o", "    _fit_context,\r\n"]
[490.236, "o", ")\r\n"]
[490.246, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[490.256, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[490.266, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[490.276, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[490.286, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[490.296, "o", "from ..utils.validation import check_is_fitted\r\n"]
[490.306, "o", "\r\n"]
[490.316, "o", "\r\n"]
[490.326, "o", "def _check_positive_coding(method, positive):\r\n"]
[490.336, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[490.346, "o", "        raise ValueError(\r\n"]
[490.356, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[490.366, "o", "        )\r\n"]
[490.376, "o", "\r\n"]
[490.386, "o", "\r\n"]
[490.396, "o", "def _sparse_encode_precomputed(\r\n"]
[490.406, "o", "    X,\r\n"]
[490.416, "o", "    dictionary,\r\n"]
[490.426, "o", "    *,\r\n"]
[490.436, "o", "    gram=None,\r\n"]
[490.446, "o", "    cov=None,\r\n"]
[490.456, "o", "    algorithm=\"lasso_lars\",\r\n"]
[490.466, "o", "    regularization=None,\r\n"]
[490.476, "o", "    copy_cov=True,\r\n"]
[490.486, "o", "    init=None,\r\n"]
[490.496, "o", "    max_iter=1000,\r\n"]
[490.506, "o", "    verbose=0,\r\n"]
[490.516, "o", "    positive=False,\r\n"]
[490.526, "o", "):\r\n"]
[490.536, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[490.546, "o", "\r\n"]
[490.556, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[490.566, "o", "\r\n"]
[490.576, "o", "    Parameters\r\n"]
[490.586, "o", "    ----------\r\n"]
[490.596, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[490.606, "o", "        Data matrix.\r\n"]
[490.616, "o", "\r\n"]
[490.626, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[490.636, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[490.646, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[490.656, "o", "\r\n"]
[490.666, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[490.676, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[490.686, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[490.696, "o", "\r\n"]
[490.706, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[490.716, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[490.726, "o", "\r\n"]
[490.736, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[490.746, "o", "            default='lasso_lars'\r\n"]
[490.756, "o", "        The algorithm used:\r\n"]
[490.766, "o", "\r\n"]
[490.776, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[490.786, "o", "          (`linear_model.lars_path`);\r\n"]
[490.796, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[490.806, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[490.816, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[490.826, "o", "          the estimated components are sparse;\r\n"]
[490.836, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[490.846, "o", "          solution;\r\n"]
[490.856, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[490.866, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[490.876, "o", "\r\n"]
[490.886, "o", "    regularization : int or float, default=None\r\n"]
[490.896, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[490.906, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[490.916, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[490.926, "o", "\r\n"]
[490.936, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[490.946, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[490.956, "o", "        `algorithm='lasso_cd'`.\r\n"]
[490.966, "o", "\r\n"]
[490.976, "o", "    max_iter : int, default=1000\r\n"]
[490.986, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[490.996, "o", "        `'lasso_lars'`.\r\n"]
[491.006, "o", "\r\n"]
[491.016, "o", "    copy_cov : bool, default=True\r\n"]
[491.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[491.036, "o", "        be overwritten.\r\n"]
[491.046, "o", "\r\n"]
[491.056, "o", "    verbose : int, default=0\r\n"]
[491.066, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[491.076, "o", "\r\n"]
[491.086, "o", "    positive: bool, default=False\r\n"]
[491.096, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[491.106, "o", "\r\n"]
[491.116, "o", "        .. versionadded:: 0.20\r\n"]
[491.126, "o", "\r\n"]
[491.136, "o", "    Returns\r\n"]
[491.146, "o", "    -------\r\n"]
[491.156, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[491.166, "o", "        The sparse codes.\r\n"]
[491.176, "o", "    \"\"\"\r\n"]
[491.186, "o", "    n_samples, n_features = X.shape\r\n"]
[491.196, "o", "    n_components = dictionary.shape[0]\r\n"]
[491.206, "o", "\r\n"]
[491.216, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[491.226, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[491.236, "o", "        try:\r\n"]
[491.246, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[491.256, "o", "\r\n"]
[491.266, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[491.276, "o", "            # corrects the verbosity level.\r\n"]
[491.286, "o", "            lasso_lars = LassoLars(\r\n"]
[491.296, "o", "                alpha=alpha,\r\n"]
[491.306, "o", "                fit_intercept=False,\r\n"]
[491.316, "o", "                verbose=verbose,\r\n"]
[491.326, "o", "                precompute=gram,\r\n"]
[491.336, "o", "                fit_path=False,\r\n"]
[491.346, "o", "                positive=positive,\r\n"]
[491.356, "o", "                max_iter=max_iter,\r\n"]
[491.366, "o", "            )\r\n"]
[491.376, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[491.386, "o", "            new_code = lasso_lars.coef_\r\n"]
[491.396, "o", "        finally:\r\n"]
[491.406, "o", "            np.seterr(**err_mgt)\r\n"]
[491.416, "o", "\r\n"]
[491.426, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[491.436, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[491.446, "o", "\r\n"]
[491.456, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[491.466, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[491.476, "o", "        # argument that we could pass in from Lasso.\r\n"]
[491.486, "o", "        clf = Lasso(\r\n"]
[491.496, "o", "            alpha=alpha,\r\n"]
[491.506, "o", "            fit_intercept=False,\r\n"]
[491.516, "o", "            precompute=gram,\r\n"]
[491.526, "o", "            max_iter=max_iter,\r\n"]
[491.536, "o", "            warm_start=True,\r\n"]
[491.546, "o", "            positive=positive,\r\n"]
[491.556, "o", "        )\r\n"]
[491.566, "o", "\r\n"]
[491.576, "o", "        if init is not None:\r\n"]
[491.586, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[491.596, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[491.606, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[491.616, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[491.626, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[491.636, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[491.646, "o", "                init = np.array(init)\r\n"]
[491.656, "o", "            clf.coef_ = init\r\n"]
[491.666, "o", "\r\n"]
[491.676, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[491.686, "o", "        new_code = clf.coef_\r\n"]
[491.696, "o", "\r\n"]
[491.706, "o", "    elif algorithm == \"lars\":\r\n"]
[491.716, "o", "        try:\r\n"]
[491.726, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[491.736, "o", "\r\n"]
[491.746, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[491.756, "o", "            # corrects the verbosity level.\r\n"]
[491.766, "o", "            lars = Lars(\r\n"]
[491.776, "o", "                fit_intercept=False,\r\n"]
[491.786, "o", "                verbose=verbose,\r\n"]
[491.796, "o", "                precompute=gram,\r\n"]
[491.806, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[491.816, "o", "                fit_path=False,\r\n"]
[491.826, "o", "            )\r\n"]
[491.836, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[491.846, "o", "            new_code = lars.coef_\r\n"]
[491.856, "o", "        finally:\r\n"]
[491.866, "o", "            np.seterr(**err_mgt)\r\n"]
[491.876, "o", "\r\n"]
[491.886, "o", "    elif algorithm == \"threshold\":\r\n"]
[491.896, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[491.906, "o", "        if positive:\r\n"]
[491.916, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[491.926, "o", "\r\n"]
[491.936, "o", "    elif algorithm == \"omp\":\r\n"]
[491.946, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[491.956, "o", "            Gram=gram,\r\n"]
[491.966, "o", "            Xy=cov,\r\n"]
[491.976, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[491.986, "o", "            tol=None,\r\n"]
[491.996, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[492.006, "o", "            copy_Xy=copy_cov,\r\n"]
[492.016, "o", "        ).T\r\n"]
[492.026, "o", "\r\n"]
[492.036, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[492.046, "o", "\r\n"]
[492.056, "o", "\r\n"]
[492.066, "o", "@validate_params(\r\n"]
[492.076, "o", "    {\r\n"]
[492.086, "o", "        \"X\": [\"array-like\"],\r\n"]
[492.096, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[492.106, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[492.116, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[492.126, "o", "        \"algorithm\": [\r\n"]
[492.136, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[492.146, "o", "        ],\r\n"]
[492.156, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[492.166, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[492.176, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[492.186, "o", "        \"init\": [\"array-like\", None],\r\n"]
[492.196, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[492.206, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[492.216, "o", "        \"check_input\": [\"boolean\"],\r\n"]
[492.226, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[492.236, "o", "        \"positive\": [\"boolean\"],\r\n"]
[492.246, "o", "    },\r\n"]
[492.256, "o", "    prefer_skip_nested_validation=True,\r\n"]
[492.266, "o", ")\r\n"]
[492.276, "o", "# XXX : could be moved to the linear_model module\r\n"]
[492.286, "o", "def sparse_encode(\r\n"]
[492.296, "o", "    X,\r\n"]
[492.306, "o", "    dictionary,\r\n"]
[492.316, "o", "    *,\r\n"]
[492.326, "o", "    gram=None,\r\n"]
[492.336, "o", "    cov=None,\r\n"]
[492.346, "o", "    algorithm=\"lasso_lars\",\r\n"]
[492.356, "o", "    n_nonzero_coefs=None,\r\n"]
[492.366, "o", "    alpha=None,\r\n"]
[492.376, "o", "    copy_cov=True,\r\n"]
[492.386, "o", "    init=None,\r\n"]
[492.396, "o", "    max_iter=1000,\r\n"]
[492.406, "o", "    n_jobs=None,\r\n"]
[492.416, "o", "    check_input=True,\r\n"]
[492.426, "o", "    verbose=0,\r\n"]
[492.436, "o", "    positive=False,\r\n"]
[492.446, "o", "):\r\n"]
[492.456, "o", "    \"\"\"Sparse coding.\r\n"]
[492.466, "o", "\r\n"]
[492.476, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[492.486, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[492.496, "o", "\r\n"]
[492.506, "o", "        X ~= code * dictionary\r\n"]
[492.516, "o", "\r\n"]
[492.526, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[492.536, "o", "\r\n"]
[492.546, "o", "    Parameters\r\n"]
[492.556, "o", "    ----------\r\n"]
[492.566, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[492.576, "o", "        Data matrix.\r\n"]
[492.586, "o", "\r\n"]
[492.596, "o", "    dictionary : array-like of shape (n_components, n_features)\r\n"]
[492.606, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[492.616, "o", "        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[492.626, "o", "        output.\r\n"]
[492.636, "o", "\r\n"]
[492.646, "o", "    gram : array-like of shape (n_components, n_components), default=None\r\n"]
[492.656, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n"]
[492.666, "o", "\r\n"]
[492.676, "o", "    cov : array-like of shape (n_components, n_samples), default=None\r\n"]
[492.686, "o", "        Precomputed covariance, `dictionary' * X`.\r\n"]
[492.696, "o", "\r\n"]
[492.706, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[492.716, "o", "            default='lasso_lars'\r\n"]
[492.726, "o", "        The algorithm used:\r\n"]
[492.736, "o", "\r\n"]
[492.746, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[492.756, "o", "          (`linear_model.lars_path`);\r\n"]
[492.766, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[492.776, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[492.786, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[492.796, "o", "          the estimated components are sparse;\r\n"]
[492.806, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[492.816, "o", "          solution;\r\n"]
[492.826, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[492.836, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[492.846, "o", "\r\n"]
[492.856, "o", "    n_nonzero_coefs : int, default=None\r\n"]
[492.866, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[492.876, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[492.886, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[492.896, "o", "        `n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[492.906, "o", "\r\n"]
[492.916, "o", "    alpha : float, default=None\r\n"]
[492.926, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[492.936, "o", "        penalty applied to the L1 norm.\r\n"]
[492.946, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[492.956, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[492.966, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[492.976, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[492.986, "o", "        `n_nonzero_coefs`.\r\n"]
[492.996, "o", "        If `None`, default to 1.\r\n"]
[493.006, "o", "\r\n"]
[493.016, "o", "    copy_cov : bool, default=True\r\n"]
[493.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[493.036, "o", "        be overwritten.\r\n"]
[493.046, "o", "\r\n"]
[493.056, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[493.066, "o", "        Initialization value of the sparse codes. Only used if\r\n"]
[493.076, "o", "        `algorithm='lasso_cd'`.\r\n"]
[493.086, "o", "\r\n"]
[493.096, "o", "    max_iter : int, default=1000\r\n"]
[493.106, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[493.116, "o", "        `'lasso_lars'`.\r\n"]
[493.126, "o", "\r\n"]
[493.136, "o", "    n_jobs : int, default=None\r\n"]
[493.146, "o", "        Number of parallel jobs to run.\r\n"]
[493.156, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[493.166, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[493.176, "o", "        for more details.\r\n"]
[493.186, "o", "\r\n"]
[493.196, "o", "    check_input : bool, default=True\r\n"]
[493.206, "o", "        If `False`, the input arrays X and dictionary will not be checked.\r\n"]
[493.216, "o", "\r\n"]
[493.226, "o", "    verbose : int, default=0\r\n"]
[493.236, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[493.246, "o", "\r\n"]
[493.256, "o", "    positive : bool, default=False\r\n"]
[493.266, "o", "        Whether to enforce positivity when finding the encoding.\r\n"]
[493.276, "o", "\r\n"]
[493.286, "o", "        .. versionadded:: 0.20\r\n"]
[493.296, "o", "\r\n"]
[493.306, "o", "    Returns\r\n"]
[493.316, "o", "    -------\r\n"]
[493.326, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[493.336, "o", "        The sparse codes.\r\n"]
[493.346, "o", "\r\n"]
[493.356, "o", "    See Also\r\n"]
[493.366, "o", "    --------\r\n"]
[493.376, "o", "    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n"]
[493.386, "o", "        path using LARS algorithm.\r\n"]
[493.396, "o", "    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n"]
[493.406, "o", "    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n"]
[493.416, "o", "    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n"]
[493.426, "o", "        dictionary.\r\n"]
[493.436, "o", "    \"\"\"\r\n"]
[493.446, "o", "    if check_input:\r\n"]
[493.456, "o", "        if algorithm == \"lasso_cd\":\r\n"]
[493.466, "o", "            dictionary = check_array(\r\n"]
[493.476, "o", "                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n"]
[493.486, "o", "            )\r\n"]
[493.496, "o", "            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n"]
[493.506, "o", "        else:\r\n"]
[493.516, "o", "            dictionary = check_array(dictionary)\r\n"]
[493.526, "o", "            X = check_array(X)\r\n"]
[493.536, "o", "\r\n"]
[493.546, "o", "    if dictionary.shape[1] != X.shape[1]:\r\n"]
[493.556, "o", "        raise ValueError(\r\n"]
[493.566, "o", "            \"Dictionary and X have different numbers of features:\"\r\n"]
[493.576, "o", "            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n"]
[493.586, "o", "        )\r\n"]
[493.596, "o", "\r\n"]
[493.606, "o", "    _check_positive_coding(algorithm, positive)\r\n"]
[493.616, "o", "\r\n"]
[493.626, "o", "    return _sparse_encode(\r\n"]
[493.636, "o", "        X,\r\n"]
[493.646, "o", "        dictionary,\r\n"]
[493.656, "o", "        gram=gram,\r\n"]
[493.666, "o", "        cov=cov,\r\n"]
[493.676, "o", "        algorithm=algorithm,\r\n"]
[493.686, "o", "        n_nonzero_coefs=n_nonzero_coefs,\r\n"]
[493.696, "o", "        alpha=alpha,\r\n"]
[493.706, "o", "        copy_cov=copy_cov,\r\n"]
[493.716, "o", "        init=init,\r\n"]
[493.726, "o", "        max_iter=max_iter,\r\n"]
[493.736, "o", "        n_jobs=n_jobs,\r\n"]
[493.746, "o", "        verbose=verbose,\r\n"]
[493.756, "o", "        positive=positive,\r\n"]
[493.766, "o", "    )\r\n"]
[493.776, "o", "\r\n"]
[493.786, "o", "\r\n"]
[493.796, "o", "def _sparse_encode(\r\n"]
[493.806, "o", "    X,\r\n"]
[493.816, "o", "    dictionary,\r\n"]
[493.826, "o", "    *,\r\n"]
[493.836, "o", "    gram=None,\r\n"]
[493.846, "o", "    cov=None,\r\n"]
[493.856, "o", "    algorithm=\"lasso_lars\",\r\n"]
[493.866, "o", "    n_nonzero_coefs=None,\r\n"]
[493.876, "o", "    alpha=None,\r\n"]
[493.886, "o", "    copy_cov=True,\r\n"]
[493.896, "o", "    init=None,\r\n"]
[493.906, "o", "    max_iter=1000,\r\n"]
[493.916, "o", "    n_jobs=None,\r\n"]
[493.926, "o", "    verbose=0,\r\n"]
[493.936, "o", "    positive=False,\r\n"]
[493.946, "o", "):\r\n"]
[493.956, "o", "    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[493.966, "o", "\r\n"]
[493.976, "o", "    n_samples, n_features = X.shape\r\n"]
[493.986, "o", "    n_components = dictionary.shape[0]\r\n"]
[493.996, "o", "\r\n"]
[494.006, "o", "    if algorithm in (\"lars\", \"omp\"):\r\n"]
[494.016, "o", "        regularization = n_nonzero_coefs\r\n"]
[494.026, "o", "        if regularization is None:\r\n"]
[494.036, "o", "            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[494.046, "o", "    else:\r\n"]
[494.056, "o", "        regularization = alpha\r\n"]
[494.066, "o", "        if regularization is None:\r\n"]
[494.076, "o", "            regularization = 1.0\r\n"]
[494.086, "o", "\r\n"]
[494.096, "o", "    if gram is None and algorithm != \"threshold\":\r\n"]
[494.106, "o", "        gram = np.dot(dictionary, dictionary.T)\r\n"]
[494.116, "o", "\r\n"]
[494.126, "o", "    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[494.136, "o", "        copy_cov = False\r\n"]
[494.146, "o", "        cov = np.dot(dictionary, X.T)\r\n"]
[494.156, "o", "\r\n"]
[494.166, "o", "    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[494.176, "o", "        code = _sparse_encode_precomputed(\r\n"]
[494.186, "o", "            X,\r\n"]
[494.196, "o", "            dictionary,\r\n"]
[494.206, "o", "            gram=gram,\r\n"]
[494.216, "o", "            cov=cov,\r\n"]
[494.226, "o", "            algorithm=algorithm,\r\n"]
[494.236, "o", "            regularization=regularization,\r\n"]
[494.246, "o", "            copy_cov=copy_cov,\r\n"]
[494.256, "o", "            init=init,\r\n"]
[494.266, "o", "            max_iter=max_iter,\r\n"]
[494.276, "o", "            verbose=verbose,\r\n"]
[494.286, "o", "            positive=positive,\r\n"]
[494.296, "o", "        )\r\n"]
[494.306, "o", "        return code\r\n"]
[494.316, "o", "\r\n"]
[494.326, "o", "    # Enter parallel code block\r\n"]
[494.336, "o", "    n_samples = X.shape[0]\r\n"]
[494.346, "o", "    n_components = dictionary.shape[0]\r\n"]
[494.356, "o", "    code = np.empty((n_samples, n_components))\r\n"]
[494.366, "o", "    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[494.376, "o", "\r\n"]
[494.386, "o", "    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[494.396, "o", "        delayed(_sparse_encode_precomputed)(\r\n"]
[494.406, "o", "            X[this_slice],\r\n"]
[494.416, "o", "            dictionary,\r\n"]
[494.426, "o", "            gram=gram,\r\n"]
[494.436, "o", "            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[494.446, "o", "            algorithm=algorithm,\r\n"]
[494.456, "o", "            regularization=regularization,\r\n"]
[494.466, "o", "            copy_cov=copy_cov,\r\n"]
[494.476, "o", "            init=init[this_slice] if init is not None else None,\r\n"]
[494.486, "o", "            max_iter=max_iter,\r\n"]
[494.496, "o", "            verbose=verbose,\r\n"]
[494.506, "o", "            positive=positive,\r\n"]
[494.516, "o", "        )\r\n"]
[494.526, "o", "        for this_slice in slices\r\n"]
[494.536, "o", "    )\r\n"]
[494.546, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[494.556, "o", "        code[this_slice] = this_view\r\n"]
[494.566, "o", "    return code\r\n"]
[494.576, "o", "\r\n"]
[494.586, "o", "\r\n"]
[494.596, "o", "def _update_dict(\r\n"]
[494.606, "o", "    dictionary,\r\n"]
[494.616, "o", "    Y,\r\n"]
[494.626, "o", "    code,\r\n"]
[494.636, "o", "    A=None,\r\n"]
[494.646, "o", "    B=None,\r\n"]
[494.656, "o", "    verbose=False,\r\n"]
[494.666, "o", "    random_state=None,\r\n"]
[494.676, "o", "    positive=False,\r\n"]
[494.686, "o", "):\r\n"]
[494.696, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[494.706, "o", "\r\n"]
[494.716, "o", "    Parameters\r\n"]
[494.726, "o", "    ----------\r\n"]
[494.736, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[494.746, "o", "        Value of the dictionary at the previous iteration.\r\n"]
[494.756, "o", "\r\n"]
[494.766, "o", "    Y : ndarray of shape (n_samples, n_features)\r\n"]
[494.776, "o", "        Data matrix.\r\n"]
[494.786, "o", "\r\n"]
[494.796, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[494.806, "o", "        Sparse coding of the data against which to optimize the dictionary.\r\n"]
[494.816, "o", "\r\n"]
[494.826, "o", "    A : ndarray of shape (n_components, n_components), default=None\r\n"]
[494.836, "o", "        Together with `B`, sufficient stats of the online model to update the\r\n"]
[494.846, "o", "        dictionary.\r\n"]
[494.856, "o", "\r\n"]
[494.866, "o", "    B : ndarray of shape (n_features, n_components), default=None\r\n"]
[494.876, "o", "        Together with `A`, sufficient stats of the online model to update the\r\n"]
[494.886, "o", "        dictionary.\r\n"]
[494.896, "o", "\r\n"]
[494.906, "o", "    verbose: bool, default=False\r\n"]
[494.916, "o", "        Degree of output the procedure will print.\r\n"]
[494.926, "o", "\r\n"]
[494.936, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[495.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[495.002, "i", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r"]
[495.004, "o", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r\n"]
[495.016, "o", "\u001b[?2004l\r\n"]
[495.026, "o", "           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[495.036, "o", "\r\n"]
[495.046, "o", "    return_n_iter : bool, default=False\r\n"]
[495.056, "o", "        Whether or not to return the number of iterations.\r\n"]
[495.066, "o", "\r\n"]
[495.076, "o", "        .. deprecated:: 1.1\r\n"]
[495.086, "o", "           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n"]
[495.096, "o", "\r\n"]
[495.106, "o", "    positive_dict : bool, default=False\r\n"]
[495.116, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[495.126, "o", "\r\n"]
[495.136, "o", "        .. versionadded:: 0.20\r\n"]
[495.146, "o", "\r\n"]
[495.156, "o", "    positive_code : bool, default=False\r\n"]
[495.166, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[495.176, "o", "\r\n"]
[495.186, "o", "        .. versionadded:: 0.20\r\n"]
[495.196, "o", "\r\n"]
[495.206, "o", "    method_max_iter : int, default=1000\r\n"]
[495.216, "o", "        Maximum number of iterations to perform when solving the lasso problem.\r\n"]
[495.226, "o", "\r\n"]
[495.236, "o", "        .. versionadded:: 0.22\r\n"]
[495.246, "o", "\r\n"]
[495.256, "o", "    tol : float, default=1e-3\r\n"]
[495.266, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[495.276, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[495.286, "o", "\r\n"]
[495.296, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[495.306, "o", "        `tol` to 0.0.\r\n"]
[495.316, "o", "\r\n"]
[495.326, "o", "        .. versionadded:: 1.1\r\n"]
[495.336, "o", "\r\n"]
[495.346, "o", "    max_no_improvement : int, default=10\r\n"]
[495.356, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[495.366, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[495.376, "o", "        `max_iter` is not None.\r\n"]
[495.386, "o", "\r\n"]
[495.396, "o", "        To disable convergence detection based on cost function, set\r\n"]
[495.406, "o", "        `max_no_improvement` to None.\r\n"]
[495.416, "o", "\r\n"]
[495.426, "o", "        .. versionadded:: 1.1\r\n"]
[495.436, "o", "\r\n"]
[495.446, "o", "    Returns\r\n"]
[495.456, "o", "    -------\r\n"]
[495.466, "o", "    code : ndarray of shape (n_samples, n_components),\r\n"]
[495.476, "o", "        The sparse code (only returned if `return_code=True`).\r\n"]
[495.486, "o", "\r\n"]
[495.496, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[495.506, "o", "        The solutions to the dictionary learning problem.\r\n"]
[495.516, "o", "\r\n"]
[495.526, "o", "    n_iter : int\r\n"]
[495.536, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[495.546, "o", "        set to `True`.\r\n"]
[495.556, "o", "\r\n"]
[495.566, "o", "    See Also\r\n"]
[495.576, "o", "    --------\r\n"]
[495.586, "o", "    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n"]
[495.596, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[495.606, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n"]
[495.616, "o", "        learning algorithm.\r\n"]
[495.626, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[495.636, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[495.646, "o", "    \"\"\"\r\n"]
[495.656, "o", "    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n"]
[495.666, "o", "    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n"]
[495.676, "o", "        raise ValueError(\r\n"]
[495.686, "o", "            \"The following arguments are incompatible with 'max_iter': \"\r\n"]
[495.696, "o", "            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n"]
[495.706, "o", "        )\r\n"]
[495.716, "o", "\r\n"]
[495.726, "o", "    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n"]
[495.736, "o", "    return_inner_stats = _check_warn_deprecated(\r\n"]
[495.746, "o", "        return_inner_stats,\r\n"]
[495.756, "o", "        \"return_inner_stats\",\r\n"]
[495.766, "o", "        default=False,\r\n"]
[495.776, "o", "        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n"]
[495.786, "o", "    )\r\n"]
[495.796, "o", "    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n"]
[495.806, "o", "    return_n_iter = _check_warn_deprecated(\r\n"]
[495.816, "o", "        return_n_iter,\r\n"]
[495.826, "o", "        \"return_n_iter\",\r\n"]
[495.836, "o", "        default=False,\r\n"]
[495.846, "o", "        additional_message=(\r\n"]
[495.856, "o", "            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n"]
[495.866, "o", "            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n"]
[495.876, "o", "        ),\r\n"]
[495.886, "o", "    )\r\n"]
[495.896, "o", "\r\n"]
[495.906, "o", "    if max_iter is not None:\r\n"]
[495.916, "o", "        transform_algorithm = \"lasso_\" + method\r\n"]
[495.926, "o", "\r\n"]
[495.936, "o", "        est = MiniBatchDictionaryLearning(\r\n"]
[495.946, "o", "            n_components=n_components,\r\n"]
[495.956, "o", "            alpha=alpha,\r\n"]
[495.966, "o", "            n_iter=n_iter,\r\n"]
[495.976, "o", "            n_jobs=n_jobs,\r\n"]
[495.986, "o", "            fit_algorithm=method,\r\n"]
[495.996, "o", "            batch_size=batch_size,\r\n"]
[496.006, "o", "            shuffle=shuffle,\r\n"]
[496.016, "o", "            dict_init=dict_init,\r\n"]
[496.026, "o", "            random_state=random_state,\r\n"]
[496.036, "o", "            transform_algorithm=transform_algorithm,\r\n"]
[496.046, "o", "            transform_alpha=alpha,\r\n"]
[496.056, "o", "            positive_code=positive_code,\r\n"]
[496.066, "o", "            positive_dict=positive_dict,\r\n"]
[496.076, "o", "            transform_max_iter=method_max_iter,\r\n"]
[496.086, "o", "            verbose=verbose,\r\n"]
[496.096, "o", "            callback=callback,\r\n"]
[496.106, "o", "            tol=tol,\r\n"]
[496.116, "o", "            max_no_improvement=max_no_improvement,\r\n"]
[496.126, "o", "        ).fit(X)\r\n"]
[496.136, "o", "\r\n"]
[496.146, "o", "        if not return_code:\r\n"]
[496.156, "o", "            return est.components_\r\n"]
[496.166, "o", "        else:\r\n"]
[496.176, "o", "            code = est.transform(X)\r\n"]
[496.186, "o", "            return code, est.components_\r\n"]
[496.196, "o", "\r\n"]
[496.206, "o", "    # TODO(1.4) remove the whole old behavior\r\n"]
[496.216, "o", "    # Fallback to old behavior\r\n"]
[496.226, "o", "\r\n"]
[496.236, "o", "    n_iter = _check_warn_deprecated(\r\n"]
[496.246, "o", "        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n"]
[496.256, "o", "    )\r\n"]
[496.266, "o", "\r\n"]
[496.276, "o", "    if n_components is None:\r\n"]
[496.286, "o", "        n_components = X.shape[1]\r\n"]
[496.296, "o", "\r\n"]
[496.306, "o", "    if method not in (\"lars\", \"cd\"):\r\n"]
[496.316, "o", "        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n"]
[496.326, "o", "\r\n"]
[496.336, "o", "    _check_positive_coding(method, positive_code)\r\n"]
[496.346, "o", "\r\n"]
[496.356, "o", "    method = \"lasso_\" + method\r\n"]
[496.366, "o", "\r\n"]
[496.376, "o", "    t0 = time.time()\r\n"]
[496.386, "o", "    n_samples, n_features = X.shape\r\n"]
[496.396, "o", "    # Avoid integer division problems\r\n"]
[496.406, "o", "    alpha = float(alpha)\r\n"]
[496.416, "o", "    random_state = check_random_state(random_state)\r\n"]
[496.426, "o", "\r\n"]
[496.436, "o", "    # Init V with SVD of X\r\n"]
[496.446, "o", "    if dict_init is not None:\r\n"]
[496.456, "o", "        dictionary = dict_init\r\n"]
[496.466, "o", "    else:\r\n"]
[496.476, "o", "        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n"]
[496.486, "o", "        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[496.496, "o", "    r = len(dictionary)\r\n"]
[496.506, "o", "    if n_components <= r:\r\n"]
[496.516, "o", "        dictionary = dictionary[:n_components, :]\r\n"]
[496.526, "o", "    else:\r\n"]
[496.536, "o", "        dictionary = np.r_[\r\n"]
[496.546, "o", "            dictionary,\r\n"]
[496.556, "o", "            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n"]
[496.566, "o", "        ]\r\n"]
[496.576, "o", "\r\n"]
[496.586, "o", "    if verbose == 1:\r\n"]
[496.596, "o", "        print(\"[dict_learning]\", end=\" \")\r\n"]
[496.606, "o", "\r\n"]
[496.616, "o", "    if shuffle:\r\n"]
[496.626, "o", "        X_train = X.copy()\r\n"]
[496.636, "o", "        random_state.shuffle(X_train)\r\n"]
[496.646, "o", "    else:\r\n"]
[496.656, "o", "        X_train = X\r\n"]
[496.666, "o", "\r\n"]
[496.676, "o", "    X_train = check_array(\r\n"]
[496.686, "o", "        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n"]
[496.696, "o", "    )\r\n"]
[496.706, "o", "\r\n"]
[496.716, "o", "    # Fortran-order dict better suited for the sparse coding which is the\r\n"]
[496.726, "o", "    # bottleneck of this algorithm.\r\n"]
[496.736, "o", "    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n"]
[496.746, "o", "    dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[496.756, "o", "\r\n"]
[496.766, "o", "    batches = gen_batches(n_samples, batch_size)\r\n"]
[496.776, "o", "    batches = itertools.cycle(batches)\r\n"]
[496.786, "o", "\r\n"]
[496.796, "o", "    # The covariance of the dictionary\r\n"]
[496.806, "o", "    if inner_stats is None:\r\n"]
[496.816, "o", "        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n"]
[496.826, "o", "        # The data approximation\r\n"]
[496.836, "o", "        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n"]
[496.846, "o", "    else:\r\n"]
[496.856, "o", "        A = inner_stats[0].copy()\r\n"]
[496.866, "o", "        B = inner_stats[1].copy()\r\n"]
[496.876, "o", "\r\n"]
[496.886, "o", "    # If n_iter is zero, we need to return zero.\r\n"]
[496.896, "o", "    ii = iter_offset - 1\r\n"]
[496.906, "o", "\r\n"]
[496.916, "o", "    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n"]
[496.926, "o", "        this_X = X_train[batch]\r\n"]
[496.936, "o", "        dt = time.time() - t0\r\n"]
[496.946, "o", "        if verbose == 1:\r\n"]
[496.956, "o", "            sys.stdout.write(\".\")\r\n"]
[496.966, "o", "            sys.stdout.flush()\r\n"]
[496.976, "o", "        elif verbose:\r\n"]
[496.986, "o", "            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n"]
[496.996, "o", "                print(\r\n"]
[497.006, "o", "                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n"]
[497.016, "o", "                )\r\n"]
[497.026, "o", "\r\n"]
[497.036, "o", "        this_code = sparse_encode(\r\n"]
[497.046, "o", "            this_X,\r\n"]
[497.056, "o", "            dictionary,\r\n"]
[497.066, "o", "            algorithm=method,\r\n"]
[497.076, "o", "            alpha=alpha,\r\n"]
[497.086, "o", "            n_jobs=n_jobs,\r\n"]
[497.096, "o", "            check_input=False,\r\n"]
[497.106, "o", "            positive=positive_code,\r\n"]
[497.116, "o", "            max_iter=method_max_iter,\r\n"]
[497.126, "o", "            verbose=verbose,\r\n"]
[497.136, "o", "        )\r\n"]
[497.146, "o", "\r\n"]
[497.156, "o", "        # Update the auxiliary variables\r\n"]
[497.166, "o", "        if ii < batch_size - 1:\r\n"]
[497.176, "o", "            theta = float((ii + 1) * batch_size)\r\n"]
[497.186, "o", "        else:\r\n"]
[497.196, "o", "            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n"]
[497.206, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[497.216, "o", "\r\n"]
[497.226, "o", "        A *= beta\r\n"]
[497.236, "o", "        A += np.dot(this_code.T, this_code)\r\n"]
[497.246, "o", "        B *= beta\r\n"]
[497.256, "o", "        B += np.dot(this_X.T, this_code)\r\n"]
[497.266, "o", "\r\n"]
[497.276, "o", "        # Update dictionary in place\r\n"]
[497.286, "o", "        _update_dict(\r\n"]
[497.296, "o", "            dictionary,\r\n"]
[497.306, "o", "            this_X,\r\n"]
[497.316, "o", "            this_code,\r\n"]
[497.326, "o", "            A,\r\n"]
[497.336, "o", "            B,\r\n"]
[497.346, "o", "            verbose=verbose,\r\n"]
[497.356, "o", "            random_state=random_state,\r\n"]
[497.366, "o", "            positive=positive_dict,\r\n"]
[497.376, "o", "        )\r\n"]
[497.386, "o", "\r\n"]
[497.396, "o", "        # Maybe we need a stopping criteria based on the amount of\r\n"]
[497.406, "o", "        # modification in the dictionary\r\n"]
[497.416, "o", "        if callback is not None:\r\n"]
[497.426, "o", "            callback(locals())\r\n"]
[497.436, "o", "\r\n"]
[497.446, "o", "    if return_inner_stats:\r\n"]
[497.456, "o", "        if return_n_iter:\r\n"]
[497.466, "o", "            return dictionary, (A, B), ii - iter_offset + 1\r\n"]
[497.476, "o", "        else:\r\n"]
[497.486, "o", "            return dictionary, (A, B)\r\n"]
[497.496, "o", "    if return_code:\r\n"]
[497.506, "o", "        if verbose > 1:\r\n"]
[497.516, "o", "            print(\"Learning code...\", end=\" \")\r\n"]
[497.526, "o", "        elif verbose == 1:\r\n"]
[497.536, "o", "            print(\"|\", end=\" \")\r\n"]
[497.546, "o", "        code = sparse_encode(\r\n"]
[497.556, "o", "            X,\r\n"]
[497.566, "o", "            dictionary,\r\n"]
[497.576, "o", "            algorithm=method,\r\n"]
[497.586, "o", "            alpha=alpha,\r\n"]
[497.596, "o", "            n_jobs=n_jobs,\r\n"]
[497.606, "o", "            check_input=False,\r\n"]
[497.616, "o", "            positive=positive_code,\r\n"]
[497.626, "o", "            max_iter=method_max_iter,\r\n"]
[497.636, "o", "            verbose=verbose,\r\n"]
[497.646, "o", "        )\r\n"]
[497.656, "o", "        if verbose > 1:\r\n"]
[497.666, "o", "            dt = time.time() - t0\r\n"]
[497.676, "o", "            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n"]
[497.686, "o", "        if return_n_iter:\r\n"]
[497.696, "o", "            return code, dictionary, ii - iter_offset + 1\r\n"]
[497.706, "o", "        else:\r\n"]
[497.716, "o", "            return code, dictionary\r\n"]
[497.726, "o", "\r\n"]
[497.736, "o", "    if return_n_iter:\r\n"]
[497.746, "o", "        return dictionary, ii - iter_offset + 1\r\n"]
[497.756, "o", "    else:\r\n"]
[497.766, "o", "        return dictionary\r\n"]
[497.776, "o", "\r\n"]
[497.786, "o", "\r\n"]
[497.796, "o", "@validate_params(\r\n"]
[497.806, "o", "    {\r\n"]
[497.816, "o", "        \"X\": [\"array-like\"],\r\n"]
[497.826, "o", "        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[497.836, "o", "        \"return_n_iter\": [\"boolean\"],\r\n"]
[497.846, "o", "        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[497.856, "o", "    },\r\n"]
[497.866, "o", "    prefer_skip_nested_validation=False,\r\n"]
[497.876, "o", ")\r\n"]
[497.886, "o", "def dict_learning(\r\n"]
[497.896, "o", "    X,\r\n"]
[497.906, "o", "    n_components,\r\n"]
[497.916, "o", "    *,\r\n"]
[497.926, "o", "    alpha,\r\n"]
[497.936, "o", "    max_iter=100,\r\n"]
[497.946, "o", "    tol=1e-8,\r\n"]
[497.956, "o", "    method=\"lars\",\r\n"]
[497.966, "o", "    n_jobs=None,\r\n"]
[497.976, "o", "    dict_init=None,\r\n"]
[497.986, "o", "    code_init=None,\r\n"]
[497.996, "o", "    callback=None,\r\n"]
[498.006, "o", "    verbose=False,\r\n"]
[498.016, "o", "    random_state=None,\r\n"]
[498.026, "o", "    return_n_iter=False,\r\n"]
[498.036, "o", "    positive_dict=False,\r\n"]
[498.046, "o", "    positive_code=False,\r\n"]
[498.056, "o", "    method_max_iter=1000,\r\n"]
[498.066, "o", "):\r\n"]
[498.076, "o", "    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n"]
[498.086, "o", "\r\n"]
[498.096, "o", "    Finds the best dictionary and the corresponding sparse code for\r\n"]
[498.106, "o", "    approximating the data matrix X by solving::\r\n"]
[498.116, "o", "\r\n"]
[498.126, "o", "        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[498.136, "o", "                     (U,V)\r\n"]
[498.146, "o", "                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n"]
[498.156, "o", "\r\n"]
[498.166, "o", "    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n"]
[498.176, "o", "    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n"]
[498.186, "o", "    which is the sum of the absolute values of all the entries in the matrix.\r\n"]
[498.196, "o", "\r\n"]
[498.206, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[498.216, "o", "\r\n"]
[498.226, "o", "    Parameters\r\n"]
[498.236, "o", "    ----------\r\n"]
[498.246, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[498.256, "o", "        Data matrix.\r\n"]
[498.266, "o", "\r\n"]
[498.276, "o", "    n_components : int\r\n"]
[498.286, "o", "        Number of dictionary atoms to extract.\r\n"]
[498.296, "o", "\r\n"]
[498.306, "o", "    alpha : int or float\r\n"]
[498.316, "o", "        Sparsity controlling parameter.\r\n"]
[498.326, "o", "\r\n"]
[498.336, "o", "    max_iter : int, default=100\r\n"]
[498.346, "o", "        Maximum number of iterations to perform.\r\n"]
[498.356, "o", "\r\n"]
[498.366, "o", "    tol : float, default=1e-8\r\n"]
[498.376, "o", "        Tolerance for the stopping condition.\r\n"]
[498.386, "o", "\r\n"]
[498.396, "o", "    method : {'lars', 'cd'}, default='lars'\r\n"]
[498.406, "o", "        The method used:\r\n"]
[498.416, "o", "\r\n"]
[498.426, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[498.436, "o", "           problem (`linear_model.lars_path`);\r\n"]
[498.446, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[498.456, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[498.466, "o", "          the estimated components are sparse.\r\n"]
[498.476, "o", "\r\n"]
[498.486, "o", "    n_jobs : int, default=None\r\n"]
[498.496, "o", "        Number of parallel jobs to run.\r\n"]
[498.506, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[498.516, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[498.526, "o", "        for more details.\r\n"]
[498.536, "o", "\r\n"]
[498.546, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[498.556, "o", "        Initial value for the dictionary for warm restart scenarios. Only used\r\n"]
[498.566, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[498.576, "o", "\r\n"]
[498.586, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[498.596, "o", "        Initial value for the sparse code for warm restart scenarios. Only used\r\n"]
[498.606, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[498.616, "o", "\r\n"]
[498.626, "o", "    callback : callable, default=None\r\n"]
[498.636, "o", "        Callable that gets invoked every five iterations.\r\n"]
[498.646, "o", "\r\n"]
[498.656, "o", "    verbose : bool, default=False\r\n"]
[498.666, "o", "        To control the verbosity of the procedure.\r\n"]
[498.676, "o", "\r\n"]
[498.686, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[498.696, "o", "        Used for randomly initializing the dictionary. Pass an int for\r\n"]
[498.706, "o", "        reproducible results across multiple function calls.\r\n"]
[498.716, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[498.726, "o", "\r\n"]
[498.736, "o", "    return_n_iter : bool, default=False\r\n"]
[498.746, "o", "        Whether or not to return the number of iterations.\r\n"]
[498.756, "o", "\r\n"]
[498.766, "o", "    positive_dict : bool, default=False\r\n"]
[498.776, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[498.786, "o", "\r\n"]
[498.796, "o", "        .. versionadded:: 0.20\r\n"]
[498.806, "o", "\r\n"]
[498.816, "o", "    positive_code : bool, default=False\r\n"]
[498.826, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[498.836, "o", "\r\n"]
[498.846, "o", "        .. versionadded:: 0.20\r\n"]
[498.856, "o", "\r\n"]
[498.866, "o", "    method_max_iter : int, default=1000\r\n"]
[498.876, "o", "        Maximum number of iterations to perform.\r\n"]
[498.886, "o", "\r\n"]
[498.896, "o", "        .. versionadded:: 0.22\r\n"]
[498.906, "o", "\r\n"]
[498.916, "o", "    Returns\r\n"]
[498.926, "o", "    -------\r\n"]
[498.936, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[498.946, "o", "        The sparse code factor in the matrix factorization.\r\n"]
[498.956, "o", "\r\n"]
[498.966, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[498.976, "o", "        The dictionary factor in the matrix factorization.\r\n"]
[498.986, "o", "\r\n"]
[498.996, "o", "    errors : array\r\n"]
[499.006, "o", "        Vector of errors at each iteration.\r\n"]
[499.016, "o", "\r\n"]
[499.026, "o", "    n_iter : int\r\n"]
[499.036, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[499.046, "o", "        set to True.\r\n"]
[499.056, "o", "\r\n"]
[499.066, "o", "    See Also\r\n"]
[499.076, "o", "    --------\r\n"]
[499.086, "o", "    dict_learning_online : Solve a dictionary learning matrix factorization\r\n"]
[499.096, "o", "        problem online.\r\n"]
[499.106, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[499.116, "o", "    MiniBatchDictionaryLearning : A faster, less accurate version\r\n"]
[499.126, "o", "        of the dictionary learning algorithm.\r\n"]
[499.136, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[499.146, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[499.156, "o", "    \"\"\"\r\n"]
[499.166, "o", "    estimator = DictionaryLearning(\r\n"]
[499.176, "o", "        n_components=n_components,\r\n"]
[499.186, "o", "        alpha=alpha,\r\n"]
[499.196, "o", "        max_iter=max_iter,\r\n"]
[499.206, "o", "        tol=tol,\r\n"]
[499.216, "o", "        fit_algorithm=method,\r\n"]
[499.226, "o", "        n_jobs=n_jobs,\r\n"]
[499.236, "o", "        dict_init=dict_init,\r\n"]
[499.246, "o", "        callback=callback,\r\n"]
[499.256, "o", "        code_init=code_init,\r\n"]
[499.266, "o", "        verbose=verbose,\r\n"]
[499.276, "o", "        random_state=random_state,\r\n"]
[499.286, "o", "        positive_code=positive_code,\r\n"]
[499.296, "o", "        positive_dict=positive_dict,\r\n"]
[499.306, "o", "        transform_max_iter=method_max_iter,\r\n"]
[499.316, "o", "    )\r\n"]
[499.326, "o", "    code = estimator.fit_transform(X)\r\n"]
[499.336, "o", "    if return_n_iter:\r\n"]
[499.346, "o", "        return (\r\n"]
[499.356, "o", "            code,\r\n"]
[499.366, "o", "            estimator.components_,\r\n"]
[499.376, "o", "            estimator.error_,\r\n"]
[499.386, "o", "            estimator.n_iter_,\r\n"]
[499.396, "o", "        )\r\n"]
[499.406, "o", "    return code, estimator.components_, estimator.error_\r\n"]
[499.416, "o", "\r\n"]
[499.426, "o", "\r\n"]
[499.436, "o", "class _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n"]
[499.446, "o", "    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n"]
[499.456, "o", "\r\n"]
[499.466, "o", "    def __init__(\r\n"]
[499.476, "o", "        self,\r\n"]
[499.486, "o", "        transform_algorithm,\r\n"]
[499.496, "o", "        transform_n_nonzero_coefs,\r\n"]
[499.506, "o", "        transform_alpha,\r\n"]
[499.516, "o", "        split_sign,\r\n"]
[499.526, "o", "        n_jobs,\r\n"]
[499.536, "o", "        positive_code,\r\n"]
[499.546, "o", "        transform_max_iter,\r\n"]
[499.556, "o", "    ):\r\n"]
[499.566, "o", "        self.transform_algorithm = transform_algorithm\r\n"]
[499.576, "o", "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n"]
[499.586, "o", "        self.transform_alpha = transform_alpha\r\n"]
[499.596, "o", "        self.transform_max_iter = transform_max_iter\r\n"]
[499.606, "o", "        self.split_sign = split_sign\r\n"]
[499.616, "o", "        self.n_jobs = n_jobs\r\n"]
[499.626, "o", "        self.positive_code = positive_code\r\n"]
[499.636, "o", "\r\n"]
[499.646, "o", "    def _transform(self, X, dictionary):\r\n"]
[499.656, "o", "        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n"]
[499.666, "o", "        SparseCoder.\"\"\"\r\n"]
[499.676, "o", "        X = self._validate_data(X, reset=False)\r\n"]
[499.686, "o", "\r\n"]
[499.696, "o", "        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n"]
[499.706, "o", "            transform_alpha = self.alpha\r\n"]
[499.716, "o", "        else:\r\n"]
[499.726, "o", "            transform_alpha = self.transform_alpha\r\n"]
[499.736, "o", "\r\n"]
[499.746, "o", "        code = sparse_encode(\r\n"]
[499.756, "o", "            X,\r\n"]
[499.766, "o", "            dictionary,\r\n"]
[499.776, "o", "            algorithm=self.transform_algorithm,\r\n"]
[499.786, "o", "            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n"]
[499.796, "o", "            alpha=transform_alpha,\r\n"]
[499.806, "o", "            max_iter=self.transform_max_iter,\r\n"]
[499.816, "o", "            n_jobs=self.n_jobs,\r\n"]
[499.826, "o", "            positive=self.positive_code,\r\n"]
[499.836, "o", "        )\r\n"]
[499.846, "o", "\r\n"]
[499.856, "o", "        if self.split_sign:\r\n"]
[499.866, "o", "            # feature vector is split into a positive and negative side\r\n"]
[499.876, "o", "            n_samples, n_features = code.shape\r\n"]
[499.886, "o", "            split_code = np.empty((n_samples, 2 * n_features))\r\n"]
[499.896, "o", "            split_code[:, :n_features] = np.maximum(code, 0)\r\n"]
[499.906, "o", "            split_code[:, n_features:] = -np.minimum(code, 0)\r\n"]
[499.916, "o", "            code = split_code\r\n"]
[499.926, "o", "\r\n"]
[499.936, "o", "        return code\r\n"]
[500.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[500.002, "i", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py\r"]
[500.004, "o", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py\r\n"]
[500.016, "o", "\u001b[?2004l\r\n"]
[500.026, "o", "\r\n"]
[500.036, "o", "    n_jobs : int or None, default=None\r\n"]
[500.046, "o", "        Number of parallel jobs to run.\r\n"]
[500.056, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[500.066, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[500.076, "o", "        for more details.\r\n"]
[500.086, "o", "\r\n"]
[500.096, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[500.106, "o", "        Initial value for the code, for warm restart. Only used if `code_init`\r\n"]
[500.116, "o", "        and `dict_init` are not None.\r\n"]
[500.126, "o", "\r\n"]
[500.136, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[500.146, "o", "        Initial values for the dictionary, for warm restart. Only used if\r\n"]
[500.156, "o", "        `code_init` and `dict_init` are not None.\r\n"]
[500.166, "o", "\r\n"]
[500.176, "o", "    callback : callable, default=None\r\n"]
[500.186, "o", "        Callable that gets invoked every five iterations.\r\n"]
[500.196, "o", "\r\n"]
[500.206, "o", "        .. versionadded:: 1.3\r\n"]
[500.216, "o", "\r\n"]
[500.226, "o", "    verbose : bool, default=False\r\n"]
[500.236, "o", "        To control the verbosity of the procedure.\r\n"]
[500.246, "o", "\r\n"]
[500.256, "o", "    split_sign : bool, default=False\r\n"]
[500.266, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[500.276, "o", "        its negative part and its positive part. This can improve the\r\n"]
[500.286, "o", "        performance of downstream classifiers.\r\n"]
[500.296, "o", "\r\n"]
[500.306, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[500.316, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[500.326, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[500.336, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[500.346, "o", "        results across multiple function calls.\r\n"]
[500.356, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[500.366, "o", "\r\n"]
[500.376, "o", "    positive_code : bool, default=False\r\n"]
[500.386, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[500.396, "o", "\r\n"]
[500.406, "o", "        .. versionadded:: 0.20\r\n"]
[500.416, "o", "\r\n"]
[500.426, "o", "    positive_dict : bool, default=False\r\n"]
[500.436, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[500.446, "o", "\r\n"]
[500.456, "o", "        .. versionadded:: 0.20\r\n"]
[500.466, "o", "\r\n"]
[500.476, "o", "    transform_max_iter : int, default=1000\r\n"]
[500.486, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[500.496, "o", "        `'lasso_lars'`.\r\n"]
[500.506, "o", "\r\n"]
[500.516, "o", "        .. versionadded:: 0.22\r\n"]
[500.526, "o", "\r\n"]
[500.536, "o", "    Attributes\r\n"]
[500.546, "o", "    ----------\r\n"]
[500.556, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[500.566, "o", "        dictionary atoms extracted from the data\r\n"]
[500.576, "o", "\r\n"]
[500.586, "o", "    error_ : array\r\n"]
[500.596, "o", "        vector of errors at each iteration\r\n"]
[500.606, "o", "\r\n"]
[500.616, "o", "    n_features_in_ : int\r\n"]
[500.626, "o", "        Number of features seen during :term:`fit`.\r\n"]
[500.636, "o", "\r\n"]
[500.646, "o", "        .. versionadded:: 0.24\r\n"]
[500.656, "o", "\r\n"]
[500.666, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[500.676, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[500.686, "o", "        has feature names that are all strings.\r\n"]
[500.696, "o", "\r\n"]
[500.706, "o", "        .. versionadded:: 1.0\r\n"]
[500.716, "o", "\r\n"]
[500.726, "o", "    n_iter_ : int\r\n"]
[500.736, "o", "        Number of iterations run.\r\n"]
[500.746, "o", "\r\n"]
[500.756, "o", "    See Also\r\n"]
[500.766, "o", "    --------\r\n"]
[500.776, "o", "    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n"]
[500.786, "o", "        dictionary learning algorithm.\r\n"]
[500.796, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[500.806, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[500.816, "o", "        precomputed dictionary.\r\n"]
[500.826, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[500.836, "o", "\r\n"]
[500.846, "o", "    References\r\n"]
[500.856, "o", "    ----------\r\n"]
[500.866, "o", "\r\n"]
[500.876, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[500.886, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[500.896, "o", "\r\n"]
[500.906, "o", "    Examples\r\n"]
[500.916, "o", "    --------\r\n"]
[500.926, "o", "    >>> import numpy as np\r\n"]
[500.936, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[500.946, "o", "    >>> from sklearn.decomposition import DictionaryLearning\r\n"]
[500.956, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[500.966, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[500.976, "o", "    ...     random_state=42,\r\n"]
[500.986, "o", "    ... )\r\n"]
[500.996, "o", "    >>> dict_learner = DictionaryLearning(\r\n"]
[501.006, "o", "    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n"]
[501.016, "o", "    ...     random_state=42,\r\n"]
[501.026, "o", "    ... )\r\n"]
[501.036, "o", "    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n"]
[501.046, "o", "\r\n"]
[501.056, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[501.066, "o", "\r\n"]
[501.076, "o", "    >>> np.mean(X_transformed == 0)\r\n"]
[501.086, "o", "    0.41...\r\n"]
[501.096, "o", "\r\n"]
[501.106, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[501.116, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[501.126, "o", "    the original signal:\r\n"]
[501.136, "o", "\r\n"]
[501.146, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[501.156, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[501.166, "o", "    0.07...\r\n"]
[501.176, "o", "    \"\"\"\r\n"]
[501.186, "o", "\r\n"]
[501.196, "o", "    _parameter_constraints: dict = {\r\n"]
[501.206, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[501.216, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[501.226, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[501.236, "o", "        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[501.246, "o", "        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[501.256, "o", "        \"transform_algorithm\": [\r\n"]
[501.266, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[501.276, "o", "        ],\r\n"]
[501.286, "o", "        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[501.296, "o", "        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[501.306, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[501.316, "o", "        \"code_init\": [np.ndarray, None],\r\n"]
[501.326, "o", "        \"dict_init\": [np.ndarray, None],\r\n"]
[501.336, "o", "        \"callback\": [callable, None],\r\n"]
[501.346, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[501.356, "o", "        \"split_sign\": [\"boolean\"],\r\n"]
[501.366, "o", "        \"random_state\": [\"random_state\"],\r\n"]
[501.376, "o", "        \"positive_code\": [\"boolean\"],\r\n"]
[501.386, "o", "        \"positive_dict\": [\"boolean\"],\r\n"]
[501.396, "o", "        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[501.406, "o", "    }\r\n"]
[501.416, "o", "\r\n"]
[501.426, "o", "    def __init__(\r\n"]
[501.436, "o", "        self,\r\n"]
[501.446, "o", "        n_components=None,\r\n"]
[501.456, "o", "        *,\r\n"]
[501.466, "o", "        alpha=1,\r\n"]
[501.476, "o", "        max_iter=1000,\r\n"]
[501.486, "o", "        tol=1e-8,\r\n"]
[501.496, "o", "        fit_algorithm=\"lars\",\r\n"]
[501.506, "o", "        transform_algorithm=\"omp\",\r\n"]
[501.516, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[501.526, "o", "        transform_alpha=None,\r\n"]
[501.536, "o", "        n_jobs=None,\r\n"]
[501.546, "o", "        code_init=None,\r\n"]
[501.556, "o", "        dict_init=None,\r\n"]
[501.566, "o", "        callback=None,\r\n"]
[501.576, "o", "        verbose=False,\r\n"]
[501.586, "o", "        split_sign=False,\r\n"]
[501.596, "o", "        random_state=None,\r\n"]
[501.606, "o", "        positive_code=False,\r\n"]
[501.616, "o", "        positive_dict=False,\r\n"]
[501.626, "o", "        transform_max_iter=1000,\r\n"]
[501.636, "o", "    ):\r\n"]
[501.646, "o", "        super().__init__(\r\n"]
[501.656, "o", "            transform_algorithm,\r\n"]
[501.666, "o", "            transform_n_nonzero_coefs,\r\n"]
[501.676, "o", "            transform_alpha,\r\n"]
[501.686, "o", "            split_sign,\r\n"]
[501.696, "o", "            n_jobs,\r\n"]
[501.706, "o", "            positive_code,\r\n"]
[501.716, "o", "            transform_max_iter,\r\n"]
[501.726, "o", "        )\r\n"]
[501.736, "o", "        self.n_components = n_components\r\n"]
[501.746, "o", "        self.alpha = alpha\r\n"]
[501.756, "o", "        self.max_iter = max_iter\r\n"]
[501.766, "o", "        self.tol = tol\r\n"]
[501.776, "o", "        self.fit_algorithm = fit_algorithm\r\n"]
[501.786, "o", "        self.code_init = code_init\r\n"]
[501.796, "o", "        self.dict_init = dict_init\r\n"]
[501.806, "o", "        self.callback = callback\r\n"]
[501.816, "o", "        self.verbose = verbose\r\n"]
[501.826, "o", "        self.random_state = random_state\r\n"]
[501.836, "o", "        self.positive_dict = positive_dict\r\n"]
[501.846, "o", "\r\n"]
[501.856, "o", "    def fit(self, X, y=None):\r\n"]
[501.866, "o", "        \"\"\"Fit the model from data in X.\r\n"]
[501.876, "o", "\r\n"]
[501.886, "o", "        Parameters\r\n"]
[501.896, "o", "        ----------\r\n"]
[501.906, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[501.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[501.926, "o", "            and `n_features` is the number of features.\r\n"]
[501.936, "o", "\r\n"]
[501.946, "o", "        y : Ignored\r\n"]
[501.956, "o", "            Not used, present for API consistency by convention.\r\n"]
[501.966, "o", "\r\n"]
[501.976, "o", "        Returns\r\n"]
[501.986, "o", "        -------\r\n"]
[501.996, "o", "        self : object\r\n"]
[502.006, "o", "            Returns the instance itself.\r\n"]
[502.016, "o", "        \"\"\"\r\n"]
[502.026, "o", "        self.fit_transform(X)\r\n"]
[502.036, "o", "        return self\r\n"]
[502.046, "o", "\r\n"]
[502.056, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[502.066, "o", "    def fit_transform(self, X, y=None):\r\n"]
[502.076, "o", "        \"\"\"Fit the model from data in X and return the transformed data.\r\n"]
[502.086, "o", "\r\n"]
[502.096, "o", "        Parameters\r\n"]
[502.106, "o", "        ----------\r\n"]
[502.116, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[502.126, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[502.136, "o", "            and `n_features` is the number of features.\r\n"]
[502.146, "o", "\r\n"]
[502.156, "o", "        y : Ignored\r\n"]
[502.166, "o", "            Not used, present for API consistency by convention.\r\n"]
[502.176, "o", "\r\n"]
[502.186, "o", "        Returns\r\n"]
[502.196, "o", "        -------\r\n"]
[502.206, "o", "        V : ndarray of shape (n_samples, n_components)\r\n"]
[502.216, "o", "            Transformed data.\r\n"]
[502.226, "o", "        \"\"\"\r\n"]
[502.236, "o", "        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n"]
[502.246, "o", "\r\n"]
[502.256, "o", "        method = \"lasso_\" + self.fit_algorithm\r\n"]
[502.266, "o", "\r\n"]
[502.276, "o", "        random_state = check_random_state(self.random_state)\r\n"]
[502.286, "o", "        X = self._validate_data(X)\r\n"]
[502.296, "o", "\r\n"]
[502.306, "o", "        if self.n_components is None:\r\n"]
[502.316, "o", "            n_components = X.shape[1]\r\n"]
[502.326, "o", "        else:\r\n"]
[502.336, "o", "            n_components = self.n_components\r\n"]
[502.346, "o", "\r\n"]
[502.356, "o", "        V, U, E, self.n_iter_ = _dict_learning(\r\n"]
[502.366, "o", "            X,\r\n"]
[502.376, "o", "            n_components,\r\n"]
[502.386, "o", "            alpha=self.alpha,\r\n"]
[502.396, "o", "            tol=self.tol,\r\n"]
[502.406, "o", "            max_iter=self.max_iter,\r\n"]
[502.416, "o", "            method=method,\r\n"]
[502.426, "o", "            method_max_iter=self.transform_max_iter,\r\n"]
[502.436, "o", "            n_jobs=self.n_jobs,\r\n"]
[502.446, "o", "            code_init=self.code_init,\r\n"]
[502.456, "o", "            dict_init=self.dict_init,\r\n"]
[502.466, "o", "            callback=self.callback,\r\n"]
[502.476, "o", "            verbose=self.verbose,\r\n"]
[502.486, "o", "            random_state=random_state,\r\n"]
[502.496, "o", "            return_n_iter=True,\r\n"]
[502.506, "o", "            positive_dict=self.positive_dict,\r\n"]
[502.516, "o", "            positive_code=self.positive_code,\r\n"]
[502.526, "o", "        )\r\n"]
[502.536, "o", "        self.components_ = U\r\n"]
[502.546, "o", "        self.error_ = E\r\n"]
[502.556, "o", "\r\n"]
[502.566, "o", "        return V\r\n"]
[502.576, "o", "\r\n"]
[502.586, "o", "    @property\r\n"]
[502.596, "o", "    def _n_features_out(self):\r\n"]
[502.606, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[502.616, "o", "        return self.components_.shape[0]\r\n"]
[502.626, "o", "\r\n"]
[502.636, "o", "    def _more_tags(self):\r\n"]
[502.646, "o", "        return {\r\n"]
[502.656, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[502.666, "o", "        }\r\n"]
[502.676, "o", "\r\n"]
[502.686, "o", "\r\n"]
[502.696, "o", "class MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[502.706, "o", "    \"\"\"Mini-batch dictionary learning.\r\n"]
[502.716, "o", "\r\n"]
[502.726, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[502.736, "o", "    encoding the fitted data.\r\n"]
[502.746, "o", "\r\n"]
[502.756, "o", "    Solves the optimization problem::\r\n"]
[502.766, "o", "\r\n"]
[502.776, "o", "       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[502.786, "o", "                    (U,V)\r\n"]
[502.796, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[502.806, "o", "\r\n"]
[502.816, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[502.826, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[502.836, "o", "    of all the entries in the matrix.\r\n"]
[502.846, "o", "\r\n"]
[502.856, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[502.866, "o", "\r\n"]
[502.876, "o", "    Parameters\r\n"]
[502.886, "o", "    ----------\r\n"]
[502.896, "o", "    n_components : int, default=None\r\n"]
[502.906, "o", "        Number of dictionary elements to extract.\r\n"]
[502.916, "o", "\r\n"]
[502.926, "o", "    alpha : float, default=1\r\n"]
[502.936, "o", "        Sparsity controlling parameter.\r\n"]
[502.946, "o", "\r\n"]
[502.956, "o", "    n_iter : int, default=1000\r\n"]
[502.966, "o", "        Total number of iterations over data batches to perform.\r\n"]
[502.976, "o", "\r\n"]
[502.986, "o", "        .. deprecated:: 1.1\r\n"]
[502.996, "o", "           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[503.006, "o", "           ``max_iter`` instead.\r\n"]
[503.016, "o", "\r\n"]
[503.026, "o", "    max_iter : int, default=None\r\n"]
[503.036, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[503.046, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[503.056, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[503.066, "o", "\r\n"]
[503.076, "o", "        .. versionadded:: 1.1\r\n"]
[503.086, "o", "\r\n"]
[503.096, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[503.106, "o", "        The algorithm used:\r\n"]
[503.116, "o", "\r\n"]
[503.126, "o", "        - `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[503.136, "o", "          problem (`linear_model.lars_path`)\r\n"]
[503.146, "o", "        - `'cd'`: uses the coordinate descent method to compute the\r\n"]
[503.156, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[503.166, "o", "          the estimated components are sparse.\r\n"]
[503.176, "o", "\r\n"]
[503.186, "o", "    n_jobs : int, default=None\r\n"]
[503.196, "o", "        Number of parallel jobs to run.\r\n"]
[503.206, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[503.216, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[503.226, "o", "        for more details.\r\n"]
[503.236, "o", "\r\n"]
[503.246, "o", "    batch_size : int, default=256\r\n"]
[503.256, "o", "        Number of samples in each mini-batch.\r\n"]
[503.266, "o", "\r\n"]
[503.276, "o", "        .. versionchanged:: 1.3\r\n"]
[503.286, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[503.296, "o", "\r\n"]
[503.306, "o", "    shuffle : bool, default=True\r\n"]
[503.316, "o", "        Whether to shuffle the samples before forming batches.\r\n"]
[503.326, "o", "\r\n"]
[503.336, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[503.346, "o", "        Initial value of the dictionary for warm restart scenarios.\r\n"]
[503.356, "o", "\r\n"]
[503.366, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[503.376, "o", "            'threshold'}, default='omp'\r\n"]
[503.386, "o", "        Algorithm used to transform the data:\r\n"]
[503.396, "o", "\r\n"]
[503.406, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[503.416, "o", "          (`linear_model.lars_path`);\r\n"]
[503.426, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[503.436, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[503.446, "o", "          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n"]
[503.456, "o", "          if the estimated components are sparse.\r\n"]
[503.466, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[503.476, "o", "          solution.\r\n"]
[503.486, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[503.496, "o", "          the projection ``dictionary * X'``.\r\n"]
[503.506, "o", "\r\n"]
[503.516, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[503.526, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[503.536, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[503.546, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[503.556, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[503.566, "o", "\r\n"]
[503.576, "o", "    transform_alpha : float, default=None\r\n"]
[503.586, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[503.596, "o", "        penalty applied to the L1 norm.\r\n"]
[503.606, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[503.616, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[503.626, "o", "        If `None`, defaults to `alpha`.\r\n"]
[503.636, "o", "\r\n"]
[503.646, "o", "        .. versionchanged:: 1.2\r\n"]
[503.656, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[503.666, "o", "\r\n"]
[503.676, "o", "    verbose : bool or int, default=False\r\n"]
[503.686, "o", "        To control the verbosity of the procedure.\r\n"]
[503.696, "o", "\r\n"]
[503.706, "o", "    split_sign : bool, default=False\r\n"]
[503.716, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[503.726, "o", "        its negative part and its positive part. This can improve the\r\n"]
[503.736, "o", "        performance of downstream classifiers.\r\n"]
[503.746, "o", "\r\n"]
[503.756, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[503.766, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[503.776, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[503.786, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[503.796, "o", "        results across multiple function calls.\r\n"]
[503.806, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[503.816, "o", "\r\n"]
[503.826, "o", "    positive_code : bool, default=False\r\n"]
[503.836, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[503.846, "o", "\r\n"]
[503.856, "o", "        .. versionadded:: 0.20\r\n"]
[503.866, "o", "\r\n"]
[503.876, "o", "    positive_dict : bool, default=False\r\n"]
[503.886, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[503.896, "o", "\r\n"]
[503.906, "o", "        .. versionadded:: 0.20\r\n"]
[503.916, "o", "\r\n"]
[503.926, "o", "    transform_max_iter : int, default=1000\r\n"]
[503.936, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[503.946, "o", "        `'lasso_lars'`.\r\n"]
[503.956, "o", "\r\n"]
[503.966, "o", "        .. versionadded:: 0.22\r\n"]
[503.976, "o", "\r\n"]
[503.986, "o", "    callback : callable, default=None\r\n"]
[503.996, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[504.006, "o", "\r\n"]
[504.016, "o", "        .. versionadded:: 1.1\r\n"]
[504.026, "o", "\r\n"]
[504.036, "o", "    tol : float, default=1e-3\r\n"]
[504.046, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[504.056, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[504.066, "o", "\r\n"]
[504.076, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[504.086, "o", "        `tol` to 0.0.\r\n"]
[504.096, "o", "\r\n"]
[504.106, "o", "        .. versionadded:: 1.1\r\n"]
[504.116, "o", "\r\n"]
[504.126, "o", "    max_no_improvement : int, default=10\r\n"]
[504.136, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[504.146, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[504.156, "o", "        `max_iter` is not None.\r\n"]
[504.166, "o", "\r\n"]
[504.176, "o", "        To disable convergence detection based on cost function, set\r\n"]
[504.186, "o", "        `max_no_improvement` to None.\r\n"]
[504.196, "o", "\r\n"]
[504.206, "o", "        .. versionadded:: 1.1\r\n"]
[504.216, "o", "\r\n"]
[504.226, "o", "    Attributes\r\n"]
[504.236, "o", "    ----------\r\n"]
[504.246, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[504.256, "o", "        Components extracted from the data.\r\n"]
[504.266, "o", "\r\n"]
[504.276, "o", "    n_features_in_ : int\r\n"]
[504.286, "o", "        Number of features seen during :term:`fit`.\r\n"]
[504.296, "o", "\r\n"]
[504.306, "o", "        .. versionadded:: 0.24\r\n"]
[504.316, "o", "\r\n"]
[504.326, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[504.336, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[504.346, "o", "        has feature names that are all strings.\r\n"]
[504.356, "o", "\r\n"]
[504.366, "o", "        .. versionadded:: 1.0\r\n"]
[504.376, "o", "\r\n"]
[504.386, "o", "    n_iter_ : int\r\n"]
[504.396, "o", "        Number of iterations over the full dataset.\r\n"]
[504.406, "o", "\r\n"]
[504.416, "o", "    n_steps_ : int\r\n"]
[504.426, "o", "        Number of mini-batches processed.\r\n"]
[504.436, "o", "\r\n"]
[504.446, "o", "        .. versionadded:: 1.1\r\n"]
[504.456, "o", "\r\n"]
[504.466, "o", "    See Also\r\n"]
[504.476, "o", "    --------\r\n"]
[504.486, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[504.496, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[504.506, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[504.516, "o", "        precomputed dictionary.\r\n"]
[504.526, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[504.536, "o", "\r\n"]
[504.546, "o", "    References\r\n"]
[504.556, "o", "    ----------\r\n"]
[504.566, "o", "\r\n"]
[504.576, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[504.586, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[504.596, "o", "\r\n"]
[504.606, "o", "    Examples\r\n"]
[504.616, "o", "    --------\r\n"]
[504.626, "o", "    >>> import numpy as np\r\n"]
[504.636, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[504.646, "o", "    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n"]
[504.656, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[504.666, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[504.676, "o", "    ...     random_state=42)\r\n"]
[504.686, "o", "    >>> dict_learner = MiniBatchDictionaryLearning(\r\n"]
[504.696, "o", "    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n"]
[504.706, "o", "    ...     transform_alpha=0.1, random_state=42)\r\n"]
[504.716, "o", "    >>> X_transformed = dict_learner.fit_transform(X)\r\n"]
[504.726, "o", "\r\n"]
[504.736, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[504.746, "o", "\r\n"]
[504.756, "o", "    >>> np.mean(X_transformed == 0) < 0.5\r\n"]
[504.766, "o", "    True\r\n"]
[504.776, "o", "\r\n"]
[504.786, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[504.796, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[504.806, "o", "    the original signal:\r\n"]
[504.816, "o", "\r\n"]
[504.826, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[504.836, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[504.846, "o", "    0.057...\r\n"]
[504.856, "o", "    \"\"\"\r\n"]
[504.866, "o", "\r\n"]
[504.876, "o", "    _parameter_constraints: dict = {\r\n"]
[504.886, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[504.896, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[504.906, "o", "        \"n_iter\": [\r\n"]
[504.916, "o", "            Interval(Integral, 0, None, closed=\"left\"),\r\n"]
[504.926, "o", "            Hidden(StrOptions({\"deprecated\"})),\r\n"]
[504.936, "o", "        ],\r\n"]
[505.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[505.002, "i", "cd asv_benchmarks\r"]
[505.004, "o", "cd asv_benchmarks\r\n"]
[505.006, "o", "\u001b[?2004l\r\n"]
[510.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[510.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[510.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[511.654, "o", "\u001b[?2004l\r\n"]
[513.302, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[515.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[515.002, "i", "cd ..\r"]
[515.004, "o", "cd ..\r\n"]
[515.006, "o", "\u001b[?2004l\r\n"]
[520.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[520.002, "i", "grep -n \"def _minibatch_step\\|_sparse_encode_precomputed\\|def dict_learning_online\" sklearn/decomposition/_dict_lea\r"]
[520.004, "o", "grep -n \"def _minibatch_step\\|_sparse_encode_precomputed\\|def dict_learning_online\" sklearn/decomposition/_dict_lea\r\n"]
[520.712286, "o", "arning.py\r\n"]
[521.418571, "o", "\u001b[?2004l\r\n"]
[522.124857, "o", "\u001b[32m\u001b[K38\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdef \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n"]
[522.831143, "o", "\u001b[32m\u001b[K416\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        code = \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n"]
[523.537429, "o", "\u001b[32m\u001b[K438\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        delayed(\u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K)(\r\n"]
[524.243714, "o", "\u001b[32m\u001b[K664\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kdef dict_learning_online\u001b[m\u001b[K(\r\n"]
[525.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[525.002, "i", "sed -n '1,400p' sklearn/decomposition/_dict_learning.py\r"]
[525.004, "o", "sed -n '1,400p' sklearn/decomposition/_dict_learning.py\r\n"]
[525.018329, "o", "\u001b[?2004l\r\n"]
[525.030658, "o", "\"\"\" Dictionary learning.\r\n"]
[525.042988, "o", "\"\"\"\r\n"]
[525.055317, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[525.067646, "o", "# License: BSD 3 clause\r\n"]
[525.079975, "o", "\r\n"]
[525.092304, "o", "import itertools\r\n"]
[525.104633, "o", "import sys\r\n"]
[525.116963, "o", "import time\r\n"]
[525.129292, "o", "import warnings\r\n"]
[525.141621, "o", "from math import ceil\r\n"]
[525.15395, "o", "from numbers import Integral, Real\r\n"]
[525.166279, "o", "\r\n"]
[525.178608, "o", "import numpy as np\r\n"]
[525.190938, "o", "from joblib import effective_n_jobs\r\n"]
[525.203267, "o", "from scipy import linalg\r\n"]
[525.215596, "o", "\r\n"]
[525.227925, "o", "from ..base import (\r\n"]
[525.240254, "o", "    BaseEstimator,\r\n"]
[525.252584, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[525.264913, "o", "    TransformerMixin,\r\n"]
[525.277242, "o", "    _fit_context,\r\n"]
[525.289571, "o", ")\r\n"]
[525.3019, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[525.314229, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[525.326559, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[525.338888, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[525.351217, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[525.363546, "o", "from ..utils.validation import check_is_fitted\r\n"]
[525.375875, "o", "\r\n"]
[525.388204, "o", "\r\n"]
[525.400534, "o", "def _check_positive_coding(method, positive):\r\n"]
[525.412863, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[525.425192, "o", "        raise ValueError(\r\n"]
[525.437521, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[525.44985, "o", "        )\r\n"]
[525.46218, "o", "\r\n"]
[525.474509, "o", "\r\n"]
[525.486838, "o", "def _sparse_encode_precomputed(\r\n"]
[525.499167, "o", "    X,\r\n"]
[525.511496, "o", "    dictionary,\r\n"]
[525.523825, "o", "    *,\r\n"]
[525.536155, "o", "    gram=None,\r\n"]
[525.548484, "o", "    cov=None,\r\n"]
[525.560813, "o", "    algorithm=\"lasso_lars\",\r\n"]
[525.573142, "o", "    regularization=None,\r\n"]
[525.585471, "o", "    copy_cov=True,\r\n"]
[525.5978, "o", "    init=None,\r\n"]
[525.61013, "o", "    max_iter=1000,\r\n"]
[525.622459, "o", "    verbose=0,\r\n"]
[525.634788, "o", "    positive=False,\r\n"]
[525.647117, "o", "):\r\n"]
[525.659446, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[525.671776, "o", "\r\n"]
[525.684105, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[525.696434, "o", "\r\n"]
[525.708763, "o", "    Parameters\r\n"]
[525.721092, "o", "    ----------\r\n"]
[525.733421, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[525.745751, "o", "        Data matrix.\r\n"]
[525.75808, "o", "\r\n"]
[525.770409, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[525.782738, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[525.795067, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[525.807397, "o", "\r\n"]
[525.819726, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[525.832055, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[525.844384, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[525.856713, "o", "\r\n"]
[525.869042, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[525.881372, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[525.893701, "o", "\r\n"]
[525.90603, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[525.918359, "o", "            default='lasso_lars'\r\n"]
[525.930688, "o", "        The algorithm used:\r\n"]
[525.943017, "o", "\r\n"]
[525.955347, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[525.967676, "o", "          (`linear_model.lars_path`);\r\n"]
[525.980005, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[525.992334, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[526.004663, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[526.016993, "o", "          the estimated components are sparse;\r\n"]
[526.029322, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[526.041651, "o", "          solution;\r\n"]
[526.05398, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[526.066309, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[526.078638, "o", "\r\n"]
[526.090968, "o", "    regularization : int or float, default=None\r\n"]
[526.103297, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[526.115626, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[526.127955, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[526.140284, "o", "\r\n"]
[526.152613, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[526.164943, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[526.177272, "o", "        `algorithm='lasso_cd'`.\r\n"]
[526.189601, "o", "\r\n"]
[526.20193, "o", "    max_iter : int, default=1000\r\n"]
[526.214259, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[526.226589, "o", "        `'lasso_lars'`.\r\n"]
[526.238918, "o", "\r\n"]
[526.251247, "o", "    copy_cov : bool, default=True\r\n"]
[526.263576, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[526.275905, "o", "        be overwritten.\r\n"]
[526.288234, "o", "\r\n"]
[526.300564, "o", "    verbose : int, default=0\r\n"]
[526.312893, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[526.325222, "o", "\r\n"]
[526.337551, "o", "    positive: bool, default=False\r\n"]
[526.34988, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[526.362209, "o", "\r\n"]
[526.374539, "o", "        .. versionadded:: 0.20\r\n"]
[526.386868, "o", "\r\n"]
[526.399197, "o", "    Returns\r\n"]
[526.411526, "o", "    -------\r\n"]
[526.423855, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[526.436185, "o", "        The sparse codes.\r\n"]
[526.448514, "o", "    \"\"\"\r\n"]
[526.460843, "o", "    n_samples, n_features = X.shape\r\n"]
[526.473172, "o", "    n_components = dictionary.shape[0]\r\n"]
[526.485501, "o", "\r\n"]
[526.49783, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[526.51016, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[526.522489, "o", "        try:\r\n"]
[526.534818, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[526.547147, "o", "\r\n"]
[526.559476, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[526.571805, "o", "            # corrects the verbosity level.\r\n"]
[526.584135, "o", "            lasso_lars = LassoLars(\r\n"]
[526.596464, "o", "                alpha=alpha,\r\n"]
[526.608793, "o", "                fit_intercept=False,\r\n"]
[526.621122, "o", "                verbose=verbose,\r\n"]
[526.633451, "o", "                precompute=gram,\r\n"]
[526.645781, "o", "                fit_path=False,\r\n"]
[526.65811, "o", "                positive=positive,\r\n"]
[526.670439, "o", "                max_iter=max_iter,\r\n"]
[526.682768, "o", "            )\r\n"]
[526.695097, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[526.707426, "o", "            new_code = lasso_lars.coef_\r\n"]
[526.719756, "o", "        finally:\r\n"]
[526.732085, "o", "            np.seterr(**err_mgt)\r\n"]
[526.744414, "o", "\r\n"]
[526.756743, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[526.769072, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[526.781401, "o", "\r\n"]
[526.793731, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[526.80606, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[526.818389, "o", "        # argument that we could pass in from Lasso.\r\n"]
[526.830718, "o", "        clf = Lasso(\r\n"]
[526.843047, "o", "            alpha=alpha,\r\n"]
[526.855377, "o", "            fit_intercept=False,\r\n"]
[526.867706, "o", "            precompute=gram,\r\n"]
[526.880035, "o", "            max_iter=max_iter,\r\n"]
[526.892364, "o", "            warm_start=True,\r\n"]
[526.904693, "o", "            positive=positive,\r\n"]
[526.917022, "o", "        )\r\n"]
[526.929352, "o", "\r\n"]
[526.941681, "o", "        if init is not None:\r\n"]
[526.95401, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[526.966339, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[526.978668, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[526.990998, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[527.003327, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[527.015656, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[527.027985, "o", "                init = np.array(init)\r\n"]
[527.040314, "o", "            clf.coef_ = init\r\n"]
[527.052643, "o", "\r\n"]
[527.064973, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[527.077302, "o", "        new_code = clf.coef_\r\n"]
[527.089631, "o", "\r\n"]
[527.10196, "o", "    elif algorithm == \"lars\":\r\n"]
[527.114289, "o", "        try:\r\n"]
[527.126618, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[527.138948, "o", "\r\n"]
[527.151277, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[527.163606, "o", "            # corrects the verbosity level.\r\n"]
[527.175935, "o", "            lars = Lars(\r\n"]
[527.188264, "o", "                fit_intercept=False,\r\n"]
[527.200594, "o", "                verbose=verbose,\r\n"]
[527.212923, "o", "                precompute=gram,\r\n"]
[527.225252, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[527.237581, "o", "                fit_path=False,\r\n"]
[527.24991, "o", "            )\r\n"]
[527.262239, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[527.274569, "o", "            new_code = lars.coef_\r\n"]
[527.286898, "o", "        finally:\r\n"]
[527.299227, "o", "            np.seterr(**err_mgt)\r\n"]
[527.311556, "o", "\r\n"]
[527.323885, "o", "    elif algorithm == \"threshold\":\r\n"]
[527.336214, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[527.348544, "o", "        if positive:\r\n"]
[527.360873, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[527.373202, "o", "\r\n"]
[527.385531, "o", "    elif algorithm == \"omp\":\r\n"]
[527.39786, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[527.41019, "o", "            Gram=gram,\r\n"]
[527.422519, "o", "            Xy=cov,\r\n"]
[527.434848, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[527.447177, "o", "            tol=None,\r\n"]
[527.459506, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[527.471835, "o", "            copy_Xy=copy_cov,\r\n"]
[527.484165, "o", "        ).T\r\n"]
[527.496494, "o", "\r\n"]
[527.508823, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[527.521152, "o", "\r\n"]
[527.533481, "o", "\r\n"]
[527.54581, "o", "@validate_params(\r\n"]
[527.55814, "o", "    {\r\n"]
[527.570469, "o", "        \"X\": [\"array-like\"],\r\n"]
[527.582798, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[527.595127, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[527.607456, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[527.619786, "o", "        \"algorithm\": [\r\n"]
[527.632115, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[527.644444, "o", "        ],\r\n"]
[527.656773, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[527.669102, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[527.681431, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[527.693761, "o", "        \"init\": [\"array-like\", None],\r\n"]
[527.70609, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[527.718419, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[527.730748, "o", "        \"check_input\": [\"boolean\"],\r\n"]
[527.743077, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[527.755406, "o", "        \"positive\": [\"boolean\"],\r\n"]
[527.767736, "o", "    },\r\n"]
[527.780065, "o", "    prefer_skip_nested_validation=True,\r\n"]
[527.792394, "o", ")\r\n"]
[527.804723, "o", "# XXX : could be moved to the linear_model module\r\n"]
[527.817052, "o", "def sparse_encode(\r\n"]
[527.829382, "o", "    X,\r\n"]
[527.841711, "o", "    dictionary,\r\n"]
[527.85404, "o", "    *,\r\n"]
[527.866369, "o", "    gram=None,\r\n"]
[527.878698, "o", "    cov=None,\r\n"]
[527.891027, "o", "    algorithm=\"lasso_lars\",\r\n"]
[527.903357, "o", "    n_nonzero_coefs=None,\r\n"]
[527.915686, "o", "    alpha=None,\r\n"]
[527.928015, "o", "    copy_cov=True,\r\n"]
[527.940344, "o", "    init=None,\r\n"]
[527.952673, "o", "    max_iter=1000,\r\n"]
[527.965002, "o", "    n_jobs=None,\r\n"]
[527.977332, "o", "    check_input=True,\r\n"]
[527.989661, "o", "    verbose=0,\r\n"]
[528.00199, "o", "    positive=False,\r\n"]
[528.014319, "o", "):\r\n"]
[528.026648, "o", "    \"\"\"Sparse coding.\r\n"]
[528.038978, "o", "\r\n"]
[528.051307, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[528.063636, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[528.075965, "o", "\r\n"]
[528.088294, "o", "        X ~= code * dictionary\r\n"]
[528.100623, "o", "\r\n"]
[528.112953, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[528.125282, "o", "\r\n"]
[528.137611, "o", "    Parameters\r\n"]
[528.14994, "o", "    ----------\r\n"]
[528.162269, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[528.174599, "o", "        Data matrix.\r\n"]
[528.186928, "o", "\r\n"]
[528.199257, "o", "    dictionary : array-like of shape (n_components, n_features)\r\n"]
[528.211586, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[528.223915, "o", "        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[528.236244, "o", "        output.\r\n"]
[528.248574, "o", "\r\n"]
[528.260903, "o", "    gram : array-like of shape (n_components, n_components), default=None\r\n"]
[528.273232, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n"]
[528.285561, "o", "\r\n"]
[528.29789, "o", "    cov : array-like of shape (n_components, n_samples), default=None\r\n"]
[528.310219, "o", "        Precomputed covariance, `dictionary' * X`.\r\n"]
[528.322549, "o", "\r\n"]
[528.334878, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[528.347207, "o", "            default='lasso_lars'\r\n"]
[528.359536, "o", "        The algorithm used:\r\n"]
[528.371865, "o", "\r\n"]
[528.384195, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[528.396524, "o", "          (`linear_model.lars_path`);\r\n"]
[528.408853, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[528.421182, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[528.433511, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[528.44584, "o", "          the estimated components are sparse;\r\n"]
[528.45817, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[528.470499, "o", "          solution;\r\n"]
[528.482828, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[528.495157, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[528.507486, "o", "\r\n"]
[528.519815, "o", "    n_nonzero_coefs : int, default=None\r\n"]
[528.532145, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[528.544474, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[528.556803, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[528.569132, "o", "        `n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[528.581461, "o", "\r\n"]
[528.593791, "o", "    alpha : float, default=None\r\n"]
[528.60612, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[528.618449, "o", "        penalty applied to the L1 norm.\r\n"]
[528.630778, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[528.643107, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[528.655436, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[528.667766, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[528.680095, "o", "        `n_nonzero_coefs`.\r\n"]
[528.692424, "o", "        If `None`, default to 1.\r\n"]
[528.704753, "o", "\r\n"]
[528.717082, "o", "    copy_cov : bool, default=True\r\n"]
[528.729411, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[528.741741, "o", "        be overwritten.\r\n"]
[528.75407, "o", "\r\n"]
[528.766399, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[528.778728, "o", "        Initialization value of the sparse codes. Only used if\r\n"]
[528.791057, "o", "        `algorithm='lasso_cd'`.\r\n"]
[528.803387, "o", "\r\n"]
[528.815716, "o", "    max_iter : int, default=1000\r\n"]
[528.828045, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[528.840374, "o", "        `'lasso_lars'`.\r\n"]
[528.852703, "o", "\r\n"]
[528.865032, "o", "    n_jobs : int, default=None\r\n"]
[528.877362, "o", "        Number of parallel jobs to run.\r\n"]
[528.889691, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[528.90202, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[528.914349, "o", "        for more details.\r\n"]
[528.926678, "o", "\r\n"]
[528.939007, "o", "    check_input : bool, default=True\r\n"]
[528.951337, "o", "        If `False`, the input arrays X and dictionary will not be checked.\r\n"]
[528.963666, "o", "\r\n"]
[528.975995, "o", "    verbose : int, default=0\r\n"]
[528.988324, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[529.000653, "o", "\r\n"]
[529.012983, "o", "    positive : bool, default=False\r\n"]
[529.025312, "o", "        Whether to enforce positivity when finding the encoding.\r\n"]
[529.037641, "o", "\r\n"]
[529.04997, "o", "        .. versionadded:: 0.20\r\n"]
[529.062299, "o", "\r\n"]
[529.074628, "o", "    Returns\r\n"]
[529.086958, "o", "    -------\r\n"]
[529.099287, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[529.111616, "o", "        The sparse codes.\r\n"]
[529.123945, "o", "\r\n"]
[529.136274, "o", "    See Also\r\n"]
[529.148603, "o", "    --------\r\n"]
[529.160933, "o", "    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n"]
[529.173262, "o", "        path using LARS algorithm.\r\n"]
[529.185591, "o", "    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n"]
[529.19792, "o", "    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n"]
[529.210249, "o", "    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n"]
[529.222579, "o", "        dictionary.\r\n"]
[529.234908, "o", "    \"\"\"\r\n"]
[529.247237, "o", "    if check_input:\r\n"]
[529.259566, "o", "        if algorithm == \"lasso_cd\":\r\n"]
[529.271895, "o", "            dictionary = check_array(\r\n"]
[529.284224, "o", "                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n"]
[529.296554, "o", "            )\r\n"]
[529.308883, "o", "            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n"]
[529.321212, "o", "        else:\r\n"]
[529.333541, "o", "            dictionary = check_array(dictionary)\r\n"]
[529.34587, "o", "            X = check_array(X)\r\n"]
[529.3582, "o", "\r\n"]
[529.370529, "o", "    if dictionary.shape[1] != X.shape[1]:\r\n"]
[529.382858, "o", "        raise ValueError(\r\n"]
[529.395187, "o", "            \"Dictionary and X have different numbers of features:\"\r\n"]
[529.407516, "o", "            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n"]
[529.419845, "o", "        )\r\n"]
[529.432175, "o", "\r\n"]
[529.444504, "o", "    _check_positive_coding(algorithm, positive)\r\n"]
[529.456833, "o", "\r\n"]
[529.469162, "o", "    return _sparse_encode(\r\n"]
[529.481491, "o", "        X,\r\n"]
[529.49382, "o", "        dictionary,\r\n"]
[529.50615, "o", "        gram=gram,\r\n"]
[529.518479, "o", "        cov=cov,\r\n"]
[529.530808, "o", "        algorithm=algorithm,\r\n"]
[529.543137, "o", "        n_nonzero_coefs=n_nonzero_coefs,\r\n"]
[529.555466, "o", "        alpha=alpha,\r\n"]
[529.567796, "o", "        copy_cov=copy_cov,\r\n"]
[529.580125, "o", "        init=init,\r\n"]
[529.592454, "o", "        max_iter=max_iter,\r\n"]
[529.604783, "o", "        n_jobs=n_jobs,\r\n"]
[529.617112, "o", "        verbose=verbose,\r\n"]
[529.629441, "o", "        positive=positive,\r\n"]
[529.641771, "o", "    )\r\n"]
[529.6541, "o", "\r\n"]
[529.666429, "o", "\r\n"]
[529.678758, "o", "def _sparse_encode(\r\n"]
[529.691087, "o", "    X,\r\n"]
[529.703416, "o", "    dictionary,\r\n"]
[529.715746, "o", "    *,\r\n"]
[529.728075, "o", "    gram=None,\r\n"]
[529.740404, "o", "    cov=None,\r\n"]
[529.752733, "o", "    algorithm=\"lasso_lars\",\r\n"]
[529.765062, "o", "    n_nonzero_coefs=None,\r\n"]
[529.777392, "o", "    alpha=None,\r\n"]
[529.789721, "o", "    copy_cov=True,\r\n"]
[529.80205, "o", "    init=None,\r\n"]
[529.814379, "o", "    max_iter=1000,\r\n"]
[529.826708, "o", "    n_jobs=None,\r\n"]
[529.839037, "o", "    verbose=0,\r\n"]
[529.851367, "o", "    positive=False,\r\n"]
[529.863696, "o", "):\r\n"]
[529.876025, "o", "    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[529.888354, "o", "\r\n"]
[529.900683, "o", "    n_samples, n_features = X.shape\r\n"]
[529.913012, "o", "    n_components = dictionary.shape[0]\r\n"]
[529.925342, "o", "\r\n"]
[529.937671, "o", "    if algorithm in (\"lars\", \"omp\"):\r\n"]
[530.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[530.002, "i", "sed -n '400,1200p' sklearn/decomposition/_dict_learning.py\r"]
[530.004, "o", "sed -n '400,1200p' sklearn/decomposition/_dict_learning.py\r\n"]
[530.016, "o", "\u001b[?2004l\r\n"]
[530.026, "o", "        regularization = n_nonzero_coefs\r\n"]
[530.036, "o", "        if regularization is None:\r\n"]
[530.046, "o", "            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[530.056, "o", "    else:\r\n"]
[530.066, "o", "        regularization = alpha\r\n"]
[530.076, "o", "        if regularization is None:\r\n"]
[530.086, "o", "            regularization = 1.0\r\n"]
[530.096, "o", "\r\n"]
[530.106, "o", "    if gram is None and algorithm != \"threshold\":\r\n"]
[530.116, "o", "        gram = np.dot(dictionary, dictionary.T)\r\n"]
[530.126, "o", "\r\n"]
[530.136, "o", "    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[530.146, "o", "        copy_cov = False\r\n"]
[530.156, "o", "        cov = np.dot(dictionary, X.T)\r\n"]
[530.166, "o", "\r\n"]
[530.176, "o", "    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[530.186, "o", "        code = _sparse_encode_precomputed(\r\n"]
[530.196, "o", "            X,\r\n"]
[530.206, "o", "            dictionary,\r\n"]
[530.216, "o", "            gram=gram,\r\n"]
[530.226, "o", "            cov=cov,\r\n"]
[530.236, "o", "            algorithm=algorithm,\r\n"]
[530.246, "o", "            regularization=regularization,\r\n"]
[530.256, "o", "            copy_cov=copy_cov,\r\n"]
[530.266, "o", "            init=init,\r\n"]
[530.276, "o", "            max_iter=max_iter,\r\n"]
[530.286, "o", "            verbose=verbose,\r\n"]
[530.296, "o", "            positive=positive,\r\n"]
[530.306, "o", "        )\r\n"]
[530.316, "o", "        return code\r\n"]
[530.326, "o", "\r\n"]
[530.336, "o", "    # Enter parallel code block\r\n"]
[530.346, "o", "    n_samples = X.shape[0]\r\n"]
[530.356, "o", "    n_components = dictionary.shape[0]\r\n"]
[530.366, "o", "    code = np.empty((n_samples, n_components))\r\n"]
[530.376, "o", "    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[530.386, "o", "\r\n"]
[530.396, "o", "    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[530.406, "o", "        delayed(_sparse_encode_precomputed)(\r\n"]
[530.416, "o", "            X[this_slice],\r\n"]
[530.426, "o", "            dictionary,\r\n"]
[530.436, "o", "            gram=gram,\r\n"]
[530.446, "o", "            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[530.456, "o", "            algorithm=algorithm,\r\n"]
[530.466, "o", "            regularization=regularization,\r\n"]
[530.476, "o", "            copy_cov=copy_cov,\r\n"]
[530.486, "o", "            init=init[this_slice] if init is not None else None,\r\n"]
[530.496, "o", "            max_iter=max_iter,\r\n"]
[530.506, "o", "            verbose=verbose,\r\n"]
[530.516, "o", "            positive=positive,\r\n"]
[530.526, "o", "        )\r\n"]
[530.536, "o", "        for this_slice in slices\r\n"]
[530.546, "o", "    )\r\n"]
[530.556, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[530.566, "o", "        code[this_slice] = this_view\r\n"]
[530.576, "o", "    return code\r\n"]
[530.586, "o", "\r\n"]
[530.596, "o", "\r\n"]
[530.606, "o", "def _update_dict(\r\n"]
[530.616, "o", "    dictionary,\r\n"]
[530.626, "o", "    Y,\r\n"]
[530.636, "o", "    code,\r\n"]
[530.646, "o", "    A=None,\r\n"]
[530.656, "o", "    B=None,\r\n"]
[530.666, "o", "    verbose=False,\r\n"]
[530.676, "o", "    random_state=None,\r\n"]
[530.686, "o", "    positive=False,\r\n"]
[530.696, "o", "):\r\n"]
[530.706, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[530.716, "o", "\r\n"]
[530.726, "o", "    Parameters\r\n"]
[530.736, "o", "    ----------\r\n"]
[530.746, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[530.756, "o", "        Value of the dictionary at the previous iteration.\r\n"]
[530.766, "o", "\r\n"]
[530.776, "o", "    Y : ndarray of shape (n_samples, n_features)\r\n"]
[530.786, "o", "        Data matrix.\r\n"]
[530.796, "o", "\r\n"]
[530.806, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[530.816, "o", "        Sparse coding of the data against which to optimize the dictionary.\r\n"]
[530.826, "o", "\r\n"]
[530.836, "o", "    A : ndarray of shape (n_components, n_components), default=None\r\n"]
[530.846, "o", "        Together with `B`, sufficient stats of the online model to update the\r\n"]
[530.856, "o", "        dictionary.\r\n"]
[530.866, "o", "\r\n"]
[530.876, "o", "    B : ndarray of shape (n_features, n_components), default=None\r\n"]
[530.886, "o", "        Together with `A`, sufficient stats of the online model to update the\r\n"]
[530.896, "o", "        dictionary.\r\n"]
[530.906, "o", "\r\n"]
[530.916, "o", "    verbose: bool, default=False\r\n"]
[530.926, "o", "        Degree of output the procedure will print.\r\n"]
[530.936, "o", "\r\n"]
[530.946, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[530.956, "o", "        Used for randomly initializing the dictionary. Pass an int for\r\n"]
[530.966, "o", "        reproducible results across multiple function calls.\r\n"]
[530.976, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[530.986, "o", "\r\n"]
[530.996, "o", "    positive : bool, default=False\r\n"]
[531.006, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[531.016, "o", "\r\n"]
[531.026, "o", "        .. versionadded:: 0.20\r\n"]
[531.036, "o", "    \"\"\"\r\n"]
[531.046, "o", "    n_samples, n_components = code.shape\r\n"]
[531.056, "o", "    random_state = check_random_state(random_state)\r\n"]
[531.066, "o", "\r\n"]
[531.076, "o", "    if A is None:\r\n"]
[531.086, "o", "        A = code.T @ code\r\n"]
[531.096, "o", "    if B is None:\r\n"]
[531.106, "o", "        B = Y.T @ code\r\n"]
[531.116, "o", "\r\n"]
[531.126, "o", "    n_unused = 0\r\n"]
[531.136, "o", "\r\n"]
[531.146, "o", "    for k in range(n_components):\r\n"]
[531.156, "o", "        if A[k, k] > 1e-6:\r\n"]
[531.166, "o", "            # 1e-6 is arbitrary but consistent with the spams implementation\r\n"]
[531.176, "o", "            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n"]
[531.186, "o", "        else:\r\n"]
[531.196, "o", "            # kth atom is almost never used -> sample a new one from the data\r\n"]
[531.206, "o", "            newd = Y[random_state.choice(n_samples)]\r\n"]
[531.216, "o", "\r\n"]
[531.226, "o", "            # add small noise to avoid making the sparse coding ill conditioned\r\n"]
[531.236, "o", "            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n"]
[531.246, "o", "            noise = random_state.normal(0, noise_level, size=len(newd))\r\n"]
[531.256, "o", "\r\n"]
[531.266, "o", "            dictionary[k] = newd + noise\r\n"]
[531.276, "o", "            code[:, k] = 0\r\n"]
[531.286, "o", "            n_unused += 1\r\n"]
[531.296, "o", "\r\n"]
[531.306, "o", "        if positive:\r\n"]
[531.316, "o", "            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n"]
[531.326, "o", "\r\n"]
[531.336, "o", "        # Projection on the constraint set ||V_k|| <= 1\r\n"]
[531.346, "o", "        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n"]
[531.356, "o", "\r\n"]
[531.366, "o", "    if verbose and n_unused > 0:\r\n"]
[531.376, "o", "        print(f\"{n_unused} unused atoms resampled.\")\r\n"]
[531.386, "o", "\r\n"]
[531.396, "o", "\r\n"]
[531.406, "o", "def _dict_learning(\r\n"]
[531.416, "o", "    X,\r\n"]
[531.426, "o", "    n_components,\r\n"]
[531.436, "o", "    *,\r\n"]
[531.446, "o", "    alpha,\r\n"]
[531.456, "o", "    max_iter,\r\n"]
[531.466, "o", "    tol,\r\n"]
[531.476, "o", "    method,\r\n"]
[531.486, "o", "    n_jobs,\r\n"]
[531.496, "o", "    dict_init,\r\n"]
[531.506, "o", "    code_init,\r\n"]
[531.516, "o", "    callback,\r\n"]
[531.526, "o", "    verbose,\r\n"]
[531.536, "o", "    random_state,\r\n"]
[531.546, "o", "    return_n_iter,\r\n"]
[531.556, "o", "    positive_dict,\r\n"]
[531.566, "o", "    positive_code,\r\n"]
[531.576, "o", "    method_max_iter,\r\n"]
[531.586, "o", "):\r\n"]
[531.596, "o", "    \"\"\"Main dictionary learning algorithm\"\"\"\r\n"]
[531.606, "o", "    t0 = time.time()\r\n"]
[531.616, "o", "    # Init the code and the dictionary with SVD of Y\r\n"]
[531.626, "o", "    if code_init is not None and dict_init is not None:\r\n"]
[531.636, "o", "        code = np.array(code_init, order=\"F\")\r\n"]
[531.646, "o", "        # Don't copy V, it will happen below\r\n"]
[531.656, "o", "        dictionary = dict_init\r\n"]
[531.666, "o", "    else:\r\n"]
[531.676, "o", "        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n"]
[531.686, "o", "        # flip the initial code's sign to enforce deterministic output\r\n"]
[531.696, "o", "        code, dictionary = svd_flip(code, dictionary)\r\n"]
[531.706, "o", "        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[531.716, "o", "    r = len(dictionary)\r\n"]
[531.726, "o", "    if n_components <= r:  # True even if n_components=None\r\n"]
[531.736, "o", "        code = code[:, :n_components]\r\n"]
[531.746, "o", "        dictionary = dictionary[:n_components, :]\r\n"]
[531.756, "o", "    else:\r\n"]
[531.766, "o", "        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n"]
[531.776, "o", "        dictionary = np.r_[\r\n"]
[531.786, "o", "            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n"]
[531.796, "o", "        ]\r\n"]
[531.806, "o", "\r\n"]
[531.816, "o", "    # Fortran-order dict better suited for the sparse coding which is the\r\n"]
[531.826, "o", "    # bottleneck of this algorithm.\r\n"]
[531.836, "o", "    dictionary = np.asfortranarray(dictionary)\r\n"]
[531.846, "o", "\r\n"]
[531.856, "o", "    errors = []\r\n"]
[531.866, "o", "    current_cost = np.nan\r\n"]
[531.876, "o", "\r\n"]
[531.886, "o", "    if verbose == 1:\r\n"]
[531.896, "o", "        print(\"[dict_learning]\", end=\" \")\r\n"]
[531.906, "o", "\r\n"]
[531.916, "o", "    # If max_iter is 0, number of iterations returned should be zero\r\n"]
[531.926, "o", "    ii = -1\r\n"]
[531.936, "o", "\r\n"]
[531.946, "o", "    for ii in range(max_iter):\r\n"]
[531.956, "o", "        dt = time.time() - t0\r\n"]
[531.966, "o", "        if verbose == 1:\r\n"]
[531.976, "o", "            sys.stdout.write(\".\")\r\n"]
[531.986, "o", "            sys.stdout.flush()\r\n"]
[531.996, "o", "        elif verbose:\r\n"]
[532.006, "o", "            print(\r\n"]
[532.016, "o", "                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n"]
[532.026, "o", "                % (ii, dt, dt / 60, current_cost)\r\n"]
[532.036, "o", "            )\r\n"]
[532.046, "o", "\r\n"]
[532.056, "o", "        # Update code\r\n"]
[532.066, "o", "        code = sparse_encode(\r\n"]
[532.076, "o", "            X,\r\n"]
[532.086, "o", "            dictionary,\r\n"]
[532.096, "o", "            algorithm=method,\r\n"]
[532.106, "o", "            alpha=alpha,\r\n"]
[532.116, "o", "            init=code,\r\n"]
[532.126, "o", "            n_jobs=n_jobs,\r\n"]
[532.136, "o", "            positive=positive_code,\r\n"]
[532.146, "o", "            max_iter=method_max_iter,\r\n"]
[532.156, "o", "            verbose=verbose,\r\n"]
[532.166, "o", "        )\r\n"]
[532.176, "o", "\r\n"]
[532.186, "o", "        # Update dictionary in place\r\n"]
[532.196, "o", "        _update_dict(\r\n"]
[532.206, "o", "            dictionary,\r\n"]
[532.216, "o", "            X,\r\n"]
[532.226, "o", "            code,\r\n"]
[532.236, "o", "            verbose=verbose,\r\n"]
[532.246, "o", "            random_state=random_state,\r\n"]
[532.256, "o", "            positive=positive_dict,\r\n"]
[532.266, "o", "        )\r\n"]
[532.276, "o", "\r\n"]
[532.286, "o", "        # Cost function\r\n"]
[532.296, "o", "        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n"]
[532.306, "o", "            np.abs(code)\r\n"]
[532.316, "o", "        )\r\n"]
[532.326, "o", "        errors.append(current_cost)\r\n"]
[532.336, "o", "\r\n"]
[532.346, "o", "        if ii > 0:\r\n"]
[532.356, "o", "            dE = errors[-2] - errors[-1]\r\n"]
[532.366, "o", "            # assert(dE >= -tol * errors[-1])\r\n"]
[532.376, "o", "            if dE < tol * errors[-1]:\r\n"]
[532.386, "o", "                if verbose == 1:\r\n"]
[532.396, "o", "                    # A line return\r\n"]
[532.406, "o", "                    print(\"\")\r\n"]
[532.416, "o", "                elif verbose:\r\n"]
[532.426, "o", "                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n"]
[532.436, "o", "                break\r\n"]
[532.446, "o", "        if ii % 5 == 0 and callback is not None:\r\n"]
[532.456, "o", "            callback(locals())\r\n"]
[532.466, "o", "\r\n"]
[532.476, "o", "    if return_n_iter:\r\n"]
[532.486, "o", "        return code, dictionary, errors, ii + 1\r\n"]
[532.496, "o", "    else:\r\n"]
[532.506, "o", "        return code, dictionary, errors\r\n"]
[532.516, "o", "\r\n"]
[532.526, "o", "\r\n"]
[532.536, "o", "def _check_warn_deprecated(param, name, default, additional_message=None):\r\n"]
[532.546, "o", "    if param != \"deprecated\":\r\n"]
[532.556, "o", "        msg = (\r\n"]
[532.566, "o", "            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n"]
[532.576, "o", "        )\r\n"]
[532.586, "o", "        if additional_message:\r\n"]
[532.596, "o", "            msg += f\" {additional_message}\"\r\n"]
[532.606, "o", "        warnings.warn(msg, FutureWarning)\r\n"]
[532.616, "o", "        return param\r\n"]
[532.626, "o", "    else:\r\n"]
[532.636, "o", "        return default\r\n"]
[532.646, "o", "\r\n"]
[532.656, "o", "\r\n"]
[532.666, "o", "def dict_learning_online(\r\n"]
[532.676, "o", "    X,\r\n"]
[532.686, "o", "    n_components=2,\r\n"]
[532.696, "o", "    *,\r\n"]
[532.706, "o", "    alpha=1,\r\n"]
[532.716, "o", "    n_iter=\"deprecated\",\r\n"]
[532.726, "o", "    max_iter=None,\r\n"]
[532.736, "o", "    return_code=True,\r\n"]
[532.746, "o", "    dict_init=None,\r\n"]
[532.756, "o", "    callback=None,\r\n"]
[532.766, "o", "    batch_size=256,\r\n"]
[532.776, "o", "    verbose=False,\r\n"]
[532.786, "o", "    shuffle=True,\r\n"]
[532.796, "o", "    n_jobs=None,\r\n"]
[532.806, "o", "    method=\"lars\",\r\n"]
[532.816, "o", "    iter_offset=\"deprecated\",\r\n"]
[532.826, "o", "    random_state=None,\r\n"]
[532.836, "o", "    return_inner_stats=\"deprecated\",\r\n"]
[532.846, "o", "    inner_stats=\"deprecated\",\r\n"]
[532.856, "o", "    return_n_iter=\"deprecated\",\r\n"]
[532.866, "o", "    positive_dict=False,\r\n"]
[532.876, "o", "    positive_code=False,\r\n"]
[532.886, "o", "    method_max_iter=1000,\r\n"]
[532.896, "o", "    tol=1e-3,\r\n"]
[532.906, "o", "    max_no_improvement=10,\r\n"]
[532.916, "o", "):\r\n"]
[532.926, "o", "    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n"]
[532.936, "o", "\r\n"]
[532.946, "o", "    Finds the best dictionary and the corresponding sparse code for\r\n"]
[532.956, "o", "    approximating the data matrix X by solving::\r\n"]
[532.966, "o", "\r\n"]
[532.976, "o", "        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[532.986, "o", "                     (U,V)\r\n"]
[532.996, "o", "                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n"]
[533.006, "o", "\r\n"]
[533.016, "o", "    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n"]
[533.026, "o", "    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n"]
[533.036, "o", "    which is the sum of the absolute values of all the entries in the matrix.\r\n"]
[533.046, "o", "    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n"]
[533.056, "o", "    the input data.\r\n"]
[533.066, "o", "\r\n"]
[533.076, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[533.086, "o", "\r\n"]
[533.096, "o", "    Parameters\r\n"]
[533.106, "o", "    ----------\r\n"]
[533.116, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[533.126, "o", "        Data matrix.\r\n"]
[533.136, "o", "\r\n"]
[533.146, "o", "    n_components : int or None, default=2\r\n"]
[533.156, "o", "        Number of dictionary atoms to extract. If None, then ``n_components``\r\n"]
[533.166, "o", "        is set to ``n_features``.\r\n"]
[533.176, "o", "\r\n"]
[533.186, "o", "    alpha : float, default=1\r\n"]
[533.196, "o", "        Sparsity controlling parameter.\r\n"]
[533.206, "o", "\r\n"]
[533.216, "o", "    n_iter : int, default=100\r\n"]
[533.226, "o", "        Number of mini-batch iterations to perform.\r\n"]
[533.236, "o", "\r\n"]
[533.246, "o", "        .. deprecated:: 1.1\r\n"]
[533.256, "o", "           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[533.266, "o", "           `max_iter` instead.\r\n"]
[533.276, "o", "\r\n"]
[533.286, "o", "    max_iter : int, default=None\r\n"]
[533.296, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[533.306, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[533.316, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[533.326, "o", "\r\n"]
[533.336, "o", "        .. versionadded:: 1.1\r\n"]
[533.346, "o", "\r\n"]
[533.356, "o", "    return_code : bool, default=True\r\n"]
[533.366, "o", "        Whether to also return the code U or just the dictionary `V`.\r\n"]
[533.376, "o", "\r\n"]
[533.386, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[533.396, "o", "        Initial values for the dictionary for warm restart scenarios.\r\n"]
[533.406, "o", "        If `None`, the initial values for the dictionary are created\r\n"]
[533.416, "o", "        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n"]
[533.426, "o", "\r\n"]
[533.436, "o", "    callback : callable, default=None\r\n"]
[533.446, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[533.456, "o", "\r\n"]
[533.466, "o", "    batch_size : int, default=256\r\n"]
[533.476, "o", "        The number of samples to take in each batch.\r\n"]
[533.486, "o", "\r\n"]
[533.496, "o", "        .. versionchanged:: 1.3\r\n"]
[533.506, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[533.516, "o", "\r\n"]
[533.526, "o", "    verbose : bool, default=False\r\n"]
[533.536, "o", "        To control the verbosity of the procedure.\r\n"]
[533.546, "o", "\r\n"]
[533.556, "o", "    shuffle : bool, default=True\r\n"]
[533.566, "o", "        Whether to shuffle the data before splitting it in batches.\r\n"]
[533.576, "o", "\r\n"]
[533.586, "o", "    n_jobs : int, default=None\r\n"]
[533.596, "o", "        Number of parallel jobs to run.\r\n"]
[533.606, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[533.616, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[533.626, "o", "        for more details.\r\n"]
[533.636, "o", "\r\n"]
[533.646, "o", "    method : {'lars', 'cd'}, default='lars'\r\n"]
[533.656, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[533.666, "o", "          problem (`linear_model.lars_path`);\r\n"]
[533.676, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[533.686, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[533.696, "o", "          the estimated components are sparse.\r\n"]
[533.706, "o", "\r\n"]
[533.716, "o", "    iter_offset : int, default=0\r\n"]
[533.726, "o", "        Number of previous iterations completed on the dictionary used for\r\n"]
[533.736, "o", "        initialization.\r\n"]
[533.746, "o", "\r\n"]
[533.756, "o", "        .. deprecated:: 1.1\r\n"]
[533.766, "o", "           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n"]
[533.776, "o", "\r\n"]
[533.786, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[533.796, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[533.806, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[533.816, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[533.826, "o", "        results across multiple function calls.\r\n"]
[533.836, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[533.846, "o", "\r\n"]
[533.856, "o", "    return_inner_stats : bool, default=False\r\n"]
[533.866, "o", "        Return the inner statistics A (dictionary covariance) and B\r\n"]
[533.876, "o", "        (data approximation). Useful to restart the algorithm in an\r\n"]
[533.886, "o", "        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n"]
[533.896, "o", "        ignored.\r\n"]
[533.906, "o", "\r\n"]
[533.916, "o", "        .. deprecated:: 1.1\r\n"]
[533.926, "o", "           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[533.936, "o", "\r\n"]
[533.946, "o", "    inner_stats : tuple of (A, B) ndarrays, default=None\r\n"]
[533.956, "o", "        Inner sufficient statistics that are kept by the algorithm.\r\n"]
[533.966, "o", "        Passing them at initialization is useful in online settings, to\r\n"]
[533.976, "o", "        avoid losing the history of the evolution.\r\n"]
[533.986, "o", "        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n"]
[533.996, "o", "        `B` `(n_features, n_components)` is the data approximation matrix.\r\n"]
[534.006, "o", "\r\n"]
[534.016, "o", "        .. deprecated:: 1.1\r\n"]
[534.026, "o", "           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[534.036, "o", "\r\n"]
[534.046, "o", "    return_n_iter : bool, default=False\r\n"]
[534.056, "o", "        Whether or not to return the number of iterations.\r\n"]
[534.066, "o", "\r\n"]
[534.076, "o", "        .. deprecated:: 1.1\r\n"]
[534.086, "o", "           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n"]
[534.096, "o", "\r\n"]
[534.106, "o", "    positive_dict : bool, default=False\r\n"]
[534.116, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[534.126, "o", "\r\n"]
[534.136, "o", "        .. versionadded:: 0.20\r\n"]
[534.146, "o", "\r\n"]
[534.156, "o", "    positive_code : bool, default=False\r\n"]
[534.166, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[534.176, "o", "\r\n"]
[534.186, "o", "        .. versionadded:: 0.20\r\n"]
[534.196, "o", "\r\n"]
[534.206, "o", "    method_max_iter : int, default=1000\r\n"]
[534.216, "o", "        Maximum number of iterations to perform when solving the lasso problem.\r\n"]
[534.226, "o", "\r\n"]
[534.236, "o", "        .. versionadded:: 0.22\r\n"]
[534.246, "o", "\r\n"]
[534.256, "o", "    tol : float, default=1e-3\r\n"]
[534.266, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[534.276, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[534.286, "o", "\r\n"]
[534.296, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[534.306, "o", "        `tol` to 0.0.\r\n"]
[534.316, "o", "\r\n"]
[534.326, "o", "        .. versionadded:: 1.1\r\n"]
[534.336, "o", "\r\n"]
[534.346, "o", "    max_no_improvement : int, default=10\r\n"]
[534.356, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[534.366, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[534.376, "o", "        `max_iter` is not None.\r\n"]
[534.386, "o", "\r\n"]
[534.396, "o", "        To disable convergence detection based on cost function, set\r\n"]
[534.406, "o", "        `max_no_improvement` to None.\r\n"]
[534.416, "o", "\r\n"]
[534.426, "o", "        .. versionadded:: 1.1\r\n"]
[534.436, "o", "\r\n"]
[534.446, "o", "    Returns\r\n"]
[534.456, "o", "    -------\r\n"]
[534.466, "o", "    code : ndarray of shape (n_samples, n_components),\r\n"]
[534.476, "o", "        The sparse code (only returned if `return_code=True`).\r\n"]
[534.486, "o", "\r\n"]
[534.496, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[534.506, "o", "        The solutions to the dictionary learning problem.\r\n"]
[534.516, "o", "\r\n"]
[534.526, "o", "    n_iter : int\r\n"]
[534.536, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[534.546, "o", "        set to `True`.\r\n"]
[534.556, "o", "\r\n"]
[534.566, "o", "    See Also\r\n"]
[534.576, "o", "    --------\r\n"]
[534.586, "o", "    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n"]
[534.596, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[534.606, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n"]
[534.616, "o", "        learning algorithm.\r\n"]
[534.626, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[534.636, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[534.646, "o", "    \"\"\"\r\n"]
[534.656, "o", "    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n"]
[534.666, "o", "    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n"]
[534.676, "o", "        raise ValueError(\r\n"]
[534.686, "o", "            \"The following arguments are incompatible with 'max_iter': \"\r\n"]
[534.696, "o", "            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n"]
[534.706, "o", "        )\r\n"]
[534.716, "o", "\r\n"]
[534.726, "o", "    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n"]
[534.736, "o", "    return_inner_stats = _check_warn_deprecated(\r\n"]
[534.746, "o", "        return_inner_stats,\r\n"]
[534.756, "o", "        \"return_inner_stats\",\r\n"]
[534.766, "o", "        default=False,\r\n"]
[534.776, "o", "        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n"]
[534.786, "o", "    )\r\n"]
[534.796, "o", "    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n"]
[534.806, "o", "    return_n_iter = _check_warn_deprecated(\r\n"]
[534.816, "o", "        return_n_iter,\r\n"]
[534.826, "o", "        \"return_n_iter\",\r\n"]
[534.836, "o", "        default=False,\r\n"]
[534.846, "o", "        additional_message=(\r\n"]
[534.856, "o", "            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n"]
[534.866, "o", "            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n"]
[534.876, "o", "        ),\r\n"]
[534.886, "o", "    )\r\n"]
[534.896, "o", "\r\n"]
[534.906, "o", "    if max_iter is not None:\r\n"]
[534.916, "o", "        transform_algorithm = \"lasso_\" + method\r\n"]
[534.926, "o", "\r\n"]
[534.936, "o", "        est = MiniBatchDictionaryLearning(\r\n"]
[535.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[535.002, "i", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r"]
[535.004, "o", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r\n"]
[535.016, "o", "\u001b[?2004l\r\n"]
[535.026, "o", "    n_iter : int\r\n"]
[535.036, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[535.046, "o", "        set to True.\r\n"]
[535.056, "o", "\r\n"]
[535.066, "o", "    See Also\r\n"]
[535.076, "o", "    --------\r\n"]
[535.086, "o", "    dict_learning_online : Solve a dictionary learning matrix factorization\r\n"]
[535.096, "o", "        problem online.\r\n"]
[535.106, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[535.116, "o", "    MiniBatchDictionaryLearning : A faster, less accurate version\r\n"]
[535.126, "o", "        of the dictionary learning algorithm.\r\n"]
[535.136, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[535.146, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[535.156, "o", "    \"\"\"\r\n"]
[535.166, "o", "    estimator = DictionaryLearning(\r\n"]
[535.176, "o", "        n_components=n_components,\r\n"]
[535.186, "o", "        alpha=alpha,\r\n"]
[535.196, "o", "        max_iter=max_iter,\r\n"]
[535.206, "o", "        tol=tol,\r\n"]
[535.216, "o", "        fit_algorithm=method,\r\n"]
[535.226, "o", "        n_jobs=n_jobs,\r\n"]
[535.236, "o", "        dict_init=dict_init,\r\n"]
[535.246, "o", "        callback=callback,\r\n"]
[535.256, "o", "        code_init=code_init,\r\n"]
[535.266, "o", "        verbose=verbose,\r\n"]
[535.276, "o", "        random_state=random_state,\r\n"]
[535.286, "o", "        positive_code=positive_code,\r\n"]
[535.296, "o", "        positive_dict=positive_dict,\r\n"]
[535.306, "o", "        transform_max_iter=method_max_iter,\r\n"]
[535.316, "o", "    )\r\n"]
[535.326, "o", "    code = estimator.fit_transform(X)\r\n"]
[535.336, "o", "    if return_n_iter:\r\n"]
[535.346, "o", "        return (\r\n"]
[535.356, "o", "            code,\r\n"]
[535.366, "o", "            estimator.components_,\r\n"]
[535.376, "o", "            estimator.error_,\r\n"]
[535.386, "o", "            estimator.n_iter_,\r\n"]
[535.396, "o", "        )\r\n"]
[535.406, "o", "    return code, estimator.components_, estimator.error_\r\n"]
[535.416, "o", "\r\n"]
[535.426, "o", "\r\n"]
[535.436, "o", "class _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n"]
[535.446, "o", "    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n"]
[535.456, "o", "\r\n"]
[535.466, "o", "    def __init__(\r\n"]
[535.476, "o", "        self,\r\n"]
[535.486, "o", "        transform_algorithm,\r\n"]
[535.496, "o", "        transform_n_nonzero_coefs,\r\n"]
[535.506, "o", "        transform_alpha,\r\n"]
[535.516, "o", "        split_sign,\r\n"]
[535.526, "o", "        n_jobs,\r\n"]
[535.536, "o", "        positive_code,\r\n"]
[535.546, "o", "        transform_max_iter,\r\n"]
[535.556, "o", "    ):\r\n"]
[535.566, "o", "        self.transform_algorithm = transform_algorithm\r\n"]
[535.576, "o", "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n"]
[535.586, "o", "        self.transform_alpha = transform_alpha\r\n"]
[535.596, "o", "        self.transform_max_iter = transform_max_iter\r\n"]
[535.606, "o", "        self.split_sign = split_sign\r\n"]
[535.616, "o", "        self.n_jobs = n_jobs\r\n"]
[535.626, "o", "        self.positive_code = positive_code\r\n"]
[535.636, "o", "\r\n"]
[535.646, "o", "    def _transform(self, X, dictionary):\r\n"]
[535.656, "o", "        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n"]
[535.666, "o", "        SparseCoder.\"\"\"\r\n"]
[535.676, "o", "        X = self._validate_data(X, reset=False)\r\n"]
[535.686, "o", "\r\n"]
[535.696, "o", "        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n"]
[535.706, "o", "            transform_alpha = self.alpha\r\n"]
[535.716, "o", "        else:\r\n"]
[535.726, "o", "            transform_alpha = self.transform_alpha\r\n"]
[535.736, "o", "\r\n"]
[535.746, "o", "        code = sparse_encode(\r\n"]
[535.756, "o", "            X,\r\n"]
[535.766, "o", "            dictionary,\r\n"]
[535.776, "o", "            algorithm=self.transform_algorithm,\r\n"]
[535.786, "o", "            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n"]
[535.796, "o", "            alpha=transform_alpha,\r\n"]
[535.806, "o", "            max_iter=self.transform_max_iter,\r\n"]
[535.816, "o", "            n_jobs=self.n_jobs,\r\n"]
[535.826, "o", "            positive=self.positive_code,\r\n"]
[535.836, "o", "        )\r\n"]
[535.846, "o", "\r\n"]
[535.856, "o", "        if self.split_sign:\r\n"]
[535.866, "o", "            # feature vector is split into a positive and negative side\r\n"]
[535.876, "o", "            n_samples, n_features = code.shape\r\n"]
[535.886, "o", "            split_code = np.empty((n_samples, 2 * n_features))\r\n"]
[535.896, "o", "            split_code[:, :n_features] = np.maximum(code, 0)\r\n"]
[535.906, "o", "            split_code[:, n_features:] = -np.minimum(code, 0)\r\n"]
[535.916, "o", "            code = split_code\r\n"]
[535.926, "o", "\r\n"]
[535.936, "o", "        return code\r\n"]
[535.946, "o", "\r\n"]
[535.956, "o", "    def transform(self, X):\r\n"]
[535.966, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[535.976, "o", "\r\n"]
[535.986, "o", "        Coding method is determined by the object parameter\r\n"]
[535.996, "o", "        `transform_algorithm`.\r\n"]
[536.006, "o", "\r\n"]
[536.016, "o", "        Parameters\r\n"]
[536.026, "o", "        ----------\r\n"]
[536.036, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[536.046, "o", "            Test data to be transformed, must have the same number of\r\n"]
[536.056, "o", "            features as the data used to train the model.\r\n"]
[536.066, "o", "\r\n"]
[536.076, "o", "        Returns\r\n"]
[536.086, "o", "        -------\r\n"]
[536.096, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[536.106, "o", "            Transformed data.\r\n"]
[536.116, "o", "        \"\"\"\r\n"]
[536.126, "o", "        check_is_fitted(self)\r\n"]
[536.136, "o", "        return self._transform(X, self.components_)\r\n"]
[536.146, "o", "\r\n"]
[536.156, "o", "\r\n"]
[536.166, "o", "class SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n"]
[536.176, "o", "    \"\"\"Sparse coding.\r\n"]
[536.186, "o", "\r\n"]
[536.196, "o", "    Finds a sparse representation of data against a fixed, precomputed\r\n"]
[536.206, "o", "    dictionary.\r\n"]
[536.216, "o", "\r\n"]
[536.226, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[536.236, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[536.246, "o", "\r\n"]
[536.256, "o", "        X ~= code * dictionary\r\n"]
[536.266, "o", "\r\n"]
[536.276, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[536.286, "o", "\r\n"]
[536.296, "o", "    Parameters\r\n"]
[536.306, "o", "    ----------\r\n"]
[536.316, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[536.326, "o", "        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n"]
[536.336, "o", "        normalized to unit norm.\r\n"]
[536.346, "o", "\r\n"]
[536.356, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[536.366, "o", "            'threshold'}, default='omp'\r\n"]
[536.376, "o", "        Algorithm used to transform the data:\r\n"]
[536.386, "o", "\r\n"]
[536.396, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[536.406, "o", "          (`linear_model.lars_path`);\r\n"]
[536.416, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[536.426, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[536.436, "o", "          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n"]
[536.446, "o", "          the estimated components are sparse;\r\n"]
[536.456, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[536.466, "o", "          solution;\r\n"]
[536.476, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[536.486, "o", "          the projection ``dictionary * X'``.\r\n"]
[536.496, "o", "\r\n"]
[536.506, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[536.516, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[536.526, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[536.536, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[536.546, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[536.556, "o", "\r\n"]
[536.566, "o", "    transform_alpha : float, default=None\r\n"]
[536.576, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[536.586, "o", "        penalty applied to the L1 norm.\r\n"]
[536.596, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[536.606, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[536.616, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[536.626, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[536.636, "o", "        `n_nonzero_coefs`.\r\n"]
[536.646, "o", "        If `None`, default to 1.\r\n"]
[536.656, "o", "\r\n"]
[536.666, "o", "    split_sign : bool, default=False\r\n"]
[536.676, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[536.686, "o", "        its negative part and its positive part. This can improve the\r\n"]
[536.696, "o", "        performance of downstream classifiers.\r\n"]
[536.706, "o", "\r\n"]
[536.716, "o", "    n_jobs : int, default=None\r\n"]
[536.726, "o", "        Number of parallel jobs to run.\r\n"]
[536.736, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[536.746, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[536.756, "o", "        for more details.\r\n"]
[536.766, "o", "\r\n"]
[536.776, "o", "    positive_code : bool, default=False\r\n"]
[536.786, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[536.796, "o", "\r\n"]
[536.806, "o", "        .. versionadded:: 0.20\r\n"]
[536.816, "o", "\r\n"]
[536.826, "o", "    transform_max_iter : int, default=1000\r\n"]
[536.836, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[536.846, "o", "        `lasso_lars`.\r\n"]
[536.856, "o", "\r\n"]
[536.866, "o", "        .. versionadded:: 0.22\r\n"]
[536.876, "o", "\r\n"]
[536.886, "o", "    Attributes\r\n"]
[536.896, "o", "    ----------\r\n"]
[536.906, "o", "    n_components_ : int\r\n"]
[536.916, "o", "        Number of atoms.\r\n"]
[536.926, "o", "\r\n"]
[536.936, "o", "    n_features_in_ : int\r\n"]
[536.946, "o", "        Number of features seen during :term:`fit`.\r\n"]
[536.956, "o", "\r\n"]
[536.966, "o", "        .. versionadded:: 0.24\r\n"]
[536.976, "o", "\r\n"]
[536.986, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[536.996, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[537.006, "o", "        has feature names that are all strings.\r\n"]
[537.016, "o", "\r\n"]
[537.026, "o", "        .. versionadded:: 1.0\r\n"]
[537.036, "o", "\r\n"]
[537.046, "o", "    See Also\r\n"]
[537.056, "o", "    --------\r\n"]
[537.066, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[537.076, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n"]
[537.086, "o", "        dictionary learning algorithm.\r\n"]
[537.096, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[537.106, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[537.116, "o", "    sparse_encode : Sparse coding where each row of the result is the solution\r\n"]
[537.126, "o", "        to a sparse coding problem.\r\n"]
[537.136, "o", "\r\n"]
[537.146, "o", "    Examples\r\n"]
[537.156, "o", "    --------\r\n"]
[537.166, "o", "    >>> import numpy as np\r\n"]
[537.176, "o", "    >>> from sklearn.decomposition import SparseCoder\r\n"]
[537.186, "o", "    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n"]
[537.196, "o", "    >>> dictionary = np.array(\r\n"]
[537.206, "o", "    ...     [[0, 1, 0],\r\n"]
[537.216, "o", "    ...      [-1, -1, 2],\r\n"]
[537.226, "o", "    ...      [1, 1, 1],\r\n"]
[537.236, "o", "    ...      [0, 1, 1],\r\n"]
[537.246, "o", "    ...      [0, 2, 1]],\r\n"]
[537.256, "o", "    ...    dtype=np.float64\r\n"]
[537.266, "o", "    ... )\r\n"]
[537.276, "o", "    >>> coder = SparseCoder(\r\n"]
[537.286, "o", "    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n"]
[537.296, "o", "    ...     transform_alpha=1e-10,\r\n"]
[537.306, "o", "    ... )\r\n"]
[537.316, "o", "    >>> coder.transform(X)\r\n"]
[537.326, "o", "    array([[ 0.,  0., -1.,  0.,  0.],\r\n"]
[537.336, "o", "           [ 0.,  1.,  1.,  0.,  0.]])\r\n"]
[537.346, "o", "    \"\"\"\r\n"]
[537.356, "o", "\r\n"]
[537.366, "o", "    _required_parameters = [\"dictionary\"]\r\n"]
[537.376, "o", "\r\n"]
[537.386, "o", "    def __init__(\r\n"]
[537.396, "o", "        self,\r\n"]
[537.406, "o", "        dictionary,\r\n"]
[537.416, "o", "        *,\r\n"]
[537.426, "o", "        transform_algorithm=\"omp\",\r\n"]
[537.436, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[537.446, "o", "        transform_alpha=None,\r\n"]
[537.456, "o", "        split_sign=False,\r\n"]
[537.466, "o", "        n_jobs=None,\r\n"]
[537.476, "o", "        positive_code=False,\r\n"]
[537.486, "o", "        transform_max_iter=1000,\r\n"]
[537.496, "o", "    ):\r\n"]
[537.506, "o", "        super().__init__(\r\n"]
[537.516, "o", "            transform_algorithm,\r\n"]
[537.526, "o", "            transform_n_nonzero_coefs,\r\n"]
[537.536, "o", "            transform_alpha,\r\n"]
[537.546, "o", "            split_sign,\r\n"]
[537.556, "o", "            n_jobs,\r\n"]
[537.566, "o", "            positive_code,\r\n"]
[537.576, "o", "            transform_max_iter,\r\n"]
[537.586, "o", "        )\r\n"]
[537.596, "o", "        self.dictionary = dictionary\r\n"]
[537.606, "o", "\r\n"]
[537.616, "o", "    def fit(self, X, y=None):\r\n"]
[537.626, "o", "        \"\"\"Do nothing and return the estimator unchanged.\r\n"]
[537.636, "o", "\r\n"]
[537.646, "o", "        This method is just there to implement the usual API and hence\r\n"]
[537.656, "o", "        work in pipelines.\r\n"]
[537.666, "o", "\r\n"]
[537.676, "o", "        Parameters\r\n"]
[537.686, "o", "        ----------\r\n"]
[537.696, "o", "        X : Ignored\r\n"]
[537.706, "o", "            Not used, present for API consistency by convention.\r\n"]
[537.716, "o", "\r\n"]
[537.726, "o", "        y : Ignored\r\n"]
[537.736, "o", "            Not used, present for API consistency by convention.\r\n"]
[537.746, "o", "\r\n"]
[537.756, "o", "        Returns\r\n"]
[537.766, "o", "        -------\r\n"]
[537.776, "o", "        self : object\r\n"]
[537.786, "o", "            Returns the instance itself.\r\n"]
[537.796, "o", "        \"\"\"\r\n"]
[537.806, "o", "        return self\r\n"]
[537.816, "o", "\r\n"]
[537.826, "o", "    def transform(self, X, y=None):\r\n"]
[537.836, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[537.846, "o", "\r\n"]
[537.856, "o", "        Coding method is determined by the object parameter\r\n"]
[537.866, "o", "        `transform_algorithm`.\r\n"]
[537.876, "o", "\r\n"]
[537.886, "o", "        Parameters\r\n"]
[537.896, "o", "        ----------\r\n"]
[537.906, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[537.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[537.926, "o", "            and `n_features` is the number of features.\r\n"]
[537.936, "o", "\r\n"]
[537.946, "o", "        y : Ignored\r\n"]
[537.956, "o", "            Not used, present for API consistency by convention.\r\n"]
[537.966, "o", "\r\n"]
[537.976, "o", "        Returns\r\n"]
[537.986, "o", "        -------\r\n"]
[537.996, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[538.006, "o", "            Transformed data.\r\n"]
[538.016, "o", "        \"\"\"\r\n"]
[538.026, "o", "        return super()._transform(X, self.dictionary)\r\n"]
[538.036, "o", "\r\n"]
[538.046, "o", "    def _more_tags(self):\r\n"]
[538.056, "o", "        return {\r\n"]
[538.066, "o", "            \"requires_fit\": False,\r\n"]
[538.076, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[538.086, "o", "        }\r\n"]
[538.096, "o", "\r\n"]
[538.106, "o", "    @property\r\n"]
[538.116, "o", "    def n_components_(self):\r\n"]
[538.126, "o", "        \"\"\"Number of atoms.\"\"\"\r\n"]
[538.136, "o", "        return self.dictionary.shape[0]\r\n"]
[538.146, "o", "\r\n"]
[538.156, "o", "    @property\r\n"]
[538.166, "o", "    def n_features_in_(self):\r\n"]
[538.176, "o", "        \"\"\"Number of features seen during `fit`.\"\"\"\r\n"]
[538.186, "o", "        return self.dictionary.shape[1]\r\n"]
[538.196, "o", "\r\n"]
[538.206, "o", "    @property\r\n"]
[538.216, "o", "    def _n_features_out(self):\r\n"]
[538.226, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[538.236, "o", "        return self.n_components_\r\n"]
[538.246, "o", "\r\n"]
[538.256, "o", "\r\n"]
[538.266, "o", "class DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[538.276, "o", "    \"\"\"Dictionary learning.\r\n"]
[538.286, "o", "\r\n"]
[538.296, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[538.306, "o", "    encoding the fitted data.\r\n"]
[538.316, "o", "\r\n"]
[538.326, "o", "    Solves the optimization problem::\r\n"]
[538.336, "o", "\r\n"]
[538.346, "o", "        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[538.356, "o", "                    (U,V)\r\n"]
[538.366, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[538.376, "o", "\r\n"]
[538.386, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[538.396, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[538.406, "o", "    of all the entries in the matrix.\r\n"]
[538.416, "o", "\r\n"]
[538.426, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[538.436, "o", "\r\n"]
[538.446, "o", "    Parameters\r\n"]
[538.456, "o", "    ----------\r\n"]
[538.466, "o", "    n_components : int, default=None\r\n"]
[538.476, "o", "        Number of dictionary elements to extract. If None, then ``n_components``\r\n"]
[538.486, "o", "        is set to ``n_features``.\r\n"]
[538.496, "o", "\r\n"]
[538.506, "o", "    alpha : float, default=1.0\r\n"]
[538.516, "o", "        Sparsity controlling parameter.\r\n"]
[538.526, "o", "\r\n"]
[538.536, "o", "    max_iter : int, default=1000\r\n"]
[538.546, "o", "        Maximum number of iterations to perform.\r\n"]
[538.556, "o", "\r\n"]
[538.566, "o", "    tol : float, default=1e-8\r\n"]
[538.576, "o", "        Tolerance for numerical error.\r\n"]
[538.586, "o", "\r\n"]
[538.596, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[538.606, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[538.616, "o", "          problem (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[538.626, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[538.636, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n"]
[538.646, "o", "          faster if the estimated components are sparse.\r\n"]
[538.656, "o", "\r\n"]
[538.666, "o", "        .. versionadded:: 0.17\r\n"]
[538.676, "o", "           *cd* coordinate descent method to improve speed.\r\n"]
[538.686, "o", "\r\n"]
[538.696, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[538.706, "o", "            'threshold'}, default='omp'\r\n"]
[538.716, "o", "        Algorithm used to transform the data:\r\n"]
[538.726, "o", "\r\n"]
[538.736, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[538.746, "o", "          (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[538.756, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[538.766, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[538.776, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n"]
[538.786, "o", "          will be faster if the estimated components are sparse.\r\n"]
[538.796, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[538.806, "o", "          solution.\r\n"]
[538.816, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[538.826, "o", "          the projection ``dictionary * X'``.\r\n"]
[538.836, "o", "\r\n"]
[538.846, "o", "        .. versionadded:: 0.17\r\n"]
[538.856, "o", "           *lasso_cd* coordinate descent method to improve speed.\r\n"]
[538.866, "o", "\r\n"]
[538.876, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[538.886, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[538.896, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[538.906, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[538.916, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[538.926, "o", "\r\n"]
[538.936, "o", "    transform_alpha : float, default=None\r\n"]
[538.946, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[538.956, "o", "        penalty applied to the L1 norm.\r\n"]
[538.966, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[538.976, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[538.986, "o", "        If `None`, defaults to `alpha`.\r\n"]
[538.996, "o", "\r\n"]
[539.006, "o", "        .. versionchanged:: 1.2\r\n"]
[539.016, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[539.026, "o", "\r\n"]
[539.036, "o", "    n_jobs : int or None, default=None\r\n"]
[539.046, "o", "        Number of parallel jobs to run.\r\n"]
[539.056, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[539.066, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[539.076, "o", "        for more details.\r\n"]
[539.086, "o", "\r\n"]
[539.096, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[539.106, "o", "        Initial value for the code, for warm restart. Only used if `code_init`\r\n"]
[539.116, "o", "        and `dict_init` are not None.\r\n"]
[539.126, "o", "\r\n"]
[539.136, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[539.146, "o", "        Initial values for the dictionary, for warm restart. Only used if\r\n"]
[539.156, "o", "        `code_init` and `dict_init` are not None.\r\n"]
[539.166, "o", "\r\n"]
[539.176, "o", "    callback : callable, default=None\r\n"]
[539.186, "o", "        Callable that gets invoked every five iterations.\r\n"]
[539.196, "o", "\r\n"]
[539.206, "o", "        .. versionadded:: 1.3\r\n"]
[539.216, "o", "\r\n"]
[539.226, "o", "    verbose : bool, default=False\r\n"]
[539.236, "o", "        To control the verbosity of the procedure.\r\n"]
[539.246, "o", "\r\n"]
[539.256, "o", "    split_sign : bool, default=False\r\n"]
[539.266, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[539.276, "o", "        its negative part and its positive part. This can improve the\r\n"]
[539.286, "o", "        performance of downstream classifiers.\r\n"]
[539.296, "o", "\r\n"]
[539.306, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[539.316, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[539.326, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[539.336, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[539.346, "o", "        results across multiple function calls.\r\n"]
[539.356, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[539.366, "o", "\r\n"]
[539.376, "o", "    positive_code : bool, default=False\r\n"]
[539.386, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[539.396, "o", "\r\n"]
[539.406, "o", "        .. versionadded:: 0.20\r\n"]
[539.416, "o", "\r\n"]
[539.426, "o", "    positive_dict : bool, default=False\r\n"]
[539.436, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[539.446, "o", "\r\n"]
[539.456, "o", "        .. versionadded:: 0.20\r\n"]
[539.466, "o", "\r\n"]
[539.476, "o", "    transform_max_iter : int, default=1000\r\n"]
[539.486, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[539.496, "o", "        `'lasso_lars'`.\r\n"]
[539.506, "o", "\r\n"]
[539.516, "o", "        .. versionadded:: 0.22\r\n"]
[539.526, "o", "\r\n"]
[539.536, "o", "    Attributes\r\n"]
[539.546, "o", "    ----------\r\n"]
[539.556, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[539.566, "o", "        dictionary atoms extracted from the data\r\n"]
[539.576, "o", "\r\n"]
[539.586, "o", "    error_ : array\r\n"]
[539.596, "o", "        vector of errors at each iteration\r\n"]
[539.606, "o", "\r\n"]
[539.616, "o", "    n_features_in_ : int\r\n"]
[539.626, "o", "        Number of features seen during :term:`fit`.\r\n"]
[539.636, "o", "\r\n"]
[539.646, "o", "        .. versionadded:: 0.24\r\n"]
[539.656, "o", "\r\n"]
[539.666, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[539.676, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[539.686, "o", "        has feature names that are all strings.\r\n"]
[539.696, "o", "\r\n"]
[539.706, "o", "        .. versionadded:: 1.0\r\n"]
[539.716, "o", "\r\n"]
[539.726, "o", "    n_iter_ : int\r\n"]
[539.736, "o", "        Number of iterations run.\r\n"]
[539.746, "o", "\r\n"]
[539.756, "o", "    See Also\r\n"]
[539.766, "o", "    --------\r\n"]
[539.776, "o", "    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n"]
[539.786, "o", "        dictionary learning algorithm.\r\n"]
[539.796, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[539.806, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[539.816, "o", "        precomputed dictionary.\r\n"]
[539.826, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[539.836, "o", "\r\n"]
[539.846, "o", "    References\r\n"]
[539.856, "o", "    ----------\r\n"]
[539.866, "o", "\r\n"]
[539.876, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[539.886, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[539.896, "o", "\r\n"]
[539.906, "o", "    Examples\r\n"]
[539.916, "o", "    --------\r\n"]
[539.926, "o", "    >>> import numpy as np\r\n"]
[539.936, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[540.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[540.002, "i", "cd asv_benchmarks\r"]
[540.004, "o", "cd asv_benchmarks\r\n"]
[540.006, "o", "\u001b[?2004l\r\n"]
[545.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[545.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[545.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[546.654, "o", "\u001b[?2004l\r\n"]
[548.302, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[550.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[550.002, "i", "cd ..\r"]
[550.004, "o", "cd ..\r\n"]
[550.006, "o", "\u001b[?2004l\r\n"]
[555.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[555.002, "i", "grep -n \"_minibatch_step\" sklearn/decomposition/_dict_learning.py\r"]
[555.004, "o", "grep -n \"_minibatch_step\" sklearn/decomposition/_dict_learning.py\r\n"]
[555.9948, "o", "\u001b[?2004l\r\n"]
[556.9836, "o", "\u001b[32m\u001b[K2219\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    def \u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(self, X, dictionary, random_state, step):\r\n"]
[557.9724, "o", "\u001b[32m\u001b[K2399\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                batch_cost = self.\u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(\r\n"]
[558.9612, "o", "\u001b[32m\u001b[K2425\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                self.\u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(X_train[batch], dictionary, self._random_state, i)\r\n"]
[560.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[560.002, "i", "grep -n \"_sparse_encode_precomputed\\|copy_cov=\" sklearn/decomposition/_dict_learning.py\r"]
[560.004, "o", "grep -n \"_sparse_encode_precomputed\\|copy_cov=\" sklearn/decomposition/_dict_learning.py\r\n"]
[560.5004, "o", "\u001b[?2004l\r\n"]
[560.9948, "o", "\u001b[32m\u001b[K38\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdef \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n"]
[561.4892, "o", "\u001b[32m\u001b[K46\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kcopy_cov=\u001b[m\u001b[KTrue,\r\n"]
[561.9836, "o", "\u001b[32m\u001b[K236\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kcopy_cov=\u001b[m\u001b[KTrue,\r\n"]
[562.478, "o", "\u001b[32m\u001b[K369\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kcopy_cov=\u001b[m\u001b[Kcopy_cov,\r\n"]
[562.9724, "o", "\u001b[32m\u001b[K387\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kcopy_cov=\u001b[m\u001b[KTrue,\r\n"]
[563.4668, "o", "\u001b[32m\u001b[K416\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        code = \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n"]
[563.9612, "o", "\u001b[32m\u001b[K423\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            \u001b[01;31m\u001b[Kcopy_cov=\u001b[m\u001b[Kcopy_cov,\r\n"]
[564.4556, "o", "\u001b[32m\u001b[K438\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        delayed(\u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K)(\r\n"]
[565.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[565.002, "i", "sed -n '1,600p' sklearn/decomposition/_dict_learning.py\r"]
[565.004, "o", "sed -n '1,600p' sklearn/decomposition/_dict_learning.py\r\n"]
[565.016, "o", "\u001b[?2004l\r\n"]
[565.026, "o", "\"\"\" Dictionary learning.\r\n"]
[565.036, "o", "\"\"\"\r\n"]
[565.046, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[565.056, "o", "# License: BSD 3 clause\r\n"]
[565.066, "o", "\r\n"]
[565.076, "o", "import itertools\r\n"]
[565.086, "o", "import sys\r\n"]
[565.096, "o", "import time\r\n"]
[565.106, "o", "import warnings\r\n"]
[565.116, "o", "from math import ceil\r\n"]
[565.126, "o", "from numbers import Integral, Real\r\n"]
[565.136, "o", "\r\n"]
[565.146, "o", "import numpy as np\r\n"]
[565.156, "o", "from joblib import effective_n_jobs\r\n"]
[565.166, "o", "from scipy import linalg\r\n"]
[565.176, "o", "\r\n"]
[565.186, "o", "from ..base import (\r\n"]
[565.196, "o", "    BaseEstimator,\r\n"]
[565.206, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[565.216, "o", "    TransformerMixin,\r\n"]
[565.226, "o", "    _fit_context,\r\n"]
[565.236, "o", ")\r\n"]
[565.246, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[565.256, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[565.266, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[565.276, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[565.286, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[565.296, "o", "from ..utils.validation import check_is_fitted\r\n"]
[565.306, "o", "\r\n"]
[565.316, "o", "\r\n"]
[565.326, "o", "def _check_positive_coding(method, positive):\r\n"]
[565.336, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[565.346, "o", "        raise ValueError(\r\n"]
[565.356, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[565.366, "o", "        )\r\n"]
[565.376, "o", "\r\n"]
[565.386, "o", "\r\n"]
[565.396, "o", "def _sparse_encode_precomputed(\r\n"]
[565.406, "o", "    X,\r\n"]
[565.416, "o", "    dictionary,\r\n"]
[565.426, "o", "    *,\r\n"]
[565.436, "o", "    gram=None,\r\n"]
[565.446, "o", "    cov=None,\r\n"]
[565.456, "o", "    algorithm=\"lasso_lars\",\r\n"]
[565.466, "o", "    regularization=None,\r\n"]
[565.476, "o", "    copy_cov=True,\r\n"]
[565.486, "o", "    init=None,\r\n"]
[565.496, "o", "    max_iter=1000,\r\n"]
[565.506, "o", "    verbose=0,\r\n"]
[565.516, "o", "    positive=False,\r\n"]
[565.526, "o", "):\r\n"]
[565.536, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[565.546, "o", "\r\n"]
[565.556, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[565.566, "o", "\r\n"]
[565.576, "o", "    Parameters\r\n"]
[565.586, "o", "    ----------\r\n"]
[565.596, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[565.606, "o", "        Data matrix.\r\n"]
[565.616, "o", "\r\n"]
[565.626, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[565.636, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[565.646, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[565.656, "o", "\r\n"]
[565.666, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[565.676, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[565.686, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[565.696, "o", "\r\n"]
[565.706, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[565.716, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[565.726, "o", "\r\n"]
[565.736, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[565.746, "o", "            default='lasso_lars'\r\n"]
[565.756, "o", "        The algorithm used:\r\n"]
[565.766, "o", "\r\n"]
[565.776, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[565.786, "o", "          (`linear_model.lars_path`);\r\n"]
[565.796, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[565.806, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[565.816, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[565.826, "o", "          the estimated components are sparse;\r\n"]
[565.836, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[565.846, "o", "          solution;\r\n"]
[565.856, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[565.866, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[565.876, "o", "\r\n"]
[565.886, "o", "    regularization : int or float, default=None\r\n"]
[565.896, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[565.906, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[565.916, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[565.926, "o", "\r\n"]
[565.936, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[565.946, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[565.956, "o", "        `algorithm='lasso_cd'`.\r\n"]
[565.966, "o", "\r\n"]
[565.976, "o", "    max_iter : int, default=1000\r\n"]
[565.986, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[565.996, "o", "        `'lasso_lars'`.\r\n"]
[566.006, "o", "\r\n"]
[566.016, "o", "    copy_cov : bool, default=True\r\n"]
[566.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[566.036, "o", "        be overwritten.\r\n"]
[566.046, "o", "\r\n"]
[566.056, "o", "    verbose : int, default=0\r\n"]
[566.066, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[566.076, "o", "\r\n"]
[566.086, "o", "    positive: bool, default=False\r\n"]
[566.096, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[566.106, "o", "\r\n"]
[566.116, "o", "        .. versionadded:: 0.20\r\n"]
[566.126, "o", "\r\n"]
[566.136, "o", "    Returns\r\n"]
[566.146, "o", "    -------\r\n"]
[566.156, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[566.166, "o", "        The sparse codes.\r\n"]
[566.176, "o", "    \"\"\"\r\n"]
[566.186, "o", "    n_samples, n_features = X.shape\r\n"]
[566.196, "o", "    n_components = dictionary.shape[0]\r\n"]
[566.206, "o", "\r\n"]
[566.216, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[566.226, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[566.236, "o", "        try:\r\n"]
[566.246, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[566.256, "o", "\r\n"]
[566.266, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[566.276, "o", "            # corrects the verbosity level.\r\n"]
[566.286, "o", "            lasso_lars = LassoLars(\r\n"]
[566.296, "o", "                alpha=alpha,\r\n"]
[566.306, "o", "                fit_intercept=False,\r\n"]
[566.316, "o", "                verbose=verbose,\r\n"]
[566.326, "o", "                precompute=gram,\r\n"]
[566.336, "o", "                fit_path=False,\r\n"]
[566.346, "o", "                positive=positive,\r\n"]
[566.356, "o", "                max_iter=max_iter,\r\n"]
[566.366, "o", "            )\r\n"]
[566.376, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[566.386, "o", "            new_code = lasso_lars.coef_\r\n"]
[566.396, "o", "        finally:\r\n"]
[566.406, "o", "            np.seterr(**err_mgt)\r\n"]
[566.416, "o", "\r\n"]
[566.426, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[566.436, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[566.446, "o", "\r\n"]
[566.456, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[566.466, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[566.476, "o", "        # argument that we could pass in from Lasso.\r\n"]
[566.486, "o", "        clf = Lasso(\r\n"]
[566.496, "o", "            alpha=alpha,\r\n"]
[566.506, "o", "            fit_intercept=False,\r\n"]
[566.516, "o", "            precompute=gram,\r\n"]
[566.526, "o", "            max_iter=max_iter,\r\n"]
[566.536, "o", "            warm_start=True,\r\n"]
[566.546, "o", "            positive=positive,\r\n"]
[566.556, "o", "        )\r\n"]
[566.566, "o", "\r\n"]
[566.576, "o", "        if init is not None:\r\n"]
[566.586, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[566.596, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[566.606, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[566.616, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[566.626, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[566.636, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[566.646, "o", "                init = np.array(init)\r\n"]
[566.656, "o", "            clf.coef_ = init\r\n"]
[566.666, "o", "\r\n"]
[566.676, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[566.686, "o", "        new_code = clf.coef_\r\n"]
[566.696, "o", "\r\n"]
[566.706, "o", "    elif algorithm == \"lars\":\r\n"]
[566.716, "o", "        try:\r\n"]
[566.726, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[566.736, "o", "\r\n"]
[566.746, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[566.756, "o", "            # corrects the verbosity level.\r\n"]
[566.766, "o", "            lars = Lars(\r\n"]
[566.776, "o", "                fit_intercept=False,\r\n"]
[566.786, "o", "                verbose=verbose,\r\n"]
[566.796, "o", "                precompute=gram,\r\n"]
[566.806, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[566.816, "o", "                fit_path=False,\r\n"]
[566.826, "o", "            )\r\n"]
[566.836, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[566.846, "o", "            new_code = lars.coef_\r\n"]
[566.856, "o", "        finally:\r\n"]
[566.866, "o", "            np.seterr(**err_mgt)\r\n"]
[566.876, "o", "\r\n"]
[566.886, "o", "    elif algorithm == \"threshold\":\r\n"]
[566.896, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[566.906, "o", "        if positive:\r\n"]
[566.916, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[566.926, "o", "\r\n"]
[566.936, "o", "    elif algorithm == \"omp\":\r\n"]
[566.946, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[566.956, "o", "            Gram=gram,\r\n"]
[566.966, "o", "            Xy=cov,\r\n"]
[566.976, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[566.986, "o", "            tol=None,\r\n"]
[566.996, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[567.006, "o", "            copy_Xy=copy_cov,\r\n"]
[567.016, "o", "        ).T\r\n"]
[567.026, "o", "\r\n"]
[567.036, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[567.046, "o", "\r\n"]
[567.056, "o", "\r\n"]
[567.066, "o", "@validate_params(\r\n"]
[567.076, "o", "    {\r\n"]
[567.086, "o", "        \"X\": [\"array-like\"],\r\n"]
[567.096, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[567.106, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[567.116, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[567.126, "o", "        \"algorithm\": [\r\n"]
[567.136, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[567.146, "o", "        ],\r\n"]
[567.156, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[567.166, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[567.176, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[567.186, "o", "        \"init\": [\"array-like\", None],\r\n"]
[567.196, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[567.206, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[567.216, "o", "        \"check_input\": [\"boolean\"],\r\n"]
[567.226, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[567.236, "o", "        \"positive\": [\"boolean\"],\r\n"]
[567.246, "o", "    },\r\n"]
[567.256, "o", "    prefer_skip_nested_validation=True,\r\n"]
[567.266, "o", ")\r\n"]
[567.276, "o", "# XXX : could be moved to the linear_model module\r\n"]
[567.286, "o", "def sparse_encode(\r\n"]
[567.296, "o", "    X,\r\n"]
[567.306, "o", "    dictionary,\r\n"]
[567.316, "o", "    *,\r\n"]
[567.326, "o", "    gram=None,\r\n"]
[567.336, "o", "    cov=None,\r\n"]
[567.346, "o", "    algorithm=\"lasso_lars\",\r\n"]
[567.356, "o", "    n_nonzero_coefs=None,\r\n"]
[567.366, "o", "    alpha=None,\r\n"]
[567.376, "o", "    copy_cov=True,\r\n"]
[567.386, "o", "    init=None,\r\n"]
[567.396, "o", "    max_iter=1000,\r\n"]
[567.406, "o", "    n_jobs=None,\r\n"]
[567.416, "o", "    check_input=True,\r\n"]
[567.426, "o", "    verbose=0,\r\n"]
[567.436, "o", "    positive=False,\r\n"]
[567.446, "o", "):\r\n"]
[567.456, "o", "    \"\"\"Sparse coding.\r\n"]
[567.466, "o", "\r\n"]
[567.476, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[567.486, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[567.496, "o", "\r\n"]
[567.506, "o", "        X ~= code * dictionary\r\n"]
[567.516, "o", "\r\n"]
[567.526, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[567.536, "o", "\r\n"]
[567.546, "o", "    Parameters\r\n"]
[567.556, "o", "    ----------\r\n"]
[567.566, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[567.576, "o", "        Data matrix.\r\n"]
[567.586, "o", "\r\n"]
[567.596, "o", "    dictionary : array-like of shape (n_components, n_features)\r\n"]
[567.606, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[567.616, "o", "        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[567.626, "o", "        output.\r\n"]
[567.636, "o", "\r\n"]
[567.646, "o", "    gram : array-like of shape (n_components, n_components), default=None\r\n"]
[567.656, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n"]
[567.666, "o", "\r\n"]
[567.676, "o", "    cov : array-like of shape (n_components, n_samples), default=None\r\n"]
[567.686, "o", "        Precomputed covariance, `dictionary' * X`.\r\n"]
[567.696, "o", "\r\n"]
[567.706, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[567.716, "o", "            default='lasso_lars'\r\n"]
[567.726, "o", "        The algorithm used:\r\n"]
[567.736, "o", "\r\n"]
[567.746, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[567.756, "o", "          (`linear_model.lars_path`);\r\n"]
[567.766, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[567.776, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[567.786, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[567.796, "o", "          the estimated components are sparse;\r\n"]
[567.806, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[567.816, "o", "          solution;\r\n"]
[567.826, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[567.836, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[567.846, "o", "\r\n"]
[567.856, "o", "    n_nonzero_coefs : int, default=None\r\n"]
[567.866, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[567.876, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[567.886, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[567.896, "o", "        `n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[567.906, "o", "\r\n"]
[567.916, "o", "    alpha : float, default=None\r\n"]
[567.926, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[567.936, "o", "        penalty applied to the L1 norm.\r\n"]
[567.946, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[567.956, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[567.966, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[567.976, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[567.986, "o", "        `n_nonzero_coefs`.\r\n"]
[567.996, "o", "        If `None`, default to 1.\r\n"]
[568.006, "o", "\r\n"]
[568.016, "o", "    copy_cov : bool, default=True\r\n"]
[568.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[568.036, "o", "        be overwritten.\r\n"]
[568.046, "o", "\r\n"]
[568.056, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[568.066, "o", "        Initialization value of the sparse codes. Only used if\r\n"]
[568.076, "o", "        `algorithm='lasso_cd'`.\r\n"]
[568.086, "o", "\r\n"]
[568.096, "o", "    max_iter : int, default=1000\r\n"]
[568.106, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[568.116, "o", "        `'lasso_lars'`.\r\n"]
[568.126, "o", "\r\n"]
[568.136, "o", "    n_jobs : int, default=None\r\n"]
[568.146, "o", "        Number of parallel jobs to run.\r\n"]
[568.156, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[568.166, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[568.176, "o", "        for more details.\r\n"]
[568.186, "o", "\r\n"]
[568.196, "o", "    check_input : bool, default=True\r\n"]
[568.206, "o", "        If `False`, the input arrays X and dictionary will not be checked.\r\n"]
[568.216, "o", "\r\n"]
[568.226, "o", "    verbose : int, default=0\r\n"]
[568.236, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[568.246, "o", "\r\n"]
[568.256, "o", "    positive : bool, default=False\r\n"]
[568.266, "o", "        Whether to enforce positivity when finding the encoding.\r\n"]
[568.276, "o", "\r\n"]
[568.286, "o", "        .. versionadded:: 0.20\r\n"]
[568.296, "o", "\r\n"]
[568.306, "o", "    Returns\r\n"]
[568.316, "o", "    -------\r\n"]
[568.326, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[568.336, "o", "        The sparse codes.\r\n"]
[568.346, "o", "\r\n"]
[568.356, "o", "    See Also\r\n"]
[568.366, "o", "    --------\r\n"]
[568.376, "o", "    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n"]
[568.386, "o", "        path using LARS algorithm.\r\n"]
[568.396, "o", "    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n"]
[568.406, "o", "    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n"]
[568.416, "o", "    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n"]
[568.426, "o", "        dictionary.\r\n"]
[568.436, "o", "    \"\"\"\r\n"]
[568.446, "o", "    if check_input:\r\n"]
[568.456, "o", "        if algorithm == \"lasso_cd\":\r\n"]
[568.466, "o", "            dictionary = check_array(\r\n"]
[568.476, "o", "                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n"]
[568.486, "o", "            )\r\n"]
[568.496, "o", "            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n"]
[568.506, "o", "        else:\r\n"]
[568.516, "o", "            dictionary = check_array(dictionary)\r\n"]
[568.526, "o", "            X = check_array(X)\r\n"]
[568.536, "o", "\r\n"]
[568.546, "o", "    if dictionary.shape[1] != X.shape[1]:\r\n"]
[568.556, "o", "        raise ValueError(\r\n"]
[568.566, "o", "            \"Dictionary and X have different numbers of features:\"\r\n"]
[568.576, "o", "            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n"]
[568.586, "o", "        )\r\n"]
[568.596, "o", "\r\n"]
[568.606, "o", "    _check_positive_coding(algorithm, positive)\r\n"]
[568.616, "o", "\r\n"]
[568.626, "o", "    return _sparse_encode(\r\n"]
[568.636, "o", "        X,\r\n"]
[568.646, "o", "        dictionary,\r\n"]
[568.656, "o", "        gram=gram,\r\n"]
[568.666, "o", "        cov=cov,\r\n"]
[568.676, "o", "        algorithm=algorithm,\r\n"]
[568.686, "o", "        n_nonzero_coefs=n_nonzero_coefs,\r\n"]
[568.696, "o", "        alpha=alpha,\r\n"]
[568.706, "o", "        copy_cov=copy_cov,\r\n"]
[568.716, "o", "        init=init,\r\n"]
[568.726, "o", "        max_iter=max_iter,\r\n"]
[568.736, "o", "        n_jobs=n_jobs,\r\n"]
[568.746, "o", "        verbose=verbose,\r\n"]
[568.756, "o", "        positive=positive,\r\n"]
[568.766, "o", "    )\r\n"]
[568.776, "o", "\r\n"]
[568.786, "o", "\r\n"]
[568.796, "o", "def _sparse_encode(\r\n"]
[568.806, "o", "    X,\r\n"]
[568.816, "o", "    dictionary,\r\n"]
[568.826, "o", "    *,\r\n"]
[568.836, "o", "    gram=None,\r\n"]
[568.846, "o", "    cov=None,\r\n"]
[568.856, "o", "    algorithm=\"lasso_lars\",\r\n"]
[568.866, "o", "    n_nonzero_coefs=None,\r\n"]
[568.876, "o", "    alpha=None,\r\n"]
[568.886, "o", "    copy_cov=True,\r\n"]
[568.896, "o", "    init=None,\r\n"]
[568.906, "o", "    max_iter=1000,\r\n"]
[568.916, "o", "    n_jobs=None,\r\n"]
[568.926, "o", "    verbose=0,\r\n"]
[568.936, "o", "    positive=False,\r\n"]
[568.946, "o", "):\r\n"]
[568.956, "o", "    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[568.966, "o", "\r\n"]
[568.976, "o", "    n_samples, n_features = X.shape\r\n"]
[568.986, "o", "    n_components = dictionary.shape[0]\r\n"]
[568.996, "o", "\r\n"]
[569.006, "o", "    if algorithm in (\"lars\", \"omp\"):\r\n"]
[569.016, "o", "        regularization = n_nonzero_coefs\r\n"]
[569.026, "o", "        if regularization is None:\r\n"]
[569.036, "o", "            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[569.046, "o", "    else:\r\n"]
[569.056, "o", "        regularization = alpha\r\n"]
[569.066, "o", "        if regularization is None:\r\n"]
[569.076, "o", "            regularization = 1.0\r\n"]
[569.086, "o", "\r\n"]
[569.096, "o", "    if gram is None and algorithm != \"threshold\":\r\n"]
[569.106, "o", "        gram = np.dot(dictionary, dictionary.T)\r\n"]
[569.116, "o", "\r\n"]
[569.126, "o", "    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[569.136, "o", "        copy_cov = False\r\n"]
[569.146, "o", "        cov = np.dot(dictionary, X.T)\r\n"]
[569.156, "o", "\r\n"]
[569.166, "o", "    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[569.176, "o", "        code = _sparse_encode_precomputed(\r\n"]
[569.186, "o", "            X,\r\n"]
[569.196, "o", "            dictionary,\r\n"]
[569.206, "o", "            gram=gram,\r\n"]
[569.216, "o", "            cov=cov,\r\n"]
[569.226, "o", "            algorithm=algorithm,\r\n"]
[569.236, "o", "            regularization=regularization,\r\n"]
[569.246, "o", "            copy_cov=copy_cov,\r\n"]
[569.256, "o", "            init=init,\r\n"]
[569.266, "o", "            max_iter=max_iter,\r\n"]
[569.276, "o", "            verbose=verbose,\r\n"]
[569.286, "o", "            positive=positive,\r\n"]
[569.296, "o", "        )\r\n"]
[569.306, "o", "        return code\r\n"]
[569.316, "o", "\r\n"]
[569.326, "o", "    # Enter parallel code block\r\n"]
[569.336, "o", "    n_samples = X.shape[0]\r\n"]
[569.346, "o", "    n_components = dictionary.shape[0]\r\n"]
[569.356, "o", "    code = np.empty((n_samples, n_components))\r\n"]
[569.366, "o", "    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[569.376, "o", "\r\n"]
[569.386, "o", "    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[569.396, "o", "        delayed(_sparse_encode_precomputed)(\r\n"]
[569.406, "o", "            X[this_slice],\r\n"]
[569.416, "o", "            dictionary,\r\n"]
[569.426, "o", "            gram=gram,\r\n"]
[569.436, "o", "            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[569.446, "o", "            algorithm=algorithm,\r\n"]
[569.456, "o", "            regularization=regularization,\r\n"]
[569.466, "o", "            copy_cov=copy_cov,\r\n"]
[569.476, "o", "            init=init[this_slice] if init is not None else None,\r\n"]
[569.486, "o", "            max_iter=max_iter,\r\n"]
[569.496, "o", "            verbose=verbose,\r\n"]
[569.506, "o", "            positive=positive,\r\n"]
[569.516, "o", "        )\r\n"]
[569.526, "o", "        for this_slice in slices\r\n"]
[569.536, "o", "    )\r\n"]
[569.546, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[569.556, "o", "        code[this_slice] = this_view\r\n"]
[569.566, "o", "    return code\r\n"]
[569.576, "o", "\r\n"]
[569.586, "o", "\r\n"]
[569.596, "o", "def _update_dict(\r\n"]
[569.606, "o", "    dictionary,\r\n"]
[569.616, "o", "    Y,\r\n"]
[569.626, "o", "    code,\r\n"]
[569.636, "o", "    A=None,\r\n"]
[569.646, "o", "    B=None,\r\n"]
[569.656, "o", "    verbose=False,\r\n"]
[569.666, "o", "    random_state=None,\r\n"]
[569.676, "o", "    positive=False,\r\n"]
[569.686, "o", "):\r\n"]
[569.696, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[569.706, "o", "\r\n"]
[569.716, "o", "    Parameters\r\n"]
[569.726, "o", "    ----------\r\n"]
[569.736, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[569.746, "o", "        Value of the dictionary at the previous iteration.\r\n"]
[569.756, "o", "\r\n"]
[569.766, "o", "    Y : ndarray of shape (n_samples, n_features)\r\n"]
[569.776, "o", "        Data matrix.\r\n"]
[569.786, "o", "\r\n"]
[569.796, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[569.806, "o", "        Sparse coding of the data against which to optimize the dictionary.\r\n"]
[569.816, "o", "\r\n"]
[569.826, "o", "    A : ndarray of shape (n_components, n_components), default=None\r\n"]
[569.836, "o", "        Together with `B`, sufficient stats of the online model to update the\r\n"]
[569.846, "o", "        dictionary.\r\n"]
[569.856, "o", "\r\n"]
[569.866, "o", "    B : ndarray of shape (n_features, n_components), default=None\r\n"]
[569.876, "o", "        Together with `A`, sufficient stats of the online model to update the\r\n"]
[569.886, "o", "        dictionary.\r\n"]
[569.896, "o", "\r\n"]
[569.906, "o", "    verbose: bool, default=False\r\n"]
[569.916, "o", "        Degree of output the procedure will print.\r\n"]
[569.926, "o", "\r\n"]
[569.936, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[570.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[570.002, "i", "sed -n '600,1200p' sklearn/decomposition/_dict_learning.py\r"]
[570.004, "o", "sed -n '600,1200p' sklearn/decomposition/_dict_learning.py\r\n"]
[570.016, "o", "\u001b[?2004l\r\n"]
[570.026, "o", "                % (ii, dt, dt / 60, current_cost)\r\n"]
[570.036, "o", "            )\r\n"]
[570.046, "o", "\r\n"]
[570.056, "o", "        # Update code\r\n"]
[570.066, "o", "        code = sparse_encode(\r\n"]
[570.076, "o", "            X,\r\n"]
[570.086, "o", "            dictionary,\r\n"]
[570.096, "o", "            algorithm=method,\r\n"]
[570.106, "o", "            alpha=alpha,\r\n"]
[570.116, "o", "            init=code,\r\n"]
[570.126, "o", "            n_jobs=n_jobs,\r\n"]
[570.136, "o", "            positive=positive_code,\r\n"]
[570.146, "o", "            max_iter=method_max_iter,\r\n"]
[570.156, "o", "            verbose=verbose,\r\n"]
[570.166, "o", "        )\r\n"]
[570.176, "o", "\r\n"]
[570.186, "o", "        # Update dictionary in place\r\n"]
[570.196, "o", "        _update_dict(\r\n"]
[570.206, "o", "            dictionary,\r\n"]
[570.216, "o", "            X,\r\n"]
[570.226, "o", "            code,\r\n"]
[570.236, "o", "            verbose=verbose,\r\n"]
[570.246, "o", "            random_state=random_state,\r\n"]
[570.256, "o", "            positive=positive_dict,\r\n"]
[570.266, "o", "        )\r\n"]
[570.276, "o", "\r\n"]
[570.286, "o", "        # Cost function\r\n"]
[570.296, "o", "        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n"]
[570.306, "o", "            np.abs(code)\r\n"]
[570.316, "o", "        )\r\n"]
[570.326, "o", "        errors.append(current_cost)\r\n"]
[570.336, "o", "\r\n"]
[570.346, "o", "        if ii > 0:\r\n"]
[570.356, "o", "            dE = errors[-2] - errors[-1]\r\n"]
[570.366, "o", "            # assert(dE >= -tol * errors[-1])\r\n"]
[570.376, "o", "            if dE < tol * errors[-1]:\r\n"]
[570.386, "o", "                if verbose == 1:\r\n"]
[570.396, "o", "                    # A line return\r\n"]
[570.406, "o", "                    print(\"\")\r\n"]
[570.416, "o", "                elif verbose:\r\n"]
[570.426, "o", "                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n"]
[570.436, "o", "                break\r\n"]
[570.446, "o", "        if ii % 5 == 0 and callback is not None:\r\n"]
[570.456, "o", "            callback(locals())\r\n"]
[570.466, "o", "\r\n"]
[570.476, "o", "    if return_n_iter:\r\n"]
[570.486, "o", "        return code, dictionary, errors, ii + 1\r\n"]
[570.496, "o", "    else:\r\n"]
[570.506, "o", "        return code, dictionary, errors\r\n"]
[570.516, "o", "\r\n"]
[570.526, "o", "\r\n"]
[570.536, "o", "def _check_warn_deprecated(param, name, default, additional_message=None):\r\n"]
[570.546, "o", "    if param != \"deprecated\":\r\n"]
[570.556, "o", "        msg = (\r\n"]
[570.566, "o", "            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n"]
[570.576, "o", "        )\r\n"]
[570.586, "o", "        if additional_message:\r\n"]
[570.596, "o", "            msg += f\" {additional_message}\"\r\n"]
[570.606, "o", "        warnings.warn(msg, FutureWarning)\r\n"]
[570.616, "o", "        return param\r\n"]
[570.626, "o", "    else:\r\n"]
[570.636, "o", "        return default\r\n"]
[570.646, "o", "\r\n"]
[570.656, "o", "\r\n"]
[570.666, "o", "def dict_learning_online(\r\n"]
[570.676, "o", "    X,\r\n"]
[570.686, "o", "    n_components=2,\r\n"]
[570.696, "o", "    *,\r\n"]
[570.706, "o", "    alpha=1,\r\n"]
[570.716, "o", "    n_iter=\"deprecated\",\r\n"]
[570.726, "o", "    max_iter=None,\r\n"]
[570.736, "o", "    return_code=True,\r\n"]
[570.746, "o", "    dict_init=None,\r\n"]
[570.756, "o", "    callback=None,\r\n"]
[570.766, "o", "    batch_size=256,\r\n"]
[570.776, "o", "    verbose=False,\r\n"]
[570.786, "o", "    shuffle=True,\r\n"]
[570.796, "o", "    n_jobs=None,\r\n"]
[570.806, "o", "    method=\"lars\",\r\n"]
[570.816, "o", "    iter_offset=\"deprecated\",\r\n"]
[570.826, "o", "    random_state=None,\r\n"]
[570.836, "o", "    return_inner_stats=\"deprecated\",\r\n"]
[570.846, "o", "    inner_stats=\"deprecated\",\r\n"]
[570.856, "o", "    return_n_iter=\"deprecated\",\r\n"]
[570.866, "o", "    positive_dict=False,\r\n"]
[570.876, "o", "    positive_code=False,\r\n"]
[570.886, "o", "    method_max_iter=1000,\r\n"]
[570.896, "o", "    tol=1e-3,\r\n"]
[570.906, "o", "    max_no_improvement=10,\r\n"]
[570.916, "o", "):\r\n"]
[570.926, "o", "    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n"]
[570.936, "o", "\r\n"]
[570.946, "o", "    Finds the best dictionary and the corresponding sparse code for\r\n"]
[570.956, "o", "    approximating the data matrix X by solving::\r\n"]
[570.966, "o", "\r\n"]
[570.976, "o", "        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[570.986, "o", "                     (U,V)\r\n"]
[570.996, "o", "                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n"]
[571.006, "o", "\r\n"]
[571.016, "o", "    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n"]
[571.026, "o", "    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n"]
[571.036, "o", "    which is the sum of the absolute values of all the entries in the matrix.\r\n"]
[571.046, "o", "    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n"]
[571.056, "o", "    the input data.\r\n"]
[571.066, "o", "\r\n"]
[571.076, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[571.086, "o", "\r\n"]
[571.096, "o", "    Parameters\r\n"]
[571.106, "o", "    ----------\r\n"]
[571.116, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[571.126, "o", "        Data matrix.\r\n"]
[571.136, "o", "\r\n"]
[571.146, "o", "    n_components : int or None, default=2\r\n"]
[571.156, "o", "        Number of dictionary atoms to extract. If None, then ``n_components``\r\n"]
[571.166, "o", "        is set to ``n_features``.\r\n"]
[571.176, "o", "\r\n"]
[571.186, "o", "    alpha : float, default=1\r\n"]
[571.196, "o", "        Sparsity controlling parameter.\r\n"]
[571.206, "o", "\r\n"]
[571.216, "o", "    n_iter : int, default=100\r\n"]
[571.226, "o", "        Number of mini-batch iterations to perform.\r\n"]
[571.236, "o", "\r\n"]
[571.246, "o", "        .. deprecated:: 1.1\r\n"]
[571.256, "o", "           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[571.266, "o", "           `max_iter` instead.\r\n"]
[571.276, "o", "\r\n"]
[571.286, "o", "    max_iter : int, default=None\r\n"]
[571.296, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[571.306, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[571.316, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[571.326, "o", "\r\n"]
[571.336, "o", "        .. versionadded:: 1.1\r\n"]
[571.346, "o", "\r\n"]
[571.356, "o", "    return_code : bool, default=True\r\n"]
[571.366, "o", "        Whether to also return the code U or just the dictionary `V`.\r\n"]
[571.376, "o", "\r\n"]
[571.386, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[571.396, "o", "        Initial values for the dictionary for warm restart scenarios.\r\n"]
[571.406, "o", "        If `None`, the initial values for the dictionary are created\r\n"]
[571.416, "o", "        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n"]
[571.426, "o", "\r\n"]
[571.436, "o", "    callback : callable, default=None\r\n"]
[571.446, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[571.456, "o", "\r\n"]
[571.466, "o", "    batch_size : int, default=256\r\n"]
[571.476, "o", "        The number of samples to take in each batch.\r\n"]
[571.486, "o", "\r\n"]
[571.496, "o", "        .. versionchanged:: 1.3\r\n"]
[571.506, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[571.516, "o", "\r\n"]
[571.526, "o", "    verbose : bool, default=False\r\n"]
[571.536, "o", "        To control the verbosity of the procedure.\r\n"]
[571.546, "o", "\r\n"]
[571.556, "o", "    shuffle : bool, default=True\r\n"]
[571.566, "o", "        Whether to shuffle the data before splitting it in batches.\r\n"]
[571.576, "o", "\r\n"]
[571.586, "o", "    n_jobs : int, default=None\r\n"]
[571.596, "o", "        Number of parallel jobs to run.\r\n"]
[571.606, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[571.616, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[571.626, "o", "        for more details.\r\n"]
[571.636, "o", "\r\n"]
[571.646, "o", "    method : {'lars', 'cd'}, default='lars'\r\n"]
[571.656, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[571.666, "o", "          problem (`linear_model.lars_path`);\r\n"]
[571.676, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[571.686, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[571.696, "o", "          the estimated components are sparse.\r\n"]
[571.706, "o", "\r\n"]
[571.716, "o", "    iter_offset : int, default=0\r\n"]
[571.726, "o", "        Number of previous iterations completed on the dictionary used for\r\n"]
[571.736, "o", "        initialization.\r\n"]
[571.746, "o", "\r\n"]
[571.756, "o", "        .. deprecated:: 1.1\r\n"]
[571.766, "o", "           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n"]
[571.776, "o", "\r\n"]
[571.786, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[571.796, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[571.806, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[571.816, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[571.826, "o", "        results across multiple function calls.\r\n"]
[571.836, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[571.846, "o", "\r\n"]
[571.856, "o", "    return_inner_stats : bool, default=False\r\n"]
[571.866, "o", "        Return the inner statistics A (dictionary covariance) and B\r\n"]
[571.876, "o", "        (data approximation). Useful to restart the algorithm in an\r\n"]
[571.886, "o", "        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n"]
[571.896, "o", "        ignored.\r\n"]
[571.906, "o", "\r\n"]
[571.916, "o", "        .. deprecated:: 1.1\r\n"]
[571.926, "o", "           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[571.936, "o", "\r\n"]
[571.946, "o", "    inner_stats : tuple of (A, B) ndarrays, default=None\r\n"]
[571.956, "o", "        Inner sufficient statistics that are kept by the algorithm.\r\n"]
[571.966, "o", "        Passing them at initialization is useful in online settings, to\r\n"]
[571.976, "o", "        avoid losing the history of the evolution.\r\n"]
[571.986, "o", "        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n"]
[571.996, "o", "        `B` `(n_features, n_components)` is the data approximation matrix.\r\n"]
[572.006, "o", "\r\n"]
[572.016, "o", "        .. deprecated:: 1.1\r\n"]
[572.026, "o", "           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[572.036, "o", "\r\n"]
[572.046, "o", "    return_n_iter : bool, default=False\r\n"]
[572.056, "o", "        Whether or not to return the number of iterations.\r\n"]
[572.066, "o", "\r\n"]
[572.076, "o", "        .. deprecated:: 1.1\r\n"]
[572.086, "o", "           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n"]
[572.096, "o", "\r\n"]
[572.106, "o", "    positive_dict : bool, default=False\r\n"]
[572.116, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[572.126, "o", "\r\n"]
[572.136, "o", "        .. versionadded:: 0.20\r\n"]
[572.146, "o", "\r\n"]
[572.156, "o", "    positive_code : bool, default=False\r\n"]
[572.166, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[572.176, "o", "\r\n"]
[572.186, "o", "        .. versionadded:: 0.20\r\n"]
[572.196, "o", "\r\n"]
[572.206, "o", "    method_max_iter : int, default=1000\r\n"]
[572.216, "o", "        Maximum number of iterations to perform when solving the lasso problem.\r\n"]
[572.226, "o", "\r\n"]
[572.236, "o", "        .. versionadded:: 0.22\r\n"]
[572.246, "o", "\r\n"]
[572.256, "o", "    tol : float, default=1e-3\r\n"]
[572.266, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[572.276, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[572.286, "o", "\r\n"]
[572.296, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[572.306, "o", "        `tol` to 0.0.\r\n"]
[572.316, "o", "\r\n"]
[572.326, "o", "        .. versionadded:: 1.1\r\n"]
[572.336, "o", "\r\n"]
[572.346, "o", "    max_no_improvement : int, default=10\r\n"]
[572.356, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[572.366, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[572.376, "o", "        `max_iter` is not None.\r\n"]
[572.386, "o", "\r\n"]
[572.396, "o", "        To disable convergence detection based on cost function, set\r\n"]
[572.406, "o", "        `max_no_improvement` to None.\r\n"]
[572.416, "o", "\r\n"]
[572.426, "o", "        .. versionadded:: 1.1\r\n"]
[572.436, "o", "\r\n"]
[572.446, "o", "    Returns\r\n"]
[572.456, "o", "    -------\r\n"]
[572.466, "o", "    code : ndarray of shape (n_samples, n_components),\r\n"]
[572.476, "o", "        The sparse code (only returned if `return_code=True`).\r\n"]
[572.486, "o", "\r\n"]
[572.496, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[572.506, "o", "        The solutions to the dictionary learning problem.\r\n"]
[572.516, "o", "\r\n"]
[572.526, "o", "    n_iter : int\r\n"]
[572.536, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[572.546, "o", "        set to `True`.\r\n"]
[572.556, "o", "\r\n"]
[572.566, "o", "    See Also\r\n"]
[572.576, "o", "    --------\r\n"]
[572.586, "o", "    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n"]
[572.596, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[572.606, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n"]
[572.616, "o", "        learning algorithm.\r\n"]
[572.626, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[572.636, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[572.646, "o", "    \"\"\"\r\n"]
[572.656, "o", "    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n"]
[572.666, "o", "    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n"]
[572.676, "o", "        raise ValueError(\r\n"]
[572.686, "o", "            \"The following arguments are incompatible with 'max_iter': \"\r\n"]
[572.696, "o", "            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n"]
[572.706, "o", "        )\r\n"]
[572.716, "o", "\r\n"]
[572.726, "o", "    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n"]
[572.736, "o", "    return_inner_stats = _check_warn_deprecated(\r\n"]
[572.746, "o", "        return_inner_stats,\r\n"]
[572.756, "o", "        \"return_inner_stats\",\r\n"]
[572.766, "o", "        default=False,\r\n"]
[572.776, "o", "        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n"]
[572.786, "o", "    )\r\n"]
[572.796, "o", "    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n"]
[572.806, "o", "    return_n_iter = _check_warn_deprecated(\r\n"]
[572.816, "o", "        return_n_iter,\r\n"]
[572.826, "o", "        \"return_n_iter\",\r\n"]
[572.836, "o", "        default=False,\r\n"]
[572.846, "o", "        additional_message=(\r\n"]
[572.856, "o", "            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n"]
[572.866, "o", "            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n"]
[572.876, "o", "        ),\r\n"]
[572.886, "o", "    )\r\n"]
[572.896, "o", "\r\n"]
[572.906, "o", "    if max_iter is not None:\r\n"]
[572.916, "o", "        transform_algorithm = \"lasso_\" + method\r\n"]
[572.926, "o", "\r\n"]
[572.936, "o", "        est = MiniBatchDictionaryLearning(\r\n"]
[572.946, "o", "            n_components=n_components,\r\n"]
[572.956, "o", "            alpha=alpha,\r\n"]
[572.966, "o", "            n_iter=n_iter,\r\n"]
[572.976, "o", "            n_jobs=n_jobs,\r\n"]
[572.986, "o", "            fit_algorithm=method,\r\n"]
[572.996, "o", "            batch_size=batch_size,\r\n"]
[573.006, "o", "            shuffle=shuffle,\r\n"]
[573.016, "o", "            dict_init=dict_init,\r\n"]
[573.026, "o", "            random_state=random_state,\r\n"]
[573.036, "o", "            transform_algorithm=transform_algorithm,\r\n"]
[573.046, "o", "            transform_alpha=alpha,\r\n"]
[573.056, "o", "            positive_code=positive_code,\r\n"]
[573.066, "o", "            positive_dict=positive_dict,\r\n"]
[573.076, "o", "            transform_max_iter=method_max_iter,\r\n"]
[573.086, "o", "            verbose=verbose,\r\n"]
[573.096, "o", "            callback=callback,\r\n"]
[573.106, "o", "            tol=tol,\r\n"]
[573.116, "o", "            max_no_improvement=max_no_improvement,\r\n"]
[573.126, "o", "        ).fit(X)\r\n"]
[573.136, "o", "\r\n"]
[573.146, "o", "        if not return_code:\r\n"]
[573.156, "o", "            return est.components_\r\n"]
[573.166, "o", "        else:\r\n"]
[573.176, "o", "            code = est.transform(X)\r\n"]
[573.186, "o", "            return code, est.components_\r\n"]
[573.196, "o", "\r\n"]
[573.206, "o", "    # TODO(1.4) remove the whole old behavior\r\n"]
[573.216, "o", "    # Fallback to old behavior\r\n"]
[573.226, "o", "\r\n"]
[573.236, "o", "    n_iter = _check_warn_deprecated(\r\n"]
[573.246, "o", "        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n"]
[573.256, "o", "    )\r\n"]
[573.266, "o", "\r\n"]
[573.276, "o", "    if n_components is None:\r\n"]
[573.286, "o", "        n_components = X.shape[1]\r\n"]
[573.296, "o", "\r\n"]
[573.306, "o", "    if method not in (\"lars\", \"cd\"):\r\n"]
[573.316, "o", "        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n"]
[573.326, "o", "\r\n"]
[573.336, "o", "    _check_positive_coding(method, positive_code)\r\n"]
[573.346, "o", "\r\n"]
[573.356, "o", "    method = \"lasso_\" + method\r\n"]
[573.366, "o", "\r\n"]
[573.376, "o", "    t0 = time.time()\r\n"]
[573.386, "o", "    n_samples, n_features = X.shape\r\n"]
[573.396, "o", "    # Avoid integer division problems\r\n"]
[573.406, "o", "    alpha = float(alpha)\r\n"]
[573.416, "o", "    random_state = check_random_state(random_state)\r\n"]
[573.426, "o", "\r\n"]
[573.436, "o", "    # Init V with SVD of X\r\n"]
[573.446, "o", "    if dict_init is not None:\r\n"]
[573.456, "o", "        dictionary = dict_init\r\n"]
[573.466, "o", "    else:\r\n"]
[573.476, "o", "        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n"]
[573.486, "o", "        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[573.496, "o", "    r = len(dictionary)\r\n"]
[573.506, "o", "    if n_components <= r:\r\n"]
[573.516, "o", "        dictionary = dictionary[:n_components, :]\r\n"]
[573.526, "o", "    else:\r\n"]
[573.536, "o", "        dictionary = np.r_[\r\n"]
[573.546, "o", "            dictionary,\r\n"]
[573.556, "o", "            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n"]
[573.566, "o", "        ]\r\n"]
[573.576, "o", "\r\n"]
[573.586, "o", "    if verbose == 1:\r\n"]
[573.596, "o", "        print(\"[dict_learning]\", end=\" \")\r\n"]
[573.606, "o", "\r\n"]
[573.616, "o", "    if shuffle:\r\n"]
[573.626, "o", "        X_train = X.copy()\r\n"]
[573.636, "o", "        random_state.shuffle(X_train)\r\n"]
[573.646, "o", "    else:\r\n"]
[573.656, "o", "        X_train = X\r\n"]
[573.666, "o", "\r\n"]
[573.676, "o", "    X_train = check_array(\r\n"]
[573.686, "o", "        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n"]
[573.696, "o", "    )\r\n"]
[573.706, "o", "\r\n"]
[573.716, "o", "    # Fortran-order dict better suited for the sparse coding which is the\r\n"]
[573.726, "o", "    # bottleneck of this algorithm.\r\n"]
[573.736, "o", "    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n"]
[573.746, "o", "    dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[573.756, "o", "\r\n"]
[573.766, "o", "    batches = gen_batches(n_samples, batch_size)\r\n"]
[573.776, "o", "    batches = itertools.cycle(batches)\r\n"]
[573.786, "o", "\r\n"]
[573.796, "o", "    # The covariance of the dictionary\r\n"]
[573.806, "o", "    if inner_stats is None:\r\n"]
[573.816, "o", "        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n"]
[573.826, "o", "        # The data approximation\r\n"]
[573.836, "o", "        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n"]
[573.846, "o", "    else:\r\n"]
[573.856, "o", "        A = inner_stats[0].copy()\r\n"]
[573.866, "o", "        B = inner_stats[1].copy()\r\n"]
[573.876, "o", "\r\n"]
[573.886, "o", "    # If n_iter is zero, we need to return zero.\r\n"]
[573.896, "o", "    ii = iter_offset - 1\r\n"]
[573.906, "o", "\r\n"]
[573.916, "o", "    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n"]
[573.926, "o", "        this_X = X_train[batch]\r\n"]
[573.936, "o", "        dt = time.time() - t0\r\n"]
[573.946, "o", "        if verbose == 1:\r\n"]
[573.956, "o", "            sys.stdout.write(\".\")\r\n"]
[573.966, "o", "            sys.stdout.flush()\r\n"]
[573.976, "o", "        elif verbose:\r\n"]
[573.986, "o", "            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n"]
[573.996, "o", "                print(\r\n"]
[574.006, "o", "                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n"]
[574.016, "o", "                )\r\n"]
[574.026, "o", "\r\n"]
[574.036, "o", "        this_code = sparse_encode(\r\n"]
[574.046, "o", "            this_X,\r\n"]
[574.056, "o", "            dictionary,\r\n"]
[574.066, "o", "            algorithm=method,\r\n"]
[574.076, "o", "            alpha=alpha,\r\n"]
[574.086, "o", "            n_jobs=n_jobs,\r\n"]
[574.096, "o", "            check_input=False,\r\n"]
[574.106, "o", "            positive=positive_code,\r\n"]
[574.116, "o", "            max_iter=method_max_iter,\r\n"]
[574.126, "o", "            verbose=verbose,\r\n"]
[574.136, "o", "        )\r\n"]
[574.146, "o", "\r\n"]
[574.156, "o", "        # Update the auxiliary variables\r\n"]
[574.166, "o", "        if ii < batch_size - 1:\r\n"]
[574.176, "o", "            theta = float((ii + 1) * batch_size)\r\n"]
[574.186, "o", "        else:\r\n"]
[574.196, "o", "            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n"]
[574.206, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[574.216, "o", "\r\n"]
[574.226, "o", "        A *= beta\r\n"]
[574.236, "o", "        A += np.dot(this_code.T, this_code)\r\n"]
[574.246, "o", "        B *= beta\r\n"]
[574.256, "o", "        B += np.dot(this_X.T, this_code)\r\n"]
[574.266, "o", "\r\n"]
[574.276, "o", "        # Update dictionary in place\r\n"]
[574.286, "o", "        _update_dict(\r\n"]
[574.296, "o", "            dictionary,\r\n"]
[574.306, "o", "            this_X,\r\n"]
[574.316, "o", "            this_code,\r\n"]
[574.326, "o", "            A,\r\n"]
[574.336, "o", "            B,\r\n"]
[574.346, "o", "            verbose=verbose,\r\n"]
[574.356, "o", "            random_state=random_state,\r\n"]
[574.366, "o", "            positive=positive_dict,\r\n"]
[574.376, "o", "        )\r\n"]
[574.386, "o", "\r\n"]
[574.396, "o", "        # Maybe we need a stopping criteria based on the amount of\r\n"]
[574.406, "o", "        # modification in the dictionary\r\n"]
[574.416, "o", "        if callback is not None:\r\n"]
[574.426, "o", "            callback(locals())\r\n"]
[574.436, "o", "\r\n"]
[574.446, "o", "    if return_inner_stats:\r\n"]
[574.456, "o", "        if return_n_iter:\r\n"]
[574.466, "o", "            return dictionary, (A, B), ii - iter_offset + 1\r\n"]
[574.476, "o", "        else:\r\n"]
[574.486, "o", "            return dictionary, (A, B)\r\n"]
[574.496, "o", "    if return_code:\r\n"]
[574.506, "o", "        if verbose > 1:\r\n"]
[574.516, "o", "            print(\"Learning code...\", end=\" \")\r\n"]
[574.526, "o", "        elif verbose == 1:\r\n"]
[574.536, "o", "            print(\"|\", end=\" \")\r\n"]
[574.546, "o", "        code = sparse_encode(\r\n"]
[574.556, "o", "            X,\r\n"]
[574.566, "o", "            dictionary,\r\n"]
[574.576, "o", "            algorithm=method,\r\n"]
[574.586, "o", "            alpha=alpha,\r\n"]
[574.596, "o", "            n_jobs=n_jobs,\r\n"]
[574.606, "o", "            check_input=False,\r\n"]
[574.616, "o", "            positive=positive_code,\r\n"]
[574.626, "o", "            max_iter=method_max_iter,\r\n"]
[574.636, "o", "            verbose=verbose,\r\n"]
[574.646, "o", "        )\r\n"]
[574.656, "o", "        if verbose > 1:\r\n"]
[574.666, "o", "            dt = time.time() - t0\r\n"]
[574.676, "o", "            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n"]
[574.686, "o", "        if return_n_iter:\r\n"]
[574.696, "o", "            return code, dictionary, ii - iter_offset + 1\r\n"]
[574.706, "o", "        else:\r\n"]
[574.716, "o", "            return code, dictionary\r\n"]
[574.726, "o", "\r\n"]
[574.736, "o", "    if return_n_iter:\r\n"]
[574.746, "o", "        return dictionary, ii - iter_offset + 1\r\n"]
[574.756, "o", "    else:\r\n"]
[574.766, "o", "        return dictionary\r\n"]
[574.776, "o", "\r\n"]
[574.786, "o", "\r\n"]
[574.796, "o", "@validate_params(\r\n"]
[574.806, "o", "    {\r\n"]
[574.816, "o", "        \"X\": [\"array-like\"],\r\n"]
[574.826, "o", "        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[574.836, "o", "        \"return_n_iter\": [\"boolean\"],\r\n"]
[574.846, "o", "        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[574.856, "o", "    },\r\n"]
[574.866, "o", "    prefer_skip_nested_validation=False,\r\n"]
[574.876, "o", ")\r\n"]
[574.886, "o", "def dict_learning(\r\n"]
[574.896, "o", "    X,\r\n"]
[574.906, "o", "    n_components,\r\n"]
[574.916, "o", "    *,\r\n"]
[574.926, "o", "    alpha,\r\n"]
[574.936, "o", "    max_iter=100,\r\n"]
[575.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[575.002, "i", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r"]
[575.004, "o", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r\n"]
[575.016, "o", "\u001b[?2004l\r\n"]
[575.026, "o", "    n_iter : int\r\n"]
[575.036, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[575.046, "o", "        set to True.\r\n"]
[575.056, "o", "\r\n"]
[575.066, "o", "    See Also\r\n"]
[575.076, "o", "    --------\r\n"]
[575.086, "o", "    dict_learning_online : Solve a dictionary learning matrix factorization\r\n"]
[575.096, "o", "        problem online.\r\n"]
[575.106, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[575.116, "o", "    MiniBatchDictionaryLearning : A faster, less accurate version\r\n"]
[575.126, "o", "        of the dictionary learning algorithm.\r\n"]
[575.136, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[575.146, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[575.156, "o", "    \"\"\"\r\n"]
[575.166, "o", "    estimator = DictionaryLearning(\r\n"]
[575.176, "o", "        n_components=n_components,\r\n"]
[575.186, "o", "        alpha=alpha,\r\n"]
[575.196, "o", "        max_iter=max_iter,\r\n"]
[575.206, "o", "        tol=tol,\r\n"]
[575.216, "o", "        fit_algorithm=method,\r\n"]
[575.226, "o", "        n_jobs=n_jobs,\r\n"]
[575.236, "o", "        dict_init=dict_init,\r\n"]
[575.246, "o", "        callback=callback,\r\n"]
[575.256, "o", "        code_init=code_init,\r\n"]
[575.266, "o", "        verbose=verbose,\r\n"]
[575.276, "o", "        random_state=random_state,\r\n"]
[575.286, "o", "        positive_code=positive_code,\r\n"]
[575.296, "o", "        positive_dict=positive_dict,\r\n"]
[575.306, "o", "        transform_max_iter=method_max_iter,\r\n"]
[575.316, "o", "    )\r\n"]
[575.326, "o", "    code = estimator.fit_transform(X)\r\n"]
[575.336, "o", "    if return_n_iter:\r\n"]
[575.346, "o", "        return (\r\n"]
[575.356, "o", "            code,\r\n"]
[575.366, "o", "            estimator.components_,\r\n"]
[575.376, "o", "            estimator.error_,\r\n"]
[575.386, "o", "            estimator.n_iter_,\r\n"]
[575.396, "o", "        )\r\n"]
[575.406, "o", "    return code, estimator.components_, estimator.error_\r\n"]
[575.416, "o", "\r\n"]
[575.426, "o", "\r\n"]
[575.436, "o", "class _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n"]
[575.446, "o", "    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n"]
[575.456, "o", "\r\n"]
[575.466, "o", "    def __init__(\r\n"]
[575.476, "o", "        self,\r\n"]
[575.486, "o", "        transform_algorithm,\r\n"]
[575.496, "o", "        transform_n_nonzero_coefs,\r\n"]
[575.506, "o", "        transform_alpha,\r\n"]
[575.516, "o", "        split_sign,\r\n"]
[575.526, "o", "        n_jobs,\r\n"]
[575.536, "o", "        positive_code,\r\n"]
[575.546, "o", "        transform_max_iter,\r\n"]
[575.556, "o", "    ):\r\n"]
[575.566, "o", "        self.transform_algorithm = transform_algorithm\r\n"]
[575.576, "o", "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n"]
[575.586, "o", "        self.transform_alpha = transform_alpha\r\n"]
[575.596, "o", "        self.transform_max_iter = transform_max_iter\r\n"]
[575.606, "o", "        self.split_sign = split_sign\r\n"]
[575.616, "o", "        self.n_jobs = n_jobs\r\n"]
[575.626, "o", "        self.positive_code = positive_code\r\n"]
[575.636, "o", "\r\n"]
[575.646, "o", "    def _transform(self, X, dictionary):\r\n"]
[575.656, "o", "        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n"]
[575.666, "o", "        SparseCoder.\"\"\"\r\n"]
[575.676, "o", "        X = self._validate_data(X, reset=False)\r\n"]
[575.686, "o", "\r\n"]
[575.696, "o", "        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n"]
[575.706, "o", "            transform_alpha = self.alpha\r\n"]
[575.716, "o", "        else:\r\n"]
[575.726, "o", "            transform_alpha = self.transform_alpha\r\n"]
[575.736, "o", "\r\n"]
[575.746, "o", "        code = sparse_encode(\r\n"]
[575.756, "o", "            X,\r\n"]
[575.766, "o", "            dictionary,\r\n"]
[575.776, "o", "            algorithm=self.transform_algorithm,\r\n"]
[575.786, "o", "            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n"]
[575.796, "o", "            alpha=transform_alpha,\r\n"]
[575.806, "o", "            max_iter=self.transform_max_iter,\r\n"]
[575.816, "o", "            n_jobs=self.n_jobs,\r\n"]
[575.826, "o", "            positive=self.positive_code,\r\n"]
[575.836, "o", "        )\r\n"]
[575.846, "o", "\r\n"]
[575.856, "o", "        if self.split_sign:\r\n"]
[575.866, "o", "            # feature vector is split into a positive and negative side\r\n"]
[575.876, "o", "            n_samples, n_features = code.shape\r\n"]
[575.886, "o", "            split_code = np.empty((n_samples, 2 * n_features))\r\n"]
[575.896, "o", "            split_code[:, :n_features] = np.maximum(code, 0)\r\n"]
[575.906, "o", "            split_code[:, n_features:] = -np.minimum(code, 0)\r\n"]
[575.916, "o", "            code = split_code\r\n"]
[575.926, "o", "\r\n"]
[575.936, "o", "        return code\r\n"]
[575.946, "o", "\r\n"]
[575.956, "o", "    def transform(self, X):\r\n"]
[575.966, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[575.976, "o", "\r\n"]
[575.986, "o", "        Coding method is determined by the object parameter\r\n"]
[575.996, "o", "        `transform_algorithm`.\r\n"]
[576.006, "o", "\r\n"]
[576.016, "o", "        Parameters\r\n"]
[576.026, "o", "        ----------\r\n"]
[576.036, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[576.046, "o", "            Test data to be transformed, must have the same number of\r\n"]
[576.056, "o", "            features as the data used to train the model.\r\n"]
[576.066, "o", "\r\n"]
[576.076, "o", "        Returns\r\n"]
[576.086, "o", "        -------\r\n"]
[576.096, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[576.106, "o", "            Transformed data.\r\n"]
[576.116, "o", "        \"\"\"\r\n"]
[576.126, "o", "        check_is_fitted(self)\r\n"]
[576.136, "o", "        return self._transform(X, self.components_)\r\n"]
[576.146, "o", "\r\n"]
[576.156, "o", "\r\n"]
[576.166, "o", "class SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n"]
[576.176, "o", "    \"\"\"Sparse coding.\r\n"]
[576.186, "o", "\r\n"]
[576.196, "o", "    Finds a sparse representation of data against a fixed, precomputed\r\n"]
[576.206, "o", "    dictionary.\r\n"]
[576.216, "o", "\r\n"]
[576.226, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[576.236, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[576.246, "o", "\r\n"]
[576.256, "o", "        X ~= code * dictionary\r\n"]
[576.266, "o", "\r\n"]
[576.276, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[576.286, "o", "\r\n"]
[576.296, "o", "    Parameters\r\n"]
[576.306, "o", "    ----------\r\n"]
[576.316, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[576.326, "o", "        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n"]
[576.336, "o", "        normalized to unit norm.\r\n"]
[576.346, "o", "\r\n"]
[576.356, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[576.366, "o", "            'threshold'}, default='omp'\r\n"]
[576.376, "o", "        Algorithm used to transform the data:\r\n"]
[576.386, "o", "\r\n"]
[576.396, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[576.406, "o", "          (`linear_model.lars_path`);\r\n"]
[576.416, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[576.426, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[576.436, "o", "          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n"]
[576.446, "o", "          the estimated components are sparse;\r\n"]
[576.456, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[576.466, "o", "          solution;\r\n"]
[576.476, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[576.486, "o", "          the projection ``dictionary * X'``.\r\n"]
[576.496, "o", "\r\n"]
[576.506, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[576.516, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[576.526, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[576.536, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[576.546, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[576.556, "o", "\r\n"]
[576.566, "o", "    transform_alpha : float, default=None\r\n"]
[576.576, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[576.586, "o", "        penalty applied to the L1 norm.\r\n"]
[576.596, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[576.606, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[576.616, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[576.626, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[576.636, "o", "        `n_nonzero_coefs`.\r\n"]
[576.646, "o", "        If `None`, default to 1.\r\n"]
[576.656, "o", "\r\n"]
[576.666, "o", "    split_sign : bool, default=False\r\n"]
[576.676, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[576.686, "o", "        its negative part and its positive part. This can improve the\r\n"]
[576.696, "o", "        performance of downstream classifiers.\r\n"]
[576.706, "o", "\r\n"]
[576.716, "o", "    n_jobs : int, default=None\r\n"]
[576.726, "o", "        Number of parallel jobs to run.\r\n"]
[576.736, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[576.746, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[576.756, "o", "        for more details.\r\n"]
[576.766, "o", "\r\n"]
[576.776, "o", "    positive_code : bool, default=False\r\n"]
[576.786, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[576.796, "o", "\r\n"]
[576.806, "o", "        .. versionadded:: 0.20\r\n"]
[576.816, "o", "\r\n"]
[576.826, "o", "    transform_max_iter : int, default=1000\r\n"]
[576.836, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[576.846, "o", "        `lasso_lars`.\r\n"]
[576.856, "o", "\r\n"]
[576.866, "o", "        .. versionadded:: 0.22\r\n"]
[576.876, "o", "\r\n"]
[576.886, "o", "    Attributes\r\n"]
[576.896, "o", "    ----------\r\n"]
[576.906, "o", "    n_components_ : int\r\n"]
[576.916, "o", "        Number of atoms.\r\n"]
[576.926, "o", "\r\n"]
[576.936, "o", "    n_features_in_ : int\r\n"]
[576.946, "o", "        Number of features seen during :term:`fit`.\r\n"]
[576.956, "o", "\r\n"]
[576.966, "o", "        .. versionadded:: 0.24\r\n"]
[576.976, "o", "\r\n"]
[576.986, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[576.996, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[577.006, "o", "        has feature names that are all strings.\r\n"]
[577.016, "o", "\r\n"]
[577.026, "o", "        .. versionadded:: 1.0\r\n"]
[577.036, "o", "\r\n"]
[577.046, "o", "    See Also\r\n"]
[577.056, "o", "    --------\r\n"]
[577.066, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[577.076, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n"]
[577.086, "o", "        dictionary learning algorithm.\r\n"]
[577.096, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[577.106, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[577.116, "o", "    sparse_encode : Sparse coding where each row of the result is the solution\r\n"]
[577.126, "o", "        to a sparse coding problem.\r\n"]
[577.136, "o", "\r\n"]
[577.146, "o", "    Examples\r\n"]
[577.156, "o", "    --------\r\n"]
[577.166, "o", "    >>> import numpy as np\r\n"]
[577.176, "o", "    >>> from sklearn.decomposition import SparseCoder\r\n"]
[577.186, "o", "    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n"]
[577.196, "o", "    >>> dictionary = np.array(\r\n"]
[577.206, "o", "    ...     [[0, 1, 0],\r\n"]
[577.216, "o", "    ...      [-1, -1, 2],\r\n"]
[577.226, "o", "    ...      [1, 1, 1],\r\n"]
[577.236, "o", "    ...      [0, 1, 1],\r\n"]
[577.246, "o", "    ...      [0, 2, 1]],\r\n"]
[577.256, "o", "    ...    dtype=np.float64\r\n"]
[577.266, "o", "    ... )\r\n"]
[577.276, "o", "    >>> coder = SparseCoder(\r\n"]
[577.286, "o", "    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n"]
[577.296, "o", "    ...     transform_alpha=1e-10,\r\n"]
[577.306, "o", "    ... )\r\n"]
[577.316, "o", "    >>> coder.transform(X)\r\n"]
[577.326, "o", "    array([[ 0.,  0., -1.,  0.,  0.],\r\n"]
[577.336, "o", "           [ 0.,  1.,  1.,  0.,  0.]])\r\n"]
[577.346, "o", "    \"\"\"\r\n"]
[577.356, "o", "\r\n"]
[577.366, "o", "    _required_parameters = [\"dictionary\"]\r\n"]
[577.376, "o", "\r\n"]
[577.386, "o", "    def __init__(\r\n"]
[577.396, "o", "        self,\r\n"]
[577.406, "o", "        dictionary,\r\n"]
[577.416, "o", "        *,\r\n"]
[577.426, "o", "        transform_algorithm=\"omp\",\r\n"]
[577.436, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[577.446, "o", "        transform_alpha=None,\r\n"]
[577.456, "o", "        split_sign=False,\r\n"]
[577.466, "o", "        n_jobs=None,\r\n"]
[577.476, "o", "        positive_code=False,\r\n"]
[577.486, "o", "        transform_max_iter=1000,\r\n"]
[577.496, "o", "    ):\r\n"]
[577.506, "o", "        super().__init__(\r\n"]
[577.516, "o", "            transform_algorithm,\r\n"]
[577.526, "o", "            transform_n_nonzero_coefs,\r\n"]
[577.536, "o", "            transform_alpha,\r\n"]
[577.546, "o", "            split_sign,\r\n"]
[577.556, "o", "            n_jobs,\r\n"]
[577.566, "o", "            positive_code,\r\n"]
[577.576, "o", "            transform_max_iter,\r\n"]
[577.586, "o", "        )\r\n"]
[577.596, "o", "        self.dictionary = dictionary\r\n"]
[577.606, "o", "\r\n"]
[577.616, "o", "    def fit(self, X, y=None):\r\n"]
[577.626, "o", "        \"\"\"Do nothing and return the estimator unchanged.\r\n"]
[577.636, "o", "\r\n"]
[577.646, "o", "        This method is just there to implement the usual API and hence\r\n"]
[577.656, "o", "        work in pipelines.\r\n"]
[577.666, "o", "\r\n"]
[577.676, "o", "        Parameters\r\n"]
[577.686, "o", "        ----------\r\n"]
[577.696, "o", "        X : Ignored\r\n"]
[577.706, "o", "            Not used, present for API consistency by convention.\r\n"]
[577.716, "o", "\r\n"]
[577.726, "o", "        y : Ignored\r\n"]
[577.736, "o", "            Not used, present for API consistency by convention.\r\n"]
[577.746, "o", "\r\n"]
[577.756, "o", "        Returns\r\n"]
[577.766, "o", "        -------\r\n"]
[577.776, "o", "        self : object\r\n"]
[577.786, "o", "            Returns the instance itself.\r\n"]
[577.796, "o", "        \"\"\"\r\n"]
[577.806, "o", "        return self\r\n"]
[577.816, "o", "\r\n"]
[577.826, "o", "    def transform(self, X, y=None):\r\n"]
[577.836, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[577.846, "o", "\r\n"]
[577.856, "o", "        Coding method is determined by the object parameter\r\n"]
[577.866, "o", "        `transform_algorithm`.\r\n"]
[577.876, "o", "\r\n"]
[577.886, "o", "        Parameters\r\n"]
[577.896, "o", "        ----------\r\n"]
[577.906, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[577.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[577.926, "o", "            and `n_features` is the number of features.\r\n"]
[577.936, "o", "\r\n"]
[577.946, "o", "        y : Ignored\r\n"]
[577.956, "o", "            Not used, present for API consistency by convention.\r\n"]
[577.966, "o", "\r\n"]
[577.976, "o", "        Returns\r\n"]
[577.986, "o", "        -------\r\n"]
[577.996, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[578.006, "o", "            Transformed data.\r\n"]
[578.016, "o", "        \"\"\"\r\n"]
[578.026, "o", "        return super()._transform(X, self.dictionary)\r\n"]
[578.036, "o", "\r\n"]
[578.046, "o", "    def _more_tags(self):\r\n"]
[578.056, "o", "        return {\r\n"]
[578.066, "o", "            \"requires_fit\": False,\r\n"]
[578.076, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[578.086, "o", "        }\r\n"]
[578.096, "o", "\r\n"]
[578.106, "o", "    @property\r\n"]
[578.116, "o", "    def n_components_(self):\r\n"]
[578.126, "o", "        \"\"\"Number of atoms.\"\"\"\r\n"]
[578.136, "o", "        return self.dictionary.shape[0]\r\n"]
[578.146, "o", "\r\n"]
[578.156, "o", "    @property\r\n"]
[578.166, "o", "    def n_features_in_(self):\r\n"]
[578.176, "o", "        \"\"\"Number of features seen during `fit`.\"\"\"\r\n"]
[578.186, "o", "        return self.dictionary.shape[1]\r\n"]
[578.196, "o", "\r\n"]
[578.206, "o", "    @property\r\n"]
[578.216, "o", "    def _n_features_out(self):\r\n"]
[578.226, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[578.236, "o", "        return self.n_components_\r\n"]
[578.246, "o", "\r\n"]
[578.256, "o", "\r\n"]
[578.266, "o", "class DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[578.276, "o", "    \"\"\"Dictionary learning.\r\n"]
[578.286, "o", "\r\n"]
[578.296, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[578.306, "o", "    encoding the fitted data.\r\n"]
[578.316, "o", "\r\n"]
[578.326, "o", "    Solves the optimization problem::\r\n"]
[578.336, "o", "\r\n"]
[578.346, "o", "        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[578.356, "o", "                    (U,V)\r\n"]
[578.366, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[578.376, "o", "\r\n"]
[578.386, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[578.396, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[578.406, "o", "    of all the entries in the matrix.\r\n"]
[578.416, "o", "\r\n"]
[578.426, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[578.436, "o", "\r\n"]
[578.446, "o", "    Parameters\r\n"]
[578.456, "o", "    ----------\r\n"]
[578.466, "o", "    n_components : int, default=None\r\n"]
[578.476, "o", "        Number of dictionary elements to extract. If None, then ``n_components``\r\n"]
[578.486, "o", "        is set to ``n_features``.\r\n"]
[578.496, "o", "\r\n"]
[578.506, "o", "    alpha : float, default=1.0\r\n"]
[578.516, "o", "        Sparsity controlling parameter.\r\n"]
[578.526, "o", "\r\n"]
[578.536, "o", "    max_iter : int, default=1000\r\n"]
[578.546, "o", "        Maximum number of iterations to perform.\r\n"]
[578.556, "o", "\r\n"]
[578.566, "o", "    tol : float, default=1e-8\r\n"]
[578.576, "o", "        Tolerance for numerical error.\r\n"]
[578.586, "o", "\r\n"]
[578.596, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[578.606, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[578.616, "o", "          problem (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[578.626, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[578.636, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n"]
[578.646, "o", "          faster if the estimated components are sparse.\r\n"]
[578.656, "o", "\r\n"]
[578.666, "o", "        .. versionadded:: 0.17\r\n"]
[578.676, "o", "           *cd* coordinate descent method to improve speed.\r\n"]
[578.686, "o", "\r\n"]
[578.696, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[578.706, "o", "            'threshold'}, default='omp'\r\n"]
[578.716, "o", "        Algorithm used to transform the data:\r\n"]
[578.726, "o", "\r\n"]
[578.736, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[578.746, "o", "          (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[578.756, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[578.766, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[578.776, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n"]
[578.786, "o", "          will be faster if the estimated components are sparse.\r\n"]
[578.796, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[578.806, "o", "          solution.\r\n"]
[578.816, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[578.826, "o", "          the projection ``dictionary * X'``.\r\n"]
[578.836, "o", "\r\n"]
[578.846, "o", "        .. versionadded:: 0.17\r\n"]
[578.856, "o", "           *lasso_cd* coordinate descent method to improve speed.\r\n"]
[578.866, "o", "\r\n"]
[578.876, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[578.886, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[578.896, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[578.906, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[578.916, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[578.926, "o", "\r\n"]
[578.936, "o", "    transform_alpha : float, default=None\r\n"]
[578.946, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[578.956, "o", "        penalty applied to the L1 norm.\r\n"]
[578.966, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[578.976, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[578.986, "o", "        If `None`, defaults to `alpha`.\r\n"]
[578.996, "o", "\r\n"]
[579.006, "o", "        .. versionchanged:: 1.2\r\n"]
[579.016, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[579.026, "o", "\r\n"]
[579.036, "o", "    n_jobs : int or None, default=None\r\n"]
[579.046, "o", "        Number of parallel jobs to run.\r\n"]
[579.056, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[579.066, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[579.076, "o", "        for more details.\r\n"]
[579.086, "o", "\r\n"]
[579.096, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[579.106, "o", "        Initial value for the code, for warm restart. Only used if `code_init`\r\n"]
[579.116, "o", "        and `dict_init` are not None.\r\n"]
[579.126, "o", "\r\n"]
[579.136, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[579.146, "o", "        Initial values for the dictionary, for warm restart. Only used if\r\n"]
[579.156, "o", "        `code_init` and `dict_init` are not None.\r\n"]
[579.166, "o", "\r\n"]
[579.176, "o", "    callback : callable, default=None\r\n"]
[579.186, "o", "        Callable that gets invoked every five iterations.\r\n"]
[579.196, "o", "\r\n"]
[579.206, "o", "        .. versionadded:: 1.3\r\n"]
[579.216, "o", "\r\n"]
[579.226, "o", "    verbose : bool, default=False\r\n"]
[579.236, "o", "        To control the verbosity of the procedure.\r\n"]
[579.246, "o", "\r\n"]
[579.256, "o", "    split_sign : bool, default=False\r\n"]
[579.266, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[579.276, "o", "        its negative part and its positive part. This can improve the\r\n"]
[579.286, "o", "        performance of downstream classifiers.\r\n"]
[579.296, "o", "\r\n"]
[579.306, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[579.316, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[579.326, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[579.336, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[579.346, "o", "        results across multiple function calls.\r\n"]
[579.356, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[579.366, "o", "\r\n"]
[579.376, "o", "    positive_code : bool, default=False\r\n"]
[579.386, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[579.396, "o", "\r\n"]
[579.406, "o", "        .. versionadded:: 0.20\r\n"]
[579.416, "o", "\r\n"]
[579.426, "o", "    positive_dict : bool, default=False\r\n"]
[579.436, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[579.446, "o", "\r\n"]
[579.456, "o", "        .. versionadded:: 0.20\r\n"]
[579.466, "o", "\r\n"]
[579.476, "o", "    transform_max_iter : int, default=1000\r\n"]
[579.486, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[579.496, "o", "        `'lasso_lars'`.\r\n"]
[579.506, "o", "\r\n"]
[579.516, "o", "        .. versionadded:: 0.22\r\n"]
[579.526, "o", "\r\n"]
[579.536, "o", "    Attributes\r\n"]
[579.546, "o", "    ----------\r\n"]
[579.556, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[579.566, "o", "        dictionary atoms extracted from the data\r\n"]
[579.576, "o", "\r\n"]
[579.586, "o", "    error_ : array\r\n"]
[579.596, "o", "        vector of errors at each iteration\r\n"]
[579.606, "o", "\r\n"]
[579.616, "o", "    n_features_in_ : int\r\n"]
[579.626, "o", "        Number of features seen during :term:`fit`.\r\n"]
[579.636, "o", "\r\n"]
[579.646, "o", "        .. versionadded:: 0.24\r\n"]
[579.656, "o", "\r\n"]
[579.666, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[579.676, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[579.686, "o", "        has feature names that are all strings.\r\n"]
[579.696, "o", "\r\n"]
[579.706, "o", "        .. versionadded:: 1.0\r\n"]
[579.716, "o", "\r\n"]
[579.726, "o", "    n_iter_ : int\r\n"]
[579.736, "o", "        Number of iterations run.\r\n"]
[579.746, "o", "\r\n"]
[579.756, "o", "    See Also\r\n"]
[579.766, "o", "    --------\r\n"]
[579.776, "o", "    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n"]
[579.786, "o", "        dictionary learning algorithm.\r\n"]
[579.796, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[579.806, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[579.816, "o", "        precomputed dictionary.\r\n"]
[579.826, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[579.836, "o", "\r\n"]
[579.846, "o", "    References\r\n"]
[579.856, "o", "    ----------\r\n"]
[579.866, "o", "\r\n"]
[579.876, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[579.886, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[579.896, "o", "\r\n"]
[579.906, "o", "    Examples\r\n"]
[579.916, "o", "    --------\r\n"]
[579.926, "o", "    >>> import numpy as np\r\n"]
[579.936, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[580.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[580.002, "i", "cd asv_benchmarks\r"]
[580.004, "o", "cd asv_benchmarks\r\n"]
[580.006, "o", "\u001b[?2004l\r\n"]
[585.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[585.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[585.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[586.654, "o", "\u001b[?2004l\r\n"]
[588.302, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[590.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[590.002, "i", "cd ..\r"]
[590.004, "o", "cd ..\r\n"]
[590.006, "o", "\u001b[?2004l\r\n"]
[595.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[595.002, "i", "grep -n \"def _minibatch_step\\|_sparse_encode_precomputed\\|cov\\s*=\\s|gram\\s*=\\s\" sklearn/decomposition/_dict_learnin\r"]
[595.004, "o", "grep -n \"def _minibatch_step\\|_sparse_encode_precomputed\\|cov\\s*=\\s|gram\\s*=\\s\" sklearn/decomposition/_dict_learnin\r\n"]
[595.83, "o", "ng.py\r\n"]
[596.654, "o", "\u001b[?2004l\r\n"]
[597.478, "o", "\u001b[32m\u001b[K38\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdef \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n"]
[598.302, "o", "\u001b[32m\u001b[K416\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        code = \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n"]
[599.126, "o", "\u001b[32m\u001b[K438\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        delayed(\u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K)(\r\n"]
[600.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[600.002, "i", "grep -n \"_sparse_encode_precomputed\" sklearn/decomposition/_dict_learning.py\r"]
[600.004, "o", "grep -n \"_sparse_encode_precomputed\" sklearn/decomposition/_dict_learning.py\r\n"]
[601.242, "o", "\u001b[?2004l\r\n"]
[602.478, "o", "\u001b[32m\u001b[K38\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdef \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n"]
[603.714, "o", "\u001b[32m\u001b[K416\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        code = \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n"]
[605.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[605.002, "i", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r"]
[605.004, "o", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r\n"]
[605.016, "o", "\u001b[?2004l\r\n"]
[605.026, "o", "\"\"\" Dictionary learning.\r\n"]
[605.036, "o", "\"\"\"\r\n"]
[605.046, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[605.056, "o", "# License: BSD 3 clause\r\n"]
[605.066, "o", "\r\n"]
[605.076, "o", "import itertools\r\n"]
[605.086, "o", "import sys\r\n"]
[605.096, "o", "import time\r\n"]
[605.106, "o", "import warnings\r\n"]
[605.116, "o", "from math import ceil\r\n"]
[605.126, "o", "from numbers import Integral, Real\r\n"]
[605.136, "o", "\r\n"]
[605.146, "o", "import numpy as np\r\n"]
[605.156, "o", "from joblib import effective_n_jobs\r\n"]
[605.166, "o", "from scipy import linalg\r\n"]
[605.176, "o", "\r\n"]
[605.186, "o", "from ..base import (\r\n"]
[605.196, "o", "    BaseEstimator,\r\n"]
[605.206, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[605.216, "o", "    TransformerMixin,\r\n"]
[605.226, "o", "    _fit_context,\r\n"]
[605.236, "o", ")\r\n"]
[605.246, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[605.256, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[605.266, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[605.276, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[605.286, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[605.296, "o", "from ..utils.validation import check_is_fitted\r\n"]
[605.306, "o", "\r\n"]
[605.316, "o", "\r\n"]
[605.326, "o", "def _check_positive_coding(method, positive):\r\n"]
[605.336, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[605.346, "o", "        raise ValueError(\r\n"]
[605.356, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[605.366, "o", "        )\r\n"]
[605.376, "o", "\r\n"]
[605.386, "o", "\r\n"]
[605.396, "o", "def _sparse_encode_precomputed(\r\n"]
[605.406, "o", "    X,\r\n"]
[605.416, "o", "    dictionary,\r\n"]
[605.426, "o", "    *,\r\n"]
[605.436, "o", "    gram=None,\r\n"]
[605.446, "o", "    cov=None,\r\n"]
[605.456, "o", "    algorithm=\"lasso_lars\",\r\n"]
[605.466, "o", "    regularization=None,\r\n"]
[605.476, "o", "    copy_cov=True,\r\n"]
[605.486, "o", "    init=None,\r\n"]
[605.496, "o", "    max_iter=1000,\r\n"]
[605.506, "o", "    verbose=0,\r\n"]
[605.516, "o", "    positive=False,\r\n"]
[605.526, "o", "):\r\n"]
[605.536, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[605.546, "o", "\r\n"]
[605.556, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[605.566, "o", "\r\n"]
[605.576, "o", "    Parameters\r\n"]
[605.586, "o", "    ----------\r\n"]
[605.596, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[605.606, "o", "        Data matrix.\r\n"]
[605.616, "o", "\r\n"]
[605.626, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[605.636, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[605.646, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[605.656, "o", "\r\n"]
[605.666, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[605.676, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[605.686, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[605.696, "o", "\r\n"]
[605.706, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[605.716, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[605.726, "o", "\r\n"]
[605.736, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[605.746, "o", "            default='lasso_lars'\r\n"]
[605.756, "o", "        The algorithm used:\r\n"]
[605.766, "o", "\r\n"]
[605.776, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[605.786, "o", "          (`linear_model.lars_path`);\r\n"]
[605.796, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[605.806, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[605.816, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[605.826, "o", "          the estimated components are sparse;\r\n"]
[605.836, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[605.846, "o", "          solution;\r\n"]
[605.856, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[605.866, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[605.876, "o", "\r\n"]
[605.886, "o", "    regularization : int or float, default=None\r\n"]
[605.896, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[605.906, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[605.916, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[605.926, "o", "\r\n"]
[605.936, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[605.946, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[605.956, "o", "        `algorithm='lasso_cd'`.\r\n"]
[605.966, "o", "\r\n"]
[605.976, "o", "    max_iter : int, default=1000\r\n"]
[605.986, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[605.996, "o", "        `'lasso_lars'`.\r\n"]
[606.006, "o", "\r\n"]
[606.016, "o", "    copy_cov : bool, default=True\r\n"]
[606.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[606.036, "o", "        be overwritten.\r\n"]
[606.046, "o", "\r\n"]
[606.056, "o", "    verbose : int, default=0\r\n"]
[606.066, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[606.076, "o", "\r\n"]
[606.086, "o", "    positive: bool, default=False\r\n"]
[606.096, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[606.106, "o", "\r\n"]
[606.116, "o", "        .. versionadded:: 0.20\r\n"]
[606.126, "o", "\r\n"]
[606.136, "o", "    Returns\r\n"]
[606.146, "o", "    -------\r\n"]
[606.156, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[606.166, "o", "        The sparse codes.\r\n"]
[606.176, "o", "    \"\"\"\r\n"]
[606.186, "o", "    n_samples, n_features = X.shape\r\n"]
[606.196, "o", "    n_components = dictionary.shape[0]\r\n"]
[606.206, "o", "\r\n"]
[606.216, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[606.226, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[606.236, "o", "        try:\r\n"]
[606.246, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[606.256, "o", "\r\n"]
[606.266, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[606.276, "o", "            # corrects the verbosity level.\r\n"]
[606.286, "o", "            lasso_lars = LassoLars(\r\n"]
[606.296, "o", "                alpha=alpha,\r\n"]
[606.306, "o", "                fit_intercept=False,\r\n"]
[606.316, "o", "                verbose=verbose,\r\n"]
[606.326, "o", "                precompute=gram,\r\n"]
[606.336, "o", "                fit_path=False,\r\n"]
[606.346, "o", "                positive=positive,\r\n"]
[606.356, "o", "                max_iter=max_iter,\r\n"]
[606.366, "o", "            )\r\n"]
[606.376, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[606.386, "o", "            new_code = lasso_lars.coef_\r\n"]
[606.396, "o", "        finally:\r\n"]
[606.406, "o", "            np.seterr(**err_mgt)\r\n"]
[606.416, "o", "\r\n"]
[606.426, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[606.436, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[606.446, "o", "\r\n"]
[606.456, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[606.466, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[606.476, "o", "        # argument that we could pass in from Lasso.\r\n"]
[606.486, "o", "        clf = Lasso(\r\n"]
[606.496, "o", "            alpha=alpha,\r\n"]
[606.506, "o", "            fit_intercept=False,\r\n"]
[606.516, "o", "            precompute=gram,\r\n"]
[606.526, "o", "            max_iter=max_iter,\r\n"]
[606.536, "o", "            warm_start=True,\r\n"]
[606.546, "o", "            positive=positive,\r\n"]
[606.556, "o", "        )\r\n"]
[606.566, "o", "\r\n"]
[606.576, "o", "        if init is not None:\r\n"]
[606.586, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[606.596, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[606.606, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[606.616, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[606.626, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[606.636, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[606.646, "o", "                init = np.array(init)\r\n"]
[606.656, "o", "            clf.coef_ = init\r\n"]
[606.666, "o", "\r\n"]
[606.676, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[606.686, "o", "        new_code = clf.coef_\r\n"]
[606.696, "o", "\r\n"]
[606.706, "o", "    elif algorithm == \"lars\":\r\n"]
[606.716, "o", "        try:\r\n"]
[606.726, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[606.736, "o", "\r\n"]
[606.746, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[606.756, "o", "            # corrects the verbosity level.\r\n"]
[606.766, "o", "            lars = Lars(\r\n"]
[606.776, "o", "                fit_intercept=False,\r\n"]
[606.786, "o", "                verbose=verbose,\r\n"]
[606.796, "o", "                precompute=gram,\r\n"]
[606.806, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[606.816, "o", "                fit_path=False,\r\n"]
[606.826, "o", "            )\r\n"]
[606.836, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[606.846, "o", "            new_code = lars.coef_\r\n"]
[606.856, "o", "        finally:\r\n"]
[606.866, "o", "            np.seterr(**err_mgt)\r\n"]
[606.876, "o", "\r\n"]
[606.886, "o", "    elif algorithm == \"threshold\":\r\n"]
[606.896, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[606.906, "o", "        if positive:\r\n"]
[606.916, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[606.926, "o", "\r\n"]
[606.936, "o", "    elif algorithm == \"omp\":\r\n"]
[606.946, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[606.956, "o", "            Gram=gram,\r\n"]
[606.966, "o", "            Xy=cov,\r\n"]
[606.976, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[606.986, "o", "            tol=None,\r\n"]
[606.996, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[607.006, "o", "            copy_Xy=copy_cov,\r\n"]
[607.016, "o", "        ).T\r\n"]
[607.026, "o", "\r\n"]
[607.036, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[607.046, "o", "\r\n"]
[607.056, "o", "\r\n"]
[607.066, "o", "@validate_params(\r\n"]
[607.076, "o", "    {\r\n"]
[607.086, "o", "        \"X\": [\"array-like\"],\r\n"]
[607.096, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[607.106, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[607.116, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[607.126, "o", "        \"algorithm\": [\r\n"]
[607.136, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[607.146, "o", "        ],\r\n"]
[607.156, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[607.166, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[607.176, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[607.186, "o", "        \"init\": [\"array-like\", None],\r\n"]
[607.196, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[607.206, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[607.216, "o", "        \"check_input\": [\"boolean\"],\r\n"]
[607.226, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[607.236, "o", "        \"positive\": [\"boolean\"],\r\n"]
[607.246, "o", "    },\r\n"]
[607.256, "o", "    prefer_skip_nested_validation=True,\r\n"]
[607.266, "o", ")\r\n"]
[607.276, "o", "# XXX : could be moved to the linear_model module\r\n"]
[607.286, "o", "def sparse_encode(\r\n"]
[607.296, "o", "    X,\r\n"]
[607.306, "o", "    dictionary,\r\n"]
[607.316, "o", "    *,\r\n"]
[607.326, "o", "    gram=None,\r\n"]
[607.336, "o", "    cov=None,\r\n"]
[607.346, "o", "    algorithm=\"lasso_lars\",\r\n"]
[607.356, "o", "    n_nonzero_coefs=None,\r\n"]
[607.366, "o", "    alpha=None,\r\n"]
[607.376, "o", "    copy_cov=True,\r\n"]
[607.386, "o", "    init=None,\r\n"]
[607.396, "o", "    max_iter=1000,\r\n"]
[607.406, "o", "    n_jobs=None,\r\n"]
[607.416, "o", "    check_input=True,\r\n"]
[607.426, "o", "    verbose=0,\r\n"]
[607.436, "o", "    positive=False,\r\n"]
[607.446, "o", "):\r\n"]
[607.456, "o", "    \"\"\"Sparse coding.\r\n"]
[607.466, "o", "\r\n"]
[607.476, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[607.486, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[607.496, "o", "\r\n"]
[607.506, "o", "        X ~= code * dictionary\r\n"]
[607.516, "o", "\r\n"]
[607.526, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[607.536, "o", "\r\n"]
[607.546, "o", "    Parameters\r\n"]
[607.556, "o", "    ----------\r\n"]
[607.566, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[607.576, "o", "        Data matrix.\r\n"]
[607.586, "o", "\r\n"]
[607.596, "o", "    dictionary : array-like of shape (n_components, n_features)\r\n"]
[607.606, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[607.616, "o", "        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[607.626, "o", "        output.\r\n"]
[607.636, "o", "\r\n"]
[607.646, "o", "    gram : array-like of shape (n_components, n_components), default=None\r\n"]
[607.656, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n"]
[607.666, "o", "\r\n"]
[607.676, "o", "    cov : array-like of shape (n_components, n_samples), default=None\r\n"]
[607.686, "o", "        Precomputed covariance, `dictionary' * X`.\r\n"]
[607.696, "o", "\r\n"]
[607.706, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[607.716, "o", "            default='lasso_lars'\r\n"]
[607.726, "o", "        The algorithm used:\r\n"]
[607.736, "o", "\r\n"]
[607.746, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[607.756, "o", "          (`linear_model.lars_path`);\r\n"]
[607.766, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[607.776, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[607.786, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[607.796, "o", "          the estimated components are sparse;\r\n"]
[607.806, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[607.816, "o", "          solution;\r\n"]
[607.826, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[607.836, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[607.846, "o", "\r\n"]
[607.856, "o", "    n_nonzero_coefs : int, default=None\r\n"]
[607.866, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[607.876, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[607.886, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[607.896, "o", "        `n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[607.906, "o", "\r\n"]
[607.916, "o", "    alpha : float, default=None\r\n"]
[607.926, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[607.936, "o", "        penalty applied to the L1 norm.\r\n"]
[607.946, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[607.956, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[607.966, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[607.976, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[607.986, "o", "        `n_nonzero_coefs`.\r\n"]
[607.996, "o", "        If `None`, default to 1.\r\n"]
[608.006, "o", "\r\n"]
[608.016, "o", "    copy_cov : bool, default=True\r\n"]
[608.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[608.036, "o", "        be overwritten.\r\n"]
[608.046, "o", "\r\n"]
[608.056, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[608.066, "o", "        Initialization value of the sparse codes. Only used if\r\n"]
[608.076, "o", "        `algorithm='lasso_cd'`.\r\n"]
[608.086, "o", "\r\n"]
[608.096, "o", "    max_iter : int, default=1000\r\n"]
[608.106, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[608.116, "o", "        `'lasso_lars'`.\r\n"]
[608.126, "o", "\r\n"]
[608.136, "o", "    n_jobs : int, default=None\r\n"]
[608.146, "o", "        Number of parallel jobs to run.\r\n"]
[608.156, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[608.166, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[608.176, "o", "        for more details.\r\n"]
[608.186, "o", "\r\n"]
[608.196, "o", "    check_input : bool, default=True\r\n"]
[608.206, "o", "        If `False`, the input arrays X and dictionary will not be checked.\r\n"]
[608.216, "o", "\r\n"]
[608.226, "o", "    verbose : int, default=0\r\n"]
[608.236, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[608.246, "o", "\r\n"]
[608.256, "o", "    positive : bool, default=False\r\n"]
[608.266, "o", "        Whether to enforce positivity when finding the encoding.\r\n"]
[608.276, "o", "\r\n"]
[608.286, "o", "        .. versionadded:: 0.20\r\n"]
[608.296, "o", "\r\n"]
[608.306, "o", "    Returns\r\n"]
[608.316, "o", "    -------\r\n"]
[608.326, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[608.336, "o", "        The sparse codes.\r\n"]
[608.346, "o", "\r\n"]
[608.356, "o", "    See Also\r\n"]
[608.366, "o", "    --------\r\n"]
[608.376, "o", "    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n"]
[608.386, "o", "        path using LARS algorithm.\r\n"]
[608.396, "o", "    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n"]
[608.406, "o", "    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n"]
[608.416, "o", "    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n"]
[608.426, "o", "        dictionary.\r\n"]
[608.436, "o", "    \"\"\"\r\n"]
[608.446, "o", "    if check_input:\r\n"]
[608.456, "o", "        if algorithm == \"lasso_cd\":\r\n"]
[608.466, "o", "            dictionary = check_array(\r\n"]
[608.476, "o", "                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n"]
[608.486, "o", "            )\r\n"]
[608.496, "o", "            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n"]
[608.506, "o", "        else:\r\n"]
[608.516, "o", "            dictionary = check_array(dictionary)\r\n"]
[608.526, "o", "            X = check_array(X)\r\n"]
[608.536, "o", "\r\n"]
[608.546, "o", "    if dictionary.shape[1] != X.shape[1]:\r\n"]
[608.556, "o", "        raise ValueError(\r\n"]
[608.566, "o", "            \"Dictionary and X have different numbers of features:\"\r\n"]
[608.576, "o", "            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n"]
[608.586, "o", "        )\r\n"]
[608.596, "o", "\r\n"]
[608.606, "o", "    _check_positive_coding(algorithm, positive)\r\n"]
[608.616, "o", "\r\n"]
[608.626, "o", "    return _sparse_encode(\r\n"]
[608.636, "o", "        X,\r\n"]
[608.646, "o", "        dictionary,\r\n"]
[608.656, "o", "        gram=gram,\r\n"]
[608.666, "o", "        cov=cov,\r\n"]
[608.676, "o", "        algorithm=algorithm,\r\n"]
[608.686, "o", "        n_nonzero_coefs=n_nonzero_coefs,\r\n"]
[608.696, "o", "        alpha=alpha,\r\n"]
[608.706, "o", "        copy_cov=copy_cov,\r\n"]
[608.716, "o", "        init=init,\r\n"]
[608.726, "o", "        max_iter=max_iter,\r\n"]
[608.736, "o", "        n_jobs=n_jobs,\r\n"]
[608.746, "o", "        verbose=verbose,\r\n"]
[608.756, "o", "        positive=positive,\r\n"]
[608.766, "o", "    )\r\n"]
[608.776, "o", "\r\n"]
[608.786, "o", "\r\n"]
[608.796, "o", "def _sparse_encode(\r\n"]
[608.806, "o", "    X,\r\n"]
[608.816, "o", "    dictionary,\r\n"]
[608.826, "o", "    *,\r\n"]
[608.836, "o", "    gram=None,\r\n"]
[608.846, "o", "    cov=None,\r\n"]
[608.856, "o", "    algorithm=\"lasso_lars\",\r\n"]
[608.866, "o", "    n_nonzero_coefs=None,\r\n"]
[608.876, "o", "    alpha=None,\r\n"]
[608.886, "o", "    copy_cov=True,\r\n"]
[608.896, "o", "    init=None,\r\n"]
[608.906, "o", "    max_iter=1000,\r\n"]
[608.916, "o", "    n_jobs=None,\r\n"]
[608.926, "o", "    verbose=0,\r\n"]
[608.936, "o", "    positive=False,\r\n"]
[608.946, "o", "):\r\n"]
[608.956, "o", "    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[608.966, "o", "\r\n"]
[608.976, "o", "    n_samples, n_features = X.shape\r\n"]
[608.986, "o", "    n_components = dictionary.shape[0]\r\n"]
[608.996, "o", "\r\n"]
[609.006, "o", "    if algorithm in (\"lars\", \"omp\"):\r\n"]
[609.016, "o", "        regularization = n_nonzero_coefs\r\n"]
[609.026, "o", "        if regularization is None:\r\n"]
[609.036, "o", "            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[609.046, "o", "    else:\r\n"]
[609.056, "o", "        regularization = alpha\r\n"]
[609.066, "o", "        if regularization is None:\r\n"]
[609.076, "o", "            regularization = 1.0\r\n"]
[609.086, "o", "\r\n"]
[609.096, "o", "    if gram is None and algorithm != \"threshold\":\r\n"]
[609.106, "o", "        gram = np.dot(dictionary, dictionary.T)\r\n"]
[609.116, "o", "\r\n"]
[609.126, "o", "    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[609.136, "o", "        copy_cov = False\r\n"]
[609.146, "o", "        cov = np.dot(dictionary, X.T)\r\n"]
[609.156, "o", "\r\n"]
[609.166, "o", "    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[609.176, "o", "        code = _sparse_encode_precomputed(\r\n"]
[609.186, "o", "            X,\r\n"]
[609.196, "o", "            dictionary,\r\n"]
[609.206, "o", "            gram=gram,\r\n"]
[609.216, "o", "            cov=cov,\r\n"]
[609.226, "o", "            algorithm=algorithm,\r\n"]
[609.236, "o", "            regularization=regularization,\r\n"]
[609.246, "o", "            copy_cov=copy_cov,\r\n"]
[609.256, "o", "            init=init,\r\n"]
[609.266, "o", "            max_iter=max_iter,\r\n"]
[609.276, "o", "            verbose=verbose,\r\n"]
[609.286, "o", "            positive=positive,\r\n"]
[609.296, "o", "        )\r\n"]
[609.306, "o", "        return code\r\n"]
[609.316, "o", "\r\n"]
[609.326, "o", "    # Enter parallel code block\r\n"]
[609.336, "o", "    n_samples = X.shape[0]\r\n"]
[609.346, "o", "    n_components = dictionary.shape[0]\r\n"]
[609.356, "o", "    code = np.empty((n_samples, n_components))\r\n"]
[609.366, "o", "    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[609.376, "o", "\r\n"]
[609.386, "o", "    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[609.396, "o", "        delayed(_sparse_encode_precomputed)(\r\n"]
[609.406, "o", "            X[this_slice],\r\n"]
[609.416, "o", "            dictionary,\r\n"]
[609.426, "o", "            gram=gram,\r\n"]
[609.436, "o", "            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[609.446, "o", "            algorithm=algorithm,\r\n"]
[609.456, "o", "            regularization=regularization,\r\n"]
[609.466, "o", "            copy_cov=copy_cov,\r\n"]
[609.476, "o", "            init=init[this_slice] if init is not None else None,\r\n"]
[609.486, "o", "            max_iter=max_iter,\r\n"]
[609.496, "o", "            verbose=verbose,\r\n"]
[609.506, "o", "            positive=positive,\r\n"]
[609.516, "o", "        )\r\n"]
[609.526, "o", "        for this_slice in slices\r\n"]
[609.536, "o", "    )\r\n"]
[609.546, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[609.556, "o", "        code[this_slice] = this_view\r\n"]
[609.566, "o", "    return code\r\n"]
[609.576, "o", "\r\n"]
[609.586, "o", "\r\n"]
[609.596, "o", "def _update_dict(\r\n"]
[609.606, "o", "    dictionary,\r\n"]
[609.616, "o", "    Y,\r\n"]
[609.626, "o", "    code,\r\n"]
[609.636, "o", "    A=None,\r\n"]
[609.646, "o", "    B=None,\r\n"]
[609.656, "o", "    verbose=False,\r\n"]
[609.666, "o", "    random_state=None,\r\n"]
[609.676, "o", "    positive=False,\r\n"]
[609.686, "o", "):\r\n"]
[609.696, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[609.706, "o", "\r\n"]
[609.716, "o", "    Parameters\r\n"]
[609.726, "o", "    ----------\r\n"]
[609.736, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[609.746, "o", "        Value of the dictionary at the previous iteration.\r\n"]
[609.756, "o", "\r\n"]
[609.766, "o", "    Y : ndarray of shape (n_samples, n_features)\r\n"]
[609.776, "o", "        Data matrix.\r\n"]
[609.786, "o", "\r\n"]
[609.796, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[609.806, "o", "        Sparse coding of the data against which to optimize the dictionary.\r\n"]
[609.816, "o", "\r\n"]
[609.826, "o", "    A : ndarray of shape (n_components, n_components), default=None\r\n"]
[609.836, "o", "        Together with `B`, sufficient stats of the online model to update the\r\n"]
[609.846, "o", "        dictionary.\r\n"]
[609.856, "o", "\r\n"]
[609.866, "o", "    B : ndarray of shape (n_features, n_components), default=None\r\n"]
[609.876, "o", "        Together with `A`, sufficient stats of the online model to update the\r\n"]
[609.886, "o", "        dictionary.\r\n"]
[609.896, "o", "\r\n"]
[609.906, "o", "    verbose: bool, default=False\r\n"]
[609.916, "o", "        Degree of output the procedure will print.\r\n"]
[609.926, "o", "\r\n"]
[609.936, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[610.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[610.002, "i", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r"]
[610.004, "o", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r\n"]
[610.016, "o", "\u001b[?2004l\r\n"]
[610.026, "o", "           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[610.036, "o", "\r\n"]
[610.046, "o", "    return_n_iter : bool, default=False\r\n"]
[610.056, "o", "        Whether or not to return the number of iterations.\r\n"]
[610.066, "o", "\r\n"]
[610.076, "o", "        .. deprecated:: 1.1\r\n"]
[610.086, "o", "           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n"]
[610.096, "o", "\r\n"]
[610.106, "o", "    positive_dict : bool, default=False\r\n"]
[610.116, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[610.126, "o", "\r\n"]
[610.136, "o", "        .. versionadded:: 0.20\r\n"]
[610.146, "o", "\r\n"]
[610.156, "o", "    positive_code : bool, default=False\r\n"]
[610.166, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[610.176, "o", "\r\n"]
[610.186, "o", "        .. versionadded:: 0.20\r\n"]
[610.196, "o", "\r\n"]
[610.206, "o", "    method_max_iter : int, default=1000\r\n"]
[610.216, "o", "        Maximum number of iterations to perform when solving the lasso problem.\r\n"]
[610.226, "o", "\r\n"]
[610.236, "o", "        .. versionadded:: 0.22\r\n"]
[610.246, "o", "\r\n"]
[610.256, "o", "    tol : float, default=1e-3\r\n"]
[610.266, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[610.276, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[610.286, "o", "\r\n"]
[610.296, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[610.306, "o", "        `tol` to 0.0.\r\n"]
[610.316, "o", "\r\n"]
[610.326, "o", "        .. versionadded:: 1.1\r\n"]
[610.336, "o", "\r\n"]
[610.346, "o", "    max_no_improvement : int, default=10\r\n"]
[610.356, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[610.366, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[610.376, "o", "        `max_iter` is not None.\r\n"]
[610.386, "o", "\r\n"]
[610.396, "o", "        To disable convergence detection based on cost function, set\r\n"]
[610.406, "o", "        `max_no_improvement` to None.\r\n"]
[610.416, "o", "\r\n"]
[610.426, "o", "        .. versionadded:: 1.1\r\n"]
[610.436, "o", "\r\n"]
[610.446, "o", "    Returns\r\n"]
[610.456, "o", "    -------\r\n"]
[610.466, "o", "    code : ndarray of shape (n_samples, n_components),\r\n"]
[610.476, "o", "        The sparse code (only returned if `return_code=True`).\r\n"]
[610.486, "o", "\r\n"]
[610.496, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[610.506, "o", "        The solutions to the dictionary learning problem.\r\n"]
[610.516, "o", "\r\n"]
[610.526, "o", "    n_iter : int\r\n"]
[610.536, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[610.546, "o", "        set to `True`.\r\n"]
[610.556, "o", "\r\n"]
[610.566, "o", "    See Also\r\n"]
[610.576, "o", "    --------\r\n"]
[610.586, "o", "    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n"]
[610.596, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[610.606, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n"]
[610.616, "o", "        learning algorithm.\r\n"]
[610.626, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[610.636, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[610.646, "o", "    \"\"\"\r\n"]
[610.656, "o", "    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n"]
[610.666, "o", "    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n"]
[610.676, "o", "        raise ValueError(\r\n"]
[610.686, "o", "            \"The following arguments are incompatible with 'max_iter': \"\r\n"]
[610.696, "o", "            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n"]
[610.706, "o", "        )\r\n"]
[610.716, "o", "\r\n"]
[610.726, "o", "    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n"]
[610.736, "o", "    return_inner_stats = _check_warn_deprecated(\r\n"]
[610.746, "o", "        return_inner_stats,\r\n"]
[610.756, "o", "        \"return_inner_stats\",\r\n"]
[610.766, "o", "        default=False,\r\n"]
[610.776, "o", "        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n"]
[610.786, "o", "    )\r\n"]
[610.796, "o", "    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n"]
[610.806, "o", "    return_n_iter = _check_warn_deprecated(\r\n"]
[610.816, "o", "        return_n_iter,\r\n"]
[610.826, "o", "        \"return_n_iter\",\r\n"]
[610.836, "o", "        default=False,\r\n"]
[610.846, "o", "        additional_message=(\r\n"]
[610.856, "o", "            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n"]
[610.866, "o", "            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n"]
[610.876, "o", "        ),\r\n"]
[610.886, "o", "    )\r\n"]
[610.896, "o", "\r\n"]
[610.906, "o", "    if max_iter is not None:\r\n"]
[610.916, "o", "        transform_algorithm = \"lasso_\" + method\r\n"]
[610.926, "o", "\r\n"]
[610.936, "o", "        est = MiniBatchDictionaryLearning(\r\n"]
[610.946, "o", "            n_components=n_components,\r\n"]
[610.956, "o", "            alpha=alpha,\r\n"]
[610.966, "o", "            n_iter=n_iter,\r\n"]
[610.976, "o", "            n_jobs=n_jobs,\r\n"]
[610.986, "o", "            fit_algorithm=method,\r\n"]
[610.996, "o", "            batch_size=batch_size,\r\n"]
[611.006, "o", "            shuffle=shuffle,\r\n"]
[611.016, "o", "            dict_init=dict_init,\r\n"]
[611.026, "o", "            random_state=random_state,\r\n"]
[611.036, "o", "            transform_algorithm=transform_algorithm,\r\n"]
[611.046, "o", "            transform_alpha=alpha,\r\n"]
[611.056, "o", "            positive_code=positive_code,\r\n"]
[611.066, "o", "            positive_dict=positive_dict,\r\n"]
[611.076, "o", "            transform_max_iter=method_max_iter,\r\n"]
[611.086, "o", "            verbose=verbose,\r\n"]
[611.096, "o", "            callback=callback,\r\n"]
[611.106, "o", "            tol=tol,\r\n"]
[611.116, "o", "            max_no_improvement=max_no_improvement,\r\n"]
[611.126, "o", "        ).fit(X)\r\n"]
[611.136, "o", "\r\n"]
[611.146, "o", "        if not return_code:\r\n"]
[611.156, "o", "            return est.components_\r\n"]
[611.166, "o", "        else:\r\n"]
[611.176, "o", "            code = est.transform(X)\r\n"]
[611.186, "o", "            return code, est.components_\r\n"]
[611.196, "o", "\r\n"]
[611.206, "o", "    # TODO(1.4) remove the whole old behavior\r\n"]
[611.216, "o", "    # Fallback to old behavior\r\n"]
[611.226, "o", "\r\n"]
[611.236, "o", "    n_iter = _check_warn_deprecated(\r\n"]
[611.246, "o", "        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n"]
[611.256, "o", "    )\r\n"]
[611.266, "o", "\r\n"]
[611.276, "o", "    if n_components is None:\r\n"]
[611.286, "o", "        n_components = X.shape[1]\r\n"]
[611.296, "o", "\r\n"]
[611.306, "o", "    if method not in (\"lars\", \"cd\"):\r\n"]
[611.316, "o", "        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n"]
[611.326, "o", "\r\n"]
[611.336, "o", "    _check_positive_coding(method, positive_code)\r\n"]
[611.346, "o", "\r\n"]
[611.356, "o", "    method = \"lasso_\" + method\r\n"]
[611.366, "o", "\r\n"]
[611.376, "o", "    t0 = time.time()\r\n"]
[611.386, "o", "    n_samples, n_features = X.shape\r\n"]
[611.396, "o", "    # Avoid integer division problems\r\n"]
[611.406, "o", "    alpha = float(alpha)\r\n"]
[611.416, "o", "    random_state = check_random_state(random_state)\r\n"]
[611.426, "o", "\r\n"]
[611.436, "o", "    # Init V with SVD of X\r\n"]
[611.446, "o", "    if dict_init is not None:\r\n"]
[611.456, "o", "        dictionary = dict_init\r\n"]
[611.466, "o", "    else:\r\n"]
[611.476, "o", "        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n"]
[611.486, "o", "        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[611.496, "o", "    r = len(dictionary)\r\n"]
[611.506, "o", "    if n_components <= r:\r\n"]
[611.516, "o", "        dictionary = dictionary[:n_components, :]\r\n"]
[611.526, "o", "    else:\r\n"]
[611.536, "o", "        dictionary = np.r_[\r\n"]
[611.546, "o", "            dictionary,\r\n"]
[611.556, "o", "            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n"]
[611.566, "o", "        ]\r\n"]
[611.576, "o", "\r\n"]
[611.586, "o", "    if verbose == 1:\r\n"]
[611.596, "o", "        print(\"[dict_learning]\", end=\" \")\r\n"]
[611.606, "o", "\r\n"]
[611.616, "o", "    if shuffle:\r\n"]
[611.626, "o", "        X_train = X.copy()\r\n"]
[611.636, "o", "        random_state.shuffle(X_train)\r\n"]
[611.646, "o", "    else:\r\n"]
[611.656, "o", "        X_train = X\r\n"]
[611.666, "o", "\r\n"]
[611.676, "o", "    X_train = check_array(\r\n"]
[611.686, "o", "        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n"]
[611.696, "o", "    )\r\n"]
[611.706, "o", "\r\n"]
[611.716, "o", "    # Fortran-order dict better suited for the sparse coding which is the\r\n"]
[611.726, "o", "    # bottleneck of this algorithm.\r\n"]
[611.736, "o", "    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n"]
[611.746, "o", "    dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[611.756, "o", "\r\n"]
[611.766, "o", "    batches = gen_batches(n_samples, batch_size)\r\n"]
[611.776, "o", "    batches = itertools.cycle(batches)\r\n"]
[611.786, "o", "\r\n"]
[611.796, "o", "    # The covariance of the dictionary\r\n"]
[611.806, "o", "    if inner_stats is None:\r\n"]
[611.816, "o", "        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n"]
[611.826, "o", "        # The data approximation\r\n"]
[611.836, "o", "        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n"]
[611.846, "o", "    else:\r\n"]
[611.856, "o", "        A = inner_stats[0].copy()\r\n"]
[611.866, "o", "        B = inner_stats[1].copy()\r\n"]
[611.876, "o", "\r\n"]
[611.886, "o", "    # If n_iter is zero, we need to return zero.\r\n"]
[611.896, "o", "    ii = iter_offset - 1\r\n"]
[611.906, "o", "\r\n"]
[611.916, "o", "    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n"]
[611.926, "o", "        this_X = X_train[batch]\r\n"]
[611.936, "o", "        dt = time.time() - t0\r\n"]
[611.946, "o", "        if verbose == 1:\r\n"]
[611.956, "o", "            sys.stdout.write(\".\")\r\n"]
[611.966, "o", "            sys.stdout.flush()\r\n"]
[611.976, "o", "        elif verbose:\r\n"]
[611.986, "o", "            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n"]
[611.996, "o", "                print(\r\n"]
[612.006, "o", "                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n"]
[612.016, "o", "                )\r\n"]
[612.026, "o", "\r\n"]
[612.036, "o", "        this_code = sparse_encode(\r\n"]
[612.046, "o", "            this_X,\r\n"]
[612.056, "o", "            dictionary,\r\n"]
[612.066, "o", "            algorithm=method,\r\n"]
[612.076, "o", "            alpha=alpha,\r\n"]
[612.086, "o", "            n_jobs=n_jobs,\r\n"]
[612.096, "o", "            check_input=False,\r\n"]
[612.106, "o", "            positive=positive_code,\r\n"]
[612.116, "o", "            max_iter=method_max_iter,\r\n"]
[612.126, "o", "            verbose=verbose,\r\n"]
[612.136, "o", "        )\r\n"]
[612.146, "o", "\r\n"]
[612.156, "o", "        # Update the auxiliary variables\r\n"]
[612.166, "o", "        if ii < batch_size - 1:\r\n"]
[612.176, "o", "            theta = float((ii + 1) * batch_size)\r\n"]
[612.186, "o", "        else:\r\n"]
[612.196, "o", "            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n"]
[612.206, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[612.216, "o", "\r\n"]
[612.226, "o", "        A *= beta\r\n"]
[612.236, "o", "        A += np.dot(this_code.T, this_code)\r\n"]
[612.246, "o", "        B *= beta\r\n"]
[612.256, "o", "        B += np.dot(this_X.T, this_code)\r\n"]
[612.266, "o", "\r\n"]
[612.276, "o", "        # Update dictionary in place\r\n"]
[612.286, "o", "        _update_dict(\r\n"]
[612.296, "o", "            dictionary,\r\n"]
[612.306, "o", "            this_X,\r\n"]
[612.316, "o", "            this_code,\r\n"]
[612.326, "o", "            A,\r\n"]
[612.336, "o", "            B,\r\n"]
[612.346, "o", "            verbose=verbose,\r\n"]
[612.356, "o", "            random_state=random_state,\r\n"]
[612.366, "o", "            positive=positive_dict,\r\n"]
[612.376, "o", "        )\r\n"]
[612.386, "o", "\r\n"]
[612.396, "o", "        # Maybe we need a stopping criteria based on the amount of\r\n"]
[612.406, "o", "        # modification in the dictionary\r\n"]
[612.416, "o", "        if callback is not None:\r\n"]
[612.426, "o", "            callback(locals())\r\n"]
[612.436, "o", "\r\n"]
[612.446, "o", "    if return_inner_stats:\r\n"]
[612.456, "o", "        if return_n_iter:\r\n"]
[612.466, "o", "            return dictionary, (A, B), ii - iter_offset + 1\r\n"]
[612.476, "o", "        else:\r\n"]
[612.486, "o", "            return dictionary, (A, B)\r\n"]
[612.496, "o", "    if return_code:\r\n"]
[612.506, "o", "        if verbose > 1:\r\n"]
[612.516, "o", "            print(\"Learning code...\", end=\" \")\r\n"]
[612.526, "o", "        elif verbose == 1:\r\n"]
[612.536, "o", "            print(\"|\", end=\" \")\r\n"]
[612.546, "o", "        code = sparse_encode(\r\n"]
[612.556, "o", "            X,\r\n"]
[612.566, "o", "            dictionary,\r\n"]
[612.576, "o", "            algorithm=method,\r\n"]
[612.586, "o", "            alpha=alpha,\r\n"]
[612.596, "o", "            n_jobs=n_jobs,\r\n"]
[612.606, "o", "            check_input=False,\r\n"]
[612.616, "o", "            positive=positive_code,\r\n"]
[612.626, "o", "            max_iter=method_max_iter,\r\n"]
[612.636, "o", "            verbose=verbose,\r\n"]
[612.646, "o", "        )\r\n"]
[612.656, "o", "        if verbose > 1:\r\n"]
[612.666, "o", "            dt = time.time() - t0\r\n"]
[612.676, "o", "            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n"]
[612.686, "o", "        if return_n_iter:\r\n"]
[612.696, "o", "            return code, dictionary, ii - iter_offset + 1\r\n"]
[612.706, "o", "        else:\r\n"]
[612.716, "o", "            return code, dictionary\r\n"]
[612.726, "o", "\r\n"]
[612.736, "o", "    if return_n_iter:\r\n"]
[612.746, "o", "        return dictionary, ii - iter_offset + 1\r\n"]
[612.756, "o", "    else:\r\n"]
[612.766, "o", "        return dictionary\r\n"]
[612.776, "o", "\r\n"]
[612.786, "o", "\r\n"]
[612.796, "o", "@validate_params(\r\n"]
[612.806, "o", "    {\r\n"]
[612.816, "o", "        \"X\": [\"array-like\"],\r\n"]
[612.826, "o", "        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[612.836, "o", "        \"return_n_iter\": [\"boolean\"],\r\n"]
[612.846, "o", "        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[612.856, "o", "    },\r\n"]
[612.866, "o", "    prefer_skip_nested_validation=False,\r\n"]
[612.876, "o", ")\r\n"]
[612.886, "o", "def dict_learning(\r\n"]
[612.896, "o", "    X,\r\n"]
[612.906, "o", "    n_components,\r\n"]
[612.916, "o", "    *,\r\n"]
[612.926, "o", "    alpha,\r\n"]
[612.936, "o", "    max_iter=100,\r\n"]
[612.946, "o", "    tol=1e-8,\r\n"]
[612.956, "o", "    method=\"lars\",\r\n"]
[612.966, "o", "    n_jobs=None,\r\n"]
[612.976, "o", "    dict_init=None,\r\n"]
[612.986, "o", "    code_init=None,\r\n"]
[612.996, "o", "    callback=None,\r\n"]
[613.006, "o", "    verbose=False,\r\n"]
[613.016, "o", "    random_state=None,\r\n"]
[613.026, "o", "    return_n_iter=False,\r\n"]
[613.036, "o", "    positive_dict=False,\r\n"]
[613.046, "o", "    positive_code=False,\r\n"]
[613.056, "o", "    method_max_iter=1000,\r\n"]
[613.066, "o", "):\r\n"]
[613.076, "o", "    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n"]
[613.086, "o", "\r\n"]
[613.096, "o", "    Finds the best dictionary and the corresponding sparse code for\r\n"]
[613.106, "o", "    approximating the data matrix X by solving::\r\n"]
[613.116, "o", "\r\n"]
[613.126, "o", "        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[613.136, "o", "                     (U,V)\r\n"]
[613.146, "o", "                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n"]
[613.156, "o", "\r\n"]
[613.166, "o", "    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n"]
[613.176, "o", "    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n"]
[613.186, "o", "    which is the sum of the absolute values of all the entries in the matrix.\r\n"]
[613.196, "o", "\r\n"]
[613.206, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[613.216, "o", "\r\n"]
[613.226, "o", "    Parameters\r\n"]
[613.236, "o", "    ----------\r\n"]
[613.246, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[613.256, "o", "        Data matrix.\r\n"]
[613.266, "o", "\r\n"]
[613.276, "o", "    n_components : int\r\n"]
[613.286, "o", "        Number of dictionary atoms to extract.\r\n"]
[613.296, "o", "\r\n"]
[613.306, "o", "    alpha : int or float\r\n"]
[613.316, "o", "        Sparsity controlling parameter.\r\n"]
[613.326, "o", "\r\n"]
[613.336, "o", "    max_iter : int, default=100\r\n"]
[613.346, "o", "        Maximum number of iterations to perform.\r\n"]
[613.356, "o", "\r\n"]
[613.366, "o", "    tol : float, default=1e-8\r\n"]
[613.376, "o", "        Tolerance for the stopping condition.\r\n"]
[613.386, "o", "\r\n"]
[613.396, "o", "    method : {'lars', 'cd'}, default='lars'\r\n"]
[613.406, "o", "        The method used:\r\n"]
[613.416, "o", "\r\n"]
[613.426, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[613.436, "o", "           problem (`linear_model.lars_path`);\r\n"]
[613.446, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[613.456, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[613.466, "o", "          the estimated components are sparse.\r\n"]
[613.476, "o", "\r\n"]
[613.486, "o", "    n_jobs : int, default=None\r\n"]
[613.496, "o", "        Number of parallel jobs to run.\r\n"]
[613.506, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[613.516, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[613.526, "o", "        for more details.\r\n"]
[613.536, "o", "\r\n"]
[613.546, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[613.556, "o", "        Initial value for the dictionary for warm restart scenarios. Only used\r\n"]
[613.566, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[613.576, "o", "\r\n"]
[613.586, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[613.596, "o", "        Initial value for the sparse code for warm restart scenarios. Only used\r\n"]
[613.606, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[613.616, "o", "\r\n"]
[613.626, "o", "    callback : callable, default=None\r\n"]
[613.636, "o", "        Callable that gets invoked every five iterations.\r\n"]
[613.646, "o", "\r\n"]
[613.656, "o", "    verbose : bool, default=False\r\n"]
[613.666, "o", "        To control the verbosity of the procedure.\r\n"]
[613.676, "o", "\r\n"]
[613.686, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[613.696, "o", "        Used for randomly initializing the dictionary. Pass an int for\r\n"]
[613.706, "o", "        reproducible results across multiple function calls.\r\n"]
[613.716, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[613.726, "o", "\r\n"]
[613.736, "o", "    return_n_iter : bool, default=False\r\n"]
[613.746, "o", "        Whether or not to return the number of iterations.\r\n"]
[613.756, "o", "\r\n"]
[613.766, "o", "    positive_dict : bool, default=False\r\n"]
[613.776, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[613.786, "o", "\r\n"]
[613.796, "o", "        .. versionadded:: 0.20\r\n"]
[613.806, "o", "\r\n"]
[613.816, "o", "    positive_code : bool, default=False\r\n"]
[613.826, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[613.836, "o", "\r\n"]
[613.846, "o", "        .. versionadded:: 0.20\r\n"]
[613.856, "o", "\r\n"]
[613.866, "o", "    method_max_iter : int, default=1000\r\n"]
[613.876, "o", "        Maximum number of iterations to perform.\r\n"]
[613.886, "o", "\r\n"]
[613.896, "o", "        .. versionadded:: 0.22\r\n"]
[613.906, "o", "\r\n"]
[613.916, "o", "    Returns\r\n"]
[613.926, "o", "    -------\r\n"]
[613.936, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[613.946, "o", "        The sparse code factor in the matrix factorization.\r\n"]
[613.956, "o", "\r\n"]
[613.966, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[613.976, "o", "        The dictionary factor in the matrix factorization.\r\n"]
[613.986, "o", "\r\n"]
[613.996, "o", "    errors : array\r\n"]
[614.006, "o", "        Vector of errors at each iteration.\r\n"]
[614.016, "o", "\r\n"]
[614.026, "o", "    n_iter : int\r\n"]
[614.036, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[614.046, "o", "        set to True.\r\n"]
[614.056, "o", "\r\n"]
[614.066, "o", "    See Also\r\n"]
[614.076, "o", "    --------\r\n"]
[614.086, "o", "    dict_learning_online : Solve a dictionary learning matrix factorization\r\n"]
[614.096, "o", "        problem online.\r\n"]
[614.106, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[614.116, "o", "    MiniBatchDictionaryLearning : A faster, less accurate version\r\n"]
[614.126, "o", "        of the dictionary learning algorithm.\r\n"]
[614.136, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[614.146, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[614.156, "o", "    \"\"\"\r\n"]
[614.166, "o", "    estimator = DictionaryLearning(\r\n"]
[614.176, "o", "        n_components=n_components,\r\n"]
[614.186, "o", "        alpha=alpha,\r\n"]
[614.196, "o", "        max_iter=max_iter,\r\n"]
[614.206, "o", "        tol=tol,\r\n"]
[614.216, "o", "        fit_algorithm=method,\r\n"]
[614.226, "o", "        n_jobs=n_jobs,\r\n"]
[614.236, "o", "        dict_init=dict_init,\r\n"]
[614.246, "o", "        callback=callback,\r\n"]
[614.256, "o", "        code_init=code_init,\r\n"]
[614.266, "o", "        verbose=verbose,\r\n"]
[614.276, "o", "        random_state=random_state,\r\n"]
[614.286, "o", "        positive_code=positive_code,\r\n"]
[614.296, "o", "        positive_dict=positive_dict,\r\n"]
[614.306, "o", "        transform_max_iter=method_max_iter,\r\n"]
[614.316, "o", "    )\r\n"]
[614.326, "o", "    code = estimator.fit_transform(X)\r\n"]
[614.336, "o", "    if return_n_iter:\r\n"]
[614.346, "o", "        return (\r\n"]
[614.356, "o", "            code,\r\n"]
[614.366, "o", "            estimator.components_,\r\n"]
[614.376, "o", "            estimator.error_,\r\n"]
[614.386, "o", "            estimator.n_iter_,\r\n"]
[614.396, "o", "        )\r\n"]
[614.406, "o", "    return code, estimator.components_, estimator.error_\r\n"]
[614.416, "o", "\r\n"]
[614.426, "o", "\r\n"]
[614.436, "o", "class _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n"]
[614.446, "o", "    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n"]
[614.456, "o", "\r\n"]
[614.466, "o", "    def __init__(\r\n"]
[614.476, "o", "        self,\r\n"]
[614.486, "o", "        transform_algorithm,\r\n"]
[614.496, "o", "        transform_n_nonzero_coefs,\r\n"]
[614.506, "o", "        transform_alpha,\r\n"]
[614.516, "o", "        split_sign,\r\n"]
[614.526, "o", "        n_jobs,\r\n"]
[614.536, "o", "        positive_code,\r\n"]
[614.546, "o", "        transform_max_iter,\r\n"]
[614.556, "o", "    ):\r\n"]
[614.566, "o", "        self.transform_algorithm = transform_algorithm\r\n"]
[614.576, "o", "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n"]
[614.586, "o", "        self.transform_alpha = transform_alpha\r\n"]
[614.596, "o", "        self.transform_max_iter = transform_max_iter\r\n"]
[614.606, "o", "        self.split_sign = split_sign\r\n"]
[614.616, "o", "        self.n_jobs = n_jobs\r\n"]
[614.626, "o", "        self.positive_code = positive_code\r\n"]
[614.636, "o", "\r\n"]
[614.646, "o", "    def _transform(self, X, dictionary):\r\n"]
[614.656, "o", "        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n"]
[614.666, "o", "        SparseCoder.\"\"\"\r\n"]
[614.676, "o", "        X = self._validate_data(X, reset=False)\r\n"]
[614.686, "o", "\r\n"]
[614.696, "o", "        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n"]
[614.706, "o", "            transform_alpha = self.alpha\r\n"]
[614.716, "o", "        else:\r\n"]
[614.726, "o", "            transform_alpha = self.transform_alpha\r\n"]
[614.736, "o", "\r\n"]
[614.746, "o", "        code = sparse_encode(\r\n"]
[614.756, "o", "            X,\r\n"]
[614.766, "o", "            dictionary,\r\n"]
[614.776, "o", "            algorithm=self.transform_algorithm,\r\n"]
[614.786, "o", "            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n"]
[614.796, "o", "            alpha=transform_alpha,\r\n"]
[614.806, "o", "            max_iter=self.transform_max_iter,\r\n"]
[614.816, "o", "            n_jobs=self.n_jobs,\r\n"]
[614.826, "o", "            positive=self.positive_code,\r\n"]
[614.836, "o", "        )\r\n"]
[614.846, "o", "\r\n"]
[614.856, "o", "        if self.split_sign:\r\n"]
[614.866, "o", "            # feature vector is split into a positive and negative side\r\n"]
[614.876, "o", "            n_samples, n_features = code.shape\r\n"]
[614.886, "o", "            split_code = np.empty((n_samples, 2 * n_features))\r\n"]
[614.896, "o", "            split_code[:, :n_features] = np.maximum(code, 0)\r\n"]
[614.906, "o", "            split_code[:, n_features:] = -np.minimum(code, 0)\r\n"]
[614.916, "o", "            code = split_code\r\n"]
[614.926, "o", "\r\n"]
[614.936, "o", "        return code\r\n"]
[615.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[615.002, "i", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py\r"]
[615.004, "o", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py\r\n"]
[615.016, "o", "\u001b[?2004l\r\n"]
[615.026, "o", "\r\n"]
[615.036, "o", "    n_jobs : int or None, default=None\r\n"]
[615.046, "o", "        Number of parallel jobs to run.\r\n"]
[615.056, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[615.066, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[615.076, "o", "        for more details.\r\n"]
[615.086, "o", "\r\n"]
[615.096, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[615.106, "o", "        Initial value for the code, for warm restart. Only used if `code_init`\r\n"]
[615.116, "o", "        and `dict_init` are not None.\r\n"]
[615.126, "o", "\r\n"]
[615.136, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[615.146, "o", "        Initial values for the dictionary, for warm restart. Only used if\r\n"]
[615.156, "o", "        `code_init` and `dict_init` are not None.\r\n"]
[615.166, "o", "\r\n"]
[615.176, "o", "    callback : callable, default=None\r\n"]
[615.186, "o", "        Callable that gets invoked every five iterations.\r\n"]
[615.196, "o", "\r\n"]
[615.206, "o", "        .. versionadded:: 1.3\r\n"]
[615.216, "o", "\r\n"]
[615.226, "o", "    verbose : bool, default=False\r\n"]
[615.236, "o", "        To control the verbosity of the procedure.\r\n"]
[615.246, "o", "\r\n"]
[615.256, "o", "    split_sign : bool, default=False\r\n"]
[615.266, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[615.276, "o", "        its negative part and its positive part. This can improve the\r\n"]
[615.286, "o", "        performance of downstream classifiers.\r\n"]
[615.296, "o", "\r\n"]
[615.306, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[615.316, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[615.326, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[615.336, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[615.346, "o", "        results across multiple function calls.\r\n"]
[615.356, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[615.366, "o", "\r\n"]
[615.376, "o", "    positive_code : bool, default=False\r\n"]
[615.386, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[615.396, "o", "\r\n"]
[615.406, "o", "        .. versionadded:: 0.20\r\n"]
[615.416, "o", "\r\n"]
[615.426, "o", "    positive_dict : bool, default=False\r\n"]
[615.436, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[615.446, "o", "\r\n"]
[615.456, "o", "        .. versionadded:: 0.20\r\n"]
[615.466, "o", "\r\n"]
[615.476, "o", "    transform_max_iter : int, default=1000\r\n"]
[615.486, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[615.496, "o", "        `'lasso_lars'`.\r\n"]
[615.506, "o", "\r\n"]
[615.516, "o", "        .. versionadded:: 0.22\r\n"]
[615.526, "o", "\r\n"]
[615.536, "o", "    Attributes\r\n"]
[615.546, "o", "    ----------\r\n"]
[615.556, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[615.566, "o", "        dictionary atoms extracted from the data\r\n"]
[615.576, "o", "\r\n"]
[615.586, "o", "    error_ : array\r\n"]
[615.596, "o", "        vector of errors at each iteration\r\n"]
[615.606, "o", "\r\n"]
[615.616, "o", "    n_features_in_ : int\r\n"]
[615.626, "o", "        Number of features seen during :term:`fit`.\r\n"]
[615.636, "o", "\r\n"]
[615.646, "o", "        .. versionadded:: 0.24\r\n"]
[615.656, "o", "\r\n"]
[615.666, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[615.676, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[615.686, "o", "        has feature names that are all strings.\r\n"]
[615.696, "o", "\r\n"]
[615.706, "o", "        .. versionadded:: 1.0\r\n"]
[615.716, "o", "\r\n"]
[615.726, "o", "    n_iter_ : int\r\n"]
[615.736, "o", "        Number of iterations run.\r\n"]
[615.746, "o", "\r\n"]
[615.756, "o", "    See Also\r\n"]
[615.766, "o", "    --------\r\n"]
[615.776, "o", "    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n"]
[615.786, "o", "        dictionary learning algorithm.\r\n"]
[615.796, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[615.806, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[615.816, "o", "        precomputed dictionary.\r\n"]
[615.826, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[615.836, "o", "\r\n"]
[615.846, "o", "    References\r\n"]
[615.856, "o", "    ----------\r\n"]
[615.866, "o", "\r\n"]
[615.876, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[615.886, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[615.896, "o", "\r\n"]
[615.906, "o", "    Examples\r\n"]
[615.916, "o", "    --------\r\n"]
[615.926, "o", "    >>> import numpy as np\r\n"]
[615.936, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[615.946, "o", "    >>> from sklearn.decomposition import DictionaryLearning\r\n"]
[615.956, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[615.966, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[615.976, "o", "    ...     random_state=42,\r\n"]
[615.986, "o", "    ... )\r\n"]
[615.996, "o", "    >>> dict_learner = DictionaryLearning(\r\n"]
[616.006, "o", "    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n"]
[616.016, "o", "    ...     random_state=42,\r\n"]
[616.026, "o", "    ... )\r\n"]
[616.036, "o", "    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n"]
[616.046, "o", "\r\n"]
[616.056, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[616.066, "o", "\r\n"]
[616.076, "o", "    >>> np.mean(X_transformed == 0)\r\n"]
[616.086, "o", "    0.41...\r\n"]
[616.096, "o", "\r\n"]
[616.106, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[616.116, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[616.126, "o", "    the original signal:\r\n"]
[616.136, "o", "\r\n"]
[616.146, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[616.156, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[616.166, "o", "    0.07...\r\n"]
[616.176, "o", "    \"\"\"\r\n"]
[616.186, "o", "\r\n"]
[616.196, "o", "    _parameter_constraints: dict = {\r\n"]
[616.206, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[616.216, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[616.226, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[616.236, "o", "        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[616.246, "o", "        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[616.256, "o", "        \"transform_algorithm\": [\r\n"]
[616.266, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[616.276, "o", "        ],\r\n"]
[616.286, "o", "        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[616.296, "o", "        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[616.306, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[616.316, "o", "        \"code_init\": [np.ndarray, None],\r\n"]
[616.326, "o", "        \"dict_init\": [np.ndarray, None],\r\n"]
[616.336, "o", "        \"callback\": [callable, None],\r\n"]
[616.346, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[616.356, "o", "        \"split_sign\": [\"boolean\"],\r\n"]
[616.366, "o", "        \"random_state\": [\"random_state\"],\r\n"]
[616.376, "o", "        \"positive_code\": [\"boolean\"],\r\n"]
[616.386, "o", "        \"positive_dict\": [\"boolean\"],\r\n"]
[616.396, "o", "        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[616.406, "o", "    }\r\n"]
[616.416, "o", "\r\n"]
[616.426, "o", "    def __init__(\r\n"]
[616.436, "o", "        self,\r\n"]
[616.446, "o", "        n_components=None,\r\n"]
[616.456, "o", "        *,\r\n"]
[616.466, "o", "        alpha=1,\r\n"]
[616.476, "o", "        max_iter=1000,\r\n"]
[616.486, "o", "        tol=1e-8,\r\n"]
[616.496, "o", "        fit_algorithm=\"lars\",\r\n"]
[616.506, "o", "        transform_algorithm=\"omp\",\r\n"]
[616.516, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[616.526, "o", "        transform_alpha=None,\r\n"]
[616.536, "o", "        n_jobs=None,\r\n"]
[616.546, "o", "        code_init=None,\r\n"]
[616.556, "o", "        dict_init=None,\r\n"]
[616.566, "o", "        callback=None,\r\n"]
[616.576, "o", "        verbose=False,\r\n"]
[616.586, "o", "        split_sign=False,\r\n"]
[616.596, "o", "        random_state=None,\r\n"]
[616.606, "o", "        positive_code=False,\r\n"]
[616.616, "o", "        positive_dict=False,\r\n"]
[616.626, "o", "        transform_max_iter=1000,\r\n"]
[616.636, "o", "    ):\r\n"]
[616.646, "o", "        super().__init__(\r\n"]
[616.656, "o", "            transform_algorithm,\r\n"]
[616.666, "o", "            transform_n_nonzero_coefs,\r\n"]
[616.676, "o", "            transform_alpha,\r\n"]
[616.686, "o", "            split_sign,\r\n"]
[616.696, "o", "            n_jobs,\r\n"]
[616.706, "o", "            positive_code,\r\n"]
[616.716, "o", "            transform_max_iter,\r\n"]
[616.726, "o", "        )\r\n"]
[616.736, "o", "        self.n_components = n_components\r\n"]
[616.746, "o", "        self.alpha = alpha\r\n"]
[616.756, "o", "        self.max_iter = max_iter\r\n"]
[616.766, "o", "        self.tol = tol\r\n"]
[616.776, "o", "        self.fit_algorithm = fit_algorithm\r\n"]
[616.786, "o", "        self.code_init = code_init\r\n"]
[616.796, "o", "        self.dict_init = dict_init\r\n"]
[616.806, "o", "        self.callback = callback\r\n"]
[616.816, "o", "        self.verbose = verbose\r\n"]
[616.826, "o", "        self.random_state = random_state\r\n"]
[616.836, "o", "        self.positive_dict = positive_dict\r\n"]
[616.846, "o", "\r\n"]
[616.856, "o", "    def fit(self, X, y=None):\r\n"]
[616.866, "o", "        \"\"\"Fit the model from data in X.\r\n"]
[616.876, "o", "\r\n"]
[616.886, "o", "        Parameters\r\n"]
[616.896, "o", "        ----------\r\n"]
[616.906, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[616.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[616.926, "o", "            and `n_features` is the number of features.\r\n"]
[616.936, "o", "\r\n"]
[616.946, "o", "        y : Ignored\r\n"]
[616.956, "o", "            Not used, present for API consistency by convention.\r\n"]
[616.966, "o", "\r\n"]
[616.976, "o", "        Returns\r\n"]
[616.986, "o", "        -------\r\n"]
[616.996, "o", "        self : object\r\n"]
[617.006, "o", "            Returns the instance itself.\r\n"]
[617.016, "o", "        \"\"\"\r\n"]
[617.026, "o", "        self.fit_transform(X)\r\n"]
[617.036, "o", "        return self\r\n"]
[617.046, "o", "\r\n"]
[617.056, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[617.066, "o", "    def fit_transform(self, X, y=None):\r\n"]
[617.076, "o", "        \"\"\"Fit the model from data in X and return the transformed data.\r\n"]
[617.086, "o", "\r\n"]
[617.096, "o", "        Parameters\r\n"]
[617.106, "o", "        ----------\r\n"]
[617.116, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[617.126, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[617.136, "o", "            and `n_features` is the number of features.\r\n"]
[617.146, "o", "\r\n"]
[617.156, "o", "        y : Ignored\r\n"]
[617.166, "o", "            Not used, present for API consistency by convention.\r\n"]
[617.176, "o", "\r\n"]
[617.186, "o", "        Returns\r\n"]
[617.196, "o", "        -------\r\n"]
[617.206, "o", "        V : ndarray of shape (n_samples, n_components)\r\n"]
[617.216, "o", "            Transformed data.\r\n"]
[617.226, "o", "        \"\"\"\r\n"]
[617.236, "o", "        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n"]
[617.246, "o", "\r\n"]
[617.256, "o", "        method = \"lasso_\" + self.fit_algorithm\r\n"]
[617.266, "o", "\r\n"]
[617.276, "o", "        random_state = check_random_state(self.random_state)\r\n"]
[617.286, "o", "        X = self._validate_data(X)\r\n"]
[617.296, "o", "\r\n"]
[617.306, "o", "        if self.n_components is None:\r\n"]
[617.316, "o", "            n_components = X.shape[1]\r\n"]
[617.326, "o", "        else:\r\n"]
[617.336, "o", "            n_components = self.n_components\r\n"]
[617.346, "o", "\r\n"]
[617.356, "o", "        V, U, E, self.n_iter_ = _dict_learning(\r\n"]
[617.366, "o", "            X,\r\n"]
[617.376, "o", "            n_components,\r\n"]
[617.386, "o", "            alpha=self.alpha,\r\n"]
[617.396, "o", "            tol=self.tol,\r\n"]
[617.406, "o", "            max_iter=self.max_iter,\r\n"]
[617.416, "o", "            method=method,\r\n"]
[617.426, "o", "            method_max_iter=self.transform_max_iter,\r\n"]
[617.436, "o", "            n_jobs=self.n_jobs,\r\n"]
[617.446, "o", "            code_init=self.code_init,\r\n"]
[617.456, "o", "            dict_init=self.dict_init,\r\n"]
[617.466, "o", "            callback=self.callback,\r\n"]
[617.476, "o", "            verbose=self.verbose,\r\n"]
[617.486, "o", "            random_state=random_state,\r\n"]
[617.496, "o", "            return_n_iter=True,\r\n"]
[617.506, "o", "            positive_dict=self.positive_dict,\r\n"]
[617.516, "o", "            positive_code=self.positive_code,\r\n"]
[617.526, "o", "        )\r\n"]
[617.536, "o", "        self.components_ = U\r\n"]
[617.546, "o", "        self.error_ = E\r\n"]
[617.556, "o", "\r\n"]
[617.566, "o", "        return V\r\n"]
[617.576, "o", "\r\n"]
[617.586, "o", "    @property\r\n"]
[617.596, "o", "    def _n_features_out(self):\r\n"]
[617.606, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[617.616, "o", "        return self.components_.shape[0]\r\n"]
[617.626, "o", "\r\n"]
[617.636, "o", "    def _more_tags(self):\r\n"]
[617.646, "o", "        return {\r\n"]
[617.656, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[617.666, "o", "        }\r\n"]
[617.676, "o", "\r\n"]
[617.686, "o", "\r\n"]
[617.696, "o", "class MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[617.706, "o", "    \"\"\"Mini-batch dictionary learning.\r\n"]
[617.716, "o", "\r\n"]
[617.726, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[617.736, "o", "    encoding the fitted data.\r\n"]
[617.746, "o", "\r\n"]
[617.756, "o", "    Solves the optimization problem::\r\n"]
[617.766, "o", "\r\n"]
[617.776, "o", "       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[617.786, "o", "                    (U,V)\r\n"]
[617.796, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[617.806, "o", "\r\n"]
[617.816, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[617.826, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[617.836, "o", "    of all the entries in the matrix.\r\n"]
[617.846, "o", "\r\n"]
[617.856, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[617.866, "o", "\r\n"]
[617.876, "o", "    Parameters\r\n"]
[617.886, "o", "    ----------\r\n"]
[617.896, "o", "    n_components : int, default=None\r\n"]
[617.906, "o", "        Number of dictionary elements to extract.\r\n"]
[617.916, "o", "\r\n"]
[617.926, "o", "    alpha : float, default=1\r\n"]
[617.936, "o", "        Sparsity controlling parameter.\r\n"]
[617.946, "o", "\r\n"]
[617.956, "o", "    n_iter : int, default=1000\r\n"]
[617.966, "o", "        Total number of iterations over data batches to perform.\r\n"]
[617.976, "o", "\r\n"]
[617.986, "o", "        .. deprecated:: 1.1\r\n"]
[617.996, "o", "           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[618.006, "o", "           ``max_iter`` instead.\r\n"]
[618.016, "o", "\r\n"]
[618.026, "o", "    max_iter : int, default=None\r\n"]
[618.036, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[618.046, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[618.056, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[618.066, "o", "\r\n"]
[618.076, "o", "        .. versionadded:: 1.1\r\n"]
[618.086, "o", "\r\n"]
[618.096, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[618.106, "o", "        The algorithm used:\r\n"]
[618.116, "o", "\r\n"]
[618.126, "o", "        - `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[618.136, "o", "          problem (`linear_model.lars_path`)\r\n"]
[618.146, "o", "        - `'cd'`: uses the coordinate descent method to compute the\r\n"]
[618.156, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[618.166, "o", "          the estimated components are sparse.\r\n"]
[618.176, "o", "\r\n"]
[618.186, "o", "    n_jobs : int, default=None\r\n"]
[618.196, "o", "        Number of parallel jobs to run.\r\n"]
[618.206, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[618.216, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[618.226, "o", "        for more details.\r\n"]
[618.236, "o", "\r\n"]
[618.246, "o", "    batch_size : int, default=256\r\n"]
[618.256, "o", "        Number of samples in each mini-batch.\r\n"]
[618.266, "o", "\r\n"]
[618.276, "o", "        .. versionchanged:: 1.3\r\n"]
[618.286, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[618.296, "o", "\r\n"]
[618.306, "o", "    shuffle : bool, default=True\r\n"]
[618.316, "o", "        Whether to shuffle the samples before forming batches.\r\n"]
[618.326, "o", "\r\n"]
[618.336, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[618.346, "o", "        Initial value of the dictionary for warm restart scenarios.\r\n"]
[618.356, "o", "\r\n"]
[618.366, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[618.376, "o", "            'threshold'}, default='omp'\r\n"]
[618.386, "o", "        Algorithm used to transform the data:\r\n"]
[618.396, "o", "\r\n"]
[618.406, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[618.416, "o", "          (`linear_model.lars_path`);\r\n"]
[618.426, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[618.436, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[618.446, "o", "          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n"]
[618.456, "o", "          if the estimated components are sparse.\r\n"]
[618.466, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[618.476, "o", "          solution.\r\n"]
[618.486, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[618.496, "o", "          the projection ``dictionary * X'``.\r\n"]
[618.506, "o", "\r\n"]
[618.516, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[618.526, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[618.536, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[618.546, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[618.556, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[618.566, "o", "\r\n"]
[618.576, "o", "    transform_alpha : float, default=None\r\n"]
[618.586, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[618.596, "o", "        penalty applied to the L1 norm.\r\n"]
[618.606, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[618.616, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[618.626, "o", "        If `None`, defaults to `alpha`.\r\n"]
[618.636, "o", "\r\n"]
[618.646, "o", "        .. versionchanged:: 1.2\r\n"]
[618.656, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[618.666, "o", "\r\n"]
[618.676, "o", "    verbose : bool or int, default=False\r\n"]
[618.686, "o", "        To control the verbosity of the procedure.\r\n"]
[618.696, "o", "\r\n"]
[618.706, "o", "    split_sign : bool, default=False\r\n"]
[618.716, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[618.726, "o", "        its negative part and its positive part. This can improve the\r\n"]
[618.736, "o", "        performance of downstream classifiers.\r\n"]
[618.746, "o", "\r\n"]
[618.756, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[618.766, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[618.776, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[618.786, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[618.796, "o", "        results across multiple function calls.\r\n"]
[618.806, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[618.816, "o", "\r\n"]
[618.826, "o", "    positive_code : bool, default=False\r\n"]
[618.836, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[618.846, "o", "\r\n"]
[618.856, "o", "        .. versionadded:: 0.20\r\n"]
[618.866, "o", "\r\n"]
[618.876, "o", "    positive_dict : bool, default=False\r\n"]
[618.886, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[618.896, "o", "\r\n"]
[618.906, "o", "        .. versionadded:: 0.20\r\n"]
[618.916, "o", "\r\n"]
[618.926, "o", "    transform_max_iter : int, default=1000\r\n"]
[618.936, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[618.946, "o", "        `'lasso_lars'`.\r\n"]
[618.956, "o", "\r\n"]
[618.966, "o", "        .. versionadded:: 0.22\r\n"]
[618.976, "o", "\r\n"]
[618.986, "o", "    callback : callable, default=None\r\n"]
[618.996, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[619.006, "o", "\r\n"]
[619.016, "o", "        .. versionadded:: 1.1\r\n"]
[619.026, "o", "\r\n"]
[619.036, "o", "    tol : float, default=1e-3\r\n"]
[619.046, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[619.056, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[619.066, "o", "\r\n"]
[619.076, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[619.086, "o", "        `tol` to 0.0.\r\n"]
[619.096, "o", "\r\n"]
[619.106, "o", "        .. versionadded:: 1.1\r\n"]
[619.116, "o", "\r\n"]
[619.126, "o", "    max_no_improvement : int, default=10\r\n"]
[619.136, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[619.146, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[619.156, "o", "        `max_iter` is not None.\r\n"]
[619.166, "o", "\r\n"]
[619.176, "o", "        To disable convergence detection based on cost function, set\r\n"]
[619.186, "o", "        `max_no_improvement` to None.\r\n"]
[619.196, "o", "\r\n"]
[619.206, "o", "        .. versionadded:: 1.1\r\n"]
[619.216, "o", "\r\n"]
[619.226, "o", "    Attributes\r\n"]
[619.236, "o", "    ----------\r\n"]
[619.246, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[619.256, "o", "        Components extracted from the data.\r\n"]
[619.266, "o", "\r\n"]
[619.276, "o", "    n_features_in_ : int\r\n"]
[619.286, "o", "        Number of features seen during :term:`fit`.\r\n"]
[619.296, "o", "\r\n"]
[619.306, "o", "        .. versionadded:: 0.24\r\n"]
[619.316, "o", "\r\n"]
[619.326, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[619.336, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[619.346, "o", "        has feature names that are all strings.\r\n"]
[619.356, "o", "\r\n"]
[619.366, "o", "        .. versionadded:: 1.0\r\n"]
[619.376, "o", "\r\n"]
[619.386, "o", "    n_iter_ : int\r\n"]
[619.396, "o", "        Number of iterations over the full dataset.\r\n"]
[619.406, "o", "\r\n"]
[619.416, "o", "    n_steps_ : int\r\n"]
[619.426, "o", "        Number of mini-batches processed.\r\n"]
[619.436, "o", "\r\n"]
[619.446, "o", "        .. versionadded:: 1.1\r\n"]
[619.456, "o", "\r\n"]
[619.466, "o", "    See Also\r\n"]
[619.476, "o", "    --------\r\n"]
[619.486, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[619.496, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[619.506, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[619.516, "o", "        precomputed dictionary.\r\n"]
[619.526, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[619.536, "o", "\r\n"]
[619.546, "o", "    References\r\n"]
[619.556, "o", "    ----------\r\n"]
[619.566, "o", "\r\n"]
[619.576, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[619.586, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[619.596, "o", "\r\n"]
[619.606, "o", "    Examples\r\n"]
[619.616, "o", "    --------\r\n"]
[619.626, "o", "    >>> import numpy as np\r\n"]
[619.636, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[619.646, "o", "    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n"]
[619.656, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[619.666, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[619.676, "o", "    ...     random_state=42)\r\n"]
[619.686, "o", "    >>> dict_learner = MiniBatchDictionaryLearning(\r\n"]
[619.696, "o", "    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n"]
[619.706, "o", "    ...     transform_alpha=0.1, random_state=42)\r\n"]
[619.716, "o", "    >>> X_transformed = dict_learner.fit_transform(X)\r\n"]
[619.726, "o", "\r\n"]
[619.736, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[619.746, "o", "\r\n"]
[619.756, "o", "    >>> np.mean(X_transformed == 0) < 0.5\r\n"]
[619.766, "o", "    True\r\n"]
[619.776, "o", "\r\n"]
[619.786, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[619.796, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[619.806, "o", "    the original signal:\r\n"]
[619.816, "o", "\r\n"]
[619.826, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[619.836, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[619.846, "o", "    0.057...\r\n"]
[619.856, "o", "    \"\"\"\r\n"]
[619.866, "o", "\r\n"]
[619.876, "o", "    _parameter_constraints: dict = {\r\n"]
[619.886, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[619.896, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[619.906, "o", "        \"n_iter\": [\r\n"]
[619.916, "o", "            Interval(Integral, 0, None, closed=\"left\"),\r\n"]
[619.926, "o", "            Hidden(StrOptions({\"deprecated\"})),\r\n"]
[619.936, "o", "        ],\r\n"]
[620.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[620.002, "i", "cd asv_benchmarks\r"]
[620.004, "o", "cd asv_benchmarks\r\n"]
[620.006, "o", "\u001b[?2004l\r\n"]
[625.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[625.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[625.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[626.654, "o", "\u001b[?2004l\r\n"]
[628.302, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[630.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[630.002, "i", "cd ..\r"]
[630.004, "o", "cd ..\r\n"]
[630.006, "o", "\u001b[?2004l\r\n"]
[635.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[635.002, "i", "grep -n \"def _minibatch_step\\|_sparse_encode_precomputed\" sklearn/decomposition/_dict_learning.py\r"]
[635.004, "o", "grep -n \"def _minibatch_step\\|_sparse_encode_precomputed\" sklearn/decomposition/_dict_learning.py\r\n"]
[635.9948, "o", "\u001b[?2004l\r\n"]
[636.9836, "o", "\u001b[32m\u001b[K38\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdef \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n"]
[637.9724, "o", "\u001b[32m\u001b[K416\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        code = \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n"]
[638.9612, "o", "\u001b[32m\u001b[K438\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        delayed(\u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K)(\r\n"]
[640.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[640.002, "i", "grep -n \"cov\\s*=\\s\" sklearn/decomposition/_dict_learning.py\r"]
[640.004, "o", "grep -n \"cov\\s*=\\s\" sklearn/decomposition/_dict_learning.py\r\n"]
[641.654, "o", "\u001b[?2004l\r\n"]
[643.302, "o", "\u001b[32m\u001b[K412\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        copy_\u001b[01;31m\u001b[Kcov = \u001b[m\u001b[KFalse\r\n"]
[645.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[645.002, "i", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r"]
[645.004, "o", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r\n"]
[645.016, "o", "\u001b[?2004l\r\n"]
[645.026, "o", "\"\"\" Dictionary learning.\r\n"]
[645.036, "o", "\"\"\"\r\n"]
[645.046, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[645.056, "o", "# License: BSD 3 clause\r\n"]
[645.066, "o", "\r\n"]
[645.076, "o", "import itertools\r\n"]
[645.086, "o", "import sys\r\n"]
[645.096, "o", "import time\r\n"]
[645.106, "o", "import warnings\r\n"]
[645.116, "o", "from math import ceil\r\n"]
[645.126, "o", "from numbers import Integral, Real\r\n"]
[645.136, "o", "\r\n"]
[645.146, "o", "import numpy as np\r\n"]
[645.156, "o", "from joblib import effective_n_jobs\r\n"]
[645.166, "o", "from scipy import linalg\r\n"]
[645.176, "o", "\r\n"]
[645.186, "o", "from ..base import (\r\n"]
[645.196, "o", "    BaseEstimator,\r\n"]
[645.206, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[645.216, "o", "    TransformerMixin,\r\n"]
[645.226, "o", "    _fit_context,\r\n"]
[645.236, "o", ")\r\n"]
[645.246, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[645.256, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[645.266, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[645.276, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[645.286, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[645.296, "o", "from ..utils.validation import check_is_fitted\r\n"]
[645.306, "o", "\r\n"]
[645.316, "o", "\r\n"]
[645.326, "o", "def _check_positive_coding(method, positive):\r\n"]
[645.336, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[645.346, "o", "        raise ValueError(\r\n"]
[645.356, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[645.366, "o", "        )\r\n"]
[645.376, "o", "\r\n"]
[645.386, "o", "\r\n"]
[645.396, "o", "def _sparse_encode_precomputed(\r\n"]
[645.406, "o", "    X,\r\n"]
[645.416, "o", "    dictionary,\r\n"]
[645.426, "o", "    *,\r\n"]
[645.436, "o", "    gram=None,\r\n"]
[645.446, "o", "    cov=None,\r\n"]
[645.456, "o", "    algorithm=\"lasso_lars\",\r\n"]
[645.466, "o", "    regularization=None,\r\n"]
[645.476, "o", "    copy_cov=True,\r\n"]
[645.486, "o", "    init=None,\r\n"]
[645.496, "o", "    max_iter=1000,\r\n"]
[645.506, "o", "    verbose=0,\r\n"]
[645.516, "o", "    positive=False,\r\n"]
[645.526, "o", "):\r\n"]
[645.536, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[645.546, "o", "\r\n"]
[645.556, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[645.566, "o", "\r\n"]
[645.576, "o", "    Parameters\r\n"]
[645.586, "o", "    ----------\r\n"]
[645.596, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[645.606, "o", "        Data matrix.\r\n"]
[645.616, "o", "\r\n"]
[645.626, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[645.636, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[645.646, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[645.656, "o", "\r\n"]
[645.666, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[645.676, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[645.686, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[645.696, "o", "\r\n"]
[645.706, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[645.716, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[645.726, "o", "\r\n"]
[645.736, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[645.746, "o", "            default='lasso_lars'\r\n"]
[645.756, "o", "        The algorithm used:\r\n"]
[645.766, "o", "\r\n"]
[645.776, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[645.786, "o", "          (`linear_model.lars_path`);\r\n"]
[645.796, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[645.806, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[645.816, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[645.826, "o", "          the estimated components are sparse;\r\n"]
[645.836, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[645.846, "o", "          solution;\r\n"]
[645.856, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[645.866, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[645.876, "o", "\r\n"]
[645.886, "o", "    regularization : int or float, default=None\r\n"]
[645.896, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[645.906, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[645.916, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[645.926, "o", "\r\n"]
[645.936, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[645.946, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[645.956, "o", "        `algorithm='lasso_cd'`.\r\n"]
[645.966, "o", "\r\n"]
[645.976, "o", "    max_iter : int, default=1000\r\n"]
[645.986, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[645.996, "o", "        `'lasso_lars'`.\r\n"]
[646.006, "o", "\r\n"]
[646.016, "o", "    copy_cov : bool, default=True\r\n"]
[646.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[646.036, "o", "        be overwritten.\r\n"]
[646.046, "o", "\r\n"]
[646.056, "o", "    verbose : int, default=0\r\n"]
[646.066, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[646.076, "o", "\r\n"]
[646.086, "o", "    positive: bool, default=False\r\n"]
[646.096, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[646.106, "o", "\r\n"]
[646.116, "o", "        .. versionadded:: 0.20\r\n"]
[646.126, "o", "\r\n"]
[646.136, "o", "    Returns\r\n"]
[646.146, "o", "    -------\r\n"]
[646.156, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[646.166, "o", "        The sparse codes.\r\n"]
[646.176, "o", "    \"\"\"\r\n"]
[646.186, "o", "    n_samples, n_features = X.shape\r\n"]
[646.196, "o", "    n_components = dictionary.shape[0]\r\n"]
[646.206, "o", "\r\n"]
[646.216, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[646.226, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[646.236, "o", "        try:\r\n"]
[646.246, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[646.256, "o", "\r\n"]
[646.266, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[646.276, "o", "            # corrects the verbosity level.\r\n"]
[646.286, "o", "            lasso_lars = LassoLars(\r\n"]
[646.296, "o", "                alpha=alpha,\r\n"]
[646.306, "o", "                fit_intercept=False,\r\n"]
[646.316, "o", "                verbose=verbose,\r\n"]
[646.326, "o", "                precompute=gram,\r\n"]
[646.336, "o", "                fit_path=False,\r\n"]
[646.346, "o", "                positive=positive,\r\n"]
[646.356, "o", "                max_iter=max_iter,\r\n"]
[646.366, "o", "            )\r\n"]
[646.376, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[646.386, "o", "            new_code = lasso_lars.coef_\r\n"]
[646.396, "o", "        finally:\r\n"]
[646.406, "o", "            np.seterr(**err_mgt)\r\n"]
[646.416, "o", "\r\n"]
[646.426, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[646.436, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[646.446, "o", "\r\n"]
[646.456, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[646.466, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[646.476, "o", "        # argument that we could pass in from Lasso.\r\n"]
[646.486, "o", "        clf = Lasso(\r\n"]
[646.496, "o", "            alpha=alpha,\r\n"]
[646.506, "o", "            fit_intercept=False,\r\n"]
[646.516, "o", "            precompute=gram,\r\n"]
[646.526, "o", "            max_iter=max_iter,\r\n"]
[646.536, "o", "            warm_start=True,\r\n"]
[646.546, "o", "            positive=positive,\r\n"]
[646.556, "o", "        )\r\n"]
[646.566, "o", "\r\n"]
[646.576, "o", "        if init is not None:\r\n"]
[646.586, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[646.596, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[646.606, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[646.616, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[646.626, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[646.636, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[646.646, "o", "                init = np.array(init)\r\n"]
[646.656, "o", "            clf.coef_ = init\r\n"]
[646.666, "o", "\r\n"]
[646.676, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[646.686, "o", "        new_code = clf.coef_\r\n"]
[646.696, "o", "\r\n"]
[646.706, "o", "    elif algorithm == \"lars\":\r\n"]
[646.716, "o", "        try:\r\n"]
[646.726, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[646.736, "o", "\r\n"]
[646.746, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[646.756, "o", "            # corrects the verbosity level.\r\n"]
[646.766, "o", "            lars = Lars(\r\n"]
[646.776, "o", "                fit_intercept=False,\r\n"]
[646.786, "o", "                verbose=verbose,\r\n"]
[646.796, "o", "                precompute=gram,\r\n"]
[646.806, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[646.816, "o", "                fit_path=False,\r\n"]
[646.826, "o", "            )\r\n"]
[646.836, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[646.846, "o", "            new_code = lars.coef_\r\n"]
[646.856, "o", "        finally:\r\n"]
[646.866, "o", "            np.seterr(**err_mgt)\r\n"]
[646.876, "o", "\r\n"]
[646.886, "o", "    elif algorithm == \"threshold\":\r\n"]
[646.896, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[646.906, "o", "        if positive:\r\n"]
[646.916, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[646.926, "o", "\r\n"]
[646.936, "o", "    elif algorithm == \"omp\":\r\n"]
[646.946, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[646.956, "o", "            Gram=gram,\r\n"]
[646.966, "o", "            Xy=cov,\r\n"]
[646.976, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[646.986, "o", "            tol=None,\r\n"]
[646.996, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[647.006, "o", "            copy_Xy=copy_cov,\r\n"]
[647.016, "o", "        ).T\r\n"]
[647.026, "o", "\r\n"]
[647.036, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[647.046, "o", "\r\n"]
[647.056, "o", "\r\n"]
[647.066, "o", "@validate_params(\r\n"]
[647.076, "o", "    {\r\n"]
[647.086, "o", "        \"X\": [\"array-like\"],\r\n"]
[647.096, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[647.106, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[647.116, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[647.126, "o", "        \"algorithm\": [\r\n"]
[647.136, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[647.146, "o", "        ],\r\n"]
[647.156, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[647.166, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[647.176, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[647.186, "o", "        \"init\": [\"array-like\", None],\r\n"]
[647.196, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[647.206, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[647.216, "o", "        \"check_input\": [\"boolean\"],\r\n"]
[647.226, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[647.236, "o", "        \"positive\": [\"boolean\"],\r\n"]
[647.246, "o", "    },\r\n"]
[647.256, "o", "    prefer_skip_nested_validation=True,\r\n"]
[647.266, "o", ")\r\n"]
[647.276, "o", "# XXX : could be moved to the linear_model module\r\n"]
[647.286, "o", "def sparse_encode(\r\n"]
[647.296, "o", "    X,\r\n"]
[647.306, "o", "    dictionary,\r\n"]
[647.316, "o", "    *,\r\n"]
[647.326, "o", "    gram=None,\r\n"]
[647.336, "o", "    cov=None,\r\n"]
[647.346, "o", "    algorithm=\"lasso_lars\",\r\n"]
[647.356, "o", "    n_nonzero_coefs=None,\r\n"]
[647.366, "o", "    alpha=None,\r\n"]
[647.376, "o", "    copy_cov=True,\r\n"]
[647.386, "o", "    init=None,\r\n"]
[647.396, "o", "    max_iter=1000,\r\n"]
[647.406, "o", "    n_jobs=None,\r\n"]
[647.416, "o", "    check_input=True,\r\n"]
[647.426, "o", "    verbose=0,\r\n"]
[647.436, "o", "    positive=False,\r\n"]
[647.446, "o", "):\r\n"]
[647.456, "o", "    \"\"\"Sparse coding.\r\n"]
[647.466, "o", "\r\n"]
[647.476, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[647.486, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[647.496, "o", "\r\n"]
[647.506, "o", "        X ~= code * dictionary\r\n"]
[647.516, "o", "\r\n"]
[647.526, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[647.536, "o", "\r\n"]
[647.546, "o", "    Parameters\r\n"]
[647.556, "o", "    ----------\r\n"]
[647.566, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[647.576, "o", "        Data matrix.\r\n"]
[647.586, "o", "\r\n"]
[647.596, "o", "    dictionary : array-like of shape (n_components, n_features)\r\n"]
[647.606, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[647.616, "o", "        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[647.626, "o", "        output.\r\n"]
[647.636, "o", "\r\n"]
[647.646, "o", "    gram : array-like of shape (n_components, n_components), default=None\r\n"]
[647.656, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n"]
[647.666, "o", "\r\n"]
[647.676, "o", "    cov : array-like of shape (n_components, n_samples), default=None\r\n"]
[647.686, "o", "        Precomputed covariance, `dictionary' * X`.\r\n"]
[647.696, "o", "\r\n"]
[647.706, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[647.716, "o", "            default='lasso_lars'\r\n"]
[647.726, "o", "        The algorithm used:\r\n"]
[647.736, "o", "\r\n"]
[647.746, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[647.756, "o", "          (`linear_model.lars_path`);\r\n"]
[647.766, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[647.776, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[647.786, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[647.796, "o", "          the estimated components are sparse;\r\n"]
[647.806, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[647.816, "o", "          solution;\r\n"]
[647.826, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[647.836, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[647.846, "o", "\r\n"]
[647.856, "o", "    n_nonzero_coefs : int, default=None\r\n"]
[647.866, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[647.876, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[647.886, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[647.896, "o", "        `n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[647.906, "o", "\r\n"]
[647.916, "o", "    alpha : float, default=None\r\n"]
[647.926, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[647.936, "o", "        penalty applied to the L1 norm.\r\n"]
[647.946, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[647.956, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[647.966, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[647.976, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[647.986, "o", "        `n_nonzero_coefs`.\r\n"]
[647.996, "o", "        If `None`, default to 1.\r\n"]
[648.006, "o", "\r\n"]
[648.016, "o", "    copy_cov : bool, default=True\r\n"]
[648.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[648.036, "o", "        be overwritten.\r\n"]
[648.046, "o", "\r\n"]
[648.056, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[648.066, "o", "        Initialization value of the sparse codes. Only used if\r\n"]
[648.076, "o", "        `algorithm='lasso_cd'`.\r\n"]
[648.086, "o", "\r\n"]
[648.096, "o", "    max_iter : int, default=1000\r\n"]
[648.106, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[648.116, "o", "        `'lasso_lars'`.\r\n"]
[648.126, "o", "\r\n"]
[648.136, "o", "    n_jobs : int, default=None\r\n"]
[648.146, "o", "        Number of parallel jobs to run.\r\n"]
[648.156, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[648.166, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[648.176, "o", "        for more details.\r\n"]
[648.186, "o", "\r\n"]
[648.196, "o", "    check_input : bool, default=True\r\n"]
[648.206, "o", "        If `False`, the input arrays X and dictionary will not be checked.\r\n"]
[648.216, "o", "\r\n"]
[648.226, "o", "    verbose : int, default=0\r\n"]
[648.236, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[648.246, "o", "\r\n"]
[648.256, "o", "    positive : bool, default=False\r\n"]
[648.266, "o", "        Whether to enforce positivity when finding the encoding.\r\n"]
[648.276, "o", "\r\n"]
[648.286, "o", "        .. versionadded:: 0.20\r\n"]
[648.296, "o", "\r\n"]
[648.306, "o", "    Returns\r\n"]
[648.316, "o", "    -------\r\n"]
[648.326, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[648.336, "o", "        The sparse codes.\r\n"]
[648.346, "o", "\r\n"]
[648.356, "o", "    See Also\r\n"]
[648.366, "o", "    --------\r\n"]
[648.376, "o", "    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n"]
[648.386, "o", "        path using LARS algorithm.\r\n"]
[648.396, "o", "    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n"]
[648.406, "o", "    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n"]
[648.416, "o", "    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n"]
[648.426, "o", "        dictionary.\r\n"]
[648.436, "o", "    \"\"\"\r\n"]
[648.446, "o", "    if check_input:\r\n"]
[648.456, "o", "        if algorithm == \"lasso_cd\":\r\n"]
[648.466, "o", "            dictionary = check_array(\r\n"]
[648.476, "o", "                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n"]
[648.486, "o", "            )\r\n"]
[648.496, "o", "            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n"]
[648.506, "o", "        else:\r\n"]
[648.516, "o", "            dictionary = check_array(dictionary)\r\n"]
[648.526, "o", "            X = check_array(X)\r\n"]
[648.536, "o", "\r\n"]
[648.546, "o", "    if dictionary.shape[1] != X.shape[1]:\r\n"]
[648.556, "o", "        raise ValueError(\r\n"]
[648.566, "o", "            \"Dictionary and X have different numbers of features:\"\r\n"]
[648.576, "o", "            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n"]
[648.586, "o", "        )\r\n"]
[648.596, "o", "\r\n"]
[648.606, "o", "    _check_positive_coding(algorithm, positive)\r\n"]
[648.616, "o", "\r\n"]
[648.626, "o", "    return _sparse_encode(\r\n"]
[648.636, "o", "        X,\r\n"]
[648.646, "o", "        dictionary,\r\n"]
[648.656, "o", "        gram=gram,\r\n"]
[648.666, "o", "        cov=cov,\r\n"]
[648.676, "o", "        algorithm=algorithm,\r\n"]
[648.686, "o", "        n_nonzero_coefs=n_nonzero_coefs,\r\n"]
[648.696, "o", "        alpha=alpha,\r\n"]
[648.706, "o", "        copy_cov=copy_cov,\r\n"]
[648.716, "o", "        init=init,\r\n"]
[648.726, "o", "        max_iter=max_iter,\r\n"]
[648.736, "o", "        n_jobs=n_jobs,\r\n"]
[648.746, "o", "        verbose=verbose,\r\n"]
[648.756, "o", "        positive=positive,\r\n"]
[648.766, "o", "    )\r\n"]
[648.776, "o", "\r\n"]
[648.786, "o", "\r\n"]
[648.796, "o", "def _sparse_encode(\r\n"]
[648.806, "o", "    X,\r\n"]
[648.816, "o", "    dictionary,\r\n"]
[648.826, "o", "    *,\r\n"]
[648.836, "o", "    gram=None,\r\n"]
[648.846, "o", "    cov=None,\r\n"]
[648.856, "o", "    algorithm=\"lasso_lars\",\r\n"]
[648.866, "o", "    n_nonzero_coefs=None,\r\n"]
[648.876, "o", "    alpha=None,\r\n"]
[648.886, "o", "    copy_cov=True,\r\n"]
[648.896, "o", "    init=None,\r\n"]
[648.906, "o", "    max_iter=1000,\r\n"]
[648.916, "o", "    n_jobs=None,\r\n"]
[648.926, "o", "    verbose=0,\r\n"]
[648.936, "o", "    positive=False,\r\n"]
[648.946, "o", "):\r\n"]
[648.956, "o", "    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[648.966, "o", "\r\n"]
[648.976, "o", "    n_samples, n_features = X.shape\r\n"]
[648.986, "o", "    n_components = dictionary.shape[0]\r\n"]
[648.996, "o", "\r\n"]
[649.006, "o", "    if algorithm in (\"lars\", \"omp\"):\r\n"]
[649.016, "o", "        regularization = n_nonzero_coefs\r\n"]
[649.026, "o", "        if regularization is None:\r\n"]
[649.036, "o", "            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[649.046, "o", "    else:\r\n"]
[649.056, "o", "        regularization = alpha\r\n"]
[649.066, "o", "        if regularization is None:\r\n"]
[649.076, "o", "            regularization = 1.0\r\n"]
[649.086, "o", "\r\n"]
[649.096, "o", "    if gram is None and algorithm != \"threshold\":\r\n"]
[649.106, "o", "        gram = np.dot(dictionary, dictionary.T)\r\n"]
[649.116, "o", "\r\n"]
[649.126, "o", "    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[649.136, "o", "        copy_cov = False\r\n"]
[649.146, "o", "        cov = np.dot(dictionary, X.T)\r\n"]
[649.156, "o", "\r\n"]
[649.166, "o", "    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[649.176, "o", "        code = _sparse_encode_precomputed(\r\n"]
[649.186, "o", "            X,\r\n"]
[649.196, "o", "            dictionary,\r\n"]
[649.206, "o", "            gram=gram,\r\n"]
[649.216, "o", "            cov=cov,\r\n"]
[649.226, "o", "            algorithm=algorithm,\r\n"]
[649.236, "o", "            regularization=regularization,\r\n"]
[649.246, "o", "            copy_cov=copy_cov,\r\n"]
[649.256, "o", "            init=init,\r\n"]
[649.266, "o", "            max_iter=max_iter,\r\n"]
[649.276, "o", "            verbose=verbose,\r\n"]
[649.286, "o", "            positive=positive,\r\n"]
[649.296, "o", "        )\r\n"]
[649.306, "o", "        return code\r\n"]
[649.316, "o", "\r\n"]
[649.326, "o", "    # Enter parallel code block\r\n"]
[649.336, "o", "    n_samples = X.shape[0]\r\n"]
[649.346, "o", "    n_components = dictionary.shape[0]\r\n"]
[649.356, "o", "    code = np.empty((n_samples, n_components))\r\n"]
[649.366, "o", "    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[649.376, "o", "\r\n"]
[649.386, "o", "    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[649.396, "o", "        delayed(_sparse_encode_precomputed)(\r\n"]
[649.406, "o", "            X[this_slice],\r\n"]
[649.416, "o", "            dictionary,\r\n"]
[649.426, "o", "            gram=gram,\r\n"]
[649.436, "o", "            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[649.446, "o", "            algorithm=algorithm,\r\n"]
[649.456, "o", "            regularization=regularization,\r\n"]
[649.466, "o", "            copy_cov=copy_cov,\r\n"]
[649.476, "o", "            init=init[this_slice] if init is not None else None,\r\n"]
[649.486, "o", "            max_iter=max_iter,\r\n"]
[649.496, "o", "            verbose=verbose,\r\n"]
[649.506, "o", "            positive=positive,\r\n"]
[649.516, "o", "        )\r\n"]
[649.526, "o", "        for this_slice in slices\r\n"]
[649.536, "o", "    )\r\n"]
[649.546, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[649.556, "o", "        code[this_slice] = this_view\r\n"]
[649.566, "o", "    return code\r\n"]
[649.576, "o", "\r\n"]
[649.586, "o", "\r\n"]
[649.596, "o", "def _update_dict(\r\n"]
[649.606, "o", "    dictionary,\r\n"]
[649.616, "o", "    Y,\r\n"]
[649.626, "o", "    code,\r\n"]
[649.636, "o", "    A=None,\r\n"]
[649.646, "o", "    B=None,\r\n"]
[649.656, "o", "    verbose=False,\r\n"]
[649.666, "o", "    random_state=None,\r\n"]
[649.676, "o", "    positive=False,\r\n"]
[649.686, "o", "):\r\n"]
[649.696, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[649.706, "o", "\r\n"]
[649.716, "o", "    Parameters\r\n"]
[649.726, "o", "    ----------\r\n"]
[649.736, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[649.746, "o", "        Value of the dictionary at the previous iteration.\r\n"]
[649.756, "o", "\r\n"]
[649.766, "o", "    Y : ndarray of shape (n_samples, n_features)\r\n"]
[649.776, "o", "        Data matrix.\r\n"]
[649.786, "o", "\r\n"]
[649.796, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[649.806, "o", "        Sparse coding of the data against which to optimize the dictionary.\r\n"]
[649.816, "o", "\r\n"]
[649.826, "o", "    A : ndarray of shape (n_components, n_components), default=None\r\n"]
[649.836, "o", "        Together with `B`, sufficient stats of the online model to update the\r\n"]
[649.846, "o", "        dictionary.\r\n"]
[649.856, "o", "\r\n"]
[649.866, "o", "    B : ndarray of shape (n_features, n_components), default=None\r\n"]
[649.876, "o", "        Together with `A`, sufficient stats of the online model to update the\r\n"]
[649.886, "o", "        dictionary.\r\n"]
[649.896, "o", "\r\n"]
[649.906, "o", "    verbose: bool, default=False\r\n"]
[649.916, "o", "        Degree of output the procedure will print.\r\n"]
[649.926, "o", "\r\n"]
[649.936, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[650.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[650.002, "i", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r"]
[650.004, "o", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r\n"]
[650.016, "o", "\u001b[?2004l\r\n"]
[650.026, "o", "           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[650.036, "o", "\r\n"]
[650.046, "o", "    return_n_iter : bool, default=False\r\n"]
[650.056, "o", "        Whether or not to return the number of iterations.\r\n"]
[650.066, "o", "\r\n"]
[650.076, "o", "        .. deprecated:: 1.1\r\n"]
[650.086, "o", "           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n"]
[650.096, "o", "\r\n"]
[650.106, "o", "    positive_dict : bool, default=False\r\n"]
[650.116, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[650.126, "o", "\r\n"]
[650.136, "o", "        .. versionadded:: 0.20\r\n"]
[650.146, "o", "\r\n"]
[650.156, "o", "    positive_code : bool, default=False\r\n"]
[650.166, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[650.176, "o", "\r\n"]
[650.186, "o", "        .. versionadded:: 0.20\r\n"]
[650.196, "o", "\r\n"]
[650.206, "o", "    method_max_iter : int, default=1000\r\n"]
[650.216, "o", "        Maximum number of iterations to perform when solving the lasso problem.\r\n"]
[650.226, "o", "\r\n"]
[650.236, "o", "        .. versionadded:: 0.22\r\n"]
[650.246, "o", "\r\n"]
[650.256, "o", "    tol : float, default=1e-3\r\n"]
[650.266, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[650.276, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[650.286, "o", "\r\n"]
[650.296, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[650.306, "o", "        `tol` to 0.0.\r\n"]
[650.316, "o", "\r\n"]
[650.326, "o", "        .. versionadded:: 1.1\r\n"]
[650.336, "o", "\r\n"]
[650.346, "o", "    max_no_improvement : int, default=10\r\n"]
[650.356, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[650.366, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[650.376, "o", "        `max_iter` is not None.\r\n"]
[650.386, "o", "\r\n"]
[650.396, "o", "        To disable convergence detection based on cost function, set\r\n"]
[650.406, "o", "        `max_no_improvement` to None.\r\n"]
[650.416, "o", "\r\n"]
[650.426, "o", "        .. versionadded:: 1.1\r\n"]
[650.436, "o", "\r\n"]
[650.446, "o", "    Returns\r\n"]
[650.456, "o", "    -------\r\n"]
[650.466, "o", "    code : ndarray of shape (n_samples, n_components),\r\n"]
[650.476, "o", "        The sparse code (only returned if `return_code=True`).\r\n"]
[650.486, "o", "\r\n"]
[650.496, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[650.506, "o", "        The solutions to the dictionary learning problem.\r\n"]
[650.516, "o", "\r\n"]
[650.526, "o", "    n_iter : int\r\n"]
[650.536, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[650.546, "o", "        set to `True`.\r\n"]
[650.556, "o", "\r\n"]
[650.566, "o", "    See Also\r\n"]
[650.576, "o", "    --------\r\n"]
[650.586, "o", "    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n"]
[650.596, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[650.606, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n"]
[650.616, "o", "        learning algorithm.\r\n"]
[650.626, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[650.636, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[650.646, "o", "    \"\"\"\r\n"]
[650.656, "o", "    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n"]
[650.666, "o", "    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n"]
[650.676, "o", "        raise ValueError(\r\n"]
[650.686, "o", "            \"The following arguments are incompatible with 'max_iter': \"\r\n"]
[650.696, "o", "            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n"]
[650.706, "o", "        )\r\n"]
[650.716, "o", "\r\n"]
[650.726, "o", "    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n"]
[650.736, "o", "    return_inner_stats = _check_warn_deprecated(\r\n"]
[650.746, "o", "        return_inner_stats,\r\n"]
[650.756, "o", "        \"return_inner_stats\",\r\n"]
[650.766, "o", "        default=False,\r\n"]
[650.776, "o", "        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n"]
[650.786, "o", "    )\r\n"]
[650.796, "o", "    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n"]
[650.806, "o", "    return_n_iter = _check_warn_deprecated(\r\n"]
[650.816, "o", "        return_n_iter,\r\n"]
[650.826, "o", "        \"return_n_iter\",\r\n"]
[650.836, "o", "        default=False,\r\n"]
[650.846, "o", "        additional_message=(\r\n"]
[650.856, "o", "            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n"]
[650.866, "o", "            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n"]
[650.876, "o", "        ),\r\n"]
[650.886, "o", "    )\r\n"]
[650.896, "o", "\r\n"]
[650.906, "o", "    if max_iter is not None:\r\n"]
[650.916, "o", "        transform_algorithm = \"lasso_\" + method\r\n"]
[650.926, "o", "\r\n"]
[650.936, "o", "        est = MiniBatchDictionaryLearning(\r\n"]
[650.946, "o", "            n_components=n_components,\r\n"]
[650.956, "o", "            alpha=alpha,\r\n"]
[650.966, "o", "            n_iter=n_iter,\r\n"]
[650.976, "o", "            n_jobs=n_jobs,\r\n"]
[650.986, "o", "            fit_algorithm=method,\r\n"]
[650.996, "o", "            batch_size=batch_size,\r\n"]
[651.006, "o", "            shuffle=shuffle,\r\n"]
[651.016, "o", "            dict_init=dict_init,\r\n"]
[651.026, "o", "            random_state=random_state,\r\n"]
[651.036, "o", "            transform_algorithm=transform_algorithm,\r\n"]
[651.046, "o", "            transform_alpha=alpha,\r\n"]
[651.056, "o", "            positive_code=positive_code,\r\n"]
[651.066, "o", "            positive_dict=positive_dict,\r\n"]
[651.076, "o", "            transform_max_iter=method_max_iter,\r\n"]
[651.086, "o", "            verbose=verbose,\r\n"]
[651.096, "o", "            callback=callback,\r\n"]
[651.106, "o", "            tol=tol,\r\n"]
[651.116, "o", "            max_no_improvement=max_no_improvement,\r\n"]
[651.126, "o", "        ).fit(X)\r\n"]
[651.136, "o", "\r\n"]
[651.146, "o", "        if not return_code:\r\n"]
[651.156, "o", "            return est.components_\r\n"]
[651.166, "o", "        else:\r\n"]
[651.176, "o", "            code = est.transform(X)\r\n"]
[651.186, "o", "            return code, est.components_\r\n"]
[651.196, "o", "\r\n"]
[651.206, "o", "    # TODO(1.4) remove the whole old behavior\r\n"]
[651.216, "o", "    # Fallback to old behavior\r\n"]
[651.226, "o", "\r\n"]
[651.236, "o", "    n_iter = _check_warn_deprecated(\r\n"]
[651.246, "o", "        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n"]
[651.256, "o", "    )\r\n"]
[651.266, "o", "\r\n"]
[651.276, "o", "    if n_components is None:\r\n"]
[651.286, "o", "        n_components = X.shape[1]\r\n"]
[651.296, "o", "\r\n"]
[651.306, "o", "    if method not in (\"lars\", \"cd\"):\r\n"]
[651.316, "o", "        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n"]
[651.326, "o", "\r\n"]
[651.336, "o", "    _check_positive_coding(method, positive_code)\r\n"]
[651.346, "o", "\r\n"]
[651.356, "o", "    method = \"lasso_\" + method\r\n"]
[651.366, "o", "\r\n"]
[651.376, "o", "    t0 = time.time()\r\n"]
[651.386, "o", "    n_samples, n_features = X.shape\r\n"]
[651.396, "o", "    # Avoid integer division problems\r\n"]
[651.406, "o", "    alpha = float(alpha)\r\n"]
[651.416, "o", "    random_state = check_random_state(random_state)\r\n"]
[651.426, "o", "\r\n"]
[651.436, "o", "    # Init V with SVD of X\r\n"]
[651.446, "o", "    if dict_init is not None:\r\n"]
[651.456, "o", "        dictionary = dict_init\r\n"]
[651.466, "o", "    else:\r\n"]
[651.476, "o", "        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n"]
[651.486, "o", "        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[651.496, "o", "    r = len(dictionary)\r\n"]
[651.506, "o", "    if n_components <= r:\r\n"]
[651.516, "o", "        dictionary = dictionary[:n_components, :]\r\n"]
[651.526, "o", "    else:\r\n"]
[651.536, "o", "        dictionary = np.r_[\r\n"]
[651.546, "o", "            dictionary,\r\n"]
[651.556, "o", "            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n"]
[651.566, "o", "        ]\r\n"]
[651.576, "o", "\r\n"]
[651.586, "o", "    if verbose == 1:\r\n"]
[651.596, "o", "        print(\"[dict_learning]\", end=\" \")\r\n"]
[651.606, "o", "\r\n"]
[651.616, "o", "    if shuffle:\r\n"]
[651.626, "o", "        X_train = X.copy()\r\n"]
[651.636, "o", "        random_state.shuffle(X_train)\r\n"]
[651.646, "o", "    else:\r\n"]
[651.656, "o", "        X_train = X\r\n"]
[651.666, "o", "\r\n"]
[651.676, "o", "    X_train = check_array(\r\n"]
[651.686, "o", "        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n"]
[651.696, "o", "    )\r\n"]
[651.706, "o", "\r\n"]
[651.716, "o", "    # Fortran-order dict better suited for the sparse coding which is the\r\n"]
[651.726, "o", "    # bottleneck of this algorithm.\r\n"]
[651.736, "o", "    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n"]
[651.746, "o", "    dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[651.756, "o", "\r\n"]
[651.766, "o", "    batches = gen_batches(n_samples, batch_size)\r\n"]
[651.776, "o", "    batches = itertools.cycle(batches)\r\n"]
[651.786, "o", "\r\n"]
[651.796, "o", "    # The covariance of the dictionary\r\n"]
[651.806, "o", "    if inner_stats is None:\r\n"]
[651.816, "o", "        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n"]
[651.826, "o", "        # The data approximation\r\n"]
[651.836, "o", "        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n"]
[651.846, "o", "    else:\r\n"]
[651.856, "o", "        A = inner_stats[0].copy()\r\n"]
[651.866, "o", "        B = inner_stats[1].copy()\r\n"]
[651.876, "o", "\r\n"]
[651.886, "o", "    # If n_iter is zero, we need to return zero.\r\n"]
[651.896, "o", "    ii = iter_offset - 1\r\n"]
[651.906, "o", "\r\n"]
[651.916, "o", "    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n"]
[651.926, "o", "        this_X = X_train[batch]\r\n"]
[651.936, "o", "        dt = time.time() - t0\r\n"]
[651.946, "o", "        if verbose == 1:\r\n"]
[651.956, "o", "            sys.stdout.write(\".\")\r\n"]
[651.966, "o", "            sys.stdout.flush()\r\n"]
[651.976, "o", "        elif verbose:\r\n"]
[651.986, "o", "            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n"]
[651.996, "o", "                print(\r\n"]
[652.006, "o", "                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n"]
[652.016, "o", "                )\r\n"]
[652.026, "o", "\r\n"]
[652.036, "o", "        this_code = sparse_encode(\r\n"]
[652.046, "o", "            this_X,\r\n"]
[652.056, "o", "            dictionary,\r\n"]
[652.066, "o", "            algorithm=method,\r\n"]
[652.076, "o", "            alpha=alpha,\r\n"]
[652.086, "o", "            n_jobs=n_jobs,\r\n"]
[652.096, "o", "            check_input=False,\r\n"]
[652.106, "o", "            positive=positive_code,\r\n"]
[652.116, "o", "            max_iter=method_max_iter,\r\n"]
[652.126, "o", "            verbose=verbose,\r\n"]
[652.136, "o", "        )\r\n"]
[652.146, "o", "\r\n"]
[652.156, "o", "        # Update the auxiliary variables\r\n"]
[652.166, "o", "        if ii < batch_size - 1:\r\n"]
[652.176, "o", "            theta = float((ii + 1) * batch_size)\r\n"]
[652.186, "o", "        else:\r\n"]
[652.196, "o", "            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n"]
[652.206, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[652.216, "o", "\r\n"]
[652.226, "o", "        A *= beta\r\n"]
[652.236, "o", "        A += np.dot(this_code.T, this_code)\r\n"]
[652.246, "o", "        B *= beta\r\n"]
[652.256, "o", "        B += np.dot(this_X.T, this_code)\r\n"]
[652.266, "o", "\r\n"]
[652.276, "o", "        # Update dictionary in place\r\n"]
[652.286, "o", "        _update_dict(\r\n"]
[652.296, "o", "            dictionary,\r\n"]
[652.306, "o", "            this_X,\r\n"]
[652.316, "o", "            this_code,\r\n"]
[652.326, "o", "            A,\r\n"]
[652.336, "o", "            B,\r\n"]
[652.346, "o", "            verbose=verbose,\r\n"]
[652.356, "o", "            random_state=random_state,\r\n"]
[652.366, "o", "            positive=positive_dict,\r\n"]
[652.376, "o", "        )\r\n"]
[652.386, "o", "\r\n"]
[652.396, "o", "        # Maybe we need a stopping criteria based on the amount of\r\n"]
[652.406, "o", "        # modification in the dictionary\r\n"]
[652.416, "o", "        if callback is not None:\r\n"]
[652.426, "o", "            callback(locals())\r\n"]
[652.436, "o", "\r\n"]
[652.446, "o", "    if return_inner_stats:\r\n"]
[652.456, "o", "        if return_n_iter:\r\n"]
[652.466, "o", "            return dictionary, (A, B), ii - iter_offset + 1\r\n"]
[652.476, "o", "        else:\r\n"]
[652.486, "o", "            return dictionary, (A, B)\r\n"]
[652.496, "o", "    if return_code:\r\n"]
[652.506, "o", "        if verbose > 1:\r\n"]
[652.516, "o", "            print(\"Learning code...\", end=\" \")\r\n"]
[652.526, "o", "        elif verbose == 1:\r\n"]
[652.536, "o", "            print(\"|\", end=\" \")\r\n"]
[652.546, "o", "        code = sparse_encode(\r\n"]
[652.556, "o", "            X,\r\n"]
[652.566, "o", "            dictionary,\r\n"]
[652.576, "o", "            algorithm=method,\r\n"]
[652.586, "o", "            alpha=alpha,\r\n"]
[652.596, "o", "            n_jobs=n_jobs,\r\n"]
[652.606, "o", "            check_input=False,\r\n"]
[652.616, "o", "            positive=positive_code,\r\n"]
[652.626, "o", "            max_iter=method_max_iter,\r\n"]
[652.636, "o", "            verbose=verbose,\r\n"]
[652.646, "o", "        )\r\n"]
[652.656, "o", "        if verbose > 1:\r\n"]
[652.666, "o", "            dt = time.time() - t0\r\n"]
[652.676, "o", "            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n"]
[652.686, "o", "        if return_n_iter:\r\n"]
[652.696, "o", "            return code, dictionary, ii - iter_offset + 1\r\n"]
[652.706, "o", "        else:\r\n"]
[652.716, "o", "            return code, dictionary\r\n"]
[652.726, "o", "\r\n"]
[652.736, "o", "    if return_n_iter:\r\n"]
[652.746, "o", "        return dictionary, ii - iter_offset + 1\r\n"]
[652.756, "o", "    else:\r\n"]
[652.766, "o", "        return dictionary\r\n"]
[652.776, "o", "\r\n"]
[652.786, "o", "\r\n"]
[652.796, "o", "@validate_params(\r\n"]
[652.806, "o", "    {\r\n"]
[652.816, "o", "        \"X\": [\"array-like\"],\r\n"]
[652.826, "o", "        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[652.836, "o", "        \"return_n_iter\": [\"boolean\"],\r\n"]
[652.846, "o", "        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[652.856, "o", "    },\r\n"]
[652.866, "o", "    prefer_skip_nested_validation=False,\r\n"]
[652.876, "o", ")\r\n"]
[652.886, "o", "def dict_learning(\r\n"]
[652.896, "o", "    X,\r\n"]
[652.906, "o", "    n_components,\r\n"]
[652.916, "o", "    *,\r\n"]
[652.926, "o", "    alpha,\r\n"]
[652.936, "o", "    max_iter=100,\r\n"]
[652.946, "o", "    tol=1e-8,\r\n"]
[652.956, "o", "    method=\"lars\",\r\n"]
[652.966, "o", "    n_jobs=None,\r\n"]
[652.976, "o", "    dict_init=None,\r\n"]
[652.986, "o", "    code_init=None,\r\n"]
[652.996, "o", "    callback=None,\r\n"]
[653.006, "o", "    verbose=False,\r\n"]
[653.016, "o", "    random_state=None,\r\n"]
[653.026, "o", "    return_n_iter=False,\r\n"]
[653.036, "o", "    positive_dict=False,\r\n"]
[653.046, "o", "    positive_code=False,\r\n"]
[653.056, "o", "    method_max_iter=1000,\r\n"]
[653.066, "o", "):\r\n"]
[653.076, "o", "    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n"]
[653.086, "o", "\r\n"]
[653.096, "o", "    Finds the best dictionary and the corresponding sparse code for\r\n"]
[653.106, "o", "    approximating the data matrix X by solving::\r\n"]
[653.116, "o", "\r\n"]
[653.126, "o", "        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[653.136, "o", "                     (U,V)\r\n"]
[653.146, "o", "                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n"]
[653.156, "o", "\r\n"]
[653.166, "o", "    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n"]
[653.176, "o", "    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n"]
[653.186, "o", "    which is the sum of the absolute values of all the entries in the matrix.\r\n"]
[653.196, "o", "\r\n"]
[653.206, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[653.216, "o", "\r\n"]
[653.226, "o", "    Parameters\r\n"]
[653.236, "o", "    ----------\r\n"]
[653.246, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[653.256, "o", "        Data matrix.\r\n"]
[653.266, "o", "\r\n"]
[653.276, "o", "    n_components : int\r\n"]
[653.286, "o", "        Number of dictionary atoms to extract.\r\n"]
[653.296, "o", "\r\n"]
[653.306, "o", "    alpha : int or float\r\n"]
[653.316, "o", "        Sparsity controlling parameter.\r\n"]
[653.326, "o", "\r\n"]
[653.336, "o", "    max_iter : int, default=100\r\n"]
[653.346, "o", "        Maximum number of iterations to perform.\r\n"]
[653.356, "o", "\r\n"]
[653.366, "o", "    tol : float, default=1e-8\r\n"]
[653.376, "o", "        Tolerance for the stopping condition.\r\n"]
[653.386, "o", "\r\n"]
[653.396, "o", "    method : {'lars', 'cd'}, default='lars'\r\n"]
[653.406, "o", "        The method used:\r\n"]
[653.416, "o", "\r\n"]
[653.426, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[653.436, "o", "           problem (`linear_model.lars_path`);\r\n"]
[653.446, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[653.456, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[653.466, "o", "          the estimated components are sparse.\r\n"]
[653.476, "o", "\r\n"]
[653.486, "o", "    n_jobs : int, default=None\r\n"]
[653.496, "o", "        Number of parallel jobs to run.\r\n"]
[653.506, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[653.516, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[653.526, "o", "        for more details.\r\n"]
[653.536, "o", "\r\n"]
[653.546, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[653.556, "o", "        Initial value for the dictionary for warm restart scenarios. Only used\r\n"]
[653.566, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[653.576, "o", "\r\n"]
[653.586, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[653.596, "o", "        Initial value for the sparse code for warm restart scenarios. Only used\r\n"]
[653.606, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[653.616, "o", "\r\n"]
[653.626, "o", "    callback : callable, default=None\r\n"]
[653.636, "o", "        Callable that gets invoked every five iterations.\r\n"]
[653.646, "o", "\r\n"]
[653.656, "o", "    verbose : bool, default=False\r\n"]
[653.666, "o", "        To control the verbosity of the procedure.\r\n"]
[653.676, "o", "\r\n"]
[653.686, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[653.696, "o", "        Used for randomly initializing the dictionary. Pass an int for\r\n"]
[653.706, "o", "        reproducible results across multiple function calls.\r\n"]
[653.716, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[653.726, "o", "\r\n"]
[653.736, "o", "    return_n_iter : bool, default=False\r\n"]
[653.746, "o", "        Whether or not to return the number of iterations.\r\n"]
[653.756, "o", "\r\n"]
[653.766, "o", "    positive_dict : bool, default=False\r\n"]
[653.776, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[653.786, "o", "\r\n"]
[653.796, "o", "        .. versionadded:: 0.20\r\n"]
[653.806, "o", "\r\n"]
[653.816, "o", "    positive_code : bool, default=False\r\n"]
[653.826, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[653.836, "o", "\r\n"]
[653.846, "o", "        .. versionadded:: 0.20\r\n"]
[653.856, "o", "\r\n"]
[653.866, "o", "    method_max_iter : int, default=1000\r\n"]
[653.876, "o", "        Maximum number of iterations to perform.\r\n"]
[653.886, "o", "\r\n"]
[653.896, "o", "        .. versionadded:: 0.22\r\n"]
[653.906, "o", "\r\n"]
[653.916, "o", "    Returns\r\n"]
[653.926, "o", "    -------\r\n"]
[653.936, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[653.946, "o", "        The sparse code factor in the matrix factorization.\r\n"]
[653.956, "o", "\r\n"]
[653.966, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[653.976, "o", "        The dictionary factor in the matrix factorization.\r\n"]
[653.986, "o", "\r\n"]
[653.996, "o", "    errors : array\r\n"]
[654.006, "o", "        Vector of errors at each iteration.\r\n"]
[654.016, "o", "\r\n"]
[654.026, "o", "    n_iter : int\r\n"]
[654.036, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[654.046, "o", "        set to True.\r\n"]
[654.056, "o", "\r\n"]
[654.066, "o", "    See Also\r\n"]
[654.076, "o", "    --------\r\n"]
[654.086, "o", "    dict_learning_online : Solve a dictionary learning matrix factorization\r\n"]
[654.096, "o", "        problem online.\r\n"]
[654.106, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[654.116, "o", "    MiniBatchDictionaryLearning : A faster, less accurate version\r\n"]
[654.126, "o", "        of the dictionary learning algorithm.\r\n"]
[654.136, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[654.146, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[654.156, "o", "    \"\"\"\r\n"]
[654.166, "o", "    estimator = DictionaryLearning(\r\n"]
[654.176, "o", "        n_components=n_components,\r\n"]
[654.186, "o", "        alpha=alpha,\r\n"]
[654.196, "o", "        max_iter=max_iter,\r\n"]
[654.206, "o", "        tol=tol,\r\n"]
[654.216, "o", "        fit_algorithm=method,\r\n"]
[654.226, "o", "        n_jobs=n_jobs,\r\n"]
[654.236, "o", "        dict_init=dict_init,\r\n"]
[654.246, "o", "        callback=callback,\r\n"]
[654.256, "o", "        code_init=code_init,\r\n"]
[654.266, "o", "        verbose=verbose,\r\n"]
[654.276, "o", "        random_state=random_state,\r\n"]
[654.286, "o", "        positive_code=positive_code,\r\n"]
[654.296, "o", "        positive_dict=positive_dict,\r\n"]
[654.306, "o", "        transform_max_iter=method_max_iter,\r\n"]
[654.316, "o", "    )\r\n"]
[654.326, "o", "    code = estimator.fit_transform(X)\r\n"]
[654.336, "o", "    if return_n_iter:\r\n"]
[654.346, "o", "        return (\r\n"]
[654.356, "o", "            code,\r\n"]
[654.366, "o", "            estimator.components_,\r\n"]
[654.376, "o", "            estimator.error_,\r\n"]
[654.386, "o", "            estimator.n_iter_,\r\n"]
[654.396, "o", "        )\r\n"]
[654.406, "o", "    return code, estimator.components_, estimator.error_\r\n"]
[654.416, "o", "\r\n"]
[654.426, "o", "\r\n"]
[654.436, "o", "class _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n"]
[654.446, "o", "    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n"]
[654.456, "o", "\r\n"]
[654.466, "o", "    def __init__(\r\n"]
[654.476, "o", "        self,\r\n"]
[654.486, "o", "        transform_algorithm,\r\n"]
[654.496, "o", "        transform_n_nonzero_coefs,\r\n"]
[654.506, "o", "        transform_alpha,\r\n"]
[654.516, "o", "        split_sign,\r\n"]
[654.526, "o", "        n_jobs,\r\n"]
[654.536, "o", "        positive_code,\r\n"]
[654.546, "o", "        transform_max_iter,\r\n"]
[654.556, "o", "    ):\r\n"]
[654.566, "o", "        self.transform_algorithm = transform_algorithm\r\n"]
[654.576, "o", "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n"]
[654.586, "o", "        self.transform_alpha = transform_alpha\r\n"]
[654.596, "o", "        self.transform_max_iter = transform_max_iter\r\n"]
[654.606, "o", "        self.split_sign = split_sign\r\n"]
[654.616, "o", "        self.n_jobs = n_jobs\r\n"]
[654.626, "o", "        self.positive_code = positive_code\r\n"]
[654.636, "o", "\r\n"]
[654.646, "o", "    def _transform(self, X, dictionary):\r\n"]
[654.656, "o", "        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n"]
[654.666, "o", "        SparseCoder.\"\"\"\r\n"]
[654.676, "o", "        X = self._validate_data(X, reset=False)\r\n"]
[654.686, "o", "\r\n"]
[654.696, "o", "        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n"]
[654.706, "o", "            transform_alpha = self.alpha\r\n"]
[654.716, "o", "        else:\r\n"]
[654.726, "o", "            transform_alpha = self.transform_alpha\r\n"]
[654.736, "o", "\r\n"]
[654.746, "o", "        code = sparse_encode(\r\n"]
[654.756, "o", "            X,\r\n"]
[654.766, "o", "            dictionary,\r\n"]
[654.776, "o", "            algorithm=self.transform_algorithm,\r\n"]
[654.786, "o", "            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n"]
[654.796, "o", "            alpha=transform_alpha,\r\n"]
[654.806, "o", "            max_iter=self.transform_max_iter,\r\n"]
[654.816, "o", "            n_jobs=self.n_jobs,\r\n"]
[654.826, "o", "            positive=self.positive_code,\r\n"]
[654.836, "o", "        )\r\n"]
[654.846, "o", "\r\n"]
[654.856, "o", "        if self.split_sign:\r\n"]
[654.866, "o", "            # feature vector is split into a positive and negative side\r\n"]
[654.876, "o", "            n_samples, n_features = code.shape\r\n"]
[654.886, "o", "            split_code = np.empty((n_samples, 2 * n_features))\r\n"]
[654.896, "o", "            split_code[:, :n_features] = np.maximum(code, 0)\r\n"]
[654.906, "o", "            split_code[:, n_features:] = -np.minimum(code, 0)\r\n"]
[654.916, "o", "            code = split_code\r\n"]
[654.926, "o", "\r\n"]
[654.936, "o", "        return code\r\n"]
[655.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[655.002, "i", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py\r"]
[655.004, "o", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py\r\n"]
[655.016, "o", "\u001b[?2004l\r\n"]
[655.026, "o", "\r\n"]
[655.036, "o", "    n_jobs : int or None, default=None\r\n"]
[655.046, "o", "        Number of parallel jobs to run.\r\n"]
[655.056, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[655.066, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[655.076, "o", "        for more details.\r\n"]
[655.086, "o", "\r\n"]
[655.096, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[655.106, "o", "        Initial value for the code, for warm restart. Only used if `code_init`\r\n"]
[655.116, "o", "        and `dict_init` are not None.\r\n"]
[655.126, "o", "\r\n"]
[655.136, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[655.146, "o", "        Initial values for the dictionary, for warm restart. Only used if\r\n"]
[655.156, "o", "        `code_init` and `dict_init` are not None.\r\n"]
[655.166, "o", "\r\n"]
[655.176, "o", "    callback : callable, default=None\r\n"]
[655.186, "o", "        Callable that gets invoked every five iterations.\r\n"]
[655.196, "o", "\r\n"]
[655.206, "o", "        .. versionadded:: 1.3\r\n"]
[655.216, "o", "\r\n"]
[655.226, "o", "    verbose : bool, default=False\r\n"]
[655.236, "o", "        To control the verbosity of the procedure.\r\n"]
[655.246, "o", "\r\n"]
[655.256, "o", "    split_sign : bool, default=False\r\n"]
[655.266, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[655.276, "o", "        its negative part and its positive part. This can improve the\r\n"]
[655.286, "o", "        performance of downstream classifiers.\r\n"]
[655.296, "o", "\r\n"]
[655.306, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[655.316, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[655.326, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[655.336, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[655.346, "o", "        results across multiple function calls.\r\n"]
[655.356, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[655.366, "o", "\r\n"]
[655.376, "o", "    positive_code : bool, default=False\r\n"]
[655.386, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[655.396, "o", "\r\n"]
[655.406, "o", "        .. versionadded:: 0.20\r\n"]
[655.416, "o", "\r\n"]
[655.426, "o", "    positive_dict : bool, default=False\r\n"]
[655.436, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[655.446, "o", "\r\n"]
[655.456, "o", "        .. versionadded:: 0.20\r\n"]
[655.466, "o", "\r\n"]
[655.476, "o", "    transform_max_iter : int, default=1000\r\n"]
[655.486, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[655.496, "o", "        `'lasso_lars'`.\r\n"]
[655.506, "o", "\r\n"]
[655.516, "o", "        .. versionadded:: 0.22\r\n"]
[655.526, "o", "\r\n"]
[655.536, "o", "    Attributes\r\n"]
[655.546, "o", "    ----------\r\n"]
[655.556, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[655.566, "o", "        dictionary atoms extracted from the data\r\n"]
[655.576, "o", "\r\n"]
[655.586, "o", "    error_ : array\r\n"]
[655.596, "o", "        vector of errors at each iteration\r\n"]
[655.606, "o", "\r\n"]
[655.616, "o", "    n_features_in_ : int\r\n"]
[655.626, "o", "        Number of features seen during :term:`fit`.\r\n"]
[655.636, "o", "\r\n"]
[655.646, "o", "        .. versionadded:: 0.24\r\n"]
[655.656, "o", "\r\n"]
[655.666, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[655.676, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[655.686, "o", "        has feature names that are all strings.\r\n"]
[655.696, "o", "\r\n"]
[655.706, "o", "        .. versionadded:: 1.0\r\n"]
[655.716, "o", "\r\n"]
[655.726, "o", "    n_iter_ : int\r\n"]
[655.736, "o", "        Number of iterations run.\r\n"]
[655.746, "o", "\r\n"]
[655.756, "o", "    See Also\r\n"]
[655.766, "o", "    --------\r\n"]
[655.776, "o", "    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n"]
[655.786, "o", "        dictionary learning algorithm.\r\n"]
[655.796, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[655.806, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[655.816, "o", "        precomputed dictionary.\r\n"]
[655.826, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[655.836, "o", "\r\n"]
[655.846, "o", "    References\r\n"]
[655.856, "o", "    ----------\r\n"]
[655.866, "o", "\r\n"]
[655.876, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[655.886, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[655.896, "o", "\r\n"]
[655.906, "o", "    Examples\r\n"]
[655.916, "o", "    --------\r\n"]
[655.926, "o", "    >>> import numpy as np\r\n"]
[655.936, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[655.946, "o", "    >>> from sklearn.decomposition import DictionaryLearning\r\n"]
[655.956, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[655.966, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[655.976, "o", "    ...     random_state=42,\r\n"]
[655.986, "o", "    ... )\r\n"]
[655.996, "o", "    >>> dict_learner = DictionaryLearning(\r\n"]
[656.006, "o", "    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n"]
[656.016, "o", "    ...     random_state=42,\r\n"]
[656.026, "o", "    ... )\r\n"]
[656.036, "o", "    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n"]
[656.046, "o", "\r\n"]
[656.056, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[656.066, "o", "\r\n"]
[656.076, "o", "    >>> np.mean(X_transformed == 0)\r\n"]
[656.086, "o", "    0.41...\r\n"]
[656.096, "o", "\r\n"]
[656.106, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[656.116, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[656.126, "o", "    the original signal:\r\n"]
[656.136, "o", "\r\n"]
[656.146, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[656.156, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[656.166, "o", "    0.07...\r\n"]
[656.176, "o", "    \"\"\"\r\n"]
[656.186, "o", "\r\n"]
[656.196, "o", "    _parameter_constraints: dict = {\r\n"]
[656.206, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[656.216, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[656.226, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[656.236, "o", "        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[656.246, "o", "        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[656.256, "o", "        \"transform_algorithm\": [\r\n"]
[656.266, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[656.276, "o", "        ],\r\n"]
[656.286, "o", "        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[656.296, "o", "        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[656.306, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[656.316, "o", "        \"code_init\": [np.ndarray, None],\r\n"]
[656.326, "o", "        \"dict_init\": [np.ndarray, None],\r\n"]
[656.336, "o", "        \"callback\": [callable, None],\r\n"]
[656.346, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[656.356, "o", "        \"split_sign\": [\"boolean\"],\r\n"]
[656.366, "o", "        \"random_state\": [\"random_state\"],\r\n"]
[656.376, "o", "        \"positive_code\": [\"boolean\"],\r\n"]
[656.386, "o", "        \"positive_dict\": [\"boolean\"],\r\n"]
[656.396, "o", "        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[656.406, "o", "    }\r\n"]
[656.416, "o", "\r\n"]
[656.426, "o", "    def __init__(\r\n"]
[656.436, "o", "        self,\r\n"]
[656.446, "o", "        n_components=None,\r\n"]
[656.456, "o", "        *,\r\n"]
[656.466, "o", "        alpha=1,\r\n"]
[656.476, "o", "        max_iter=1000,\r\n"]
[656.486, "o", "        tol=1e-8,\r\n"]
[656.496, "o", "        fit_algorithm=\"lars\",\r\n"]
[656.506, "o", "        transform_algorithm=\"omp\",\r\n"]
[656.516, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[656.526, "o", "        transform_alpha=None,\r\n"]
[656.536, "o", "        n_jobs=None,\r\n"]
[656.546, "o", "        code_init=None,\r\n"]
[656.556, "o", "        dict_init=None,\r\n"]
[656.566, "o", "        callback=None,\r\n"]
[656.576, "o", "        verbose=False,\r\n"]
[656.586, "o", "        split_sign=False,\r\n"]
[656.596, "o", "        random_state=None,\r\n"]
[656.606, "o", "        positive_code=False,\r\n"]
[656.616, "o", "        positive_dict=False,\r\n"]
[656.626, "o", "        transform_max_iter=1000,\r\n"]
[656.636, "o", "    ):\r\n"]
[656.646, "o", "        super().__init__(\r\n"]
[656.656, "o", "            transform_algorithm,\r\n"]
[656.666, "o", "            transform_n_nonzero_coefs,\r\n"]
[656.676, "o", "            transform_alpha,\r\n"]
[656.686, "o", "            split_sign,\r\n"]
[656.696, "o", "            n_jobs,\r\n"]
[656.706, "o", "            positive_code,\r\n"]
[656.716, "o", "            transform_max_iter,\r\n"]
[656.726, "o", "        )\r\n"]
[656.736, "o", "        self.n_components = n_components\r\n"]
[656.746, "o", "        self.alpha = alpha\r\n"]
[656.756, "o", "        self.max_iter = max_iter\r\n"]
[656.766, "o", "        self.tol = tol\r\n"]
[656.776, "o", "        self.fit_algorithm = fit_algorithm\r\n"]
[656.786, "o", "        self.code_init = code_init\r\n"]
[656.796, "o", "        self.dict_init = dict_init\r\n"]
[656.806, "o", "        self.callback = callback\r\n"]
[656.816, "o", "        self.verbose = verbose\r\n"]
[656.826, "o", "        self.random_state = random_state\r\n"]
[656.836, "o", "        self.positive_dict = positive_dict\r\n"]
[656.846, "o", "\r\n"]
[656.856, "o", "    def fit(self, X, y=None):\r\n"]
[656.866, "o", "        \"\"\"Fit the model from data in X.\r\n"]
[656.876, "o", "\r\n"]
[656.886, "o", "        Parameters\r\n"]
[656.896, "o", "        ----------\r\n"]
[656.906, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[656.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[656.926, "o", "            and `n_features` is the number of features.\r\n"]
[656.936, "o", "\r\n"]
[656.946, "o", "        y : Ignored\r\n"]
[656.956, "o", "            Not used, present for API consistency by convention.\r\n"]
[656.966, "o", "\r\n"]
[656.976, "o", "        Returns\r\n"]
[656.986, "o", "        -------\r\n"]
[656.996, "o", "        self : object\r\n"]
[657.006, "o", "            Returns the instance itself.\r\n"]
[657.016, "o", "        \"\"\"\r\n"]
[657.026, "o", "        self.fit_transform(X)\r\n"]
[657.036, "o", "        return self\r\n"]
[657.046, "o", "\r\n"]
[657.056, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[657.066, "o", "    def fit_transform(self, X, y=None):\r\n"]
[657.076, "o", "        \"\"\"Fit the model from data in X and return the transformed data.\r\n"]
[657.086, "o", "\r\n"]
[657.096, "o", "        Parameters\r\n"]
[657.106, "o", "        ----------\r\n"]
[657.116, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[657.126, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[657.136, "o", "            and `n_features` is the number of features.\r\n"]
[657.146, "o", "\r\n"]
[657.156, "o", "        y : Ignored\r\n"]
[657.166, "o", "            Not used, present for API consistency by convention.\r\n"]
[657.176, "o", "\r\n"]
[657.186, "o", "        Returns\r\n"]
[657.196, "o", "        -------\r\n"]
[657.206, "o", "        V : ndarray of shape (n_samples, n_components)\r\n"]
[657.216, "o", "            Transformed data.\r\n"]
[657.226, "o", "        \"\"\"\r\n"]
[657.236, "o", "        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n"]
[657.246, "o", "\r\n"]
[657.256, "o", "        method = \"lasso_\" + self.fit_algorithm\r\n"]
[657.266, "o", "\r\n"]
[657.276, "o", "        random_state = check_random_state(self.random_state)\r\n"]
[657.286, "o", "        X = self._validate_data(X)\r\n"]
[657.296, "o", "\r\n"]
[657.306, "o", "        if self.n_components is None:\r\n"]
[657.316, "o", "            n_components = X.shape[1]\r\n"]
[657.326, "o", "        else:\r\n"]
[657.336, "o", "            n_components = self.n_components\r\n"]
[657.346, "o", "\r\n"]
[657.356, "o", "        V, U, E, self.n_iter_ = _dict_learning(\r\n"]
[657.366, "o", "            X,\r\n"]
[657.376, "o", "            n_components,\r\n"]
[657.386, "o", "            alpha=self.alpha,\r\n"]
[657.396, "o", "            tol=self.tol,\r\n"]
[657.406, "o", "            max_iter=self.max_iter,\r\n"]
[657.416, "o", "            method=method,\r\n"]
[657.426, "o", "            method_max_iter=self.transform_max_iter,\r\n"]
[657.436, "o", "            n_jobs=self.n_jobs,\r\n"]
[657.446, "o", "            code_init=self.code_init,\r\n"]
[657.456, "o", "            dict_init=self.dict_init,\r\n"]
[657.466, "o", "            callback=self.callback,\r\n"]
[657.476, "o", "            verbose=self.verbose,\r\n"]
[657.486, "o", "            random_state=random_state,\r\n"]
[657.496, "o", "            return_n_iter=True,\r\n"]
[657.506, "o", "            positive_dict=self.positive_dict,\r\n"]
[657.516, "o", "            positive_code=self.positive_code,\r\n"]
[657.526, "o", "        )\r\n"]
[657.536, "o", "        self.components_ = U\r\n"]
[657.546, "o", "        self.error_ = E\r\n"]
[657.556, "o", "\r\n"]
[657.566, "o", "        return V\r\n"]
[657.576, "o", "\r\n"]
[657.586, "o", "    @property\r\n"]
[657.596, "o", "    def _n_features_out(self):\r\n"]
[657.606, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[657.616, "o", "        return self.components_.shape[0]\r\n"]
[657.626, "o", "\r\n"]
[657.636, "o", "    def _more_tags(self):\r\n"]
[657.646, "o", "        return {\r\n"]
[657.656, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[657.666, "o", "        }\r\n"]
[657.676, "o", "\r\n"]
[657.686, "o", "\r\n"]
[657.696, "o", "class MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[657.706, "o", "    \"\"\"Mini-batch dictionary learning.\r\n"]
[657.716, "o", "\r\n"]
[657.726, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[657.736, "o", "    encoding the fitted data.\r\n"]
[657.746, "o", "\r\n"]
[657.756, "o", "    Solves the optimization problem::\r\n"]
[657.766, "o", "\r\n"]
[657.776, "o", "       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[657.786, "o", "                    (U,V)\r\n"]
[657.796, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[657.806, "o", "\r\n"]
[657.816, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[657.826, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[657.836, "o", "    of all the entries in the matrix.\r\n"]
[657.846, "o", "\r\n"]
[657.856, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[657.866, "o", "\r\n"]
[657.876, "o", "    Parameters\r\n"]
[657.886, "o", "    ----------\r\n"]
[657.896, "o", "    n_components : int, default=None\r\n"]
[657.906, "o", "        Number of dictionary elements to extract.\r\n"]
[657.916, "o", "\r\n"]
[657.926, "o", "    alpha : float, default=1\r\n"]
[657.936, "o", "        Sparsity controlling parameter.\r\n"]
[657.946, "o", "\r\n"]
[657.956, "o", "    n_iter : int, default=1000\r\n"]
[657.966, "o", "        Total number of iterations over data batches to perform.\r\n"]
[657.976, "o", "\r\n"]
[657.986, "o", "        .. deprecated:: 1.1\r\n"]
[657.996, "o", "           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[658.006, "o", "           ``max_iter`` instead.\r\n"]
[658.016, "o", "\r\n"]
[658.026, "o", "    max_iter : int, default=None\r\n"]
[658.036, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[658.046, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[658.056, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[658.066, "o", "\r\n"]
[658.076, "o", "        .. versionadded:: 1.1\r\n"]
[658.086, "o", "\r\n"]
[658.096, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[658.106, "o", "        The algorithm used:\r\n"]
[658.116, "o", "\r\n"]
[658.126, "o", "        - `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[658.136, "o", "          problem (`linear_model.lars_path`)\r\n"]
[658.146, "o", "        - `'cd'`: uses the coordinate descent method to compute the\r\n"]
[658.156, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[658.166, "o", "          the estimated components are sparse.\r\n"]
[658.176, "o", "\r\n"]
[658.186, "o", "    n_jobs : int, default=None\r\n"]
[658.196, "o", "        Number of parallel jobs to run.\r\n"]
[658.206, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[658.216, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[658.226, "o", "        for more details.\r\n"]
[658.236, "o", "\r\n"]
[658.246, "o", "    batch_size : int, default=256\r\n"]
[658.256, "o", "        Number of samples in each mini-batch.\r\n"]
[658.266, "o", "\r\n"]
[658.276, "o", "        .. versionchanged:: 1.3\r\n"]
[658.286, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[658.296, "o", "\r\n"]
[658.306, "o", "    shuffle : bool, default=True\r\n"]
[658.316, "o", "        Whether to shuffle the samples before forming batches.\r\n"]
[658.326, "o", "\r\n"]
[658.336, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[658.346, "o", "        Initial value of the dictionary for warm restart scenarios.\r\n"]
[658.356, "o", "\r\n"]
[658.366, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[658.376, "o", "            'threshold'}, default='omp'\r\n"]
[658.386, "o", "        Algorithm used to transform the data:\r\n"]
[658.396, "o", "\r\n"]
[658.406, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[658.416, "o", "          (`linear_model.lars_path`);\r\n"]
[658.426, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[658.436, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[658.446, "o", "          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n"]
[658.456, "o", "          if the estimated components are sparse.\r\n"]
[658.466, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[658.476, "o", "          solution.\r\n"]
[658.486, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[658.496, "o", "          the projection ``dictionary * X'``.\r\n"]
[658.506, "o", "\r\n"]
[658.516, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[658.526, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[658.536, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[658.546, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[658.556, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[658.566, "o", "\r\n"]
[658.576, "o", "    transform_alpha : float, default=None\r\n"]
[658.586, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[658.596, "o", "        penalty applied to the L1 norm.\r\n"]
[658.606, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[658.616, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[658.626, "o", "        If `None`, defaults to `alpha`.\r\n"]
[658.636, "o", "\r\n"]
[658.646, "o", "        .. versionchanged:: 1.2\r\n"]
[658.656, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[658.666, "o", "\r\n"]
[658.676, "o", "    verbose : bool or int, default=False\r\n"]
[658.686, "o", "        To control the verbosity of the procedure.\r\n"]
[658.696, "o", "\r\n"]
[658.706, "o", "    split_sign : bool, default=False\r\n"]
[658.716, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[658.726, "o", "        its negative part and its positive part. This can improve the\r\n"]
[658.736, "o", "        performance of downstream classifiers.\r\n"]
[658.746, "o", "\r\n"]
[658.756, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[658.766, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[658.776, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[658.786, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[658.796, "o", "        results across multiple function calls.\r\n"]
[658.806, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[658.816, "o", "\r\n"]
[658.826, "o", "    positive_code : bool, default=False\r\n"]
[658.836, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[658.846, "o", "\r\n"]
[658.856, "o", "        .. versionadded:: 0.20\r\n"]
[658.866, "o", "\r\n"]
[658.876, "o", "    positive_dict : bool, default=False\r\n"]
[658.886, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[658.896, "o", "\r\n"]
[658.906, "o", "        .. versionadded:: 0.20\r\n"]
[658.916, "o", "\r\n"]
[658.926, "o", "    transform_max_iter : int, default=1000\r\n"]
[658.936, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[658.946, "o", "        `'lasso_lars'`.\r\n"]
[658.956, "o", "\r\n"]
[658.966, "o", "        .. versionadded:: 0.22\r\n"]
[658.976, "o", "\r\n"]
[658.986, "o", "    callback : callable, default=None\r\n"]
[658.996, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[659.006, "o", "\r\n"]
[659.016, "o", "        .. versionadded:: 1.1\r\n"]
[659.026, "o", "\r\n"]
[659.036, "o", "    tol : float, default=1e-3\r\n"]
[659.046, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[659.056, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[659.066, "o", "\r\n"]
[659.076, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[659.086, "o", "        `tol` to 0.0.\r\n"]
[659.096, "o", "\r\n"]
[659.106, "o", "        .. versionadded:: 1.1\r\n"]
[659.116, "o", "\r\n"]
[659.126, "o", "    max_no_improvement : int, default=10\r\n"]
[659.136, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[659.146, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[659.156, "o", "        `max_iter` is not None.\r\n"]
[659.166, "o", "\r\n"]
[659.176, "o", "        To disable convergence detection based on cost function, set\r\n"]
[659.186, "o", "        `max_no_improvement` to None.\r\n"]
[659.196, "o", "\r\n"]
[659.206, "o", "        .. versionadded:: 1.1\r\n"]
[659.216, "o", "\r\n"]
[659.226, "o", "    Attributes\r\n"]
[659.236, "o", "    ----------\r\n"]
[659.246, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[659.256, "o", "        Components extracted from the data.\r\n"]
[659.266, "o", "\r\n"]
[659.276, "o", "    n_features_in_ : int\r\n"]
[659.286, "o", "        Number of features seen during :term:`fit`.\r\n"]
[659.296, "o", "\r\n"]
[659.306, "o", "        .. versionadded:: 0.24\r\n"]
[659.316, "o", "\r\n"]
[659.326, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[659.336, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[659.346, "o", "        has feature names that are all strings.\r\n"]
[659.356, "o", "\r\n"]
[659.366, "o", "        .. versionadded:: 1.0\r\n"]
[659.376, "o", "\r\n"]
[659.386, "o", "    n_iter_ : int\r\n"]
[659.396, "o", "        Number of iterations over the full dataset.\r\n"]
[659.406, "o", "\r\n"]
[659.416, "o", "    n_steps_ : int\r\n"]
[659.426, "o", "        Number of mini-batches processed.\r\n"]
[659.436, "o", "\r\n"]
[659.446, "o", "        .. versionadded:: 1.1\r\n"]
[659.456, "o", "\r\n"]
[659.466, "o", "    See Also\r\n"]
[659.476, "o", "    --------\r\n"]
[659.486, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[659.496, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[659.506, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[659.516, "o", "        precomputed dictionary.\r\n"]
[659.526, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[659.536, "o", "\r\n"]
[659.546, "o", "    References\r\n"]
[659.556, "o", "    ----------\r\n"]
[659.566, "o", "\r\n"]
[659.576, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[659.586, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[659.596, "o", "\r\n"]
[659.606, "o", "    Examples\r\n"]
[659.616, "o", "    --------\r\n"]
[659.626, "o", "    >>> import numpy as np\r\n"]
[659.636, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[659.646, "o", "    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n"]
[659.656, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[659.666, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[659.676, "o", "    ...     random_state=42)\r\n"]
[659.686, "o", "    >>> dict_learner = MiniBatchDictionaryLearning(\r\n"]
[659.696, "o", "    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n"]
[659.706, "o", "    ...     transform_alpha=0.1, random_state=42)\r\n"]
[659.716, "o", "    >>> X_transformed = dict_learner.fit_transform(X)\r\n"]
[659.726, "o", "\r\n"]
[659.736, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[659.746, "o", "\r\n"]
[659.756, "o", "    >>> np.mean(X_transformed == 0) < 0.5\r\n"]
[659.766, "o", "    True\r\n"]
[659.776, "o", "\r\n"]
[659.786, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[659.796, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[659.806, "o", "    the original signal:\r\n"]
[659.816, "o", "\r\n"]
[659.826, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[659.836, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[659.846, "o", "    0.057...\r\n"]
[659.856, "o", "    \"\"\"\r\n"]
[659.866, "o", "\r\n"]
[659.876, "o", "    _parameter_constraints: dict = {\r\n"]
[659.886, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[659.896, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[659.906, "o", "        \"n_iter\": [\r\n"]
[659.916, "o", "            Interval(Integral, 0, None, closed=\"left\"),\r\n"]
[659.926, "o", "            Hidden(StrOptions({\"deprecated\"})),\r\n"]
[659.936, "o", "        ],\r\n"]
[660.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[660.002, "i", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py\r"]
[660.004, "o", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py\r\n"]
[662.478, "o", "\u001b[?2004l\r\n"]
[665.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[665.002, "i", "grep -n \"_sparse_encode_precomputed\" sklearn/decomposition/_dict_learning.py\r"]
[665.004, "o", "grep -n \"_sparse_encode_precomputed\" sklearn/decomposition/_dict_learning.py\r\n"]
[666.242, "o", "\u001b[?2004l\r\n"]
[667.478, "o", "\u001b[32m\u001b[K38\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdef \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n"]
[668.714, "o", "\u001b[32m\u001b[K416\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        code = \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n"]
[670.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[670.002, "i", "grep -n \"cov\\s*=\\s\" sklearn/decomposition/_dict_learning.py\r"]
[670.004, "o", "grep -n \"cov\\s*=\\s\" sklearn/decomposition/_dict_learning.py\r\n"]
[671.654, "o", "\u001b[?2004l\r\n"]
[673.302, "o", "\u001b[32m\u001b[K412\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        copy_\u001b[01;31m\u001b[Kcov = \u001b[m\u001b[KFalse\r\n"]
[675.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[675.002, "i", "grep -n \"gram\\s*=\\s\" sklearn/decomposition/_dict_learning.py\r"]
[675.004, "o", "grep -n \"gram\\s*=\\s\" sklearn/decomposition/_dict_learning.py\r\n"]
[677.478, "o", "\u001b[?2004l\r\n"]
[680.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[680.002, "i", "sed -n '1,500p' sklearn/decomposition/_dict_learning.py\r"]
[680.004, "o", "sed -n '1,500p' sklearn/decomposition/_dict_learning.py\r\n"]
[680.016, "o", "\u001b[?2004l\r\n"]
[680.026, "o", "\"\"\" Dictionary learning.\r\n"]
[680.036, "o", "\"\"\"\r\n"]
[680.046, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[680.056, "o", "# License: BSD 3 clause\r\n"]
[680.066, "o", "\r\n"]
[680.076, "o", "import itertools\r\n"]
[680.086, "o", "import sys\r\n"]
[680.096, "o", "import time\r\n"]
[680.106, "o", "import warnings\r\n"]
[680.116, "o", "from math import ceil\r\n"]
[680.126, "o", "from numbers import Integral, Real\r\n"]
[680.136, "o", "\r\n"]
[680.146, "o", "import numpy as np\r\n"]
[680.156, "o", "from joblib import effective_n_jobs\r\n"]
[680.166, "o", "from scipy import linalg\r\n"]
[680.176, "o", "\r\n"]
[680.186, "o", "from ..base import (\r\n"]
[680.196, "o", "    BaseEstimator,\r\n"]
[680.206, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[680.216, "o", "    TransformerMixin,\r\n"]
[680.226, "o", "    _fit_context,\r\n"]
[680.236, "o", ")\r\n"]
[680.246, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[680.256, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[680.266, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[680.276, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[680.286, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[680.296, "o", "from ..utils.validation import check_is_fitted\r\n"]
[680.306, "o", "\r\n"]
[680.316, "o", "\r\n"]
[680.326, "o", "def _check_positive_coding(method, positive):\r\n"]
[680.336, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[680.346, "o", "        raise ValueError(\r\n"]
[680.356, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[680.366, "o", "        )\r\n"]
[680.376, "o", "\r\n"]
[680.386, "o", "\r\n"]
[680.396, "o", "def _sparse_encode_precomputed(\r\n"]
[680.406, "o", "    X,\r\n"]
[680.416, "o", "    dictionary,\r\n"]
[680.426, "o", "    *,\r\n"]
[680.436, "o", "    gram=None,\r\n"]
[680.446, "o", "    cov=None,\r\n"]
[680.456, "o", "    algorithm=\"lasso_lars\",\r\n"]
[680.466, "o", "    regularization=None,\r\n"]
[680.476, "o", "    copy_cov=True,\r\n"]
[680.486, "o", "    init=None,\r\n"]
[680.496, "o", "    max_iter=1000,\r\n"]
[680.506, "o", "    verbose=0,\r\n"]
[680.516, "o", "    positive=False,\r\n"]
[680.526, "o", "):\r\n"]
[680.536, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[680.546, "o", "\r\n"]
[680.556, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[680.566, "o", "\r\n"]
[680.576, "o", "    Parameters\r\n"]
[680.586, "o", "    ----------\r\n"]
[680.596, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[680.606, "o", "        Data matrix.\r\n"]
[680.616, "o", "\r\n"]
[680.626, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[680.636, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[680.646, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[680.656, "o", "\r\n"]
[680.666, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[680.676, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[680.686, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[680.696, "o", "\r\n"]
[680.706, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[680.716, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[680.726, "o", "\r\n"]
[680.736, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[680.746, "o", "            default='lasso_lars'\r\n"]
[680.756, "o", "        The algorithm used:\r\n"]
[680.766, "o", "\r\n"]
[680.776, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[680.786, "o", "          (`linear_model.lars_path`);\r\n"]
[680.796, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[680.806, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[680.816, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[680.826, "o", "          the estimated components are sparse;\r\n"]
[680.836, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[680.846, "o", "          solution;\r\n"]
[680.856, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[680.866, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[680.876, "o", "\r\n"]
[680.886, "o", "    regularization : int or float, default=None\r\n"]
[680.896, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[680.906, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[680.916, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[680.926, "o", "\r\n"]
[680.936, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[680.946, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[680.956, "o", "        `algorithm='lasso_cd'`.\r\n"]
[680.966, "o", "\r\n"]
[680.976, "o", "    max_iter : int, default=1000\r\n"]
[680.986, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[680.996, "o", "        `'lasso_lars'`.\r\n"]
[681.006, "o", "\r\n"]
[681.016, "o", "    copy_cov : bool, default=True\r\n"]
[681.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[681.036, "o", "        be overwritten.\r\n"]
[681.046, "o", "\r\n"]
[681.056, "o", "    verbose : int, default=0\r\n"]
[681.066, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[681.076, "o", "\r\n"]
[681.086, "o", "    positive: bool, default=False\r\n"]
[681.096, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[681.106, "o", "\r\n"]
[681.116, "o", "        .. versionadded:: 0.20\r\n"]
[681.126, "o", "\r\n"]
[681.136, "o", "    Returns\r\n"]
[681.146, "o", "    -------\r\n"]
[681.156, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[681.166, "o", "        The sparse codes.\r\n"]
[681.176, "o", "    \"\"\"\r\n"]
[681.186, "o", "    n_samples, n_features = X.shape\r\n"]
[681.196, "o", "    n_components = dictionary.shape[0]\r\n"]
[681.206, "o", "\r\n"]
[681.216, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[681.226, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[681.236, "o", "        try:\r\n"]
[681.246, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[681.256, "o", "\r\n"]
[681.266, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[681.276, "o", "            # corrects the verbosity level.\r\n"]
[681.286, "o", "            lasso_lars = LassoLars(\r\n"]
[681.296, "o", "                alpha=alpha,\r\n"]
[681.306, "o", "                fit_intercept=False,\r\n"]
[681.316, "o", "                verbose=verbose,\r\n"]
[681.326, "o", "                precompute=gram,\r\n"]
[681.336, "o", "                fit_path=False,\r\n"]
[681.346, "o", "                positive=positive,\r\n"]
[681.356, "o", "                max_iter=max_iter,\r\n"]
[681.366, "o", "            )\r\n"]
[681.376, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[681.386, "o", "            new_code = lasso_lars.coef_\r\n"]
[681.396, "o", "        finally:\r\n"]
[681.406, "o", "            np.seterr(**err_mgt)\r\n"]
[681.416, "o", "\r\n"]
[681.426, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[681.436, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[681.446, "o", "\r\n"]
[681.456, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[681.466, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[681.476, "o", "        # argument that we could pass in from Lasso.\r\n"]
[681.486, "o", "        clf = Lasso(\r\n"]
[681.496, "o", "            alpha=alpha,\r\n"]
[681.506, "o", "            fit_intercept=False,\r\n"]
[681.516, "o", "            precompute=gram,\r\n"]
[681.526, "o", "            max_iter=max_iter,\r\n"]
[681.536, "o", "            warm_start=True,\r\n"]
[681.546, "o", "            positive=positive,\r\n"]
[681.556, "o", "        )\r\n"]
[681.566, "o", "\r\n"]
[681.576, "o", "        if init is not None:\r\n"]
[681.586, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[681.596, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[681.606, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[681.616, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[681.626, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[681.636, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[681.646, "o", "                init = np.array(init)\r\n"]
[681.656, "o", "            clf.coef_ = init\r\n"]
[681.666, "o", "\r\n"]
[681.676, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[681.686, "o", "        new_code = clf.coef_\r\n"]
[681.696, "o", "\r\n"]
[681.706, "o", "    elif algorithm == \"lars\":\r\n"]
[681.716, "o", "        try:\r\n"]
[681.726, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[681.736, "o", "\r\n"]
[681.746, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[681.756, "o", "            # corrects the verbosity level.\r\n"]
[681.766, "o", "            lars = Lars(\r\n"]
[681.776, "o", "                fit_intercept=False,\r\n"]
[681.786, "o", "                verbose=verbose,\r\n"]
[681.796, "o", "                precompute=gram,\r\n"]
[681.806, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[681.816, "o", "                fit_path=False,\r\n"]
[681.826, "o", "            )\r\n"]
[681.836, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[681.846, "o", "            new_code = lars.coef_\r\n"]
[681.856, "o", "        finally:\r\n"]
[681.866, "o", "            np.seterr(**err_mgt)\r\n"]
[681.876, "o", "\r\n"]
[681.886, "o", "    elif algorithm == \"threshold\":\r\n"]
[681.896, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[681.906, "o", "        if positive:\r\n"]
[681.916, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[681.926, "o", "\r\n"]
[681.936, "o", "    elif algorithm == \"omp\":\r\n"]
[681.946, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[681.956, "o", "            Gram=gram,\r\n"]
[681.966, "o", "            Xy=cov,\r\n"]
[681.976, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[681.986, "o", "            tol=None,\r\n"]
[681.996, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[682.006, "o", "            copy_Xy=copy_cov,\r\n"]
[682.016, "o", "        ).T\r\n"]
[682.026, "o", "\r\n"]
[682.036, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[682.046, "o", "\r\n"]
[682.056, "o", "\r\n"]
[682.066, "o", "@validate_params(\r\n"]
[682.076, "o", "    {\r\n"]
[682.086, "o", "        \"X\": [\"array-like\"],\r\n"]
[682.096, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[682.106, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[682.116, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[682.126, "o", "        \"algorithm\": [\r\n"]
[682.136, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[682.146, "o", "        ],\r\n"]
[682.156, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[682.166, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[682.176, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[682.186, "o", "        \"init\": [\"array-like\", None],\r\n"]
[682.196, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[682.206, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[682.216, "o", "        \"check_input\": [\"boolean\"],\r\n"]
[682.226, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[682.236, "o", "        \"positive\": [\"boolean\"],\r\n"]
[682.246, "o", "    },\r\n"]
[682.256, "o", "    prefer_skip_nested_validation=True,\r\n"]
[682.266, "o", ")\r\n"]
[682.276, "o", "# XXX : could be moved to the linear_model module\r\n"]
[682.286, "o", "def sparse_encode(\r\n"]
[682.296, "o", "    X,\r\n"]
[682.306, "o", "    dictionary,\r\n"]
[682.316, "o", "    *,\r\n"]
[682.326, "o", "    gram=None,\r\n"]
[682.336, "o", "    cov=None,\r\n"]
[682.346, "o", "    algorithm=\"lasso_lars\",\r\n"]
[682.356, "o", "    n_nonzero_coefs=None,\r\n"]
[682.366, "o", "    alpha=None,\r\n"]
[682.376, "o", "    copy_cov=True,\r\n"]
[682.386, "o", "    init=None,\r\n"]
[682.396, "o", "    max_iter=1000,\r\n"]
[682.406, "o", "    n_jobs=None,\r\n"]
[682.416, "o", "    check_input=True,\r\n"]
[682.426, "o", "    verbose=0,\r\n"]
[682.436, "o", "    positive=False,\r\n"]
[682.446, "o", "):\r\n"]
[682.456, "o", "    \"\"\"Sparse coding.\r\n"]
[682.466, "o", "\r\n"]
[682.476, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[682.486, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[682.496, "o", "\r\n"]
[682.506, "o", "        X ~= code * dictionary\r\n"]
[682.516, "o", "\r\n"]
[682.526, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[682.536, "o", "\r\n"]
[682.546, "o", "    Parameters\r\n"]
[682.556, "o", "    ----------\r\n"]
[682.566, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[682.576, "o", "        Data matrix.\r\n"]
[682.586, "o", "\r\n"]
[682.596, "o", "    dictionary : array-like of shape (n_components, n_features)\r\n"]
[682.606, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[682.616, "o", "        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[682.626, "o", "        output.\r\n"]
[682.636, "o", "\r\n"]
[682.646, "o", "    gram : array-like of shape (n_components, n_components), default=None\r\n"]
[682.656, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n"]
[682.666, "o", "\r\n"]
[682.676, "o", "    cov : array-like of shape (n_components, n_samples), default=None\r\n"]
[682.686, "o", "        Precomputed covariance, `dictionary' * X`.\r\n"]
[682.696, "o", "\r\n"]
[682.706, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[682.716, "o", "            default='lasso_lars'\r\n"]
[682.726, "o", "        The algorithm used:\r\n"]
[682.736, "o", "\r\n"]
[682.746, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[682.756, "o", "          (`linear_model.lars_path`);\r\n"]
[682.766, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[682.776, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[682.786, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[682.796, "o", "          the estimated components are sparse;\r\n"]
[682.806, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[682.816, "o", "          solution;\r\n"]
[682.826, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[682.836, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[682.846, "o", "\r\n"]
[682.856, "o", "    n_nonzero_coefs : int, default=None\r\n"]
[682.866, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[682.876, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[682.886, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[682.896, "o", "        `n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[682.906, "o", "\r\n"]
[682.916, "o", "    alpha : float, default=None\r\n"]
[682.926, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[682.936, "o", "        penalty applied to the L1 norm.\r\n"]
[682.946, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[682.956, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[682.966, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[682.976, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[682.986, "o", "        `n_nonzero_coefs`.\r\n"]
[682.996, "o", "        If `None`, default to 1.\r\n"]
[683.006, "o", "\r\n"]
[683.016, "o", "    copy_cov : bool, default=True\r\n"]
[683.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[683.036, "o", "        be overwritten.\r\n"]
[683.046, "o", "\r\n"]
[683.056, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[683.066, "o", "        Initialization value of the sparse codes. Only used if\r\n"]
[683.076, "o", "        `algorithm='lasso_cd'`.\r\n"]
[683.086, "o", "\r\n"]
[683.096, "o", "    max_iter : int, default=1000\r\n"]
[683.106, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[683.116, "o", "        `'lasso_lars'`.\r\n"]
[683.126, "o", "\r\n"]
[683.136, "o", "    n_jobs : int, default=None\r\n"]
[683.146, "o", "        Number of parallel jobs to run.\r\n"]
[683.156, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[683.166, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[683.176, "o", "        for more details.\r\n"]
[683.186, "o", "\r\n"]
[683.196, "o", "    check_input : bool, default=True\r\n"]
[683.206, "o", "        If `False`, the input arrays X and dictionary will not be checked.\r\n"]
[683.216, "o", "\r\n"]
[683.226, "o", "    verbose : int, default=0\r\n"]
[683.236, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[683.246, "o", "\r\n"]
[683.256, "o", "    positive : bool, default=False\r\n"]
[683.266, "o", "        Whether to enforce positivity when finding the encoding.\r\n"]
[683.276, "o", "\r\n"]
[683.286, "o", "        .. versionadded:: 0.20\r\n"]
[683.296, "o", "\r\n"]
[683.306, "o", "    Returns\r\n"]
[683.316, "o", "    -------\r\n"]
[683.326, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[683.336, "o", "        The sparse codes.\r\n"]
[683.346, "o", "\r\n"]
[683.356, "o", "    See Also\r\n"]
[683.366, "o", "    --------\r\n"]
[683.376, "o", "    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n"]
[683.386, "o", "        path using LARS algorithm.\r\n"]
[683.396, "o", "    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n"]
[683.406, "o", "    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n"]
[683.416, "o", "    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n"]
[683.426, "o", "        dictionary.\r\n"]
[683.436, "o", "    \"\"\"\r\n"]
[683.446, "o", "    if check_input:\r\n"]
[683.456, "o", "        if algorithm == \"lasso_cd\":\r\n"]
[683.466, "o", "            dictionary = check_array(\r\n"]
[683.476, "o", "                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n"]
[683.486, "o", "            )\r\n"]
[683.496, "o", "            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n"]
[683.506, "o", "        else:\r\n"]
[683.516, "o", "            dictionary = check_array(dictionary)\r\n"]
[683.526, "o", "            X = check_array(X)\r\n"]
[683.536, "o", "\r\n"]
[683.546, "o", "    if dictionary.shape[1] != X.shape[1]:\r\n"]
[683.556, "o", "        raise ValueError(\r\n"]
[683.566, "o", "            \"Dictionary and X have different numbers of features:\"\r\n"]
[683.576, "o", "            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n"]
[683.586, "o", "        )\r\n"]
[683.596, "o", "\r\n"]
[683.606, "o", "    _check_positive_coding(algorithm, positive)\r\n"]
[683.616, "o", "\r\n"]
[683.626, "o", "    return _sparse_encode(\r\n"]
[683.636, "o", "        X,\r\n"]
[683.646, "o", "        dictionary,\r\n"]
[683.656, "o", "        gram=gram,\r\n"]
[683.666, "o", "        cov=cov,\r\n"]
[683.676, "o", "        algorithm=algorithm,\r\n"]
[683.686, "o", "        n_nonzero_coefs=n_nonzero_coefs,\r\n"]
[683.696, "o", "        alpha=alpha,\r\n"]
[683.706, "o", "        copy_cov=copy_cov,\r\n"]
[683.716, "o", "        init=init,\r\n"]
[683.726, "o", "        max_iter=max_iter,\r\n"]
[683.736, "o", "        n_jobs=n_jobs,\r\n"]
[683.746, "o", "        verbose=verbose,\r\n"]
[683.756, "o", "        positive=positive,\r\n"]
[683.766, "o", "    )\r\n"]
[683.776, "o", "\r\n"]
[683.786, "o", "\r\n"]
[683.796, "o", "def _sparse_encode(\r\n"]
[683.806, "o", "    X,\r\n"]
[683.816, "o", "    dictionary,\r\n"]
[683.826, "o", "    *,\r\n"]
[683.836, "o", "    gram=None,\r\n"]
[683.846, "o", "    cov=None,\r\n"]
[683.856, "o", "    algorithm=\"lasso_lars\",\r\n"]
[683.866, "o", "    n_nonzero_coefs=None,\r\n"]
[683.876, "o", "    alpha=None,\r\n"]
[683.886, "o", "    copy_cov=True,\r\n"]
[683.896, "o", "    init=None,\r\n"]
[683.906, "o", "    max_iter=1000,\r\n"]
[683.916, "o", "    n_jobs=None,\r\n"]
[683.926, "o", "    verbose=0,\r\n"]
[683.936, "o", "    positive=False,\r\n"]
[683.946, "o", "):\r\n"]
[683.956, "o", "    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[683.966, "o", "\r\n"]
[683.976, "o", "    n_samples, n_features = X.shape\r\n"]
[683.986, "o", "    n_components = dictionary.shape[0]\r\n"]
[683.996, "o", "\r\n"]
[684.006, "o", "    if algorithm in (\"lars\", \"omp\"):\r\n"]
[684.016, "o", "        regularization = n_nonzero_coefs\r\n"]
[684.026, "o", "        if regularization is None:\r\n"]
[684.036, "o", "            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[684.046, "o", "    else:\r\n"]
[684.056, "o", "        regularization = alpha\r\n"]
[684.066, "o", "        if regularization is None:\r\n"]
[684.076, "o", "            regularization = 1.0\r\n"]
[684.086, "o", "\r\n"]
[684.096, "o", "    if gram is None and algorithm != \"threshold\":\r\n"]
[684.106, "o", "        gram = np.dot(dictionary, dictionary.T)\r\n"]
[684.116, "o", "\r\n"]
[684.126, "o", "    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[684.136, "o", "        copy_cov = False\r\n"]
[684.146, "o", "        cov = np.dot(dictionary, X.T)\r\n"]
[684.156, "o", "\r\n"]
[684.166, "o", "    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[684.176, "o", "        code = _sparse_encode_precomputed(\r\n"]
[684.186, "o", "            X,\r\n"]
[684.196, "o", "            dictionary,\r\n"]
[684.206, "o", "            gram=gram,\r\n"]
[684.216, "o", "            cov=cov,\r\n"]
[684.226, "o", "            algorithm=algorithm,\r\n"]
[684.236, "o", "            regularization=regularization,\r\n"]
[684.246, "o", "            copy_cov=copy_cov,\r\n"]
[684.256, "o", "            init=init,\r\n"]
[684.266, "o", "            max_iter=max_iter,\r\n"]
[684.276, "o", "            verbose=verbose,\r\n"]
[684.286, "o", "            positive=positive,\r\n"]
[684.296, "o", "        )\r\n"]
[684.306, "o", "        return code\r\n"]
[684.316, "o", "\r\n"]
[684.326, "o", "    # Enter parallel code block\r\n"]
[684.336, "o", "    n_samples = X.shape[0]\r\n"]
[684.346, "o", "    n_components = dictionary.shape[0]\r\n"]
[684.356, "o", "    code = np.empty((n_samples, n_components))\r\n"]
[684.366, "o", "    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[684.376, "o", "\r\n"]
[684.386, "o", "    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[684.396, "o", "        delayed(_sparse_encode_precomputed)(\r\n"]
[684.406, "o", "            X[this_slice],\r\n"]
[684.416, "o", "            dictionary,\r\n"]
[684.426, "o", "            gram=gram,\r\n"]
[684.436, "o", "            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[684.446, "o", "            algorithm=algorithm,\r\n"]
[684.456, "o", "            regularization=regularization,\r\n"]
[684.466, "o", "            copy_cov=copy_cov,\r\n"]
[684.476, "o", "            init=init[this_slice] if init is not None else None,\r\n"]
[684.486, "o", "            max_iter=max_iter,\r\n"]
[684.496, "o", "            verbose=verbose,\r\n"]
[684.506, "o", "            positive=positive,\r\n"]
[684.516, "o", "        )\r\n"]
[684.526, "o", "        for this_slice in slices\r\n"]
[684.536, "o", "    )\r\n"]
[684.546, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[684.556, "o", "        code[this_slice] = this_view\r\n"]
[684.566, "o", "    return code\r\n"]
[684.576, "o", "\r\n"]
[684.586, "o", "\r\n"]
[684.596, "o", "def _update_dict(\r\n"]
[684.606, "o", "    dictionary,\r\n"]
[684.616, "o", "    Y,\r\n"]
[684.626, "o", "    code,\r\n"]
[684.636, "o", "    A=None,\r\n"]
[684.646, "o", "    B=None,\r\n"]
[684.656, "o", "    verbose=False,\r\n"]
[684.666, "o", "    random_state=None,\r\n"]
[684.676, "o", "    positive=False,\r\n"]
[684.686, "o", "):\r\n"]
[684.696, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[684.706, "o", "\r\n"]
[684.716, "o", "    Parameters\r\n"]
[684.726, "o", "    ----------\r\n"]
[684.736, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[684.746, "o", "        Value of the dictionary at the previous iteration.\r\n"]
[684.756, "o", "\r\n"]
[684.766, "o", "    Y : ndarray of shape (n_samples, n_features)\r\n"]
[684.776, "o", "        Data matrix.\r\n"]
[684.786, "o", "\r\n"]
[684.796, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[684.806, "o", "        Sparse coding of the data against which to optimize the dictionary.\r\n"]
[684.816, "o", "\r\n"]
[684.826, "o", "    A : ndarray of shape (n_components, n_components), default=None\r\n"]
[684.836, "o", "        Together with `B`, sufficient stats of the online model to update the\r\n"]
[684.846, "o", "        dictionary.\r\n"]
[684.856, "o", "\r\n"]
[684.866, "o", "    B : ndarray of shape (n_features, n_components), default=None\r\n"]
[684.876, "o", "        Together with `A`, sufficient stats of the online model to update the\r\n"]
[684.886, "o", "        dictionary.\r\n"]
[684.896, "o", "\r\n"]
[684.906, "o", "    verbose: bool, default=False\r\n"]
[684.916, "o", "        Degree of output the procedure will print.\r\n"]
[684.926, "o", "\r\n"]
[684.936, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[685.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[685.002, "i", "sed -n '500,1200p' sklearn/decomposition/_dict_learning.py\r"]
[685.004, "o", "sed -n '500,1200p' sklearn/decomposition/_dict_learning.py\r\n"]
[685.016, "o", "\u001b[?2004l\r\n"]
[685.026, "o", "        .. versionadded:: 0.20\r\n"]
[685.036, "o", "    \"\"\"\r\n"]
[685.046, "o", "    n_samples, n_components = code.shape\r\n"]
[685.056, "o", "    random_state = check_random_state(random_state)\r\n"]
[685.066, "o", "\r\n"]
[685.076, "o", "    if A is None:\r\n"]
[685.086, "o", "        A = code.T @ code\r\n"]
[685.096, "o", "    if B is None:\r\n"]
[685.106, "o", "        B = Y.T @ code\r\n"]
[685.116, "o", "\r\n"]
[685.126, "o", "    n_unused = 0\r\n"]
[685.136, "o", "\r\n"]
[685.146, "o", "    for k in range(n_components):\r\n"]
[685.156, "o", "        if A[k, k] > 1e-6:\r\n"]
[685.166, "o", "            # 1e-6 is arbitrary but consistent with the spams implementation\r\n"]
[685.176, "o", "            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n"]
[685.186, "o", "        else:\r\n"]
[685.196, "o", "            # kth atom is almost never used -> sample a new one from the data\r\n"]
[685.206, "o", "            newd = Y[random_state.choice(n_samples)]\r\n"]
[685.216, "o", "\r\n"]
[685.226, "o", "            # add small noise to avoid making the sparse coding ill conditioned\r\n"]
[685.236, "o", "            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n"]
[685.246, "o", "            noise = random_state.normal(0, noise_level, size=len(newd))\r\n"]
[685.256, "o", "\r\n"]
[685.266, "o", "            dictionary[k] = newd + noise\r\n"]
[685.276, "o", "            code[:, k] = 0\r\n"]
[685.286, "o", "            n_unused += 1\r\n"]
[685.296, "o", "\r\n"]
[685.306, "o", "        if positive:\r\n"]
[685.316, "o", "            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n"]
[685.326, "o", "\r\n"]
[685.336, "o", "        # Projection on the constraint set ||V_k|| <= 1\r\n"]
[685.346, "o", "        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n"]
[685.356, "o", "\r\n"]
[685.366, "o", "    if verbose and n_unused > 0:\r\n"]
[685.376, "o", "        print(f\"{n_unused} unused atoms resampled.\")\r\n"]
[685.386, "o", "\r\n"]
[685.396, "o", "\r\n"]
[685.406, "o", "def _dict_learning(\r\n"]
[685.416, "o", "    X,\r\n"]
[685.426, "o", "    n_components,\r\n"]
[685.436, "o", "    *,\r\n"]
[685.446, "o", "    alpha,\r\n"]
[685.456, "o", "    max_iter,\r\n"]
[685.466, "o", "    tol,\r\n"]
[685.476, "o", "    method,\r\n"]
[685.486, "o", "    n_jobs,\r\n"]
[685.496, "o", "    dict_init,\r\n"]
[685.506, "o", "    code_init,\r\n"]
[685.516, "o", "    callback,\r\n"]
[685.526, "o", "    verbose,\r\n"]
[685.536, "o", "    random_state,\r\n"]
[685.546, "o", "    return_n_iter,\r\n"]
[685.556, "o", "    positive_dict,\r\n"]
[685.566, "o", "    positive_code,\r\n"]
[685.576, "o", "    method_max_iter,\r\n"]
[685.586, "o", "):\r\n"]
[685.596, "o", "    \"\"\"Main dictionary learning algorithm\"\"\"\r\n"]
[685.606, "o", "    t0 = time.time()\r\n"]
[685.616, "o", "    # Init the code and the dictionary with SVD of Y\r\n"]
[685.626, "o", "    if code_init is not None and dict_init is not None:\r\n"]
[685.636, "o", "        code = np.array(code_init, order=\"F\")\r\n"]
[685.646, "o", "        # Don't copy V, it will happen below\r\n"]
[685.656, "o", "        dictionary = dict_init\r\n"]
[685.666, "o", "    else:\r\n"]
[685.676, "o", "        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n"]
[685.686, "o", "        # flip the initial code's sign to enforce deterministic output\r\n"]
[685.696, "o", "        code, dictionary = svd_flip(code, dictionary)\r\n"]
[685.706, "o", "        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[685.716, "o", "    r = len(dictionary)\r\n"]
[685.726, "o", "    if n_components <= r:  # True even if n_components=None\r\n"]
[685.736, "o", "        code = code[:, :n_components]\r\n"]
[685.746, "o", "        dictionary = dictionary[:n_components, :]\r\n"]
[685.756, "o", "    else:\r\n"]
[685.766, "o", "        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n"]
[685.776, "o", "        dictionary = np.r_[\r\n"]
[685.786, "o", "            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n"]
[685.796, "o", "        ]\r\n"]
[685.806, "o", "\r\n"]
[685.816, "o", "    # Fortran-order dict better suited for the sparse coding which is the\r\n"]
[685.826, "o", "    # bottleneck of this algorithm.\r\n"]
[685.836, "o", "    dictionary = np.asfortranarray(dictionary)\r\n"]
[685.846, "o", "\r\n"]
[685.856, "o", "    errors = []\r\n"]
[685.866, "o", "    current_cost = np.nan\r\n"]
[685.876, "o", "\r\n"]
[685.886, "o", "    if verbose == 1:\r\n"]
[685.896, "o", "        print(\"[dict_learning]\", end=\" \")\r\n"]
[685.906, "o", "\r\n"]
[685.916, "o", "    # If max_iter is 0, number of iterations returned should be zero\r\n"]
[685.926, "o", "    ii = -1\r\n"]
[685.936, "o", "\r\n"]
[685.946, "o", "    for ii in range(max_iter):\r\n"]
[685.956, "o", "        dt = time.time() - t0\r\n"]
[685.966, "o", "        if verbose == 1:\r\n"]
[685.976, "o", "            sys.stdout.write(\".\")\r\n"]
[685.986, "o", "            sys.stdout.flush()\r\n"]
[685.996, "o", "        elif verbose:\r\n"]
[686.006, "o", "            print(\r\n"]
[686.016, "o", "                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n"]
[686.026, "o", "                % (ii, dt, dt / 60, current_cost)\r\n"]
[686.036, "o", "            )\r\n"]
[686.046, "o", "\r\n"]
[686.056, "o", "        # Update code\r\n"]
[686.066, "o", "        code = sparse_encode(\r\n"]
[686.076, "o", "            X,\r\n"]
[686.086, "o", "            dictionary,\r\n"]
[686.096, "o", "            algorithm=method,\r\n"]
[686.106, "o", "            alpha=alpha,\r\n"]
[686.116, "o", "            init=code,\r\n"]
[686.126, "o", "            n_jobs=n_jobs,\r\n"]
[686.136, "o", "            positive=positive_code,\r\n"]
[686.146, "o", "            max_iter=method_max_iter,\r\n"]
[686.156, "o", "            verbose=verbose,\r\n"]
[686.166, "o", "        )\r\n"]
[686.176, "o", "\r\n"]
[686.186, "o", "        # Update dictionary in place\r\n"]
[686.196, "o", "        _update_dict(\r\n"]
[686.206, "o", "            dictionary,\r\n"]
[686.216, "o", "            X,\r\n"]
[686.226, "o", "            code,\r\n"]
[686.236, "o", "            verbose=verbose,\r\n"]
[686.246, "o", "            random_state=random_state,\r\n"]
[686.256, "o", "            positive=positive_dict,\r\n"]
[686.266, "o", "        )\r\n"]
[686.276, "o", "\r\n"]
[686.286, "o", "        # Cost function\r\n"]
[686.296, "o", "        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n"]
[686.306, "o", "            np.abs(code)\r\n"]
[686.316, "o", "        )\r\n"]
[686.326, "o", "        errors.append(current_cost)\r\n"]
[686.336, "o", "\r\n"]
[686.346, "o", "        if ii > 0:\r\n"]
[686.356, "o", "            dE = errors[-2] - errors[-1]\r\n"]
[686.366, "o", "            # assert(dE >= -tol * errors[-1])\r\n"]
[686.376, "o", "            if dE < tol * errors[-1]:\r\n"]
[686.386, "o", "                if verbose == 1:\r\n"]
[686.396, "o", "                    # A line return\r\n"]
[686.406, "o", "                    print(\"\")\r\n"]
[686.416, "o", "                elif verbose:\r\n"]
[686.426, "o", "                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n"]
[686.436, "o", "                break\r\n"]
[686.446, "o", "        if ii % 5 == 0 and callback is not None:\r\n"]
[686.456, "o", "            callback(locals())\r\n"]
[686.466, "o", "\r\n"]
[686.476, "o", "    if return_n_iter:\r\n"]
[686.486, "o", "        return code, dictionary, errors, ii + 1\r\n"]
[686.496, "o", "    else:\r\n"]
[686.506, "o", "        return code, dictionary, errors\r\n"]
[686.516, "o", "\r\n"]
[686.526, "o", "\r\n"]
[686.536, "o", "def _check_warn_deprecated(param, name, default, additional_message=None):\r\n"]
[686.546, "o", "    if param != \"deprecated\":\r\n"]
[686.556, "o", "        msg = (\r\n"]
[686.566, "o", "            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n"]
[686.576, "o", "        )\r\n"]
[686.586, "o", "        if additional_message:\r\n"]
[686.596, "o", "            msg += f\" {additional_message}\"\r\n"]
[686.606, "o", "        warnings.warn(msg, FutureWarning)\r\n"]
[686.616, "o", "        return param\r\n"]
[686.626, "o", "    else:\r\n"]
[686.636, "o", "        return default\r\n"]
[686.646, "o", "\r\n"]
[686.656, "o", "\r\n"]
[686.666, "o", "def dict_learning_online(\r\n"]
[686.676, "o", "    X,\r\n"]
[686.686, "o", "    n_components=2,\r\n"]
[686.696, "o", "    *,\r\n"]
[686.706, "o", "    alpha=1,\r\n"]
[686.716, "o", "    n_iter=\"deprecated\",\r\n"]
[686.726, "o", "    max_iter=None,\r\n"]
[686.736, "o", "    return_code=True,\r\n"]
[686.746, "o", "    dict_init=None,\r\n"]
[686.756, "o", "    callback=None,\r\n"]
[686.766, "o", "    batch_size=256,\r\n"]
[686.776, "o", "    verbose=False,\r\n"]
[686.786, "o", "    shuffle=True,\r\n"]
[686.796, "o", "    n_jobs=None,\r\n"]
[686.806, "o", "    method=\"lars\",\r\n"]
[686.816, "o", "    iter_offset=\"deprecated\",\r\n"]
[686.826, "o", "    random_state=None,\r\n"]
[686.836, "o", "    return_inner_stats=\"deprecated\",\r\n"]
[686.846, "o", "    inner_stats=\"deprecated\",\r\n"]
[686.856, "o", "    return_n_iter=\"deprecated\",\r\n"]
[686.866, "o", "    positive_dict=False,\r\n"]
[686.876, "o", "    positive_code=False,\r\n"]
[686.886, "o", "    method_max_iter=1000,\r\n"]
[686.896, "o", "    tol=1e-3,\r\n"]
[686.906, "o", "    max_no_improvement=10,\r\n"]
[686.916, "o", "):\r\n"]
[686.926, "o", "    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n"]
[686.936, "o", "\r\n"]
[686.946, "o", "    Finds the best dictionary and the corresponding sparse code for\r\n"]
[686.956, "o", "    approximating the data matrix X by solving::\r\n"]
[686.966, "o", "\r\n"]
[686.976, "o", "        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[686.986, "o", "                     (U,V)\r\n"]
[686.996, "o", "                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n"]
[687.006, "o", "\r\n"]
[687.016, "o", "    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n"]
[687.026, "o", "    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n"]
[687.036, "o", "    which is the sum of the absolute values of all the entries in the matrix.\r\n"]
[687.046, "o", "    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n"]
[687.056, "o", "    the input data.\r\n"]
[687.066, "o", "\r\n"]
[687.076, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[687.086, "o", "\r\n"]
[687.096, "o", "    Parameters\r\n"]
[687.106, "o", "    ----------\r\n"]
[687.116, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[687.126, "o", "        Data matrix.\r\n"]
[687.136, "o", "\r\n"]
[687.146, "o", "    n_components : int or None, default=2\r\n"]
[687.156, "o", "        Number of dictionary atoms to extract. If None, then ``n_components``\r\n"]
[687.166, "o", "        is set to ``n_features``.\r\n"]
[687.176, "o", "\r\n"]
[687.186, "o", "    alpha : float, default=1\r\n"]
[687.196, "o", "        Sparsity controlling parameter.\r\n"]
[687.206, "o", "\r\n"]
[687.216, "o", "    n_iter : int, default=100\r\n"]
[687.226, "o", "        Number of mini-batch iterations to perform.\r\n"]
[687.236, "o", "\r\n"]
[687.246, "o", "        .. deprecated:: 1.1\r\n"]
[687.256, "o", "           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[687.266, "o", "           `max_iter` instead.\r\n"]
[687.276, "o", "\r\n"]
[687.286, "o", "    max_iter : int, default=None\r\n"]
[687.296, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[687.306, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[687.316, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[687.326, "o", "\r\n"]
[687.336, "o", "        .. versionadded:: 1.1\r\n"]
[687.346, "o", "\r\n"]
[687.356, "o", "    return_code : bool, default=True\r\n"]
[687.366, "o", "        Whether to also return the code U or just the dictionary `V`.\r\n"]
[687.376, "o", "\r\n"]
[687.386, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[687.396, "o", "        Initial values for the dictionary for warm restart scenarios.\r\n"]
[687.406, "o", "        If `None`, the initial values for the dictionary are created\r\n"]
[687.416, "o", "        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n"]
[687.426, "o", "\r\n"]
[687.436, "o", "    callback : callable, default=None\r\n"]
[687.446, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[687.456, "o", "\r\n"]
[687.466, "o", "    batch_size : int, default=256\r\n"]
[687.476, "o", "        The number of samples to take in each batch.\r\n"]
[687.486, "o", "\r\n"]
[687.496, "o", "        .. versionchanged:: 1.3\r\n"]
[687.506, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[687.516, "o", "\r\n"]
[687.526, "o", "    verbose : bool, default=False\r\n"]
[687.536, "o", "        To control the verbosity of the procedure.\r\n"]
[687.546, "o", "\r\n"]
[687.556, "o", "    shuffle : bool, default=True\r\n"]
[687.566, "o", "        Whether to shuffle the data before splitting it in batches.\r\n"]
[687.576, "o", "\r\n"]
[687.586, "o", "    n_jobs : int, default=None\r\n"]
[687.596, "o", "        Number of parallel jobs to run.\r\n"]
[687.606, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[687.616, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[687.626, "o", "        for more details.\r\n"]
[687.636, "o", "\r\n"]
[687.646, "o", "    method : {'lars', 'cd'}, default='lars'\r\n"]
[687.656, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[687.666, "o", "          problem (`linear_model.lars_path`);\r\n"]
[687.676, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[687.686, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[687.696, "o", "          the estimated components are sparse.\r\n"]
[687.706, "o", "\r\n"]
[687.716, "o", "    iter_offset : int, default=0\r\n"]
[687.726, "o", "        Number of previous iterations completed on the dictionary used for\r\n"]
[687.736, "o", "        initialization.\r\n"]
[687.746, "o", "\r\n"]
[687.756, "o", "        .. deprecated:: 1.1\r\n"]
[687.766, "o", "           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n"]
[687.776, "o", "\r\n"]
[687.786, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[687.796, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[687.806, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[687.816, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[687.826, "o", "        results across multiple function calls.\r\n"]
[687.836, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[687.846, "o", "\r\n"]
[687.856, "o", "    return_inner_stats : bool, default=False\r\n"]
[687.866, "o", "        Return the inner statistics A (dictionary covariance) and B\r\n"]
[687.876, "o", "        (data approximation). Useful to restart the algorithm in an\r\n"]
[687.886, "o", "        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n"]
[687.896, "o", "        ignored.\r\n"]
[687.906, "o", "\r\n"]
[687.916, "o", "        .. deprecated:: 1.1\r\n"]
[687.926, "o", "           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[687.936, "o", "\r\n"]
[687.946, "o", "    inner_stats : tuple of (A, B) ndarrays, default=None\r\n"]
[687.956, "o", "        Inner sufficient statistics that are kept by the algorithm.\r\n"]
[687.966, "o", "        Passing them at initialization is useful in online settings, to\r\n"]
[687.976, "o", "        avoid losing the history of the evolution.\r\n"]
[687.986, "o", "        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n"]
[687.996, "o", "        `B` `(n_features, n_components)` is the data approximation matrix.\r\n"]
[688.006, "o", "\r\n"]
[688.016, "o", "        .. deprecated:: 1.1\r\n"]
[688.026, "o", "           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[688.036, "o", "\r\n"]
[688.046, "o", "    return_n_iter : bool, default=False\r\n"]
[688.056, "o", "        Whether or not to return the number of iterations.\r\n"]
[688.066, "o", "\r\n"]
[688.076, "o", "        .. deprecated:: 1.1\r\n"]
[688.086, "o", "           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n"]
[688.096, "o", "\r\n"]
[688.106, "o", "    positive_dict : bool, default=False\r\n"]
[688.116, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[688.126, "o", "\r\n"]
[688.136, "o", "        .. versionadded:: 0.20\r\n"]
[688.146, "o", "\r\n"]
[688.156, "o", "    positive_code : bool, default=False\r\n"]
[688.166, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[688.176, "o", "\r\n"]
[688.186, "o", "        .. versionadded:: 0.20\r\n"]
[688.196, "o", "\r\n"]
[688.206, "o", "    method_max_iter : int, default=1000\r\n"]
[688.216, "o", "        Maximum number of iterations to perform when solving the lasso problem.\r\n"]
[688.226, "o", "\r\n"]
[688.236, "o", "        .. versionadded:: 0.22\r\n"]
[688.246, "o", "\r\n"]
[688.256, "o", "    tol : float, default=1e-3\r\n"]
[688.266, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[688.276, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[688.286, "o", "\r\n"]
[688.296, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[688.306, "o", "        `tol` to 0.0.\r\n"]
[688.316, "o", "\r\n"]
[688.326, "o", "        .. versionadded:: 1.1\r\n"]
[688.336, "o", "\r\n"]
[688.346, "o", "    max_no_improvement : int, default=10\r\n"]
[688.356, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[688.366, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[688.376, "o", "        `max_iter` is not None.\r\n"]
[688.386, "o", "\r\n"]
[688.396, "o", "        To disable convergence detection based on cost function, set\r\n"]
[688.406, "o", "        `max_no_improvement` to None.\r\n"]
[688.416, "o", "\r\n"]
[688.426, "o", "        .. versionadded:: 1.1\r\n"]
[688.436, "o", "\r\n"]
[688.446, "o", "    Returns\r\n"]
[688.456, "o", "    -------\r\n"]
[688.466, "o", "    code : ndarray of shape (n_samples, n_components),\r\n"]
[688.476, "o", "        The sparse code (only returned if `return_code=True`).\r\n"]
[688.486, "o", "\r\n"]
[688.496, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[688.506, "o", "        The solutions to the dictionary learning problem.\r\n"]
[688.516, "o", "\r\n"]
[688.526, "o", "    n_iter : int\r\n"]
[688.536, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[688.546, "o", "        set to `True`.\r\n"]
[688.556, "o", "\r\n"]
[688.566, "o", "    See Also\r\n"]
[688.576, "o", "    --------\r\n"]
[688.586, "o", "    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n"]
[688.596, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[688.606, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n"]
[688.616, "o", "        learning algorithm.\r\n"]
[688.626, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[688.636, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[688.646, "o", "    \"\"\"\r\n"]
[688.656, "o", "    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n"]
[688.666, "o", "    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n"]
[688.676, "o", "        raise ValueError(\r\n"]
[688.686, "o", "            \"The following arguments are incompatible with 'max_iter': \"\r\n"]
[688.696, "o", "            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n"]
[688.706, "o", "        )\r\n"]
[688.716, "o", "\r\n"]
[688.726, "o", "    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n"]
[688.736, "o", "    return_inner_stats = _check_warn_deprecated(\r\n"]
[688.746, "o", "        return_inner_stats,\r\n"]
[688.756, "o", "        \"return_inner_stats\",\r\n"]
[688.766, "o", "        default=False,\r\n"]
[688.776, "o", "        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n"]
[688.786, "o", "    )\r\n"]
[688.796, "o", "    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n"]
[688.806, "o", "    return_n_iter = _check_warn_deprecated(\r\n"]
[688.816, "o", "        return_n_iter,\r\n"]
[688.826, "o", "        \"return_n_iter\",\r\n"]
[688.836, "o", "        default=False,\r\n"]
[688.846, "o", "        additional_message=(\r\n"]
[688.856, "o", "            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n"]
[688.866, "o", "            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n"]
[688.876, "o", "        ),\r\n"]
[688.886, "o", "    )\r\n"]
[688.896, "o", "\r\n"]
[688.906, "o", "    if max_iter is not None:\r\n"]
[688.916, "o", "        transform_algorithm = \"lasso_\" + method\r\n"]
[688.926, "o", "\r\n"]
[688.936, "o", "        est = MiniBatchDictionaryLearning(\r\n"]
[688.946, "o", "            n_components=n_components,\r\n"]
[688.956, "o", "            alpha=alpha,\r\n"]
[688.966, "o", "            n_iter=n_iter,\r\n"]
[688.976, "o", "            n_jobs=n_jobs,\r\n"]
[688.986, "o", "            fit_algorithm=method,\r\n"]
[688.996, "o", "            batch_size=batch_size,\r\n"]
[689.006, "o", "            shuffle=shuffle,\r\n"]
[689.016, "o", "            dict_init=dict_init,\r\n"]
[689.026, "o", "            random_state=random_state,\r\n"]
[689.036, "o", "            transform_algorithm=transform_algorithm,\r\n"]
[689.046, "o", "            transform_alpha=alpha,\r\n"]
[689.056, "o", "            positive_code=positive_code,\r\n"]
[689.066, "o", "            positive_dict=positive_dict,\r\n"]
[689.076, "o", "            transform_max_iter=method_max_iter,\r\n"]
[689.086, "o", "            verbose=verbose,\r\n"]
[689.096, "o", "            callback=callback,\r\n"]
[689.106, "o", "            tol=tol,\r\n"]
[689.116, "o", "            max_no_improvement=max_no_improvement,\r\n"]
[689.126, "o", "        ).fit(X)\r\n"]
[689.136, "o", "\r\n"]
[689.146, "o", "        if not return_code:\r\n"]
[689.156, "o", "            return est.components_\r\n"]
[689.166, "o", "        else:\r\n"]
[689.176, "o", "            code = est.transform(X)\r\n"]
[689.186, "o", "            return code, est.components_\r\n"]
[689.196, "o", "\r\n"]
[689.206, "o", "    # TODO(1.4) remove the whole old behavior\r\n"]
[689.216, "o", "    # Fallback to old behavior\r\n"]
[689.226, "o", "\r\n"]
[689.236, "o", "    n_iter = _check_warn_deprecated(\r\n"]
[689.246, "o", "        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n"]
[689.256, "o", "    )\r\n"]
[689.266, "o", "\r\n"]
[689.276, "o", "    if n_components is None:\r\n"]
[689.286, "o", "        n_components = X.shape[1]\r\n"]
[689.296, "o", "\r\n"]
[689.306, "o", "    if method not in (\"lars\", \"cd\"):\r\n"]
[689.316, "o", "        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n"]
[689.326, "o", "\r\n"]
[689.336, "o", "    _check_positive_coding(method, positive_code)\r\n"]
[689.346, "o", "\r\n"]
[689.356, "o", "    method = \"lasso_\" + method\r\n"]
[689.366, "o", "\r\n"]
[689.376, "o", "    t0 = time.time()\r\n"]
[689.386, "o", "    n_samples, n_features = X.shape\r\n"]
[689.396, "o", "    # Avoid integer division problems\r\n"]
[689.406, "o", "    alpha = float(alpha)\r\n"]
[689.416, "o", "    random_state = check_random_state(random_state)\r\n"]
[689.426, "o", "\r\n"]
[689.436, "o", "    # Init V with SVD of X\r\n"]
[689.446, "o", "    if dict_init is not None:\r\n"]
[689.456, "o", "        dictionary = dict_init\r\n"]
[689.466, "o", "    else:\r\n"]
[689.476, "o", "        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n"]
[689.486, "o", "        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[689.496, "o", "    r = len(dictionary)\r\n"]
[689.506, "o", "    if n_components <= r:\r\n"]
[689.516, "o", "        dictionary = dictionary[:n_components, :]\r\n"]
[689.526, "o", "    else:\r\n"]
[689.536, "o", "        dictionary = np.r_[\r\n"]
[689.546, "o", "            dictionary,\r\n"]
[689.556, "o", "            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n"]
[689.566, "o", "        ]\r\n"]
[689.576, "o", "\r\n"]
[689.586, "o", "    if verbose == 1:\r\n"]
[689.596, "o", "        print(\"[dict_learning]\", end=\" \")\r\n"]
[689.606, "o", "\r\n"]
[689.616, "o", "    if shuffle:\r\n"]
[689.626, "o", "        X_train = X.copy()\r\n"]
[689.636, "o", "        random_state.shuffle(X_train)\r\n"]
[689.646, "o", "    else:\r\n"]
[689.656, "o", "        X_train = X\r\n"]
[689.666, "o", "\r\n"]
[689.676, "o", "    X_train = check_array(\r\n"]
[689.686, "o", "        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n"]
[689.696, "o", "    )\r\n"]
[689.706, "o", "\r\n"]
[689.716, "o", "    # Fortran-order dict better suited for the sparse coding which is the\r\n"]
[689.726, "o", "    # bottleneck of this algorithm.\r\n"]
[689.736, "o", "    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n"]
[689.746, "o", "    dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[689.756, "o", "\r\n"]
[689.766, "o", "    batches = gen_batches(n_samples, batch_size)\r\n"]
[689.776, "o", "    batches = itertools.cycle(batches)\r\n"]
[689.786, "o", "\r\n"]
[689.796, "o", "    # The covariance of the dictionary\r\n"]
[689.806, "o", "    if inner_stats is None:\r\n"]
[689.816, "o", "        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n"]
[689.826, "o", "        # The data approximation\r\n"]
[689.836, "o", "        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n"]
[689.846, "o", "    else:\r\n"]
[689.856, "o", "        A = inner_stats[0].copy()\r\n"]
[689.866, "o", "        B = inner_stats[1].copy()\r\n"]
[689.876, "o", "\r\n"]
[689.886, "o", "    # If n_iter is zero, we need to return zero.\r\n"]
[689.896, "o", "    ii = iter_offset - 1\r\n"]
[689.906, "o", "\r\n"]
[689.916, "o", "    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n"]
[689.926, "o", "        this_X = X_train[batch]\r\n"]
[689.936, "o", "        dt = time.time() - t0\r\n"]
[690.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[690.002, "i", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r"]
[690.004, "o", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r\n"]
[690.016, "o", "\u001b[?2004l\r\n"]
[690.026, "o", "    n_iter : int\r\n"]
[690.036, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[690.046, "o", "        set to True.\r\n"]
[690.056, "o", "\r\n"]
[690.066, "o", "    See Also\r\n"]
[690.076, "o", "    --------\r\n"]
[690.086, "o", "    dict_learning_online : Solve a dictionary learning matrix factorization\r\n"]
[690.096, "o", "        problem online.\r\n"]
[690.106, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[690.116, "o", "    MiniBatchDictionaryLearning : A faster, less accurate version\r\n"]
[690.126, "o", "        of the dictionary learning algorithm.\r\n"]
[690.136, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[690.146, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[690.156, "o", "    \"\"\"\r\n"]
[690.166, "o", "    estimator = DictionaryLearning(\r\n"]
[690.176, "o", "        n_components=n_components,\r\n"]
[690.186, "o", "        alpha=alpha,\r\n"]
[690.196, "o", "        max_iter=max_iter,\r\n"]
[690.206, "o", "        tol=tol,\r\n"]
[690.216, "o", "        fit_algorithm=method,\r\n"]
[690.226, "o", "        n_jobs=n_jobs,\r\n"]
[690.236, "o", "        dict_init=dict_init,\r\n"]
[690.246, "o", "        callback=callback,\r\n"]
[690.256, "o", "        code_init=code_init,\r\n"]
[690.266, "o", "        verbose=verbose,\r\n"]
[690.276, "o", "        random_state=random_state,\r\n"]
[690.286, "o", "        positive_code=positive_code,\r\n"]
[690.296, "o", "        positive_dict=positive_dict,\r\n"]
[690.306, "o", "        transform_max_iter=method_max_iter,\r\n"]
[690.316, "o", "    )\r\n"]
[690.326, "o", "    code = estimator.fit_transform(X)\r\n"]
[690.336, "o", "    if return_n_iter:\r\n"]
[690.346, "o", "        return (\r\n"]
[690.356, "o", "            code,\r\n"]
[690.366, "o", "            estimator.components_,\r\n"]
[690.376, "o", "            estimator.error_,\r\n"]
[690.386, "o", "            estimator.n_iter_,\r\n"]
[690.396, "o", "        )\r\n"]
[690.406, "o", "    return code, estimator.components_, estimator.error_\r\n"]
[690.416, "o", "\r\n"]
[690.426, "o", "\r\n"]
[690.436, "o", "class _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n"]
[690.446, "o", "    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n"]
[690.456, "o", "\r\n"]
[690.466, "o", "    def __init__(\r\n"]
[690.476, "o", "        self,\r\n"]
[690.486, "o", "        transform_algorithm,\r\n"]
[690.496, "o", "        transform_n_nonzero_coefs,\r\n"]
[690.506, "o", "        transform_alpha,\r\n"]
[690.516, "o", "        split_sign,\r\n"]
[690.526, "o", "        n_jobs,\r\n"]
[690.536, "o", "        positive_code,\r\n"]
[690.546, "o", "        transform_max_iter,\r\n"]
[690.556, "o", "    ):\r\n"]
[690.566, "o", "        self.transform_algorithm = transform_algorithm\r\n"]
[690.576, "o", "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n"]
[690.586, "o", "        self.transform_alpha = transform_alpha\r\n"]
[690.596, "o", "        self.transform_max_iter = transform_max_iter\r\n"]
[690.606, "o", "        self.split_sign = split_sign\r\n"]
[690.616, "o", "        self.n_jobs = n_jobs\r\n"]
[690.626, "o", "        self.positive_code = positive_code\r\n"]
[690.636, "o", "\r\n"]
[690.646, "o", "    def _transform(self, X, dictionary):\r\n"]
[690.656, "o", "        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n"]
[690.666, "o", "        SparseCoder.\"\"\"\r\n"]
[690.676, "o", "        X = self._validate_data(X, reset=False)\r\n"]
[690.686, "o", "\r\n"]
[690.696, "o", "        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n"]
[690.706, "o", "            transform_alpha = self.alpha\r\n"]
[690.716, "o", "        else:\r\n"]
[690.726, "o", "            transform_alpha = self.transform_alpha\r\n"]
[690.736, "o", "\r\n"]
[690.746, "o", "        code = sparse_encode(\r\n"]
[690.756, "o", "            X,\r\n"]
[690.766, "o", "            dictionary,\r\n"]
[690.776, "o", "            algorithm=self.transform_algorithm,\r\n"]
[690.786, "o", "            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n"]
[690.796, "o", "            alpha=transform_alpha,\r\n"]
[690.806, "o", "            max_iter=self.transform_max_iter,\r\n"]
[690.816, "o", "            n_jobs=self.n_jobs,\r\n"]
[690.826, "o", "            positive=self.positive_code,\r\n"]
[690.836, "o", "        )\r\n"]
[690.846, "o", "\r\n"]
[690.856, "o", "        if self.split_sign:\r\n"]
[690.866, "o", "            # feature vector is split into a positive and negative side\r\n"]
[690.876, "o", "            n_samples, n_features = code.shape\r\n"]
[690.886, "o", "            split_code = np.empty((n_samples, 2 * n_features))\r\n"]
[690.896, "o", "            split_code[:, :n_features] = np.maximum(code, 0)\r\n"]
[690.906, "o", "            split_code[:, n_features:] = -np.minimum(code, 0)\r\n"]
[690.916, "o", "            code = split_code\r\n"]
[690.926, "o", "\r\n"]
[690.936, "o", "        return code\r\n"]
[690.946, "o", "\r\n"]
[690.956, "o", "    def transform(self, X):\r\n"]
[690.966, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[690.976, "o", "\r\n"]
[690.986, "o", "        Coding method is determined by the object parameter\r\n"]
[690.996, "o", "        `transform_algorithm`.\r\n"]
[691.006, "o", "\r\n"]
[691.016, "o", "        Parameters\r\n"]
[691.026, "o", "        ----------\r\n"]
[691.036, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[691.046, "o", "            Test data to be transformed, must have the same number of\r\n"]
[691.056, "o", "            features as the data used to train the model.\r\n"]
[691.066, "o", "\r\n"]
[691.076, "o", "        Returns\r\n"]
[691.086, "o", "        -------\r\n"]
[691.096, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[691.106, "o", "            Transformed data.\r\n"]
[691.116, "o", "        \"\"\"\r\n"]
[691.126, "o", "        check_is_fitted(self)\r\n"]
[691.136, "o", "        return self._transform(X, self.components_)\r\n"]
[691.146, "o", "\r\n"]
[691.156, "o", "\r\n"]
[691.166, "o", "class SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n"]
[691.176, "o", "    \"\"\"Sparse coding.\r\n"]
[691.186, "o", "\r\n"]
[691.196, "o", "    Finds a sparse representation of data against a fixed, precomputed\r\n"]
[691.206, "o", "    dictionary.\r\n"]
[691.216, "o", "\r\n"]
[691.226, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[691.236, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[691.246, "o", "\r\n"]
[691.256, "o", "        X ~= code * dictionary\r\n"]
[691.266, "o", "\r\n"]
[691.276, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[691.286, "o", "\r\n"]
[691.296, "o", "    Parameters\r\n"]
[691.306, "o", "    ----------\r\n"]
[691.316, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[691.326, "o", "        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n"]
[691.336, "o", "        normalized to unit norm.\r\n"]
[691.346, "o", "\r\n"]
[691.356, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[691.366, "o", "            'threshold'}, default='omp'\r\n"]
[691.376, "o", "        Algorithm used to transform the data:\r\n"]
[691.386, "o", "\r\n"]
[691.396, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[691.406, "o", "          (`linear_model.lars_path`);\r\n"]
[691.416, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[691.426, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[691.436, "o", "          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n"]
[691.446, "o", "          the estimated components are sparse;\r\n"]
[691.456, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[691.466, "o", "          solution;\r\n"]
[691.476, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[691.486, "o", "          the projection ``dictionary * X'``.\r\n"]
[691.496, "o", "\r\n"]
[691.506, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[691.516, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[691.526, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[691.536, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[691.546, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[691.556, "o", "\r\n"]
[691.566, "o", "    transform_alpha : float, default=None\r\n"]
[691.576, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[691.586, "o", "        penalty applied to the L1 norm.\r\n"]
[691.596, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[691.606, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[691.616, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[691.626, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[691.636, "o", "        `n_nonzero_coefs`.\r\n"]
[691.646, "o", "        If `None`, default to 1.\r\n"]
[691.656, "o", "\r\n"]
[691.666, "o", "    split_sign : bool, default=False\r\n"]
[691.676, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[691.686, "o", "        its negative part and its positive part. This can improve the\r\n"]
[691.696, "o", "        performance of downstream classifiers.\r\n"]
[691.706, "o", "\r\n"]
[691.716, "o", "    n_jobs : int, default=None\r\n"]
[691.726, "o", "        Number of parallel jobs to run.\r\n"]
[691.736, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[691.746, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[691.756, "o", "        for more details.\r\n"]
[691.766, "o", "\r\n"]
[691.776, "o", "    positive_code : bool, default=False\r\n"]
[691.786, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[691.796, "o", "\r\n"]
[691.806, "o", "        .. versionadded:: 0.20\r\n"]
[691.816, "o", "\r\n"]
[691.826, "o", "    transform_max_iter : int, default=1000\r\n"]
[691.836, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[691.846, "o", "        `lasso_lars`.\r\n"]
[691.856, "o", "\r\n"]
[691.866, "o", "        .. versionadded:: 0.22\r\n"]
[691.876, "o", "\r\n"]
[691.886, "o", "    Attributes\r\n"]
[691.896, "o", "    ----------\r\n"]
[691.906, "o", "    n_components_ : int\r\n"]
[691.916, "o", "        Number of atoms.\r\n"]
[691.926, "o", "\r\n"]
[691.936, "o", "    n_features_in_ : int\r\n"]
[691.946, "o", "        Number of features seen during :term:`fit`.\r\n"]
[691.956, "o", "\r\n"]
[691.966, "o", "        .. versionadded:: 0.24\r\n"]
[691.976, "o", "\r\n"]
[691.986, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[691.996, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[692.006, "o", "        has feature names that are all strings.\r\n"]
[692.016, "o", "\r\n"]
[692.026, "o", "        .. versionadded:: 1.0\r\n"]
[692.036, "o", "\r\n"]
[692.046, "o", "    See Also\r\n"]
[692.056, "o", "    --------\r\n"]
[692.066, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[692.076, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n"]
[692.086, "o", "        dictionary learning algorithm.\r\n"]
[692.096, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[692.106, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[692.116, "o", "    sparse_encode : Sparse coding where each row of the result is the solution\r\n"]
[692.126, "o", "        to a sparse coding problem.\r\n"]
[692.136, "o", "\r\n"]
[692.146, "o", "    Examples\r\n"]
[692.156, "o", "    --------\r\n"]
[692.166, "o", "    >>> import numpy as np\r\n"]
[692.176, "o", "    >>> from sklearn.decomposition import SparseCoder\r\n"]
[692.186, "o", "    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n"]
[692.196, "o", "    >>> dictionary = np.array(\r\n"]
[692.206, "o", "    ...     [[0, 1, 0],\r\n"]
[692.216, "o", "    ...      [-1, -1, 2],\r\n"]
[692.226, "o", "    ...      [1, 1, 1],\r\n"]
[692.236, "o", "    ...      [0, 1, 1],\r\n"]
[692.246, "o", "    ...      [0, 2, 1]],\r\n"]
[692.256, "o", "    ...    dtype=np.float64\r\n"]
[692.266, "o", "    ... )\r\n"]
[692.276, "o", "    >>> coder = SparseCoder(\r\n"]
[692.286, "o", "    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n"]
[692.296, "o", "    ...     transform_alpha=1e-10,\r\n"]
[692.306, "o", "    ... )\r\n"]
[692.316, "o", "    >>> coder.transform(X)\r\n"]
[692.326, "o", "    array([[ 0.,  0., -1.,  0.,  0.],\r\n"]
[692.336, "o", "           [ 0.,  1.,  1.,  0.,  0.]])\r\n"]
[692.346, "o", "    \"\"\"\r\n"]
[692.356, "o", "\r\n"]
[692.366, "o", "    _required_parameters = [\"dictionary\"]\r\n"]
[692.376, "o", "\r\n"]
[692.386, "o", "    def __init__(\r\n"]
[692.396, "o", "        self,\r\n"]
[692.406, "o", "        dictionary,\r\n"]
[692.416, "o", "        *,\r\n"]
[692.426, "o", "        transform_algorithm=\"omp\",\r\n"]
[692.436, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[692.446, "o", "        transform_alpha=None,\r\n"]
[692.456, "o", "        split_sign=False,\r\n"]
[692.466, "o", "        n_jobs=None,\r\n"]
[692.476, "o", "        positive_code=False,\r\n"]
[692.486, "o", "        transform_max_iter=1000,\r\n"]
[692.496, "o", "    ):\r\n"]
[692.506, "o", "        super().__init__(\r\n"]
[692.516, "o", "            transform_algorithm,\r\n"]
[692.526, "o", "            transform_n_nonzero_coefs,\r\n"]
[692.536, "o", "            transform_alpha,\r\n"]
[692.546, "o", "            split_sign,\r\n"]
[692.556, "o", "            n_jobs,\r\n"]
[692.566, "o", "            positive_code,\r\n"]
[692.576, "o", "            transform_max_iter,\r\n"]
[692.586, "o", "        )\r\n"]
[692.596, "o", "        self.dictionary = dictionary\r\n"]
[692.606, "o", "\r\n"]
[692.616, "o", "    def fit(self, X, y=None):\r\n"]
[692.626, "o", "        \"\"\"Do nothing and return the estimator unchanged.\r\n"]
[692.636, "o", "\r\n"]
[692.646, "o", "        This method is just there to implement the usual API and hence\r\n"]
[692.656, "o", "        work in pipelines.\r\n"]
[692.666, "o", "\r\n"]
[692.676, "o", "        Parameters\r\n"]
[692.686, "o", "        ----------\r\n"]
[692.696, "o", "        X : Ignored\r\n"]
[692.706, "o", "            Not used, present for API consistency by convention.\r\n"]
[692.716, "o", "\r\n"]
[692.726, "o", "        y : Ignored\r\n"]
[692.736, "o", "            Not used, present for API consistency by convention.\r\n"]
[692.746, "o", "\r\n"]
[692.756, "o", "        Returns\r\n"]
[692.766, "o", "        -------\r\n"]
[692.776, "o", "        self : object\r\n"]
[692.786, "o", "            Returns the instance itself.\r\n"]
[692.796, "o", "        \"\"\"\r\n"]
[692.806, "o", "        return self\r\n"]
[692.816, "o", "\r\n"]
[692.826, "o", "    def transform(self, X, y=None):\r\n"]
[692.836, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[692.846, "o", "\r\n"]
[692.856, "o", "        Coding method is determined by the object parameter\r\n"]
[692.866, "o", "        `transform_algorithm`.\r\n"]
[692.876, "o", "\r\n"]
[692.886, "o", "        Parameters\r\n"]
[692.896, "o", "        ----------\r\n"]
[692.906, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[692.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[692.926, "o", "            and `n_features` is the number of features.\r\n"]
[692.936, "o", "\r\n"]
[692.946, "o", "        y : Ignored\r\n"]
[692.956, "o", "            Not used, present for API consistency by convention.\r\n"]
[692.966, "o", "\r\n"]
[692.976, "o", "        Returns\r\n"]
[692.986, "o", "        -------\r\n"]
[692.996, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[693.006, "o", "            Transformed data.\r\n"]
[693.016, "o", "        \"\"\"\r\n"]
[693.026, "o", "        return super()._transform(X, self.dictionary)\r\n"]
[693.036, "o", "\r\n"]
[693.046, "o", "    def _more_tags(self):\r\n"]
[693.056, "o", "        return {\r\n"]
[693.066, "o", "            \"requires_fit\": False,\r\n"]
[693.076, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[693.086, "o", "        }\r\n"]
[693.096, "o", "\r\n"]
[693.106, "o", "    @property\r\n"]
[693.116, "o", "    def n_components_(self):\r\n"]
[693.126, "o", "        \"\"\"Number of atoms.\"\"\"\r\n"]
[693.136, "o", "        return self.dictionary.shape[0]\r\n"]
[693.146, "o", "\r\n"]
[693.156, "o", "    @property\r\n"]
[693.166, "o", "    def n_features_in_(self):\r\n"]
[693.176, "o", "        \"\"\"Number of features seen during `fit`.\"\"\"\r\n"]
[693.186, "o", "        return self.dictionary.shape[1]\r\n"]
[693.196, "o", "\r\n"]
[693.206, "o", "    @property\r\n"]
[693.216, "o", "    def _n_features_out(self):\r\n"]
[693.226, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[693.236, "o", "        return self.n_components_\r\n"]
[693.246, "o", "\r\n"]
[693.256, "o", "\r\n"]
[693.266, "o", "class DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[693.276, "o", "    \"\"\"Dictionary learning.\r\n"]
[693.286, "o", "\r\n"]
[693.296, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[693.306, "o", "    encoding the fitted data.\r\n"]
[693.316, "o", "\r\n"]
[693.326, "o", "    Solves the optimization problem::\r\n"]
[693.336, "o", "\r\n"]
[693.346, "o", "        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[693.356, "o", "                    (U,V)\r\n"]
[693.366, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[693.376, "o", "\r\n"]
[693.386, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[693.396, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[693.406, "o", "    of all the entries in the matrix.\r\n"]
[693.416, "o", "\r\n"]
[693.426, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[693.436, "o", "\r\n"]
[693.446, "o", "    Parameters\r\n"]
[693.456, "o", "    ----------\r\n"]
[693.466, "o", "    n_components : int, default=None\r\n"]
[693.476, "o", "        Number of dictionary elements to extract. If None, then ``n_components``\r\n"]
[693.486, "o", "        is set to ``n_features``.\r\n"]
[693.496, "o", "\r\n"]
[693.506, "o", "    alpha : float, default=1.0\r\n"]
[693.516, "o", "        Sparsity controlling parameter.\r\n"]
[693.526, "o", "\r\n"]
[693.536, "o", "    max_iter : int, default=1000\r\n"]
[693.546, "o", "        Maximum number of iterations to perform.\r\n"]
[693.556, "o", "\r\n"]
[693.566, "o", "    tol : float, default=1e-8\r\n"]
[693.576, "o", "        Tolerance for numerical error.\r\n"]
[693.586, "o", "\r\n"]
[693.596, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[693.606, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[693.616, "o", "          problem (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[693.626, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[693.636, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n"]
[693.646, "o", "          faster if the estimated components are sparse.\r\n"]
[693.656, "o", "\r\n"]
[693.666, "o", "        .. versionadded:: 0.17\r\n"]
[693.676, "o", "           *cd* coordinate descent method to improve speed.\r\n"]
[693.686, "o", "\r\n"]
[693.696, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[693.706, "o", "            'threshold'}, default='omp'\r\n"]
[693.716, "o", "        Algorithm used to transform the data:\r\n"]
[693.726, "o", "\r\n"]
[693.736, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[693.746, "o", "          (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[693.756, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[693.766, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[693.776, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n"]
[693.786, "o", "          will be faster if the estimated components are sparse.\r\n"]
[693.796, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[693.806, "o", "          solution.\r\n"]
[693.816, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[693.826, "o", "          the projection ``dictionary * X'``.\r\n"]
[693.836, "o", "\r\n"]
[693.846, "o", "        .. versionadded:: 0.17\r\n"]
[693.856, "o", "           *lasso_cd* coordinate descent method to improve speed.\r\n"]
[693.866, "o", "\r\n"]
[693.876, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[693.886, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[693.896, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[693.906, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[693.916, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[693.926, "o", "\r\n"]
[693.936, "o", "    transform_alpha : float, default=None\r\n"]
[693.946, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[693.956, "o", "        penalty applied to the L1 norm.\r\n"]
[693.966, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[693.976, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[693.986, "o", "        If `None`, defaults to `alpha`.\r\n"]
[693.996, "o", "\r\n"]
[694.006, "o", "        .. versionchanged:: 1.2\r\n"]
[694.016, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[694.026, "o", "\r\n"]
[694.036, "o", "    n_jobs : int or None, default=None\r\n"]
[694.046, "o", "        Number of parallel jobs to run.\r\n"]
[694.056, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[694.066, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[694.076, "o", "        for more details.\r\n"]
[694.086, "o", "\r\n"]
[694.096, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[694.106, "o", "        Initial value for the code, for warm restart. Only used if `code_init`\r\n"]
[694.116, "o", "        and `dict_init` are not None.\r\n"]
[694.126, "o", "\r\n"]
[694.136, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[694.146, "o", "        Initial values for the dictionary, for warm restart. Only used if\r\n"]
[694.156, "o", "        `code_init` and `dict_init` are not None.\r\n"]
[694.166, "o", "\r\n"]
[694.176, "o", "    callback : callable, default=None\r\n"]
[694.186, "o", "        Callable that gets invoked every five iterations.\r\n"]
[694.196, "o", "\r\n"]
[694.206, "o", "        .. versionadded:: 1.3\r\n"]
[694.216, "o", "\r\n"]
[694.226, "o", "    verbose : bool, default=False\r\n"]
[694.236, "o", "        To control the verbosity of the procedure.\r\n"]
[694.246, "o", "\r\n"]
[694.256, "o", "    split_sign : bool, default=False\r\n"]
[694.266, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[694.276, "o", "        its negative part and its positive part. This can improve the\r\n"]
[694.286, "o", "        performance of downstream classifiers.\r\n"]
[694.296, "o", "\r\n"]
[694.306, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[694.316, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[694.326, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[694.336, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[694.346, "o", "        results across multiple function calls.\r\n"]
[694.356, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[694.366, "o", "\r\n"]
[694.376, "o", "    positive_code : bool, default=False\r\n"]
[694.386, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[694.396, "o", "\r\n"]
[694.406, "o", "        .. versionadded:: 0.20\r\n"]
[694.416, "o", "\r\n"]
[694.426, "o", "    positive_dict : bool, default=False\r\n"]
[694.436, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[694.446, "o", "\r\n"]
[694.456, "o", "        .. versionadded:: 0.20\r\n"]
[694.466, "o", "\r\n"]
[694.476, "o", "    transform_max_iter : int, default=1000\r\n"]
[694.486, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[694.496, "o", "        `'lasso_lars'`.\r\n"]
[694.506, "o", "\r\n"]
[694.516, "o", "        .. versionadded:: 0.22\r\n"]
[694.526, "o", "\r\n"]
[694.536, "o", "    Attributes\r\n"]
[694.546, "o", "    ----------\r\n"]
[694.556, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[694.566, "o", "        dictionary atoms extracted from the data\r\n"]
[694.576, "o", "\r\n"]
[694.586, "o", "    error_ : array\r\n"]
[694.596, "o", "        vector of errors at each iteration\r\n"]
[694.606, "o", "\r\n"]
[694.616, "o", "    n_features_in_ : int\r\n"]
[694.626, "o", "        Number of features seen during :term:`fit`.\r\n"]
[694.636, "o", "\r\n"]
[694.646, "o", "        .. versionadded:: 0.24\r\n"]
[694.656, "o", "\r\n"]
[694.666, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[694.676, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[694.686, "o", "        has feature names that are all strings.\r\n"]
[694.696, "o", "\r\n"]
[694.706, "o", "        .. versionadded:: 1.0\r\n"]
[694.716, "o", "\r\n"]
[694.726, "o", "    n_iter_ : int\r\n"]
[694.736, "o", "        Number of iterations run.\r\n"]
[694.746, "o", "\r\n"]
[694.756, "o", "    See Also\r\n"]
[694.766, "o", "    --------\r\n"]
[694.776, "o", "    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n"]
[694.786, "o", "        dictionary learning algorithm.\r\n"]
[694.796, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[694.806, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[694.816, "o", "        precomputed dictionary.\r\n"]
[694.826, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[694.836, "o", "\r\n"]
[694.846, "o", "    References\r\n"]
[694.856, "o", "    ----------\r\n"]
[694.866, "o", "\r\n"]
[694.876, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[694.886, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[694.896, "o", "\r\n"]
[694.906, "o", "    Examples\r\n"]
[694.916, "o", "    --------\r\n"]
[694.926, "o", "    >>> import numpy as np\r\n"]
[694.936, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[695.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[695.002, "i", "cd asv_benchmarks\r"]
[695.004, "o", "cd asv_benchmarks\r\n"]
[695.006, "o", "\u001b[?2004l\r\n"]
[700.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[700.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[700.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[701.654, "o", "\u001b[?2004l\r\n"]
[703.302, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[705.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[705.002, "i", "cd ..\r"]
[705.004, "o", "cd ..\r\n"]
[705.006, "o", "\u001b[?2004l\r\n"]
[710.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[710.002, "i", "cd asv_benchmarks\r"]
[710.004, "o", "cd asv_benchmarks\r\n"]
[710.006, "o", "\u001b[?2004l\r\n"]
[715.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[715.002, "i", "asv run --python=same --bench \".*MiniBatchDictionaryLearningBenchmark.*\" --quick\r"]
[715.004, "o", "asv run --python=same --bench \".*MiniBatchDictionaryLearningBenchmark.*\" --quick\r\n"]
[715.079791, "o", "\u001b[?2004l\r\n"]
[715.153582, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[715.227373, "o", "\u00b7 \u001b[0;32mRunning 6 total benchmarks (1 commits * 1 environments * 6 benchmarks)\u001b[0m\r\n"]
[715.301164, "o", "[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n"]
[715.374955, "o", "[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n"]
[715.448746, "o", "[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.peakmem_fit\u001b[0m                                                                                  ok\r\n"]
[715.522537, "o", "[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =======\u001b[0m\r\n"]
[715.596328, "o", "             --               n_jobs\r\n"]
[715.670119, "o", "             --------------- -------\r\n"]
[715.74391, "o", "              fit_algorithm     1   \r\n"]
[715.817701, "o", "             =============== =======\r\n"]
[715.891493, "o", "                   lars       93.7M \r\n"]
[715.965284, "o", "                    cd        93.3M \r\n"]
[716.039075, "o", "             =============== =======\r\n"]
[716.112866, "o", "\r\n"]
[716.186657, "o", "[33.33%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.peakmem_transform\u001b[0m                                                                            ok\r\n"]
[716.260448, "o", "[33.33%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =======\u001b[0m\r\n"]
[716.334239, "o", "             --               n_jobs\r\n"]
[716.40803, "o", "             --------------- -------\r\n"]
[716.481821, "o", "              fit_algorithm     1   \r\n"]
[716.555612, "o", "             =============== =======\r\n"]
[716.629403, "o", "                   lars       84.2M \r\n"]
[716.703194, "o", "                    cd         84M  \r\n"]
[716.776985, "o", "             =============== =======\r\n"]
[716.850776, "o", "\r\n"]
[716.924567, "o", "[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0masv dev -l | sed -n '1,200p'\r\n"]
[716.998358, "o", "asv dev -l | grep -n \"MiniBatchDictionaryLearningBenchmark\" || true\r\n"]
[717.072149, "o", "                                                                                     ok\r\n"]
[717.14594, "o", "[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =========\u001b[0m\r\n"]
[717.219731, "o", "             --                n_jobs \r\n"]
[717.293522, "o", "             --------------- ---------\r\n"]
[717.367313, "o", "              fit_algorithm      1    \r\n"]
[717.441104, "o", "             =============== =========\r\n"]
[717.514896, "o", "                   lars       6.62\u00b10s \r\n"]
[717.588687, "o", "                    cd        1.56\u00b10s \r\n"]
[717.662478, "o", "             =============== =========\r\n"]
[717.736269, "o", "\r\n"]
[717.81006, "o", "[66.67%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_transform\u001b[0m                                                                               ok\r\n"]
[717.883851, "o", "[66.67%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =========\u001b[0m\r\n"]
[717.957642, "o", "             --                n_jobs \r\n"]
[718.031433, "o", "             --------------- ---------\r\n"]
[718.105224, "o", "              fit_algorithm      1    \r\n"]
[718.179015, "o", "             =============== =========\r\n"]
[718.252806, "o", "                   lars       157\u00b10ms \r\n"]
[718.326597, "o", "                    cd        157\u00b10ms \r\n"]
[718.400388, "o", "             =============== =========\r\n"]
[718.474179, "o", "\r\n"]
[718.54797, "o", "[83.33%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.track_test_score\u001b[0m                                                                             ok\r\n"]
[718.621761, "o", "[83.33%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ======================\u001b[0m\r\n"]
[718.695552, "o", "             --                      n_jobs        \r\n"]
[718.769343, "o", "             --------------- ----------------------\r\n"]
[718.843134, "o", "              fit_algorithm            1           \r\n"]
[718.916925, "o", "             =============== ======================\r\n"]
[718.990716, "o", "                   lars       -0.07507291436195374 \r\n"]
[719.064507, "o", "                    cd        -0.07509346306324005 \r\n"]
[719.138299, "o", "             =============== ======================\r\n"]
[719.21209, "o", "\r\n"]
[719.285881, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.track_train_score\u001b[0m                                                                            ok\r\n"]
[719.359672, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ======================\u001b[0m\r\n"]
[719.433463, "o", "              --                      n_jobs        \r\n"]
[719.507254, "o", "              --------------- ----------------------\r\n"]
[719.581045, "o", "               fit_algorithm            1           \r\n"]
[719.654836, "o", "              =============== ======================\r\n"]
[719.728627, "o", "                    lars       -0.07244297116994858 \r\n"]
[719.802418, "o", "                     cd        -0.07244537025690079 \r\n"]
[719.876209, "o", "              =============== ======================\r\n"]
[720.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[720.002, "i", "asv dev -l | sed -n '1,200p'\r"]
[720.004, "o", "asv dev -l | sed -n '1,200p'\r\n"]
[720.9948, "o", "\u001b[?2004l\r\n"]
[721.9836, "o", "usage: asv [-h] [--verbose] [--config CONFIG] [--version]\r\n"]
[722.9724, "o", "           {help,quickstart,machine,setup,run,continuous,find,rm,publish,preview,profile,update,show,compare,check,gh-pages}\r\n"]
[723.9612, "o", "           ...\r\n"]
[725.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[725.002, "i", "asv dev -l | grep -n \"MiniBatchDictionaryLearningBenchmark\" || true\r"]
[725.004, "o", "asv dev -l | grep -n \"MiniBatchDictionaryLearningBenchmark\" || true\r\n"]
[725.9948, "o", "\u001b[?2004l\r\n"]
[726.9836, "o", "usage: asv [-h] [--verbose] [--config CONFIG] [--version]\r\n"]
[727.9724, "o", "           {help,quickstart,machine,setup,run,continuous,find,rm,publish,preview,profile,update,show,compare,check,gh-pages}\r\n"]
[728.9612, "o", "           ...\r\n"]
[730.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[730.002, "i", "asv profile --python=same benchmarks.decomposition.MiniBatchDictionaryLearningBenchmark.time_fit || \r"]
[730.004, "o", "asv profile --python=same benchmarks.decomposition.MiniBatchDictionaryLearningBenchmark.time_fit || \r\n"]
[730.83, "o", " asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit || true\r\n"]
[731.654, "o", "\u001b[?2004l\r\n"]
[732.478, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[733.302, "o", "\u00b7 \u001b[0;31m'benchmarks.decomposition.MiniBatchDictionaryLearningBenchmark.time_fit' benchmark not found\u001b[0m\r\n"]
[734.126, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[735.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[735.002, "i", "cd ..\r"]
[735.004, "o", "cd ..\r\n"]
[735.006, "o", "\u001b[?2004l\r\n"]
[740.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[740.002, "i", "grep -n \"def _minibatch_step\\|_sparse_encode_precomputed\\|cov\\s*=\\s|gram\\s*=\\s\" sklearn/decomposition/_dict_learnin\r"]
[740.004, "o", "grep -n \"def _minibatch_step\\|_sparse_encode_precomputed\\|cov\\s*=\\s|gram\\s*=\\s\" sklearn/decomposition/_dict_learnin\r\n"]
[740.83, "o", "ng.py\r\n"]
[741.654, "o", "\u001b[?2004l\r\n"]
[742.478, "o", "\u001b[32m\u001b[K38\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdef \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n"]
[743.302, "o", "\u001b[32m\u001b[K416\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        code = \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n"]
[744.126, "o", "\u001b[32m\u001b[K438\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        delayed(\u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K)(\r\n"]
[745.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[745.002, "i", "grep -n \"_sparse_encode_precomputed\" sklearn/decomposition/_dict_learning.py\r"]
[745.004, "o", "grep -n \"_sparse_encode_precomputed\" sklearn/decomposition/_dict_learning.py\r\n"]
[746.242, "o", "\u001b[?2004l\r\n"]
[747.478, "o", "\u001b[32m\u001b[K38\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdef \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n"]
[748.714, "o", "\u001b[32m\u001b[K416\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        code = \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n"]
[750.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[750.002, "i", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r"]
[750.004, "o", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r\n"]
[750.016, "o", "\u001b[?2004l\r\n"]
[750.026, "o", "\"\"\" Dictionary learning.\r\n"]
[750.036, "o", "\"\"\"\r\n"]
[750.046, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[750.056, "o", "# License: BSD 3 clause\r\n"]
[750.066, "o", "\r\n"]
[750.076, "o", "import itertools\r\n"]
[750.086, "o", "import sys\r\n"]
[750.096, "o", "import time\r\n"]
[750.106, "o", "import warnings\r\n"]
[750.116, "o", "from math import ceil\r\n"]
[750.126, "o", "from numbers import Integral, Real\r\n"]
[750.136, "o", "\r\n"]
[750.146, "o", "import numpy as np\r\n"]
[750.156, "o", "from joblib import effective_n_jobs\r\n"]
[750.166, "o", "from scipy import linalg\r\n"]
[750.176, "o", "\r\n"]
[750.186, "o", "from ..base import (\r\n"]
[750.196, "o", "    BaseEstimator,\r\n"]
[750.206, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[750.216, "o", "    TransformerMixin,\r\n"]
[750.226, "o", "    _fit_context,\r\n"]
[750.236, "o", ")\r\n"]
[750.246, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[750.256, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[750.266, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[750.276, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[750.286, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[750.296, "o", "from ..utils.validation import check_is_fitted\r\n"]
[750.306, "o", "\r\n"]
[750.316, "o", "\r\n"]
[750.326, "o", "def _check_positive_coding(method, positive):\r\n"]
[750.336, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[750.346, "o", "        raise ValueError(\r\n"]
[750.356, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[750.366, "o", "        )\r\n"]
[750.376, "o", "\r\n"]
[750.386, "o", "\r\n"]
[750.396, "o", "def _sparse_encode_precomputed(\r\n"]
[750.406, "o", "    X,\r\n"]
[750.416, "o", "    dictionary,\r\n"]
[750.426, "o", "    *,\r\n"]
[750.436, "o", "    gram=None,\r\n"]
[750.446, "o", "    cov=None,\r\n"]
[750.456, "o", "    algorithm=\"lasso_lars\",\r\n"]
[750.466, "o", "    regularization=None,\r\n"]
[750.476, "o", "    copy_cov=True,\r\n"]
[750.486, "o", "    init=None,\r\n"]
[750.496, "o", "    max_iter=1000,\r\n"]
[750.506, "o", "    verbose=0,\r\n"]
[750.516, "o", "    positive=False,\r\n"]
[750.526, "o", "):\r\n"]
[750.536, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[750.546, "o", "\r\n"]
[750.556, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[750.566, "o", "\r\n"]
[750.576, "o", "    Parameters\r\n"]
[750.586, "o", "    ----------\r\n"]
[750.596, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[750.606, "o", "        Data matrix.\r\n"]
[750.616, "o", "\r\n"]
[750.626, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[750.636, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[750.646, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[750.656, "o", "\r\n"]
[750.666, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[750.676, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[750.686, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[750.696, "o", "\r\n"]
[750.706, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[750.716, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[750.726, "o", "\r\n"]
[750.736, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[750.746, "o", "            default='lasso_lars'\r\n"]
[750.756, "o", "        The algorithm used:\r\n"]
[750.766, "o", "\r\n"]
[750.776, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[750.786, "o", "          (`linear_model.lars_path`);\r\n"]
[750.796, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[750.806, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[750.816, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[750.826, "o", "          the estimated components are sparse;\r\n"]
[750.836, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[750.846, "o", "          solution;\r\n"]
[750.856, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[750.866, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[750.876, "o", "\r\n"]
[750.886, "o", "    regularization : int or float, default=None\r\n"]
[750.896, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[750.906, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[750.916, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[750.926, "o", "\r\n"]
[750.936, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[750.946, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[750.956, "o", "        `algorithm='lasso_cd'`.\r\n"]
[750.966, "o", "\r\n"]
[750.976, "o", "    max_iter : int, default=1000\r\n"]
[750.986, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[750.996, "o", "        `'lasso_lars'`.\r\n"]
[751.006, "o", "\r\n"]
[751.016, "o", "    copy_cov : bool, default=True\r\n"]
[751.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[751.036, "o", "        be overwritten.\r\n"]
[751.046, "o", "\r\n"]
[751.056, "o", "    verbose : int, default=0\r\n"]
[751.066, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[751.076, "o", "\r\n"]
[751.086, "o", "    positive: bool, default=False\r\n"]
[751.096, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[751.106, "o", "\r\n"]
[751.116, "o", "        .. versionadded:: 0.20\r\n"]
[751.126, "o", "\r\n"]
[751.136, "o", "    Returns\r\n"]
[751.146, "o", "    -------\r\n"]
[751.156, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[751.166, "o", "        The sparse codes.\r\n"]
[751.176, "o", "    \"\"\"\r\n"]
[751.186, "o", "    n_samples, n_features = X.shape\r\n"]
[751.196, "o", "    n_components = dictionary.shape[0]\r\n"]
[751.206, "o", "\r\n"]
[751.216, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[751.226, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[751.236, "o", "        try:\r\n"]
[751.246, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[751.256, "o", "\r\n"]
[751.266, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[751.276, "o", "            # corrects the verbosity level.\r\n"]
[751.286, "o", "            lasso_lars = LassoLars(\r\n"]
[751.296, "o", "                alpha=alpha,\r\n"]
[751.306, "o", "                fit_intercept=False,\r\n"]
[751.316, "o", "                verbose=verbose,\r\n"]
[751.326, "o", "                precompute=gram,\r\n"]
[751.336, "o", "                fit_path=False,\r\n"]
[751.346, "o", "                positive=positive,\r\n"]
[751.356, "o", "                max_iter=max_iter,\r\n"]
[751.366, "o", "            )\r\n"]
[751.376, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[751.386, "o", "            new_code = lasso_lars.coef_\r\n"]
[751.396, "o", "        finally:\r\n"]
[751.406, "o", "            np.seterr(**err_mgt)\r\n"]
[751.416, "o", "\r\n"]
[751.426, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[751.436, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[751.446, "o", "\r\n"]
[751.456, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[751.466, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[751.476, "o", "        # argument that we could pass in from Lasso.\r\n"]
[751.486, "o", "        clf = Lasso(\r\n"]
[751.496, "o", "            alpha=alpha,\r\n"]
[751.506, "o", "            fit_intercept=False,\r\n"]
[751.516, "o", "            precompute=gram,\r\n"]
[751.526, "o", "            max_iter=max_iter,\r\n"]
[751.536, "o", "            warm_start=True,\r\n"]
[751.546, "o", "            positive=positive,\r\n"]
[751.556, "o", "        )\r\n"]
[751.566, "o", "\r\n"]
[751.576, "o", "        if init is not None:\r\n"]
[751.586, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[751.596, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[751.606, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[751.616, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[751.626, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[751.636, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[751.646, "o", "                init = np.array(init)\r\n"]
[751.656, "o", "            clf.coef_ = init\r\n"]
[751.666, "o", "\r\n"]
[751.676, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[751.686, "o", "        new_code = clf.coef_\r\n"]
[751.696, "o", "\r\n"]
[751.706, "o", "    elif algorithm == \"lars\":\r\n"]
[751.716, "o", "        try:\r\n"]
[751.726, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[751.736, "o", "\r\n"]
[751.746, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[751.756, "o", "            # corrects the verbosity level.\r\n"]
[751.766, "o", "            lars = Lars(\r\n"]
[751.776, "o", "                fit_intercept=False,\r\n"]
[751.786, "o", "                verbose=verbose,\r\n"]
[751.796, "o", "                precompute=gram,\r\n"]
[751.806, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[751.816, "o", "                fit_path=False,\r\n"]
[751.826, "o", "            )\r\n"]
[751.836, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[751.846, "o", "            new_code = lars.coef_\r\n"]
[751.856, "o", "        finally:\r\n"]
[751.866, "o", "            np.seterr(**err_mgt)\r\n"]
[751.876, "o", "\r\n"]
[751.886, "o", "    elif algorithm == \"threshold\":\r\n"]
[751.896, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[751.906, "o", "        if positive:\r\n"]
[751.916, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[751.926, "o", "\r\n"]
[751.936, "o", "    elif algorithm == \"omp\":\r\n"]
[751.946, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[751.956, "o", "            Gram=gram,\r\n"]
[751.966, "o", "            Xy=cov,\r\n"]
[751.976, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[751.986, "o", "            tol=None,\r\n"]
[751.996, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[752.006, "o", "            copy_Xy=copy_cov,\r\n"]
[752.016, "o", "        ).T\r\n"]
[752.026, "o", "\r\n"]
[752.036, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[752.046, "o", "\r\n"]
[752.056, "o", "\r\n"]
[752.066, "o", "@validate_params(\r\n"]
[752.076, "o", "    {\r\n"]
[752.086, "o", "        \"X\": [\"array-like\"],\r\n"]
[752.096, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[752.106, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[752.116, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[752.126, "o", "        \"algorithm\": [\r\n"]
[752.136, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[752.146, "o", "        ],\r\n"]
[752.156, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[752.166, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[752.176, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[752.186, "o", "        \"init\": [\"array-like\", None],\r\n"]
[752.196, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[752.206, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[752.216, "o", "        \"check_input\": [\"boolean\"],\r\n"]
[752.226, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[752.236, "o", "        \"positive\": [\"boolean\"],\r\n"]
[752.246, "o", "    },\r\n"]
[752.256, "o", "    prefer_skip_nested_validation=True,\r\n"]
[752.266, "o", ")\r\n"]
[752.276, "o", "# XXX : could be moved to the linear_model module\r\n"]
[752.286, "o", "def sparse_encode(\r\n"]
[752.296, "o", "    X,\r\n"]
[752.306, "o", "    dictionary,\r\n"]
[752.316, "o", "    *,\r\n"]
[752.326, "o", "    gram=None,\r\n"]
[752.336, "o", "    cov=None,\r\n"]
[752.346, "o", "    algorithm=\"lasso_lars\",\r\n"]
[752.356, "o", "    n_nonzero_coefs=None,\r\n"]
[752.366, "o", "    alpha=None,\r\n"]
[752.376, "o", "    copy_cov=True,\r\n"]
[752.386, "o", "    init=None,\r\n"]
[752.396, "o", "    max_iter=1000,\r\n"]
[752.406, "o", "    n_jobs=None,\r\n"]
[752.416, "o", "    check_input=True,\r\n"]
[752.426, "o", "    verbose=0,\r\n"]
[752.436, "o", "    positive=False,\r\n"]
[752.446, "o", "):\r\n"]
[752.456, "o", "    \"\"\"Sparse coding.\r\n"]
[752.466, "o", "\r\n"]
[752.476, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[752.486, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[752.496, "o", "\r\n"]
[752.506, "o", "        X ~= code * dictionary\r\n"]
[752.516, "o", "\r\n"]
[752.526, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[752.536, "o", "\r\n"]
[752.546, "o", "    Parameters\r\n"]
[752.556, "o", "    ----------\r\n"]
[752.566, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[752.576, "o", "        Data matrix.\r\n"]
[752.586, "o", "\r\n"]
[752.596, "o", "    dictionary : array-like of shape (n_components, n_features)\r\n"]
[752.606, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[752.616, "o", "        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[752.626, "o", "        output.\r\n"]
[752.636, "o", "\r\n"]
[752.646, "o", "    gram : array-like of shape (n_components, n_components), default=None\r\n"]
[752.656, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n"]
[752.666, "o", "\r\n"]
[752.676, "o", "    cov : array-like of shape (n_components, n_samples), default=None\r\n"]
[752.686, "o", "        Precomputed covariance, `dictionary' * X`.\r\n"]
[752.696, "o", "\r\n"]
[752.706, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[752.716, "o", "            default='lasso_lars'\r\n"]
[752.726, "o", "        The algorithm used:\r\n"]
[752.736, "o", "\r\n"]
[752.746, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[752.756, "o", "          (`linear_model.lars_path`);\r\n"]
[752.766, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[752.776, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[752.786, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[752.796, "o", "          the estimated components are sparse;\r\n"]
[752.806, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[752.816, "o", "          solution;\r\n"]
[752.826, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[752.836, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[752.846, "o", "\r\n"]
[752.856, "o", "    n_nonzero_coefs : int, default=None\r\n"]
[752.866, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[752.876, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[752.886, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[752.896, "o", "        `n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[752.906, "o", "\r\n"]
[752.916, "o", "    alpha : float, default=None\r\n"]
[752.926, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[752.936, "o", "        penalty applied to the L1 norm.\r\n"]
[752.946, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[752.956, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[752.966, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[752.976, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[752.986, "o", "        `n_nonzero_coefs`.\r\n"]
[752.996, "o", "        If `None`, default to 1.\r\n"]
[753.006, "o", "\r\n"]
[753.016, "o", "    copy_cov : bool, default=True\r\n"]
[753.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[753.036, "o", "        be overwritten.\r\n"]
[753.046, "o", "\r\n"]
[753.056, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[753.066, "o", "        Initialization value of the sparse codes. Only used if\r\n"]
[753.076, "o", "        `algorithm='lasso_cd'`.\r\n"]
[753.086, "o", "\r\n"]
[753.096, "o", "    max_iter : int, default=1000\r\n"]
[753.106, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[753.116, "o", "        `'lasso_lars'`.\r\n"]
[753.126, "o", "\r\n"]
[753.136, "o", "    n_jobs : int, default=None\r\n"]
[753.146, "o", "        Number of parallel jobs to run.\r\n"]
[753.156, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[753.166, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[753.176, "o", "        for more details.\r\n"]
[753.186, "o", "\r\n"]
[753.196, "o", "    check_input : bool, default=True\r\n"]
[753.206, "o", "        If `False`, the input arrays X and dictionary will not be checked.\r\n"]
[753.216, "o", "\r\n"]
[753.226, "o", "    verbose : int, default=0\r\n"]
[753.236, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[753.246, "o", "\r\n"]
[753.256, "o", "    positive : bool, default=False\r\n"]
[753.266, "o", "        Whether to enforce positivity when finding the encoding.\r\n"]
[753.276, "o", "\r\n"]
[753.286, "o", "        .. versionadded:: 0.20\r\n"]
[753.296, "o", "\r\n"]
[753.306, "o", "    Returns\r\n"]
[753.316, "o", "    -------\r\n"]
[753.326, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[753.336, "o", "        The sparse codes.\r\n"]
[753.346, "o", "\r\n"]
[753.356, "o", "    See Also\r\n"]
[753.366, "o", "    --------\r\n"]
[753.376, "o", "    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n"]
[753.386, "o", "        path using LARS algorithm.\r\n"]
[753.396, "o", "    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n"]
[753.406, "o", "    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n"]
[753.416, "o", "    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n"]
[753.426, "o", "        dictionary.\r\n"]
[753.436, "o", "    \"\"\"\r\n"]
[753.446, "o", "    if check_input:\r\n"]
[753.456, "o", "        if algorithm == \"lasso_cd\":\r\n"]
[753.466, "o", "            dictionary = check_array(\r\n"]
[753.476, "o", "                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n"]
[753.486, "o", "            )\r\n"]
[753.496, "o", "            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n"]
[753.506, "o", "        else:\r\n"]
[753.516, "o", "            dictionary = check_array(dictionary)\r\n"]
[753.526, "o", "            X = check_array(X)\r\n"]
[753.536, "o", "\r\n"]
[753.546, "o", "    if dictionary.shape[1] != X.shape[1]:\r\n"]
[753.556, "o", "        raise ValueError(\r\n"]
[753.566, "o", "            \"Dictionary and X have different numbers of features:\"\r\n"]
[753.576, "o", "            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n"]
[753.586, "o", "        )\r\n"]
[753.596, "o", "\r\n"]
[753.606, "o", "    _check_positive_coding(algorithm, positive)\r\n"]
[753.616, "o", "\r\n"]
[753.626, "o", "    return _sparse_encode(\r\n"]
[753.636, "o", "        X,\r\n"]
[753.646, "o", "        dictionary,\r\n"]
[753.656, "o", "        gram=gram,\r\n"]
[753.666, "o", "        cov=cov,\r\n"]
[753.676, "o", "        algorithm=algorithm,\r\n"]
[753.686, "o", "        n_nonzero_coefs=n_nonzero_coefs,\r\n"]
[753.696, "o", "        alpha=alpha,\r\n"]
[753.706, "o", "        copy_cov=copy_cov,\r\n"]
[753.716, "o", "        init=init,\r\n"]
[753.726, "o", "        max_iter=max_iter,\r\n"]
[753.736, "o", "        n_jobs=n_jobs,\r\n"]
[753.746, "o", "        verbose=verbose,\r\n"]
[753.756, "o", "        positive=positive,\r\n"]
[753.766, "o", "    )\r\n"]
[753.776, "o", "\r\n"]
[753.786, "o", "\r\n"]
[753.796, "o", "def _sparse_encode(\r\n"]
[753.806, "o", "    X,\r\n"]
[753.816, "o", "    dictionary,\r\n"]
[753.826, "o", "    *,\r\n"]
[753.836, "o", "    gram=None,\r\n"]
[753.846, "o", "    cov=None,\r\n"]
[753.856, "o", "    algorithm=\"lasso_lars\",\r\n"]
[753.866, "o", "    n_nonzero_coefs=None,\r\n"]
[753.876, "o", "    alpha=None,\r\n"]
[753.886, "o", "    copy_cov=True,\r\n"]
[753.896, "o", "    init=None,\r\n"]
[753.906, "o", "    max_iter=1000,\r\n"]
[753.916, "o", "    n_jobs=None,\r\n"]
[753.926, "o", "    verbose=0,\r\n"]
[753.936, "o", "    positive=False,\r\n"]
[753.946, "o", "):\r\n"]
[753.956, "o", "    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[753.966, "o", "\r\n"]
[753.976, "o", "    n_samples, n_features = X.shape\r\n"]
[753.986, "o", "    n_components = dictionary.shape[0]\r\n"]
[753.996, "o", "\r\n"]
[754.006, "o", "    if algorithm in (\"lars\", \"omp\"):\r\n"]
[754.016, "o", "        regularization = n_nonzero_coefs\r\n"]
[754.026, "o", "        if regularization is None:\r\n"]
[754.036, "o", "            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[754.046, "o", "    else:\r\n"]
[754.056, "o", "        regularization = alpha\r\n"]
[754.066, "o", "        if regularization is None:\r\n"]
[754.076, "o", "            regularization = 1.0\r\n"]
[754.086, "o", "\r\n"]
[754.096, "o", "    if gram is None and algorithm != \"threshold\":\r\n"]
[754.106, "o", "        gram = np.dot(dictionary, dictionary.T)\r\n"]
[754.116, "o", "\r\n"]
[754.126, "o", "    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[754.136, "o", "        copy_cov = False\r\n"]
[754.146, "o", "        cov = np.dot(dictionary, X.T)\r\n"]
[754.156, "o", "\r\n"]
[754.166, "o", "    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[754.176, "o", "        code = _sparse_encode_precomputed(\r\n"]
[754.186, "o", "            X,\r\n"]
[754.196, "o", "            dictionary,\r\n"]
[754.206, "o", "            gram=gram,\r\n"]
[754.216, "o", "            cov=cov,\r\n"]
[754.226, "o", "            algorithm=algorithm,\r\n"]
[754.236, "o", "            regularization=regularization,\r\n"]
[754.246, "o", "            copy_cov=copy_cov,\r\n"]
[754.256, "o", "            init=init,\r\n"]
[754.266, "o", "            max_iter=max_iter,\r\n"]
[754.276, "o", "            verbose=verbose,\r\n"]
[754.286, "o", "            positive=positive,\r\n"]
[754.296, "o", "        )\r\n"]
[754.306, "o", "        return code\r\n"]
[754.316, "o", "\r\n"]
[754.326, "o", "    # Enter parallel code block\r\n"]
[754.336, "o", "    n_samples = X.shape[0]\r\n"]
[754.346, "o", "    n_components = dictionary.shape[0]\r\n"]
[754.356, "o", "    code = np.empty((n_samples, n_components))\r\n"]
[754.366, "o", "    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[754.376, "o", "\r\n"]
[754.386, "o", "    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[754.396, "o", "        delayed(_sparse_encode_precomputed)(\r\n"]
[754.406, "o", "            X[this_slice],\r\n"]
[754.416, "o", "            dictionary,\r\n"]
[754.426, "o", "            gram=gram,\r\n"]
[754.436, "o", "            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[754.446, "o", "            algorithm=algorithm,\r\n"]
[754.456, "o", "            regularization=regularization,\r\n"]
[754.466, "o", "            copy_cov=copy_cov,\r\n"]
[754.476, "o", "            init=init[this_slice] if init is not None else None,\r\n"]
[754.486, "o", "            max_iter=max_iter,\r\n"]
[754.496, "o", "            verbose=verbose,\r\n"]
[754.506, "o", "            positive=positive,\r\n"]
[754.516, "o", "        )\r\n"]
[754.526, "o", "        for this_slice in slices\r\n"]
[754.536, "o", "    )\r\n"]
[754.546, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[754.556, "o", "        code[this_slice] = this_view\r\n"]
[754.566, "o", "    return code\r\n"]
[754.576, "o", "\r\n"]
[754.586, "o", "\r\n"]
[754.596, "o", "def _update_dict(\r\n"]
[754.606, "o", "    dictionary,\r\n"]
[754.616, "o", "    Y,\r\n"]
[754.626, "o", "    code,\r\n"]
[754.636, "o", "    A=None,\r\n"]
[754.646, "o", "    B=None,\r\n"]
[754.656, "o", "    verbose=False,\r\n"]
[754.666, "o", "    random_state=None,\r\n"]
[754.676, "o", "    positive=False,\r\n"]
[754.686, "o", "):\r\n"]
[754.696, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[754.706, "o", "\r\n"]
[754.716, "o", "    Parameters\r\n"]
[754.726, "o", "    ----------\r\n"]
[754.736, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[754.746, "o", "        Value of the dictionary at the previous iteration.\r\n"]
[754.756, "o", "\r\n"]
[754.766, "o", "    Y : ndarray of shape (n_samples, n_features)\r\n"]
[754.776, "o", "        Data matrix.\r\n"]
[754.786, "o", "\r\n"]
[754.796, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[754.806, "o", "        Sparse coding of the data against which to optimize the dictionary.\r\n"]
[754.816, "o", "\r\n"]
[754.826, "o", "    A : ndarray of shape (n_components, n_components), default=None\r\n"]
[754.836, "o", "        Together with `B`, sufficient stats of the online model to update the\r\n"]
[754.846, "o", "        dictionary.\r\n"]
[754.856, "o", "\r\n"]
[754.866, "o", "    B : ndarray of shape (n_features, n_components), default=None\r\n"]
[754.876, "o", "        Together with `A`, sufficient stats of the online model to update the\r\n"]
[754.886, "o", "        dictionary.\r\n"]
[754.896, "o", "\r\n"]
[754.906, "o", "    verbose: bool, default=False\r\n"]
[754.916, "o", "        Degree of output the procedure will print.\r\n"]
[754.926, "o", "\r\n"]
[754.936, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[755.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[755.002, "i", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r"]
[755.004, "o", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r\n"]
[755.016, "o", "\u001b[?2004l\r\n"]
[755.026, "o", "           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[755.036, "o", "\r\n"]
[755.046, "o", "    return_n_iter : bool, default=False\r\n"]
[755.056, "o", "        Whether or not to return the number of iterations.\r\n"]
[755.066, "o", "\r\n"]
[755.076, "o", "        .. deprecated:: 1.1\r\n"]
[755.086, "o", "           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n"]
[755.096, "o", "\r\n"]
[755.106, "o", "    positive_dict : bool, default=False\r\n"]
[755.116, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[755.126, "o", "\r\n"]
[755.136, "o", "        .. versionadded:: 0.20\r\n"]
[755.146, "o", "\r\n"]
[755.156, "o", "    positive_code : bool, default=False\r\n"]
[755.166, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[755.176, "o", "\r\n"]
[755.186, "o", "        .. versionadded:: 0.20\r\n"]
[755.196, "o", "\r\n"]
[755.206, "o", "    method_max_iter : int, default=1000\r\n"]
[755.216, "o", "        Maximum number of iterations to perform when solving the lasso problem.\r\n"]
[755.226, "o", "\r\n"]
[755.236, "o", "        .. versionadded:: 0.22\r\n"]
[755.246, "o", "\r\n"]
[755.256, "o", "    tol : float, default=1e-3\r\n"]
[755.266, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[755.276, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[755.286, "o", "\r\n"]
[755.296, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[755.306, "o", "        `tol` to 0.0.\r\n"]
[755.316, "o", "\r\n"]
[755.326, "o", "        .. versionadded:: 1.1\r\n"]
[755.336, "o", "\r\n"]
[755.346, "o", "    max_no_improvement : int, default=10\r\n"]
[755.356, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[755.366, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[755.376, "o", "        `max_iter` is not None.\r\n"]
[755.386, "o", "\r\n"]
[755.396, "o", "        To disable convergence detection based on cost function, set\r\n"]
[755.406, "o", "        `max_no_improvement` to None.\r\n"]
[755.416, "o", "\r\n"]
[755.426, "o", "        .. versionadded:: 1.1\r\n"]
[755.436, "o", "\r\n"]
[755.446, "o", "    Returns\r\n"]
[755.456, "o", "    -------\r\n"]
[755.466, "o", "    code : ndarray of shape (n_samples, n_components),\r\n"]
[755.476, "o", "        The sparse code (only returned if `return_code=True`).\r\n"]
[755.486, "o", "\r\n"]
[755.496, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[755.506, "o", "        The solutions to the dictionary learning problem.\r\n"]
[755.516, "o", "\r\n"]
[755.526, "o", "    n_iter : int\r\n"]
[755.536, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[755.546, "o", "        set to `True`.\r\n"]
[755.556, "o", "\r\n"]
[755.566, "o", "    See Also\r\n"]
[755.576, "o", "    --------\r\n"]
[755.586, "o", "    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n"]
[755.596, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[755.606, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n"]
[755.616, "o", "        learning algorithm.\r\n"]
[755.626, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[755.636, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[755.646, "o", "    \"\"\"\r\n"]
[755.656, "o", "    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n"]
[755.666, "o", "    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n"]
[755.676, "o", "        raise ValueError(\r\n"]
[755.686, "o", "            \"The following arguments are incompatible with 'max_iter': \"\r\n"]
[755.696, "o", "            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n"]
[755.706, "o", "        )\r\n"]
[755.716, "o", "\r\n"]
[755.726, "o", "    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n"]
[755.736, "o", "    return_inner_stats = _check_warn_deprecated(\r\n"]
[755.746, "o", "        return_inner_stats,\r\n"]
[755.756, "o", "        \"return_inner_stats\",\r\n"]
[755.766, "o", "        default=False,\r\n"]
[755.776, "o", "        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n"]
[755.786, "o", "    )\r\n"]
[755.796, "o", "    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n"]
[755.806, "o", "    return_n_iter = _check_warn_deprecated(\r\n"]
[755.816, "o", "        return_n_iter,\r\n"]
[755.826, "o", "        \"return_n_iter\",\r\n"]
[755.836, "o", "        default=False,\r\n"]
[755.846, "o", "        additional_message=(\r\n"]
[755.856, "o", "            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n"]
[755.866, "o", "            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n"]
[755.876, "o", "        ),\r\n"]
[755.886, "o", "    )\r\n"]
[755.896, "o", "\r\n"]
[755.906, "o", "    if max_iter is not None:\r\n"]
[755.916, "o", "        transform_algorithm = \"lasso_\" + method\r\n"]
[755.926, "o", "\r\n"]
[755.936, "o", "        est = MiniBatchDictionaryLearning(\r\n"]
[755.946, "o", "            n_components=n_components,\r\n"]
[755.956, "o", "            alpha=alpha,\r\n"]
[755.966, "o", "            n_iter=n_iter,\r\n"]
[755.976, "o", "            n_jobs=n_jobs,\r\n"]
[755.986, "o", "            fit_algorithm=method,\r\n"]
[755.996, "o", "            batch_size=batch_size,\r\n"]
[756.006, "o", "            shuffle=shuffle,\r\n"]
[756.016, "o", "            dict_init=dict_init,\r\n"]
[756.026, "o", "            random_state=random_state,\r\n"]
[756.036, "o", "            transform_algorithm=transform_algorithm,\r\n"]
[756.046, "o", "            transform_alpha=alpha,\r\n"]
[756.056, "o", "            positive_code=positive_code,\r\n"]
[756.066, "o", "            positive_dict=positive_dict,\r\n"]
[756.076, "o", "            transform_max_iter=method_max_iter,\r\n"]
[756.086, "o", "            verbose=verbose,\r\n"]
[756.096, "o", "            callback=callback,\r\n"]
[756.106, "o", "            tol=tol,\r\n"]
[756.116, "o", "            max_no_improvement=max_no_improvement,\r\n"]
[756.126, "o", "        ).fit(X)\r\n"]
[756.136, "o", "\r\n"]
[756.146, "o", "        if not return_code:\r\n"]
[756.156, "o", "            return est.components_\r\n"]
[756.166, "o", "        else:\r\n"]
[756.176, "o", "            code = est.transform(X)\r\n"]
[756.186, "o", "            return code, est.components_\r\n"]
[756.196, "o", "\r\n"]
[756.206, "o", "    # TODO(1.4) remove the whole old behavior\r\n"]
[756.216, "o", "    # Fallback to old behavior\r\n"]
[756.226, "o", "\r\n"]
[756.236, "o", "    n_iter = _check_warn_deprecated(\r\n"]
[756.246, "o", "        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n"]
[756.256, "o", "    )\r\n"]
[756.266, "o", "\r\n"]
[756.276, "o", "    if n_components is None:\r\n"]
[756.286, "o", "        n_components = X.shape[1]\r\n"]
[756.296, "o", "\r\n"]
[756.306, "o", "    if method not in (\"lars\", \"cd\"):\r\n"]
[756.316, "o", "        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n"]
[756.326, "o", "\r\n"]
[756.336, "o", "    _check_positive_coding(method, positive_code)\r\n"]
[756.346, "o", "\r\n"]
[756.356, "o", "    method = \"lasso_\" + method\r\n"]
[756.366, "o", "\r\n"]
[756.376, "o", "    t0 = time.time()\r\n"]
[756.386, "o", "    n_samples, n_features = X.shape\r\n"]
[756.396, "o", "    # Avoid integer division problems\r\n"]
[756.406, "o", "    alpha = float(alpha)\r\n"]
[756.416, "o", "    random_state = check_random_state(random_state)\r\n"]
[756.426, "o", "\r\n"]
[756.436, "o", "    # Init V with SVD of X\r\n"]
[756.446, "o", "    if dict_init is not None:\r\n"]
[756.456, "o", "        dictionary = dict_init\r\n"]
[756.466, "o", "    else:\r\n"]
[756.476, "o", "        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n"]
[756.486, "o", "        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[756.496, "o", "    r = len(dictionary)\r\n"]
[756.506, "o", "    if n_components <= r:\r\n"]
[756.516, "o", "        dictionary = dictionary[:n_components, :]\r\n"]
[756.526, "o", "    else:\r\n"]
[756.536, "o", "        dictionary = np.r_[\r\n"]
[756.546, "o", "            dictionary,\r\n"]
[756.556, "o", "            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n"]
[756.566, "o", "        ]\r\n"]
[756.576, "o", "\r\n"]
[756.586, "o", "    if verbose == 1:\r\n"]
[756.596, "o", "        print(\"[dict_learning]\", end=\" \")\r\n"]
[756.606, "o", "\r\n"]
[756.616, "o", "    if shuffle:\r\n"]
[756.626, "o", "        X_train = X.copy()\r\n"]
[756.636, "o", "        random_state.shuffle(X_train)\r\n"]
[756.646, "o", "    else:\r\n"]
[756.656, "o", "        X_train = X\r\n"]
[756.666, "o", "\r\n"]
[756.676, "o", "    X_train = check_array(\r\n"]
[756.686, "o", "        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n"]
[756.696, "o", "    )\r\n"]
[756.706, "o", "\r\n"]
[756.716, "o", "    # Fortran-order dict better suited for the sparse coding which is the\r\n"]
[756.726, "o", "    # bottleneck of this algorithm.\r\n"]
[756.736, "o", "    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n"]
[756.746, "o", "    dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[756.756, "o", "\r\n"]
[756.766, "o", "    batches = gen_batches(n_samples, batch_size)\r\n"]
[756.776, "o", "    batches = itertools.cycle(batches)\r\n"]
[756.786, "o", "\r\n"]
[756.796, "o", "    # The covariance of the dictionary\r\n"]
[756.806, "o", "    if inner_stats is None:\r\n"]
[756.816, "o", "        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n"]
[756.826, "o", "        # The data approximation\r\n"]
[756.836, "o", "        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n"]
[756.846, "o", "    else:\r\n"]
[756.856, "o", "        A = inner_stats[0].copy()\r\n"]
[756.866, "o", "        B = inner_stats[1].copy()\r\n"]
[756.876, "o", "\r\n"]
[756.886, "o", "    # If n_iter is zero, we need to return zero.\r\n"]
[756.896, "o", "    ii = iter_offset - 1\r\n"]
[756.906, "o", "\r\n"]
[756.916, "o", "    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n"]
[756.926, "o", "        this_X = X_train[batch]\r\n"]
[756.936, "o", "        dt = time.time() - t0\r\n"]
[756.946, "o", "        if verbose == 1:\r\n"]
[756.956, "o", "            sys.stdout.write(\".\")\r\n"]
[756.966, "o", "            sys.stdout.flush()\r\n"]
[756.976, "o", "        elif verbose:\r\n"]
[756.986, "o", "            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n"]
[756.996, "o", "                print(\r\n"]
[757.006, "o", "                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n"]
[757.016, "o", "                )\r\n"]
[757.026, "o", "\r\n"]
[757.036, "o", "        this_code = sparse_encode(\r\n"]
[757.046, "o", "            this_X,\r\n"]
[757.056, "o", "            dictionary,\r\n"]
[757.066, "o", "            algorithm=method,\r\n"]
[757.076, "o", "            alpha=alpha,\r\n"]
[757.086, "o", "            n_jobs=n_jobs,\r\n"]
[757.096, "o", "            check_input=False,\r\n"]
[757.106, "o", "            positive=positive_code,\r\n"]
[757.116, "o", "            max_iter=method_max_iter,\r\n"]
[757.126, "o", "            verbose=verbose,\r\n"]
[757.136, "o", "        )\r\n"]
[757.146, "o", "\r\n"]
[757.156, "o", "        # Update the auxiliary variables\r\n"]
[757.166, "o", "        if ii < batch_size - 1:\r\n"]
[757.176, "o", "            theta = float((ii + 1) * batch_size)\r\n"]
[757.186, "o", "        else:\r\n"]
[757.196, "o", "            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n"]
[757.206, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[757.216, "o", "\r\n"]
[757.226, "o", "        A *= beta\r\n"]
[757.236, "o", "        A += np.dot(this_code.T, this_code)\r\n"]
[757.246, "o", "        B *= beta\r\n"]
[757.256, "o", "        B += np.dot(this_X.T, this_code)\r\n"]
[757.266, "o", "\r\n"]
[757.276, "o", "        # Update dictionary in place\r\n"]
[757.286, "o", "        _update_dict(\r\n"]
[757.296, "o", "            dictionary,\r\n"]
[757.306, "o", "            this_X,\r\n"]
[757.316, "o", "            this_code,\r\n"]
[757.326, "o", "            A,\r\n"]
[757.336, "o", "            B,\r\n"]
[757.346, "o", "            verbose=verbose,\r\n"]
[757.356, "o", "            random_state=random_state,\r\n"]
[757.366, "o", "            positive=positive_dict,\r\n"]
[757.376, "o", "        )\r\n"]
[757.386, "o", "\r\n"]
[757.396, "o", "        # Maybe we need a stopping criteria based on the amount of\r\n"]
[757.406, "o", "        # modification in the dictionary\r\n"]
[757.416, "o", "        if callback is not None:\r\n"]
[757.426, "o", "            callback(locals())\r\n"]
[757.436, "o", "\r\n"]
[757.446, "o", "    if return_inner_stats:\r\n"]
[757.456, "o", "        if return_n_iter:\r\n"]
[757.466, "o", "            return dictionary, (A, B), ii - iter_offset + 1\r\n"]
[757.476, "o", "        else:\r\n"]
[757.486, "o", "            return dictionary, (A, B)\r\n"]
[757.496, "o", "    if return_code:\r\n"]
[757.506, "o", "        if verbose > 1:\r\n"]
[757.516, "o", "            print(\"Learning code...\", end=\" \")\r\n"]
[757.526, "o", "        elif verbose == 1:\r\n"]
[757.536, "o", "            print(\"|\", end=\" \")\r\n"]
[757.546, "o", "        code = sparse_encode(\r\n"]
[757.556, "o", "            X,\r\n"]
[757.566, "o", "            dictionary,\r\n"]
[757.576, "o", "            algorithm=method,\r\n"]
[757.586, "o", "            alpha=alpha,\r\n"]
[757.596, "o", "            n_jobs=n_jobs,\r\n"]
[757.606, "o", "            check_input=False,\r\n"]
[757.616, "o", "            positive=positive_code,\r\n"]
[757.626, "o", "            max_iter=method_max_iter,\r\n"]
[757.636, "o", "            verbose=verbose,\r\n"]
[757.646, "o", "        )\r\n"]
[757.656, "o", "        if verbose > 1:\r\n"]
[757.666, "o", "            dt = time.time() - t0\r\n"]
[757.676, "o", "            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n"]
[757.686, "o", "        if return_n_iter:\r\n"]
[757.696, "o", "            return code, dictionary, ii - iter_offset + 1\r\n"]
[757.706, "o", "        else:\r\n"]
[757.716, "o", "            return code, dictionary\r\n"]
[757.726, "o", "\r\n"]
[757.736, "o", "    if return_n_iter:\r\n"]
[757.746, "o", "        return dictionary, ii - iter_offset + 1\r\n"]
[757.756, "o", "    else:\r\n"]
[757.766, "o", "        return dictionary\r\n"]
[757.776, "o", "\r\n"]
[757.786, "o", "\r\n"]
[757.796, "o", "@validate_params(\r\n"]
[757.806, "o", "    {\r\n"]
[757.816, "o", "        \"X\": [\"array-like\"],\r\n"]
[757.826, "o", "        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[757.836, "o", "        \"return_n_iter\": [\"boolean\"],\r\n"]
[757.846, "o", "        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[757.856, "o", "    },\r\n"]
[757.866, "o", "    prefer_skip_nested_validation=False,\r\n"]
[757.876, "o", ")\r\n"]
[757.886, "o", "def dict_learning(\r\n"]
[757.896, "o", "    X,\r\n"]
[757.906, "o", "    n_components,\r\n"]
[757.916, "o", "    *,\r\n"]
[757.926, "o", "    alpha,\r\n"]
[757.936, "o", "    max_iter=100,\r\n"]
[757.946, "o", "    tol=1e-8,\r\n"]
[757.956, "o", "    method=\"lars\",\r\n"]
[757.966, "o", "    n_jobs=None,\r\n"]
[757.976, "o", "    dict_init=None,\r\n"]
[757.986, "o", "    code_init=None,\r\n"]
[757.996, "o", "    callback=None,\r\n"]
[758.006, "o", "    verbose=False,\r\n"]
[758.016, "o", "    random_state=None,\r\n"]
[758.026, "o", "    return_n_iter=False,\r\n"]
[758.036, "o", "    positive_dict=False,\r\n"]
[758.046, "o", "    positive_code=False,\r\n"]
[758.056, "o", "    method_max_iter=1000,\r\n"]
[758.066, "o", "):\r\n"]
[758.076, "o", "    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n"]
[758.086, "o", "\r\n"]
[758.096, "o", "    Finds the best dictionary and the corresponding sparse code for\r\n"]
[758.106, "o", "    approximating the data matrix X by solving::\r\n"]
[758.116, "o", "\r\n"]
[758.126, "o", "        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[758.136, "o", "                     (U,V)\r\n"]
[758.146, "o", "                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n"]
[758.156, "o", "\r\n"]
[758.166, "o", "    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n"]
[758.176, "o", "    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n"]
[758.186, "o", "    which is the sum of the absolute values of all the entries in the matrix.\r\n"]
[758.196, "o", "\r\n"]
[758.206, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[758.216, "o", "\r\n"]
[758.226, "o", "    Parameters\r\n"]
[758.236, "o", "    ----------\r\n"]
[758.246, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[758.256, "o", "        Data matrix.\r\n"]
[758.266, "o", "\r\n"]
[758.276, "o", "    n_components : int\r\n"]
[758.286, "o", "        Number of dictionary atoms to extract.\r\n"]
[758.296, "o", "\r\n"]
[758.306, "o", "    alpha : int or float\r\n"]
[758.316, "o", "        Sparsity controlling parameter.\r\n"]
[758.326, "o", "\r\n"]
[758.336, "o", "    max_iter : int, default=100\r\n"]
[758.346, "o", "        Maximum number of iterations to perform.\r\n"]
[758.356, "o", "\r\n"]
[758.366, "o", "    tol : float, default=1e-8\r\n"]
[758.376, "o", "        Tolerance for the stopping condition.\r\n"]
[758.386, "o", "\r\n"]
[758.396, "o", "    method : {'lars', 'cd'}, default='lars'\r\n"]
[758.406, "o", "        The method used:\r\n"]
[758.416, "o", "\r\n"]
[758.426, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[758.436, "o", "           problem (`linear_model.lars_path`);\r\n"]
[758.446, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[758.456, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[758.466, "o", "          the estimated components are sparse.\r\n"]
[758.476, "o", "\r\n"]
[758.486, "o", "    n_jobs : int, default=None\r\n"]
[758.496, "o", "        Number of parallel jobs to run.\r\n"]
[758.506, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[758.516, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[758.526, "o", "        for more details.\r\n"]
[758.536, "o", "\r\n"]
[758.546, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[758.556, "o", "        Initial value for the dictionary for warm restart scenarios. Only used\r\n"]
[758.566, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[758.576, "o", "\r\n"]
[758.586, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[758.596, "o", "        Initial value for the sparse code for warm restart scenarios. Only used\r\n"]
[758.606, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[758.616, "o", "\r\n"]
[758.626, "o", "    callback : callable, default=None\r\n"]
[758.636, "o", "        Callable that gets invoked every five iterations.\r\n"]
[758.646, "o", "\r\n"]
[758.656, "o", "    verbose : bool, default=False\r\n"]
[758.666, "o", "        To control the verbosity of the procedure.\r\n"]
[758.676, "o", "\r\n"]
[758.686, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[758.696, "o", "        Used for randomly initializing the dictionary. Pass an int for\r\n"]
[758.706, "o", "        reproducible results across multiple function calls.\r\n"]
[758.716, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[758.726, "o", "\r\n"]
[758.736, "o", "    return_n_iter : bool, default=False\r\n"]
[758.746, "o", "        Whether or not to return the number of iterations.\r\n"]
[758.756, "o", "\r\n"]
[758.766, "o", "    positive_dict : bool, default=False\r\n"]
[758.776, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[758.786, "o", "\r\n"]
[758.796, "o", "        .. versionadded:: 0.20\r\n"]
[758.806, "o", "\r\n"]
[758.816, "o", "    positive_code : bool, default=False\r\n"]
[758.826, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[758.836, "o", "\r\n"]
[758.846, "o", "        .. versionadded:: 0.20\r\n"]
[758.856, "o", "\r\n"]
[758.866, "o", "    method_max_iter : int, default=1000\r\n"]
[758.876, "o", "        Maximum number of iterations to perform.\r\n"]
[758.886, "o", "\r\n"]
[758.896, "o", "        .. versionadded:: 0.22\r\n"]
[758.906, "o", "\r\n"]
[758.916, "o", "    Returns\r\n"]
[758.926, "o", "    -------\r\n"]
[758.936, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[758.946, "o", "        The sparse code factor in the matrix factorization.\r\n"]
[758.956, "o", "\r\n"]
[758.966, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[758.976, "o", "        The dictionary factor in the matrix factorization.\r\n"]
[758.986, "o", "\r\n"]
[758.996, "o", "    errors : array\r\n"]
[759.006, "o", "        Vector of errors at each iteration.\r\n"]
[759.016, "o", "\r\n"]
[759.026, "o", "    n_iter : int\r\n"]
[759.036, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[759.046, "o", "        set to True.\r\n"]
[759.056, "o", "\r\n"]
[759.066, "o", "    See Also\r\n"]
[759.076, "o", "    --------\r\n"]
[759.086, "o", "    dict_learning_online : Solve a dictionary learning matrix factorization\r\n"]
[759.096, "o", "        problem online.\r\n"]
[759.106, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[759.116, "o", "    MiniBatchDictionaryLearning : A faster, less accurate version\r\n"]
[759.126, "o", "        of the dictionary learning algorithm.\r\n"]
[759.136, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[759.146, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[759.156, "o", "    \"\"\"\r\n"]
[759.166, "o", "    estimator = DictionaryLearning(\r\n"]
[759.176, "o", "        n_components=n_components,\r\n"]
[759.186, "o", "        alpha=alpha,\r\n"]
[759.196, "o", "        max_iter=max_iter,\r\n"]
[759.206, "o", "        tol=tol,\r\n"]
[759.216, "o", "        fit_algorithm=method,\r\n"]
[759.226, "o", "        n_jobs=n_jobs,\r\n"]
[759.236, "o", "        dict_init=dict_init,\r\n"]
[759.246, "o", "        callback=callback,\r\n"]
[759.256, "o", "        code_init=code_init,\r\n"]
[759.266, "o", "        verbose=verbose,\r\n"]
[759.276, "o", "        random_state=random_state,\r\n"]
[759.286, "o", "        positive_code=positive_code,\r\n"]
[759.296, "o", "        positive_dict=positive_dict,\r\n"]
[759.306, "o", "        transform_max_iter=method_max_iter,\r\n"]
[759.316, "o", "    )\r\n"]
[759.326, "o", "    code = estimator.fit_transform(X)\r\n"]
[759.336, "o", "    if return_n_iter:\r\n"]
[759.346, "o", "        return (\r\n"]
[759.356, "o", "            code,\r\n"]
[759.366, "o", "            estimator.components_,\r\n"]
[759.376, "o", "            estimator.error_,\r\n"]
[759.386, "o", "            estimator.n_iter_,\r\n"]
[759.396, "o", "        )\r\n"]
[759.406, "o", "    return code, estimator.components_, estimator.error_\r\n"]
[759.416, "o", "\r\n"]
[759.426, "o", "\r\n"]
[759.436, "o", "class _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n"]
[759.446, "o", "    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n"]
[759.456, "o", "\r\n"]
[759.466, "o", "    def __init__(\r\n"]
[759.476, "o", "        self,\r\n"]
[759.486, "o", "        transform_algorithm,\r\n"]
[759.496, "o", "        transform_n_nonzero_coefs,\r\n"]
[759.506, "o", "        transform_alpha,\r\n"]
[759.516, "o", "        split_sign,\r\n"]
[759.526, "o", "        n_jobs,\r\n"]
[759.536, "o", "        positive_code,\r\n"]
[759.546, "o", "        transform_max_iter,\r\n"]
[759.556, "o", "    ):\r\n"]
[759.566, "o", "        self.transform_algorithm = transform_algorithm\r\n"]
[759.576, "o", "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n"]
[759.586, "o", "        self.transform_alpha = transform_alpha\r\n"]
[759.596, "o", "        self.transform_max_iter = transform_max_iter\r\n"]
[759.606, "o", "        self.split_sign = split_sign\r\n"]
[759.616, "o", "        self.n_jobs = n_jobs\r\n"]
[759.626, "o", "        self.positive_code = positive_code\r\n"]
[759.636, "o", "\r\n"]
[759.646, "o", "    def _transform(self, X, dictionary):\r\n"]
[759.656, "o", "        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n"]
[759.666, "o", "        SparseCoder.\"\"\"\r\n"]
[759.676, "o", "        X = self._validate_data(X, reset=False)\r\n"]
[759.686, "o", "\r\n"]
[759.696, "o", "        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n"]
[759.706, "o", "            transform_alpha = self.alpha\r\n"]
[759.716, "o", "        else:\r\n"]
[759.726, "o", "            transform_alpha = self.transform_alpha\r\n"]
[759.736, "o", "\r\n"]
[759.746, "o", "        code = sparse_encode(\r\n"]
[759.756, "o", "            X,\r\n"]
[759.766, "o", "            dictionary,\r\n"]
[759.776, "o", "            algorithm=self.transform_algorithm,\r\n"]
[759.786, "o", "            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n"]
[759.796, "o", "            alpha=transform_alpha,\r\n"]
[759.806, "o", "            max_iter=self.transform_max_iter,\r\n"]
[759.816, "o", "            n_jobs=self.n_jobs,\r\n"]
[759.826, "o", "            positive=self.positive_code,\r\n"]
[759.836, "o", "        )\r\n"]
[759.846, "o", "\r\n"]
[759.856, "o", "        if self.split_sign:\r\n"]
[759.866, "o", "            # feature vector is split into a positive and negative side\r\n"]
[759.876, "o", "            n_samples, n_features = code.shape\r\n"]
[759.886, "o", "            split_code = np.empty((n_samples, 2 * n_features))\r\n"]
[759.896, "o", "            split_code[:, :n_features] = np.maximum(code, 0)\r\n"]
[759.906, "o", "            split_code[:, n_features:] = -np.minimum(code, 0)\r\n"]
[759.916, "o", "            code = split_code\r\n"]
[759.926, "o", "\r\n"]
[759.936, "o", "        return code\r\n"]
[760.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[760.002, "i", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py\r"]
[760.004, "o", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py\r\n"]
[760.016, "o", "\u001b[?2004l\r\n"]
[760.026, "o", "\r\n"]
[760.036, "o", "    n_jobs : int or None, default=None\r\n"]
[760.046, "o", "        Number of parallel jobs to run.\r\n"]
[760.056, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[760.066, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[760.076, "o", "        for more details.\r\n"]
[760.086, "o", "\r\n"]
[760.096, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[760.106, "o", "        Initial value for the code, for warm restart. Only used if `code_init`\r\n"]
[760.116, "o", "        and `dict_init` are not None.\r\n"]
[760.126, "o", "\r\n"]
[760.136, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[760.146, "o", "        Initial values for the dictionary, for warm restart. Only used if\r\n"]
[760.156, "o", "        `code_init` and `dict_init` are not None.\r\n"]
[760.166, "o", "\r\n"]
[760.176, "o", "    callback : callable, default=None\r\n"]
[760.186, "o", "        Callable that gets invoked every five iterations.\r\n"]
[760.196, "o", "\r\n"]
[760.206, "o", "        .. versionadded:: 1.3\r\n"]
[760.216, "o", "\r\n"]
[760.226, "o", "    verbose : bool, default=False\r\n"]
[760.236, "o", "        To control the verbosity of the procedure.\r\n"]
[760.246, "o", "\r\n"]
[760.256, "o", "    split_sign : bool, default=False\r\n"]
[760.266, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[760.276, "o", "        its negative part and its positive part. This can improve the\r\n"]
[760.286, "o", "        performance of downstream classifiers.\r\n"]
[760.296, "o", "\r\n"]
[760.306, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[760.316, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[760.326, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[760.336, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[760.346, "o", "        results across multiple function calls.\r\n"]
[760.356, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[760.366, "o", "\r\n"]
[760.376, "o", "    positive_code : bool, default=False\r\n"]
[760.386, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[760.396, "o", "\r\n"]
[760.406, "o", "        .. versionadded:: 0.20\r\n"]
[760.416, "o", "\r\n"]
[760.426, "o", "    positive_dict : bool, default=False\r\n"]
[760.436, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[760.446, "o", "\r\n"]
[760.456, "o", "        .. versionadded:: 0.20\r\n"]
[760.466, "o", "\r\n"]
[760.476, "o", "    transform_max_iter : int, default=1000\r\n"]
[760.486, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[760.496, "o", "        `'lasso_lars'`.\r\n"]
[760.506, "o", "\r\n"]
[760.516, "o", "        .. versionadded:: 0.22\r\n"]
[760.526, "o", "\r\n"]
[760.536, "o", "    Attributes\r\n"]
[760.546, "o", "    ----------\r\n"]
[760.556, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[760.566, "o", "        dictionary atoms extracted from the data\r\n"]
[760.576, "o", "\r\n"]
[760.586, "o", "    error_ : array\r\n"]
[760.596, "o", "        vector of errors at each iteration\r\n"]
[760.606, "o", "\r\n"]
[760.616, "o", "    n_features_in_ : int\r\n"]
[760.626, "o", "        Number of features seen during :term:`fit`.\r\n"]
[760.636, "o", "\r\n"]
[760.646, "o", "        .. versionadded:: 0.24\r\n"]
[760.656, "o", "\r\n"]
[760.666, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[760.676, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[760.686, "o", "        has feature names that are all strings.\r\n"]
[760.696, "o", "\r\n"]
[760.706, "o", "        .. versionadded:: 1.0\r\n"]
[760.716, "o", "\r\n"]
[760.726, "o", "    n_iter_ : int\r\n"]
[760.736, "o", "        Number of iterations run.\r\n"]
[760.746, "o", "\r\n"]
[760.756, "o", "    See Also\r\n"]
[760.766, "o", "    --------\r\n"]
[760.776, "o", "    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n"]
[760.786, "o", "        dictionary learning algorithm.\r\n"]
[760.796, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[760.806, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[760.816, "o", "        precomputed dictionary.\r\n"]
[760.826, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[760.836, "o", "\r\n"]
[760.846, "o", "    References\r\n"]
[760.856, "o", "    ----------\r\n"]
[760.866, "o", "\r\n"]
[760.876, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[760.886, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[760.896, "o", "\r\n"]
[760.906, "o", "    Examples\r\n"]
[760.916, "o", "    --------\r\n"]
[760.926, "o", "    >>> import numpy as np\r\n"]
[760.936, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[760.946, "o", "    >>> from sklearn.decomposition import DictionaryLearning\r\n"]
[760.956, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[760.966, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[760.976, "o", "    ...     random_state=42,\r\n"]
[760.986, "o", "    ... )\r\n"]
[760.996, "o", "    >>> dict_learner = DictionaryLearning(\r\n"]
[761.006, "o", "    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n"]
[761.016, "o", "    ...     random_state=42,\r\n"]
[761.026, "o", "    ... )\r\n"]
[761.036, "o", "    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n"]
[761.046, "o", "\r\n"]
[761.056, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[761.066, "o", "\r\n"]
[761.076, "o", "    >>> np.mean(X_transformed == 0)\r\n"]
[761.086, "o", "    0.41...\r\n"]
[761.096, "o", "\r\n"]
[761.106, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[761.116, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[761.126, "o", "    the original signal:\r\n"]
[761.136, "o", "\r\n"]
[761.146, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[761.156, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[761.166, "o", "    0.07...\r\n"]
[761.176, "o", "    \"\"\"\r\n"]
[761.186, "o", "\r\n"]
[761.196, "o", "    _parameter_constraints: dict = {\r\n"]
[761.206, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[761.216, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[761.226, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[761.236, "o", "        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[761.246, "o", "        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[761.256, "o", "        \"transform_algorithm\": [\r\n"]
[761.266, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[761.276, "o", "        ],\r\n"]
[761.286, "o", "        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[761.296, "o", "        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[761.306, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[761.316, "o", "        \"code_init\": [np.ndarray, None],\r\n"]
[761.326, "o", "        \"dict_init\": [np.ndarray, None],\r\n"]
[761.336, "o", "        \"callback\": [callable, None],\r\n"]
[761.346, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[761.356, "o", "        \"split_sign\": [\"boolean\"],\r\n"]
[761.366, "o", "        \"random_state\": [\"random_state\"],\r\n"]
[761.376, "o", "        \"positive_code\": [\"boolean\"],\r\n"]
[761.386, "o", "        \"positive_dict\": [\"boolean\"],\r\n"]
[761.396, "o", "        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[761.406, "o", "    }\r\n"]
[761.416, "o", "\r\n"]
[761.426, "o", "    def __init__(\r\n"]
[761.436, "o", "        self,\r\n"]
[761.446, "o", "        n_components=None,\r\n"]
[761.456, "o", "        *,\r\n"]
[761.466, "o", "        alpha=1,\r\n"]
[761.476, "o", "        max_iter=1000,\r\n"]
[761.486, "o", "        tol=1e-8,\r\n"]
[761.496, "o", "        fit_algorithm=\"lars\",\r\n"]
[761.506, "o", "        transform_algorithm=\"omp\",\r\n"]
[761.516, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[761.526, "o", "        transform_alpha=None,\r\n"]
[761.536, "o", "        n_jobs=None,\r\n"]
[761.546, "o", "        code_init=None,\r\n"]
[761.556, "o", "        dict_init=None,\r\n"]
[761.566, "o", "        callback=None,\r\n"]
[761.576, "o", "        verbose=False,\r\n"]
[761.586, "o", "        split_sign=False,\r\n"]
[761.596, "o", "        random_state=None,\r\n"]
[761.606, "o", "        positive_code=False,\r\n"]
[761.616, "o", "        positive_dict=False,\r\n"]
[761.626, "o", "        transform_max_iter=1000,\r\n"]
[761.636, "o", "    ):\r\n"]
[761.646, "o", "        super().__init__(\r\n"]
[761.656, "o", "            transform_algorithm,\r\n"]
[761.666, "o", "            transform_n_nonzero_coefs,\r\n"]
[761.676, "o", "            transform_alpha,\r\n"]
[761.686, "o", "            split_sign,\r\n"]
[761.696, "o", "            n_jobs,\r\n"]
[761.706, "o", "            positive_code,\r\n"]
[761.716, "o", "            transform_max_iter,\r\n"]
[761.726, "o", "        )\r\n"]
[761.736, "o", "        self.n_components = n_components\r\n"]
[761.746, "o", "        self.alpha = alpha\r\n"]
[761.756, "o", "        self.max_iter = max_iter\r\n"]
[761.766, "o", "        self.tol = tol\r\n"]
[761.776, "o", "        self.fit_algorithm = fit_algorithm\r\n"]
[761.786, "o", "        self.code_init = code_init\r\n"]
[761.796, "o", "        self.dict_init = dict_init\r\n"]
[761.806, "o", "        self.callback = callback\r\n"]
[761.816, "o", "        self.verbose = verbose\r\n"]
[761.826, "o", "        self.random_state = random_state\r\n"]
[761.836, "o", "        self.positive_dict = positive_dict\r\n"]
[761.846, "o", "\r\n"]
[761.856, "o", "    def fit(self, X, y=None):\r\n"]
[761.866, "o", "        \"\"\"Fit the model from data in X.\r\n"]
[761.876, "o", "\r\n"]
[761.886, "o", "        Parameters\r\n"]
[761.896, "o", "        ----------\r\n"]
[761.906, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[761.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[761.926, "o", "            and `n_features` is the number of features.\r\n"]
[761.936, "o", "\r\n"]
[761.946, "o", "        y : Ignored\r\n"]
[761.956, "o", "            Not used, present for API consistency by convention.\r\n"]
[761.966, "o", "\r\n"]
[761.976, "o", "        Returns\r\n"]
[761.986, "o", "        -------\r\n"]
[761.996, "o", "        self : object\r\n"]
[762.006, "o", "            Returns the instance itself.\r\n"]
[762.016, "o", "        \"\"\"\r\n"]
[762.026, "o", "        self.fit_transform(X)\r\n"]
[762.036, "o", "        return self\r\n"]
[762.046, "o", "\r\n"]
[762.056, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[762.066, "o", "    def fit_transform(self, X, y=None):\r\n"]
[762.076, "o", "        \"\"\"Fit the model from data in X and return the transformed data.\r\n"]
[762.086, "o", "\r\n"]
[762.096, "o", "        Parameters\r\n"]
[762.106, "o", "        ----------\r\n"]
[762.116, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[762.126, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[762.136, "o", "            and `n_features` is the number of features.\r\n"]
[762.146, "o", "\r\n"]
[762.156, "o", "        y : Ignored\r\n"]
[762.166, "o", "            Not used, present for API consistency by convention.\r\n"]
[762.176, "o", "\r\n"]
[762.186, "o", "        Returns\r\n"]
[762.196, "o", "        -------\r\n"]
[762.206, "o", "        V : ndarray of shape (n_samples, n_components)\r\n"]
[762.216, "o", "            Transformed data.\r\n"]
[762.226, "o", "        \"\"\"\r\n"]
[762.236, "o", "        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n"]
[762.246, "o", "\r\n"]
[762.256, "o", "        method = \"lasso_\" + self.fit_algorithm\r\n"]
[762.266, "o", "\r\n"]
[762.276, "o", "        random_state = check_random_state(self.random_state)\r\n"]
[762.286, "o", "        X = self._validate_data(X)\r\n"]
[762.296, "o", "\r\n"]
[762.306, "o", "        if self.n_components is None:\r\n"]
[762.316, "o", "            n_components = X.shape[1]\r\n"]
[762.326, "o", "        else:\r\n"]
[762.336, "o", "            n_components = self.n_components\r\n"]
[762.346, "o", "\r\n"]
[762.356, "o", "        V, U, E, self.n_iter_ = _dict_learning(\r\n"]
[762.366, "o", "            X,\r\n"]
[762.376, "o", "            n_components,\r\n"]
[762.386, "o", "            alpha=self.alpha,\r\n"]
[762.396, "o", "            tol=self.tol,\r\n"]
[762.406, "o", "            max_iter=self.max_iter,\r\n"]
[762.416, "o", "            method=method,\r\n"]
[762.426, "o", "            method_max_iter=self.transform_max_iter,\r\n"]
[762.436, "o", "            n_jobs=self.n_jobs,\r\n"]
[762.446, "o", "            code_init=self.code_init,\r\n"]
[762.456, "o", "            dict_init=self.dict_init,\r\n"]
[762.466, "o", "            callback=self.callback,\r\n"]
[762.476, "o", "            verbose=self.verbose,\r\n"]
[762.486, "o", "            random_state=random_state,\r\n"]
[762.496, "o", "            return_n_iter=True,\r\n"]
[762.506, "o", "            positive_dict=self.positive_dict,\r\n"]
[762.516, "o", "            positive_code=self.positive_code,\r\n"]
[762.526, "o", "        )\r\n"]
[762.536, "o", "        self.components_ = U\r\n"]
[762.546, "o", "        self.error_ = E\r\n"]
[762.556, "o", "\r\n"]
[762.566, "o", "        return V\r\n"]
[762.576, "o", "\r\n"]
[762.586, "o", "    @property\r\n"]
[762.596, "o", "    def _n_features_out(self):\r\n"]
[762.606, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[762.616, "o", "        return self.components_.shape[0]\r\n"]
[762.626, "o", "\r\n"]
[762.636, "o", "    def _more_tags(self):\r\n"]
[762.646, "o", "        return {\r\n"]
[762.656, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[762.666, "o", "        }\r\n"]
[762.676, "o", "\r\n"]
[762.686, "o", "\r\n"]
[762.696, "o", "class MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[762.706, "o", "    \"\"\"Mini-batch dictionary learning.\r\n"]
[762.716, "o", "\r\n"]
[762.726, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[762.736, "o", "    encoding the fitted data.\r\n"]
[762.746, "o", "\r\n"]
[762.756, "o", "    Solves the optimization problem::\r\n"]
[762.766, "o", "\r\n"]
[762.776, "o", "       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[762.786, "o", "                    (U,V)\r\n"]
[762.796, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[762.806, "o", "\r\n"]
[762.816, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[762.826, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[762.836, "o", "    of all the entries in the matrix.\r\n"]
[762.846, "o", "\r\n"]
[762.856, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[762.866, "o", "\r\n"]
[762.876, "o", "    Parameters\r\n"]
[762.886, "o", "    ----------\r\n"]
[762.896, "o", "    n_components : int, default=None\r\n"]
[762.906, "o", "        Number of dictionary elements to extract.\r\n"]
[762.916, "o", "\r\n"]
[762.926, "o", "    alpha : float, default=1\r\n"]
[762.936, "o", "        Sparsity controlling parameter.\r\n"]
[762.946, "o", "\r\n"]
[762.956, "o", "    n_iter : int, default=1000\r\n"]
[762.966, "o", "        Total number of iterations over data batches to perform.\r\n"]
[762.976, "o", "\r\n"]
[762.986, "o", "        .. deprecated:: 1.1\r\n"]
[762.996, "o", "           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[763.006, "o", "           ``max_iter`` instead.\r\n"]
[763.016, "o", "\r\n"]
[763.026, "o", "    max_iter : int, default=None\r\n"]
[763.036, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[763.046, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[763.056, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[763.066, "o", "\r\n"]
[763.076, "o", "        .. versionadded:: 1.1\r\n"]
[763.086, "o", "\r\n"]
[763.096, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[763.106, "o", "        The algorithm used:\r\n"]
[763.116, "o", "\r\n"]
[763.126, "o", "        - `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[763.136, "o", "          problem (`linear_model.lars_path`)\r\n"]
[763.146, "o", "        - `'cd'`: uses the coordinate descent method to compute the\r\n"]
[763.156, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[763.166, "o", "          the estimated components are sparse.\r\n"]
[763.176, "o", "\r\n"]
[763.186, "o", "    n_jobs : int, default=None\r\n"]
[763.196, "o", "        Number of parallel jobs to run.\r\n"]
[763.206, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[763.216, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[763.226, "o", "        for more details.\r\n"]
[763.236, "o", "\r\n"]
[763.246, "o", "    batch_size : int, default=256\r\n"]
[763.256, "o", "        Number of samples in each mini-batch.\r\n"]
[763.266, "o", "\r\n"]
[763.276, "o", "        .. versionchanged:: 1.3\r\n"]
[763.286, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[763.296, "o", "\r\n"]
[763.306, "o", "    shuffle : bool, default=True\r\n"]
[763.316, "o", "        Whether to shuffle the samples before forming batches.\r\n"]
[763.326, "o", "\r\n"]
[763.336, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[763.346, "o", "        Initial value of the dictionary for warm restart scenarios.\r\n"]
[763.356, "o", "\r\n"]
[763.366, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[763.376, "o", "            'threshold'}, default='omp'\r\n"]
[763.386, "o", "        Algorithm used to transform the data:\r\n"]
[763.396, "o", "\r\n"]
[763.406, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[763.416, "o", "          (`linear_model.lars_path`);\r\n"]
[763.426, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[763.436, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[763.446, "o", "          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n"]
[763.456, "o", "          if the estimated components are sparse.\r\n"]
[763.466, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[763.476, "o", "          solution.\r\n"]
[763.486, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[763.496, "o", "          the projection ``dictionary * X'``.\r\n"]
[763.506, "o", "\r\n"]
[763.516, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[763.526, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[763.536, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[763.546, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[763.556, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[763.566, "o", "\r\n"]
[763.576, "o", "    transform_alpha : float, default=None\r\n"]
[763.586, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[763.596, "o", "        penalty applied to the L1 norm.\r\n"]
[763.606, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[763.616, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[763.626, "o", "        If `None`, defaults to `alpha`.\r\n"]
[763.636, "o", "\r\n"]
[763.646, "o", "        .. versionchanged:: 1.2\r\n"]
[763.656, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[763.666, "o", "\r\n"]
[763.676, "o", "    verbose : bool or int, default=False\r\n"]
[763.686, "o", "        To control the verbosity of the procedure.\r\n"]
[763.696, "o", "\r\n"]
[763.706, "o", "    split_sign : bool, default=False\r\n"]
[763.716, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[763.726, "o", "        its negative part and its positive part. This can improve the\r\n"]
[763.736, "o", "        performance of downstream classifiers.\r\n"]
[763.746, "o", "\r\n"]
[763.756, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[763.766, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[763.776, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[763.786, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[763.796, "o", "        results across multiple function calls.\r\n"]
[763.806, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[763.816, "o", "\r\n"]
[763.826, "o", "    positive_code : bool, default=False\r\n"]
[763.836, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[763.846, "o", "\r\n"]
[763.856, "o", "        .. versionadded:: 0.20\r\n"]
[763.866, "o", "\r\n"]
[763.876, "o", "    positive_dict : bool, default=False\r\n"]
[763.886, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[763.896, "o", "\r\n"]
[763.906, "o", "        .. versionadded:: 0.20\r\n"]
[763.916, "o", "\r\n"]
[763.926, "o", "    transform_max_iter : int, default=1000\r\n"]
[763.936, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[763.946, "o", "        `'lasso_lars'`.\r\n"]
[763.956, "o", "\r\n"]
[763.966, "o", "        .. versionadded:: 0.22\r\n"]
[763.976, "o", "\r\n"]
[763.986, "o", "    callback : callable, default=None\r\n"]
[763.996, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[764.006, "o", "\r\n"]
[764.016, "o", "        .. versionadded:: 1.1\r\n"]
[764.026, "o", "\r\n"]
[764.036, "o", "    tol : float, default=1e-3\r\n"]
[764.046, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[764.056, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[764.066, "o", "\r\n"]
[764.076, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[764.086, "o", "        `tol` to 0.0.\r\n"]
[764.096, "o", "\r\n"]
[764.106, "o", "        .. versionadded:: 1.1\r\n"]
[764.116, "o", "\r\n"]
[764.126, "o", "    max_no_improvement : int, default=10\r\n"]
[764.136, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[764.146, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[764.156, "o", "        `max_iter` is not None.\r\n"]
[764.166, "o", "\r\n"]
[764.176, "o", "        To disable convergence detection based on cost function, set\r\n"]
[764.186, "o", "        `max_no_improvement` to None.\r\n"]
[764.196, "o", "\r\n"]
[764.206, "o", "        .. versionadded:: 1.1\r\n"]
[764.216, "o", "\r\n"]
[764.226, "o", "    Attributes\r\n"]
[764.236, "o", "    ----------\r\n"]
[764.246, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[764.256, "o", "        Components extracted from the data.\r\n"]
[764.266, "o", "\r\n"]
[764.276, "o", "    n_features_in_ : int\r\n"]
[764.286, "o", "        Number of features seen during :term:`fit`.\r\n"]
[764.296, "o", "\r\n"]
[764.306, "o", "        .. versionadded:: 0.24\r\n"]
[764.316, "o", "\r\n"]
[764.326, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[764.336, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[764.346, "o", "        has feature names that are all strings.\r\n"]
[764.356, "o", "\r\n"]
[764.366, "o", "        .. versionadded:: 1.0\r\n"]
[764.376, "o", "\r\n"]
[764.386, "o", "    n_iter_ : int\r\n"]
[764.396, "o", "        Number of iterations over the full dataset.\r\n"]
[764.406, "o", "\r\n"]
[764.416, "o", "    n_steps_ : int\r\n"]
[764.426, "o", "        Number of mini-batches processed.\r\n"]
[764.436, "o", "\r\n"]
[764.446, "o", "        .. versionadded:: 1.1\r\n"]
[764.456, "o", "\r\n"]
[764.466, "o", "    See Also\r\n"]
[764.476, "o", "    --------\r\n"]
[764.486, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[764.496, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[764.506, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[764.516, "o", "        precomputed dictionary.\r\n"]
[764.526, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[764.536, "o", "\r\n"]
[764.546, "o", "    References\r\n"]
[764.556, "o", "    ----------\r\n"]
[764.566, "o", "\r\n"]
[764.576, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[764.586, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[764.596, "o", "\r\n"]
[764.606, "o", "    Examples\r\n"]
[764.616, "o", "    --------\r\n"]
[764.626, "o", "    >>> import numpy as np\r\n"]
[764.636, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[764.646, "o", "    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n"]
[764.656, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[764.666, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[764.676, "o", "    ...     random_state=42)\r\n"]
[764.686, "o", "    >>> dict_learner = MiniBatchDictionaryLearning(\r\n"]
[764.696, "o", "    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n"]
[764.706, "o", "    ...     transform_alpha=0.1, random_state=42)\r\n"]
[764.716, "o", "    >>> X_transformed = dict_learner.fit_transform(X)\r\n"]
[764.726, "o", "\r\n"]
[764.736, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[764.746, "o", "\r\n"]
[764.756, "o", "    >>> np.mean(X_transformed == 0) < 0.5\r\n"]
[764.766, "o", "    True\r\n"]
[764.776, "o", "\r\n"]
[764.786, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[764.796, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[764.806, "o", "    the original signal:\r\n"]
[764.816, "o", "\r\n"]
[764.826, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[764.836, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[764.846, "o", "    0.057...\r\n"]
[764.856, "o", "    \"\"\"\r\n"]
[764.866, "o", "\r\n"]
[764.876, "o", "    _parameter_constraints: dict = {\r\n"]
[764.886, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[764.896, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[764.906, "o", "        \"n_iter\": [\r\n"]
[764.916, "o", "            Interval(Integral, 0, None, closed=\"left\"),\r\n"]
[764.926, "o", "            Hidden(StrOptions({\"deprecated\"})),\r\n"]
[764.936, "o", "        ],\r\n"]
[765.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[765.002, "i", "cd asv_benchmarks\r"]
[765.004, "o", "cd asv_benchmarks\r\n"]
[765.006, "o", "\u001b[?2004l\r\n"]
[770.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[770.002, "i", "asv dev -l | grep -n \"MiniBatchDictionaryLearningBenchmark\" || asv dev -l | head -n 200\r"]
[770.004, "o", "asv dev -l | grep -n \"MiniBatchDictionaryLearningBenchmark\" || asv dev -l | head -n 200\r\n"]
[770.555333, "o", "\u001b[?2004l\r\n"]
[771.104667, "o", "usage: asv [-h] [--verbose] [--config CONFIG] [--version]\r\n"]
[771.654, "o", "           {help,quickstart,machine,setup,run,continuous,find,rm,publish,preview,profile,update,show,compare,check,gh-pages}\r\n"]
[772.203333, "o", "           ...\r\n"]
[772.752667, "o", "asv: error: argument {help,quickstart,machine,setup,run,continuous,find,rm,publish,preview,profile,update,show,compare,check,gh-pages}: invalid choice: 'dev' (choose from 'help', 'quickstart', 'machine', 'setup', 'run', 'continuous', 'find', 'rm', 'publish', 'preview', 'profile', 'update', 'show', 'compare', 'check', 'gh-pages')\r\n"]
[773.302, "o", "usage: asv [-h] [--verbose] [--config CONFIG] [--version]\r\n"]
[773.851333, "o", "           {help,quickstart,machine,setup,run,continuous,find,rm,publish,preview,profile,update,show,compare,check,gh-pages}\r\n"]
[774.400667, "o", "           ...\r\n"]
[775.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[775.002, "i", "asv profile --python=same benchmarks.decomposition.MiniBatchDictionaryLearningBenchmark.time_fit || \r"]
[775.004, "o", "asv profile --python=same benchmarks.decomposition.MiniBatchDictionaryLearningBenchmark.time_fit || \r\n"]
[775.83, "o", " asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit || true\r\n"]
[776.654, "o", "\u001b[?2004l\r\n"]
[777.478, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[778.302, "o", "\u00b7 \u001b[0;31m'benchmarks.decomposition.MiniBatchDictionaryLearningBenchmark.time_fit' benchmark not found\u001b[0m\r\n"]
[779.126, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[780.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[780.002, "i", "cd ..\r"]
[780.004, "o", "cd ..\r\n"]
[780.006, "o", "\u001b[?2004l\r\n"]
[785.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[785.002, "i", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py\r"]
[785.004, "o", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py\r\n"]
[787.478, "o", "\u001b[?2004l\r\n"]
[790.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[790.002, "i", "grep -n \"_sparse_encode_precomputed\" sklearn/decomposition/_dict_learning.py\r"]
[790.004, "o", "grep -n \"_sparse_encode_precomputed\" sklearn/decomposition/_dict_learning.py\r\n"]
[791.242, "o", "\u001b[?2004l\r\n"]
[792.478, "o", "\u001b[32m\u001b[K38\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdef \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n"]
[793.714, "o", "\u001b[32m\u001b[K416\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        code = \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n"]
[795.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[795.002, "i", "sed -n '1,400p' sklearn/decomposition/_dict_learning.py\r"]
[795.004, "o", "sed -n '1,400p' sklearn/decomposition/_dict_learning.py\r\n"]
[795.018329, "o", "\u001b[?2004l\r\n"]
[795.030658, "o", "\"\"\" Dictionary learning.\r\n"]
[795.042988, "o", "\"\"\"\r\n"]
[795.055317, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[795.067646, "o", "# License: BSD 3 clause\r\n"]
[795.079975, "o", "\r\n"]
[795.092304, "o", "import itertools\r\n"]
[795.104633, "o", "import sys\r\n"]
[795.116963, "o", "import time\r\n"]
[795.129292, "o", "import warnings\r\n"]
[795.141621, "o", "from math import ceil\r\n"]
[795.15395, "o", "from numbers import Integral, Real\r\n"]
[795.166279, "o", "\r\n"]
[795.178608, "o", "import numpy as np\r\n"]
[795.190938, "o", "from joblib import effective_n_jobs\r\n"]
[795.203267, "o", "from scipy import linalg\r\n"]
[795.215596, "o", "\r\n"]
[795.227925, "o", "from ..base import (\r\n"]
[795.240254, "o", "    BaseEstimator,\r\n"]
[795.252584, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[795.264913, "o", "    TransformerMixin,\r\n"]
[795.277242, "o", "    _fit_context,\r\n"]
[795.289571, "o", ")\r\n"]
[795.3019, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[795.314229, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[795.326559, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[795.338888, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[795.351217, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[795.363546, "o", "from ..utils.validation import check_is_fitted\r\n"]
[795.375875, "o", "\r\n"]
[795.388204, "o", "\r\n"]
[795.400534, "o", "def _check_positive_coding(method, positive):\r\n"]
[795.412863, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[795.425192, "o", "        raise ValueError(\r\n"]
[795.437521, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[795.44985, "o", "        )\r\n"]
[795.46218, "o", "\r\n"]
[795.474509, "o", "\r\n"]
[795.486838, "o", "def _sparse_encode_precomputed(\r\n"]
[795.499167, "o", "    X,\r\n"]
[795.511496, "o", "    dictionary,\r\n"]
[795.523825, "o", "    *,\r\n"]
[795.536155, "o", "    gram=None,\r\n"]
[795.548484, "o", "    cov=None,\r\n"]
[795.560813, "o", "    algorithm=\"lasso_lars\",\r\n"]
[795.573142, "o", "    regularization=None,\r\n"]
[795.585471, "o", "    copy_cov=True,\r\n"]
[795.5978, "o", "    init=None,\r\n"]
[795.61013, "o", "    max_iter=1000,\r\n"]
[795.622459, "o", "    verbose=0,\r\n"]
[795.634788, "o", "    positive=False,\r\n"]
[795.647117, "o", "):\r\n"]
[795.659446, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[795.671776, "o", "\r\n"]
[795.684105, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[795.696434, "o", "\r\n"]
[795.708763, "o", "    Parameters\r\n"]
[795.721092, "o", "    ----------\r\n"]
[795.733421, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[795.745751, "o", "        Data matrix.\r\n"]
[795.75808, "o", "\r\n"]
[795.770409, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[795.782738, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[795.795067, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[795.807397, "o", "\r\n"]
[795.819726, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[795.832055, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[795.844384, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[795.856713, "o", "\r\n"]
[795.869042, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[795.881372, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[795.893701, "o", "\r\n"]
[795.90603, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[795.918359, "o", "            default='lasso_lars'\r\n"]
[795.930688, "o", "        The algorithm used:\r\n"]
[795.943017, "o", "\r\n"]
[795.955347, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[795.967676, "o", "          (`linear_model.lars_path`);\r\n"]
[795.980005, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[795.992334, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[796.004663, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[796.016993, "o", "          the estimated components are sparse;\r\n"]
[796.029322, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[796.041651, "o", "          solution;\r\n"]
[796.05398, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[796.066309, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[796.078638, "o", "\r\n"]
[796.090968, "o", "    regularization : int or float, default=None\r\n"]
[796.103297, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[796.115626, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[796.127955, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[796.140284, "o", "\r\n"]
[796.152613, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[796.164943, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[796.177272, "o", "        `algorithm='lasso_cd'`.\r\n"]
[796.189601, "o", "\r\n"]
[796.20193, "o", "    max_iter : int, default=1000\r\n"]
[796.214259, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[796.226589, "o", "        `'lasso_lars'`.\r\n"]
[796.238918, "o", "\r\n"]
[796.251247, "o", "    copy_cov : bool, default=True\r\n"]
[796.263576, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[796.275905, "o", "        be overwritten.\r\n"]
[796.288234, "o", "\r\n"]
[796.300564, "o", "    verbose : int, default=0\r\n"]
[796.312893, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[796.325222, "o", "\r\n"]
[796.337551, "o", "    positive: bool, default=False\r\n"]
[796.34988, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[796.362209, "o", "\r\n"]
[796.374539, "o", "        .. versionadded:: 0.20\r\n"]
[796.386868, "o", "\r\n"]
[796.399197, "o", "    Returns\r\n"]
[796.411526, "o", "    -------\r\n"]
[796.423855, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[796.436185, "o", "        The sparse codes.\r\n"]
[796.448514, "o", "    \"\"\"\r\n"]
[796.460843, "o", "    n_samples, n_features = X.shape\r\n"]
[796.473172, "o", "    n_components = dictionary.shape[0]\r\n"]
[796.485501, "o", "\r\n"]
[796.49783, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[796.51016, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[796.522489, "o", "        try:\r\n"]
[796.534818, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[796.547147, "o", "\r\n"]
[796.559476, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[796.571805, "o", "            # corrects the verbosity level.\r\n"]
[796.584135, "o", "            lasso_lars = LassoLars(\r\n"]
[796.596464, "o", "                alpha=alpha,\r\n"]
[796.608793, "o", "                fit_intercept=False,\r\n"]
[796.621122, "o", "                verbose=verbose,\r\n"]
[796.633451, "o", "                precompute=gram,\r\n"]
[796.645781, "o", "                fit_path=False,\r\n"]
[796.65811, "o", "                positive=positive,\r\n"]
[796.670439, "o", "                max_iter=max_iter,\r\n"]
[796.682768, "o", "            )\r\n"]
[796.695097, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[796.707426, "o", "            new_code = lasso_lars.coef_\r\n"]
[796.719756, "o", "        finally:\r\n"]
[796.732085, "o", "            np.seterr(**err_mgt)\r\n"]
[796.744414, "o", "\r\n"]
[796.756743, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[796.769072, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[796.781401, "o", "\r\n"]
[796.793731, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[796.80606, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[796.818389, "o", "        # argument that we could pass in from Lasso.\r\n"]
[796.830718, "o", "        clf = Lasso(\r\n"]
[796.843047, "o", "            alpha=alpha,\r\n"]
[796.855377, "o", "            fit_intercept=False,\r\n"]
[796.867706, "o", "            precompute=gram,\r\n"]
[796.880035, "o", "            max_iter=max_iter,\r\n"]
[796.892364, "o", "            warm_start=True,\r\n"]
[796.904693, "o", "            positive=positive,\r\n"]
[796.917022, "o", "        )\r\n"]
[796.929352, "o", "\r\n"]
[796.941681, "o", "        if init is not None:\r\n"]
[796.95401, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[796.966339, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[796.978668, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[796.990998, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[797.003327, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[797.015656, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[797.027985, "o", "                init = np.array(init)\r\n"]
[797.040314, "o", "            clf.coef_ = init\r\n"]
[797.052643, "o", "\r\n"]
[797.064973, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[797.077302, "o", "        new_code = clf.coef_\r\n"]
[797.089631, "o", "\r\n"]
[797.10196, "o", "    elif algorithm == \"lars\":\r\n"]
[797.114289, "o", "        try:\r\n"]
[797.126618, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[797.138948, "o", "\r\n"]
[797.151277, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[797.163606, "o", "            # corrects the verbosity level.\r\n"]
[797.175935, "o", "            lars = Lars(\r\n"]
[797.188264, "o", "                fit_intercept=False,\r\n"]
[797.200594, "o", "                verbose=verbose,\r\n"]
[797.212923, "o", "                precompute=gram,\r\n"]
[797.225252, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[797.237581, "o", "                fit_path=False,\r\n"]
[797.24991, "o", "            )\r\n"]
[797.262239, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[797.274569, "o", "            new_code = lars.coef_\r\n"]
[797.286898, "o", "        finally:\r\n"]
[797.299227, "o", "            np.seterr(**err_mgt)\r\n"]
[797.311556, "o", "\r\n"]
[797.323885, "o", "    elif algorithm == \"threshold\":\r\n"]
[797.336214, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[797.348544, "o", "        if positive:\r\n"]
[797.360873, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[797.373202, "o", "\r\n"]
[797.385531, "o", "    elif algorithm == \"omp\":\r\n"]
[797.39786, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[797.41019, "o", "            Gram=gram,\r\n"]
[797.422519, "o", "            Xy=cov,\r\n"]
[797.434848, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[797.447177, "o", "            tol=None,\r\n"]
[797.459506, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[797.471835, "o", "            copy_Xy=copy_cov,\r\n"]
[797.484165, "o", "        ).T\r\n"]
[797.496494, "o", "\r\n"]
[797.508823, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[797.521152, "o", "\r\n"]
[797.533481, "o", "\r\n"]
[797.54581, "o", "@validate_params(\r\n"]
[797.55814, "o", "    {\r\n"]
[797.570469, "o", "        \"X\": [\"array-like\"],\r\n"]
[797.582798, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[797.595127, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[797.607456, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[797.619786, "o", "        \"algorithm\": [\r\n"]
[797.632115, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[797.644444, "o", "        ],\r\n"]
[797.656773, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[797.669102, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[797.681431, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[797.693761, "o", "        \"init\": [\"array-like\", None],\r\n"]
[797.70609, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[797.718419, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[797.730748, "o", "        \"check_input\": [\"boolean\"],\r\n"]
[797.743077, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[797.755406, "o", "        \"positive\": [\"boolean\"],\r\n"]
[797.767736, "o", "    },\r\n"]
[797.780065, "o", "    prefer_skip_nested_validation=True,\r\n"]
[797.792394, "o", ")\r\n"]
[797.804723, "o", "# XXX : could be moved to the linear_model module\r\n"]
[797.817052, "o", "def sparse_encode(\r\n"]
[797.829382, "o", "    X,\r\n"]
[797.841711, "o", "    dictionary,\r\n"]
[797.85404, "o", "    *,\r\n"]
[797.866369, "o", "    gram=None,\r\n"]
[797.878698, "o", "    cov=None,\r\n"]
[797.891027, "o", "    algorithm=\"lasso_lars\",\r\n"]
[797.903357, "o", "    n_nonzero_coefs=None,\r\n"]
[797.915686, "o", "    alpha=None,\r\n"]
[797.928015, "o", "    copy_cov=True,\r\n"]
[797.940344, "o", "    init=None,\r\n"]
[797.952673, "o", "    max_iter=1000,\r\n"]
[797.965002, "o", "    n_jobs=None,\r\n"]
[797.977332, "o", "    check_input=True,\r\n"]
[797.989661, "o", "    verbose=0,\r\n"]
[798.00199, "o", "    positive=False,\r\n"]
[798.014319, "o", "):\r\n"]
[798.026648, "o", "    \"\"\"Sparse coding.\r\n"]
[798.038978, "o", "\r\n"]
[798.051307, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[798.063636, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[798.075965, "o", "\r\n"]
[798.088294, "o", "        X ~= code * dictionary\r\n"]
[798.100623, "o", "\r\n"]
[798.112953, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[798.125282, "o", "\r\n"]
[798.137611, "o", "    Parameters\r\n"]
[798.14994, "o", "    ----------\r\n"]
[798.162269, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[798.174599, "o", "        Data matrix.\r\n"]
[798.186928, "o", "\r\n"]
[798.199257, "o", "    dictionary : array-like of shape (n_components, n_features)\r\n"]
[798.211586, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[798.223915, "o", "        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[798.236244, "o", "        output.\r\n"]
[798.248574, "o", "\r\n"]
[798.260903, "o", "    gram : array-like of shape (n_components, n_components), default=None\r\n"]
[798.273232, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n"]
[798.285561, "o", "\r\n"]
[798.29789, "o", "    cov : array-like of shape (n_components, n_samples), default=None\r\n"]
[798.310219, "o", "        Precomputed covariance, `dictionary' * X`.\r\n"]
[798.322549, "o", "\r\n"]
[798.334878, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[798.347207, "o", "            default='lasso_lars'\r\n"]
[798.359536, "o", "        The algorithm used:\r\n"]
[798.371865, "o", "\r\n"]
[798.384195, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[798.396524, "o", "          (`linear_model.lars_path`);\r\n"]
[798.408853, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[798.421182, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[798.433511, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[798.44584, "o", "          the estimated components are sparse;\r\n"]
[798.45817, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[798.470499, "o", "          solution;\r\n"]
[798.482828, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[798.495157, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[798.507486, "o", "\r\n"]
[798.519815, "o", "    n_nonzero_coefs : int, default=None\r\n"]
[798.532145, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[798.544474, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[798.556803, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[798.569132, "o", "        `n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[798.581461, "o", "\r\n"]
[798.593791, "o", "    alpha : float, default=None\r\n"]
[798.60612, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[798.618449, "o", "        penalty applied to the L1 norm.\r\n"]
[798.630778, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[798.643107, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[798.655436, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[798.667766, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[798.680095, "o", "        `n_nonzero_coefs`.\r\n"]
[798.692424, "o", "        If `None`, default to 1.\r\n"]
[798.704753, "o", "\r\n"]
[798.717082, "o", "    copy_cov : bool, default=True\r\n"]
[798.729411, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[798.741741, "o", "        be overwritten.\r\n"]
[798.75407, "o", "\r\n"]
[798.766399, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[798.778728, "o", "        Initialization value of the sparse codes. Only used if\r\n"]
[798.791057, "o", "        `algorithm='lasso_cd'`.\r\n"]
[798.803387, "o", "\r\n"]
[798.815716, "o", "    max_iter : int, default=1000\r\n"]
[798.828045, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[798.840374, "o", "        `'lasso_lars'`.\r\n"]
[798.852703, "o", "\r\n"]
[798.865032, "o", "    n_jobs : int, default=None\r\n"]
[798.877362, "o", "        Number of parallel jobs to run.\r\n"]
[798.889691, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[798.90202, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[798.914349, "o", "        for more details.\r\n"]
[798.926678, "o", "\r\n"]
[798.939007, "o", "    check_input : bool, default=True\r\n"]
[798.951337, "o", "        If `False`, the input arrays X and dictionary will not be checked.\r\n"]
[798.963666, "o", "\r\n"]
[798.975995, "o", "    verbose : int, default=0\r\n"]
[798.988324, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[799.000653, "o", "\r\n"]
[799.012983, "o", "    positive : bool, default=False\r\n"]
[799.025312, "o", "        Whether to enforce positivity when finding the encoding.\r\n"]
[799.037641, "o", "\r\n"]
[799.04997, "o", "        .. versionadded:: 0.20\r\n"]
[799.062299, "o", "\r\n"]
[799.074628, "o", "    Returns\r\n"]
[799.086958, "o", "    -------\r\n"]
[799.099287, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[799.111616, "o", "        The sparse codes.\r\n"]
[799.123945, "o", "\r\n"]
[799.136274, "o", "    See Also\r\n"]
[799.148603, "o", "    --------\r\n"]
[799.160933, "o", "    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n"]
[799.173262, "o", "        path using LARS algorithm.\r\n"]
[799.185591, "o", "    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n"]
[799.19792, "o", "    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n"]
[799.210249, "o", "    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n"]
[799.222579, "o", "        dictionary.\r\n"]
[799.234908, "o", "    \"\"\"\r\n"]
[799.247237, "o", "    if check_input:\r\n"]
[799.259566, "o", "        if algorithm == \"lasso_cd\":\r\n"]
[799.271895, "o", "            dictionary = check_array(\r\n"]
[799.284224, "o", "                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n"]
[799.296554, "o", "            )\r\n"]
[799.308883, "o", "            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n"]
[799.321212, "o", "        else:\r\n"]
[799.333541, "o", "            dictionary = check_array(dictionary)\r\n"]
[799.34587, "o", "            X = check_array(X)\r\n"]
[799.3582, "o", "\r\n"]
[799.370529, "o", "    if dictionary.shape[1] != X.shape[1]:\r\n"]
[799.382858, "o", "        raise ValueError(\r\n"]
[799.395187, "o", "            \"Dictionary and X have different numbers of features:\"\r\n"]
[799.407516, "o", "            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n"]
[799.419845, "o", "        )\r\n"]
[799.432175, "o", "\r\n"]
[799.444504, "o", "    _check_positive_coding(algorithm, positive)\r\n"]
[799.456833, "o", "\r\n"]
[799.469162, "o", "    return _sparse_encode(\r\n"]
[799.481491, "o", "        X,\r\n"]
[799.49382, "o", "        dictionary,\r\n"]
[799.50615, "o", "        gram=gram,\r\n"]
[799.518479, "o", "        cov=cov,\r\n"]
[799.530808, "o", "        algorithm=algorithm,\r\n"]
[799.543137, "o", "        n_nonzero_coefs=n_nonzero_coefs,\r\n"]
[799.555466, "o", "        alpha=alpha,\r\n"]
[799.567796, "o", "        copy_cov=copy_cov,\r\n"]
[799.580125, "o", "        init=init,\r\n"]
[799.592454, "o", "        max_iter=max_iter,\r\n"]
[799.604783, "o", "        n_jobs=n_jobs,\r\n"]
[799.617112, "o", "        verbose=verbose,\r\n"]
[799.629441, "o", "        positive=positive,\r\n"]
[799.641771, "o", "    )\r\n"]
[799.6541, "o", "\r\n"]
[799.666429, "o", "\r\n"]
[799.678758, "o", "def _sparse_encode(\r\n"]
[799.691087, "o", "    X,\r\n"]
[799.703416, "o", "    dictionary,\r\n"]
[799.715746, "o", "    *,\r\n"]
[799.728075, "o", "    gram=None,\r\n"]
[799.740404, "o", "    cov=None,\r\n"]
[799.752733, "o", "    algorithm=\"lasso_lars\",\r\n"]
[799.765062, "o", "    n_nonzero_coefs=None,\r\n"]
[799.777392, "o", "    alpha=None,\r\n"]
[799.789721, "o", "    copy_cov=True,\r\n"]
[799.80205, "o", "    init=None,\r\n"]
[799.814379, "o", "    max_iter=1000,\r\n"]
[799.826708, "o", "    n_jobs=None,\r\n"]
[799.839037, "o", "    verbose=0,\r\n"]
[799.851367, "o", "    positive=False,\r\n"]
[799.863696, "o", "):\r\n"]
[799.876025, "o", "    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[799.888354, "o", "\r\n"]
[799.900683, "o", "    n_samples, n_features = X.shape\r\n"]
[799.913012, "o", "    n_components = dictionary.shape[0]\r\n"]
[799.925342, "o", "\r\n"]
[799.937671, "o", "    if algorithm in (\"lars\", \"omp\"):\r\n"]
[800.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[800.002, "i", "sed -n '400,1000p' sklearn/decomposition/_dict_learning.py\r"]
[800.004, "o", "sed -n '400,1000p' sklearn/decomposition/_dict_learning.py\r\n"]
[800.016, "o", "\u001b[?2004l\r\n"]
[800.026, "o", "        regularization = n_nonzero_coefs\r\n"]
[800.036, "o", "        if regularization is None:\r\n"]
[800.046, "o", "            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[800.056, "o", "    else:\r\n"]
[800.066, "o", "        regularization = alpha\r\n"]
[800.076, "o", "        if regularization is None:\r\n"]
[800.086, "o", "            regularization = 1.0\r\n"]
[800.096, "o", "\r\n"]
[800.106, "o", "    if gram is None and algorithm != \"threshold\":\r\n"]
[800.116, "o", "        gram = np.dot(dictionary, dictionary.T)\r\n"]
[800.126, "o", "\r\n"]
[800.136, "o", "    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[800.146, "o", "        copy_cov = False\r\n"]
[800.156, "o", "        cov = np.dot(dictionary, X.T)\r\n"]
[800.166, "o", "\r\n"]
[800.176, "o", "    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[800.186, "o", "        code = _sparse_encode_precomputed(\r\n"]
[800.196, "o", "            X,\r\n"]
[800.206, "o", "            dictionary,\r\n"]
[800.216, "o", "            gram=gram,\r\n"]
[800.226, "o", "            cov=cov,\r\n"]
[800.236, "o", "            algorithm=algorithm,\r\n"]
[800.246, "o", "            regularization=regularization,\r\n"]
[800.256, "o", "            copy_cov=copy_cov,\r\n"]
[800.266, "o", "            init=init,\r\n"]
[800.276, "o", "            max_iter=max_iter,\r\n"]
[800.286, "o", "            verbose=verbose,\r\n"]
[800.296, "o", "            positive=positive,\r\n"]
[800.306, "o", "        )\r\n"]
[800.316, "o", "        return code\r\n"]
[800.326, "o", "\r\n"]
[800.336, "o", "    # Enter parallel code block\r\n"]
[800.346, "o", "    n_samples = X.shape[0]\r\n"]
[800.356, "o", "    n_components = dictionary.shape[0]\r\n"]
[800.366, "o", "    code = np.empty((n_samples, n_components))\r\n"]
[800.376, "o", "    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[800.386, "o", "\r\n"]
[800.396, "o", "    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[800.406, "o", "        delayed(_sparse_encode_precomputed)(\r\n"]
[800.416, "o", "            X[this_slice],\r\n"]
[800.426, "o", "            dictionary,\r\n"]
[800.436, "o", "            gram=gram,\r\n"]
[800.446, "o", "            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[800.456, "o", "            algorithm=algorithm,\r\n"]
[800.466, "o", "            regularization=regularization,\r\n"]
[800.476, "o", "            copy_cov=copy_cov,\r\n"]
[800.486, "o", "            init=init[this_slice] if init is not None else None,\r\n"]
[800.496, "o", "            max_iter=max_iter,\r\n"]
[800.506, "o", "            verbose=verbose,\r\n"]
[800.516, "o", "            positive=positive,\r\n"]
[800.526, "o", "        )\r\n"]
[800.536, "o", "        for this_slice in slices\r\n"]
[800.546, "o", "    )\r\n"]
[800.556, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[800.566, "o", "        code[this_slice] = this_view\r\n"]
[800.576, "o", "    return code\r\n"]
[800.586, "o", "\r\n"]
[800.596, "o", "\r\n"]
[800.606, "o", "def _update_dict(\r\n"]
[800.616, "o", "    dictionary,\r\n"]
[800.626, "o", "    Y,\r\n"]
[800.636, "o", "    code,\r\n"]
[800.646, "o", "    A=None,\r\n"]
[800.656, "o", "    B=None,\r\n"]
[800.666, "o", "    verbose=False,\r\n"]
[800.676, "o", "    random_state=None,\r\n"]
[800.686, "o", "    positive=False,\r\n"]
[800.696, "o", "):\r\n"]
[800.706, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[800.716, "o", "\r\n"]
[800.726, "o", "    Parameters\r\n"]
[800.736, "o", "    ----------\r\n"]
[800.746, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[800.756, "o", "        Value of the dictionary at the previous iteration.\r\n"]
[800.766, "o", "\r\n"]
[800.776, "o", "    Y : ndarray of shape (n_samples, n_features)\r\n"]
[800.786, "o", "        Data matrix.\r\n"]
[800.796, "o", "\r\n"]
[800.806, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[800.816, "o", "        Sparse coding of the data against which to optimize the dictionary.\r\n"]
[800.826, "o", "\r\n"]
[800.836, "o", "    A : ndarray of shape (n_components, n_components), default=None\r\n"]
[800.846, "o", "        Together with `B`, sufficient stats of the online model to update the\r\n"]
[800.856, "o", "        dictionary.\r\n"]
[800.866, "o", "\r\n"]
[800.876, "o", "    B : ndarray of shape (n_features, n_components), default=None\r\n"]
[800.886, "o", "        Together with `A`, sufficient stats of the online model to update the\r\n"]
[800.896, "o", "        dictionary.\r\n"]
[800.906, "o", "\r\n"]
[800.916, "o", "    verbose: bool, default=False\r\n"]
[800.926, "o", "        Degree of output the procedure will print.\r\n"]
[800.936, "o", "\r\n"]
[800.946, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[800.956, "o", "        Used for randomly initializing the dictionary. Pass an int for\r\n"]
[800.966, "o", "        reproducible results across multiple function calls.\r\n"]
[800.976, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[800.986, "o", "\r\n"]
[800.996, "o", "    positive : bool, default=False\r\n"]
[801.006, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[801.016, "o", "\r\n"]
[801.026, "o", "        .. versionadded:: 0.20\r\n"]
[801.036, "o", "    \"\"\"\r\n"]
[801.046, "o", "    n_samples, n_components = code.shape\r\n"]
[801.056, "o", "    random_state = check_random_state(random_state)\r\n"]
[801.066, "o", "\r\n"]
[801.076, "o", "    if A is None:\r\n"]
[801.086, "o", "        A = code.T @ code\r\n"]
[801.096, "o", "    if B is None:\r\n"]
[801.106, "o", "        B = Y.T @ code\r\n"]
[801.116, "o", "\r\n"]
[801.126, "o", "    n_unused = 0\r\n"]
[801.136, "o", "\r\n"]
[801.146, "o", "    for k in range(n_components):\r\n"]
[801.156, "o", "        if A[k, k] > 1e-6:\r\n"]
[801.166, "o", "            # 1e-6 is arbitrary but consistent with the spams implementation\r\n"]
[801.176, "o", "            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n"]
[801.186, "o", "        else:\r\n"]
[801.196, "o", "            # kth atom is almost never used -> sample a new one from the data\r\n"]
[801.206, "o", "            newd = Y[random_state.choice(n_samples)]\r\n"]
[801.216, "o", "\r\n"]
[801.226, "o", "            # add small noise to avoid making the sparse coding ill conditioned\r\n"]
[801.236, "o", "            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n"]
[801.246, "o", "            noise = random_state.normal(0, noise_level, size=len(newd))\r\n"]
[801.256, "o", "\r\n"]
[801.266, "o", "            dictionary[k] = newd + noise\r\n"]
[801.276, "o", "            code[:, k] = 0\r\n"]
[801.286, "o", "            n_unused += 1\r\n"]
[801.296, "o", "\r\n"]
[801.306, "o", "        if positive:\r\n"]
[801.316, "o", "            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n"]
[801.326, "o", "\r\n"]
[801.336, "o", "        # Projection on the constraint set ||V_k|| <= 1\r\n"]
[801.346, "o", "        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n"]
[801.356, "o", "\r\n"]
[801.366, "o", "    if verbose and n_unused > 0:\r\n"]
[801.376, "o", "        print(f\"{n_unused} unused atoms resampled.\")\r\n"]
[801.386, "o", "\r\n"]
[801.396, "o", "\r\n"]
[801.406, "o", "def _dict_learning(\r\n"]
[801.416, "o", "    X,\r\n"]
[801.426, "o", "    n_components,\r\n"]
[801.436, "o", "    *,\r\n"]
[801.446, "o", "    alpha,\r\n"]
[801.456, "o", "    max_iter,\r\n"]
[801.466, "o", "    tol,\r\n"]
[801.476, "o", "    method,\r\n"]
[801.486, "o", "    n_jobs,\r\n"]
[801.496, "o", "    dict_init,\r\n"]
[801.506, "o", "    code_init,\r\n"]
[801.516, "o", "    callback,\r\n"]
[801.526, "o", "    verbose,\r\n"]
[801.536, "o", "    random_state,\r\n"]
[801.546, "o", "    return_n_iter,\r\n"]
[801.556, "o", "    positive_dict,\r\n"]
[801.566, "o", "    positive_code,\r\n"]
[801.576, "o", "    method_max_iter,\r\n"]
[801.586, "o", "):\r\n"]
[801.596, "o", "    \"\"\"Main dictionary learning algorithm\"\"\"\r\n"]
[801.606, "o", "    t0 = time.time()\r\n"]
[801.616, "o", "    # Init the code and the dictionary with SVD of Y\r\n"]
[801.626, "o", "    if code_init is not None and dict_init is not None:\r\n"]
[801.636, "o", "        code = np.array(code_init, order=\"F\")\r\n"]
[801.646, "o", "        # Don't copy V, it will happen below\r\n"]
[801.656, "o", "        dictionary = dict_init\r\n"]
[801.666, "o", "    else:\r\n"]
[801.676, "o", "        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n"]
[801.686, "o", "        # flip the initial code's sign to enforce deterministic output\r\n"]
[801.696, "o", "        code, dictionary = svd_flip(code, dictionary)\r\n"]
[801.706, "o", "        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[801.716, "o", "    r = len(dictionary)\r\n"]
[801.726, "o", "    if n_components <= r:  # True even if n_components=None\r\n"]
[801.736, "o", "        code = code[:, :n_components]\r\n"]
[801.746, "o", "        dictionary = dictionary[:n_components, :]\r\n"]
[801.756, "o", "    else:\r\n"]
[801.766, "o", "        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n"]
[801.776, "o", "        dictionary = np.r_[\r\n"]
[801.786, "o", "            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n"]
[801.796, "o", "        ]\r\n"]
[801.806, "o", "\r\n"]
[801.816, "o", "    # Fortran-order dict better suited for the sparse coding which is the\r\n"]
[801.826, "o", "    # bottleneck of this algorithm.\r\n"]
[801.836, "o", "    dictionary = np.asfortranarray(dictionary)\r\n"]
[801.846, "o", "\r\n"]
[801.856, "o", "    errors = []\r\n"]
[801.866, "o", "    current_cost = np.nan\r\n"]
[801.876, "o", "\r\n"]
[801.886, "o", "    if verbose == 1:\r\n"]
[801.896, "o", "        print(\"[dict_learning]\", end=\" \")\r\n"]
[801.906, "o", "\r\n"]
[801.916, "o", "    # If max_iter is 0, number of iterations returned should be zero\r\n"]
[801.926, "o", "    ii = -1\r\n"]
[801.936, "o", "\r\n"]
[801.946, "o", "    for ii in range(max_iter):\r\n"]
[801.956, "o", "        dt = time.time() - t0\r\n"]
[801.966, "o", "        if verbose == 1:\r\n"]
[801.976, "o", "            sys.stdout.write(\".\")\r\n"]
[801.986, "o", "            sys.stdout.flush()\r\n"]
[801.996, "o", "        elif verbose:\r\n"]
[802.006, "o", "            print(\r\n"]
[802.016, "o", "                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n"]
[802.026, "o", "                % (ii, dt, dt / 60, current_cost)\r\n"]
[802.036, "o", "            )\r\n"]
[802.046, "o", "\r\n"]
[802.056, "o", "        # Update code\r\n"]
[802.066, "o", "        code = sparse_encode(\r\n"]
[802.076, "o", "            X,\r\n"]
[802.086, "o", "            dictionary,\r\n"]
[802.096, "o", "            algorithm=method,\r\n"]
[802.106, "o", "            alpha=alpha,\r\n"]
[802.116, "o", "            init=code,\r\n"]
[802.126, "o", "            n_jobs=n_jobs,\r\n"]
[802.136, "o", "            positive=positive_code,\r\n"]
[802.146, "o", "            max_iter=method_max_iter,\r\n"]
[802.156, "o", "            verbose=verbose,\r\n"]
[802.166, "o", "        )\r\n"]
[802.176, "o", "\r\n"]
[802.186, "o", "        # Update dictionary in place\r\n"]
[802.196, "o", "        _update_dict(\r\n"]
[802.206, "o", "            dictionary,\r\n"]
[802.216, "o", "            X,\r\n"]
[802.226, "o", "            code,\r\n"]
[802.236, "o", "            verbose=verbose,\r\n"]
[802.246, "o", "            random_state=random_state,\r\n"]
[802.256, "o", "            positive=positive_dict,\r\n"]
[802.266, "o", "        )\r\n"]
[802.276, "o", "\r\n"]
[802.286, "o", "        # Cost function\r\n"]
[802.296, "o", "        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n"]
[802.306, "o", "            np.abs(code)\r\n"]
[802.316, "o", "        )\r\n"]
[802.326, "o", "        errors.append(current_cost)\r\n"]
[802.336, "o", "\r\n"]
[802.346, "o", "        if ii > 0:\r\n"]
[802.356, "o", "            dE = errors[-2] - errors[-1]\r\n"]
[802.366, "o", "            # assert(dE >= -tol * errors[-1])\r\n"]
[802.376, "o", "            if dE < tol * errors[-1]:\r\n"]
[802.386, "o", "                if verbose == 1:\r\n"]
[802.396, "o", "                    # A line return\r\n"]
[802.406, "o", "                    print(\"\")\r\n"]
[802.416, "o", "                elif verbose:\r\n"]
[802.426, "o", "                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n"]
[802.436, "o", "                break\r\n"]
[802.446, "o", "        if ii % 5 == 0 and callback is not None:\r\n"]
[802.456, "o", "            callback(locals())\r\n"]
[802.466, "o", "\r\n"]
[802.476, "o", "    if return_n_iter:\r\n"]
[802.486, "o", "        return code, dictionary, errors, ii + 1\r\n"]
[802.496, "o", "    else:\r\n"]
[802.506, "o", "        return code, dictionary, errors\r\n"]
[802.516, "o", "\r\n"]
[802.526, "o", "\r\n"]
[802.536, "o", "def _check_warn_deprecated(param, name, default, additional_message=None):\r\n"]
[802.546, "o", "    if param != \"deprecated\":\r\n"]
[802.556, "o", "        msg = (\r\n"]
[802.566, "o", "            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n"]
[802.576, "o", "        )\r\n"]
[802.586, "o", "        if additional_message:\r\n"]
[802.596, "o", "            msg += f\" {additional_message}\"\r\n"]
[802.606, "o", "        warnings.warn(msg, FutureWarning)\r\n"]
[802.616, "o", "        return param\r\n"]
[802.626, "o", "    else:\r\n"]
[802.636, "o", "        return default\r\n"]
[802.646, "o", "\r\n"]
[802.656, "o", "\r\n"]
[802.666, "o", "def dict_learning_online(\r\n"]
[802.676, "o", "    X,\r\n"]
[802.686, "o", "    n_components=2,\r\n"]
[802.696, "o", "    *,\r\n"]
[802.706, "o", "    alpha=1,\r\n"]
[802.716, "o", "    n_iter=\"deprecated\",\r\n"]
[802.726, "o", "    max_iter=None,\r\n"]
[802.736, "o", "    return_code=True,\r\n"]
[802.746, "o", "    dict_init=None,\r\n"]
[802.756, "o", "    callback=None,\r\n"]
[802.766, "o", "    batch_size=256,\r\n"]
[802.776, "o", "    verbose=False,\r\n"]
[802.786, "o", "    shuffle=True,\r\n"]
[802.796, "o", "    n_jobs=None,\r\n"]
[802.806, "o", "    method=\"lars\",\r\n"]
[802.816, "o", "    iter_offset=\"deprecated\",\r\n"]
[802.826, "o", "    random_state=None,\r\n"]
[802.836, "o", "    return_inner_stats=\"deprecated\",\r\n"]
[802.846, "o", "    inner_stats=\"deprecated\",\r\n"]
[802.856, "o", "    return_n_iter=\"deprecated\",\r\n"]
[802.866, "o", "    positive_dict=False,\r\n"]
[802.876, "o", "    positive_code=False,\r\n"]
[802.886, "o", "    method_max_iter=1000,\r\n"]
[802.896, "o", "    tol=1e-3,\r\n"]
[802.906, "o", "    max_no_improvement=10,\r\n"]
[802.916, "o", "):\r\n"]
[802.926, "o", "    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n"]
[802.936, "o", "\r\n"]
[802.946, "o", "    Finds the best dictionary and the corresponding sparse code for\r\n"]
[802.956, "o", "    approximating the data matrix X by solving::\r\n"]
[802.966, "o", "\r\n"]
[802.976, "o", "        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[802.986, "o", "                     (U,V)\r\n"]
[802.996, "o", "                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n"]
[803.006, "o", "\r\n"]
[803.016, "o", "    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n"]
[803.026, "o", "    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n"]
[803.036, "o", "    which is the sum of the absolute values of all the entries in the matrix.\r\n"]
[803.046, "o", "    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n"]
[803.056, "o", "    the input data.\r\n"]
[803.066, "o", "\r\n"]
[803.076, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[803.086, "o", "\r\n"]
[803.096, "o", "    Parameters\r\n"]
[803.106, "o", "    ----------\r\n"]
[803.116, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[803.126, "o", "        Data matrix.\r\n"]
[803.136, "o", "\r\n"]
[803.146, "o", "    n_components : int or None, default=2\r\n"]
[803.156, "o", "        Number of dictionary atoms to extract. If None, then ``n_components``\r\n"]
[803.166, "o", "        is set to ``n_features``.\r\n"]
[803.176, "o", "\r\n"]
[803.186, "o", "    alpha : float, default=1\r\n"]
[803.196, "o", "        Sparsity controlling parameter.\r\n"]
[803.206, "o", "\r\n"]
[803.216, "o", "    n_iter : int, default=100\r\n"]
[803.226, "o", "        Number of mini-batch iterations to perform.\r\n"]
[803.236, "o", "\r\n"]
[803.246, "o", "        .. deprecated:: 1.1\r\n"]
[803.256, "o", "           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[803.266, "o", "           `max_iter` instead.\r\n"]
[803.276, "o", "\r\n"]
[803.286, "o", "    max_iter : int, default=None\r\n"]
[803.296, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[803.306, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[803.316, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[803.326, "o", "\r\n"]
[803.336, "o", "        .. versionadded:: 1.1\r\n"]
[803.346, "o", "\r\n"]
[803.356, "o", "    return_code : bool, default=True\r\n"]
[803.366, "o", "        Whether to also return the code U or just the dictionary `V`.\r\n"]
[803.376, "o", "\r\n"]
[803.386, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[803.396, "o", "        Initial values for the dictionary for warm restart scenarios.\r\n"]
[803.406, "o", "        If `None`, the initial values for the dictionary are created\r\n"]
[803.416, "o", "        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n"]
[803.426, "o", "\r\n"]
[803.436, "o", "    callback : callable, default=None\r\n"]
[803.446, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[803.456, "o", "\r\n"]
[803.466, "o", "    batch_size : int, default=256\r\n"]
[803.476, "o", "        The number of samples to take in each batch.\r\n"]
[803.486, "o", "\r\n"]
[803.496, "o", "        .. versionchanged:: 1.3\r\n"]
[803.506, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[803.516, "o", "\r\n"]
[803.526, "o", "    verbose : bool, default=False\r\n"]
[803.536, "o", "        To control the verbosity of the procedure.\r\n"]
[803.546, "o", "\r\n"]
[803.556, "o", "    shuffle : bool, default=True\r\n"]
[803.566, "o", "        Whether to shuffle the data before splitting it in batches.\r\n"]
[803.576, "o", "\r\n"]
[803.586, "o", "    n_jobs : int, default=None\r\n"]
[803.596, "o", "        Number of parallel jobs to run.\r\n"]
[803.606, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[803.616, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[803.626, "o", "        for more details.\r\n"]
[803.636, "o", "\r\n"]
[803.646, "o", "    method : {'lars', 'cd'}, default='lars'\r\n"]
[803.656, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[803.666, "o", "          problem (`linear_model.lars_path`);\r\n"]
[803.676, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[803.686, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[803.696, "o", "          the estimated components are sparse.\r\n"]
[803.706, "o", "\r\n"]
[803.716, "o", "    iter_offset : int, default=0\r\n"]
[803.726, "o", "        Number of previous iterations completed on the dictionary used for\r\n"]
[803.736, "o", "        initialization.\r\n"]
[803.746, "o", "\r\n"]
[803.756, "o", "        .. deprecated:: 1.1\r\n"]
[803.766, "o", "           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n"]
[803.776, "o", "\r\n"]
[803.786, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[803.796, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[803.806, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[803.816, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[803.826, "o", "        results across multiple function calls.\r\n"]
[803.836, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[803.846, "o", "\r\n"]
[803.856, "o", "    return_inner_stats : bool, default=False\r\n"]
[803.866, "o", "        Return the inner statistics A (dictionary covariance) and B\r\n"]
[803.876, "o", "        (data approximation). Useful to restart the algorithm in an\r\n"]
[803.886, "o", "        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n"]
[803.896, "o", "        ignored.\r\n"]
[803.906, "o", "\r\n"]
[803.916, "o", "        .. deprecated:: 1.1\r\n"]
[803.926, "o", "           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[803.936, "o", "\r\n"]
[803.946, "o", "    inner_stats : tuple of (A, B) ndarrays, default=None\r\n"]
[803.956, "o", "        Inner sufficient statistics that are kept by the algorithm.\r\n"]
[803.966, "o", "        Passing them at initialization is useful in online settings, to\r\n"]
[803.976, "o", "        avoid losing the history of the evolution.\r\n"]
[803.986, "o", "        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n"]
[803.996, "o", "        `B` `(n_features, n_components)` is the data approximation matrix.\r\n"]
[804.006, "o", "\r\n"]
[804.016, "o", "        .. deprecated:: 1.1\r\n"]
[804.026, "o", "           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[804.036, "o", "\r\n"]
[804.046, "o", "    return_n_iter : bool, default=False\r\n"]
[804.056, "o", "        Whether or not to return the number of iterations.\r\n"]
[804.066, "o", "\r\n"]
[804.076, "o", "        .. deprecated:: 1.1\r\n"]
[804.086, "o", "           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n"]
[804.096, "o", "\r\n"]
[804.106, "o", "    positive_dict : bool, default=False\r\n"]
[804.116, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[804.126, "o", "\r\n"]
[804.136, "o", "        .. versionadded:: 0.20\r\n"]
[804.146, "o", "\r\n"]
[804.156, "o", "    positive_code : bool, default=False\r\n"]
[804.166, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[804.176, "o", "\r\n"]
[804.186, "o", "        .. versionadded:: 0.20\r\n"]
[804.196, "o", "\r\n"]
[804.206, "o", "    method_max_iter : int, default=1000\r\n"]
[804.216, "o", "        Maximum number of iterations to perform when solving the lasso problem.\r\n"]
[804.226, "o", "\r\n"]
[804.236, "o", "        .. versionadded:: 0.22\r\n"]
[804.246, "o", "\r\n"]
[804.256, "o", "    tol : float, default=1e-3\r\n"]
[804.266, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[804.276, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[804.286, "o", "\r\n"]
[804.296, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[804.306, "o", "        `tol` to 0.0.\r\n"]
[804.316, "o", "\r\n"]
[804.326, "o", "        .. versionadded:: 1.1\r\n"]
[804.336, "o", "\r\n"]
[804.346, "o", "    max_no_improvement : int, default=10\r\n"]
[804.356, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[804.366, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[804.376, "o", "        `max_iter` is not None.\r\n"]
[804.386, "o", "\r\n"]
[804.396, "o", "        To disable convergence detection based on cost function, set\r\n"]
[804.406, "o", "        `max_no_improvement` to None.\r\n"]
[804.416, "o", "\r\n"]
[804.426, "o", "        .. versionadded:: 1.1\r\n"]
[804.436, "o", "\r\n"]
[804.446, "o", "    Returns\r\n"]
[804.456, "o", "    -------\r\n"]
[804.466, "o", "    code : ndarray of shape (n_samples, n_components),\r\n"]
[804.476, "o", "        The sparse code (only returned if `return_code=True`).\r\n"]
[804.486, "o", "\r\n"]
[804.496, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[804.506, "o", "        The solutions to the dictionary learning problem.\r\n"]
[804.516, "o", "\r\n"]
[804.526, "o", "    n_iter : int\r\n"]
[804.536, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[804.546, "o", "        set to `True`.\r\n"]
[804.556, "o", "\r\n"]
[804.566, "o", "    See Also\r\n"]
[804.576, "o", "    --------\r\n"]
[804.586, "o", "    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n"]
[804.596, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[804.606, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n"]
[804.616, "o", "        learning algorithm.\r\n"]
[804.626, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[804.636, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[804.646, "o", "    \"\"\"\r\n"]
[804.656, "o", "    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n"]
[804.666, "o", "    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n"]
[804.676, "o", "        raise ValueError(\r\n"]
[804.686, "o", "            \"The following arguments are incompatible with 'max_iter': \"\r\n"]
[804.696, "o", "            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n"]
[804.706, "o", "        )\r\n"]
[804.716, "o", "\r\n"]
[804.726, "o", "    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n"]
[804.736, "o", "    return_inner_stats = _check_warn_deprecated(\r\n"]
[804.746, "o", "        return_inner_stats,\r\n"]
[804.756, "o", "        \"return_inner_stats\",\r\n"]
[804.766, "o", "        default=False,\r\n"]
[804.776, "o", "        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n"]
[804.786, "o", "    )\r\n"]
[804.796, "o", "    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n"]
[804.806, "o", "    return_n_iter = _check_warn_deprecated(\r\n"]
[804.816, "o", "        return_n_iter,\r\n"]
[804.826, "o", "        \"return_n_iter\",\r\n"]
[804.836, "o", "        default=False,\r\n"]
[804.846, "o", "        additional_message=(\r\n"]
[804.856, "o", "            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n"]
[804.866, "o", "            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n"]
[804.876, "o", "        ),\r\n"]
[804.886, "o", "    )\r\n"]
[804.896, "o", "\r\n"]
[804.906, "o", "    if max_iter is not None:\r\n"]
[804.916, "o", "        transform_algorithm = \"lasso_\" + method\r\n"]
[804.926, "o", "\r\n"]
[804.936, "o", "        est = MiniBatchDictionaryLearning(\r\n"]
[805.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[805.002, "i", "sed -n '1000,1800p' sklearn/decomposition/_dict_learning.py\r"]
[805.004, "o", "sed -n '1000,1800p' sklearn/decomposition/_dict_learning.py\r\n"]
[805.016, "o", "\u001b[?2004l\r\n"]
[805.026, "o", "\r\n"]
[805.036, "o", "        this_code = sparse_encode(\r\n"]
[805.046, "o", "            this_X,\r\n"]
[805.056, "o", "            dictionary,\r\n"]
[805.066, "o", "            algorithm=method,\r\n"]
[805.076, "o", "            alpha=alpha,\r\n"]
[805.086, "o", "            n_jobs=n_jobs,\r\n"]
[805.096, "o", "            check_input=False,\r\n"]
[805.106, "o", "            positive=positive_code,\r\n"]
[805.116, "o", "            max_iter=method_max_iter,\r\n"]
[805.126, "o", "            verbose=verbose,\r\n"]
[805.136, "o", "        )\r\n"]
[805.146, "o", "\r\n"]
[805.156, "o", "        # Update the auxiliary variables\r\n"]
[805.166, "o", "        if ii < batch_size - 1:\r\n"]
[805.176, "o", "            theta = float((ii + 1) * batch_size)\r\n"]
[805.186, "o", "        else:\r\n"]
[805.196, "o", "            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n"]
[805.206, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[805.216, "o", "\r\n"]
[805.226, "o", "        A *= beta\r\n"]
[805.236, "o", "        A += np.dot(this_code.T, this_code)\r\n"]
[805.246, "o", "        B *= beta\r\n"]
[805.256, "o", "        B += np.dot(this_X.T, this_code)\r\n"]
[805.266, "o", "\r\n"]
[805.276, "o", "        # Update dictionary in place\r\n"]
[805.286, "o", "        _update_dict(\r\n"]
[805.296, "o", "            dictionary,\r\n"]
[805.306, "o", "            this_X,\r\n"]
[805.316, "o", "            this_code,\r\n"]
[805.326, "o", "            A,\r\n"]
[805.336, "o", "            B,\r\n"]
[805.346, "o", "            verbose=verbose,\r\n"]
[805.356, "o", "            random_state=random_state,\r\n"]
[805.366, "o", "            positive=positive_dict,\r\n"]
[805.376, "o", "        )\r\n"]
[805.386, "o", "\r\n"]
[805.396, "o", "        # Maybe we need a stopping criteria based on the amount of\r\n"]
[805.406, "o", "        # modification in the dictionary\r\n"]
[805.416, "o", "        if callback is not None:\r\n"]
[805.426, "o", "            callback(locals())\r\n"]
[805.436, "o", "\r\n"]
[805.446, "o", "    if return_inner_stats:\r\n"]
[805.456, "o", "        if return_n_iter:\r\n"]
[805.466, "o", "            return dictionary, (A, B), ii - iter_offset + 1\r\n"]
[805.476, "o", "        else:\r\n"]
[805.486, "o", "            return dictionary, (A, B)\r\n"]
[805.496, "o", "    if return_code:\r\n"]
[805.506, "o", "        if verbose > 1:\r\n"]
[805.516, "o", "            print(\"Learning code...\", end=\" \")\r\n"]
[805.526, "o", "        elif verbose == 1:\r\n"]
[805.536, "o", "            print(\"|\", end=\" \")\r\n"]
[805.546, "o", "        code = sparse_encode(\r\n"]
[805.556, "o", "            X,\r\n"]
[805.566, "o", "            dictionary,\r\n"]
[805.576, "o", "            algorithm=method,\r\n"]
[805.586, "o", "            alpha=alpha,\r\n"]
[805.596, "o", "            n_jobs=n_jobs,\r\n"]
[805.606, "o", "            check_input=False,\r\n"]
[805.616, "o", "            positive=positive_code,\r\n"]
[805.626, "o", "            max_iter=method_max_iter,\r\n"]
[805.636, "o", "            verbose=verbose,\r\n"]
[805.646, "o", "        )\r\n"]
[805.656, "o", "        if verbose > 1:\r\n"]
[805.666, "o", "            dt = time.time() - t0\r\n"]
[805.676, "o", "            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n"]
[805.686, "o", "        if return_n_iter:\r\n"]
[805.696, "o", "            return code, dictionary, ii - iter_offset + 1\r\n"]
[805.706, "o", "        else:\r\n"]
[805.716, "o", "            return code, dictionary\r\n"]
[805.726, "o", "\r\n"]
[805.736, "o", "    if return_n_iter:\r\n"]
[805.746, "o", "        return dictionary, ii - iter_offset + 1\r\n"]
[805.756, "o", "    else:\r\n"]
[805.766, "o", "        return dictionary\r\n"]
[805.776, "o", "\r\n"]
[805.786, "o", "\r\n"]
[805.796, "o", "@validate_params(\r\n"]
[805.806, "o", "    {\r\n"]
[805.816, "o", "        \"X\": [\"array-like\"],\r\n"]
[805.826, "o", "        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[805.836, "o", "        \"return_n_iter\": [\"boolean\"],\r\n"]
[805.846, "o", "        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[805.856, "o", "    },\r\n"]
[805.866, "o", "    prefer_skip_nested_validation=False,\r\n"]
[805.876, "o", ")\r\n"]
[805.886, "o", "def dict_learning(\r\n"]
[805.896, "o", "    X,\r\n"]
[805.906, "o", "    n_components,\r\n"]
[805.916, "o", "    *,\r\n"]
[805.926, "o", "    alpha,\r\n"]
[805.936, "o", "    max_iter=100,\r\n"]
[805.946, "o", "    tol=1e-8,\r\n"]
[805.956, "o", "    method=\"lars\",\r\n"]
[805.966, "o", "    n_jobs=None,\r\n"]
[805.976, "o", "    dict_init=None,\r\n"]
[805.986, "o", "    code_init=None,\r\n"]
[805.996, "o", "    callback=None,\r\n"]
[806.006, "o", "    verbose=False,\r\n"]
[806.016, "o", "    random_state=None,\r\n"]
[806.026, "o", "    return_n_iter=False,\r\n"]
[806.036, "o", "    positive_dict=False,\r\n"]
[806.046, "o", "    positive_code=False,\r\n"]
[806.056, "o", "    method_max_iter=1000,\r\n"]
[806.066, "o", "):\r\n"]
[806.076, "o", "    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n"]
[806.086, "o", "\r\n"]
[806.096, "o", "    Finds the best dictionary and the corresponding sparse code for\r\n"]
[806.106, "o", "    approximating the data matrix X by solving::\r\n"]
[806.116, "o", "\r\n"]
[806.126, "o", "        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[806.136, "o", "                     (U,V)\r\n"]
[806.146, "o", "                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n"]
[806.156, "o", "\r\n"]
[806.166, "o", "    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n"]
[806.176, "o", "    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n"]
[806.186, "o", "    which is the sum of the absolute values of all the entries in the matrix.\r\n"]
[806.196, "o", "\r\n"]
[806.206, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[806.216, "o", "\r\n"]
[806.226, "o", "    Parameters\r\n"]
[806.236, "o", "    ----------\r\n"]
[806.246, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[806.256, "o", "        Data matrix.\r\n"]
[806.266, "o", "\r\n"]
[806.276, "o", "    n_components : int\r\n"]
[806.286, "o", "        Number of dictionary atoms to extract.\r\n"]
[806.296, "o", "\r\n"]
[806.306, "o", "    alpha : int or float\r\n"]
[806.316, "o", "        Sparsity controlling parameter.\r\n"]
[806.326, "o", "\r\n"]
[806.336, "o", "    max_iter : int, default=100\r\n"]
[806.346, "o", "        Maximum number of iterations to perform.\r\n"]
[806.356, "o", "\r\n"]
[806.366, "o", "    tol : float, default=1e-8\r\n"]
[806.376, "o", "        Tolerance for the stopping condition.\r\n"]
[806.386, "o", "\r\n"]
[806.396, "o", "    method : {'lars', 'cd'}, default='lars'\r\n"]
[806.406, "o", "        The method used:\r\n"]
[806.416, "o", "\r\n"]
[806.426, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[806.436, "o", "           problem (`linear_model.lars_path`);\r\n"]
[806.446, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[806.456, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[806.466, "o", "          the estimated components are sparse.\r\n"]
[806.476, "o", "\r\n"]
[806.486, "o", "    n_jobs : int, default=None\r\n"]
[806.496, "o", "        Number of parallel jobs to run.\r\n"]
[806.506, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[806.516, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[806.526, "o", "        for more details.\r\n"]
[806.536, "o", "\r\n"]
[806.546, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[806.556, "o", "        Initial value for the dictionary for warm restart scenarios. Only used\r\n"]
[806.566, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[806.576, "o", "\r\n"]
[806.586, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[806.596, "o", "        Initial value for the sparse code for warm restart scenarios. Only used\r\n"]
[806.606, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[806.616, "o", "\r\n"]
[806.626, "o", "    callback : callable, default=None\r\n"]
[806.636, "o", "        Callable that gets invoked every five iterations.\r\n"]
[806.646, "o", "\r\n"]
[806.656, "o", "    verbose : bool, default=False\r\n"]
[806.666, "o", "        To control the verbosity of the procedure.\r\n"]
[806.676, "o", "\r\n"]
[806.686, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[806.696, "o", "        Used for randomly initializing the dictionary. Pass an int for\r\n"]
[806.706, "o", "        reproducible results across multiple function calls.\r\n"]
[806.716, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[806.726, "o", "\r\n"]
[806.736, "o", "    return_n_iter : bool, default=False\r\n"]
[806.746, "o", "        Whether or not to return the number of iterations.\r\n"]
[806.756, "o", "\r\n"]
[806.766, "o", "    positive_dict : bool, default=False\r\n"]
[806.776, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[806.786, "o", "\r\n"]
[806.796, "o", "        .. versionadded:: 0.20\r\n"]
[806.806, "o", "\r\n"]
[806.816, "o", "    positive_code : bool, default=False\r\n"]
[806.826, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[806.836, "o", "\r\n"]
[806.846, "o", "        .. versionadded:: 0.20\r\n"]
[806.856, "o", "\r\n"]
[806.866, "o", "    method_max_iter : int, default=1000\r\n"]
[806.876, "o", "        Maximum number of iterations to perform.\r\n"]
[806.886, "o", "\r\n"]
[806.896, "o", "        .. versionadded:: 0.22\r\n"]
[806.906, "o", "\r\n"]
[806.916, "o", "    Returns\r\n"]
[806.926, "o", "    -------\r\n"]
[806.936, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[806.946, "o", "        The sparse code factor in the matrix factorization.\r\n"]
[806.956, "o", "\r\n"]
[806.966, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[806.976, "o", "        The dictionary factor in the matrix factorization.\r\n"]
[806.986, "o", "\r\n"]
[806.996, "o", "    errors : array\r\n"]
[807.006, "o", "        Vector of errors at each iteration.\r\n"]
[807.016, "o", "\r\n"]
[807.026, "o", "    n_iter : int\r\n"]
[807.036, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[807.046, "o", "        set to True.\r\n"]
[807.056, "o", "\r\n"]
[807.066, "o", "    See Also\r\n"]
[807.076, "o", "    --------\r\n"]
[807.086, "o", "    dict_learning_online : Solve a dictionary learning matrix factorization\r\n"]
[807.096, "o", "        problem online.\r\n"]
[807.106, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[807.116, "o", "    MiniBatchDictionaryLearning : A faster, less accurate version\r\n"]
[807.126, "o", "        of the dictionary learning algorithm.\r\n"]
[807.136, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[807.146, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[807.156, "o", "    \"\"\"\r\n"]
[807.166, "o", "    estimator = DictionaryLearning(\r\n"]
[807.176, "o", "        n_components=n_components,\r\n"]
[807.186, "o", "        alpha=alpha,\r\n"]
[807.196, "o", "        max_iter=max_iter,\r\n"]
[807.206, "o", "        tol=tol,\r\n"]
[807.216, "o", "        fit_algorithm=method,\r\n"]
[807.226, "o", "        n_jobs=n_jobs,\r\n"]
[807.236, "o", "        dict_init=dict_init,\r\n"]
[807.246, "o", "        callback=callback,\r\n"]
[807.256, "o", "        code_init=code_init,\r\n"]
[807.266, "o", "        verbose=verbose,\r\n"]
[807.276, "o", "        random_state=random_state,\r\n"]
[807.286, "o", "        positive_code=positive_code,\r\n"]
[807.296, "o", "        positive_dict=positive_dict,\r\n"]
[807.306, "o", "        transform_max_iter=method_max_iter,\r\n"]
[807.316, "o", "    )\r\n"]
[807.326, "o", "    code = estimator.fit_transform(X)\r\n"]
[807.336, "o", "    if return_n_iter:\r\n"]
[807.346, "o", "        return (\r\n"]
[807.356, "o", "            code,\r\n"]
[807.366, "o", "            estimator.components_,\r\n"]
[807.376, "o", "            estimator.error_,\r\n"]
[807.386, "o", "            estimator.n_iter_,\r\n"]
[807.396, "o", "        )\r\n"]
[807.406, "o", "    return code, estimator.components_, estimator.error_\r\n"]
[807.416, "o", "\r\n"]
[807.426, "o", "\r\n"]
[807.436, "o", "class _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n"]
[807.446, "o", "    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n"]
[807.456, "o", "\r\n"]
[807.466, "o", "    def __init__(\r\n"]
[807.476, "o", "        self,\r\n"]
[807.486, "o", "        transform_algorithm,\r\n"]
[807.496, "o", "        transform_n_nonzero_coefs,\r\n"]
[807.506, "o", "        transform_alpha,\r\n"]
[807.516, "o", "        split_sign,\r\n"]
[807.526, "o", "        n_jobs,\r\n"]
[807.536, "o", "        positive_code,\r\n"]
[807.546, "o", "        transform_max_iter,\r\n"]
[807.556, "o", "    ):\r\n"]
[807.566, "o", "        self.transform_algorithm = transform_algorithm\r\n"]
[807.576, "o", "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n"]
[807.586, "o", "        self.transform_alpha = transform_alpha\r\n"]
[807.596, "o", "        self.transform_max_iter = transform_max_iter\r\n"]
[807.606, "o", "        self.split_sign = split_sign\r\n"]
[807.616, "o", "        self.n_jobs = n_jobs\r\n"]
[807.626, "o", "        self.positive_code = positive_code\r\n"]
[807.636, "o", "\r\n"]
[807.646, "o", "    def _transform(self, X, dictionary):\r\n"]
[807.656, "o", "        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n"]
[807.666, "o", "        SparseCoder.\"\"\"\r\n"]
[807.676, "o", "        X = self._validate_data(X, reset=False)\r\n"]
[807.686, "o", "\r\n"]
[807.696, "o", "        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n"]
[807.706, "o", "            transform_alpha = self.alpha\r\n"]
[807.716, "o", "        else:\r\n"]
[807.726, "o", "            transform_alpha = self.transform_alpha\r\n"]
[807.736, "o", "\r\n"]
[807.746, "o", "        code = sparse_encode(\r\n"]
[807.756, "o", "            X,\r\n"]
[807.766, "o", "            dictionary,\r\n"]
[807.776, "o", "            algorithm=self.transform_algorithm,\r\n"]
[807.786, "o", "            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n"]
[807.796, "o", "            alpha=transform_alpha,\r\n"]
[807.806, "o", "            max_iter=self.transform_max_iter,\r\n"]
[807.816, "o", "            n_jobs=self.n_jobs,\r\n"]
[807.826, "o", "            positive=self.positive_code,\r\n"]
[807.836, "o", "        )\r\n"]
[807.846, "o", "\r\n"]
[807.856, "o", "        if self.split_sign:\r\n"]
[807.866, "o", "            # feature vector is split into a positive and negative side\r\n"]
[807.876, "o", "            n_samples, n_features = code.shape\r\n"]
[807.886, "o", "            split_code = np.empty((n_samples, 2 * n_features))\r\n"]
[807.896, "o", "            split_code[:, :n_features] = np.maximum(code, 0)\r\n"]
[807.906, "o", "            split_code[:, n_features:] = -np.minimum(code, 0)\r\n"]
[807.916, "o", "            code = split_code\r\n"]
[807.926, "o", "\r\n"]
[807.936, "o", "        return code\r\n"]
[807.946, "o", "\r\n"]
[807.956, "o", "    def transform(self, X):\r\n"]
[807.966, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[807.976, "o", "\r\n"]
[807.986, "o", "        Coding method is determined by the object parameter\r\n"]
[807.996, "o", "        `transform_algorithm`.\r\n"]
[808.006, "o", "\r\n"]
[808.016, "o", "        Parameters\r\n"]
[808.026, "o", "        ----------\r\n"]
[808.036, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[808.046, "o", "            Test data to be transformed, must have the same number of\r\n"]
[808.056, "o", "            features as the data used to train the model.\r\n"]
[808.066, "o", "\r\n"]
[808.076, "o", "        Returns\r\n"]
[808.086, "o", "        -------\r\n"]
[808.096, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[808.106, "o", "            Transformed data.\r\n"]
[808.116, "o", "        \"\"\"\r\n"]
[808.126, "o", "        check_is_fitted(self)\r\n"]
[808.136, "o", "        return self._transform(X, self.components_)\r\n"]
[808.146, "o", "\r\n"]
[808.156, "o", "\r\n"]
[808.166, "o", "class SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n"]
[808.176, "o", "    \"\"\"Sparse coding.\r\n"]
[808.186, "o", "\r\n"]
[808.196, "o", "    Finds a sparse representation of data against a fixed, precomputed\r\n"]
[808.206, "o", "    dictionary.\r\n"]
[808.216, "o", "\r\n"]
[808.226, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[808.236, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[808.246, "o", "\r\n"]
[808.256, "o", "        X ~= code * dictionary\r\n"]
[808.266, "o", "\r\n"]
[808.276, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[808.286, "o", "\r\n"]
[808.296, "o", "    Parameters\r\n"]
[808.306, "o", "    ----------\r\n"]
[808.316, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[808.326, "o", "        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n"]
[808.336, "o", "        normalized to unit norm.\r\n"]
[808.346, "o", "\r\n"]
[808.356, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[808.366, "o", "            'threshold'}, default='omp'\r\n"]
[808.376, "o", "        Algorithm used to transform the data:\r\n"]
[808.386, "o", "\r\n"]
[808.396, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[808.406, "o", "          (`linear_model.lars_path`);\r\n"]
[808.416, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[808.426, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[808.436, "o", "          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n"]
[808.446, "o", "          the estimated components are sparse;\r\n"]
[808.456, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[808.466, "o", "          solution;\r\n"]
[808.476, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[808.486, "o", "          the projection ``dictionary * X'``.\r\n"]
[808.496, "o", "\r\n"]
[808.506, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[808.516, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[808.526, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[808.536, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[808.546, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[808.556, "o", "\r\n"]
[808.566, "o", "    transform_alpha : float, default=None\r\n"]
[808.576, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[808.586, "o", "        penalty applied to the L1 norm.\r\n"]
[808.596, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[808.606, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[808.616, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[808.626, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[808.636, "o", "        `n_nonzero_coefs`.\r\n"]
[808.646, "o", "        If `None`, default to 1.\r\n"]
[808.656, "o", "\r\n"]
[808.666, "o", "    split_sign : bool, default=False\r\n"]
[808.676, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[808.686, "o", "        its negative part and its positive part. This can improve the\r\n"]
[808.696, "o", "        performance of downstream classifiers.\r\n"]
[808.706, "o", "\r\n"]
[808.716, "o", "    n_jobs : int, default=None\r\n"]
[808.726, "o", "        Number of parallel jobs to run.\r\n"]
[808.736, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[808.746, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[808.756, "o", "        for more details.\r\n"]
[808.766, "o", "\r\n"]
[808.776, "o", "    positive_code : bool, default=False\r\n"]
[808.786, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[808.796, "o", "\r\n"]
[808.806, "o", "        .. versionadded:: 0.20\r\n"]
[808.816, "o", "\r\n"]
[808.826, "o", "    transform_max_iter : int, default=1000\r\n"]
[808.836, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[808.846, "o", "        `lasso_lars`.\r\n"]
[808.856, "o", "\r\n"]
[808.866, "o", "        .. versionadded:: 0.22\r\n"]
[808.876, "o", "\r\n"]
[808.886, "o", "    Attributes\r\n"]
[808.896, "o", "    ----------\r\n"]
[808.906, "o", "    n_components_ : int\r\n"]
[808.916, "o", "        Number of atoms.\r\n"]
[808.926, "o", "\r\n"]
[808.936, "o", "    n_features_in_ : int\r\n"]
[808.946, "o", "        Number of features seen during :term:`fit`.\r\n"]
[808.956, "o", "\r\n"]
[808.966, "o", "        .. versionadded:: 0.24\r\n"]
[808.976, "o", "\r\n"]
[808.986, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[808.996, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[809.006, "o", "        has feature names that are all strings.\r\n"]
[809.016, "o", "\r\n"]
[809.026, "o", "        .. versionadded:: 1.0\r\n"]
[809.036, "o", "\r\n"]
[809.046, "o", "    See Also\r\n"]
[809.056, "o", "    --------\r\n"]
[809.066, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[809.076, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n"]
[809.086, "o", "        dictionary learning algorithm.\r\n"]
[809.096, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[809.106, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[809.116, "o", "    sparse_encode : Sparse coding where each row of the result is the solution\r\n"]
[809.126, "o", "        to a sparse coding problem.\r\n"]
[809.136, "o", "\r\n"]
[809.146, "o", "    Examples\r\n"]
[809.156, "o", "    --------\r\n"]
[809.166, "o", "    >>> import numpy as np\r\n"]
[809.176, "o", "    >>> from sklearn.decomposition import SparseCoder\r\n"]
[809.186, "o", "    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n"]
[809.196, "o", "    >>> dictionary = np.array(\r\n"]
[809.206, "o", "    ...     [[0, 1, 0],\r\n"]
[809.216, "o", "    ...      [-1, -1, 2],\r\n"]
[809.226, "o", "    ...      [1, 1, 1],\r\n"]
[809.236, "o", "    ...      [0, 1, 1],\r\n"]
[809.246, "o", "    ...      [0, 2, 1]],\r\n"]
[809.256, "o", "    ...    dtype=np.float64\r\n"]
[809.266, "o", "    ... )\r\n"]
[809.276, "o", "    >>> coder = SparseCoder(\r\n"]
[809.286, "o", "    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n"]
[809.296, "o", "    ...     transform_alpha=1e-10,\r\n"]
[809.306, "o", "    ... )\r\n"]
[809.316, "o", "    >>> coder.transform(X)\r\n"]
[809.326, "o", "    array([[ 0.,  0., -1.,  0.,  0.],\r\n"]
[809.336, "o", "           [ 0.,  1.,  1.,  0.,  0.]])\r\n"]
[809.346, "o", "    \"\"\"\r\n"]
[809.356, "o", "\r\n"]
[809.366, "o", "    _required_parameters = [\"dictionary\"]\r\n"]
[809.376, "o", "\r\n"]
[809.386, "o", "    def __init__(\r\n"]
[809.396, "o", "        self,\r\n"]
[809.406, "o", "        dictionary,\r\n"]
[809.416, "o", "        *,\r\n"]
[809.426, "o", "        transform_algorithm=\"omp\",\r\n"]
[809.436, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[809.446, "o", "        transform_alpha=None,\r\n"]
[809.456, "o", "        split_sign=False,\r\n"]
[809.466, "o", "        n_jobs=None,\r\n"]
[809.476, "o", "        positive_code=False,\r\n"]
[809.486, "o", "        transform_max_iter=1000,\r\n"]
[809.496, "o", "    ):\r\n"]
[809.506, "o", "        super().__init__(\r\n"]
[809.516, "o", "            transform_algorithm,\r\n"]
[809.526, "o", "            transform_n_nonzero_coefs,\r\n"]
[809.536, "o", "            transform_alpha,\r\n"]
[809.546, "o", "            split_sign,\r\n"]
[809.556, "o", "            n_jobs,\r\n"]
[809.566, "o", "            positive_code,\r\n"]
[809.576, "o", "            transform_max_iter,\r\n"]
[809.586, "o", "        )\r\n"]
[809.596, "o", "        self.dictionary = dictionary\r\n"]
[809.606, "o", "\r\n"]
[809.616, "o", "    def fit(self, X, y=None):\r\n"]
[809.626, "o", "        \"\"\"Do nothing and return the estimator unchanged.\r\n"]
[809.636, "o", "\r\n"]
[809.646, "o", "        This method is just there to implement the usual API and hence\r\n"]
[809.656, "o", "        work in pipelines.\r\n"]
[809.666, "o", "\r\n"]
[809.676, "o", "        Parameters\r\n"]
[809.686, "o", "        ----------\r\n"]
[809.696, "o", "        X : Ignored\r\n"]
[809.706, "o", "            Not used, present for API consistency by convention.\r\n"]
[809.716, "o", "\r\n"]
[809.726, "o", "        y : Ignored\r\n"]
[809.736, "o", "            Not used, present for API consistency by convention.\r\n"]
[809.746, "o", "\r\n"]
[809.756, "o", "        Returns\r\n"]
[809.766, "o", "        -------\r\n"]
[809.776, "o", "        self : object\r\n"]
[809.786, "o", "            Returns the instance itself.\r\n"]
[809.796, "o", "        \"\"\"\r\n"]
[809.806, "o", "        return self\r\n"]
[809.816, "o", "\r\n"]
[809.826, "o", "    def transform(self, X, y=None):\r\n"]
[809.836, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[809.846, "o", "\r\n"]
[809.856, "o", "        Coding method is determined by the object parameter\r\n"]
[809.866, "o", "        `transform_algorithm`.\r\n"]
[809.876, "o", "\r\n"]
[809.886, "o", "        Parameters\r\n"]
[809.896, "o", "        ----------\r\n"]
[809.906, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[809.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[809.926, "o", "            and `n_features` is the number of features.\r\n"]
[809.936, "o", "\r\n"]
[810.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[810.002, "i", "sed -n '1800,2600p' sklearn/decomposition/_dict_learning.py\r"]
[810.004, "o", "sed -n '1800,2600p' sklearn/decomposition/_dict_learning.py\r\n"]
[810.016, "o", "\u001b[?2004l\r\n"]
[810.026, "o", "        self.fit_transform(X)\r\n"]
[810.036, "o", "        return self\r\n"]
[810.046, "o", "\r\n"]
[810.056, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[810.066, "o", "    def fit_transform(self, X, y=None):\r\n"]
[810.076, "o", "        \"\"\"Fit the model from data in X and return the transformed data.\r\n"]
[810.086, "o", "\r\n"]
[810.096, "o", "        Parameters\r\n"]
[810.106, "o", "        ----------\r\n"]
[810.116, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[810.126, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[810.136, "o", "            and `n_features` is the number of features.\r\n"]
[810.146, "o", "\r\n"]
[810.156, "o", "        y : Ignored\r\n"]
[810.166, "o", "            Not used, present for API consistency by convention.\r\n"]
[810.176, "o", "\r\n"]
[810.186, "o", "        Returns\r\n"]
[810.196, "o", "        -------\r\n"]
[810.206, "o", "        V : ndarray of shape (n_samples, n_components)\r\n"]
[810.216, "o", "            Transformed data.\r\n"]
[810.226, "o", "        \"\"\"\r\n"]
[810.236, "o", "        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n"]
[810.246, "o", "\r\n"]
[810.256, "o", "        method = \"lasso_\" + self.fit_algorithm\r\n"]
[810.266, "o", "\r\n"]
[810.276, "o", "        random_state = check_random_state(self.random_state)\r\n"]
[810.286, "o", "        X = self._validate_data(X)\r\n"]
[810.296, "o", "\r\n"]
[810.306, "o", "        if self.n_components is None:\r\n"]
[810.316, "o", "            n_components = X.shape[1]\r\n"]
[810.326, "o", "        else:\r\n"]
[810.336, "o", "            n_components = self.n_components\r\n"]
[810.346, "o", "\r\n"]
[810.356, "o", "        V, U, E, self.n_iter_ = _dict_learning(\r\n"]
[810.366, "o", "            X,\r\n"]
[810.376, "o", "            n_components,\r\n"]
[810.386, "o", "            alpha=self.alpha,\r\n"]
[810.396, "o", "            tol=self.tol,\r\n"]
[810.406, "o", "            max_iter=self.max_iter,\r\n"]
[810.416, "o", "            method=method,\r\n"]
[810.426, "o", "            method_max_iter=self.transform_max_iter,\r\n"]
[810.436, "o", "            n_jobs=self.n_jobs,\r\n"]
[810.446, "o", "            code_init=self.code_init,\r\n"]
[810.456, "o", "            dict_init=self.dict_init,\r\n"]
[810.466, "o", "            callback=self.callback,\r\n"]
[810.476, "o", "            verbose=self.verbose,\r\n"]
[810.486, "o", "            random_state=random_state,\r\n"]
[810.496, "o", "            return_n_iter=True,\r\n"]
[810.506, "o", "            positive_dict=self.positive_dict,\r\n"]
[810.516, "o", "            positive_code=self.positive_code,\r\n"]
[810.526, "o", "        )\r\n"]
[810.536, "o", "        self.components_ = U\r\n"]
[810.546, "o", "        self.error_ = E\r\n"]
[810.556, "o", "\r\n"]
[810.566, "o", "        return V\r\n"]
[810.576, "o", "\r\n"]
[810.586, "o", "    @property\r\n"]
[810.596, "o", "    def _n_features_out(self):\r\n"]
[810.606, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[810.616, "o", "        return self.components_.shape[0]\r\n"]
[810.626, "o", "\r\n"]
[810.636, "o", "    def _more_tags(self):\r\n"]
[810.646, "o", "        return {\r\n"]
[810.656, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[810.666, "o", "        }\r\n"]
[810.676, "o", "\r\n"]
[810.686, "o", "\r\n"]
[810.696, "o", "class MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[810.706, "o", "    \"\"\"Mini-batch dictionary learning.\r\n"]
[810.716, "o", "\r\n"]
[810.726, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[810.736, "o", "    encoding the fitted data.\r\n"]
[810.746, "o", "\r\n"]
[810.756, "o", "    Solves the optimization problem::\r\n"]
[810.766, "o", "\r\n"]
[810.776, "o", "       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[810.786, "o", "                    (U,V)\r\n"]
[810.796, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[810.806, "o", "\r\n"]
[810.816, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[810.826, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[810.836, "o", "    of all the entries in the matrix.\r\n"]
[810.846, "o", "\r\n"]
[810.856, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[810.866, "o", "\r\n"]
[810.876, "o", "    Parameters\r\n"]
[810.886, "o", "    ----------\r\n"]
[810.896, "o", "    n_components : int, default=None\r\n"]
[810.906, "o", "        Number of dictionary elements to extract.\r\n"]
[810.916, "o", "\r\n"]
[810.926, "o", "    alpha : float, default=1\r\n"]
[810.936, "o", "        Sparsity controlling parameter.\r\n"]
[810.946, "o", "\r\n"]
[810.956, "o", "    n_iter : int, default=1000\r\n"]
[810.966, "o", "        Total number of iterations over data batches to perform.\r\n"]
[810.976, "o", "\r\n"]
[810.986, "o", "        .. deprecated:: 1.1\r\n"]
[810.996, "o", "           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[811.006, "o", "           ``max_iter`` instead.\r\n"]
[811.016, "o", "\r\n"]
[811.026, "o", "    max_iter : int, default=None\r\n"]
[811.036, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[811.046, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[811.056, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[811.066, "o", "\r\n"]
[811.076, "o", "        .. versionadded:: 1.1\r\n"]
[811.086, "o", "\r\n"]
[811.096, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[811.106, "o", "        The algorithm used:\r\n"]
[811.116, "o", "\r\n"]
[811.126, "o", "        - `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[811.136, "o", "          problem (`linear_model.lars_path`)\r\n"]
[811.146, "o", "        - `'cd'`: uses the coordinate descent method to compute the\r\n"]
[811.156, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[811.166, "o", "          the estimated components are sparse.\r\n"]
[811.176, "o", "\r\n"]
[811.186, "o", "    n_jobs : int, default=None\r\n"]
[811.196, "o", "        Number of parallel jobs to run.\r\n"]
[811.206, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[811.216, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[811.226, "o", "        for more details.\r\n"]
[811.236, "o", "\r\n"]
[811.246, "o", "    batch_size : int, default=256\r\n"]
[811.256, "o", "        Number of samples in each mini-batch.\r\n"]
[811.266, "o", "\r\n"]
[811.276, "o", "        .. versionchanged:: 1.3\r\n"]
[811.286, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[811.296, "o", "\r\n"]
[811.306, "o", "    shuffle : bool, default=True\r\n"]
[811.316, "o", "        Whether to shuffle the samples before forming batches.\r\n"]
[811.326, "o", "\r\n"]
[811.336, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[811.346, "o", "        Initial value of the dictionary for warm restart scenarios.\r\n"]
[811.356, "o", "\r\n"]
[811.366, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[811.376, "o", "            'threshold'}, default='omp'\r\n"]
[811.386, "o", "        Algorithm used to transform the data:\r\n"]
[811.396, "o", "\r\n"]
[811.406, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[811.416, "o", "          (`linear_model.lars_path`);\r\n"]
[811.426, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[811.436, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[811.446, "o", "          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n"]
[811.456, "o", "          if the estimated components are sparse.\r\n"]
[811.466, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[811.476, "o", "          solution.\r\n"]
[811.486, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[811.496, "o", "          the projection ``dictionary * X'``.\r\n"]
[811.506, "o", "\r\n"]
[811.516, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[811.526, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[811.536, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[811.546, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[811.556, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[811.566, "o", "\r\n"]
[811.576, "o", "    transform_alpha : float, default=None\r\n"]
[811.586, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[811.596, "o", "        penalty applied to the L1 norm.\r\n"]
[811.606, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[811.616, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[811.626, "o", "        If `None`, defaults to `alpha`.\r\n"]
[811.636, "o", "\r\n"]
[811.646, "o", "        .. versionchanged:: 1.2\r\n"]
[811.656, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[811.666, "o", "\r\n"]
[811.676, "o", "    verbose : bool or int, default=False\r\n"]
[811.686, "o", "        To control the verbosity of the procedure.\r\n"]
[811.696, "o", "\r\n"]
[811.706, "o", "    split_sign : bool, default=False\r\n"]
[811.716, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[811.726, "o", "        its negative part and its positive part. This can improve the\r\n"]
[811.736, "o", "        performance of downstream classifiers.\r\n"]
[811.746, "o", "\r\n"]
[811.756, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[811.766, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[811.776, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[811.786, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[811.796, "o", "        results across multiple function calls.\r\n"]
[811.806, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[811.816, "o", "\r\n"]
[811.826, "o", "    positive_code : bool, default=False\r\n"]
[811.836, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[811.846, "o", "\r\n"]
[811.856, "o", "        .. versionadded:: 0.20\r\n"]
[811.866, "o", "\r\n"]
[811.876, "o", "    positive_dict : bool, default=False\r\n"]
[811.886, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[811.896, "o", "\r\n"]
[811.906, "o", "        .. versionadded:: 0.20\r\n"]
[811.916, "o", "\r\n"]
[811.926, "o", "    transform_max_iter : int, default=1000\r\n"]
[811.936, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[811.946, "o", "        `'lasso_lars'`.\r\n"]
[811.956, "o", "\r\n"]
[811.966, "o", "        .. versionadded:: 0.22\r\n"]
[811.976, "o", "\r\n"]
[811.986, "o", "    callback : callable, default=None\r\n"]
[811.996, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[812.006, "o", "\r\n"]
[812.016, "o", "        .. versionadded:: 1.1\r\n"]
[812.026, "o", "\r\n"]
[812.036, "o", "    tol : float, default=1e-3\r\n"]
[812.046, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[812.056, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[812.066, "o", "\r\n"]
[812.076, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[812.086, "o", "        `tol` to 0.0.\r\n"]
[812.096, "o", "\r\n"]
[812.106, "o", "        .. versionadded:: 1.1\r\n"]
[812.116, "o", "\r\n"]
[812.126, "o", "    max_no_improvement : int, default=10\r\n"]
[812.136, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[812.146, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[812.156, "o", "        `max_iter` is not None.\r\n"]
[812.166, "o", "\r\n"]
[812.176, "o", "        To disable convergence detection based on cost function, set\r\n"]
[812.186, "o", "        `max_no_improvement` to None.\r\n"]
[812.196, "o", "\r\n"]
[812.206, "o", "        .. versionadded:: 1.1\r\n"]
[812.216, "o", "\r\n"]
[812.226, "o", "    Attributes\r\n"]
[812.236, "o", "    ----------\r\n"]
[812.246, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[812.256, "o", "        Components extracted from the data.\r\n"]
[812.266, "o", "\r\n"]
[812.276, "o", "    n_features_in_ : int\r\n"]
[812.286, "o", "        Number of features seen during :term:`fit`.\r\n"]
[812.296, "o", "\r\n"]
[812.306, "o", "        .. versionadded:: 0.24\r\n"]
[812.316, "o", "\r\n"]
[812.326, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[812.336, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[812.346, "o", "        has feature names that are all strings.\r\n"]
[812.356, "o", "\r\n"]
[812.366, "o", "        .. versionadded:: 1.0\r\n"]
[812.376, "o", "\r\n"]
[812.386, "o", "    n_iter_ : int\r\n"]
[812.396, "o", "        Number of iterations over the full dataset.\r\n"]
[812.406, "o", "\r\n"]
[812.416, "o", "    n_steps_ : int\r\n"]
[812.426, "o", "        Number of mini-batches processed.\r\n"]
[812.436, "o", "\r\n"]
[812.446, "o", "        .. versionadded:: 1.1\r\n"]
[812.456, "o", "\r\n"]
[812.466, "o", "    See Also\r\n"]
[812.476, "o", "    --------\r\n"]
[812.486, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[812.496, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[812.506, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[812.516, "o", "        precomputed dictionary.\r\n"]
[812.526, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[812.536, "o", "\r\n"]
[812.546, "o", "    References\r\n"]
[812.556, "o", "    ----------\r\n"]
[812.566, "o", "\r\n"]
[812.576, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[812.586, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[812.596, "o", "\r\n"]
[812.606, "o", "    Examples\r\n"]
[812.616, "o", "    --------\r\n"]
[812.626, "o", "    >>> import numpy as np\r\n"]
[812.636, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[812.646, "o", "    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n"]
[812.656, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[812.666, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[812.676, "o", "    ...     random_state=42)\r\n"]
[812.686, "o", "    >>> dict_learner = MiniBatchDictionaryLearning(\r\n"]
[812.696, "o", "    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n"]
[812.706, "o", "    ...     transform_alpha=0.1, random_state=42)\r\n"]
[812.716, "o", "    >>> X_transformed = dict_learner.fit_transform(X)\r\n"]
[812.726, "o", "\r\n"]
[812.736, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[812.746, "o", "\r\n"]
[812.756, "o", "    >>> np.mean(X_transformed == 0) < 0.5\r\n"]
[812.766, "o", "    True\r\n"]
[812.776, "o", "\r\n"]
[812.786, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[812.796, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[812.806, "o", "    the original signal:\r\n"]
[812.816, "o", "\r\n"]
[812.826, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[812.836, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[812.846, "o", "    0.057...\r\n"]
[812.856, "o", "    \"\"\"\r\n"]
[812.866, "o", "\r\n"]
[812.876, "o", "    _parameter_constraints: dict = {\r\n"]
[812.886, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[812.896, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[812.906, "o", "        \"n_iter\": [\r\n"]
[812.916, "o", "            Interval(Integral, 0, None, closed=\"left\"),\r\n"]
[812.926, "o", "            Hidden(StrOptions({\"deprecated\"})),\r\n"]
[812.936, "o", "        ],\r\n"]
[812.946, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n"]
[812.956, "o", "        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n"]
[812.966, "o", "        \"n_jobs\": [None, Integral],\r\n"]
[812.976, "o", "        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n"]
[812.986, "o", "        \"shuffle\": [\"boolean\"],\r\n"]
[812.996, "o", "        \"dict_init\": [None, np.ndarray],\r\n"]
[813.006, "o", "        \"transform_algorithm\": [\r\n"]
[813.016, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[813.026, "o", "        ],\r\n"]
[813.036, "o", "        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[813.046, "o", "        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[813.056, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[813.066, "o", "        \"split_sign\": [\"boolean\"],\r\n"]
[813.076, "o", "        \"random_state\": [\"random_state\"],\r\n"]
[813.086, "o", "        \"positive_code\": [\"boolean\"],\r\n"]
[813.096, "o", "        \"positive_dict\": [\"boolean\"],\r\n"]
[813.106, "o", "        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[813.116, "o", "        \"callback\": [None, callable],\r\n"]
[813.126, "o", "        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[813.136, "o", "        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n"]
[813.146, "o", "    }\r\n"]
[813.156, "o", "\r\n"]
[813.166, "o", "    def __init__(\r\n"]
[813.176, "o", "        self,\r\n"]
[813.186, "o", "        n_components=None,\r\n"]
[813.196, "o", "        *,\r\n"]
[813.206, "o", "        alpha=1,\r\n"]
[813.216, "o", "        n_iter=\"deprecated\",\r\n"]
[813.226, "o", "        max_iter=None,\r\n"]
[813.236, "o", "        fit_algorithm=\"lars\",\r\n"]
[813.246, "o", "        n_jobs=None,\r\n"]
[813.256, "o", "        batch_size=256,\r\n"]
[813.266, "o", "        shuffle=True,\r\n"]
[813.276, "o", "        dict_init=None,\r\n"]
[813.286, "o", "        transform_algorithm=\"omp\",\r\n"]
[813.296, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[813.306, "o", "        transform_alpha=None,\r\n"]
[813.316, "o", "        verbose=False,\r\n"]
[813.326, "o", "        split_sign=False,\r\n"]
[813.336, "o", "        random_state=None,\r\n"]
[813.346, "o", "        positive_code=False,\r\n"]
[813.356, "o", "        positive_dict=False,\r\n"]
[813.366, "o", "        transform_max_iter=1000,\r\n"]
[813.376, "o", "        callback=None,\r\n"]
[813.386, "o", "        tol=1e-3,\r\n"]
[813.396, "o", "        max_no_improvement=10,\r\n"]
[813.406, "o", "    ):\r\n"]
[813.416, "o", "        super().__init__(\r\n"]
[813.426, "o", "            transform_algorithm,\r\n"]
[813.436, "o", "            transform_n_nonzero_coefs,\r\n"]
[813.446, "o", "            transform_alpha,\r\n"]
[813.456, "o", "            split_sign,\r\n"]
[813.466, "o", "            n_jobs,\r\n"]
[813.476, "o", "            positive_code,\r\n"]
[813.486, "o", "            transform_max_iter,\r\n"]
[813.496, "o", "        )\r\n"]
[813.506, "o", "        self.n_components = n_components\r\n"]
[813.516, "o", "        self.alpha = alpha\r\n"]
[813.526, "o", "        self.n_iter = n_iter\r\n"]
[813.536, "o", "        self.max_iter = max_iter\r\n"]
[813.546, "o", "        self.fit_algorithm = fit_algorithm\r\n"]
[813.556, "o", "        self.dict_init = dict_init\r\n"]
[813.566, "o", "        self.verbose = verbose\r\n"]
[813.576, "o", "        self.shuffle = shuffle\r\n"]
[813.586, "o", "        self.batch_size = batch_size\r\n"]
[813.596, "o", "        self.split_sign = split_sign\r\n"]
[813.606, "o", "        self.random_state = random_state\r\n"]
[813.616, "o", "        self.positive_dict = positive_dict\r\n"]
[813.626, "o", "        self.callback = callback\r\n"]
[813.636, "o", "        self.max_no_improvement = max_no_improvement\r\n"]
[813.646, "o", "        self.tol = tol\r\n"]
[813.656, "o", "\r\n"]
[813.666, "o", "    def _check_params(self, X):\r\n"]
[813.676, "o", "        # n_components\r\n"]
[813.686, "o", "        self._n_components = self.n_components\r\n"]
[813.696, "o", "        if self._n_components is None:\r\n"]
[813.706, "o", "            self._n_components = X.shape[1]\r\n"]
[813.716, "o", "\r\n"]
[813.726, "o", "        # fit_algorithm\r\n"]
[813.736, "o", "        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n"]
[813.746, "o", "        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n"]
[813.756, "o", "\r\n"]
[813.766, "o", "        # batch_size\r\n"]
[813.776, "o", "        self._batch_size = min(self.batch_size, X.shape[0])\r\n"]
[813.786, "o", "\r\n"]
[813.796, "o", "    def _initialize_dict(self, X, random_state):\r\n"]
[813.806, "o", "        \"\"\"Initialization of the dictionary.\"\"\"\r\n"]
[813.816, "o", "        if self.dict_init is not None:\r\n"]
[813.826, "o", "            dictionary = self.dict_init\r\n"]
[813.836, "o", "        else:\r\n"]
[813.846, "o", "            # Init V with SVD of X\r\n"]
[813.856, "o", "            _, S, dictionary = randomized_svd(\r\n"]
[813.866, "o", "                X, self._n_components, random_state=random_state\r\n"]
[813.876, "o", "            )\r\n"]
[813.886, "o", "            dictionary = S[:, np.newaxis] * dictionary\r\n"]
[813.896, "o", "\r\n"]
[813.906, "o", "        if self._n_components <= len(dictionary):\r\n"]
[813.916, "o", "            dictionary = dictionary[: self._n_components, :]\r\n"]
[813.926, "o", "        else:\r\n"]
[813.936, "o", "            dictionary = np.concatenate(\r\n"]
[813.946, "o", "                (\r\n"]
[813.956, "o", "                    dictionary,\r\n"]
[813.966, "o", "                    np.zeros(\r\n"]
[813.976, "o", "                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n"]
[813.986, "o", "                        dtype=dictionary.dtype,\r\n"]
[813.996, "o", "                    ),\r\n"]
[814.006, "o", "                )\r\n"]
[814.016, "o", "            )\r\n"]
[814.026, "o", "\r\n"]
[814.036, "o", "        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n"]
[814.046, "o", "        dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[814.056, "o", "\r\n"]
[814.066, "o", "        return dictionary\r\n"]
[814.076, "o", "\r\n"]
[814.086, "o", "    def _update_inner_stats(self, X, code, batch_size, step):\r\n"]
[814.096, "o", "        \"\"\"Update the inner stats inplace.\"\"\"\r\n"]
[814.106, "o", "        if step < batch_size - 1:\r\n"]
[814.116, "o", "            theta = (step + 1) * batch_size\r\n"]
[814.126, "o", "        else:\r\n"]
[814.136, "o", "            theta = batch_size**2 + step + 1 - batch_size\r\n"]
[814.146, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[814.156, "o", "\r\n"]
[814.166, "o", "        self._A *= beta\r\n"]
[814.176, "o", "        self._A += code.T @ code / batch_size\r\n"]
[814.186, "o", "        self._B *= beta\r\n"]
[814.196, "o", "        self._B += X.T @ code / batch_size\r\n"]
[814.206, "o", "\r\n"]
[814.216, "o", "    def _minibatch_step(self, X, dictionary, random_state, step):\r\n"]
[814.226, "o", "        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n"]
[814.236, "o", "        batch_size = X.shape[0]\r\n"]
[814.246, "o", "\r\n"]
[814.256, "o", "        # Compute code for this batch\r\n"]
[814.266, "o", "        code = _sparse_encode(\r\n"]
[814.276, "o", "            X,\r\n"]
[814.286, "o", "            dictionary,\r\n"]
[814.296, "o", "            algorithm=self._fit_algorithm,\r\n"]
[814.306, "o", "            alpha=self.alpha,\r\n"]
[814.316, "o", "            n_jobs=self.n_jobs,\r\n"]
[814.326, "o", "            positive=self.positive_code,\r\n"]
[814.336, "o", "            max_iter=self.transform_max_iter,\r\n"]
[814.346, "o", "            verbose=self.verbose,\r\n"]
[814.356, "o", "        )\r\n"]
[814.366, "o", "\r\n"]
[814.376, "o", "        batch_cost = (\r\n"]
[814.386, "o", "            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n"]
[814.396, "o", "            + self.alpha * np.sum(np.abs(code))\r\n"]
[814.406, "o", "        ) / batch_size\r\n"]
[814.416, "o", "\r\n"]
[814.426, "o", "        # Update inner stats\r\n"]
[814.436, "o", "        self._update_inner_stats(X, code, batch_size, step)\r\n"]
[814.446, "o", "\r\n"]
[814.456, "o", "        # Update dictionary\r\n"]
[814.466, "o", "        _update_dict(\r\n"]
[814.476, "o", "            dictionary,\r\n"]
[814.486, "o", "            X,\r\n"]
[814.496, "o", "            code,\r\n"]
[814.506, "o", "            self._A,\r\n"]
[814.516, "o", "            self._B,\r\n"]
[814.526, "o", "            verbose=self.verbose,\r\n"]
[814.536, "o", "            random_state=random_state,\r\n"]
[814.546, "o", "            positive=self.positive_dict,\r\n"]
[814.556, "o", "        )\r\n"]
[814.566, "o", "\r\n"]
[814.576, "o", "        return batch_cost\r\n"]
[814.586, "o", "\r\n"]
[814.596, "o", "    def _check_convergence(\r\n"]
[814.606, "o", "        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n"]
[814.616, "o", "    ):\r\n"]
[814.626, "o", "        \"\"\"Helper function to encapsulate the early stopping logic.\r\n"]
[814.636, "o", "\r\n"]
[814.646, "o", "        Early stopping is based on two factors:\r\n"]
[814.656, "o", "        - A small change of the dictionary between two minibatch updates. This is\r\n"]
[814.666, "o", "          controlled by the tol parameter.\r\n"]
[814.676, "o", "        - No more improvement on a smoothed estimate of the objective function for a\r\n"]
[814.686, "o", "          a certain number of consecutive minibatch updates. This is controlled by\r\n"]
[814.696, "o", "          the max_no_improvement parameter.\r\n"]
[814.706, "o", "        \"\"\"\r\n"]
[814.716, "o", "        batch_size = X.shape[0]\r\n"]
[814.726, "o", "\r\n"]
[814.736, "o", "        # counts steps starting from 1 for user friendly verbose mode.\r\n"]
[814.746, "o", "        step = step + 1\r\n"]
[814.756, "o", "\r\n"]
[814.766, "o", "        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n"]
[814.776, "o", "        # too bad value\r\n"]
[814.786, "o", "        if step <= min(100, n_samples / batch_size):\r\n"]
[814.796, "o", "            if self.verbose:\r\n"]
[814.806, "o", "                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n"]
[814.816, "o", "            return False\r\n"]
[814.826, "o", "\r\n"]
[814.836, "o", "        # Compute an Exponentially Weighted Average of the cost function to\r\n"]
[814.846, "o", "        # monitor the convergence while discarding minibatch-local stochastic\r\n"]
[814.856, "o", "        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n"]
[814.866, "o", "        if self._ewa_cost is None:\r\n"]
[814.876, "o", "            self._ewa_cost = batch_cost\r\n"]
[814.886, "o", "        else:\r\n"]
[814.896, "o", "            alpha = batch_size / (n_samples + 1)\r\n"]
[814.906, "o", "            alpha = min(alpha, 1)\r\n"]
[814.916, "o", "            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n"]
[814.926, "o", "\r\n"]
[814.936, "o", "        if self.verbose:\r\n"]
[815.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[815.002, "i", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py\r"]
[815.004, "o", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py\r\n"]
[817.478, "o", "\u001b[?2004l\r\n"]
[820.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[820.002, "i", "grep -n \"_sparse_encode_precomputed\\s*\\(\" sklearn/decomposition/_dict_learning.py\r"]
[820.004, "o", "grep -n \"_sparse_encode_precomputed\\s*\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[822.478, "o", "\u001b[?2004l\r\n"]
[825.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[825.002, "i", "grep -n \"cov\\s*=\\s|gram\\s*=\\s\" sklearn/decomposition/_dict_learning.py || true\r"]
[825.004, "o", "grep -n \"cov\\s*=\\s|gram\\s*=\\s\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[825.006, "o", "\u001b[?2004l\r\n"]
[830.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[830.002, "i", "sed -n '1,500p' sklearn/decomposition/_dict_learning.py\r"]
[830.004, "o", "sed -n '1,500p' sklearn/decomposition/_dict_learning.py\r\n"]
[830.016, "o", "\u001b[?2004l\r\n"]
[830.026, "o", "\"\"\" Dictionary learning.\r\n"]
[830.036, "o", "\"\"\"\r\n"]
[830.046, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[830.056, "o", "# License: BSD 3 clause\r\n"]
[830.066, "o", "\r\n"]
[830.076, "o", "import itertools\r\n"]
[830.086, "o", "import sys\r\n"]
[830.096, "o", "import time\r\n"]
[830.106, "o", "import warnings\r\n"]
[830.116, "o", "from math import ceil\r\n"]
[830.126, "o", "from numbers import Integral, Real\r\n"]
[830.136, "o", "\r\n"]
[830.146, "o", "import numpy as np\r\n"]
[830.156, "o", "from joblib import effective_n_jobs\r\n"]
[830.166, "o", "from scipy import linalg\r\n"]
[830.176, "o", "\r\n"]
[830.186, "o", "from ..base import (\r\n"]
[830.196, "o", "    BaseEstimator,\r\n"]
[830.206, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[830.216, "o", "    TransformerMixin,\r\n"]
[830.226, "o", "    _fit_context,\r\n"]
[830.236, "o", ")\r\n"]
[830.246, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[830.256, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[830.266, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[830.276, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[830.286, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[830.296, "o", "from ..utils.validation import check_is_fitted\r\n"]
[830.306, "o", "\r\n"]
[830.316, "o", "\r\n"]
[830.326, "o", "def _check_positive_coding(method, positive):\r\n"]
[830.336, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[830.346, "o", "        raise ValueError(\r\n"]
[830.356, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[830.366, "o", "        )\r\n"]
[830.376, "o", "\r\n"]
[830.386, "o", "\r\n"]
[830.396, "o", "def _sparse_encode_precomputed(\r\n"]
[830.406, "o", "    X,\r\n"]
[830.416, "o", "    dictionary,\r\n"]
[830.426, "o", "    *,\r\n"]
[830.436, "o", "    gram=None,\r\n"]
[830.446, "o", "    cov=None,\r\n"]
[830.456, "o", "    algorithm=\"lasso_lars\",\r\n"]
[830.466, "o", "    regularization=None,\r\n"]
[830.476, "o", "    copy_cov=True,\r\n"]
[830.486, "o", "    init=None,\r\n"]
[830.496, "o", "    max_iter=1000,\r\n"]
[830.506, "o", "    verbose=0,\r\n"]
[830.516, "o", "    positive=False,\r\n"]
[830.526, "o", "):\r\n"]
[830.536, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[830.546, "o", "\r\n"]
[830.556, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[830.566, "o", "\r\n"]
[830.576, "o", "    Parameters\r\n"]
[830.586, "o", "    ----------\r\n"]
[830.596, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[830.606, "o", "        Data matrix.\r\n"]
[830.616, "o", "\r\n"]
[830.626, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[830.636, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[830.646, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[830.656, "o", "\r\n"]
[830.666, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[830.676, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[830.686, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[830.696, "o", "\r\n"]
[830.706, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[830.716, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[830.726, "o", "\r\n"]
[830.736, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[830.746, "o", "            default='lasso_lars'\r\n"]
[830.756, "o", "        The algorithm used:\r\n"]
[830.766, "o", "\r\n"]
[830.776, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[830.786, "o", "          (`linear_model.lars_path`);\r\n"]
[830.796, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[830.806, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[830.816, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[830.826, "o", "          the estimated components are sparse;\r\n"]
[830.836, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[830.846, "o", "          solution;\r\n"]
[830.856, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[830.866, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[830.876, "o", "\r\n"]
[830.886, "o", "    regularization : int or float, default=None\r\n"]
[830.896, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[830.906, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[830.916, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[830.926, "o", "\r\n"]
[830.936, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[830.946, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[830.956, "o", "        `algorithm='lasso_cd'`.\r\n"]
[830.966, "o", "\r\n"]
[830.976, "o", "    max_iter : int, default=1000\r\n"]
[830.986, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[830.996, "o", "        `'lasso_lars'`.\r\n"]
[831.006, "o", "\r\n"]
[831.016, "o", "    copy_cov : bool, default=True\r\n"]
[831.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[831.036, "o", "        be overwritten.\r\n"]
[831.046, "o", "\r\n"]
[831.056, "o", "    verbose : int, default=0\r\n"]
[831.066, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[831.076, "o", "\r\n"]
[831.086, "o", "    positive: bool, default=False\r\n"]
[831.096, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[831.106, "o", "\r\n"]
[831.116, "o", "        .. versionadded:: 0.20\r\n"]
[831.126, "o", "\r\n"]
[831.136, "o", "    Returns\r\n"]
[831.146, "o", "    -------\r\n"]
[831.156, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[831.166, "o", "        The sparse codes.\r\n"]
[831.176, "o", "    \"\"\"\r\n"]
[831.186, "o", "    n_samples, n_features = X.shape\r\n"]
[831.196, "o", "    n_components = dictionary.shape[0]\r\n"]
[831.206, "o", "\r\n"]
[831.216, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[831.226, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[831.236, "o", "        try:\r\n"]
[831.246, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[831.256, "o", "\r\n"]
[831.266, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[831.276, "o", "            # corrects the verbosity level.\r\n"]
[831.286, "o", "            lasso_lars = LassoLars(\r\n"]
[831.296, "o", "                alpha=alpha,\r\n"]
[831.306, "o", "                fit_intercept=False,\r\n"]
[831.316, "o", "                verbose=verbose,\r\n"]
[831.326, "o", "                precompute=gram,\r\n"]
[831.336, "o", "                fit_path=False,\r\n"]
[831.346, "o", "                positive=positive,\r\n"]
[831.356, "o", "                max_iter=max_iter,\r\n"]
[831.366, "o", "            )\r\n"]
[831.376, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[831.386, "o", "            new_code = lasso_lars.coef_\r\n"]
[831.396, "o", "        finally:\r\n"]
[831.406, "o", "            np.seterr(**err_mgt)\r\n"]
[831.416, "o", "\r\n"]
[831.426, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[831.436, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[831.446, "o", "\r\n"]
[831.456, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[831.466, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[831.476, "o", "        # argument that we could pass in from Lasso.\r\n"]
[831.486, "o", "        clf = Lasso(\r\n"]
[831.496, "o", "            alpha=alpha,\r\n"]
[831.506, "o", "            fit_intercept=False,\r\n"]
[831.516, "o", "            precompute=gram,\r\n"]
[831.526, "o", "            max_iter=max_iter,\r\n"]
[831.536, "o", "            warm_start=True,\r\n"]
[831.546, "o", "            positive=positive,\r\n"]
[831.556, "o", "        )\r\n"]
[831.566, "o", "\r\n"]
[831.576, "o", "        if init is not None:\r\n"]
[831.586, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[831.596, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[831.606, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[831.616, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[831.626, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[831.636, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[831.646, "o", "                init = np.array(init)\r\n"]
[831.656, "o", "            clf.coef_ = init\r\n"]
[831.666, "o", "\r\n"]
[831.676, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[831.686, "o", "        new_code = clf.coef_\r\n"]
[831.696, "o", "\r\n"]
[831.706, "o", "    elif algorithm == \"lars\":\r\n"]
[831.716, "o", "        try:\r\n"]
[831.726, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[831.736, "o", "\r\n"]
[831.746, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[831.756, "o", "            # corrects the verbosity level.\r\n"]
[831.766, "o", "            lars = Lars(\r\n"]
[831.776, "o", "                fit_intercept=False,\r\n"]
[831.786, "o", "                verbose=verbose,\r\n"]
[831.796, "o", "                precompute=gram,\r\n"]
[831.806, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[831.816, "o", "                fit_path=False,\r\n"]
[831.826, "o", "            )\r\n"]
[831.836, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[831.846, "o", "            new_code = lars.coef_\r\n"]
[831.856, "o", "        finally:\r\n"]
[831.866, "o", "            np.seterr(**err_mgt)\r\n"]
[831.876, "o", "\r\n"]
[831.886, "o", "    elif algorithm == \"threshold\":\r\n"]
[831.896, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[831.906, "o", "        if positive:\r\n"]
[831.916, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[831.926, "o", "\r\n"]
[831.936, "o", "    elif algorithm == \"omp\":\r\n"]
[831.946, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[831.956, "o", "            Gram=gram,\r\n"]
[831.966, "o", "            Xy=cov,\r\n"]
[831.976, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[831.986, "o", "            tol=None,\r\n"]
[831.996, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[832.006, "o", "            copy_Xy=copy_cov,\r\n"]
[832.016, "o", "        ).T\r\n"]
[832.026, "o", "\r\n"]
[832.036, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[832.046, "o", "\r\n"]
[832.056, "o", "\r\n"]
[832.066, "o", "@validate_params(\r\n"]
[832.076, "o", "    {\r\n"]
[832.086, "o", "        \"X\": [\"array-like\"],\r\n"]
[832.096, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[832.106, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[832.116, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[832.126, "o", "        \"algorithm\": [\r\n"]
[832.136, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[832.146, "o", "        ],\r\n"]
[832.156, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[832.166, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[832.176, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[832.186, "o", "        \"init\": [\"array-like\", None],\r\n"]
[832.196, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[832.206, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[832.216, "o", "        \"check_input\": [\"boolean\"],\r\n"]
[832.226, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[832.236, "o", "        \"positive\": [\"boolean\"],\r\n"]
[832.246, "o", "    },\r\n"]
[832.256, "o", "    prefer_skip_nested_validation=True,\r\n"]
[832.266, "o", ")\r\n"]
[832.276, "o", "# XXX : could be moved to the linear_model module\r\n"]
[832.286, "o", "def sparse_encode(\r\n"]
[832.296, "o", "    X,\r\n"]
[832.306, "o", "    dictionary,\r\n"]
[832.316, "o", "    *,\r\n"]
[832.326, "o", "    gram=None,\r\n"]
[832.336, "o", "    cov=None,\r\n"]
[832.346, "o", "    algorithm=\"lasso_lars\",\r\n"]
[832.356, "o", "    n_nonzero_coefs=None,\r\n"]
[832.366, "o", "    alpha=None,\r\n"]
[832.376, "o", "    copy_cov=True,\r\n"]
[832.386, "o", "    init=None,\r\n"]
[832.396, "o", "    max_iter=1000,\r\n"]
[832.406, "o", "    n_jobs=None,\r\n"]
[832.416, "o", "    check_input=True,\r\n"]
[832.426, "o", "    verbose=0,\r\n"]
[832.436, "o", "    positive=False,\r\n"]
[832.446, "o", "):\r\n"]
[832.456, "o", "    \"\"\"Sparse coding.\r\n"]
[832.466, "o", "\r\n"]
[832.476, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[832.486, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[832.496, "o", "\r\n"]
[832.506, "o", "        X ~= code * dictionary\r\n"]
[832.516, "o", "\r\n"]
[832.526, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[832.536, "o", "\r\n"]
[832.546, "o", "    Parameters\r\n"]
[832.556, "o", "    ----------\r\n"]
[832.566, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[832.576, "o", "        Data matrix.\r\n"]
[832.586, "o", "\r\n"]
[832.596, "o", "    dictionary : array-like of shape (n_components, n_features)\r\n"]
[832.606, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[832.616, "o", "        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[832.626, "o", "        output.\r\n"]
[832.636, "o", "\r\n"]
[832.646, "o", "    gram : array-like of shape (n_components, n_components), default=None\r\n"]
[832.656, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n"]
[832.666, "o", "\r\n"]
[832.676, "o", "    cov : array-like of shape (n_components, n_samples), default=None\r\n"]
[832.686, "o", "        Precomputed covariance, `dictionary' * X`.\r\n"]
[832.696, "o", "\r\n"]
[832.706, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[832.716, "o", "            default='lasso_lars'\r\n"]
[832.726, "o", "        The algorithm used:\r\n"]
[832.736, "o", "\r\n"]
[832.746, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[832.756, "o", "          (`linear_model.lars_path`);\r\n"]
[832.766, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[832.776, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[832.786, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[832.796, "o", "          the estimated components are sparse;\r\n"]
[832.806, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[832.816, "o", "          solution;\r\n"]
[832.826, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[832.836, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[832.846, "o", "\r\n"]
[832.856, "o", "    n_nonzero_coefs : int, default=None\r\n"]
[832.866, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[832.876, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[832.886, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[832.896, "o", "        `n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[832.906, "o", "\r\n"]
[832.916, "o", "    alpha : float, default=None\r\n"]
[832.926, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[832.936, "o", "        penalty applied to the L1 norm.\r\n"]
[832.946, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[832.956, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[832.966, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[832.976, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[832.986, "o", "        `n_nonzero_coefs`.\r\n"]
[832.996, "o", "        If `None`, default to 1.\r\n"]
[833.006, "o", "\r\n"]
[833.016, "o", "    copy_cov : bool, default=True\r\n"]
[833.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[833.036, "o", "        be overwritten.\r\n"]
[833.046, "o", "\r\n"]
[833.056, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[833.066, "o", "        Initialization value of the sparse codes. Only used if\r\n"]
[833.076, "o", "        `algorithm='lasso_cd'`.\r\n"]
[833.086, "o", "\r\n"]
[833.096, "o", "    max_iter : int, default=1000\r\n"]
[833.106, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[833.116, "o", "        `'lasso_lars'`.\r\n"]
[833.126, "o", "\r\n"]
[833.136, "o", "    n_jobs : int, default=None\r\n"]
[833.146, "o", "        Number of parallel jobs to run.\r\n"]
[833.156, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[833.166, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[833.176, "o", "        for more details.\r\n"]
[833.186, "o", "\r\n"]
[833.196, "o", "    check_input : bool, default=True\r\n"]
[833.206, "o", "        If `False`, the input arrays X and dictionary will not be checked.\r\n"]
[833.216, "o", "\r\n"]
[833.226, "o", "    verbose : int, default=0\r\n"]
[833.236, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[833.246, "o", "\r\n"]
[833.256, "o", "    positive : bool, default=False\r\n"]
[833.266, "o", "        Whether to enforce positivity when finding the encoding.\r\n"]
[833.276, "o", "\r\n"]
[833.286, "o", "        .. versionadded:: 0.20\r\n"]
[833.296, "o", "\r\n"]
[833.306, "o", "    Returns\r\n"]
[833.316, "o", "    -------\r\n"]
[833.326, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[833.336, "o", "        The sparse codes.\r\n"]
[833.346, "o", "\r\n"]
[833.356, "o", "    See Also\r\n"]
[833.366, "o", "    --------\r\n"]
[833.376, "o", "    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n"]
[833.386, "o", "        path using LARS algorithm.\r\n"]
[833.396, "o", "    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n"]
[833.406, "o", "    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n"]
[833.416, "o", "    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n"]
[833.426, "o", "        dictionary.\r\n"]
[833.436, "o", "    \"\"\"\r\n"]
[833.446, "o", "    if check_input:\r\n"]
[833.456, "o", "        if algorithm == \"lasso_cd\":\r\n"]
[833.466, "o", "            dictionary = check_array(\r\n"]
[833.476, "o", "                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n"]
[833.486, "o", "            )\r\n"]
[833.496, "o", "            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n"]
[833.506, "o", "        else:\r\n"]
[833.516, "o", "            dictionary = check_array(dictionary)\r\n"]
[833.526, "o", "            X = check_array(X)\r\n"]
[833.536, "o", "\r\n"]
[833.546, "o", "    if dictionary.shape[1] != X.shape[1]:\r\n"]
[833.556, "o", "        raise ValueError(\r\n"]
[833.566, "o", "            \"Dictionary and X have different numbers of features:\"\r\n"]
[833.576, "o", "            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n"]
[833.586, "o", "        )\r\n"]
[833.596, "o", "\r\n"]
[833.606, "o", "    _check_positive_coding(algorithm, positive)\r\n"]
[833.616, "o", "\r\n"]
[833.626, "o", "    return _sparse_encode(\r\n"]
[833.636, "o", "        X,\r\n"]
[833.646, "o", "        dictionary,\r\n"]
[833.656, "o", "        gram=gram,\r\n"]
[833.666, "o", "        cov=cov,\r\n"]
[833.676, "o", "        algorithm=algorithm,\r\n"]
[833.686, "o", "        n_nonzero_coefs=n_nonzero_coefs,\r\n"]
[833.696, "o", "        alpha=alpha,\r\n"]
[833.706, "o", "        copy_cov=copy_cov,\r\n"]
[833.716, "o", "        init=init,\r\n"]
[833.726, "o", "        max_iter=max_iter,\r\n"]
[833.736, "o", "        n_jobs=n_jobs,\r\n"]
[833.746, "o", "        verbose=verbose,\r\n"]
[833.756, "o", "        positive=positive,\r\n"]
[833.766, "o", "    )\r\n"]
[833.776, "o", "\r\n"]
[833.786, "o", "\r\n"]
[833.796, "o", "def _sparse_encode(\r\n"]
[833.806, "o", "    X,\r\n"]
[833.816, "o", "    dictionary,\r\n"]
[833.826, "o", "    *,\r\n"]
[833.836, "o", "    gram=None,\r\n"]
[833.846, "o", "    cov=None,\r\n"]
[833.856, "o", "    algorithm=\"lasso_lars\",\r\n"]
[833.866, "o", "    n_nonzero_coefs=None,\r\n"]
[833.876, "o", "    alpha=None,\r\n"]
[833.886, "o", "    copy_cov=True,\r\n"]
[833.896, "o", "    init=None,\r\n"]
[833.906, "o", "    max_iter=1000,\r\n"]
[833.916, "o", "    n_jobs=None,\r\n"]
[833.926, "o", "    verbose=0,\r\n"]
[833.936, "o", "    positive=False,\r\n"]
[833.946, "o", "):\r\n"]
[833.956, "o", "    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[833.966, "o", "\r\n"]
[833.976, "o", "    n_samples, n_features = X.shape\r\n"]
[833.986, "o", "    n_components = dictionary.shape[0]\r\n"]
[833.996, "o", "\r\n"]
[834.006, "o", "    if algorithm in (\"lars\", \"omp\"):\r\n"]
[834.016, "o", "        regularization = n_nonzero_coefs\r\n"]
[834.026, "o", "        if regularization is None:\r\n"]
[834.036, "o", "            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[834.046, "o", "    else:\r\n"]
[834.056, "o", "        regularization = alpha\r\n"]
[834.066, "o", "        if regularization is None:\r\n"]
[834.076, "o", "            regularization = 1.0\r\n"]
[834.086, "o", "\r\n"]
[834.096, "o", "    if gram is None and algorithm != \"threshold\":\r\n"]
[834.106, "o", "        gram = np.dot(dictionary, dictionary.T)\r\n"]
[834.116, "o", "\r\n"]
[834.126, "o", "    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[834.136, "o", "        copy_cov = False\r\n"]
[834.146, "o", "        cov = np.dot(dictionary, X.T)\r\n"]
[834.156, "o", "\r\n"]
[834.166, "o", "    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[834.176, "o", "        code = _sparse_encode_precomputed(\r\n"]
[834.186, "o", "            X,\r\n"]
[834.196, "o", "            dictionary,\r\n"]
[834.206, "o", "            gram=gram,\r\n"]
[834.216, "o", "            cov=cov,\r\n"]
[834.226, "o", "            algorithm=algorithm,\r\n"]
[834.236, "o", "            regularization=regularization,\r\n"]
[834.246, "o", "            copy_cov=copy_cov,\r\n"]
[834.256, "o", "            init=init,\r\n"]
[834.266, "o", "            max_iter=max_iter,\r\n"]
[834.276, "o", "            verbose=verbose,\r\n"]
[834.286, "o", "            positive=positive,\r\n"]
[834.296, "o", "        )\r\n"]
[834.306, "o", "        return code\r\n"]
[834.316, "o", "\r\n"]
[834.326, "o", "    # Enter parallel code block\r\n"]
[834.336, "o", "    n_samples = X.shape[0]\r\n"]
[834.346, "o", "    n_components = dictionary.shape[0]\r\n"]
[834.356, "o", "    code = np.empty((n_samples, n_components))\r\n"]
[834.366, "o", "    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[834.376, "o", "\r\n"]
[834.386, "o", "    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[834.396, "o", "        delayed(_sparse_encode_precomputed)(\r\n"]
[834.406, "o", "            X[this_slice],\r\n"]
[834.416, "o", "            dictionary,\r\n"]
[834.426, "o", "            gram=gram,\r\n"]
[834.436, "o", "            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[834.446, "o", "            algorithm=algorithm,\r\n"]
[834.456, "o", "            regularization=regularization,\r\n"]
[834.466, "o", "            copy_cov=copy_cov,\r\n"]
[834.476, "o", "            init=init[this_slice] if init is not None else None,\r\n"]
[834.486, "o", "            max_iter=max_iter,\r\n"]
[834.496, "o", "            verbose=verbose,\r\n"]
[834.506, "o", "            positive=positive,\r\n"]
[834.516, "o", "        )\r\n"]
[834.526, "o", "        for this_slice in slices\r\n"]
[834.536, "o", "    )\r\n"]
[834.546, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[834.556, "o", "        code[this_slice] = this_view\r\n"]
[834.566, "o", "    return code\r\n"]
[834.576, "o", "\r\n"]
[834.586, "o", "\r\n"]
[834.596, "o", "def _update_dict(\r\n"]
[834.606, "o", "    dictionary,\r\n"]
[834.616, "o", "    Y,\r\n"]
[834.626, "o", "    code,\r\n"]
[834.636, "o", "    A=None,\r\n"]
[834.646, "o", "    B=None,\r\n"]
[834.656, "o", "    verbose=False,\r\n"]
[834.666, "o", "    random_state=None,\r\n"]
[834.676, "o", "    positive=False,\r\n"]
[834.686, "o", "):\r\n"]
[834.696, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[834.706, "o", "\r\n"]
[834.716, "o", "    Parameters\r\n"]
[834.726, "o", "    ----------\r\n"]
[834.736, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[834.746, "o", "        Value of the dictionary at the previous iteration.\r\n"]
[834.756, "o", "\r\n"]
[834.766, "o", "    Y : ndarray of shape (n_samples, n_features)\r\n"]
[834.776, "o", "        Data matrix.\r\n"]
[834.786, "o", "\r\n"]
[834.796, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[834.806, "o", "        Sparse coding of the data against which to optimize the dictionary.\r\n"]
[834.816, "o", "\r\n"]
[834.826, "o", "    A : ndarray of shape (n_components, n_components), default=None\r\n"]
[834.836, "o", "        Together with `B`, sufficient stats of the online model to update the\r\n"]
[834.846, "o", "        dictionary.\r\n"]
[834.856, "o", "\r\n"]
[834.866, "o", "    B : ndarray of shape (n_features, n_components), default=None\r\n"]
[834.876, "o", "        Together with `A`, sufficient stats of the online model to update the\r\n"]
[834.886, "o", "        dictionary.\r\n"]
[834.896, "o", "\r\n"]
[834.906, "o", "    verbose: bool, default=False\r\n"]
[834.916, "o", "        Degree of output the procedure will print.\r\n"]
[834.926, "o", "\r\n"]
[834.936, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[835.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[835.002, "i", "sed -n '500,1200p' sklearn/decomposition/_dict_learning.py\r"]
[835.004, "o", "sed -n '500,1200p' sklearn/decomposition/_dict_learning.py\r\n"]
[835.016, "o", "\u001b[?2004l\r\n"]
[835.026, "o", "        .. versionadded:: 0.20\r\n"]
[835.036, "o", "    \"\"\"\r\n"]
[835.046, "o", "    n_samples, n_components = code.shape\r\n"]
[835.056, "o", "    random_state = check_random_state(random_state)\r\n"]
[835.066, "o", "\r\n"]
[835.076, "o", "    if A is None:\r\n"]
[835.086, "o", "        A = code.T @ code\r\n"]
[835.096, "o", "    if B is None:\r\n"]
[835.106, "o", "        B = Y.T @ code\r\n"]
[835.116, "o", "\r\n"]
[835.126, "o", "    n_unused = 0\r\n"]
[835.136, "o", "\r\n"]
[835.146, "o", "    for k in range(n_components):\r\n"]
[835.156, "o", "        if A[k, k] > 1e-6:\r\n"]
[835.166, "o", "            # 1e-6 is arbitrary but consistent with the spams implementation\r\n"]
[835.176, "o", "            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n"]
[835.186, "o", "        else:\r\n"]
[835.196, "o", "            # kth atom is almost never used -> sample a new one from the data\r\n"]
[835.206, "o", "            newd = Y[random_state.choice(n_samples)]\r\n"]
[835.216, "o", "\r\n"]
[835.226, "o", "            # add small noise to avoid making the sparse coding ill conditioned\r\n"]
[835.236, "o", "            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n"]
[835.246, "o", "            noise = random_state.normal(0, noise_level, size=len(newd))\r\n"]
[835.256, "o", "\r\n"]
[835.266, "o", "            dictionary[k] = newd + noise\r\n"]
[835.276, "o", "            code[:, k] = 0\r\n"]
[835.286, "o", "            n_unused += 1\r\n"]
[835.296, "o", "\r\n"]
[835.306, "o", "        if positive:\r\n"]
[835.316, "o", "            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n"]
[835.326, "o", "\r\n"]
[835.336, "o", "        # Projection on the constraint set ||V_k|| <= 1\r\n"]
[835.346, "o", "        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n"]
[835.356, "o", "\r\n"]
[835.366, "o", "    if verbose and n_unused > 0:\r\n"]
[835.376, "o", "        print(f\"{n_unused} unused atoms resampled.\")\r\n"]
[835.386, "o", "\r\n"]
[835.396, "o", "\r\n"]
[835.406, "o", "def _dict_learning(\r\n"]
[835.416, "o", "    X,\r\n"]
[835.426, "o", "    n_components,\r\n"]
[835.436, "o", "    *,\r\n"]
[835.446, "o", "    alpha,\r\n"]
[835.456, "o", "    max_iter,\r\n"]
[835.466, "o", "    tol,\r\n"]
[835.476, "o", "    method,\r\n"]
[835.486, "o", "    n_jobs,\r\n"]
[835.496, "o", "    dict_init,\r\n"]
[835.506, "o", "    code_init,\r\n"]
[835.516, "o", "    callback,\r\n"]
[835.526, "o", "    verbose,\r\n"]
[835.536, "o", "    random_state,\r\n"]
[835.546, "o", "    return_n_iter,\r\n"]
[835.556, "o", "    positive_dict,\r\n"]
[835.566, "o", "    positive_code,\r\n"]
[835.576, "o", "    method_max_iter,\r\n"]
[835.586, "o", "):\r\n"]
[835.596, "o", "    \"\"\"Main dictionary learning algorithm\"\"\"\r\n"]
[835.606, "o", "    t0 = time.time()\r\n"]
[835.616, "o", "    # Init the code and the dictionary with SVD of Y\r\n"]
[835.626, "o", "    if code_init is not None and dict_init is not None:\r\n"]
[835.636, "o", "        code = np.array(code_init, order=\"F\")\r\n"]
[835.646, "o", "        # Don't copy V, it will happen below\r\n"]
[835.656, "o", "        dictionary = dict_init\r\n"]
[835.666, "o", "    else:\r\n"]
[835.676, "o", "        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n"]
[835.686, "o", "        # flip the initial code's sign to enforce deterministic output\r\n"]
[835.696, "o", "        code, dictionary = svd_flip(code, dictionary)\r\n"]
[835.706, "o", "        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[835.716, "o", "    r = len(dictionary)\r\n"]
[835.726, "o", "    if n_components <= r:  # True even if n_components=None\r\n"]
[835.736, "o", "        code = code[:, :n_components]\r\n"]
[835.746, "o", "        dictionary = dictionary[:n_components, :]\r\n"]
[835.756, "o", "    else:\r\n"]
[835.766, "o", "        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n"]
[835.776, "o", "        dictionary = np.r_[\r\n"]
[835.786, "o", "            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n"]
[835.796, "o", "        ]\r\n"]
[835.806, "o", "\r\n"]
[835.816, "o", "    # Fortran-order dict better suited for the sparse coding which is the\r\n"]
[835.826, "o", "    # bottleneck of this algorithm.\r\n"]
[835.836, "o", "    dictionary = np.asfortranarray(dictionary)\r\n"]
[835.846, "o", "\r\n"]
[835.856, "o", "    errors = []\r\n"]
[835.866, "o", "    current_cost = np.nan\r\n"]
[835.876, "o", "\r\n"]
[835.886, "o", "    if verbose == 1:\r\n"]
[835.896, "o", "        print(\"[dict_learning]\", end=\" \")\r\n"]
[835.906, "o", "\r\n"]
[835.916, "o", "    # If max_iter is 0, number of iterations returned should be zero\r\n"]
[835.926, "o", "    ii = -1\r\n"]
[835.936, "o", "\r\n"]
[835.946, "o", "    for ii in range(max_iter):\r\n"]
[835.956, "o", "        dt = time.time() - t0\r\n"]
[835.966, "o", "        if verbose == 1:\r\n"]
[835.976, "o", "            sys.stdout.write(\".\")\r\n"]
[835.986, "o", "            sys.stdout.flush()\r\n"]
[835.996, "o", "        elif verbose:\r\n"]
[836.006, "o", "            print(\r\n"]
[836.016, "o", "                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n"]
[836.026, "o", "                % (ii, dt, dt / 60, current_cost)\r\n"]
[836.036, "o", "            )\r\n"]
[836.046, "o", "\r\n"]
[836.056, "o", "        # Update code\r\n"]
[836.066, "o", "        code = sparse_encode(\r\n"]
[836.076, "o", "            X,\r\n"]
[836.086, "o", "            dictionary,\r\n"]
[836.096, "o", "            algorithm=method,\r\n"]
[836.106, "o", "            alpha=alpha,\r\n"]
[836.116, "o", "            init=code,\r\n"]
[836.126, "o", "            n_jobs=n_jobs,\r\n"]
[836.136, "o", "            positive=positive_code,\r\n"]
[836.146, "o", "            max_iter=method_max_iter,\r\n"]
[836.156, "o", "            verbose=verbose,\r\n"]
[836.166, "o", "        )\r\n"]
[836.176, "o", "\r\n"]
[836.186, "o", "        # Update dictionary in place\r\n"]
[836.196, "o", "        _update_dict(\r\n"]
[836.206, "o", "            dictionary,\r\n"]
[836.216, "o", "            X,\r\n"]
[836.226, "o", "            code,\r\n"]
[836.236, "o", "            verbose=verbose,\r\n"]
[836.246, "o", "            random_state=random_state,\r\n"]
[836.256, "o", "            positive=positive_dict,\r\n"]
[836.266, "o", "        )\r\n"]
[836.276, "o", "\r\n"]
[836.286, "o", "        # Cost function\r\n"]
[836.296, "o", "        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n"]
[836.306, "o", "            np.abs(code)\r\n"]
[836.316, "o", "        )\r\n"]
[836.326, "o", "        errors.append(current_cost)\r\n"]
[836.336, "o", "\r\n"]
[836.346, "o", "        if ii > 0:\r\n"]
[836.356, "o", "            dE = errors[-2] - errors[-1]\r\n"]
[836.366, "o", "            # assert(dE >= -tol * errors[-1])\r\n"]
[836.376, "o", "            if dE < tol * errors[-1]:\r\n"]
[836.386, "o", "                if verbose == 1:\r\n"]
[836.396, "o", "                    # A line return\r\n"]
[836.406, "o", "                    print(\"\")\r\n"]
[836.416, "o", "                elif verbose:\r\n"]
[836.426, "o", "                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n"]
[836.436, "o", "                break\r\n"]
[836.446, "o", "        if ii % 5 == 0 and callback is not None:\r\n"]
[836.456, "o", "            callback(locals())\r\n"]
[836.466, "o", "\r\n"]
[836.476, "o", "    if return_n_iter:\r\n"]
[836.486, "o", "        return code, dictionary, errors, ii + 1\r\n"]
[836.496, "o", "    else:\r\n"]
[836.506, "o", "        return code, dictionary, errors\r\n"]
[836.516, "o", "\r\n"]
[836.526, "o", "\r\n"]
[836.536, "o", "def _check_warn_deprecated(param, name, default, additional_message=None):\r\n"]
[836.546, "o", "    if param != \"deprecated\":\r\n"]
[836.556, "o", "        msg = (\r\n"]
[836.566, "o", "            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n"]
[836.576, "o", "        )\r\n"]
[836.586, "o", "        if additional_message:\r\n"]
[836.596, "o", "            msg += f\" {additional_message}\"\r\n"]
[836.606, "o", "        warnings.warn(msg, FutureWarning)\r\n"]
[836.616, "o", "        return param\r\n"]
[836.626, "o", "    else:\r\n"]
[836.636, "o", "        return default\r\n"]
[836.646, "o", "\r\n"]
[836.656, "o", "\r\n"]
[836.666, "o", "def dict_learning_online(\r\n"]
[836.676, "o", "    X,\r\n"]
[836.686, "o", "    n_components=2,\r\n"]
[836.696, "o", "    *,\r\n"]
[836.706, "o", "    alpha=1,\r\n"]
[836.716, "o", "    n_iter=\"deprecated\",\r\n"]
[836.726, "o", "    max_iter=None,\r\n"]
[836.736, "o", "    return_code=True,\r\n"]
[836.746, "o", "    dict_init=None,\r\n"]
[836.756, "o", "    callback=None,\r\n"]
[836.766, "o", "    batch_size=256,\r\n"]
[836.776, "o", "    verbose=False,\r\n"]
[836.786, "o", "    shuffle=True,\r\n"]
[836.796, "o", "    n_jobs=None,\r\n"]
[836.806, "o", "    method=\"lars\",\r\n"]
[836.816, "o", "    iter_offset=\"deprecated\",\r\n"]
[836.826, "o", "    random_state=None,\r\n"]
[836.836, "o", "    return_inner_stats=\"deprecated\",\r\n"]
[836.846, "o", "    inner_stats=\"deprecated\",\r\n"]
[836.856, "o", "    return_n_iter=\"deprecated\",\r\n"]
[836.866, "o", "    positive_dict=False,\r\n"]
[836.876, "o", "    positive_code=False,\r\n"]
[836.886, "o", "    method_max_iter=1000,\r\n"]
[836.896, "o", "    tol=1e-3,\r\n"]
[836.906, "o", "    max_no_improvement=10,\r\n"]
[836.916, "o", "):\r\n"]
[836.926, "o", "    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n"]
[836.936, "o", "\r\n"]
[836.946, "o", "    Finds the best dictionary and the corresponding sparse code for\r\n"]
[836.956, "o", "    approximating the data matrix X by solving::\r\n"]
[836.966, "o", "\r\n"]
[836.976, "o", "        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[836.986, "o", "                     (U,V)\r\n"]
[836.996, "o", "                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n"]
[837.006, "o", "\r\n"]
[837.016, "o", "    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n"]
[837.026, "o", "    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n"]
[837.036, "o", "    which is the sum of the absolute values of all the entries in the matrix.\r\n"]
[837.046, "o", "    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n"]
[837.056, "o", "    the input data.\r\n"]
[837.066, "o", "\r\n"]
[837.076, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[837.086, "o", "\r\n"]
[837.096, "o", "    Parameters\r\n"]
[837.106, "o", "    ----------\r\n"]
[837.116, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[837.126, "o", "        Data matrix.\r\n"]
[837.136, "o", "\r\n"]
[837.146, "o", "    n_components : int or None, default=2\r\n"]
[837.156, "o", "        Number of dictionary atoms to extract. If None, then ``n_components``\r\n"]
[837.166, "o", "        is set to ``n_features``.\r\n"]
[837.176, "o", "\r\n"]
[837.186, "o", "    alpha : float, default=1\r\n"]
[837.196, "o", "        Sparsity controlling parameter.\r\n"]
[837.206, "o", "\r\n"]
[837.216, "o", "    n_iter : int, default=100\r\n"]
[837.226, "o", "        Number of mini-batch iterations to perform.\r\n"]
[837.236, "o", "\r\n"]
[837.246, "o", "        .. deprecated:: 1.1\r\n"]
[837.256, "o", "           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[837.266, "o", "           `max_iter` instead.\r\n"]
[837.276, "o", "\r\n"]
[837.286, "o", "    max_iter : int, default=None\r\n"]
[837.296, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[837.306, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[837.316, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[837.326, "o", "\r\n"]
[837.336, "o", "        .. versionadded:: 1.1\r\n"]
[837.346, "o", "\r\n"]
[837.356, "o", "    return_code : bool, default=True\r\n"]
[837.366, "o", "        Whether to also return the code U or just the dictionary `V`.\r\n"]
[837.376, "o", "\r\n"]
[837.386, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[837.396, "o", "        Initial values for the dictionary for warm restart scenarios.\r\n"]
[837.406, "o", "        If `None`, the initial values for the dictionary are created\r\n"]
[837.416, "o", "        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n"]
[837.426, "o", "\r\n"]
[837.436, "o", "    callback : callable, default=None\r\n"]
[837.446, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[837.456, "o", "\r\n"]
[837.466, "o", "    batch_size : int, default=256\r\n"]
[837.476, "o", "        The number of samples to take in each batch.\r\n"]
[837.486, "o", "\r\n"]
[837.496, "o", "        .. versionchanged:: 1.3\r\n"]
[837.506, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[837.516, "o", "\r\n"]
[837.526, "o", "    verbose : bool, default=False\r\n"]
[837.536, "o", "        To control the verbosity of the procedure.\r\n"]
[837.546, "o", "\r\n"]
[837.556, "o", "    shuffle : bool, default=True\r\n"]
[837.566, "o", "        Whether to shuffle the data before splitting it in batches.\r\n"]
[837.576, "o", "\r\n"]
[837.586, "o", "    n_jobs : int, default=None\r\n"]
[837.596, "o", "        Number of parallel jobs to run.\r\n"]
[837.606, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[837.616, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[837.626, "o", "        for more details.\r\n"]
[837.636, "o", "\r\n"]
[837.646, "o", "    method : {'lars', 'cd'}, default='lars'\r\n"]
[837.656, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[837.666, "o", "          problem (`linear_model.lars_path`);\r\n"]
[837.676, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[837.686, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[837.696, "o", "          the estimated components are sparse.\r\n"]
[837.706, "o", "\r\n"]
[837.716, "o", "    iter_offset : int, default=0\r\n"]
[837.726, "o", "        Number of previous iterations completed on the dictionary used for\r\n"]
[837.736, "o", "        initialization.\r\n"]
[837.746, "o", "\r\n"]
[837.756, "o", "        .. deprecated:: 1.1\r\n"]
[837.766, "o", "           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n"]
[837.776, "o", "\r\n"]
[837.786, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[837.796, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[837.806, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[837.816, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[837.826, "o", "        results across multiple function calls.\r\n"]
[837.836, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[837.846, "o", "\r\n"]
[837.856, "o", "    return_inner_stats : bool, default=False\r\n"]
[837.866, "o", "        Return the inner statistics A (dictionary covariance) and B\r\n"]
[837.876, "o", "        (data approximation). Useful to restart the algorithm in an\r\n"]
[837.886, "o", "        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n"]
[837.896, "o", "        ignored.\r\n"]
[837.906, "o", "\r\n"]
[837.916, "o", "        .. deprecated:: 1.1\r\n"]
[837.926, "o", "           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[837.936, "o", "\r\n"]
[837.946, "o", "    inner_stats : tuple of (A, B) ndarrays, default=None\r\n"]
[837.956, "o", "        Inner sufficient statistics that are kept by the algorithm.\r\n"]
[837.966, "o", "        Passing them at initialization is useful in online settings, to\r\n"]
[837.976, "o", "        avoid losing the history of the evolution.\r\n"]
[837.986, "o", "        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n"]
[837.996, "o", "        `B` `(n_features, n_components)` is the data approximation matrix.\r\n"]
[838.006, "o", "\r\n"]
[838.016, "o", "        .. deprecated:: 1.1\r\n"]
[838.026, "o", "           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[838.036, "o", "\r\n"]
[838.046, "o", "    return_n_iter : bool, default=False\r\n"]
[838.056, "o", "        Whether or not to return the number of iterations.\r\n"]
[838.066, "o", "\r\n"]
[838.076, "o", "        .. deprecated:: 1.1\r\n"]
[838.086, "o", "           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n"]
[838.096, "o", "\r\n"]
[838.106, "o", "    positive_dict : bool, default=False\r\n"]
[838.116, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[838.126, "o", "\r\n"]
[838.136, "o", "        .. versionadded:: 0.20\r\n"]
[838.146, "o", "\r\n"]
[838.156, "o", "    positive_code : bool, default=False\r\n"]
[838.166, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[838.176, "o", "\r\n"]
[838.186, "o", "        .. versionadded:: 0.20\r\n"]
[838.196, "o", "\r\n"]
[838.206, "o", "    method_max_iter : int, default=1000\r\n"]
[838.216, "o", "        Maximum number of iterations to perform when solving the lasso problem.\r\n"]
[838.226, "o", "\r\n"]
[838.236, "o", "        .. versionadded:: 0.22\r\n"]
[838.246, "o", "\r\n"]
[838.256, "o", "    tol : float, default=1e-3\r\n"]
[838.266, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[838.276, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[838.286, "o", "\r\n"]
[838.296, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[838.306, "o", "        `tol` to 0.0.\r\n"]
[838.316, "o", "\r\n"]
[838.326, "o", "        .. versionadded:: 1.1\r\n"]
[838.336, "o", "\r\n"]
[838.346, "o", "    max_no_improvement : int, default=10\r\n"]
[838.356, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[838.366, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[838.376, "o", "        `max_iter` is not None.\r\n"]
[838.386, "o", "\r\n"]
[838.396, "o", "        To disable convergence detection based on cost function, set\r\n"]
[838.406, "o", "        `max_no_improvement` to None.\r\n"]
[838.416, "o", "\r\n"]
[838.426, "o", "        .. versionadded:: 1.1\r\n"]
[838.436, "o", "\r\n"]
[838.446, "o", "    Returns\r\n"]
[838.456, "o", "    -------\r\n"]
[838.466, "o", "    code : ndarray of shape (n_samples, n_components),\r\n"]
[838.476, "o", "        The sparse code (only returned if `return_code=True`).\r\n"]
[838.486, "o", "\r\n"]
[838.496, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[838.506, "o", "        The solutions to the dictionary learning problem.\r\n"]
[838.516, "o", "\r\n"]
[838.526, "o", "    n_iter : int\r\n"]
[838.536, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[838.546, "o", "        set to `True`.\r\n"]
[838.556, "o", "\r\n"]
[838.566, "o", "    See Also\r\n"]
[838.576, "o", "    --------\r\n"]
[838.586, "o", "    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n"]
[838.596, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[838.606, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n"]
[838.616, "o", "        learning algorithm.\r\n"]
[838.626, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[838.636, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[838.646, "o", "    \"\"\"\r\n"]
[838.656, "o", "    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n"]
[838.666, "o", "    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n"]
[838.676, "o", "        raise ValueError(\r\n"]
[838.686, "o", "            \"The following arguments are incompatible with 'max_iter': \"\r\n"]
[838.696, "o", "            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n"]
[838.706, "o", "        )\r\n"]
[838.716, "o", "\r\n"]
[838.726, "o", "    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n"]
[838.736, "o", "    return_inner_stats = _check_warn_deprecated(\r\n"]
[838.746, "o", "        return_inner_stats,\r\n"]
[838.756, "o", "        \"return_inner_stats\",\r\n"]
[838.766, "o", "        default=False,\r\n"]
[838.776, "o", "        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n"]
[838.786, "o", "    )\r\n"]
[838.796, "o", "    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n"]
[838.806, "o", "    return_n_iter = _check_warn_deprecated(\r\n"]
[838.816, "o", "        return_n_iter,\r\n"]
[838.826, "o", "        \"return_n_iter\",\r\n"]
[838.836, "o", "        default=False,\r\n"]
[838.846, "o", "        additional_message=(\r\n"]
[838.856, "o", "            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n"]
[838.866, "o", "            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n"]
[838.876, "o", "        ),\r\n"]
[838.886, "o", "    )\r\n"]
[838.896, "o", "\r\n"]
[838.906, "o", "    if max_iter is not None:\r\n"]
[838.916, "o", "        transform_algorithm = \"lasso_\" + method\r\n"]
[838.926, "o", "\r\n"]
[838.936, "o", "        est = MiniBatchDictionaryLearning(\r\n"]
[838.946, "o", "            n_components=n_components,\r\n"]
[838.956, "o", "            alpha=alpha,\r\n"]
[838.966, "o", "            n_iter=n_iter,\r\n"]
[838.976, "o", "            n_jobs=n_jobs,\r\n"]
[838.986, "o", "            fit_algorithm=method,\r\n"]
[838.996, "o", "            batch_size=batch_size,\r\n"]
[839.006, "o", "            shuffle=shuffle,\r\n"]
[839.016, "o", "            dict_init=dict_init,\r\n"]
[839.026, "o", "            random_state=random_state,\r\n"]
[839.036, "o", "            transform_algorithm=transform_algorithm,\r\n"]
[839.046, "o", "            transform_alpha=alpha,\r\n"]
[839.056, "o", "            positive_code=positive_code,\r\n"]
[839.066, "o", "            positive_dict=positive_dict,\r\n"]
[839.076, "o", "            transform_max_iter=method_max_iter,\r\n"]
[839.086, "o", "            verbose=verbose,\r\n"]
[839.096, "o", "            callback=callback,\r\n"]
[839.106, "o", "            tol=tol,\r\n"]
[839.116, "o", "            max_no_improvement=max_no_improvement,\r\n"]
[839.126, "o", "        ).fit(X)\r\n"]
[839.136, "o", "\r\n"]
[839.146, "o", "        if not return_code:\r\n"]
[839.156, "o", "            return est.components_\r\n"]
[839.166, "o", "        else:\r\n"]
[839.176, "o", "            code = est.transform(X)\r\n"]
[839.186, "o", "            return code, est.components_\r\n"]
[839.196, "o", "\r\n"]
[839.206, "o", "    # TODO(1.4) remove the whole old behavior\r\n"]
[839.216, "o", "    # Fallback to old behavior\r\n"]
[839.226, "o", "\r\n"]
[839.236, "o", "    n_iter = _check_warn_deprecated(\r\n"]
[839.246, "o", "        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n"]
[839.256, "o", "    )\r\n"]
[839.266, "o", "\r\n"]
[839.276, "o", "    if n_components is None:\r\n"]
[839.286, "o", "        n_components = X.shape[1]\r\n"]
[839.296, "o", "\r\n"]
[839.306, "o", "    if method not in (\"lars\", \"cd\"):\r\n"]
[839.316, "o", "        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n"]
[839.326, "o", "\r\n"]
[839.336, "o", "    _check_positive_coding(method, positive_code)\r\n"]
[839.346, "o", "\r\n"]
[839.356, "o", "    method = \"lasso_\" + method\r\n"]
[839.366, "o", "\r\n"]
[839.376, "o", "    t0 = time.time()\r\n"]
[839.386, "o", "    n_samples, n_features = X.shape\r\n"]
[839.396, "o", "    # Avoid integer division problems\r\n"]
[839.406, "o", "    alpha = float(alpha)\r\n"]
[839.416, "o", "    random_state = check_random_state(random_state)\r\n"]
[839.426, "o", "\r\n"]
[839.436, "o", "    # Init V with SVD of X\r\n"]
[839.446, "o", "    if dict_init is not None:\r\n"]
[839.456, "o", "        dictionary = dict_init\r\n"]
[839.466, "o", "    else:\r\n"]
[839.476, "o", "        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n"]
[839.486, "o", "        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[839.496, "o", "    r = len(dictionary)\r\n"]
[839.506, "o", "    if n_components <= r:\r\n"]
[839.516, "o", "        dictionary = dictionary[:n_components, :]\r\n"]
[839.526, "o", "    else:\r\n"]
[839.536, "o", "        dictionary = np.r_[\r\n"]
[839.546, "o", "            dictionary,\r\n"]
[839.556, "o", "            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n"]
[839.566, "o", "        ]\r\n"]
[839.576, "o", "\r\n"]
[839.586, "o", "    if verbose == 1:\r\n"]
[839.596, "o", "        print(\"[dict_learning]\", end=\" \")\r\n"]
[839.606, "o", "\r\n"]
[839.616, "o", "    if shuffle:\r\n"]
[839.626, "o", "        X_train = X.copy()\r\n"]
[839.636, "o", "        random_state.shuffle(X_train)\r\n"]
[839.646, "o", "    else:\r\n"]
[839.656, "o", "        X_train = X\r\n"]
[839.666, "o", "\r\n"]
[839.676, "o", "    X_train = check_array(\r\n"]
[839.686, "o", "        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n"]
[839.696, "o", "    )\r\n"]
[839.706, "o", "\r\n"]
[839.716, "o", "    # Fortran-order dict better suited for the sparse coding which is the\r\n"]
[839.726, "o", "    # bottleneck of this algorithm.\r\n"]
[839.736, "o", "    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n"]
[839.746, "o", "    dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[839.756, "o", "\r\n"]
[839.766, "o", "    batches = gen_batches(n_samples, batch_size)\r\n"]
[839.776, "o", "    batches = itertools.cycle(batches)\r\n"]
[839.786, "o", "\r\n"]
[839.796, "o", "    # The covariance of the dictionary\r\n"]
[839.806, "o", "    if inner_stats is None:\r\n"]
[839.816, "o", "        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n"]
[839.826, "o", "        # The data approximation\r\n"]
[839.836, "o", "        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n"]
[839.846, "o", "    else:\r\n"]
[839.856, "o", "        A = inner_stats[0].copy()\r\n"]
[839.866, "o", "        B = inner_stats[1].copy()\r\n"]
[839.876, "o", "\r\n"]
[839.886, "o", "    # If n_iter is zero, we need to return zero.\r\n"]
[839.896, "o", "    ii = iter_offset - 1\r\n"]
[839.906, "o", "\r\n"]
[839.916, "o", "    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n"]
[839.926, "o", "        this_X = X_train[batch]\r\n"]
[839.936, "o", "        dt = time.time() - t0\r\n"]
[840.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[840.002, "i", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r"]
[840.004, "o", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r\n"]
[840.016, "o", "\u001b[?2004l\r\n"]
[840.026, "o", "    n_iter : int\r\n"]
[840.036, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[840.046, "o", "        set to True.\r\n"]
[840.056, "o", "\r\n"]
[840.066, "o", "    See Also\r\n"]
[840.076, "o", "    --------\r\n"]
[840.086, "o", "    dict_learning_online : Solve a dictionary learning matrix factorization\r\n"]
[840.096, "o", "        problem online.\r\n"]
[840.106, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[840.116, "o", "    MiniBatchDictionaryLearning : A faster, less accurate version\r\n"]
[840.126, "o", "        of the dictionary learning algorithm.\r\n"]
[840.136, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[840.146, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[840.156, "o", "    \"\"\"\r\n"]
[840.166, "o", "    estimator = DictionaryLearning(\r\n"]
[840.176, "o", "        n_components=n_components,\r\n"]
[840.186, "o", "        alpha=alpha,\r\n"]
[840.196, "o", "        max_iter=max_iter,\r\n"]
[840.206, "o", "        tol=tol,\r\n"]
[840.216, "o", "        fit_algorithm=method,\r\n"]
[840.226, "o", "        n_jobs=n_jobs,\r\n"]
[840.236, "o", "        dict_init=dict_init,\r\n"]
[840.246, "o", "        callback=callback,\r\n"]
[840.256, "o", "        code_init=code_init,\r\n"]
[840.266, "o", "        verbose=verbose,\r\n"]
[840.276, "o", "        random_state=random_state,\r\n"]
[840.286, "o", "        positive_code=positive_code,\r\n"]
[840.296, "o", "        positive_dict=positive_dict,\r\n"]
[840.306, "o", "        transform_max_iter=method_max_iter,\r\n"]
[840.316, "o", "    )\r\n"]
[840.326, "o", "    code = estimator.fit_transform(X)\r\n"]
[840.336, "o", "    if return_n_iter:\r\n"]
[840.346, "o", "        return (\r\n"]
[840.356, "o", "            code,\r\n"]
[840.366, "o", "            estimator.components_,\r\n"]
[840.376, "o", "            estimator.error_,\r\n"]
[840.386, "o", "            estimator.n_iter_,\r\n"]
[840.396, "o", "        )\r\n"]
[840.406, "o", "    return code, estimator.components_, estimator.error_\r\n"]
[840.416, "o", "\r\n"]
[840.426, "o", "\r\n"]
[840.436, "o", "class _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n"]
[840.446, "o", "    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n"]
[840.456, "o", "\r\n"]
[840.466, "o", "    def __init__(\r\n"]
[840.476, "o", "        self,\r\n"]
[840.486, "o", "        transform_algorithm,\r\n"]
[840.496, "o", "        transform_n_nonzero_coefs,\r\n"]
[840.506, "o", "        transform_alpha,\r\n"]
[840.516, "o", "        split_sign,\r\n"]
[840.526, "o", "        n_jobs,\r\n"]
[840.536, "o", "        positive_code,\r\n"]
[840.546, "o", "        transform_max_iter,\r\n"]
[840.556, "o", "    ):\r\n"]
[840.566, "o", "        self.transform_algorithm = transform_algorithm\r\n"]
[840.576, "o", "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n"]
[840.586, "o", "        self.transform_alpha = transform_alpha\r\n"]
[840.596, "o", "        self.transform_max_iter = transform_max_iter\r\n"]
[840.606, "o", "        self.split_sign = split_sign\r\n"]
[840.616, "o", "        self.n_jobs = n_jobs\r\n"]
[840.626, "o", "        self.positive_code = positive_code\r\n"]
[840.636, "o", "\r\n"]
[840.646, "o", "    def _transform(self, X, dictionary):\r\n"]
[840.656, "o", "        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n"]
[840.666, "o", "        SparseCoder.\"\"\"\r\n"]
[840.676, "o", "        X = self._validate_data(X, reset=False)\r\n"]
[840.686, "o", "\r\n"]
[840.696, "o", "        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n"]
[840.706, "o", "            transform_alpha = self.alpha\r\n"]
[840.716, "o", "        else:\r\n"]
[840.726, "o", "            transform_alpha = self.transform_alpha\r\n"]
[840.736, "o", "\r\n"]
[840.746, "o", "        code = sparse_encode(\r\n"]
[840.756, "o", "            X,\r\n"]
[840.766, "o", "            dictionary,\r\n"]
[840.776, "o", "            algorithm=self.transform_algorithm,\r\n"]
[840.786, "o", "            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n"]
[840.796, "o", "            alpha=transform_alpha,\r\n"]
[840.806, "o", "            max_iter=self.transform_max_iter,\r\n"]
[840.816, "o", "            n_jobs=self.n_jobs,\r\n"]
[840.826, "o", "            positive=self.positive_code,\r\n"]
[840.836, "o", "        )\r\n"]
[840.846, "o", "\r\n"]
[840.856, "o", "        if self.split_sign:\r\n"]
[840.866, "o", "            # feature vector is split into a positive and negative side\r\n"]
[840.876, "o", "            n_samples, n_features = code.shape\r\n"]
[840.886, "o", "            split_code = np.empty((n_samples, 2 * n_features))\r\n"]
[840.896, "o", "            split_code[:, :n_features] = np.maximum(code, 0)\r\n"]
[840.906, "o", "            split_code[:, n_features:] = -np.minimum(code, 0)\r\n"]
[840.916, "o", "            code = split_code\r\n"]
[840.926, "o", "\r\n"]
[840.936, "o", "        return code\r\n"]
[840.946, "o", "\r\n"]
[840.956, "o", "    def transform(self, X):\r\n"]
[840.966, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[840.976, "o", "\r\n"]
[840.986, "o", "        Coding method is determined by the object parameter\r\n"]
[840.996, "o", "        `transform_algorithm`.\r\n"]
[841.006, "o", "\r\n"]
[841.016, "o", "        Parameters\r\n"]
[841.026, "o", "        ----------\r\n"]
[841.036, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[841.046, "o", "            Test data to be transformed, must have the same number of\r\n"]
[841.056, "o", "            features as the data used to train the model.\r\n"]
[841.066, "o", "\r\n"]
[841.076, "o", "        Returns\r\n"]
[841.086, "o", "        -------\r\n"]
[841.096, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[841.106, "o", "            Transformed data.\r\n"]
[841.116, "o", "        \"\"\"\r\n"]
[841.126, "o", "        check_is_fitted(self)\r\n"]
[841.136, "o", "        return self._transform(X, self.components_)\r\n"]
[841.146, "o", "\r\n"]
[841.156, "o", "\r\n"]
[841.166, "o", "class SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n"]
[841.176, "o", "    \"\"\"Sparse coding.\r\n"]
[841.186, "o", "\r\n"]
[841.196, "o", "    Finds a sparse representation of data against a fixed, precomputed\r\n"]
[841.206, "o", "    dictionary.\r\n"]
[841.216, "o", "\r\n"]
[841.226, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[841.236, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[841.246, "o", "\r\n"]
[841.256, "o", "        X ~= code * dictionary\r\n"]
[841.266, "o", "\r\n"]
[841.276, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[841.286, "o", "\r\n"]
[841.296, "o", "    Parameters\r\n"]
[841.306, "o", "    ----------\r\n"]
[841.316, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[841.326, "o", "        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n"]
[841.336, "o", "        normalized to unit norm.\r\n"]
[841.346, "o", "\r\n"]
[841.356, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[841.366, "o", "            'threshold'}, default='omp'\r\n"]
[841.376, "o", "        Algorithm used to transform the data:\r\n"]
[841.386, "o", "\r\n"]
[841.396, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[841.406, "o", "          (`linear_model.lars_path`);\r\n"]
[841.416, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[841.426, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[841.436, "o", "          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n"]
[841.446, "o", "          the estimated components are sparse;\r\n"]
[841.456, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[841.466, "o", "          solution;\r\n"]
[841.476, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[841.486, "o", "          the projection ``dictionary * X'``.\r\n"]
[841.496, "o", "\r\n"]
[841.506, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[841.516, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[841.526, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[841.536, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[841.546, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[841.556, "o", "\r\n"]
[841.566, "o", "    transform_alpha : float, default=None\r\n"]
[841.576, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[841.586, "o", "        penalty applied to the L1 norm.\r\n"]
[841.596, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[841.606, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[841.616, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[841.626, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[841.636, "o", "        `n_nonzero_coefs`.\r\n"]
[841.646, "o", "        If `None`, default to 1.\r\n"]
[841.656, "o", "\r\n"]
[841.666, "o", "    split_sign : bool, default=False\r\n"]
[841.676, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[841.686, "o", "        its negative part and its positive part. This can improve the\r\n"]
[841.696, "o", "        performance of downstream classifiers.\r\n"]
[841.706, "o", "\r\n"]
[841.716, "o", "    n_jobs : int, default=None\r\n"]
[841.726, "o", "        Number of parallel jobs to run.\r\n"]
[841.736, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[841.746, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[841.756, "o", "        for more details.\r\n"]
[841.766, "o", "\r\n"]
[841.776, "o", "    positive_code : bool, default=False\r\n"]
[841.786, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[841.796, "o", "\r\n"]
[841.806, "o", "        .. versionadded:: 0.20\r\n"]
[841.816, "o", "\r\n"]
[841.826, "o", "    transform_max_iter : int, default=1000\r\n"]
[841.836, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[841.846, "o", "        `lasso_lars`.\r\n"]
[841.856, "o", "\r\n"]
[841.866, "o", "        .. versionadded:: 0.22\r\n"]
[841.876, "o", "\r\n"]
[841.886, "o", "    Attributes\r\n"]
[841.896, "o", "    ----------\r\n"]
[841.906, "o", "    n_components_ : int\r\n"]
[841.916, "o", "        Number of atoms.\r\n"]
[841.926, "o", "\r\n"]
[841.936, "o", "    n_features_in_ : int\r\n"]
[841.946, "o", "        Number of features seen during :term:`fit`.\r\n"]
[841.956, "o", "\r\n"]
[841.966, "o", "        .. versionadded:: 0.24\r\n"]
[841.976, "o", "\r\n"]
[841.986, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[841.996, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[842.006, "o", "        has feature names that are all strings.\r\n"]
[842.016, "o", "\r\n"]
[842.026, "o", "        .. versionadded:: 1.0\r\n"]
[842.036, "o", "\r\n"]
[842.046, "o", "    See Also\r\n"]
[842.056, "o", "    --------\r\n"]
[842.066, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[842.076, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n"]
[842.086, "o", "        dictionary learning algorithm.\r\n"]
[842.096, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[842.106, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[842.116, "o", "    sparse_encode : Sparse coding where each row of the result is the solution\r\n"]
[842.126, "o", "        to a sparse coding problem.\r\n"]
[842.136, "o", "\r\n"]
[842.146, "o", "    Examples\r\n"]
[842.156, "o", "    --------\r\n"]
[842.166, "o", "    >>> import numpy as np\r\n"]
[842.176, "o", "    >>> from sklearn.decomposition import SparseCoder\r\n"]
[842.186, "o", "    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n"]
[842.196, "o", "    >>> dictionary = np.array(\r\n"]
[842.206, "o", "    ...     [[0, 1, 0],\r\n"]
[842.216, "o", "    ...      [-1, -1, 2],\r\n"]
[842.226, "o", "    ...      [1, 1, 1],\r\n"]
[842.236, "o", "    ...      [0, 1, 1],\r\n"]
[842.246, "o", "    ...      [0, 2, 1]],\r\n"]
[842.256, "o", "    ...    dtype=np.float64\r\n"]
[842.266, "o", "    ... )\r\n"]
[842.276, "o", "    >>> coder = SparseCoder(\r\n"]
[842.286, "o", "    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n"]
[842.296, "o", "    ...     transform_alpha=1e-10,\r\n"]
[842.306, "o", "    ... )\r\n"]
[842.316, "o", "    >>> coder.transform(X)\r\n"]
[842.326, "o", "    array([[ 0.,  0., -1.,  0.,  0.],\r\n"]
[842.336, "o", "           [ 0.,  1.,  1.,  0.,  0.]])\r\n"]
[842.346, "o", "    \"\"\"\r\n"]
[842.356, "o", "\r\n"]
[842.366, "o", "    _required_parameters = [\"dictionary\"]\r\n"]
[842.376, "o", "\r\n"]
[842.386, "o", "    def __init__(\r\n"]
[842.396, "o", "        self,\r\n"]
[842.406, "o", "        dictionary,\r\n"]
[842.416, "o", "        *,\r\n"]
[842.426, "o", "        transform_algorithm=\"omp\",\r\n"]
[842.436, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[842.446, "o", "        transform_alpha=None,\r\n"]
[842.456, "o", "        split_sign=False,\r\n"]
[842.466, "o", "        n_jobs=None,\r\n"]
[842.476, "o", "        positive_code=False,\r\n"]
[842.486, "o", "        transform_max_iter=1000,\r\n"]
[842.496, "o", "    ):\r\n"]
[842.506, "o", "        super().__init__(\r\n"]
[842.516, "o", "            transform_algorithm,\r\n"]
[842.526, "o", "            transform_n_nonzero_coefs,\r\n"]
[842.536, "o", "            transform_alpha,\r\n"]
[842.546, "o", "            split_sign,\r\n"]
[842.556, "o", "            n_jobs,\r\n"]
[842.566, "o", "            positive_code,\r\n"]
[842.576, "o", "            transform_max_iter,\r\n"]
[842.586, "o", "        )\r\n"]
[842.596, "o", "        self.dictionary = dictionary\r\n"]
[842.606, "o", "\r\n"]
[842.616, "o", "    def fit(self, X, y=None):\r\n"]
[842.626, "o", "        \"\"\"Do nothing and return the estimator unchanged.\r\n"]
[842.636, "o", "\r\n"]
[842.646, "o", "        This method is just there to implement the usual API and hence\r\n"]
[842.656, "o", "        work in pipelines.\r\n"]
[842.666, "o", "\r\n"]
[842.676, "o", "        Parameters\r\n"]
[842.686, "o", "        ----------\r\n"]
[842.696, "o", "        X : Ignored\r\n"]
[842.706, "o", "            Not used, present for API consistency by convention.\r\n"]
[842.716, "o", "\r\n"]
[842.726, "o", "        y : Ignored\r\n"]
[842.736, "o", "            Not used, present for API consistency by convention.\r\n"]
[842.746, "o", "\r\n"]
[842.756, "o", "        Returns\r\n"]
[842.766, "o", "        -------\r\n"]
[842.776, "o", "        self : object\r\n"]
[842.786, "o", "            Returns the instance itself.\r\n"]
[842.796, "o", "        \"\"\"\r\n"]
[842.806, "o", "        return self\r\n"]
[842.816, "o", "\r\n"]
[842.826, "o", "    def transform(self, X, y=None):\r\n"]
[842.836, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[842.846, "o", "\r\n"]
[842.856, "o", "        Coding method is determined by the object parameter\r\n"]
[842.866, "o", "        `transform_algorithm`.\r\n"]
[842.876, "o", "\r\n"]
[842.886, "o", "        Parameters\r\n"]
[842.896, "o", "        ----------\r\n"]
[842.906, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[842.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[842.926, "o", "            and `n_features` is the number of features.\r\n"]
[842.936, "o", "\r\n"]
[842.946, "o", "        y : Ignored\r\n"]
[842.956, "o", "            Not used, present for API consistency by convention.\r\n"]
[842.966, "o", "\r\n"]
[842.976, "o", "        Returns\r\n"]
[842.986, "o", "        -------\r\n"]
[842.996, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[843.006, "o", "            Transformed data.\r\n"]
[843.016, "o", "        \"\"\"\r\n"]
[843.026, "o", "        return super()._transform(X, self.dictionary)\r\n"]
[843.036, "o", "\r\n"]
[843.046, "o", "    def _more_tags(self):\r\n"]
[843.056, "o", "        return {\r\n"]
[843.066, "o", "            \"requires_fit\": False,\r\n"]
[843.076, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[843.086, "o", "        }\r\n"]
[843.096, "o", "\r\n"]
[843.106, "o", "    @property\r\n"]
[843.116, "o", "    def n_components_(self):\r\n"]
[843.126, "o", "        \"\"\"Number of atoms.\"\"\"\r\n"]
[843.136, "o", "        return self.dictionary.shape[0]\r\n"]
[843.146, "o", "\r\n"]
[843.156, "o", "    @property\r\n"]
[843.166, "o", "    def n_features_in_(self):\r\n"]
[843.176, "o", "        \"\"\"Number of features seen during `fit`.\"\"\"\r\n"]
[843.186, "o", "        return self.dictionary.shape[1]\r\n"]
[843.196, "o", "\r\n"]
[843.206, "o", "    @property\r\n"]
[843.216, "o", "    def _n_features_out(self):\r\n"]
[843.226, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[843.236, "o", "        return self.n_components_\r\n"]
[843.246, "o", "\r\n"]
[843.256, "o", "\r\n"]
[843.266, "o", "class DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[843.276, "o", "    \"\"\"Dictionary learning.\r\n"]
[843.286, "o", "\r\n"]
[843.296, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[843.306, "o", "    encoding the fitted data.\r\n"]
[843.316, "o", "\r\n"]
[843.326, "o", "    Solves the optimization problem::\r\n"]
[843.336, "o", "\r\n"]
[843.346, "o", "        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[843.356, "o", "                    (U,V)\r\n"]
[843.366, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[843.376, "o", "\r\n"]
[843.386, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[843.396, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[843.406, "o", "    of all the entries in the matrix.\r\n"]
[843.416, "o", "\r\n"]
[843.426, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[843.436, "o", "\r\n"]
[843.446, "o", "    Parameters\r\n"]
[843.456, "o", "    ----------\r\n"]
[843.466, "o", "    n_components : int, default=None\r\n"]
[843.476, "o", "        Number of dictionary elements to extract. If None, then ``n_components``\r\n"]
[843.486, "o", "        is set to ``n_features``.\r\n"]
[843.496, "o", "\r\n"]
[843.506, "o", "    alpha : float, default=1.0\r\n"]
[843.516, "o", "        Sparsity controlling parameter.\r\n"]
[843.526, "o", "\r\n"]
[843.536, "o", "    max_iter : int, default=1000\r\n"]
[843.546, "o", "        Maximum number of iterations to perform.\r\n"]
[843.556, "o", "\r\n"]
[843.566, "o", "    tol : float, default=1e-8\r\n"]
[843.576, "o", "        Tolerance for numerical error.\r\n"]
[843.586, "o", "\r\n"]
[843.596, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[843.606, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[843.616, "o", "          problem (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[843.626, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[843.636, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n"]
[843.646, "o", "          faster if the estimated components are sparse.\r\n"]
[843.656, "o", "\r\n"]
[843.666, "o", "        .. versionadded:: 0.17\r\n"]
[843.676, "o", "           *cd* coordinate descent method to improve speed.\r\n"]
[843.686, "o", "\r\n"]
[843.696, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[843.706, "o", "            'threshold'}, default='omp'\r\n"]
[843.716, "o", "        Algorithm used to transform the data:\r\n"]
[843.726, "o", "\r\n"]
[843.736, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[843.746, "o", "          (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[843.756, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[843.766, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[843.776, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n"]
[843.786, "o", "          will be faster if the estimated components are sparse.\r\n"]
[843.796, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[843.806, "o", "          solution.\r\n"]
[843.816, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[843.826, "o", "          the projection ``dictionary * X'``.\r\n"]
[843.836, "o", "\r\n"]
[843.846, "o", "        .. versionadded:: 0.17\r\n"]
[843.856, "o", "           *lasso_cd* coordinate descent method to improve speed.\r\n"]
[843.866, "o", "\r\n"]
[843.876, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[843.886, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[843.896, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[843.906, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[843.916, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[843.926, "o", "\r\n"]
[843.936, "o", "    transform_alpha : float, default=None\r\n"]
[843.946, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[843.956, "o", "        penalty applied to the L1 norm.\r\n"]
[843.966, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[843.976, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[843.986, "o", "        If `None`, defaults to `alpha`.\r\n"]
[843.996, "o", "\r\n"]
[844.006, "o", "        .. versionchanged:: 1.2\r\n"]
[844.016, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[844.026, "o", "\r\n"]
[844.036, "o", "    n_jobs : int or None, default=None\r\n"]
[844.046, "o", "        Number of parallel jobs to run.\r\n"]
[844.056, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[844.066, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[844.076, "o", "        for more details.\r\n"]
[844.086, "o", "\r\n"]
[844.096, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[844.106, "o", "        Initial value for the code, for warm restart. Only used if `code_init`\r\n"]
[844.116, "o", "        and `dict_init` are not None.\r\n"]
[844.126, "o", "\r\n"]
[844.136, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[844.146, "o", "        Initial values for the dictionary, for warm restart. Only used if\r\n"]
[844.156, "o", "        `code_init` and `dict_init` are not None.\r\n"]
[844.166, "o", "\r\n"]
[844.176, "o", "    callback : callable, default=None\r\n"]
[844.186, "o", "        Callable that gets invoked every five iterations.\r\n"]
[844.196, "o", "\r\n"]
[844.206, "o", "        .. versionadded:: 1.3\r\n"]
[844.216, "o", "\r\n"]
[844.226, "o", "    verbose : bool, default=False\r\n"]
[844.236, "o", "        To control the verbosity of the procedure.\r\n"]
[844.246, "o", "\r\n"]
[844.256, "o", "    split_sign : bool, default=False\r\n"]
[844.266, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[844.276, "o", "        its negative part and its positive part. This can improve the\r\n"]
[844.286, "o", "        performance of downstream classifiers.\r\n"]
[844.296, "o", "\r\n"]
[844.306, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[844.316, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[844.326, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[844.336, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[844.346, "o", "        results across multiple function calls.\r\n"]
[844.356, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[844.366, "o", "\r\n"]
[844.376, "o", "    positive_code : bool, default=False\r\n"]
[844.386, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[844.396, "o", "\r\n"]
[844.406, "o", "        .. versionadded:: 0.20\r\n"]
[844.416, "o", "\r\n"]
[844.426, "o", "    positive_dict : bool, default=False\r\n"]
[844.436, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[844.446, "o", "\r\n"]
[844.456, "o", "        .. versionadded:: 0.20\r\n"]
[844.466, "o", "\r\n"]
[844.476, "o", "    transform_max_iter : int, default=1000\r\n"]
[844.486, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[844.496, "o", "        `'lasso_lars'`.\r\n"]
[844.506, "o", "\r\n"]
[844.516, "o", "        .. versionadded:: 0.22\r\n"]
[844.526, "o", "\r\n"]
[844.536, "o", "    Attributes\r\n"]
[844.546, "o", "    ----------\r\n"]
[844.556, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[844.566, "o", "        dictionary atoms extracted from the data\r\n"]
[844.576, "o", "\r\n"]
[844.586, "o", "    error_ : array\r\n"]
[844.596, "o", "        vector of errors at each iteration\r\n"]
[844.606, "o", "\r\n"]
[844.616, "o", "    n_features_in_ : int\r\n"]
[844.626, "o", "        Number of features seen during :term:`fit`.\r\n"]
[844.636, "o", "\r\n"]
[844.646, "o", "        .. versionadded:: 0.24\r\n"]
[844.656, "o", "\r\n"]
[844.666, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[844.676, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[844.686, "o", "        has feature names that are all strings.\r\n"]
[844.696, "o", "\r\n"]
[844.706, "o", "        .. versionadded:: 1.0\r\n"]
[844.716, "o", "\r\n"]
[844.726, "o", "    n_iter_ : int\r\n"]
[844.736, "o", "        Number of iterations run.\r\n"]
[844.746, "o", "\r\n"]
[844.756, "o", "    See Also\r\n"]
[844.766, "o", "    --------\r\n"]
[844.776, "o", "    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n"]
[844.786, "o", "        dictionary learning algorithm.\r\n"]
[844.796, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[844.806, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[844.816, "o", "        precomputed dictionary.\r\n"]
[844.826, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[844.836, "o", "\r\n"]
[844.846, "o", "    References\r\n"]
[844.856, "o", "    ----------\r\n"]
[844.866, "o", "\r\n"]
[844.876, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[844.886, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[844.896, "o", "\r\n"]
[844.906, "o", "    Examples\r\n"]
[844.916, "o", "    --------\r\n"]
[844.926, "o", "    >>> import numpy as np\r\n"]
[844.936, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[845.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[845.002, "i", "cd asv_benchmarks\r"]
[845.004, "o", "cd asv_benchmarks\r\n"]
[845.006, "o", "\u001b[?2004l\r\n"]
[850.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[850.002, "i", "asv dev -l | grep -n \"MiniBatchDictionaryLearningBenchmark\" || asv dev -l | sed -n '1,200p'\r"]
[850.004, "o", "asv dev -l | grep -n \"MiniBatchDictionaryLearningBenchmark\" || asv dev -l | sed -n '1,200p'\r\n"]
[850.555333, "o", "\u001b[?2004l\r\n"]
[851.104667, "o", "usage: asv [-h] [--verbose] [--config CONFIG] [--version]\r\n"]
[851.654, "o", "           {help,quickstart,machine,setup,run,continuous,find,rm,publish,preview,profile,update,show,compare,check,gh-pages}\r\n"]
[852.203333, "o", "           ...\r\n"]
[852.752667, "o", "asv: error: argument {help,quickstart,machine,setup,run,continuous,find,rm,publish,preview,profile,update,show,compare,check,gh-pages}: invalid choice: 'dev' (choose from 'help', 'quickstart', 'machine', 'setup', 'run', 'continuous', 'find', 'rm', 'publish', 'preview', 'profile', 'update', 'show', 'compare', 'check', 'gh-pages')\r\n"]
[853.302, "o", "usage: asv [-h] [--verbose] [--config CONFIG] [--version]\r\n"]
[853.851333, "o", "           {help,quickstart,machine,setup,run,continuous,find,rm,publish,preview,profile,update,show,compare,check,gh-pages}\r\n"]
[854.400667, "o", "           ...\r\n"]
[855.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[855.002, "i", "asv profile --python=same benchmarks.decomposition.MiniBatchDictionaryLearningBenchmark.time_fit || \r"]
[855.004, "o", "asv profile --python=same benchmarks.decomposition.MiniBatchDictionaryLearningBenchmark.time_fit || \r\n"]
[855.83, "o", " asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit || true\r\n"]
[856.654, "o", "\u001b[?2004l\r\n"]
[857.478, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[858.302, "o", "\u00b7 \u001b[0;31m'benchmarks.decomposition.MiniBatchDictionaryLearningBenchmark.time_fit' benchmark not found\u001b[0m\r\n"]
[859.126, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[860.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[860.002, "i", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py || true\r"]
[860.004, "o", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[862.478, "o", "\u001b[?2004l\r\n"]
[865.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[865.002, "i", "grep -n \"_sparse_encode_precomputed\\s*\\(\" -n sklearn/decomposition/_dict_learning.py || true\r"]
[865.004, "o", "grep -n \"_sparse_encode_precomputed\\s*\\(\" -n sklearn/decomposition/_dict_learning.py || true\r\n"]
[867.478, "o", "\u001b[?2004l\r\n"]
[870.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[870.002, "i", "grep -n \"cov\\s*=\\s|gram\\s*=\\s\" sklearn/decomposition/_dict_learning.py || true\r"]
[870.004, "o", "grep -n \"cov\\s*=\\s|gram\\s*=\\s\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[872.478, "o", "\u001b[?2004l\r\n"]
[875.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[875.002, "i", "sed -n '1,500p' sklearn/decomposition/_dict_learning.py\r"]
[875.004, "o", "sed -n '1,500p' sklearn/decomposition/_dict_learning.py\r\n"]
[877.478, "o", "\u001b[?2004l\r\n"]
[880.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[880.002, "i", "sed -n '500,1200p' sklearn/decomposition/_dict_learning.py\r"]
[880.004, "o", "sed -n '500,1200p' sklearn/decomposition/_dict_learning.py\r\n"]
[882.478, "o", "\u001b[?2004l\r\n"]
[885.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[885.002, "i", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r"]
[885.004, "o", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r\n"]
[887.478, "o", "\u001b[?2004l\r\n"]
[890.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[890.002, "i", "cd asv_benchmarks\r"]
[890.004, "o", "cd asv_benchmarks\r\n"]
[892.478, "o", "\u001b[?2004l\r\n"]
[895.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[895.002, "i", "asv run --python=same --quick --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\" --show-\r"]
[895.004, "o", "asv run --python=same --quick --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\" --show-\r\n"]
[895.068582, "o", "-stderr\r\n"]
[895.131165, "o", "\u001b[?2004l\r\n"]
[895.193747, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[895.256329, "o", "\u00b7 \u001b[0;32mRunning 6 total benchmarks (1 commits * 1 environments * 6 benchmarks)\u001b[0m\r\n"]
[895.318911, "o", "[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n"]
[895.381494, "o", "[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n"]
[895.444076, "o", "[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.peakmem_fit\u001b[0m                                                                                  ok\r\n"]
[895.506658, "o", "[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =======\u001b[0m\r\n"]
[895.569241, "o", "             --               n_jobs\r\n"]
[895.631823, "o", "             --------------- -------\r\n"]
[895.694405, "o", "              fit_algorithm     1   \r\n"]
[895.756987, "o", "             =============== =======\r\n"]
[895.81957, "o", "                   lars       93.8M \r\n"]
[895.882152, "o", "                    cd        93.1M \r\n"]
[895.944734, "o", "             =============== =======\r\n"]
[896.007316, "o", "\r\n"]
[896.069899, "o", "[33.33%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.peakmem_transform\u001b[0m                                                                            ok\r\n"]
[896.132481, "o", "[33.33%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =======\u001b[0m\r\n"]
[896.195063, "o", "             --               n_jobs\r\n"]
[896.257646, "o", "             --------------- -------\r\n"]
[896.320228, "o", "              fit_algorithm     1   \r\n"]
[896.38281, "o", "             =============== =======\r\n"]
[896.445392, "o", "                   lars       84.4M \r\n"]
[896.507975, "o", "                    cd        84.2M \r\n"]
[896.570557, "o", "             =============== =======\r\n"]
[896.633139, "o", "\r\n"]
[896.695722, "o", "[33.33%] \u00b7\u00b7\u00b7\u00b7 \u001b[0;31mFor parameters: 'lars', 1\u001b[0m\r\n"]
[896.758304, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[896.820886, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[896.883468, "o", "\r\n"]
[896.946051, "o", "[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0masv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r\n"]
[897.008633, "o", "                                                                                     ok\r\n"]
[897.071215, "o", "[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =========\u001b[0m\r\n"]
[897.133797, "o", "             --                n_jobs \r\n"]
[897.19638, "o", "             --------------- ---------\r\n"]
[897.258962, "o", "              fit_algorithm      1    \r\n"]
[897.321544, "o", "             =============== =========\r\n"]
[897.384127, "o", "                   lars       6.93\u00b10s \r\n"]
[897.446709, "o", "                    cd        1.57\u00b10s \r\n"]
[897.509291, "o", "             =============== =========\r\n"]
[897.571873, "o", "\r\n"]
[897.634456, "o", "[66.67%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_transform\u001b[0m                                                                               ok\r\n"]
[897.697038, "o", "[66.67%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =========\u001b[0m\r\n"]
[897.75962, "o", "             --                n_jobs \r\n"]
[897.822203, "o", "             --------------- ---------\r\n"]
[897.884785, "o", "              fit_algorithm      1    \r\n"]
[897.947367, "o", "             =============== =========\r\n"]
[898.009949, "o", "                   lars       161\u00b10ms \r\n"]
[898.072532, "o", "                    cd        159\u00b10ms \r\n"]
[898.135114, "o", "             =============== =========\r\n"]
[898.197696, "o", "\r\n"]
[898.260278, "o", "[66.67%] \u00b7\u00b7\u00b7\u00b7 \u001b[0;31mFor parameters: 'lars', 1\u001b[0m\r\n"]
[898.322861, "o", "              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[898.385443, "o", "                new_code = orthogonal_mp_gram(\r\n"]
[898.448025, "o", "\r\n"]
[898.510608, "o", "[83.33%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.track_test_score\u001b[0m                                                                             ok\r\n"]
[898.57319, "o", "[83.33%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ======================\u001b[0m\r\n"]
[898.635772, "o", "             --                      n_jobs        \r\n"]
[898.698354, "o", "             --------------- ----------------------\r\n"]
[898.760937, "o", "              fit_algorithm            1           \r\n"]
[898.823519, "o", "             =============== ======================\r\n"]
[898.886101, "o", "                   lars       -0.07507291436195374 \r\n"]
[898.948684, "o", "                    cd        -0.07509346306324005 \r\n"]
[899.011266, "o", "             =============== ======================\r\n"]
[899.073848, "o", "\r\n"]
[899.13643, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.track_train_score\u001b[0m                                                                            ok\r\n"]
[899.199013, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ======================\u001b[0m\r\n"]
[899.261595, "o", "              --                      n_jobs        \r\n"]
[899.324177, "o", "              --------------- ----------------------\r\n"]
[899.386759, "o", "               fit_algorithm            1           \r\n"]
[899.449342, "o", "              =============== ======================\r\n"]
[899.511924, "o", "                    lars       -0.07244297116994858 \r\n"]
[899.574506, "o", "                     cd        -0.07244537025690079 \r\n"]
[899.637089, "o", "              =============== ======================\r\n"]
[899.699671, "o", "\r\n"]
[899.762253, "o", "[100.00%] \u00b7\u00b7\u00b7\u00b7 \u001b[0;31mFor parameters: 'lars', 1\u001b[0m\r\n"]
[899.824835, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[899.887418, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[900.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[900.002, "i", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r"]
[900.004, "o", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r\n"]
[901.654, "o", "\u001b[?2004l\r\n"]
[903.302, "o", "Commits with results:\r\n"]
[905.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[905.002, "i", "asv profile --python=same \"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit(lars,1)\" || a\r"]
[905.004, "o", "asv profile --python=same \"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit(lars,1)\" || a\r\n"]
[905.83, "o", "asv profile --python=same \"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit(cd,1)\" || true\r\n"]
[906.654, "o", "\u001b[?2004l\r\n"]
[907.478, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[908.302, "o", "\u00b7 \u001b[0;31m'decomposition.MiniBatchDictionaryLearningBenchmark.time_fit(lars,1)' benchmark not found\u001b[0m\r\n"]
[909.126, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[910.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[910.002, "i", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py\r"]
[910.004, "o", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py\r\n"]
[912.478, "o", "\u001b[?2004l\r\n"]
[915.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[915.002, "i", "grep -n \"_sparse_encode_precomputed\\s*\\(\" sklearn/decomposition/_dict_learning.py\r"]
[915.004, "o", "grep -n \"_sparse_encode_precomputed\\s*\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[917.478, "o", "\u001b[?2004l\r\n"]
[920.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[920.002, "i", "grep -n \"cov\\s*=\\s|gram\\s*=\\s\" sklearn/decomposition/_dict_learning.py || true\r"]
[920.004, "o", "grep -n \"cov\\s*=\\s|gram\\s*=\\s\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[922.478, "o", "\u001b[?2004l\r\n"]
[925.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[925.002, "i", "sed -n '1,600p' sklearn/decomposition/_dict_learning.py\r"]
[925.004, "o", "sed -n '1,600p' sklearn/decomposition/_dict_learning.py\r\n"]
[927.478, "o", "\u001b[?2004l\r\n"]
[930.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[930.002, "i", "sed -n '600,1400p' sklearn/decomposition/_dict_learning.py\r"]
[930.004, "o", "sed -n '600,1400p' sklearn/decomposition/_dict_learning.py\r\n"]
[932.478, "o", "\u001b[?2004l\r\n"]
[935.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[935.002, "i", "sed -n '1400,2200p' sklearn/decomposition/_dict_learning.py\r"]
[935.004, "o", "sed -n '1400,2200p' sklearn/decomposition/_dict_learning.py\r\n"]
[937.478, "o", "\u001b[?2004l\r\n"]
[940.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[940.002, "i", "cd asv_benchmarks\r"]
[940.004, "o", "cd asv_benchmarks\r\n"]
[942.478, "o", "\u001b[?2004l\r\n"]
[945.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[945.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r"]
[945.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r\n"]
[945.296824, "o", "stderr\r\n"]
[945.587647, "o", "\u001b[?2004l\r\n"]
[945.878471, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[946.169294, "o", "\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n"]
[946.460118, "o", "[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n"]
[946.750941, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n"]
[947.041765, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0masv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit || asv profile --config asv.conf.json --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[947.332588, "o", "                                                                                     ok\r\n"]
[947.623412, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ============\u001b[0m\r\n"]
[947.914235, "o", "              --                 n_jobs   \r\n"]
[948.205059, "o", "              --------------- ------------\r\n"]
[948.495882, "o", "               fit_algorithm       1      \r\n"]
[948.786706, "o", "              =============== ============\r\n"]
[949.077529, "o", "                    lars       6.84\u00b10.03s \r\n"]
[949.368353, "o", "                     cd        1.58\u00b10.04s \r\n"]
[949.659176, "o", "              =============== ============\r\n"]
[950.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[950.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit || asv profile\r"]
[950.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit || asv profile\r\n"]
[950.83, "o", "e --config asv.conf.json --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[951.654, "o", "\u001b[?2004l\r\n"]
[952.478, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[953.302, "o", "\u00b7 \u001b[0;31m'decomposition.MiniBatchDictionaryLearningBenchmark.time_fit' benchmark not found\u001b[0m\r\n"]
[954.126, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[955.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[955.002, "i", "cd ..\r"]
[955.004, "o", "cd ..\r\n"]
[955.006, "o", "\u001b[?2004l\r\n"]
[960.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[960.002, "i", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py\r"]
[960.004, "o", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py\r\n"]
[962.478, "o", "\u001b[?2004l\r\n"]
[965.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[965.002, "i", "grep -n \"sparse_encode\\(|_sparse_encode_precomputed\\(\" sklearn/decomposition/_dict_learning.py\r"]
[965.004, "o", "grep -n \"sparse_encode\\(|_sparse_encode_precomputed\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[967.478, "o", "\u001b[?2004l\r\n"]
[970.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[970.002, "i", "sed -n '1,400p' sklearn/decomposition/_dict_learning.py\r"]
[970.004, "o", "sed -n '1,400p' sklearn/decomposition/_dict_learning.py\r\n"]
[970.018329, "o", "\u001b[?2004l\r\n"]
[970.030658, "o", "\"\"\" Dictionary learning.\r\n"]
[970.042988, "o", "\"\"\"\r\n"]
[970.055317, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[970.067646, "o", "# License: BSD 3 clause\r\n"]
[970.079975, "o", "\r\n"]
[970.092304, "o", "import itertools\r\n"]
[970.104633, "o", "import sys\r\n"]
[970.116963, "o", "import time\r\n"]
[970.129292, "o", "import warnings\r\n"]
[970.141621, "o", "from math import ceil\r\n"]
[970.15395, "o", "from numbers import Integral, Real\r\n"]
[970.166279, "o", "\r\n"]
[970.178608, "o", "import numpy as np\r\n"]
[970.190938, "o", "from joblib import effective_n_jobs\r\n"]
[970.203267, "o", "from scipy import linalg\r\n"]
[970.215596, "o", "\r\n"]
[970.227925, "o", "from ..base import (\r\n"]
[970.240254, "o", "    BaseEstimator,\r\n"]
[970.252584, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[970.264913, "o", "    TransformerMixin,\r\n"]
[970.277242, "o", "    _fit_context,\r\n"]
[970.289571, "o", ")\r\n"]
[970.3019, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[970.314229, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[970.326559, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[970.338888, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[970.351217, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[970.363546, "o", "from ..utils.validation import check_is_fitted\r\n"]
[970.375875, "o", "\r\n"]
[970.388204, "o", "\r\n"]
[970.400534, "o", "def _check_positive_coding(method, positive):\r\n"]
[970.412863, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[970.425192, "o", "        raise ValueError(\r\n"]
[970.437521, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[970.44985, "o", "        )\r\n"]
[970.46218, "o", "\r\n"]
[970.474509, "o", "\r\n"]
[970.486838, "o", "def _sparse_encode_precomputed(\r\n"]
[970.499167, "o", "    X,\r\n"]
[970.511496, "o", "    dictionary,\r\n"]
[970.523825, "o", "    *,\r\n"]
[970.536155, "o", "    gram=None,\r\n"]
[970.548484, "o", "    cov=None,\r\n"]
[970.560813, "o", "    algorithm=\"lasso_lars\",\r\n"]
[970.573142, "o", "    regularization=None,\r\n"]
[970.585471, "o", "    copy_cov=True,\r\n"]
[970.5978, "o", "    init=None,\r\n"]
[970.61013, "o", "    max_iter=1000,\r\n"]
[970.622459, "o", "    verbose=0,\r\n"]
[970.634788, "o", "    positive=False,\r\n"]
[970.647117, "o", "):\r\n"]
[970.659446, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[970.671776, "o", "\r\n"]
[970.684105, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[970.696434, "o", "\r\n"]
[970.708763, "o", "    Parameters\r\n"]
[970.721092, "o", "    ----------\r\n"]
[970.733421, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[970.745751, "o", "        Data matrix.\r\n"]
[970.75808, "o", "\r\n"]
[970.770409, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[970.782738, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[970.795067, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[970.807397, "o", "\r\n"]
[970.819726, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[970.832055, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[970.844384, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[970.856713, "o", "\r\n"]
[970.869042, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[970.881372, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[970.893701, "o", "\r\n"]
[970.90603, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[970.918359, "o", "            default='lasso_lars'\r\n"]
[970.930688, "o", "        The algorithm used:\r\n"]
[970.943017, "o", "\r\n"]
[970.955347, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[970.967676, "o", "          (`linear_model.lars_path`);\r\n"]
[970.980005, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[970.992334, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[971.004663, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[971.016993, "o", "          the estimated components are sparse;\r\n"]
[971.029322, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[971.041651, "o", "          solution;\r\n"]
[971.05398, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[971.066309, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[971.078638, "o", "\r\n"]
[971.090968, "o", "    regularization : int or float, default=None\r\n"]
[971.103297, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[971.115626, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[971.127955, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[971.140284, "o", "\r\n"]
[971.152613, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[971.164943, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[971.177272, "o", "        `algorithm='lasso_cd'`.\r\n"]
[971.189601, "o", "\r\n"]
[971.20193, "o", "    max_iter : int, default=1000\r\n"]
[971.214259, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[971.226589, "o", "        `'lasso_lars'`.\r\n"]
[971.238918, "o", "\r\n"]
[971.251247, "o", "    copy_cov : bool, default=True\r\n"]
[971.263576, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[971.275905, "o", "        be overwritten.\r\n"]
[971.288234, "o", "\r\n"]
[971.300564, "o", "    verbose : int, default=0\r\n"]
[971.312893, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[971.325222, "o", "\r\n"]
[971.337551, "o", "    positive: bool, default=False\r\n"]
[971.34988, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[971.362209, "o", "\r\n"]
[971.374539, "o", "        .. versionadded:: 0.20\r\n"]
[971.386868, "o", "\r\n"]
[971.399197, "o", "    Returns\r\n"]
[971.411526, "o", "    -------\r\n"]
[971.423855, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[971.436185, "o", "        The sparse codes.\r\n"]
[971.448514, "o", "    \"\"\"\r\n"]
[971.460843, "o", "    n_samples, n_features = X.shape\r\n"]
[971.473172, "o", "    n_components = dictionary.shape[0]\r\n"]
[971.485501, "o", "\r\n"]
[971.49783, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[971.51016, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[971.522489, "o", "        try:\r\n"]
[971.534818, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[971.547147, "o", "\r\n"]
[971.559476, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[971.571805, "o", "            # corrects the verbosity level.\r\n"]
[971.584135, "o", "            lasso_lars = LassoLars(\r\n"]
[971.596464, "o", "                alpha=alpha,\r\n"]
[971.608793, "o", "                fit_intercept=False,\r\n"]
[971.621122, "o", "                verbose=verbose,\r\n"]
[971.633451, "o", "                precompute=gram,\r\n"]
[971.645781, "o", "                fit_path=False,\r\n"]
[971.65811, "o", "                positive=positive,\r\n"]
[971.670439, "o", "                max_iter=max_iter,\r\n"]
[971.682768, "o", "            )\r\n"]
[971.695097, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[971.707426, "o", "            new_code = lasso_lars.coef_\r\n"]
[971.719756, "o", "        finally:\r\n"]
[971.732085, "o", "            np.seterr(**err_mgt)\r\n"]
[971.744414, "o", "\r\n"]
[971.756743, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[971.769072, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[971.781401, "o", "\r\n"]
[971.793731, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[971.80606, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[971.818389, "o", "        # argument that we could pass in from Lasso.\r\n"]
[971.830718, "o", "        clf = Lasso(\r\n"]
[971.843047, "o", "            alpha=alpha,\r\n"]
[971.855377, "o", "            fit_intercept=False,\r\n"]
[971.867706, "o", "            precompute=gram,\r\n"]
[971.880035, "o", "            max_iter=max_iter,\r\n"]
[971.892364, "o", "            warm_start=True,\r\n"]
[971.904693, "o", "            positive=positive,\r\n"]
[971.917022, "o", "        )\r\n"]
[971.929352, "o", "\r\n"]
[971.941681, "o", "        if init is not None:\r\n"]
[971.95401, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[971.966339, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[971.978668, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[971.990998, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[972.003327, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[972.015656, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[972.027985, "o", "                init = np.array(init)\r\n"]
[972.040314, "o", "            clf.coef_ = init\r\n"]
[972.052643, "o", "\r\n"]
[972.064973, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[972.077302, "o", "        new_code = clf.coef_\r\n"]
[972.089631, "o", "\r\n"]
[972.10196, "o", "    elif algorithm == \"lars\":\r\n"]
[972.114289, "o", "        try:\r\n"]
[972.126618, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[972.138948, "o", "\r\n"]
[972.151277, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[972.163606, "o", "            # corrects the verbosity level.\r\n"]
[972.175935, "o", "            lars = Lars(\r\n"]
[972.188264, "o", "                fit_intercept=False,\r\n"]
[972.200594, "o", "                verbose=verbose,\r\n"]
[972.212923, "o", "                precompute=gram,\r\n"]
[972.225252, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[972.237581, "o", "                fit_path=False,\r\n"]
[972.24991, "o", "            )\r\n"]
[972.262239, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[972.274569, "o", "            new_code = lars.coef_\r\n"]
[972.286898, "o", "        finally:\r\n"]
[972.299227, "o", "            np.seterr(**err_mgt)\r\n"]
[972.311556, "o", "\r\n"]
[972.323885, "o", "    elif algorithm == \"threshold\":\r\n"]
[972.336214, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[972.348544, "o", "        if positive:\r\n"]
[972.360873, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[972.373202, "o", "\r\n"]
[972.385531, "o", "    elif algorithm == \"omp\":\r\n"]
[972.39786, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[972.41019, "o", "            Gram=gram,\r\n"]
[972.422519, "o", "            Xy=cov,\r\n"]
[972.434848, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[972.447177, "o", "            tol=None,\r\n"]
[972.459506, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[972.471835, "o", "            copy_Xy=copy_cov,\r\n"]
[972.484165, "o", "        ).T\r\n"]
[972.496494, "o", "\r\n"]
[972.508823, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[972.521152, "o", "\r\n"]
[972.533481, "o", "\r\n"]
[972.54581, "o", "@validate_params(\r\n"]
[972.55814, "o", "    {\r\n"]
[972.570469, "o", "        \"X\": [\"array-like\"],\r\n"]
[972.582798, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[972.595127, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[972.607456, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[972.619786, "o", "        \"algorithm\": [\r\n"]
[972.632115, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[972.644444, "o", "        ],\r\n"]
[972.656773, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[972.669102, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[972.681431, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[972.693761, "o", "        \"init\": [\"array-like\", None],\r\n"]
[972.70609, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[972.718419, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[972.730748, "o", "        \"check_input\": [\"boolean\"],\r\n"]
[972.743077, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[972.755406, "o", "        \"positive\": [\"boolean\"],\r\n"]
[972.767736, "o", "    },\r\n"]
[972.780065, "o", "    prefer_skip_nested_validation=True,\r\n"]
[972.792394, "o", ")\r\n"]
[972.804723, "o", "# XXX : could be moved to the linear_model module\r\n"]
[972.817052, "o", "def sparse_encode(\r\n"]
[972.829382, "o", "    X,\r\n"]
[972.841711, "o", "    dictionary,\r\n"]
[972.85404, "o", "    *,\r\n"]
[972.866369, "o", "    gram=None,\r\n"]
[972.878698, "o", "    cov=None,\r\n"]
[972.891027, "o", "    algorithm=\"lasso_lars\",\r\n"]
[972.903357, "o", "    n_nonzero_coefs=None,\r\n"]
[972.915686, "o", "    alpha=None,\r\n"]
[972.928015, "o", "    copy_cov=True,\r\n"]
[972.940344, "o", "    init=None,\r\n"]
[972.952673, "o", "    max_iter=1000,\r\n"]
[972.965002, "o", "    n_jobs=None,\r\n"]
[972.977332, "o", "    check_input=True,\r\n"]
[972.989661, "o", "    verbose=0,\r\n"]
[973.00199, "o", "    positive=False,\r\n"]
[973.014319, "o", "):\r\n"]
[973.026648, "o", "    \"\"\"Sparse coding.\r\n"]
[973.038978, "o", "\r\n"]
[973.051307, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[973.063636, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[973.075965, "o", "\r\n"]
[973.088294, "o", "        X ~= code * dictionary\r\n"]
[973.100623, "o", "\r\n"]
[973.112953, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[973.125282, "o", "\r\n"]
[973.137611, "o", "    Parameters\r\n"]
[973.14994, "o", "    ----------\r\n"]
[973.162269, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[973.174599, "o", "        Data matrix.\r\n"]
[973.186928, "o", "\r\n"]
[973.199257, "o", "    dictionary : array-like of shape (n_components, n_features)\r\n"]
[973.211586, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[973.223915, "o", "        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[973.236244, "o", "        output.\r\n"]
[973.248574, "o", "\r\n"]
[973.260903, "o", "    gram : array-like of shape (n_components, n_components), default=None\r\n"]
[973.273232, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n"]
[973.285561, "o", "\r\n"]
[973.29789, "o", "    cov : array-like of shape (n_components, n_samples), default=None\r\n"]
[973.310219, "o", "        Precomputed covariance, `dictionary' * X`.\r\n"]
[973.322549, "o", "\r\n"]
[973.334878, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[973.347207, "o", "            default='lasso_lars'\r\n"]
[973.359536, "o", "        The algorithm used:\r\n"]
[973.371865, "o", "\r\n"]
[973.384195, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[973.396524, "o", "          (`linear_model.lars_path`);\r\n"]
[973.408853, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[973.421182, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[973.433511, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[973.44584, "o", "          the estimated components are sparse;\r\n"]
[973.45817, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[973.470499, "o", "          solution;\r\n"]
[973.482828, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[973.495157, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[973.507486, "o", "\r\n"]
[973.519815, "o", "    n_nonzero_coefs : int, default=None\r\n"]
[973.532145, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[973.544474, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[973.556803, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[973.569132, "o", "        `n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[973.581461, "o", "\r\n"]
[973.593791, "o", "    alpha : float, default=None\r\n"]
[973.60612, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[973.618449, "o", "        penalty applied to the L1 norm.\r\n"]
[973.630778, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[973.643107, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[973.655436, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[973.667766, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[973.680095, "o", "        `n_nonzero_coefs`.\r\n"]
[973.692424, "o", "        If `None`, default to 1.\r\n"]
[973.704753, "o", "\r\n"]
[973.717082, "o", "    copy_cov : bool, default=True\r\n"]
[973.729411, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[973.741741, "o", "        be overwritten.\r\n"]
[973.75407, "o", "\r\n"]
[973.766399, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[973.778728, "o", "        Initialization value of the sparse codes. Only used if\r\n"]
[973.791057, "o", "        `algorithm='lasso_cd'`.\r\n"]
[973.803387, "o", "\r\n"]
[973.815716, "o", "    max_iter : int, default=1000\r\n"]
[973.828045, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[973.840374, "o", "        `'lasso_lars'`.\r\n"]
[973.852703, "o", "\r\n"]
[973.865032, "o", "    n_jobs : int, default=None\r\n"]
[973.877362, "o", "        Number of parallel jobs to run.\r\n"]
[973.889691, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[973.90202, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[973.914349, "o", "        for more details.\r\n"]
[973.926678, "o", "\r\n"]
[973.939007, "o", "    check_input : bool, default=True\r\n"]
[973.951337, "o", "        If `False`, the input arrays X and dictionary will not be checked.\r\n"]
[973.963666, "o", "\r\n"]
[973.975995, "o", "    verbose : int, default=0\r\n"]
[973.988324, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[974.000653, "o", "\r\n"]
[974.012983, "o", "    positive : bool, default=False\r\n"]
[974.025312, "o", "        Whether to enforce positivity when finding the encoding.\r\n"]
[974.037641, "o", "\r\n"]
[974.04997, "o", "        .. versionadded:: 0.20\r\n"]
[974.062299, "o", "\r\n"]
[974.074628, "o", "    Returns\r\n"]
[974.086958, "o", "    -------\r\n"]
[974.099287, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[974.111616, "o", "        The sparse codes.\r\n"]
[974.123945, "o", "\r\n"]
[974.136274, "o", "    See Also\r\n"]
[974.148603, "o", "    --------\r\n"]
[974.160933, "o", "    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n"]
[974.173262, "o", "        path using LARS algorithm.\r\n"]
[974.185591, "o", "    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n"]
[974.19792, "o", "    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n"]
[974.210249, "o", "    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n"]
[974.222579, "o", "        dictionary.\r\n"]
[974.234908, "o", "    \"\"\"\r\n"]
[974.247237, "o", "    if check_input:\r\n"]
[974.259566, "o", "        if algorithm == \"lasso_cd\":\r\n"]
[974.271895, "o", "            dictionary = check_array(\r\n"]
[974.284224, "o", "                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n"]
[974.296554, "o", "            )\r\n"]
[974.308883, "o", "            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n"]
[974.321212, "o", "        else:\r\n"]
[974.333541, "o", "            dictionary = check_array(dictionary)\r\n"]
[974.34587, "o", "            X = check_array(X)\r\n"]
[974.3582, "o", "\r\n"]
[974.370529, "o", "    if dictionary.shape[1] != X.shape[1]:\r\n"]
[974.382858, "o", "        raise ValueError(\r\n"]
[974.395187, "o", "            \"Dictionary and X have different numbers of features:\"\r\n"]
[974.407516, "o", "            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n"]
[974.419845, "o", "        )\r\n"]
[974.432175, "o", "\r\n"]
[974.444504, "o", "    _check_positive_coding(algorithm, positive)\r\n"]
[974.456833, "o", "\r\n"]
[974.469162, "o", "    return _sparse_encode(\r\n"]
[974.481491, "o", "        X,\r\n"]
[974.49382, "o", "        dictionary,\r\n"]
[974.50615, "o", "        gram=gram,\r\n"]
[974.518479, "o", "        cov=cov,\r\n"]
[974.530808, "o", "        algorithm=algorithm,\r\n"]
[974.543137, "o", "        n_nonzero_coefs=n_nonzero_coefs,\r\n"]
[974.555466, "o", "        alpha=alpha,\r\n"]
[974.567796, "o", "        copy_cov=copy_cov,\r\n"]
[974.580125, "o", "        init=init,\r\n"]
[974.592454, "o", "        max_iter=max_iter,\r\n"]
[974.604783, "o", "        n_jobs=n_jobs,\r\n"]
[974.617112, "o", "        verbose=verbose,\r\n"]
[974.629441, "o", "        positive=positive,\r\n"]
[974.641771, "o", "    )\r\n"]
[974.6541, "o", "\r\n"]
[974.666429, "o", "\r\n"]
[974.678758, "o", "def _sparse_encode(\r\n"]
[974.691087, "o", "    X,\r\n"]
[974.703416, "o", "    dictionary,\r\n"]
[974.715746, "o", "    *,\r\n"]
[974.728075, "o", "    gram=None,\r\n"]
[974.740404, "o", "    cov=None,\r\n"]
[974.752733, "o", "    algorithm=\"lasso_lars\",\r\n"]
[974.765062, "o", "    n_nonzero_coefs=None,\r\n"]
[974.777392, "o", "    alpha=None,\r\n"]
[974.789721, "o", "    copy_cov=True,\r\n"]
[974.80205, "o", "    init=None,\r\n"]
[974.814379, "o", "    max_iter=1000,\r\n"]
[974.826708, "o", "    n_jobs=None,\r\n"]
[974.839037, "o", "    verbose=0,\r\n"]
[974.851367, "o", "    positive=False,\r\n"]
[974.863696, "o", "):\r\n"]
[974.876025, "o", "    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[974.888354, "o", "\r\n"]
[974.900683, "o", "    n_samples, n_features = X.shape\r\n"]
[974.913012, "o", "    n_components = dictionary.shape[0]\r\n"]
[974.925342, "o", "\r\n"]
[974.937671, "o", "    if algorithm in (\"lars\", \"omp\"):\r\n"]
[975.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[975.002, "i", "sed -n '400,1200p' sklearn/decomposition/_dict_learning.py\r"]
[975.004, "o", "sed -n '400,1200p' sklearn/decomposition/_dict_learning.py\r\n"]
[975.016, "o", "\u001b[?2004l\r\n"]
[975.026, "o", "        regularization = n_nonzero_coefs\r\n"]
[975.036, "o", "        if regularization is None:\r\n"]
[975.046, "o", "            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[975.056, "o", "    else:\r\n"]
[975.066, "o", "        regularization = alpha\r\n"]
[975.076, "o", "        if regularization is None:\r\n"]
[975.086, "o", "            regularization = 1.0\r\n"]
[975.096, "o", "\r\n"]
[975.106, "o", "    if gram is None and algorithm != \"threshold\":\r\n"]
[975.116, "o", "        gram = np.dot(dictionary, dictionary.T)\r\n"]
[975.126, "o", "\r\n"]
[975.136, "o", "    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[975.146, "o", "        copy_cov = False\r\n"]
[975.156, "o", "        cov = np.dot(dictionary, X.T)\r\n"]
[975.166, "o", "\r\n"]
[975.176, "o", "    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[975.186, "o", "        code = _sparse_encode_precomputed(\r\n"]
[975.196, "o", "            X,\r\n"]
[975.206, "o", "            dictionary,\r\n"]
[975.216, "o", "            gram=gram,\r\n"]
[975.226, "o", "            cov=cov,\r\n"]
[975.236, "o", "            algorithm=algorithm,\r\n"]
[975.246, "o", "            regularization=regularization,\r\n"]
[975.256, "o", "            copy_cov=copy_cov,\r\n"]
[975.266, "o", "            init=init,\r\n"]
[975.276, "o", "            max_iter=max_iter,\r\n"]
[975.286, "o", "            verbose=verbose,\r\n"]
[975.296, "o", "            positive=positive,\r\n"]
[975.306, "o", "        )\r\n"]
[975.316, "o", "        return code\r\n"]
[975.326, "o", "\r\n"]
[975.336, "o", "    # Enter parallel code block\r\n"]
[975.346, "o", "    n_samples = X.shape[0]\r\n"]
[975.356, "o", "    n_components = dictionary.shape[0]\r\n"]
[975.366, "o", "    code = np.empty((n_samples, n_components))\r\n"]
[975.376, "o", "    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[975.386, "o", "\r\n"]
[975.396, "o", "    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[975.406, "o", "        delayed(_sparse_encode_precomputed)(\r\n"]
[975.416, "o", "            X[this_slice],\r\n"]
[975.426, "o", "            dictionary,\r\n"]
[975.436, "o", "            gram=gram,\r\n"]
[975.446, "o", "            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[975.456, "o", "            algorithm=algorithm,\r\n"]
[975.466, "o", "            regularization=regularization,\r\n"]
[975.476, "o", "            copy_cov=copy_cov,\r\n"]
[975.486, "o", "            init=init[this_slice] if init is not None else None,\r\n"]
[975.496, "o", "            max_iter=max_iter,\r\n"]
[975.506, "o", "            verbose=verbose,\r\n"]
[975.516, "o", "            positive=positive,\r\n"]
[975.526, "o", "        )\r\n"]
[975.536, "o", "        for this_slice in slices\r\n"]
[975.546, "o", "    )\r\n"]
[975.556, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[975.566, "o", "        code[this_slice] = this_view\r\n"]
[975.576, "o", "    return code\r\n"]
[975.586, "o", "\r\n"]
[975.596, "o", "\r\n"]
[975.606, "o", "def _update_dict(\r\n"]
[975.616, "o", "    dictionary,\r\n"]
[975.626, "o", "    Y,\r\n"]
[975.636, "o", "    code,\r\n"]
[975.646, "o", "    A=None,\r\n"]
[975.656, "o", "    B=None,\r\n"]
[975.666, "o", "    verbose=False,\r\n"]
[975.676, "o", "    random_state=None,\r\n"]
[975.686, "o", "    positive=False,\r\n"]
[975.696, "o", "):\r\n"]
[975.706, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[975.716, "o", "\r\n"]
[975.726, "o", "    Parameters\r\n"]
[975.736, "o", "    ----------\r\n"]
[975.746, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[975.756, "o", "        Value of the dictionary at the previous iteration.\r\n"]
[975.766, "o", "\r\n"]
[975.776, "o", "    Y : ndarray of shape (n_samples, n_features)\r\n"]
[975.786, "o", "        Data matrix.\r\n"]
[975.796, "o", "\r\n"]
[975.806, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[975.816, "o", "        Sparse coding of the data against which to optimize the dictionary.\r\n"]
[975.826, "o", "\r\n"]
[975.836, "o", "    A : ndarray of shape (n_components, n_components), default=None\r\n"]
[975.846, "o", "        Together with `B`, sufficient stats of the online model to update the\r\n"]
[975.856, "o", "        dictionary.\r\n"]
[975.866, "o", "\r\n"]
[975.876, "o", "    B : ndarray of shape (n_features, n_components), default=None\r\n"]
[975.886, "o", "        Together with `A`, sufficient stats of the online model to update the\r\n"]
[975.896, "o", "        dictionary.\r\n"]
[975.906, "o", "\r\n"]
[975.916, "o", "    verbose: bool, default=False\r\n"]
[975.926, "o", "        Degree of output the procedure will print.\r\n"]
[975.936, "o", "\r\n"]
[975.946, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[975.956, "o", "        Used for randomly initializing the dictionary. Pass an int for\r\n"]
[975.966, "o", "        reproducible results across multiple function calls.\r\n"]
[975.976, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[975.986, "o", "\r\n"]
[975.996, "o", "    positive : bool, default=False\r\n"]
[976.006, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[976.016, "o", "\r\n"]
[976.026, "o", "        .. versionadded:: 0.20\r\n"]
[976.036, "o", "    \"\"\"\r\n"]
[976.046, "o", "    n_samples, n_components = code.shape\r\n"]
[976.056, "o", "    random_state = check_random_state(random_state)\r\n"]
[976.066, "o", "\r\n"]
[976.076, "o", "    if A is None:\r\n"]
[976.086, "o", "        A = code.T @ code\r\n"]
[976.096, "o", "    if B is None:\r\n"]
[976.106, "o", "        B = Y.T @ code\r\n"]
[976.116, "o", "\r\n"]
[976.126, "o", "    n_unused = 0\r\n"]
[976.136, "o", "\r\n"]
[976.146, "o", "    for k in range(n_components):\r\n"]
[976.156, "o", "        if A[k, k] > 1e-6:\r\n"]
[976.166, "o", "            # 1e-6 is arbitrary but consistent with the spams implementation\r\n"]
[976.176, "o", "            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n"]
[976.186, "o", "        else:\r\n"]
[976.196, "o", "            # kth atom is almost never used -> sample a new one from the data\r\n"]
[976.206, "o", "            newd = Y[random_state.choice(n_samples)]\r\n"]
[976.216, "o", "\r\n"]
[976.226, "o", "            # add small noise to avoid making the sparse coding ill conditioned\r\n"]
[976.236, "o", "            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n"]
[976.246, "o", "            noise = random_state.normal(0, noise_level, size=len(newd))\r\n"]
[976.256, "o", "\r\n"]
[976.266, "o", "            dictionary[k] = newd + noise\r\n"]
[976.276, "o", "            code[:, k] = 0\r\n"]
[976.286, "o", "            n_unused += 1\r\n"]
[976.296, "o", "\r\n"]
[976.306, "o", "        if positive:\r\n"]
[976.316, "o", "            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n"]
[976.326, "o", "\r\n"]
[976.336, "o", "        # Projection on the constraint set ||V_k|| <= 1\r\n"]
[976.346, "o", "        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n"]
[976.356, "o", "\r\n"]
[976.366, "o", "    if verbose and n_unused > 0:\r\n"]
[976.376, "o", "        print(f\"{n_unused} unused atoms resampled.\")\r\n"]
[976.386, "o", "\r\n"]
[976.396, "o", "\r\n"]
[976.406, "o", "def _dict_learning(\r\n"]
[976.416, "o", "    X,\r\n"]
[976.426, "o", "    n_components,\r\n"]
[976.436, "o", "    *,\r\n"]
[976.446, "o", "    alpha,\r\n"]
[976.456, "o", "    max_iter,\r\n"]
[976.466, "o", "    tol,\r\n"]
[976.476, "o", "    method,\r\n"]
[976.486, "o", "    n_jobs,\r\n"]
[976.496, "o", "    dict_init,\r\n"]
[976.506, "o", "    code_init,\r\n"]
[976.516, "o", "    callback,\r\n"]
[976.526, "o", "    verbose,\r\n"]
[976.536, "o", "    random_state,\r\n"]
[976.546, "o", "    return_n_iter,\r\n"]
[976.556, "o", "    positive_dict,\r\n"]
[976.566, "o", "    positive_code,\r\n"]
[976.576, "o", "    method_max_iter,\r\n"]
[976.586, "o", "):\r\n"]
[976.596, "o", "    \"\"\"Main dictionary learning algorithm\"\"\"\r\n"]
[976.606, "o", "    t0 = time.time()\r\n"]
[976.616, "o", "    # Init the code and the dictionary with SVD of Y\r\n"]
[976.626, "o", "    if code_init is not None and dict_init is not None:\r\n"]
[976.636, "o", "        code = np.array(code_init, order=\"F\")\r\n"]
[976.646, "o", "        # Don't copy V, it will happen below\r\n"]
[976.656, "o", "        dictionary = dict_init\r\n"]
[976.666, "o", "    else:\r\n"]
[976.676, "o", "        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n"]
[976.686, "o", "        # flip the initial code's sign to enforce deterministic output\r\n"]
[976.696, "o", "        code, dictionary = svd_flip(code, dictionary)\r\n"]
[976.706, "o", "        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[976.716, "o", "    r = len(dictionary)\r\n"]
[976.726, "o", "    if n_components <= r:  # True even if n_components=None\r\n"]
[976.736, "o", "        code = code[:, :n_components]\r\n"]
[976.746, "o", "        dictionary = dictionary[:n_components, :]\r\n"]
[976.756, "o", "    else:\r\n"]
[976.766, "o", "        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n"]
[976.776, "o", "        dictionary = np.r_[\r\n"]
[976.786, "o", "            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n"]
[976.796, "o", "        ]\r\n"]
[976.806, "o", "\r\n"]
[976.816, "o", "    # Fortran-order dict better suited for the sparse coding which is the\r\n"]
[976.826, "o", "    # bottleneck of this algorithm.\r\n"]
[976.836, "o", "    dictionary = np.asfortranarray(dictionary)\r\n"]
[976.846, "o", "\r\n"]
[976.856, "o", "    errors = []\r\n"]
[976.866, "o", "    current_cost = np.nan\r\n"]
[976.876, "o", "\r\n"]
[976.886, "o", "    if verbose == 1:\r\n"]
[976.896, "o", "        print(\"[dict_learning]\", end=\" \")\r\n"]
[976.906, "o", "\r\n"]
[976.916, "o", "    # If max_iter is 0, number of iterations returned should be zero\r\n"]
[976.926, "o", "    ii = -1\r\n"]
[976.936, "o", "\r\n"]
[976.946, "o", "    for ii in range(max_iter):\r\n"]
[976.956, "o", "        dt = time.time() - t0\r\n"]
[976.966, "o", "        if verbose == 1:\r\n"]
[976.976, "o", "            sys.stdout.write(\".\")\r\n"]
[976.986, "o", "            sys.stdout.flush()\r\n"]
[976.996, "o", "        elif verbose:\r\n"]
[977.006, "o", "            print(\r\n"]
[977.016, "o", "                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n"]
[977.026, "o", "                % (ii, dt, dt / 60, current_cost)\r\n"]
[977.036, "o", "            )\r\n"]
[977.046, "o", "\r\n"]
[977.056, "o", "        # Update code\r\n"]
[977.066, "o", "        code = sparse_encode(\r\n"]
[977.076, "o", "            X,\r\n"]
[977.086, "o", "            dictionary,\r\n"]
[977.096, "o", "            algorithm=method,\r\n"]
[977.106, "o", "            alpha=alpha,\r\n"]
[977.116, "o", "            init=code,\r\n"]
[977.126, "o", "            n_jobs=n_jobs,\r\n"]
[977.136, "o", "            positive=positive_code,\r\n"]
[977.146, "o", "            max_iter=method_max_iter,\r\n"]
[977.156, "o", "            verbose=verbose,\r\n"]
[977.166, "o", "        )\r\n"]
[977.176, "o", "\r\n"]
[977.186, "o", "        # Update dictionary in place\r\n"]
[977.196, "o", "        _update_dict(\r\n"]
[977.206, "o", "            dictionary,\r\n"]
[977.216, "o", "            X,\r\n"]
[977.226, "o", "            code,\r\n"]
[977.236, "o", "            verbose=verbose,\r\n"]
[977.246, "o", "            random_state=random_state,\r\n"]
[977.256, "o", "            positive=positive_dict,\r\n"]
[977.266, "o", "        )\r\n"]
[977.276, "o", "\r\n"]
[977.286, "o", "        # Cost function\r\n"]
[977.296, "o", "        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n"]
[977.306, "o", "            np.abs(code)\r\n"]
[977.316, "o", "        )\r\n"]
[977.326, "o", "        errors.append(current_cost)\r\n"]
[977.336, "o", "\r\n"]
[977.346, "o", "        if ii > 0:\r\n"]
[977.356, "o", "            dE = errors[-2] - errors[-1]\r\n"]
[977.366, "o", "            # assert(dE >= -tol * errors[-1])\r\n"]
[977.376, "o", "            if dE < tol * errors[-1]:\r\n"]
[977.386, "o", "                if verbose == 1:\r\n"]
[977.396, "o", "                    # A line return\r\n"]
[977.406, "o", "                    print(\"\")\r\n"]
[977.416, "o", "                elif verbose:\r\n"]
[977.426, "o", "                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n"]
[977.436, "o", "                break\r\n"]
[977.446, "o", "        if ii % 5 == 0 and callback is not None:\r\n"]
[977.456, "o", "            callback(locals())\r\n"]
[977.466, "o", "\r\n"]
[977.476, "o", "    if return_n_iter:\r\n"]
[977.486, "o", "        return code, dictionary, errors, ii + 1\r\n"]
[977.496, "o", "    else:\r\n"]
[977.506, "o", "        return code, dictionary, errors\r\n"]
[977.516, "o", "\r\n"]
[977.526, "o", "\r\n"]
[977.536, "o", "def _check_warn_deprecated(param, name, default, additional_message=None):\r\n"]
[977.546, "o", "    if param != \"deprecated\":\r\n"]
[977.556, "o", "        msg = (\r\n"]
[977.566, "o", "            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n"]
[977.576, "o", "        )\r\n"]
[977.586, "o", "        if additional_message:\r\n"]
[977.596, "o", "            msg += f\" {additional_message}\"\r\n"]
[977.606, "o", "        warnings.warn(msg, FutureWarning)\r\n"]
[977.616, "o", "        return param\r\n"]
[977.626, "o", "    else:\r\n"]
[977.636, "o", "        return default\r\n"]
[977.646, "o", "\r\n"]
[977.656, "o", "\r\n"]
[977.666, "o", "def dict_learning_online(\r\n"]
[977.676, "o", "    X,\r\n"]
[977.686, "o", "    n_components=2,\r\n"]
[977.696, "o", "    *,\r\n"]
[977.706, "o", "    alpha=1,\r\n"]
[977.716, "o", "    n_iter=\"deprecated\",\r\n"]
[977.726, "o", "    max_iter=None,\r\n"]
[977.736, "o", "    return_code=True,\r\n"]
[977.746, "o", "    dict_init=None,\r\n"]
[977.756, "o", "    callback=None,\r\n"]
[977.766, "o", "    batch_size=256,\r\n"]
[977.776, "o", "    verbose=False,\r\n"]
[977.786, "o", "    shuffle=True,\r\n"]
[977.796, "o", "    n_jobs=None,\r\n"]
[977.806, "o", "    method=\"lars\",\r\n"]
[977.816, "o", "    iter_offset=\"deprecated\",\r\n"]
[977.826, "o", "    random_state=None,\r\n"]
[977.836, "o", "    return_inner_stats=\"deprecated\",\r\n"]
[977.846, "o", "    inner_stats=\"deprecated\",\r\n"]
[977.856, "o", "    return_n_iter=\"deprecated\",\r\n"]
[977.866, "o", "    positive_dict=False,\r\n"]
[977.876, "o", "    positive_code=False,\r\n"]
[977.886, "o", "    method_max_iter=1000,\r\n"]
[977.896, "o", "    tol=1e-3,\r\n"]
[977.906, "o", "    max_no_improvement=10,\r\n"]
[977.916, "o", "):\r\n"]
[977.926, "o", "    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n"]
[977.936, "o", "\r\n"]
[977.946, "o", "    Finds the best dictionary and the corresponding sparse code for\r\n"]
[977.956, "o", "    approximating the data matrix X by solving::\r\n"]
[977.966, "o", "\r\n"]
[977.976, "o", "        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[977.986, "o", "                     (U,V)\r\n"]
[977.996, "o", "                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n"]
[978.006, "o", "\r\n"]
[978.016, "o", "    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n"]
[978.026, "o", "    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n"]
[978.036, "o", "    which is the sum of the absolute values of all the entries in the matrix.\r\n"]
[978.046, "o", "    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n"]
[978.056, "o", "    the input data.\r\n"]
[978.066, "o", "\r\n"]
[978.076, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[978.086, "o", "\r\n"]
[978.096, "o", "    Parameters\r\n"]
[978.106, "o", "    ----------\r\n"]
[978.116, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[978.126, "o", "        Data matrix.\r\n"]
[978.136, "o", "\r\n"]
[978.146, "o", "    n_components : int or None, default=2\r\n"]
[978.156, "o", "        Number of dictionary atoms to extract. If None, then ``n_components``\r\n"]
[978.166, "o", "        is set to ``n_features``.\r\n"]
[978.176, "o", "\r\n"]
[978.186, "o", "    alpha : float, default=1\r\n"]
[978.196, "o", "        Sparsity controlling parameter.\r\n"]
[978.206, "o", "\r\n"]
[978.216, "o", "    n_iter : int, default=100\r\n"]
[978.226, "o", "        Number of mini-batch iterations to perform.\r\n"]
[978.236, "o", "\r\n"]
[978.246, "o", "        .. deprecated:: 1.1\r\n"]
[978.256, "o", "           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[978.266, "o", "           `max_iter` instead.\r\n"]
[978.276, "o", "\r\n"]
[978.286, "o", "    max_iter : int, default=None\r\n"]
[978.296, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[978.306, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[978.316, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[978.326, "o", "\r\n"]
[978.336, "o", "        .. versionadded:: 1.1\r\n"]
[978.346, "o", "\r\n"]
[978.356, "o", "    return_code : bool, default=True\r\n"]
[978.366, "o", "        Whether to also return the code U or just the dictionary `V`.\r\n"]
[978.376, "o", "\r\n"]
[978.386, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[978.396, "o", "        Initial values for the dictionary for warm restart scenarios.\r\n"]
[978.406, "o", "        If `None`, the initial values for the dictionary are created\r\n"]
[978.416, "o", "        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n"]
[978.426, "o", "\r\n"]
[978.436, "o", "    callback : callable, default=None\r\n"]
[978.446, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[978.456, "o", "\r\n"]
[978.466, "o", "    batch_size : int, default=256\r\n"]
[978.476, "o", "        The number of samples to take in each batch.\r\n"]
[978.486, "o", "\r\n"]
[978.496, "o", "        .. versionchanged:: 1.3\r\n"]
[978.506, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[978.516, "o", "\r\n"]
[978.526, "o", "    verbose : bool, default=False\r\n"]
[978.536, "o", "        To control the verbosity of the procedure.\r\n"]
[978.546, "o", "\r\n"]
[978.556, "o", "    shuffle : bool, default=True\r\n"]
[978.566, "o", "        Whether to shuffle the data before splitting it in batches.\r\n"]
[978.576, "o", "\r\n"]
[978.586, "o", "    n_jobs : int, default=None\r\n"]
[978.596, "o", "        Number of parallel jobs to run.\r\n"]
[978.606, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[978.616, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[978.626, "o", "        for more details.\r\n"]
[978.636, "o", "\r\n"]
[978.646, "o", "    method : {'lars', 'cd'}, default='lars'\r\n"]
[978.656, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[978.666, "o", "          problem (`linear_model.lars_path`);\r\n"]
[978.676, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[978.686, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[978.696, "o", "          the estimated components are sparse.\r\n"]
[978.706, "o", "\r\n"]
[978.716, "o", "    iter_offset : int, default=0\r\n"]
[978.726, "o", "        Number of previous iterations completed on the dictionary used for\r\n"]
[978.736, "o", "        initialization.\r\n"]
[978.746, "o", "\r\n"]
[978.756, "o", "        .. deprecated:: 1.1\r\n"]
[978.766, "o", "           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n"]
[978.776, "o", "\r\n"]
[978.786, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[978.796, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[978.806, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[978.816, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[978.826, "o", "        results across multiple function calls.\r\n"]
[978.836, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[978.846, "o", "\r\n"]
[978.856, "o", "    return_inner_stats : bool, default=False\r\n"]
[978.866, "o", "        Return the inner statistics A (dictionary covariance) and B\r\n"]
[978.876, "o", "        (data approximation). Useful to restart the algorithm in an\r\n"]
[978.886, "o", "        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n"]
[978.896, "o", "        ignored.\r\n"]
[978.906, "o", "\r\n"]
[978.916, "o", "        .. deprecated:: 1.1\r\n"]
[978.926, "o", "           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[978.936, "o", "\r\n"]
[978.946, "o", "    inner_stats : tuple of (A, B) ndarrays, default=None\r\n"]
[978.956, "o", "        Inner sufficient statistics that are kept by the algorithm.\r\n"]
[978.966, "o", "        Passing them at initialization is useful in online settings, to\r\n"]
[978.976, "o", "        avoid losing the history of the evolution.\r\n"]
[978.986, "o", "        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n"]
[978.996, "o", "        `B` `(n_features, n_components)` is the data approximation matrix.\r\n"]
[979.006, "o", "\r\n"]
[979.016, "o", "        .. deprecated:: 1.1\r\n"]
[979.026, "o", "           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[979.036, "o", "\r\n"]
[979.046, "o", "    return_n_iter : bool, default=False\r\n"]
[979.056, "o", "        Whether or not to return the number of iterations.\r\n"]
[979.066, "o", "\r\n"]
[979.076, "o", "        .. deprecated:: 1.1\r\n"]
[979.086, "o", "           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n"]
[979.096, "o", "\r\n"]
[979.106, "o", "    positive_dict : bool, default=False\r\n"]
[979.116, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[979.126, "o", "\r\n"]
[979.136, "o", "        .. versionadded:: 0.20\r\n"]
[979.146, "o", "\r\n"]
[979.156, "o", "    positive_code : bool, default=False\r\n"]
[979.166, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[979.176, "o", "\r\n"]
[979.186, "o", "        .. versionadded:: 0.20\r\n"]
[979.196, "o", "\r\n"]
[979.206, "o", "    method_max_iter : int, default=1000\r\n"]
[979.216, "o", "        Maximum number of iterations to perform when solving the lasso problem.\r\n"]
[979.226, "o", "\r\n"]
[979.236, "o", "        .. versionadded:: 0.22\r\n"]
[979.246, "o", "\r\n"]
[979.256, "o", "    tol : float, default=1e-3\r\n"]
[979.266, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[979.276, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[979.286, "o", "\r\n"]
[979.296, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[979.306, "o", "        `tol` to 0.0.\r\n"]
[979.316, "o", "\r\n"]
[979.326, "o", "        .. versionadded:: 1.1\r\n"]
[979.336, "o", "\r\n"]
[979.346, "o", "    max_no_improvement : int, default=10\r\n"]
[979.356, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[979.366, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[979.376, "o", "        `max_iter` is not None.\r\n"]
[979.386, "o", "\r\n"]
[979.396, "o", "        To disable convergence detection based on cost function, set\r\n"]
[979.406, "o", "        `max_no_improvement` to None.\r\n"]
[979.416, "o", "\r\n"]
[979.426, "o", "        .. versionadded:: 1.1\r\n"]
[979.436, "o", "\r\n"]
[979.446, "o", "    Returns\r\n"]
[979.456, "o", "    -------\r\n"]
[979.466, "o", "    code : ndarray of shape (n_samples, n_components),\r\n"]
[979.476, "o", "        The sparse code (only returned if `return_code=True`).\r\n"]
[979.486, "o", "\r\n"]
[979.496, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[979.506, "o", "        The solutions to the dictionary learning problem.\r\n"]
[979.516, "o", "\r\n"]
[979.526, "o", "    n_iter : int\r\n"]
[979.536, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[979.546, "o", "        set to `True`.\r\n"]
[979.556, "o", "\r\n"]
[979.566, "o", "    See Also\r\n"]
[979.576, "o", "    --------\r\n"]
[979.586, "o", "    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n"]
[979.596, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[979.606, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n"]
[979.616, "o", "        learning algorithm.\r\n"]
[979.626, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[979.636, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[979.646, "o", "    \"\"\"\r\n"]
[979.656, "o", "    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n"]
[979.666, "o", "    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n"]
[979.676, "o", "        raise ValueError(\r\n"]
[979.686, "o", "            \"The following arguments are incompatible with 'max_iter': \"\r\n"]
[979.696, "o", "            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n"]
[979.706, "o", "        )\r\n"]
[979.716, "o", "\r\n"]
[979.726, "o", "    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n"]
[979.736, "o", "    return_inner_stats = _check_warn_deprecated(\r\n"]
[979.746, "o", "        return_inner_stats,\r\n"]
[979.756, "o", "        \"return_inner_stats\",\r\n"]
[979.766, "o", "        default=False,\r\n"]
[979.776, "o", "        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n"]
[979.786, "o", "    )\r\n"]
[979.796, "o", "    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n"]
[979.806, "o", "    return_n_iter = _check_warn_deprecated(\r\n"]
[979.816, "o", "        return_n_iter,\r\n"]
[979.826, "o", "        \"return_n_iter\",\r\n"]
[979.836, "o", "        default=False,\r\n"]
[979.846, "o", "        additional_message=(\r\n"]
[979.856, "o", "            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n"]
[979.866, "o", "            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n"]
[979.876, "o", "        ),\r\n"]
[979.886, "o", "    )\r\n"]
[979.896, "o", "\r\n"]
[979.906, "o", "    if max_iter is not None:\r\n"]
[979.916, "o", "        transform_algorithm = \"lasso_\" + method\r\n"]
[979.926, "o", "\r\n"]
[979.936, "o", "        est = MiniBatchDictionaryLearning(\r\n"]
[980.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[980.002, "i", "sed -n '1200,2200p' sklearn/decomposition/_dict_learning.py\r"]
[980.004, "o", "sed -n '1200,2200p' sklearn/decomposition/_dict_learning.py\r\n"]
[980.016, "o", "\u001b[?2004l\r\n"]
[980.026, "o", "    n_iter : int\r\n"]
[980.036, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[980.046, "o", "        set to True.\r\n"]
[980.056, "o", "\r\n"]
[980.066, "o", "    See Also\r\n"]
[980.076, "o", "    --------\r\n"]
[980.086, "o", "    dict_learning_online : Solve a dictionary learning matrix factorization\r\n"]
[980.096, "o", "        problem online.\r\n"]
[980.106, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[980.116, "o", "    MiniBatchDictionaryLearning : A faster, less accurate version\r\n"]
[980.126, "o", "        of the dictionary learning algorithm.\r\n"]
[980.136, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[980.146, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[980.156, "o", "    \"\"\"\r\n"]
[980.166, "o", "    estimator = DictionaryLearning(\r\n"]
[980.176, "o", "        n_components=n_components,\r\n"]
[980.186, "o", "        alpha=alpha,\r\n"]
[980.196, "o", "        max_iter=max_iter,\r\n"]
[980.206, "o", "        tol=tol,\r\n"]
[980.216, "o", "        fit_algorithm=method,\r\n"]
[980.226, "o", "        n_jobs=n_jobs,\r\n"]
[980.236, "o", "        dict_init=dict_init,\r\n"]
[980.246, "o", "        callback=callback,\r\n"]
[980.256, "o", "        code_init=code_init,\r\n"]
[980.266, "o", "        verbose=verbose,\r\n"]
[980.276, "o", "        random_state=random_state,\r\n"]
[980.286, "o", "        positive_code=positive_code,\r\n"]
[980.296, "o", "        positive_dict=positive_dict,\r\n"]
[980.306, "o", "        transform_max_iter=method_max_iter,\r\n"]
[980.316, "o", "    )\r\n"]
[980.326, "o", "    code = estimator.fit_transform(X)\r\n"]
[980.336, "o", "    if return_n_iter:\r\n"]
[980.346, "o", "        return (\r\n"]
[980.356, "o", "            code,\r\n"]
[980.366, "o", "            estimator.components_,\r\n"]
[980.376, "o", "            estimator.error_,\r\n"]
[980.386, "o", "            estimator.n_iter_,\r\n"]
[980.396, "o", "        )\r\n"]
[980.406, "o", "    return code, estimator.components_, estimator.error_\r\n"]
[980.416, "o", "\r\n"]
[980.426, "o", "\r\n"]
[980.436, "o", "class _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n"]
[980.446, "o", "    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n"]
[980.456, "o", "\r\n"]
[980.466, "o", "    def __init__(\r\n"]
[980.476, "o", "        self,\r\n"]
[980.486, "o", "        transform_algorithm,\r\n"]
[980.496, "o", "        transform_n_nonzero_coefs,\r\n"]
[980.506, "o", "        transform_alpha,\r\n"]
[980.516, "o", "        split_sign,\r\n"]
[980.526, "o", "        n_jobs,\r\n"]
[980.536, "o", "        positive_code,\r\n"]
[980.546, "o", "        transform_max_iter,\r\n"]
[980.556, "o", "    ):\r\n"]
[980.566, "o", "        self.transform_algorithm = transform_algorithm\r\n"]
[980.576, "o", "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n"]
[980.586, "o", "        self.transform_alpha = transform_alpha\r\n"]
[980.596, "o", "        self.transform_max_iter = transform_max_iter\r\n"]
[980.606, "o", "        self.split_sign = split_sign\r\n"]
[980.616, "o", "        self.n_jobs = n_jobs\r\n"]
[980.626, "o", "        self.positive_code = positive_code\r\n"]
[980.636, "o", "\r\n"]
[980.646, "o", "    def _transform(self, X, dictionary):\r\n"]
[980.656, "o", "        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n"]
[980.666, "o", "        SparseCoder.\"\"\"\r\n"]
[980.676, "o", "        X = self._validate_data(X, reset=False)\r\n"]
[980.686, "o", "\r\n"]
[980.696, "o", "        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n"]
[980.706, "o", "            transform_alpha = self.alpha\r\n"]
[980.716, "o", "        else:\r\n"]
[980.726, "o", "            transform_alpha = self.transform_alpha\r\n"]
[980.736, "o", "\r\n"]
[980.746, "o", "        code = sparse_encode(\r\n"]
[980.756, "o", "            X,\r\n"]
[980.766, "o", "            dictionary,\r\n"]
[980.776, "o", "            algorithm=self.transform_algorithm,\r\n"]
[980.786, "o", "            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n"]
[980.796, "o", "            alpha=transform_alpha,\r\n"]
[980.806, "o", "            max_iter=self.transform_max_iter,\r\n"]
[980.816, "o", "            n_jobs=self.n_jobs,\r\n"]
[980.826, "o", "            positive=self.positive_code,\r\n"]
[980.836, "o", "        )\r\n"]
[980.846, "o", "\r\n"]
[980.856, "o", "        if self.split_sign:\r\n"]
[980.866, "o", "            # feature vector is split into a positive and negative side\r\n"]
[980.876, "o", "            n_samples, n_features = code.shape\r\n"]
[980.886, "o", "            split_code = np.empty((n_samples, 2 * n_features))\r\n"]
[980.896, "o", "            split_code[:, :n_features] = np.maximum(code, 0)\r\n"]
[980.906, "o", "            split_code[:, n_features:] = -np.minimum(code, 0)\r\n"]
[980.916, "o", "            code = split_code\r\n"]
[980.926, "o", "\r\n"]
[980.936, "o", "        return code\r\n"]
[980.946, "o", "\r\n"]
[980.956, "o", "    def transform(self, X):\r\n"]
[980.966, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[980.976, "o", "\r\n"]
[980.986, "o", "        Coding method is determined by the object parameter\r\n"]
[980.996, "o", "        `transform_algorithm`.\r\n"]
[981.006, "o", "\r\n"]
[981.016, "o", "        Parameters\r\n"]
[981.026, "o", "        ----------\r\n"]
[981.036, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[981.046, "o", "            Test data to be transformed, must have the same number of\r\n"]
[981.056, "o", "            features as the data used to train the model.\r\n"]
[981.066, "o", "\r\n"]
[981.076, "o", "        Returns\r\n"]
[981.086, "o", "        -------\r\n"]
[981.096, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[981.106, "o", "            Transformed data.\r\n"]
[981.116, "o", "        \"\"\"\r\n"]
[981.126, "o", "        check_is_fitted(self)\r\n"]
[981.136, "o", "        return self._transform(X, self.components_)\r\n"]
[981.146, "o", "\r\n"]
[981.156, "o", "\r\n"]
[981.166, "o", "class SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n"]
[981.176, "o", "    \"\"\"Sparse coding.\r\n"]
[981.186, "o", "\r\n"]
[981.196, "o", "    Finds a sparse representation of data against a fixed, precomputed\r\n"]
[981.206, "o", "    dictionary.\r\n"]
[981.216, "o", "\r\n"]
[981.226, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[981.236, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[981.246, "o", "\r\n"]
[981.256, "o", "        X ~= code * dictionary\r\n"]
[981.266, "o", "\r\n"]
[981.276, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[981.286, "o", "\r\n"]
[981.296, "o", "    Parameters\r\n"]
[981.306, "o", "    ----------\r\n"]
[981.316, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[981.326, "o", "        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n"]
[981.336, "o", "        normalized to unit norm.\r\n"]
[981.346, "o", "\r\n"]
[981.356, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[981.366, "o", "            'threshold'}, default='omp'\r\n"]
[981.376, "o", "        Algorithm used to transform the data:\r\n"]
[981.386, "o", "\r\n"]
[981.396, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[981.406, "o", "          (`linear_model.lars_path`);\r\n"]
[981.416, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[981.426, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[981.436, "o", "          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n"]
[981.446, "o", "          the estimated components are sparse;\r\n"]
[981.456, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[981.466, "o", "          solution;\r\n"]
[981.476, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[981.486, "o", "          the projection ``dictionary * X'``.\r\n"]
[981.496, "o", "\r\n"]
[981.506, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[981.516, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[981.526, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[981.536, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[981.546, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[981.556, "o", "\r\n"]
[981.566, "o", "    transform_alpha : float, default=None\r\n"]
[981.576, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[981.586, "o", "        penalty applied to the L1 norm.\r\n"]
[981.596, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[981.606, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[981.616, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[981.626, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[981.636, "o", "        `n_nonzero_coefs`.\r\n"]
[981.646, "o", "        If `None`, default to 1.\r\n"]
[981.656, "o", "\r\n"]
[981.666, "o", "    split_sign : bool, default=False\r\n"]
[981.676, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[981.686, "o", "        its negative part and its positive part. This can improve the\r\n"]
[981.696, "o", "        performance of downstream classifiers.\r\n"]
[981.706, "o", "\r\n"]
[981.716, "o", "    n_jobs : int, default=None\r\n"]
[981.726, "o", "        Number of parallel jobs to run.\r\n"]
[981.736, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[981.746, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[981.756, "o", "        for more details.\r\n"]
[981.766, "o", "\r\n"]
[981.776, "o", "    positive_code : bool, default=False\r\n"]
[981.786, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[981.796, "o", "\r\n"]
[981.806, "o", "        .. versionadded:: 0.20\r\n"]
[981.816, "o", "\r\n"]
[981.826, "o", "    transform_max_iter : int, default=1000\r\n"]
[981.836, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[981.846, "o", "        `lasso_lars`.\r\n"]
[981.856, "o", "\r\n"]
[981.866, "o", "        .. versionadded:: 0.22\r\n"]
[981.876, "o", "\r\n"]
[981.886, "o", "    Attributes\r\n"]
[981.896, "o", "    ----------\r\n"]
[981.906, "o", "    n_components_ : int\r\n"]
[981.916, "o", "        Number of atoms.\r\n"]
[981.926, "o", "\r\n"]
[981.936, "o", "    n_features_in_ : int\r\n"]
[981.946, "o", "        Number of features seen during :term:`fit`.\r\n"]
[981.956, "o", "\r\n"]
[981.966, "o", "        .. versionadded:: 0.24\r\n"]
[981.976, "o", "\r\n"]
[981.986, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[981.996, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[982.006, "o", "        has feature names that are all strings.\r\n"]
[982.016, "o", "\r\n"]
[982.026, "o", "        .. versionadded:: 1.0\r\n"]
[982.036, "o", "\r\n"]
[982.046, "o", "    See Also\r\n"]
[982.056, "o", "    --------\r\n"]
[982.066, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[982.076, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n"]
[982.086, "o", "        dictionary learning algorithm.\r\n"]
[982.096, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[982.106, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[982.116, "o", "    sparse_encode : Sparse coding where each row of the result is the solution\r\n"]
[982.126, "o", "        to a sparse coding problem.\r\n"]
[982.136, "o", "\r\n"]
[982.146, "o", "    Examples\r\n"]
[982.156, "o", "    --------\r\n"]
[982.166, "o", "    >>> import numpy as np\r\n"]
[982.176, "o", "    >>> from sklearn.decomposition import SparseCoder\r\n"]
[982.186, "o", "    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n"]
[982.196, "o", "    >>> dictionary = np.array(\r\n"]
[982.206, "o", "    ...     [[0, 1, 0],\r\n"]
[982.216, "o", "    ...      [-1, -1, 2],\r\n"]
[982.226, "o", "    ...      [1, 1, 1],\r\n"]
[982.236, "o", "    ...      [0, 1, 1],\r\n"]
[982.246, "o", "    ...      [0, 2, 1]],\r\n"]
[982.256, "o", "    ...    dtype=np.float64\r\n"]
[982.266, "o", "    ... )\r\n"]
[982.276, "o", "    >>> coder = SparseCoder(\r\n"]
[982.286, "o", "    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n"]
[982.296, "o", "    ...     transform_alpha=1e-10,\r\n"]
[982.306, "o", "    ... )\r\n"]
[982.316, "o", "    >>> coder.transform(X)\r\n"]
[982.326, "o", "    array([[ 0.,  0., -1.,  0.,  0.],\r\n"]
[982.336, "o", "           [ 0.,  1.,  1.,  0.,  0.]])\r\n"]
[982.346, "o", "    \"\"\"\r\n"]
[982.356, "o", "\r\n"]
[982.366, "o", "    _required_parameters = [\"dictionary\"]\r\n"]
[982.376, "o", "\r\n"]
[982.386, "o", "    def __init__(\r\n"]
[982.396, "o", "        self,\r\n"]
[982.406, "o", "        dictionary,\r\n"]
[982.416, "o", "        *,\r\n"]
[982.426, "o", "        transform_algorithm=\"omp\",\r\n"]
[982.436, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[982.446, "o", "        transform_alpha=None,\r\n"]
[982.456, "o", "        split_sign=False,\r\n"]
[982.466, "o", "        n_jobs=None,\r\n"]
[982.476, "o", "        positive_code=False,\r\n"]
[982.486, "o", "        transform_max_iter=1000,\r\n"]
[982.496, "o", "    ):\r\n"]
[982.506, "o", "        super().__init__(\r\n"]
[982.516, "o", "            transform_algorithm,\r\n"]
[982.526, "o", "            transform_n_nonzero_coefs,\r\n"]
[982.536, "o", "            transform_alpha,\r\n"]
[982.546, "o", "            split_sign,\r\n"]
[982.556, "o", "            n_jobs,\r\n"]
[982.566, "o", "            positive_code,\r\n"]
[982.576, "o", "            transform_max_iter,\r\n"]
[982.586, "o", "        )\r\n"]
[982.596, "o", "        self.dictionary = dictionary\r\n"]
[982.606, "o", "\r\n"]
[982.616, "o", "    def fit(self, X, y=None):\r\n"]
[982.626, "o", "        \"\"\"Do nothing and return the estimator unchanged.\r\n"]
[982.636, "o", "\r\n"]
[982.646, "o", "        This method is just there to implement the usual API and hence\r\n"]
[982.656, "o", "        work in pipelines.\r\n"]
[982.666, "o", "\r\n"]
[982.676, "o", "        Parameters\r\n"]
[982.686, "o", "        ----------\r\n"]
[982.696, "o", "        X : Ignored\r\n"]
[982.706, "o", "            Not used, present for API consistency by convention.\r\n"]
[982.716, "o", "\r\n"]
[982.726, "o", "        y : Ignored\r\n"]
[982.736, "o", "            Not used, present for API consistency by convention.\r\n"]
[982.746, "o", "\r\n"]
[982.756, "o", "        Returns\r\n"]
[982.766, "o", "        -------\r\n"]
[982.776, "o", "        self : object\r\n"]
[982.786, "o", "            Returns the instance itself.\r\n"]
[982.796, "o", "        \"\"\"\r\n"]
[982.806, "o", "        return self\r\n"]
[982.816, "o", "\r\n"]
[982.826, "o", "    def transform(self, X, y=None):\r\n"]
[982.836, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[982.846, "o", "\r\n"]
[982.856, "o", "        Coding method is determined by the object parameter\r\n"]
[982.866, "o", "        `transform_algorithm`.\r\n"]
[982.876, "o", "\r\n"]
[982.886, "o", "        Parameters\r\n"]
[982.896, "o", "        ----------\r\n"]
[982.906, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[982.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[982.926, "o", "            and `n_features` is the number of features.\r\n"]
[982.936, "o", "\r\n"]
[982.946, "o", "        y : Ignored\r\n"]
[982.956, "o", "            Not used, present for API consistency by convention.\r\n"]
[982.966, "o", "\r\n"]
[982.976, "o", "        Returns\r\n"]
[982.986, "o", "        -------\r\n"]
[982.996, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[983.006, "o", "            Transformed data.\r\n"]
[983.016, "o", "        \"\"\"\r\n"]
[983.026, "o", "        return super()._transform(X, self.dictionary)\r\n"]
[983.036, "o", "\r\n"]
[983.046, "o", "    def _more_tags(self):\r\n"]
[983.056, "o", "        return {\r\n"]
[983.066, "o", "            \"requires_fit\": False,\r\n"]
[983.076, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[983.086, "o", "        }\r\n"]
[983.096, "o", "\r\n"]
[983.106, "o", "    @property\r\n"]
[983.116, "o", "    def n_components_(self):\r\n"]
[983.126, "o", "        \"\"\"Number of atoms.\"\"\"\r\n"]
[983.136, "o", "        return self.dictionary.shape[0]\r\n"]
[983.146, "o", "\r\n"]
[983.156, "o", "    @property\r\n"]
[983.166, "o", "    def n_features_in_(self):\r\n"]
[983.176, "o", "        \"\"\"Number of features seen during `fit`.\"\"\"\r\n"]
[983.186, "o", "        return self.dictionary.shape[1]\r\n"]
[983.196, "o", "\r\n"]
[983.206, "o", "    @property\r\n"]
[983.216, "o", "    def _n_features_out(self):\r\n"]
[983.226, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[983.236, "o", "        return self.n_components_\r\n"]
[983.246, "o", "\r\n"]
[983.256, "o", "\r\n"]
[983.266, "o", "class DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[983.276, "o", "    \"\"\"Dictionary learning.\r\n"]
[983.286, "o", "\r\n"]
[983.296, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[983.306, "o", "    encoding the fitted data.\r\n"]
[983.316, "o", "\r\n"]
[983.326, "o", "    Solves the optimization problem::\r\n"]
[983.336, "o", "\r\n"]
[983.346, "o", "        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[983.356, "o", "                    (U,V)\r\n"]
[983.366, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[983.376, "o", "\r\n"]
[983.386, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[983.396, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[983.406, "o", "    of all the entries in the matrix.\r\n"]
[983.416, "o", "\r\n"]
[983.426, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[983.436, "o", "\r\n"]
[983.446, "o", "    Parameters\r\n"]
[983.456, "o", "    ----------\r\n"]
[983.466, "o", "    n_components : int, default=None\r\n"]
[983.476, "o", "        Number of dictionary elements to extract. If None, then ``n_components``\r\n"]
[983.486, "o", "        is set to ``n_features``.\r\n"]
[983.496, "o", "\r\n"]
[983.506, "o", "    alpha : float, default=1.0\r\n"]
[983.516, "o", "        Sparsity controlling parameter.\r\n"]
[983.526, "o", "\r\n"]
[983.536, "o", "    max_iter : int, default=1000\r\n"]
[983.546, "o", "        Maximum number of iterations to perform.\r\n"]
[983.556, "o", "\r\n"]
[983.566, "o", "    tol : float, default=1e-8\r\n"]
[983.576, "o", "        Tolerance for numerical error.\r\n"]
[983.586, "o", "\r\n"]
[983.596, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[983.606, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[983.616, "o", "          problem (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[983.626, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[983.636, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n"]
[983.646, "o", "          faster if the estimated components are sparse.\r\n"]
[983.656, "o", "\r\n"]
[983.666, "o", "        .. versionadded:: 0.17\r\n"]
[983.676, "o", "           *cd* coordinate descent method to improve speed.\r\n"]
[983.686, "o", "\r\n"]
[983.696, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[983.706, "o", "            'threshold'}, default='omp'\r\n"]
[983.716, "o", "        Algorithm used to transform the data:\r\n"]
[983.726, "o", "\r\n"]
[983.736, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[983.746, "o", "          (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[983.756, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[983.766, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[983.776, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n"]
[983.786, "o", "          will be faster if the estimated components are sparse.\r\n"]
[983.796, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[983.806, "o", "          solution.\r\n"]
[983.816, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[983.826, "o", "          the projection ``dictionary * X'``.\r\n"]
[983.836, "o", "\r\n"]
[983.846, "o", "        .. versionadded:: 0.17\r\n"]
[983.856, "o", "           *lasso_cd* coordinate descent method to improve speed.\r\n"]
[983.866, "o", "\r\n"]
[983.876, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[983.886, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[983.896, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[983.906, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[983.916, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[983.926, "o", "\r\n"]
[983.936, "o", "    transform_alpha : float, default=None\r\n"]
[983.946, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[983.956, "o", "        penalty applied to the L1 norm.\r\n"]
[983.966, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[983.976, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[983.986, "o", "        If `None`, defaults to `alpha`.\r\n"]
[983.996, "o", "\r\n"]
[984.006, "o", "        .. versionchanged:: 1.2\r\n"]
[984.016, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[984.026, "o", "\r\n"]
[984.036, "o", "    n_jobs : int or None, default=None\r\n"]
[984.046, "o", "        Number of parallel jobs to run.\r\n"]
[984.056, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[984.066, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[984.076, "o", "        for more details.\r\n"]
[984.086, "o", "\r\n"]
[984.096, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[984.106, "o", "        Initial value for the code, for warm restart. Only used if `code_init`\r\n"]
[984.116, "o", "        and `dict_init` are not None.\r\n"]
[984.126, "o", "\r\n"]
[984.136, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[984.146, "o", "        Initial values for the dictionary, for warm restart. Only used if\r\n"]
[984.156, "o", "        `code_init` and `dict_init` are not None.\r\n"]
[984.166, "o", "\r\n"]
[984.176, "o", "    callback : callable, default=None\r\n"]
[984.186, "o", "        Callable that gets invoked every five iterations.\r\n"]
[984.196, "o", "\r\n"]
[984.206, "o", "        .. versionadded:: 1.3\r\n"]
[984.216, "o", "\r\n"]
[984.226, "o", "    verbose : bool, default=False\r\n"]
[984.236, "o", "        To control the verbosity of the procedure.\r\n"]
[984.246, "o", "\r\n"]
[984.256, "o", "    split_sign : bool, default=False\r\n"]
[984.266, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[984.276, "o", "        its negative part and its positive part. This can improve the\r\n"]
[984.286, "o", "        performance of downstream classifiers.\r\n"]
[984.296, "o", "\r\n"]
[984.306, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[984.316, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[984.326, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[984.336, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[984.346, "o", "        results across multiple function calls.\r\n"]
[984.356, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[984.366, "o", "\r\n"]
[984.376, "o", "    positive_code : bool, default=False\r\n"]
[984.386, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[984.396, "o", "\r\n"]
[984.406, "o", "        .. versionadded:: 0.20\r\n"]
[984.416, "o", "\r\n"]
[984.426, "o", "    positive_dict : bool, default=False\r\n"]
[984.436, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[984.446, "o", "\r\n"]
[984.456, "o", "        .. versionadded:: 0.20\r\n"]
[984.466, "o", "\r\n"]
[984.476, "o", "    transform_max_iter : int, default=1000\r\n"]
[984.486, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[984.496, "o", "        `'lasso_lars'`.\r\n"]
[984.506, "o", "\r\n"]
[984.516, "o", "        .. versionadded:: 0.22\r\n"]
[984.526, "o", "\r\n"]
[984.536, "o", "    Attributes\r\n"]
[984.546, "o", "    ----------\r\n"]
[984.556, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[984.566, "o", "        dictionary atoms extracted from the data\r\n"]
[984.576, "o", "\r\n"]
[984.586, "o", "    error_ : array\r\n"]
[984.596, "o", "        vector of errors at each iteration\r\n"]
[984.606, "o", "\r\n"]
[984.616, "o", "    n_features_in_ : int\r\n"]
[984.626, "o", "        Number of features seen during :term:`fit`.\r\n"]
[984.636, "o", "\r\n"]
[984.646, "o", "        .. versionadded:: 0.24\r\n"]
[984.656, "o", "\r\n"]
[984.666, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[984.676, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[984.686, "o", "        has feature names that are all strings.\r\n"]
[984.696, "o", "\r\n"]
[984.706, "o", "        .. versionadded:: 1.0\r\n"]
[984.716, "o", "\r\n"]
[984.726, "o", "    n_iter_ : int\r\n"]
[984.736, "o", "        Number of iterations run.\r\n"]
[984.746, "o", "\r\n"]
[984.756, "o", "    See Also\r\n"]
[984.766, "o", "    --------\r\n"]
[984.776, "o", "    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n"]
[984.786, "o", "        dictionary learning algorithm.\r\n"]
[984.796, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[984.806, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[984.816, "o", "        precomputed dictionary.\r\n"]
[984.826, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[984.836, "o", "\r\n"]
[984.846, "o", "    References\r\n"]
[984.856, "o", "    ----------\r\n"]
[984.866, "o", "\r\n"]
[984.876, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[984.886, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[984.896, "o", "\r\n"]
[984.906, "o", "    Examples\r\n"]
[984.916, "o", "    --------\r\n"]
[984.926, "o", "    >>> import numpy as np\r\n"]
[984.936, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[985.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[985.002, "i", "applypatch << 'PATCH'\r"]
[985.004, "o", "applypatch << 'PATCH'\r\n"]
[985.123714, "o", "\u001b[?2004l\r\n"]
[985.241429, "o", "\u001b[?2004h> *** Begin Patch\r\n"]
[985.359143, "o", "\u001b[?2004l\r\n"]
[985.476857, "o", "\u001b[?2004h> *** Update File: sklearn/decomposition/_dict_learning.py\r\n"]
[985.594571, "o", "\u001b[?2004l\r\n"]
[985.712286, "o", "\u001b[?2004h> @@\r\n"]
[985.83, "o", "\u001b[?2004l\r\n"]
[985.947714, "o", "\u001b[?2004h> -        new_code = orthogonal_mp_gram(\r\n"]
[986.065429, "o", "\u001b[?2004l\r\n"]
[986.183143, "o", "\u001b[?2004h> +        # Avoid unnecessary copies of the covariance / Xy array in the\r\n"]
[986.300857, "o", "\u001b[?2004l\r\n"]
[986.418571, "o", "\u001b[?2004h> +        # hot path. The covariance is not reused after sparse coding within\r\n"]
[986.536286, "o", "\u001b[?2004l\r\n"]
[986.654, "o", "\u001b[?2004h> +        # the minibatch update, so we can safely allow in-place updates.\r\n"]
[986.771714, "o", "\u001b[?2004l\r\n"]
[986.889429, "o", "\u001b[?2004h> +        # This reduces allocation and memory traffic in tight loops.\r\n"]
[987.007143, "o", "\u001b[?2004l\r\n"]
[987.124857, "o", "\u001b[?2004h> +        new_code = orthogonal_mp_gram(\r\n"]
[987.242571, "o", "\u001b[?2004l\r\n"]
[987.360286, "o", "\u001b[?2004h>              Gram=gram,\r\n"]
[987.478, "o", "\u001b[?2004l\r\n"]
[987.595714, "o", "\u001b[?2004h>              Xy=cov,\r\n"]
[987.713429, "o", "\u001b[?2004l\r\n"]
[987.831143, "o", "\u001b[?2004h>              n_nonzero_coefs=int(regularization),\r\n"]
[987.948857, "o", "\u001b[?2004l\r\n"]
[988.066571, "o", "\u001b[?2004h>              tol=None,\r\n"]
[988.184286, "o", "\u001b[?2004l\r\n"]
[988.302, "o", "\u001b[?2004h> -            norms_squared=row_norms(X, squared=True),\r\n"]
[988.419714, "o", "\u001b[?2004l\r\n"]
[988.537429, "o", "\u001b[?2004h> -            copy_Xy=copy_cov,\r\n"]
[988.655143, "o", "\u001b[?2004l\r\n"]
[988.772857, "o", "\u001b[?2004h> +            norms_squared=row_norms(X, squared=True),\r\n"]
[988.890571, "o", "\u001b[?2004l\r\n"]
[989.008286, "o", "\u001b[?2004h> +            copy_Xy=copy_cov,\r\n"]
[989.126, "o", "\u001b[?2004l\r\n"]
[989.243714, "o", "\u001b[?2004h>          ).T\r\n"]
[989.361429, "o", "\u001b[?2004l\r\n"]
[989.479143, "o", "\u001b[?2004h> *** End Patch\r\n"]
[989.596857, "o", "\u001b[?2004l\r\n"]
[989.714571, "o", "\u001b[?2004h> PATCH\r\n"]
[989.832286, "o", "\u001b[?2004l\r\n"]
[990.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[990.002, "i", "applypatch << 'PATCH'\r"]
[990.004, "o", "applypatch << 'PATCH'\r\n"]
[990.1605, "o", "\u001b[?2004l\r\n"]
[990.315, "o", "\u001b[?2004h> *** Begin Patch\r\n"]
[990.4695, "o", "\u001b[?2004l\r\n"]
[990.624, "o", "\u001b[?2004h> *** Update File: sklearn/decomposition/_dict_learning.py\r\n"]
[990.7785, "o", "\u001b[?2004l\r\n"]
[990.933, "o", "\u001b[?2004h> @@\r\n"]
[991.0875, "o", "\u001b[?2004l\r\n"]
[991.242, "o", "\u001b[?2004h> -        clf = Lasso(\r\n"]
[991.3965, "o", "\u001b[?2004l\r\n"]
[991.551, "o", "\u001b[?2004h> +        # Reuse precomputed Gram and avoid redundant allocations when possible\r\n"]
[991.7055, "o", "\u001b[?2004l\r\n"]
[991.86, "o", "\u001b[?2004h> +        clf = Lasso(\r\n"]
[992.0145, "o", "\u001b[?2004l\r\n"]
[992.169, "o", "\u001b[?2004h>              alpha=alpha,\r\n"]
[992.3235, "o", "\u001b[?2004l\r\n"]
[992.478, "o", "\u001b[?2004h>              fit_intercept=False,\r\n"]
[992.6325, "o", "\u001b[?2004l\r\n"]
[992.787, "o", "\u001b[?2004h>              precompute=gram,\r\n"]
[992.9415, "o", "\u001b[?2004l\r\n"]
[993.096, "o", "\u001b[?2004h>              max_iter=max_iter,\r\n"]
[993.2505, "o", "\u001b[?2004l\r\n"]
[993.405, "o", "\u001b[?2004h>              warm_start=True,\r\n"]
[993.5595, "o", "\u001b[?2004l\r\n"]
[993.714, "o", "\u001b[?2004h>              positive=positive,\r\n"]
[993.8685, "o", "\u001b[?2004l\r\n"]
[994.023, "o", "\u001b[?2004h>          )\r\n"]
[994.1775, "o", "\u001b[?2004l\r\n"]
[994.332, "o", "\u001b[?2004h> *** End Patch\r\n"]
[994.4865, "o", "\u001b[?2004l\r\n"]
[994.641, "o", "\u001b[?2004h> PATCH\r\n"]
[994.7955, "o", "\u001b[?2004l\r\n"]
[995.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[995.002, "i", "applypatch << 'PATCH'\r"]
[995.004, "o", "applypatch << 'PATCH'\r\n"]
[995.143333, "o", "\u001b[?2004l\r\n"]
[995.280667, "o", "\u001b[?2004h> *** Begin Patch\r\n"]
[995.418, "o", "\u001b[?2004l\r\n"]
[995.555333, "o", "\u001b[?2004h> *** Update File: sklearn/decomposition/_dict_learning.py\r\n"]
[995.692667, "o", "\u001b[?2004l\r\n"]
[995.83, "o", "\u001b[?2004h> @@\r\n"]
[995.967333, "o", "\u001b[?2004l\r\n"]
[996.104667, "o", "\u001b[?2004h> -            lasso_lars = LassoLars(\r\n"]
[996.242, "o", "\u001b[?2004l\r\n"]
[996.379333, "o", "\u001b[?2004h> +            # Instantiate the estimator with precomputed Gram to avoid\r\n"]
[996.516667, "o", "\u001b[?2004l\r\n"]
[996.654, "o", "\u001b[?2004h> +            # recomputations inside the solver.\r\n"]
[996.791333, "o", "\u001b[?2004l\r\n"]
[996.928667, "o", "\u001b[?2004h> +            lasso_lars = LassoLars(\r\n"]
[997.066, "o", "\u001b[?2004l\r\n"]
[997.203333, "o", "\u001b[?2004h>                  alpha=alpha,\r\n"]
[997.340667, "o", "\u001b[?2004l\r\n"]
[997.478, "o", "\u001b[?2004h>                  fit_intercept=False,\r\n"]
[997.615333, "o", "\u001b[?2004l\r\n"]
[997.752667, "o", "\u001b[?2004h>                  verbose=verbose,\r\n"]
[997.89, "o", "\u001b[?2004l\r\n"]
[998.027333, "o", "\u001b[?2004h>                  precompute=gram,\r\n"]
[998.164667, "o", "\u001b[?2004l\r\n"]
[998.302, "o", "\u001b[?2004h>                  fit_path=False,\r\n"]
[998.439333, "o", "\u001b[?2004l\r\n"]
[998.576667, "o", "\u001b[?2004h>                  positive=positive,\r\n"]
[998.714, "o", "\u001b[?2004l\r\n"]
[998.851333, "o", "\u001b[?2004h>                  max_iter=max_iter,\r\n"]
[998.988667, "o", "\u001b[?2004l\r\n"]
[999.126, "o", "\u001b[?2004h>              )\r\n"]
[999.263333, "o", "\u001b[?2004l\r\n"]
[999.400667, "o", "\u001b[?2004h> *** End Patch\r\n"]
[999.538, "o", "\u001b[?2004l\r\n"]
[999.675333, "o", "\u001b[?2004h> PATCH\r\n"]
[999.812667, "o", "\u001b[?2004l\r\n"]
[1000.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1000.002, "i", "git diff -- sklearn/decomposition/_dict_learning.py | sed -n '1,200p'\r"]
[1000.004, "o", "git diff -- sklearn/decomposition/_dict_learning.py | sed -n '1,200p'\r\n"]
[1000.006, "o", "\u001b[?2004l\r\n"]
[1005.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1005.002, "i", "pytest -q sklearn/decomposition/tests/test_dict_learning.py -k minibatch -q || true\r"]
[1005.004, "o", "pytest -q sklearn/decomposition/tests/test_dict_learning.py -k minibatch -q || true\r\n"]
[1007.478, "o", "\u001b[?2004l\r\n"]
[1010.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1010.002, "i", "cd asv_benchmarks\r"]
[1010.004, "o", "cd asv_benchmarks\r\n"]
[1010.006, "o", "\u001b[?2004l\r\n"]
[1015.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1015.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r"]
[1015.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r\n"]
[1015.296824, "o", "stderr\r\n"]
[1015.587647, "o", "\u001b[?2004l\r\n"]
[1015.878471, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[1016.169294, "o", "\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n"]
[1016.460118, "o", "[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n"]
[1016.750941, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n"]
[1017.041765, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0masv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_transform\" --show-stderr\r\n"]
[1017.332588, "o", "                                                                                     ok\r\n"]
[1017.623412, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ============\u001b[0m\r\n"]
[1017.914235, "o", "              --                 n_jobs   \r\n"]
[1018.205059, "o", "              --------------- ------------\r\n"]
[1018.495882, "o", "               fit_algorithm       1      \r\n"]
[1018.786706, "o", "              =============== ============\r\n"]
[1019.077529, "o", "                    lars       6.61\u00b10.06s \r\n"]
[1019.368353, "o", "                     cd        1.54\u00b10.01s \r\n"]
[1019.659176, "o", "              =============== ============\r\n"]
[1020.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1020.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_transform\" --\r"]
[1020.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_transform\" --\r\n"]
[1020.026949, "o", "-show-stderr\r\n"]
[1020.047898, "o", "\u001b[?2004l\r\n"]
[1020.068847, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[1020.089797, "o", "\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n"]
[1020.110746, "o", "[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n"]
[1020.131695, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n"]
[1020.152644, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_transform\u001b[0mcd ..\r\n"]
[1020.173593, "o", "grep -n \"def _minibatch_step\\|_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1020.194542, "o", "sed -n '1,500p' sklearn/decomposition/_dict_learning.py\r\n"]
[1020.215492, "o", "sed -n '500,1200p' sklearn/decomposition/_dict_learning.py\r\n"]
[1020.236441, "o", "                                                                               ok\r\n"]
[1020.25739, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =========\u001b[0m\r\n"]
[1020.278339, "o", "              --                n_jobs \r\n"]
[1020.299288, "o", "              --------------- ---------\r\n"]
[1020.320237, "o", "               fit_algorithm      1    \r\n"]
[1020.341186, "o", "              =============== =========\r\n"]
[1020.362136, "o", "                    lars       162\u00b11ms \r\n"]
[1020.383085, "o", "                     cd        155\u00b13ms \r\n"]
[1020.404034, "o", "              =============== =========\r\n"]
[1020.424983, "o", "\r\n"]
[1020.445932, "o", "[100.00%] \u00b7\u00b7\u00b7\u00b7 \u001b[0;31mFor parameters: 'lars', 1\u001b[0m\r\n"]
[1020.466881, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1020.487831, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1020.50878, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1020.529729, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1020.550678, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1020.571627, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1020.592576, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1020.613525, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1020.634475, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1020.655424, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1020.676373, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1020.697322, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1020.718271, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1020.73922, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1020.760169, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1020.781119, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1020.802068, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1020.823017, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1020.843966, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1020.864915, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1020.885864, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1020.906814, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1020.927763, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1020.948712, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1020.969661, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1020.99061, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1021.011559, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1021.032508, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1021.053458, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1021.074407, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1021.095356, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1021.116305, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1021.137254, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1021.158203, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1021.179153, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1021.200102, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1021.221051, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1021.242, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1021.262949, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1021.283898, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1021.304847, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1021.325797, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1021.346746, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1021.367695, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1021.388644, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1021.409593, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1021.430542, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1021.451492, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1021.472441, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1021.49339, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1021.514339, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1021.535288, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1021.556237, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1021.577186, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1021.598136, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1021.619085, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1021.640034, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1021.660983, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1021.681932, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1021.702881, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1021.723831, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1021.74478, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1021.765729, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1021.786678, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1021.807627, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1021.828576, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1021.849525, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1021.870475, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1021.891424, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1021.912373, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1021.933322, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1021.954271, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1021.97522, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1021.996169, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1022.017119, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1022.038068, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1022.059017, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1022.079966, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1022.100915, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1022.121864, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1022.142814, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1022.163763, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1022.184712, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1022.205661, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1022.22661, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1022.247559, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1022.268508, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1022.289458, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1022.310407, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1022.331356, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1022.352305, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1022.373254, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1022.394203, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1022.415153, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1022.436102, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1022.457051, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1022.478, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1022.498949, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1022.519898, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1022.540847, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1022.561797, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1022.582746, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1022.603695, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1022.624644, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1022.645593, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1022.666542, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1022.687492, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1022.708441, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1022.72939, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1022.750339, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1022.771288, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1022.792237, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1022.813186, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1022.834136, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1022.855085, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1022.876034, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1022.896983, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1022.917932, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1022.938881, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1022.959831, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1022.98078, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1023.001729, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1023.022678, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1023.043627, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1023.064576, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1023.085525, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1023.106475, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1023.127424, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1023.148373, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1023.169322, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1023.190271, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1023.21122, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1023.232169, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1023.253119, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1023.274068, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1023.295017, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1023.315966, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1023.336915, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1023.357864, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1023.378814, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1023.399763, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1023.420712, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1023.441661, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1023.46261, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1023.483559, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1023.504508, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1023.525458, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1023.546407, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1023.567356, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1023.588305, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1023.609254, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1023.630203, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1023.651153, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1023.672102, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1023.693051, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1023.714, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1023.734949, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1023.755898, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1023.776847, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1023.797797, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1023.818746, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1023.839695, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1023.860644, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1023.881593, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1023.902542, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1023.923492, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1023.944441, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1023.96539, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1023.986339, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1024.007288, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1024.028237, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1024.049186, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1024.070136, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1024.091085, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1024.112034, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1024.132983, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1024.153932, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1024.174881, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1024.195831, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1024.21678, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1024.237729, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1024.258678, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1024.279627, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1024.300576, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1024.321525, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1024.342475, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1024.363424, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1024.384373, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1024.405322, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1024.426271, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1024.44722, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1024.468169, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1024.489119, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1024.510068, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1024.531017, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1024.551966, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1024.572915, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1024.593864, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1024.614814, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1024.635763, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1024.656712, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1024.677661, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1024.69861, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1024.719559, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1024.740508, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1024.761458, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1024.782407, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1024.803356, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1024.824305, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1024.845254, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1024.866203, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1024.887153, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1024.908102, "o", "               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n"]
[1024.929051, "o", "                 new_code = orthogonal_mp_gram(\r\n"]
[1025.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1025.002, "i", "cd ..\r"]
[1025.004, "o", "cd ..\r\n"]
[1025.006, "o", "\u001b[?2004l\r\n"]
[1030.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1030.002, "i", "grep -n \"def _minibatch_step\\|_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.py \r"]
[1030.004, "o", "grep -n \"def _minibatch_step\\|_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.py \r\n"]
[1035.0, "o", "\u001bM(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1035.002, "i", "grep -n \"def _minibatch_step\\|_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.p\u001b[Ky\r"]
[1035.004, "o", "grep -n \"def _minibatch_step\\|_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.p\u001b[Ky\r\n"]
[1037.478, "o", "\u001b[?2004l\r\n"]
[1040.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1040.002, "i", "sed -n '1,500p' sklearn/decomposition/_dict_learning.py\r"]
[1040.004, "o", "sed -n '1,500p' sklearn/decomposition/_dict_learning.py\r\n"]
[1040.016, "o", "\u001b[?2004l\r\n"]
[1040.026, "o", "\"\"\" Dictionary learning.\r\n"]
[1040.036, "o", "\"\"\"\r\n"]
[1040.046, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[1040.056, "o", "# License: BSD 3 clause\r\n"]
[1040.066, "o", "\r\n"]
[1040.076, "o", "import itertools\r\n"]
[1040.086, "o", "import sys\r\n"]
[1040.096, "o", "import time\r\n"]
[1040.106, "o", "import warnings\r\n"]
[1040.116, "o", "from math import ceil\r\n"]
[1040.126, "o", "from numbers import Integral, Real\r\n"]
[1040.136, "o", "\r\n"]
[1040.146, "o", "import numpy as np\r\n"]
[1040.156, "o", "from joblib import effective_n_jobs\r\n"]
[1040.166, "o", "from scipy import linalg\r\n"]
[1040.176, "o", "\r\n"]
[1040.186, "o", "from ..base import (\r\n"]
[1040.196, "o", "    BaseEstimator,\r\n"]
[1040.206, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[1040.216, "o", "    TransformerMixin,\r\n"]
[1040.226, "o", "    _fit_context,\r\n"]
[1040.236, "o", ")\r\n"]
[1040.246, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[1040.256, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[1040.266, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[1040.276, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[1040.286, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[1040.296, "o", "from ..utils.validation import check_is_fitted\r\n"]
[1040.306, "o", "\r\n"]
[1040.316, "o", "\r\n"]
[1040.326, "o", "def _check_positive_coding(method, positive):\r\n"]
[1040.336, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[1040.346, "o", "        raise ValueError(\r\n"]
[1040.356, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[1040.366, "o", "        )\r\n"]
[1040.376, "o", "\r\n"]
[1040.386, "o", "\r\n"]
[1040.396, "o", "def _sparse_encode_precomputed(\r\n"]
[1040.406, "o", "    X,\r\n"]
[1040.416, "o", "    dictionary,\r\n"]
[1040.426, "o", "    *,\r\n"]
[1040.436, "o", "    gram=None,\r\n"]
[1040.446, "o", "    cov=None,\r\n"]
[1040.456, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1040.466, "o", "    regularization=None,\r\n"]
[1040.476, "o", "    copy_cov=True,\r\n"]
[1040.486, "o", "    init=None,\r\n"]
[1040.496, "o", "    max_iter=1000,\r\n"]
[1040.506, "o", "    verbose=0,\r\n"]
[1040.516, "o", "    positive=False,\r\n"]
[1040.526, "o", "):\r\n"]
[1040.536, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[1040.546, "o", "\r\n"]
[1040.556, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[1040.566, "o", "\r\n"]
[1040.576, "o", "    Parameters\r\n"]
[1040.586, "o", "    ----------\r\n"]
[1040.596, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[1040.606, "o", "        Data matrix.\r\n"]
[1040.616, "o", "\r\n"]
[1040.626, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1040.636, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1040.646, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[1040.656, "o", "\r\n"]
[1040.666, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[1040.676, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[1040.686, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[1040.696, "o", "\r\n"]
[1040.706, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[1040.716, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[1040.726, "o", "\r\n"]
[1040.736, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1040.746, "o", "            default='lasso_lars'\r\n"]
[1040.756, "o", "        The algorithm used:\r\n"]
[1040.766, "o", "\r\n"]
[1040.776, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[1040.786, "o", "          (`linear_model.lars_path`);\r\n"]
[1040.796, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1040.806, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1040.816, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1040.826, "o", "          the estimated components are sparse;\r\n"]
[1040.836, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1040.846, "o", "          solution;\r\n"]
[1040.856, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1040.866, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[1040.876, "o", "\r\n"]
[1040.886, "o", "    regularization : int or float, default=None\r\n"]
[1040.896, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[1040.906, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[1040.916, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[1040.926, "o", "\r\n"]
[1040.936, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1040.946, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[1040.956, "o", "        `algorithm='lasso_cd'`.\r\n"]
[1040.966, "o", "\r\n"]
[1040.976, "o", "    max_iter : int, default=1000\r\n"]
[1040.986, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1040.996, "o", "        `'lasso_lars'`.\r\n"]
[1041.006, "o", "\r\n"]
[1041.016, "o", "    copy_cov : bool, default=True\r\n"]
[1041.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1041.036, "o", "        be overwritten.\r\n"]
[1041.046, "o", "\r\n"]
[1041.056, "o", "    verbose : int, default=0\r\n"]
[1041.066, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[1041.076, "o", "\r\n"]
[1041.086, "o", "    positive: bool, default=False\r\n"]
[1041.096, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[1041.106, "o", "\r\n"]
[1041.116, "o", "        .. versionadded:: 0.20\r\n"]
[1041.126, "o", "\r\n"]
[1041.136, "o", "    Returns\r\n"]
[1041.146, "o", "    -------\r\n"]
[1041.156, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[1041.166, "o", "        The sparse codes.\r\n"]
[1041.176, "o", "    \"\"\"\r\n"]
[1041.186, "o", "    n_samples, n_features = X.shape\r\n"]
[1041.196, "o", "    n_components = dictionary.shape[0]\r\n"]
[1041.206, "o", "\r\n"]
[1041.216, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[1041.226, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1041.236, "o", "        try:\r\n"]
[1041.246, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1041.256, "o", "\r\n"]
[1041.266, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1041.276, "o", "            # corrects the verbosity level.\r\n"]
[1041.286, "o", "            lasso_lars = LassoLars(\r\n"]
[1041.296, "o", "                alpha=alpha,\r\n"]
[1041.306, "o", "                fit_intercept=False,\r\n"]
[1041.316, "o", "                verbose=verbose,\r\n"]
[1041.326, "o", "                precompute=gram,\r\n"]
[1041.336, "o", "                fit_path=False,\r\n"]
[1041.346, "o", "                positive=positive,\r\n"]
[1041.356, "o", "                max_iter=max_iter,\r\n"]
[1041.366, "o", "            )\r\n"]
[1041.376, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1041.386, "o", "            new_code = lasso_lars.coef_\r\n"]
[1041.396, "o", "        finally:\r\n"]
[1041.406, "o", "            np.seterr(**err_mgt)\r\n"]
[1041.416, "o", "\r\n"]
[1041.426, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[1041.436, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1041.446, "o", "\r\n"]
[1041.456, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[1041.466, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[1041.476, "o", "        # argument that we could pass in from Lasso.\r\n"]
[1041.486, "o", "        clf = Lasso(\r\n"]
[1041.496, "o", "            alpha=alpha,\r\n"]
[1041.506, "o", "            fit_intercept=False,\r\n"]
[1041.516, "o", "            precompute=gram,\r\n"]
[1041.526, "o", "            max_iter=max_iter,\r\n"]
[1041.536, "o", "            warm_start=True,\r\n"]
[1041.546, "o", "            positive=positive,\r\n"]
[1041.556, "o", "        )\r\n"]
[1041.566, "o", "\r\n"]
[1041.576, "o", "        if init is not None:\r\n"]
[1041.586, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[1041.596, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[1041.606, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[1041.616, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[1041.626, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[1041.636, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[1041.646, "o", "                init = np.array(init)\r\n"]
[1041.656, "o", "            clf.coef_ = init\r\n"]
[1041.666, "o", "\r\n"]
[1041.676, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[1041.686, "o", "        new_code = clf.coef_\r\n"]
[1041.696, "o", "\r\n"]
[1041.706, "o", "    elif algorithm == \"lars\":\r\n"]
[1041.716, "o", "        try:\r\n"]
[1041.726, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1041.736, "o", "\r\n"]
[1041.746, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1041.756, "o", "            # corrects the verbosity level.\r\n"]
[1041.766, "o", "            lars = Lars(\r\n"]
[1041.776, "o", "                fit_intercept=False,\r\n"]
[1041.786, "o", "                verbose=verbose,\r\n"]
[1041.796, "o", "                precompute=gram,\r\n"]
[1041.806, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[1041.816, "o", "                fit_path=False,\r\n"]
[1041.826, "o", "            )\r\n"]
[1041.836, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1041.846, "o", "            new_code = lars.coef_\r\n"]
[1041.856, "o", "        finally:\r\n"]
[1041.866, "o", "            np.seterr(**err_mgt)\r\n"]
[1041.876, "o", "\r\n"]
[1041.886, "o", "    elif algorithm == \"threshold\":\r\n"]
[1041.896, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[1041.906, "o", "        if positive:\r\n"]
[1041.916, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[1041.926, "o", "\r\n"]
[1041.936, "o", "    elif algorithm == \"omp\":\r\n"]
[1041.946, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[1041.956, "o", "            Gram=gram,\r\n"]
[1041.966, "o", "            Xy=cov,\r\n"]
[1041.976, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[1041.986, "o", "            tol=None,\r\n"]
[1041.996, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[1042.006, "o", "            copy_Xy=copy_cov,\r\n"]
[1042.016, "o", "        ).T\r\n"]
[1042.026, "o", "\r\n"]
[1042.036, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[1042.046, "o", "\r\n"]
[1042.056, "o", "\r\n"]
[1042.066, "o", "@validate_params(\r\n"]
[1042.076, "o", "    {\r\n"]
[1042.086, "o", "        \"X\": [\"array-like\"],\r\n"]
[1042.096, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[1042.106, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[1042.116, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[1042.126, "o", "        \"algorithm\": [\r\n"]
[1042.136, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[1042.146, "o", "        ],\r\n"]
[1042.156, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1042.166, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[1042.176, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[1042.186, "o", "        \"init\": [\"array-like\", None],\r\n"]
[1042.196, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1042.206, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[1042.216, "o", "        \"check_input\": [\"boolean\"],\r\n"]
[1042.226, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[1042.236, "o", "        \"positive\": [\"boolean\"],\r\n"]
[1042.246, "o", "    },\r\n"]
[1042.256, "o", "    prefer_skip_nested_validation=True,\r\n"]
[1042.266, "o", ")\r\n"]
[1042.276, "o", "# XXX : could be moved to the linear_model module\r\n"]
[1042.286, "o", "def sparse_encode(\r\n"]
[1042.296, "o", "    X,\r\n"]
[1042.306, "o", "    dictionary,\r\n"]
[1042.316, "o", "    *,\r\n"]
[1042.326, "o", "    gram=None,\r\n"]
[1042.336, "o", "    cov=None,\r\n"]
[1042.346, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1042.356, "o", "    n_nonzero_coefs=None,\r\n"]
[1042.366, "o", "    alpha=None,\r\n"]
[1042.376, "o", "    copy_cov=True,\r\n"]
[1042.386, "o", "    init=None,\r\n"]
[1042.396, "o", "    max_iter=1000,\r\n"]
[1042.406, "o", "    n_jobs=None,\r\n"]
[1042.416, "o", "    check_input=True,\r\n"]
[1042.426, "o", "    verbose=0,\r\n"]
[1042.436, "o", "    positive=False,\r\n"]
[1042.446, "o", "):\r\n"]
[1042.456, "o", "    \"\"\"Sparse coding.\r\n"]
[1042.466, "o", "\r\n"]
[1042.476, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[1042.486, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[1042.496, "o", "\r\n"]
[1042.506, "o", "        X ~= code * dictionary\r\n"]
[1042.516, "o", "\r\n"]
[1042.526, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[1042.536, "o", "\r\n"]
[1042.546, "o", "    Parameters\r\n"]
[1042.556, "o", "    ----------\r\n"]
[1042.566, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[1042.576, "o", "        Data matrix.\r\n"]
[1042.586, "o", "\r\n"]
[1042.596, "o", "    dictionary : array-like of shape (n_components, n_features)\r\n"]
[1042.606, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1042.616, "o", "        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[1042.626, "o", "        output.\r\n"]
[1042.636, "o", "\r\n"]
[1042.646, "o", "    gram : array-like of shape (n_components, n_components), default=None\r\n"]
[1042.656, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n"]
[1042.666, "o", "\r\n"]
[1042.676, "o", "    cov : array-like of shape (n_components, n_samples), default=None\r\n"]
[1042.686, "o", "        Precomputed covariance, `dictionary' * X`.\r\n"]
[1042.696, "o", "\r\n"]
[1042.706, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1042.716, "o", "            default='lasso_lars'\r\n"]
[1042.726, "o", "        The algorithm used:\r\n"]
[1042.736, "o", "\r\n"]
[1042.746, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[1042.756, "o", "          (`linear_model.lars_path`);\r\n"]
[1042.766, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1042.776, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1042.786, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1042.796, "o", "          the estimated components are sparse;\r\n"]
[1042.806, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1042.816, "o", "          solution;\r\n"]
[1042.826, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1042.836, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[1042.846, "o", "\r\n"]
[1042.856, "o", "    n_nonzero_coefs : int, default=None\r\n"]
[1042.866, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1042.876, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[1042.886, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[1042.896, "o", "        `n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1042.906, "o", "\r\n"]
[1042.916, "o", "    alpha : float, default=None\r\n"]
[1042.926, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1042.936, "o", "        penalty applied to the L1 norm.\r\n"]
[1042.946, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1042.956, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1042.966, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[1042.976, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[1042.986, "o", "        `n_nonzero_coefs`.\r\n"]
[1042.996, "o", "        If `None`, default to 1.\r\n"]
[1043.006, "o", "\r\n"]
[1043.016, "o", "    copy_cov : bool, default=True\r\n"]
[1043.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1043.036, "o", "        be overwritten.\r\n"]
[1043.046, "o", "\r\n"]
[1043.056, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1043.066, "o", "        Initialization value of the sparse codes. Only used if\r\n"]
[1043.076, "o", "        `algorithm='lasso_cd'`.\r\n"]
[1043.086, "o", "\r\n"]
[1043.096, "o", "    max_iter : int, default=1000\r\n"]
[1043.106, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1043.116, "o", "        `'lasso_lars'`.\r\n"]
[1043.126, "o", "\r\n"]
[1043.136, "o", "    n_jobs : int, default=None\r\n"]
[1043.146, "o", "        Number of parallel jobs to run.\r\n"]
[1043.156, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1043.166, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1043.176, "o", "        for more details.\r\n"]
[1043.186, "o", "\r\n"]
[1043.196, "o", "    check_input : bool, default=True\r\n"]
[1043.206, "o", "        If `False`, the input arrays X and dictionary will not be checked.\r\n"]
[1043.216, "o", "\r\n"]
[1043.226, "o", "    verbose : int, default=0\r\n"]
[1043.236, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[1043.246, "o", "\r\n"]
[1043.256, "o", "    positive : bool, default=False\r\n"]
[1043.266, "o", "        Whether to enforce positivity when finding the encoding.\r\n"]
[1043.276, "o", "\r\n"]
[1043.286, "o", "        .. versionadded:: 0.20\r\n"]
[1043.296, "o", "\r\n"]
[1043.306, "o", "    Returns\r\n"]
[1043.316, "o", "    -------\r\n"]
[1043.326, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1043.336, "o", "        The sparse codes.\r\n"]
[1043.346, "o", "\r\n"]
[1043.356, "o", "    See Also\r\n"]
[1043.366, "o", "    --------\r\n"]
[1043.376, "o", "    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n"]
[1043.386, "o", "        path using LARS algorithm.\r\n"]
[1043.396, "o", "    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n"]
[1043.406, "o", "    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n"]
[1043.416, "o", "    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n"]
[1043.426, "o", "        dictionary.\r\n"]
[1043.436, "o", "    \"\"\"\r\n"]
[1043.446, "o", "    if check_input:\r\n"]
[1043.456, "o", "        if algorithm == \"lasso_cd\":\r\n"]
[1043.466, "o", "            dictionary = check_array(\r\n"]
[1043.476, "o", "                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n"]
[1043.486, "o", "            )\r\n"]
[1043.496, "o", "            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n"]
[1043.506, "o", "        else:\r\n"]
[1043.516, "o", "            dictionary = check_array(dictionary)\r\n"]
[1043.526, "o", "            X = check_array(X)\r\n"]
[1043.536, "o", "\r\n"]
[1043.546, "o", "    if dictionary.shape[1] != X.shape[1]:\r\n"]
[1043.556, "o", "        raise ValueError(\r\n"]
[1043.566, "o", "            \"Dictionary and X have different numbers of features:\"\r\n"]
[1043.576, "o", "            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n"]
[1043.586, "o", "        )\r\n"]
[1043.596, "o", "\r\n"]
[1043.606, "o", "    _check_positive_coding(algorithm, positive)\r\n"]
[1043.616, "o", "\r\n"]
[1043.626, "o", "    return _sparse_encode(\r\n"]
[1043.636, "o", "        X,\r\n"]
[1043.646, "o", "        dictionary,\r\n"]
[1043.656, "o", "        gram=gram,\r\n"]
[1043.666, "o", "        cov=cov,\r\n"]
[1043.676, "o", "        algorithm=algorithm,\r\n"]
[1043.686, "o", "        n_nonzero_coefs=n_nonzero_coefs,\r\n"]
[1043.696, "o", "        alpha=alpha,\r\n"]
[1043.706, "o", "        copy_cov=copy_cov,\r\n"]
[1043.716, "o", "        init=init,\r\n"]
[1043.726, "o", "        max_iter=max_iter,\r\n"]
[1043.736, "o", "        n_jobs=n_jobs,\r\n"]
[1043.746, "o", "        verbose=verbose,\r\n"]
[1043.756, "o", "        positive=positive,\r\n"]
[1043.766, "o", "    )\r\n"]
[1043.776, "o", "\r\n"]
[1043.786, "o", "\r\n"]
[1043.796, "o", "def _sparse_encode(\r\n"]
[1043.806, "o", "    X,\r\n"]
[1043.816, "o", "    dictionary,\r\n"]
[1043.826, "o", "    *,\r\n"]
[1043.836, "o", "    gram=None,\r\n"]
[1043.846, "o", "    cov=None,\r\n"]
[1043.856, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1043.866, "o", "    n_nonzero_coefs=None,\r\n"]
[1043.876, "o", "    alpha=None,\r\n"]
[1043.886, "o", "    copy_cov=True,\r\n"]
[1043.896, "o", "    init=None,\r\n"]
[1043.906, "o", "    max_iter=1000,\r\n"]
[1043.916, "o", "    n_jobs=None,\r\n"]
[1043.926, "o", "    verbose=0,\r\n"]
[1043.936, "o", "    positive=False,\r\n"]
[1043.946, "o", "):\r\n"]
[1043.956, "o", "    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[1043.966, "o", "\r\n"]
[1043.976, "o", "    n_samples, n_features = X.shape\r\n"]
[1043.986, "o", "    n_components = dictionary.shape[0]\r\n"]
[1043.996, "o", "\r\n"]
[1044.006, "o", "    if algorithm in (\"lars\", \"omp\"):\r\n"]
[1044.016, "o", "        regularization = n_nonzero_coefs\r\n"]
[1044.026, "o", "        if regularization is None:\r\n"]
[1044.036, "o", "            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[1044.046, "o", "    else:\r\n"]
[1044.056, "o", "        regularization = alpha\r\n"]
[1044.066, "o", "        if regularization is None:\r\n"]
[1044.076, "o", "            regularization = 1.0\r\n"]
[1044.086, "o", "\r\n"]
[1044.096, "o", "    if gram is None and algorithm != \"threshold\":\r\n"]
[1044.106, "o", "        gram = np.dot(dictionary, dictionary.T)\r\n"]
[1044.116, "o", "\r\n"]
[1044.126, "o", "    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[1044.136, "o", "        copy_cov = False\r\n"]
[1044.146, "o", "        cov = np.dot(dictionary, X.T)\r\n"]
[1044.156, "o", "\r\n"]
[1044.166, "o", "    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[1044.176, "o", "        code = _sparse_encode_precomputed(\r\n"]
[1044.186, "o", "            X,\r\n"]
[1044.196, "o", "            dictionary,\r\n"]
[1044.206, "o", "            gram=gram,\r\n"]
[1044.216, "o", "            cov=cov,\r\n"]
[1044.226, "o", "            algorithm=algorithm,\r\n"]
[1044.236, "o", "            regularization=regularization,\r\n"]
[1044.246, "o", "            copy_cov=copy_cov,\r\n"]
[1044.256, "o", "            init=init,\r\n"]
[1044.266, "o", "            max_iter=max_iter,\r\n"]
[1044.276, "o", "            verbose=verbose,\r\n"]
[1044.286, "o", "            positive=positive,\r\n"]
[1044.296, "o", "        )\r\n"]
[1044.306, "o", "        return code\r\n"]
[1044.316, "o", "\r\n"]
[1044.326, "o", "    # Enter parallel code block\r\n"]
[1044.336, "o", "    n_samples = X.shape[0]\r\n"]
[1044.346, "o", "    n_components = dictionary.shape[0]\r\n"]
[1044.356, "o", "    code = np.empty((n_samples, n_components))\r\n"]
[1044.366, "o", "    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[1044.376, "o", "\r\n"]
[1044.386, "o", "    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[1044.396, "o", "        delayed(_sparse_encode_precomputed)(\r\n"]
[1044.406, "o", "            X[this_slice],\r\n"]
[1044.416, "o", "            dictionary,\r\n"]
[1044.426, "o", "            gram=gram,\r\n"]
[1044.436, "o", "            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[1044.446, "o", "            algorithm=algorithm,\r\n"]
[1044.456, "o", "            regularization=regularization,\r\n"]
[1044.466, "o", "            copy_cov=copy_cov,\r\n"]
[1044.476, "o", "            init=init[this_slice] if init is not None else None,\r\n"]
[1044.486, "o", "            max_iter=max_iter,\r\n"]
[1044.496, "o", "            verbose=verbose,\r\n"]
[1044.506, "o", "            positive=positive,\r\n"]
[1044.516, "o", "        )\r\n"]
[1044.526, "o", "        for this_slice in slices\r\n"]
[1044.536, "o", "    )\r\n"]
[1044.546, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[1044.556, "o", "        code[this_slice] = this_view\r\n"]
[1044.566, "o", "    return code\r\n"]
[1044.576, "o", "\r\n"]
[1044.586, "o", "\r\n"]
[1044.596, "o", "def _update_dict(\r\n"]
[1044.606, "o", "    dictionary,\r\n"]
[1044.616, "o", "    Y,\r\n"]
[1044.626, "o", "    code,\r\n"]
[1044.636, "o", "    A=None,\r\n"]
[1044.646, "o", "    B=None,\r\n"]
[1044.656, "o", "    verbose=False,\r\n"]
[1044.666, "o", "    random_state=None,\r\n"]
[1044.676, "o", "    positive=False,\r\n"]
[1044.686, "o", "):\r\n"]
[1044.696, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[1044.706, "o", "\r\n"]
[1044.716, "o", "    Parameters\r\n"]
[1044.726, "o", "    ----------\r\n"]
[1044.736, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1044.746, "o", "        Value of the dictionary at the previous iteration.\r\n"]
[1044.756, "o", "\r\n"]
[1044.766, "o", "    Y : ndarray of shape (n_samples, n_features)\r\n"]
[1044.776, "o", "        Data matrix.\r\n"]
[1044.786, "o", "\r\n"]
[1044.796, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1044.806, "o", "        Sparse coding of the data against which to optimize the dictionary.\r\n"]
[1044.816, "o", "\r\n"]
[1044.826, "o", "    A : ndarray of shape (n_components, n_components), default=None\r\n"]
[1044.836, "o", "        Together with `B`, sufficient stats of the online model to update the\r\n"]
[1044.846, "o", "        dictionary.\r\n"]
[1044.856, "o", "\r\n"]
[1044.866, "o", "    B : ndarray of shape (n_features, n_components), default=None\r\n"]
[1044.876, "o", "        Together with `A`, sufficient stats of the online model to update the\r\n"]
[1044.886, "o", "        dictionary.\r\n"]
[1044.896, "o", "\r\n"]
[1044.906, "o", "    verbose: bool, default=False\r\n"]
[1044.916, "o", "        Degree of output the procedure will print.\r\n"]
[1044.926, "o", "\r\n"]
[1044.936, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1045.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1045.002, "i", "sed -n '500,1200p' sklearn/decomposition/_dict_learning.py\r"]
[1045.004, "o", "sed -n '500,1200p' sklearn/decomposition/_dict_learning.py\r\n"]
[1045.016, "o", "\u001b[?2004l\r\n"]
[1045.026, "o", "        .. versionadded:: 0.20\r\n"]
[1045.036, "o", "    \"\"\"\r\n"]
[1045.046, "o", "    n_samples, n_components = code.shape\r\n"]
[1045.056, "o", "    random_state = check_random_state(random_state)\r\n"]
[1045.066, "o", "\r\n"]
[1045.076, "o", "    if A is None:\r\n"]
[1045.086, "o", "        A = code.T @ code\r\n"]
[1045.096, "o", "    if B is None:\r\n"]
[1045.106, "o", "        B = Y.T @ code\r\n"]
[1045.116, "o", "\r\n"]
[1045.126, "o", "    n_unused = 0\r\n"]
[1045.136, "o", "\r\n"]
[1045.146, "o", "    for k in range(n_components):\r\n"]
[1045.156, "o", "        if A[k, k] > 1e-6:\r\n"]
[1045.166, "o", "            # 1e-6 is arbitrary but consistent with the spams implementation\r\n"]
[1045.176, "o", "            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n"]
[1045.186, "o", "        else:\r\n"]
[1045.196, "o", "            # kth atom is almost never used -> sample a new one from the data\r\n"]
[1045.206, "o", "            newd = Y[random_state.choice(n_samples)]\r\n"]
[1045.216, "o", "\r\n"]
[1045.226, "o", "            # add small noise to avoid making the sparse coding ill conditioned\r\n"]
[1045.236, "o", "            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n"]
[1045.246, "o", "            noise = random_state.normal(0, noise_level, size=len(newd))\r\n"]
[1045.256, "o", "\r\n"]
[1045.266, "o", "            dictionary[k] = newd + noise\r\n"]
[1045.276, "o", "            code[:, k] = 0\r\n"]
[1045.286, "o", "            n_unused += 1\r\n"]
[1045.296, "o", "\r\n"]
[1045.306, "o", "        if positive:\r\n"]
[1045.316, "o", "            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n"]
[1045.326, "o", "\r\n"]
[1045.336, "o", "        # Projection on the constraint set ||V_k|| <= 1\r\n"]
[1045.346, "o", "        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n"]
[1045.356, "o", "\r\n"]
[1045.366, "o", "    if verbose and n_unused > 0:\r\n"]
[1045.376, "o", "        print(f\"{n_unused} unused atoms resampled.\")\r\n"]
[1045.386, "o", "\r\n"]
[1045.396, "o", "\r\n"]
[1045.406, "o", "def _dict_learning(\r\n"]
[1045.416, "o", "    X,\r\n"]
[1045.426, "o", "    n_components,\r\n"]
[1045.436, "o", "    *,\r\n"]
[1045.446, "o", "    alpha,\r\n"]
[1045.456, "o", "    max_iter,\r\n"]
[1045.466, "o", "    tol,\r\n"]
[1045.476, "o", "    method,\r\n"]
[1045.486, "o", "    n_jobs,\r\n"]
[1045.496, "o", "    dict_init,\r\n"]
[1045.506, "o", "    code_init,\r\n"]
[1045.516, "o", "    callback,\r\n"]
[1045.526, "o", "    verbose,\r\n"]
[1045.536, "o", "    random_state,\r\n"]
[1045.546, "o", "    return_n_iter,\r\n"]
[1045.556, "o", "    positive_dict,\r\n"]
[1045.566, "o", "    positive_code,\r\n"]
[1045.576, "o", "    method_max_iter,\r\n"]
[1045.586, "o", "):\r\n"]
[1045.596, "o", "    \"\"\"Main dictionary learning algorithm\"\"\"\r\n"]
[1045.606, "o", "    t0 = time.time()\r\n"]
[1045.616, "o", "    # Init the code and the dictionary with SVD of Y\r\n"]
[1045.626, "o", "    if code_init is not None and dict_init is not None:\r\n"]
[1045.636, "o", "        code = np.array(code_init, order=\"F\")\r\n"]
[1045.646, "o", "        # Don't copy V, it will happen below\r\n"]
[1045.656, "o", "        dictionary = dict_init\r\n"]
[1045.666, "o", "    else:\r\n"]
[1045.676, "o", "        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n"]
[1045.686, "o", "        # flip the initial code's sign to enforce deterministic output\r\n"]
[1045.696, "o", "        code, dictionary = svd_flip(code, dictionary)\r\n"]
[1045.706, "o", "        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[1045.716, "o", "    r = len(dictionary)\r\n"]
[1045.726, "o", "    if n_components <= r:  # True even if n_components=None\r\n"]
[1045.736, "o", "        code = code[:, :n_components]\r\n"]
[1045.746, "o", "        dictionary = dictionary[:n_components, :]\r\n"]
[1045.756, "o", "    else:\r\n"]
[1045.766, "o", "        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n"]
[1045.776, "o", "        dictionary = np.r_[\r\n"]
[1045.786, "o", "            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n"]
[1045.796, "o", "        ]\r\n"]
[1045.806, "o", "\r\n"]
[1045.816, "o", "    # Fortran-order dict better suited for the sparse coding which is the\r\n"]
[1045.826, "o", "    # bottleneck of this algorithm.\r\n"]
[1045.836, "o", "    dictionary = np.asfortranarray(dictionary)\r\n"]
[1045.846, "o", "\r\n"]
[1045.856, "o", "    errors = []\r\n"]
[1045.866, "o", "    current_cost = np.nan\r\n"]
[1045.876, "o", "\r\n"]
[1045.886, "o", "    if verbose == 1:\r\n"]
[1045.896, "o", "        print(\"[dict_learning]\", end=\" \")\r\n"]
[1045.906, "o", "\r\n"]
[1045.916, "o", "    # If max_iter is 0, number of iterations returned should be zero\r\n"]
[1045.926, "o", "    ii = -1\r\n"]
[1045.936, "o", "\r\n"]
[1045.946, "o", "    for ii in range(max_iter):\r\n"]
[1045.956, "o", "        dt = time.time() - t0\r\n"]
[1045.966, "o", "        if verbose == 1:\r\n"]
[1045.976, "o", "            sys.stdout.write(\".\")\r\n"]
[1045.986, "o", "            sys.stdout.flush()\r\n"]
[1045.996, "o", "        elif verbose:\r\n"]
[1046.006, "o", "            print(\r\n"]
[1046.016, "o", "                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n"]
[1046.026, "o", "                % (ii, dt, dt / 60, current_cost)\r\n"]
[1046.036, "o", "            )\r\n"]
[1046.046, "o", "\r\n"]
[1046.056, "o", "        # Update code\r\n"]
[1046.066, "o", "        code = sparse_encode(\r\n"]
[1046.076, "o", "            X,\r\n"]
[1046.086, "o", "            dictionary,\r\n"]
[1046.096, "o", "            algorithm=method,\r\n"]
[1046.106, "o", "            alpha=alpha,\r\n"]
[1046.116, "o", "            init=code,\r\n"]
[1046.126, "o", "            n_jobs=n_jobs,\r\n"]
[1046.136, "o", "            positive=positive_code,\r\n"]
[1046.146, "o", "            max_iter=method_max_iter,\r\n"]
[1046.156, "o", "            verbose=verbose,\r\n"]
[1046.166, "o", "        )\r\n"]
[1046.176, "o", "\r\n"]
[1046.186, "o", "        # Update dictionary in place\r\n"]
[1046.196, "o", "        _update_dict(\r\n"]
[1046.206, "o", "            dictionary,\r\n"]
[1046.216, "o", "            X,\r\n"]
[1046.226, "o", "            code,\r\n"]
[1046.236, "o", "            verbose=verbose,\r\n"]
[1046.246, "o", "            random_state=random_state,\r\n"]
[1046.256, "o", "            positive=positive_dict,\r\n"]
[1046.266, "o", "        )\r\n"]
[1046.276, "o", "\r\n"]
[1046.286, "o", "        # Cost function\r\n"]
[1046.296, "o", "        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n"]
[1046.306, "o", "            np.abs(code)\r\n"]
[1046.316, "o", "        )\r\n"]
[1046.326, "o", "        errors.append(current_cost)\r\n"]
[1046.336, "o", "\r\n"]
[1046.346, "o", "        if ii > 0:\r\n"]
[1046.356, "o", "            dE = errors[-2] - errors[-1]\r\n"]
[1046.366, "o", "            # assert(dE >= -tol * errors[-1])\r\n"]
[1046.376, "o", "            if dE < tol * errors[-1]:\r\n"]
[1046.386, "o", "                if verbose == 1:\r\n"]
[1046.396, "o", "                    # A line return\r\n"]
[1046.406, "o", "                    print(\"\")\r\n"]
[1046.416, "o", "                elif verbose:\r\n"]
[1046.426, "o", "                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n"]
[1046.436, "o", "                break\r\n"]
[1046.446, "o", "        if ii % 5 == 0 and callback is not None:\r\n"]
[1046.456, "o", "            callback(locals())\r\n"]
[1046.466, "o", "\r\n"]
[1046.476, "o", "    if return_n_iter:\r\n"]
[1046.486, "o", "        return code, dictionary, errors, ii + 1\r\n"]
[1046.496, "o", "    else:\r\n"]
[1046.506, "o", "        return code, dictionary, errors\r\n"]
[1046.516, "o", "\r\n"]
[1046.526, "o", "\r\n"]
[1046.536, "o", "def _check_warn_deprecated(param, name, default, additional_message=None):\r\n"]
[1046.546, "o", "    if param != \"deprecated\":\r\n"]
[1046.556, "o", "        msg = (\r\n"]
[1046.566, "o", "            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n"]
[1046.576, "o", "        )\r\n"]
[1046.586, "o", "        if additional_message:\r\n"]
[1046.596, "o", "            msg += f\" {additional_message}\"\r\n"]
[1046.606, "o", "        warnings.warn(msg, FutureWarning)\r\n"]
[1046.616, "o", "        return param\r\n"]
[1046.626, "o", "    else:\r\n"]
[1046.636, "o", "        return default\r\n"]
[1046.646, "o", "\r\n"]
[1046.656, "o", "\r\n"]
[1046.666, "o", "def dict_learning_online(\r\n"]
[1046.676, "o", "    X,\r\n"]
[1046.686, "o", "    n_components=2,\r\n"]
[1046.696, "o", "    *,\r\n"]
[1046.706, "o", "    alpha=1,\r\n"]
[1046.716, "o", "    n_iter=\"deprecated\",\r\n"]
[1046.726, "o", "    max_iter=None,\r\n"]
[1046.736, "o", "    return_code=True,\r\n"]
[1046.746, "o", "    dict_init=None,\r\n"]
[1046.756, "o", "    callback=None,\r\n"]
[1046.766, "o", "    batch_size=256,\r\n"]
[1046.776, "o", "    verbose=False,\r\n"]
[1046.786, "o", "    shuffle=True,\r\n"]
[1046.796, "o", "    n_jobs=None,\r\n"]
[1046.806, "o", "    method=\"lars\",\r\n"]
[1046.816, "o", "    iter_offset=\"deprecated\",\r\n"]
[1046.826, "o", "    random_state=None,\r\n"]
[1046.836, "o", "    return_inner_stats=\"deprecated\",\r\n"]
[1046.846, "o", "    inner_stats=\"deprecated\",\r\n"]
[1046.856, "o", "    return_n_iter=\"deprecated\",\r\n"]
[1046.866, "o", "    positive_dict=False,\r\n"]
[1046.876, "o", "    positive_code=False,\r\n"]
[1046.886, "o", "    method_max_iter=1000,\r\n"]
[1046.896, "o", "    tol=1e-3,\r\n"]
[1046.906, "o", "    max_no_improvement=10,\r\n"]
[1046.916, "o", "):\r\n"]
[1046.926, "o", "    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n"]
[1046.936, "o", "\r\n"]
[1046.946, "o", "    Finds the best dictionary and the corresponding sparse code for\r\n"]
[1046.956, "o", "    approximating the data matrix X by solving::\r\n"]
[1046.966, "o", "\r\n"]
[1046.976, "o", "        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[1046.986, "o", "                     (U,V)\r\n"]
[1046.996, "o", "                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n"]
[1047.006, "o", "\r\n"]
[1047.016, "o", "    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n"]
[1047.026, "o", "    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n"]
[1047.036, "o", "    which is the sum of the absolute values of all the entries in the matrix.\r\n"]
[1047.046, "o", "    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n"]
[1047.056, "o", "    the input data.\r\n"]
[1047.066, "o", "\r\n"]
[1047.076, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[1047.086, "o", "\r\n"]
[1047.096, "o", "    Parameters\r\n"]
[1047.106, "o", "    ----------\r\n"]
[1047.116, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[1047.126, "o", "        Data matrix.\r\n"]
[1047.136, "o", "\r\n"]
[1047.146, "o", "    n_components : int or None, default=2\r\n"]
[1047.156, "o", "        Number of dictionary atoms to extract. If None, then ``n_components``\r\n"]
[1047.166, "o", "        is set to ``n_features``.\r\n"]
[1047.176, "o", "\r\n"]
[1047.186, "o", "    alpha : float, default=1\r\n"]
[1047.196, "o", "        Sparsity controlling parameter.\r\n"]
[1047.206, "o", "\r\n"]
[1047.216, "o", "    n_iter : int, default=100\r\n"]
[1047.226, "o", "        Number of mini-batch iterations to perform.\r\n"]
[1047.236, "o", "\r\n"]
[1047.246, "o", "        .. deprecated:: 1.1\r\n"]
[1047.256, "o", "           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[1047.266, "o", "           `max_iter` instead.\r\n"]
[1047.276, "o", "\r\n"]
[1047.286, "o", "    max_iter : int, default=None\r\n"]
[1047.296, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[1047.306, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[1047.316, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[1047.326, "o", "\r\n"]
[1047.336, "o", "        .. versionadded:: 1.1\r\n"]
[1047.346, "o", "\r\n"]
[1047.356, "o", "    return_code : bool, default=True\r\n"]
[1047.366, "o", "        Whether to also return the code U or just the dictionary `V`.\r\n"]
[1047.376, "o", "\r\n"]
[1047.386, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1047.396, "o", "        Initial values for the dictionary for warm restart scenarios.\r\n"]
[1047.406, "o", "        If `None`, the initial values for the dictionary are created\r\n"]
[1047.416, "o", "        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n"]
[1047.426, "o", "\r\n"]
[1047.436, "o", "    callback : callable, default=None\r\n"]
[1047.446, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[1047.456, "o", "\r\n"]
[1047.466, "o", "    batch_size : int, default=256\r\n"]
[1047.476, "o", "        The number of samples to take in each batch.\r\n"]
[1047.486, "o", "\r\n"]
[1047.496, "o", "        .. versionchanged:: 1.3\r\n"]
[1047.506, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[1047.516, "o", "\r\n"]
[1047.526, "o", "    verbose : bool, default=False\r\n"]
[1047.536, "o", "        To control the verbosity of the procedure.\r\n"]
[1047.546, "o", "\r\n"]
[1047.556, "o", "    shuffle : bool, default=True\r\n"]
[1047.566, "o", "        Whether to shuffle the data before splitting it in batches.\r\n"]
[1047.576, "o", "\r\n"]
[1047.586, "o", "    n_jobs : int, default=None\r\n"]
[1047.596, "o", "        Number of parallel jobs to run.\r\n"]
[1047.606, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1047.616, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1047.626, "o", "        for more details.\r\n"]
[1047.636, "o", "\r\n"]
[1047.646, "o", "    method : {'lars', 'cd'}, default='lars'\r\n"]
[1047.656, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[1047.666, "o", "          problem (`linear_model.lars_path`);\r\n"]
[1047.676, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[1047.686, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[1047.696, "o", "          the estimated components are sparse.\r\n"]
[1047.706, "o", "\r\n"]
[1047.716, "o", "    iter_offset : int, default=0\r\n"]
[1047.726, "o", "        Number of previous iterations completed on the dictionary used for\r\n"]
[1047.736, "o", "        initialization.\r\n"]
[1047.746, "o", "\r\n"]
[1047.756, "o", "        .. deprecated:: 1.1\r\n"]
[1047.766, "o", "           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n"]
[1047.776, "o", "\r\n"]
[1047.786, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1047.796, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[1047.806, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[1047.816, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[1047.826, "o", "        results across multiple function calls.\r\n"]
[1047.836, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1047.846, "o", "\r\n"]
[1047.856, "o", "    return_inner_stats : bool, default=False\r\n"]
[1047.866, "o", "        Return the inner statistics A (dictionary covariance) and B\r\n"]
[1047.876, "o", "        (data approximation). Useful to restart the algorithm in an\r\n"]
[1047.886, "o", "        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n"]
[1047.896, "o", "        ignored.\r\n"]
[1047.906, "o", "\r\n"]
[1047.916, "o", "        .. deprecated:: 1.1\r\n"]
[1047.926, "o", "           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[1047.936, "o", "\r\n"]
[1047.946, "o", "    inner_stats : tuple of (A, B) ndarrays, default=None\r\n"]
[1047.956, "o", "        Inner sufficient statistics that are kept by the algorithm.\r\n"]
[1047.966, "o", "        Passing them at initialization is useful in online settings, to\r\n"]
[1047.976, "o", "        avoid losing the history of the evolution.\r\n"]
[1047.986, "o", "        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n"]
[1047.996, "o", "        `B` `(n_features, n_components)` is the data approximation matrix.\r\n"]
[1048.006, "o", "\r\n"]
[1048.016, "o", "        .. deprecated:: 1.1\r\n"]
[1048.026, "o", "           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[1048.036, "o", "\r\n"]
[1048.046, "o", "    return_n_iter : bool, default=False\r\n"]
[1048.056, "o", "        Whether or not to return the number of iterations.\r\n"]
[1048.066, "o", "\r\n"]
[1048.076, "o", "        .. deprecated:: 1.1\r\n"]
[1048.086, "o", "           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n"]
[1048.096, "o", "\r\n"]
[1048.106, "o", "    positive_dict : bool, default=False\r\n"]
[1048.116, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1048.126, "o", "\r\n"]
[1048.136, "o", "        .. versionadded:: 0.20\r\n"]
[1048.146, "o", "\r\n"]
[1048.156, "o", "    positive_code : bool, default=False\r\n"]
[1048.166, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1048.176, "o", "\r\n"]
[1048.186, "o", "        .. versionadded:: 0.20\r\n"]
[1048.196, "o", "\r\n"]
[1048.206, "o", "    method_max_iter : int, default=1000\r\n"]
[1048.216, "o", "        Maximum number of iterations to perform when solving the lasso problem.\r\n"]
[1048.226, "o", "\r\n"]
[1048.236, "o", "        .. versionadded:: 0.22\r\n"]
[1048.246, "o", "\r\n"]
[1048.256, "o", "    tol : float, default=1e-3\r\n"]
[1048.266, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[1048.276, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[1048.286, "o", "\r\n"]
[1048.296, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[1048.306, "o", "        `tol` to 0.0.\r\n"]
[1048.316, "o", "\r\n"]
[1048.326, "o", "        .. versionadded:: 1.1\r\n"]
[1048.336, "o", "\r\n"]
[1048.346, "o", "    max_no_improvement : int, default=10\r\n"]
[1048.356, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[1048.366, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[1048.376, "o", "        `max_iter` is not None.\r\n"]
[1048.386, "o", "\r\n"]
[1048.396, "o", "        To disable convergence detection based on cost function, set\r\n"]
[1048.406, "o", "        `max_no_improvement` to None.\r\n"]
[1048.416, "o", "\r\n"]
[1048.426, "o", "        .. versionadded:: 1.1\r\n"]
[1048.436, "o", "\r\n"]
[1048.446, "o", "    Returns\r\n"]
[1048.456, "o", "    -------\r\n"]
[1048.466, "o", "    code : ndarray of shape (n_samples, n_components),\r\n"]
[1048.476, "o", "        The sparse code (only returned if `return_code=True`).\r\n"]
[1048.486, "o", "\r\n"]
[1048.496, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[1048.506, "o", "        The solutions to the dictionary learning problem.\r\n"]
[1048.516, "o", "\r\n"]
[1048.526, "o", "    n_iter : int\r\n"]
[1048.536, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[1048.546, "o", "        set to `True`.\r\n"]
[1048.556, "o", "\r\n"]
[1048.566, "o", "    See Also\r\n"]
[1048.576, "o", "    --------\r\n"]
[1048.586, "o", "    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n"]
[1048.596, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1048.606, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n"]
[1048.616, "o", "        learning algorithm.\r\n"]
[1048.626, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1048.636, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1048.646, "o", "    \"\"\"\r\n"]
[1048.656, "o", "    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n"]
[1048.666, "o", "    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n"]
[1048.676, "o", "        raise ValueError(\r\n"]
[1048.686, "o", "            \"The following arguments are incompatible with 'max_iter': \"\r\n"]
[1048.696, "o", "            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n"]
[1048.706, "o", "        )\r\n"]
[1048.716, "o", "\r\n"]
[1048.726, "o", "    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n"]
[1048.736, "o", "    return_inner_stats = _check_warn_deprecated(\r\n"]
[1048.746, "o", "        return_inner_stats,\r\n"]
[1048.756, "o", "        \"return_inner_stats\",\r\n"]
[1048.766, "o", "        default=False,\r\n"]
[1048.776, "o", "        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n"]
[1048.786, "o", "    )\r\n"]
[1048.796, "o", "    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n"]
[1048.806, "o", "    return_n_iter = _check_warn_deprecated(\r\n"]
[1048.816, "o", "        return_n_iter,\r\n"]
[1048.826, "o", "        \"return_n_iter\",\r\n"]
[1048.836, "o", "        default=False,\r\n"]
[1048.846, "o", "        additional_message=(\r\n"]
[1048.856, "o", "            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n"]
[1048.866, "o", "            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n"]
[1048.876, "o", "        ),\r\n"]
[1048.886, "o", "    )\r\n"]
[1048.896, "o", "\r\n"]
[1048.906, "o", "    if max_iter is not None:\r\n"]
[1048.916, "o", "        transform_algorithm = \"lasso_\" + method\r\n"]
[1048.926, "o", "\r\n"]
[1048.936, "o", "        est = MiniBatchDictionaryLearning(\r\n"]
[1048.946, "o", "            n_components=n_components,\r\n"]
[1048.956, "o", "            alpha=alpha,\r\n"]
[1048.966, "o", "            n_iter=n_iter,\r\n"]
[1048.976, "o", "            n_jobs=n_jobs,\r\n"]
[1048.986, "o", "            fit_algorithm=method,\r\n"]
[1048.996, "o", "            batch_size=batch_size,\r\n"]
[1049.006, "o", "            shuffle=shuffle,\r\n"]
[1049.016, "o", "            dict_init=dict_init,\r\n"]
[1049.026, "o", "            random_state=random_state,\r\n"]
[1049.036, "o", "            transform_algorithm=transform_algorithm,\r\n"]
[1049.046, "o", "            transform_alpha=alpha,\r\n"]
[1049.056, "o", "            positive_code=positive_code,\r\n"]
[1049.066, "o", "            positive_dict=positive_dict,\r\n"]
[1049.076, "o", "            transform_max_iter=method_max_iter,\r\n"]
[1049.086, "o", "            verbose=verbose,\r\n"]
[1049.096, "o", "            callback=callback,\r\n"]
[1049.106, "o", "            tol=tol,\r\n"]
[1049.116, "o", "            max_no_improvement=max_no_improvement,\r\n"]
[1049.126, "o", "        ).fit(X)\r\n"]
[1049.136, "o", "\r\n"]
[1049.146, "o", "        if not return_code:\r\n"]
[1049.156, "o", "            return est.components_\r\n"]
[1049.166, "o", "        else:\r\n"]
[1049.176, "o", "            code = est.transform(X)\r\n"]
[1049.186, "o", "            return code, est.components_\r\n"]
[1049.196, "o", "\r\n"]
[1049.206, "o", "    # TODO(1.4) remove the whole old behavior\r\n"]
[1049.216, "o", "    # Fallback to old behavior\r\n"]
[1049.226, "o", "\r\n"]
[1049.236, "o", "    n_iter = _check_warn_deprecated(\r\n"]
[1049.246, "o", "        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n"]
[1049.256, "o", "    )\r\n"]
[1049.266, "o", "\r\n"]
[1049.276, "o", "    if n_components is None:\r\n"]
[1049.286, "o", "        n_components = X.shape[1]\r\n"]
[1049.296, "o", "\r\n"]
[1049.306, "o", "    if method not in (\"lars\", \"cd\"):\r\n"]
[1049.316, "o", "        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n"]
[1049.326, "o", "\r\n"]
[1049.336, "o", "    _check_positive_coding(method, positive_code)\r\n"]
[1049.346, "o", "\r\n"]
[1049.356, "o", "    method = \"lasso_\" + method\r\n"]
[1049.366, "o", "\r\n"]
[1049.376, "o", "    t0 = time.time()\r\n"]
[1049.386, "o", "    n_samples, n_features = X.shape\r\n"]
[1049.396, "o", "    # Avoid integer division problems\r\n"]
[1049.406, "o", "    alpha = float(alpha)\r\n"]
[1049.416, "o", "    random_state = check_random_state(random_state)\r\n"]
[1049.426, "o", "\r\n"]
[1049.436, "o", "    # Init V with SVD of X\r\n"]
[1049.446, "o", "    if dict_init is not None:\r\n"]
[1049.456, "o", "        dictionary = dict_init\r\n"]
[1049.466, "o", "    else:\r\n"]
[1049.476, "o", "        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n"]
[1049.486, "o", "        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[1049.496, "o", "    r = len(dictionary)\r\n"]
[1049.506, "o", "    if n_components <= r:\r\n"]
[1049.516, "o", "        dictionary = dictionary[:n_components, :]\r\n"]
[1049.526, "o", "    else:\r\n"]
[1049.536, "o", "        dictionary = np.r_[\r\n"]
[1049.546, "o", "            dictionary,\r\n"]
[1049.556, "o", "            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n"]
[1049.566, "o", "        ]\r\n"]
[1049.576, "o", "\r\n"]
[1049.586, "o", "    if verbose == 1:\r\n"]
[1049.596, "o", "        print(\"[dict_learning]\", end=\" \")\r\n"]
[1049.606, "o", "\r\n"]
[1049.616, "o", "    if shuffle:\r\n"]
[1049.626, "o", "        X_train = X.copy()\r\n"]
[1049.636, "o", "        random_state.shuffle(X_train)\r\n"]
[1049.646, "o", "    else:\r\n"]
[1049.656, "o", "        X_train = X\r\n"]
[1049.666, "o", "\r\n"]
[1049.676, "o", "    X_train = check_array(\r\n"]
[1049.686, "o", "        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n"]
[1049.696, "o", "    )\r\n"]
[1049.706, "o", "\r\n"]
[1049.716, "o", "    # Fortran-order dict better suited for the sparse coding which is the\r\n"]
[1049.726, "o", "    # bottleneck of this algorithm.\r\n"]
[1049.736, "o", "    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n"]
[1049.746, "o", "    dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[1049.756, "o", "\r\n"]
[1049.766, "o", "    batches = gen_batches(n_samples, batch_size)\r\n"]
[1049.776, "o", "    batches = itertools.cycle(batches)\r\n"]
[1049.786, "o", "\r\n"]
[1049.796, "o", "    # The covariance of the dictionary\r\n"]
[1049.806, "o", "    if inner_stats is None:\r\n"]
[1049.816, "o", "        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n"]
[1049.826, "o", "        # The data approximation\r\n"]
[1049.836, "o", "        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n"]
[1049.846, "o", "    else:\r\n"]
[1049.856, "o", "        A = inner_stats[0].copy()\r\n"]
[1049.866, "o", "        B = inner_stats[1].copy()\r\n"]
[1049.876, "o", "\r\n"]
[1049.886, "o", "    # If n_iter is zero, we need to return zero.\r\n"]
[1049.896, "o", "    ii = iter_offset - 1\r\n"]
[1049.906, "o", "\r\n"]
[1049.916, "o", "    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n"]
[1049.926, "o", "        this_X = X_train[batch]\r\n"]
[1049.936, "o", "        dt = time.time() - t0\r\n"]
[1050.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1050.002, "i", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r"]
[1050.004, "o", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r\n"]
[1050.016, "o", "\u001b[?2004l\r\n"]
[1050.026, "o", "    n_iter : int\r\n"]
[1050.036, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[1050.046, "o", "        set to True.\r\n"]
[1050.056, "o", "\r\n"]
[1050.066, "o", "    See Also\r\n"]
[1050.076, "o", "    --------\r\n"]
[1050.086, "o", "    dict_learning_online : Solve a dictionary learning matrix factorization\r\n"]
[1050.096, "o", "        problem online.\r\n"]
[1050.106, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1050.116, "o", "    MiniBatchDictionaryLearning : A faster, less accurate version\r\n"]
[1050.126, "o", "        of the dictionary learning algorithm.\r\n"]
[1050.136, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1050.146, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1050.156, "o", "    \"\"\"\r\n"]
[1050.166, "o", "    estimator = DictionaryLearning(\r\n"]
[1050.176, "o", "        n_components=n_components,\r\n"]
[1050.186, "o", "        alpha=alpha,\r\n"]
[1050.196, "o", "        max_iter=max_iter,\r\n"]
[1050.206, "o", "        tol=tol,\r\n"]
[1050.216, "o", "        fit_algorithm=method,\r\n"]
[1050.226, "o", "        n_jobs=n_jobs,\r\n"]
[1050.236, "o", "        dict_init=dict_init,\r\n"]
[1050.246, "o", "        callback=callback,\r\n"]
[1050.256, "o", "        code_init=code_init,\r\n"]
[1050.266, "o", "        verbose=verbose,\r\n"]
[1050.276, "o", "        random_state=random_state,\r\n"]
[1050.286, "o", "        positive_code=positive_code,\r\n"]
[1050.296, "o", "        positive_dict=positive_dict,\r\n"]
[1050.306, "o", "        transform_max_iter=method_max_iter,\r\n"]
[1050.316, "o", "    )\r\n"]
[1050.326, "o", "    code = estimator.fit_transform(X)\r\n"]
[1050.336, "o", "    if return_n_iter:\r\n"]
[1050.346, "o", "        return (\r\n"]
[1050.356, "o", "            code,\r\n"]
[1050.366, "o", "            estimator.components_,\r\n"]
[1050.376, "o", "            estimator.error_,\r\n"]
[1050.386, "o", "            estimator.n_iter_,\r\n"]
[1050.396, "o", "        )\r\n"]
[1050.406, "o", "    return code, estimator.components_, estimator.error_\r\n"]
[1050.416, "o", "\r\n"]
[1050.426, "o", "\r\n"]
[1050.436, "o", "class _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n"]
[1050.446, "o", "    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n"]
[1050.456, "o", "\r\n"]
[1050.466, "o", "    def __init__(\r\n"]
[1050.476, "o", "        self,\r\n"]
[1050.486, "o", "        transform_algorithm,\r\n"]
[1050.496, "o", "        transform_n_nonzero_coefs,\r\n"]
[1050.506, "o", "        transform_alpha,\r\n"]
[1050.516, "o", "        split_sign,\r\n"]
[1050.526, "o", "        n_jobs,\r\n"]
[1050.536, "o", "        positive_code,\r\n"]
[1050.546, "o", "        transform_max_iter,\r\n"]
[1050.556, "o", "    ):\r\n"]
[1050.566, "o", "        self.transform_algorithm = transform_algorithm\r\n"]
[1050.576, "o", "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n"]
[1050.586, "o", "        self.transform_alpha = transform_alpha\r\n"]
[1050.596, "o", "        self.transform_max_iter = transform_max_iter\r\n"]
[1050.606, "o", "        self.split_sign = split_sign\r\n"]
[1050.616, "o", "        self.n_jobs = n_jobs\r\n"]
[1050.626, "o", "        self.positive_code = positive_code\r\n"]
[1050.636, "o", "\r\n"]
[1050.646, "o", "    def _transform(self, X, dictionary):\r\n"]
[1050.656, "o", "        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n"]
[1050.666, "o", "        SparseCoder.\"\"\"\r\n"]
[1050.676, "o", "        X = self._validate_data(X, reset=False)\r\n"]
[1050.686, "o", "\r\n"]
[1050.696, "o", "        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n"]
[1050.706, "o", "            transform_alpha = self.alpha\r\n"]
[1050.716, "o", "        else:\r\n"]
[1050.726, "o", "            transform_alpha = self.transform_alpha\r\n"]
[1050.736, "o", "\r\n"]
[1050.746, "o", "        code = sparse_encode(\r\n"]
[1050.756, "o", "            X,\r\n"]
[1050.766, "o", "            dictionary,\r\n"]
[1050.776, "o", "            algorithm=self.transform_algorithm,\r\n"]
[1050.786, "o", "            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n"]
[1050.796, "o", "            alpha=transform_alpha,\r\n"]
[1050.806, "o", "            max_iter=self.transform_max_iter,\r\n"]
[1050.816, "o", "            n_jobs=self.n_jobs,\r\n"]
[1050.826, "o", "            positive=self.positive_code,\r\n"]
[1050.836, "o", "        )\r\n"]
[1050.846, "o", "\r\n"]
[1050.856, "o", "        if self.split_sign:\r\n"]
[1050.866, "o", "            # feature vector is split into a positive and negative side\r\n"]
[1050.876, "o", "            n_samples, n_features = code.shape\r\n"]
[1050.886, "o", "            split_code = np.empty((n_samples, 2 * n_features))\r\n"]
[1050.896, "o", "            split_code[:, :n_features] = np.maximum(code, 0)\r\n"]
[1050.906, "o", "            split_code[:, n_features:] = -np.minimum(code, 0)\r\n"]
[1050.916, "o", "            code = split_code\r\n"]
[1050.926, "o", "\r\n"]
[1050.936, "o", "        return code\r\n"]
[1050.946, "o", "\r\n"]
[1050.956, "o", "    def transform(self, X):\r\n"]
[1050.966, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[1050.976, "o", "\r\n"]
[1050.986, "o", "        Coding method is determined by the object parameter\r\n"]
[1050.996, "o", "        `transform_algorithm`.\r\n"]
[1051.006, "o", "\r\n"]
[1051.016, "o", "        Parameters\r\n"]
[1051.026, "o", "        ----------\r\n"]
[1051.036, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[1051.046, "o", "            Test data to be transformed, must have the same number of\r\n"]
[1051.056, "o", "            features as the data used to train the model.\r\n"]
[1051.066, "o", "\r\n"]
[1051.076, "o", "        Returns\r\n"]
[1051.086, "o", "        -------\r\n"]
[1051.096, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[1051.106, "o", "            Transformed data.\r\n"]
[1051.116, "o", "        \"\"\"\r\n"]
[1051.126, "o", "        check_is_fitted(self)\r\n"]
[1051.136, "o", "        return self._transform(X, self.components_)\r\n"]
[1051.146, "o", "\r\n"]
[1051.156, "o", "\r\n"]
[1051.166, "o", "class SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n"]
[1051.176, "o", "    \"\"\"Sparse coding.\r\n"]
[1051.186, "o", "\r\n"]
[1051.196, "o", "    Finds a sparse representation of data against a fixed, precomputed\r\n"]
[1051.206, "o", "    dictionary.\r\n"]
[1051.216, "o", "\r\n"]
[1051.226, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[1051.236, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[1051.246, "o", "\r\n"]
[1051.256, "o", "        X ~= code * dictionary\r\n"]
[1051.266, "o", "\r\n"]
[1051.276, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[1051.286, "o", "\r\n"]
[1051.296, "o", "    Parameters\r\n"]
[1051.306, "o", "    ----------\r\n"]
[1051.316, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1051.326, "o", "        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n"]
[1051.336, "o", "        normalized to unit norm.\r\n"]
[1051.346, "o", "\r\n"]
[1051.356, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[1051.366, "o", "            'threshold'}, default='omp'\r\n"]
[1051.376, "o", "        Algorithm used to transform the data:\r\n"]
[1051.386, "o", "\r\n"]
[1051.396, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[1051.406, "o", "          (`linear_model.lars_path`);\r\n"]
[1051.416, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1051.426, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1051.436, "o", "          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n"]
[1051.446, "o", "          the estimated components are sparse;\r\n"]
[1051.456, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1051.466, "o", "          solution;\r\n"]
[1051.476, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[1051.486, "o", "          the projection ``dictionary * X'``.\r\n"]
[1051.496, "o", "\r\n"]
[1051.506, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[1051.516, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1051.526, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[1051.536, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[1051.546, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1051.556, "o", "\r\n"]
[1051.566, "o", "    transform_alpha : float, default=None\r\n"]
[1051.576, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1051.586, "o", "        penalty applied to the L1 norm.\r\n"]
[1051.596, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1051.606, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1051.616, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[1051.626, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[1051.636, "o", "        `n_nonzero_coefs`.\r\n"]
[1051.646, "o", "        If `None`, default to 1.\r\n"]
[1051.656, "o", "\r\n"]
[1051.666, "o", "    split_sign : bool, default=False\r\n"]
[1051.676, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[1051.686, "o", "        its negative part and its positive part. This can improve the\r\n"]
[1051.696, "o", "        performance of downstream classifiers.\r\n"]
[1051.706, "o", "\r\n"]
[1051.716, "o", "    n_jobs : int, default=None\r\n"]
[1051.726, "o", "        Number of parallel jobs to run.\r\n"]
[1051.736, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1051.746, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1051.756, "o", "        for more details.\r\n"]
[1051.766, "o", "\r\n"]
[1051.776, "o", "    positive_code : bool, default=False\r\n"]
[1051.786, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1051.796, "o", "\r\n"]
[1051.806, "o", "        .. versionadded:: 0.20\r\n"]
[1051.816, "o", "\r\n"]
[1051.826, "o", "    transform_max_iter : int, default=1000\r\n"]
[1051.836, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1051.846, "o", "        `lasso_lars`.\r\n"]
[1051.856, "o", "\r\n"]
[1051.866, "o", "        .. versionadded:: 0.22\r\n"]
[1051.876, "o", "\r\n"]
[1051.886, "o", "    Attributes\r\n"]
[1051.896, "o", "    ----------\r\n"]
[1051.906, "o", "    n_components_ : int\r\n"]
[1051.916, "o", "        Number of atoms.\r\n"]
[1051.926, "o", "\r\n"]
[1051.936, "o", "    n_features_in_ : int\r\n"]
[1051.946, "o", "        Number of features seen during :term:`fit`.\r\n"]
[1051.956, "o", "\r\n"]
[1051.966, "o", "        .. versionadded:: 0.24\r\n"]
[1051.976, "o", "\r\n"]
[1051.986, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[1051.996, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[1052.006, "o", "        has feature names that are all strings.\r\n"]
[1052.016, "o", "\r\n"]
[1052.026, "o", "        .. versionadded:: 1.0\r\n"]
[1052.036, "o", "\r\n"]
[1052.046, "o", "    See Also\r\n"]
[1052.056, "o", "    --------\r\n"]
[1052.066, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1052.076, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n"]
[1052.086, "o", "        dictionary learning algorithm.\r\n"]
[1052.096, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1052.106, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1052.116, "o", "    sparse_encode : Sparse coding where each row of the result is the solution\r\n"]
[1052.126, "o", "        to a sparse coding problem.\r\n"]
[1052.136, "o", "\r\n"]
[1052.146, "o", "    Examples\r\n"]
[1052.156, "o", "    --------\r\n"]
[1052.166, "o", "    >>> import numpy as np\r\n"]
[1052.176, "o", "    >>> from sklearn.decomposition import SparseCoder\r\n"]
[1052.186, "o", "    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n"]
[1052.196, "o", "    >>> dictionary = np.array(\r\n"]
[1052.206, "o", "    ...     [[0, 1, 0],\r\n"]
[1052.216, "o", "    ...      [-1, -1, 2],\r\n"]
[1052.226, "o", "    ...      [1, 1, 1],\r\n"]
[1052.236, "o", "    ...      [0, 1, 1],\r\n"]
[1052.246, "o", "    ...      [0, 2, 1]],\r\n"]
[1052.256, "o", "    ...    dtype=np.float64\r\n"]
[1052.266, "o", "    ... )\r\n"]
[1052.276, "o", "    >>> coder = SparseCoder(\r\n"]
[1052.286, "o", "    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n"]
[1052.296, "o", "    ...     transform_alpha=1e-10,\r\n"]
[1052.306, "o", "    ... )\r\n"]
[1052.316, "o", "    >>> coder.transform(X)\r\n"]
[1052.326, "o", "    array([[ 0.,  0., -1.,  0.,  0.],\r\n"]
[1052.336, "o", "           [ 0.,  1.,  1.,  0.,  0.]])\r\n"]
[1052.346, "o", "    \"\"\"\r\n"]
[1052.356, "o", "\r\n"]
[1052.366, "o", "    _required_parameters = [\"dictionary\"]\r\n"]
[1052.376, "o", "\r\n"]
[1052.386, "o", "    def __init__(\r\n"]
[1052.396, "o", "        self,\r\n"]
[1052.406, "o", "        dictionary,\r\n"]
[1052.416, "o", "        *,\r\n"]
[1052.426, "o", "        transform_algorithm=\"omp\",\r\n"]
[1052.436, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[1052.446, "o", "        transform_alpha=None,\r\n"]
[1052.456, "o", "        split_sign=False,\r\n"]
[1052.466, "o", "        n_jobs=None,\r\n"]
[1052.476, "o", "        positive_code=False,\r\n"]
[1052.486, "o", "        transform_max_iter=1000,\r\n"]
[1052.496, "o", "    ):\r\n"]
[1052.506, "o", "        super().__init__(\r\n"]
[1052.516, "o", "            transform_algorithm,\r\n"]
[1052.526, "o", "            transform_n_nonzero_coefs,\r\n"]
[1052.536, "o", "            transform_alpha,\r\n"]
[1052.546, "o", "            split_sign,\r\n"]
[1052.556, "o", "            n_jobs,\r\n"]
[1052.566, "o", "            positive_code,\r\n"]
[1052.576, "o", "            transform_max_iter,\r\n"]
[1052.586, "o", "        )\r\n"]
[1052.596, "o", "        self.dictionary = dictionary\r\n"]
[1052.606, "o", "\r\n"]
[1052.616, "o", "    def fit(self, X, y=None):\r\n"]
[1052.626, "o", "        \"\"\"Do nothing and return the estimator unchanged.\r\n"]
[1052.636, "o", "\r\n"]
[1052.646, "o", "        This method is just there to implement the usual API and hence\r\n"]
[1052.656, "o", "        work in pipelines.\r\n"]
[1052.666, "o", "\r\n"]
[1052.676, "o", "        Parameters\r\n"]
[1052.686, "o", "        ----------\r\n"]
[1052.696, "o", "        X : Ignored\r\n"]
[1052.706, "o", "            Not used, present for API consistency by convention.\r\n"]
[1052.716, "o", "\r\n"]
[1052.726, "o", "        y : Ignored\r\n"]
[1052.736, "o", "            Not used, present for API consistency by convention.\r\n"]
[1052.746, "o", "\r\n"]
[1052.756, "o", "        Returns\r\n"]
[1052.766, "o", "        -------\r\n"]
[1052.776, "o", "        self : object\r\n"]
[1052.786, "o", "            Returns the instance itself.\r\n"]
[1052.796, "o", "        \"\"\"\r\n"]
[1052.806, "o", "        return self\r\n"]
[1052.816, "o", "\r\n"]
[1052.826, "o", "    def transform(self, X, y=None):\r\n"]
[1052.836, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[1052.846, "o", "\r\n"]
[1052.856, "o", "        Coding method is determined by the object parameter\r\n"]
[1052.866, "o", "        `transform_algorithm`.\r\n"]
[1052.876, "o", "\r\n"]
[1052.886, "o", "        Parameters\r\n"]
[1052.896, "o", "        ----------\r\n"]
[1052.906, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[1052.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1052.926, "o", "            and `n_features` is the number of features.\r\n"]
[1052.936, "o", "\r\n"]
[1052.946, "o", "        y : Ignored\r\n"]
[1052.956, "o", "            Not used, present for API consistency by convention.\r\n"]
[1052.966, "o", "\r\n"]
[1052.976, "o", "        Returns\r\n"]
[1052.986, "o", "        -------\r\n"]
[1052.996, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[1053.006, "o", "            Transformed data.\r\n"]
[1053.016, "o", "        \"\"\"\r\n"]
[1053.026, "o", "        return super()._transform(X, self.dictionary)\r\n"]
[1053.036, "o", "\r\n"]
[1053.046, "o", "    def _more_tags(self):\r\n"]
[1053.056, "o", "        return {\r\n"]
[1053.066, "o", "            \"requires_fit\": False,\r\n"]
[1053.076, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[1053.086, "o", "        }\r\n"]
[1053.096, "o", "\r\n"]
[1053.106, "o", "    @property\r\n"]
[1053.116, "o", "    def n_components_(self):\r\n"]
[1053.126, "o", "        \"\"\"Number of atoms.\"\"\"\r\n"]
[1053.136, "o", "        return self.dictionary.shape[0]\r\n"]
[1053.146, "o", "\r\n"]
[1053.156, "o", "    @property\r\n"]
[1053.166, "o", "    def n_features_in_(self):\r\n"]
[1053.176, "o", "        \"\"\"Number of features seen during `fit`.\"\"\"\r\n"]
[1053.186, "o", "        return self.dictionary.shape[1]\r\n"]
[1053.196, "o", "\r\n"]
[1053.206, "o", "    @property\r\n"]
[1053.216, "o", "    def _n_features_out(self):\r\n"]
[1053.226, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[1053.236, "o", "        return self.n_components_\r\n"]
[1053.246, "o", "\r\n"]
[1053.256, "o", "\r\n"]
[1053.266, "o", "class DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[1053.276, "o", "    \"\"\"Dictionary learning.\r\n"]
[1053.286, "o", "\r\n"]
[1053.296, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[1053.306, "o", "    encoding the fitted data.\r\n"]
[1053.316, "o", "\r\n"]
[1053.326, "o", "    Solves the optimization problem::\r\n"]
[1053.336, "o", "\r\n"]
[1053.346, "o", "        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[1053.356, "o", "                    (U,V)\r\n"]
[1053.366, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[1053.376, "o", "\r\n"]
[1053.386, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[1053.396, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[1053.406, "o", "    of all the entries in the matrix.\r\n"]
[1053.416, "o", "\r\n"]
[1053.426, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[1053.436, "o", "\r\n"]
[1053.446, "o", "    Parameters\r\n"]
[1053.456, "o", "    ----------\r\n"]
[1053.466, "o", "    n_components : int, default=None\r\n"]
[1053.476, "o", "        Number of dictionary elements to extract. If None, then ``n_components``\r\n"]
[1053.486, "o", "        is set to ``n_features``.\r\n"]
[1053.496, "o", "\r\n"]
[1053.506, "o", "    alpha : float, default=1.0\r\n"]
[1053.516, "o", "        Sparsity controlling parameter.\r\n"]
[1053.526, "o", "\r\n"]
[1053.536, "o", "    max_iter : int, default=1000\r\n"]
[1053.546, "o", "        Maximum number of iterations to perform.\r\n"]
[1053.556, "o", "\r\n"]
[1053.566, "o", "    tol : float, default=1e-8\r\n"]
[1053.576, "o", "        Tolerance for numerical error.\r\n"]
[1053.586, "o", "\r\n"]
[1053.596, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[1053.606, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[1053.616, "o", "          problem (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[1053.626, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[1053.636, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n"]
[1053.646, "o", "          faster if the estimated components are sparse.\r\n"]
[1053.656, "o", "\r\n"]
[1053.666, "o", "        .. versionadded:: 0.17\r\n"]
[1053.676, "o", "           *cd* coordinate descent method to improve speed.\r\n"]
[1053.686, "o", "\r\n"]
[1053.696, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[1053.706, "o", "            'threshold'}, default='omp'\r\n"]
[1053.716, "o", "        Algorithm used to transform the data:\r\n"]
[1053.726, "o", "\r\n"]
[1053.736, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[1053.746, "o", "          (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[1053.756, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[1053.766, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1053.776, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n"]
[1053.786, "o", "          will be faster if the estimated components are sparse.\r\n"]
[1053.796, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1053.806, "o", "          solution.\r\n"]
[1053.816, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[1053.826, "o", "          the projection ``dictionary * X'``.\r\n"]
[1053.836, "o", "\r\n"]
[1053.846, "o", "        .. versionadded:: 0.17\r\n"]
[1053.856, "o", "           *lasso_cd* coordinate descent method to improve speed.\r\n"]
[1053.866, "o", "\r\n"]
[1053.876, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[1053.886, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1053.896, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[1053.906, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[1053.916, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1053.926, "o", "\r\n"]
[1053.936, "o", "    transform_alpha : float, default=None\r\n"]
[1053.946, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1053.956, "o", "        penalty applied to the L1 norm.\r\n"]
[1053.966, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1053.976, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1053.986, "o", "        If `None`, defaults to `alpha`.\r\n"]
[1053.996, "o", "\r\n"]
[1054.006, "o", "        .. versionchanged:: 1.2\r\n"]
[1054.016, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[1054.026, "o", "\r\n"]
[1054.036, "o", "    n_jobs : int or None, default=None\r\n"]
[1054.046, "o", "        Number of parallel jobs to run.\r\n"]
[1054.056, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1054.066, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1054.076, "o", "        for more details.\r\n"]
[1054.086, "o", "\r\n"]
[1054.096, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1054.106, "o", "        Initial value for the code, for warm restart. Only used if `code_init`\r\n"]
[1054.116, "o", "        and `dict_init` are not None.\r\n"]
[1054.126, "o", "\r\n"]
[1054.136, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1054.146, "o", "        Initial values for the dictionary, for warm restart. Only used if\r\n"]
[1054.156, "o", "        `code_init` and `dict_init` are not None.\r\n"]
[1054.166, "o", "\r\n"]
[1054.176, "o", "    callback : callable, default=None\r\n"]
[1054.186, "o", "        Callable that gets invoked every five iterations.\r\n"]
[1054.196, "o", "\r\n"]
[1054.206, "o", "        .. versionadded:: 1.3\r\n"]
[1054.216, "o", "\r\n"]
[1054.226, "o", "    verbose : bool, default=False\r\n"]
[1054.236, "o", "        To control the verbosity of the procedure.\r\n"]
[1054.246, "o", "\r\n"]
[1054.256, "o", "    split_sign : bool, default=False\r\n"]
[1054.266, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[1054.276, "o", "        its negative part and its positive part. This can improve the\r\n"]
[1054.286, "o", "        performance of downstream classifiers.\r\n"]
[1054.296, "o", "\r\n"]
[1054.306, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1054.316, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[1054.326, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[1054.336, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[1054.346, "o", "        results across multiple function calls.\r\n"]
[1054.356, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1054.366, "o", "\r\n"]
[1054.376, "o", "    positive_code : bool, default=False\r\n"]
[1054.386, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1054.396, "o", "\r\n"]
[1054.406, "o", "        .. versionadded:: 0.20\r\n"]
[1054.416, "o", "\r\n"]
[1054.426, "o", "    positive_dict : bool, default=False\r\n"]
[1054.436, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1054.446, "o", "\r\n"]
[1054.456, "o", "        .. versionadded:: 0.20\r\n"]
[1054.466, "o", "\r\n"]
[1054.476, "o", "    transform_max_iter : int, default=1000\r\n"]
[1054.486, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1054.496, "o", "        `'lasso_lars'`.\r\n"]
[1054.506, "o", "\r\n"]
[1054.516, "o", "        .. versionadded:: 0.22\r\n"]
[1054.526, "o", "\r\n"]
[1054.536, "o", "    Attributes\r\n"]
[1054.546, "o", "    ----------\r\n"]
[1054.556, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[1054.566, "o", "        dictionary atoms extracted from the data\r\n"]
[1054.576, "o", "\r\n"]
[1054.586, "o", "    error_ : array\r\n"]
[1054.596, "o", "        vector of errors at each iteration\r\n"]
[1054.606, "o", "\r\n"]
[1054.616, "o", "    n_features_in_ : int\r\n"]
[1054.626, "o", "        Number of features seen during :term:`fit`.\r\n"]
[1054.636, "o", "\r\n"]
[1054.646, "o", "        .. versionadded:: 0.24\r\n"]
[1054.656, "o", "\r\n"]
[1054.666, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[1054.676, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[1054.686, "o", "        has feature names that are all strings.\r\n"]
[1054.696, "o", "\r\n"]
[1054.706, "o", "        .. versionadded:: 1.0\r\n"]
[1054.716, "o", "\r\n"]
[1054.726, "o", "    n_iter_ : int\r\n"]
[1054.736, "o", "        Number of iterations run.\r\n"]
[1054.746, "o", "\r\n"]
[1054.756, "o", "    See Also\r\n"]
[1054.766, "o", "    --------\r\n"]
[1054.776, "o", "    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n"]
[1054.786, "o", "        dictionary learning algorithm.\r\n"]
[1054.796, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1054.806, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[1054.816, "o", "        precomputed dictionary.\r\n"]
[1054.826, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1054.836, "o", "\r\n"]
[1054.846, "o", "    References\r\n"]
[1054.856, "o", "    ----------\r\n"]
[1054.866, "o", "\r\n"]
[1054.876, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[1054.886, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[1054.896, "o", "\r\n"]
[1054.906, "o", "    Examples\r\n"]
[1054.916, "o", "    --------\r\n"]
[1054.926, "o", "    >>> import numpy as np\r\n"]
[1054.936, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[1055.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1055.002, "i", "sed -n '2000,2600p' sklearn/decomposition/_dict_learning.py\r"]
[1055.004, "o", "sed -n '2000,2600p' sklearn/decomposition/_dict_learning.py\r\n"]
[1055.016, "o", "\u001b[?2004l\r\n"]
[1055.026, "o", "\r\n"]
[1055.036, "o", "    tol : float, default=1e-3\r\n"]
[1055.046, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[1055.056, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[1055.066, "o", "\r\n"]
[1055.076, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[1055.086, "o", "        `tol` to 0.0.\r\n"]
[1055.096, "o", "\r\n"]
[1055.106, "o", "        .. versionadded:: 1.1\r\n"]
[1055.116, "o", "\r\n"]
[1055.126, "o", "    max_no_improvement : int, default=10\r\n"]
[1055.136, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[1055.146, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[1055.156, "o", "        `max_iter` is not None.\r\n"]
[1055.166, "o", "\r\n"]
[1055.176, "o", "        To disable convergence detection based on cost function, set\r\n"]
[1055.186, "o", "        `max_no_improvement` to None.\r\n"]
[1055.196, "o", "\r\n"]
[1055.206, "o", "        .. versionadded:: 1.1\r\n"]
[1055.216, "o", "\r\n"]
[1055.226, "o", "    Attributes\r\n"]
[1055.236, "o", "    ----------\r\n"]
[1055.246, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[1055.256, "o", "        Components extracted from the data.\r\n"]
[1055.266, "o", "\r\n"]
[1055.276, "o", "    n_features_in_ : int\r\n"]
[1055.286, "o", "        Number of features seen during :term:`fit`.\r\n"]
[1055.296, "o", "\r\n"]
[1055.306, "o", "        .. versionadded:: 0.24\r\n"]
[1055.316, "o", "\r\n"]
[1055.326, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[1055.336, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[1055.346, "o", "        has feature names that are all strings.\r\n"]
[1055.356, "o", "\r\n"]
[1055.366, "o", "        .. versionadded:: 1.0\r\n"]
[1055.376, "o", "\r\n"]
[1055.386, "o", "    n_iter_ : int\r\n"]
[1055.396, "o", "        Number of iterations over the full dataset.\r\n"]
[1055.406, "o", "\r\n"]
[1055.416, "o", "    n_steps_ : int\r\n"]
[1055.426, "o", "        Number of mini-batches processed.\r\n"]
[1055.436, "o", "\r\n"]
[1055.446, "o", "        .. versionadded:: 1.1\r\n"]
[1055.456, "o", "\r\n"]
[1055.466, "o", "    See Also\r\n"]
[1055.476, "o", "    --------\r\n"]
[1055.486, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1055.496, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1055.506, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[1055.516, "o", "        precomputed dictionary.\r\n"]
[1055.526, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1055.536, "o", "\r\n"]
[1055.546, "o", "    References\r\n"]
[1055.556, "o", "    ----------\r\n"]
[1055.566, "o", "\r\n"]
[1055.576, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[1055.586, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[1055.596, "o", "\r\n"]
[1055.606, "o", "    Examples\r\n"]
[1055.616, "o", "    --------\r\n"]
[1055.626, "o", "    >>> import numpy as np\r\n"]
[1055.636, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[1055.646, "o", "    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n"]
[1055.656, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[1055.666, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[1055.676, "o", "    ...     random_state=42)\r\n"]
[1055.686, "o", "    >>> dict_learner = MiniBatchDictionaryLearning(\r\n"]
[1055.696, "o", "    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n"]
[1055.706, "o", "    ...     transform_alpha=0.1, random_state=42)\r\n"]
[1055.716, "o", "    >>> X_transformed = dict_learner.fit_transform(X)\r\n"]
[1055.726, "o", "\r\n"]
[1055.736, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[1055.746, "o", "\r\n"]
[1055.756, "o", "    >>> np.mean(X_transformed == 0) < 0.5\r\n"]
[1055.766, "o", "    True\r\n"]
[1055.776, "o", "\r\n"]
[1055.786, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[1055.796, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[1055.806, "o", "    the original signal:\r\n"]
[1055.816, "o", "\r\n"]
[1055.826, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[1055.836, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[1055.846, "o", "    0.057...\r\n"]
[1055.856, "o", "    \"\"\"\r\n"]
[1055.866, "o", "\r\n"]
[1055.876, "o", "    _parameter_constraints: dict = {\r\n"]
[1055.886, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1055.896, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1055.906, "o", "        \"n_iter\": [\r\n"]
[1055.916, "o", "            Interval(Integral, 0, None, closed=\"left\"),\r\n"]
[1055.926, "o", "            Hidden(StrOptions({\"deprecated\"})),\r\n"]
[1055.936, "o", "        ],\r\n"]
[1055.946, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n"]
[1055.956, "o", "        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n"]
[1055.966, "o", "        \"n_jobs\": [None, Integral],\r\n"]
[1055.976, "o", "        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n"]
[1055.986, "o", "        \"shuffle\": [\"boolean\"],\r\n"]
[1055.996, "o", "        \"dict_init\": [None, np.ndarray],\r\n"]
[1056.006, "o", "        \"transform_algorithm\": [\r\n"]
[1056.016, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[1056.026, "o", "        ],\r\n"]
[1056.036, "o", "        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1056.046, "o", "        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[1056.056, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[1056.066, "o", "        \"split_sign\": [\"boolean\"],\r\n"]
[1056.076, "o", "        \"random_state\": [\"random_state\"],\r\n"]
[1056.086, "o", "        \"positive_code\": [\"boolean\"],\r\n"]
[1056.096, "o", "        \"positive_dict\": [\"boolean\"],\r\n"]
[1056.106, "o", "        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1056.116, "o", "        \"callback\": [None, callable],\r\n"]
[1056.126, "o", "        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1056.136, "o", "        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n"]
[1056.146, "o", "    }\r\n"]
[1056.156, "o", "\r\n"]
[1056.166, "o", "    def __init__(\r\n"]
[1056.176, "o", "        self,\r\n"]
[1056.186, "o", "        n_components=None,\r\n"]
[1056.196, "o", "        *,\r\n"]
[1056.206, "o", "        alpha=1,\r\n"]
[1056.216, "o", "        n_iter=\"deprecated\",\r\n"]
[1056.226, "o", "        max_iter=None,\r\n"]
[1056.236, "o", "        fit_algorithm=\"lars\",\r\n"]
[1056.246, "o", "        n_jobs=None,\r\n"]
[1056.256, "o", "        batch_size=256,\r\n"]
[1056.266, "o", "        shuffle=True,\r\n"]
[1056.276, "o", "        dict_init=None,\r\n"]
[1056.286, "o", "        transform_algorithm=\"omp\",\r\n"]
[1056.296, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[1056.306, "o", "        transform_alpha=None,\r\n"]
[1056.316, "o", "        verbose=False,\r\n"]
[1056.326, "o", "        split_sign=False,\r\n"]
[1056.336, "o", "        random_state=None,\r\n"]
[1056.346, "o", "        positive_code=False,\r\n"]
[1056.356, "o", "        positive_dict=False,\r\n"]
[1056.366, "o", "        transform_max_iter=1000,\r\n"]
[1056.376, "o", "        callback=None,\r\n"]
[1056.386, "o", "        tol=1e-3,\r\n"]
[1056.396, "o", "        max_no_improvement=10,\r\n"]
[1056.406, "o", "    ):\r\n"]
[1056.416, "o", "        super().__init__(\r\n"]
[1056.426, "o", "            transform_algorithm,\r\n"]
[1056.436, "o", "            transform_n_nonzero_coefs,\r\n"]
[1056.446, "o", "            transform_alpha,\r\n"]
[1056.456, "o", "            split_sign,\r\n"]
[1056.466, "o", "            n_jobs,\r\n"]
[1056.476, "o", "            positive_code,\r\n"]
[1056.486, "o", "            transform_max_iter,\r\n"]
[1056.496, "o", "        )\r\n"]
[1056.506, "o", "        self.n_components = n_components\r\n"]
[1056.516, "o", "        self.alpha = alpha\r\n"]
[1056.526, "o", "        self.n_iter = n_iter\r\n"]
[1056.536, "o", "        self.max_iter = max_iter\r\n"]
[1056.546, "o", "        self.fit_algorithm = fit_algorithm\r\n"]
[1056.556, "o", "        self.dict_init = dict_init\r\n"]
[1056.566, "o", "        self.verbose = verbose\r\n"]
[1056.576, "o", "        self.shuffle = shuffle\r\n"]
[1056.586, "o", "        self.batch_size = batch_size\r\n"]
[1056.596, "o", "        self.split_sign = split_sign\r\n"]
[1056.606, "o", "        self.random_state = random_state\r\n"]
[1056.616, "o", "        self.positive_dict = positive_dict\r\n"]
[1056.626, "o", "        self.callback = callback\r\n"]
[1056.636, "o", "        self.max_no_improvement = max_no_improvement\r\n"]
[1056.646, "o", "        self.tol = tol\r\n"]
[1056.656, "o", "\r\n"]
[1056.666, "o", "    def _check_params(self, X):\r\n"]
[1056.676, "o", "        # n_components\r\n"]
[1056.686, "o", "        self._n_components = self.n_components\r\n"]
[1056.696, "o", "        if self._n_components is None:\r\n"]
[1056.706, "o", "            self._n_components = X.shape[1]\r\n"]
[1056.716, "o", "\r\n"]
[1056.726, "o", "        # fit_algorithm\r\n"]
[1056.736, "o", "        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n"]
[1056.746, "o", "        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n"]
[1056.756, "o", "\r\n"]
[1056.766, "o", "        # batch_size\r\n"]
[1056.776, "o", "        self._batch_size = min(self.batch_size, X.shape[0])\r\n"]
[1056.786, "o", "\r\n"]
[1056.796, "o", "    def _initialize_dict(self, X, random_state):\r\n"]
[1056.806, "o", "        \"\"\"Initialization of the dictionary.\"\"\"\r\n"]
[1056.816, "o", "        if self.dict_init is not None:\r\n"]
[1056.826, "o", "            dictionary = self.dict_init\r\n"]
[1056.836, "o", "        else:\r\n"]
[1056.846, "o", "            # Init V with SVD of X\r\n"]
[1056.856, "o", "            _, S, dictionary = randomized_svd(\r\n"]
[1056.866, "o", "                X, self._n_components, random_state=random_state\r\n"]
[1056.876, "o", "            )\r\n"]
[1056.886, "o", "            dictionary = S[:, np.newaxis] * dictionary\r\n"]
[1056.896, "o", "\r\n"]
[1056.906, "o", "        if self._n_components <= len(dictionary):\r\n"]
[1056.916, "o", "            dictionary = dictionary[: self._n_components, :]\r\n"]
[1056.926, "o", "        else:\r\n"]
[1056.936, "o", "            dictionary = np.concatenate(\r\n"]
[1056.946, "o", "                (\r\n"]
[1056.956, "o", "                    dictionary,\r\n"]
[1056.966, "o", "                    np.zeros(\r\n"]
[1056.976, "o", "                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n"]
[1056.986, "o", "                        dtype=dictionary.dtype,\r\n"]
[1056.996, "o", "                    ),\r\n"]
[1057.006, "o", "                )\r\n"]
[1057.016, "o", "            )\r\n"]
[1057.026, "o", "\r\n"]
[1057.036, "o", "        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n"]
[1057.046, "o", "        dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[1057.056, "o", "\r\n"]
[1057.066, "o", "        return dictionary\r\n"]
[1057.076, "o", "\r\n"]
[1057.086, "o", "    def _update_inner_stats(self, X, code, batch_size, step):\r\n"]
[1057.096, "o", "        \"\"\"Update the inner stats inplace.\"\"\"\r\n"]
[1057.106, "o", "        if step < batch_size - 1:\r\n"]
[1057.116, "o", "            theta = (step + 1) * batch_size\r\n"]
[1057.126, "o", "        else:\r\n"]
[1057.136, "o", "            theta = batch_size**2 + step + 1 - batch_size\r\n"]
[1057.146, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[1057.156, "o", "\r\n"]
[1057.166, "o", "        self._A *= beta\r\n"]
[1057.176, "o", "        self._A += code.T @ code / batch_size\r\n"]
[1057.186, "o", "        self._B *= beta\r\n"]
[1057.196, "o", "        self._B += X.T @ code / batch_size\r\n"]
[1057.206, "o", "\r\n"]
[1057.216, "o", "    def _minibatch_step(self, X, dictionary, random_state, step):\r\n"]
[1057.226, "o", "        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n"]
[1057.236, "o", "        batch_size = X.shape[0]\r\n"]
[1057.246, "o", "\r\n"]
[1057.256, "o", "        # Compute code for this batch\r\n"]
[1057.266, "o", "        code = _sparse_encode(\r\n"]
[1057.276, "o", "            X,\r\n"]
[1057.286, "o", "            dictionary,\r\n"]
[1057.296, "o", "            algorithm=self._fit_algorithm,\r\n"]
[1057.306, "o", "            alpha=self.alpha,\r\n"]
[1057.316, "o", "            n_jobs=self.n_jobs,\r\n"]
[1057.326, "o", "            positive=self.positive_code,\r\n"]
[1057.336, "o", "            max_iter=self.transform_max_iter,\r\n"]
[1057.346, "o", "            verbose=self.verbose,\r\n"]
[1057.356, "o", "        )\r\n"]
[1057.366, "o", "\r\n"]
[1057.376, "o", "        batch_cost = (\r\n"]
[1057.386, "o", "            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n"]
[1057.396, "o", "            + self.alpha * np.sum(np.abs(code))\r\n"]
[1057.406, "o", "        ) / batch_size\r\n"]
[1057.416, "o", "\r\n"]
[1057.426, "o", "        # Update inner stats\r\n"]
[1057.436, "o", "        self._update_inner_stats(X, code, batch_size, step)\r\n"]
[1057.446, "o", "\r\n"]
[1057.456, "o", "        # Update dictionary\r\n"]
[1057.466, "o", "        _update_dict(\r\n"]
[1057.476, "o", "            dictionary,\r\n"]
[1057.486, "o", "            X,\r\n"]
[1057.496, "o", "            code,\r\n"]
[1057.506, "o", "            self._A,\r\n"]
[1057.516, "o", "            self._B,\r\n"]
[1057.526, "o", "            verbose=self.verbose,\r\n"]
[1057.536, "o", "            random_state=random_state,\r\n"]
[1057.546, "o", "            positive=self.positive_dict,\r\n"]
[1057.556, "o", "        )\r\n"]
[1057.566, "o", "\r\n"]
[1057.576, "o", "        return batch_cost\r\n"]
[1057.586, "o", "\r\n"]
[1057.596, "o", "    def _check_convergence(\r\n"]
[1057.606, "o", "        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n"]
[1057.616, "o", "    ):\r\n"]
[1057.626, "o", "        \"\"\"Helper function to encapsulate the early stopping logic.\r\n"]
[1057.636, "o", "\r\n"]
[1057.646, "o", "        Early stopping is based on two factors:\r\n"]
[1057.656, "o", "        - A small change of the dictionary between two minibatch updates. This is\r\n"]
[1057.666, "o", "          controlled by the tol parameter.\r\n"]
[1057.676, "o", "        - No more improvement on a smoothed estimate of the objective function for a\r\n"]
[1057.686, "o", "          a certain number of consecutive minibatch updates. This is controlled by\r\n"]
[1057.696, "o", "          the max_no_improvement parameter.\r\n"]
[1057.706, "o", "        \"\"\"\r\n"]
[1057.716, "o", "        batch_size = X.shape[0]\r\n"]
[1057.726, "o", "\r\n"]
[1057.736, "o", "        # counts steps starting from 1 for user friendly verbose mode.\r\n"]
[1057.746, "o", "        step = step + 1\r\n"]
[1057.756, "o", "\r\n"]
[1057.766, "o", "        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n"]
[1057.776, "o", "        # too bad value\r\n"]
[1057.786, "o", "        if step <= min(100, n_samples / batch_size):\r\n"]
[1057.796, "o", "            if self.verbose:\r\n"]
[1057.806, "o", "                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n"]
[1057.816, "o", "            return False\r\n"]
[1057.826, "o", "\r\n"]
[1057.836, "o", "        # Compute an Exponentially Weighted Average of the cost function to\r\n"]
[1057.846, "o", "        # monitor the convergence while discarding minibatch-local stochastic\r\n"]
[1057.856, "o", "        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n"]
[1057.866, "o", "        if self._ewa_cost is None:\r\n"]
[1057.876, "o", "            self._ewa_cost = batch_cost\r\n"]
[1057.886, "o", "        else:\r\n"]
[1057.896, "o", "            alpha = batch_size / (n_samples + 1)\r\n"]
[1057.906, "o", "            alpha = min(alpha, 1)\r\n"]
[1057.916, "o", "            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n"]
[1057.926, "o", "\r\n"]
[1057.936, "o", "        if self.verbose:\r\n"]
[1057.946, "o", "            print(\r\n"]
[1057.956, "o", "                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\r\n"]
[1057.966, "o", "                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\r\n"]
[1057.976, "o", "            )\r\n"]
[1057.986, "o", "\r\n"]
[1057.996, "o", "        # Early stopping based on change of dictionary\r\n"]
[1058.006, "o", "        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\r\n"]
[1058.016, "o", "        if self.tol > 0 and dict_diff <= self.tol:\r\n"]
[1058.026, "o", "            if self.verbose:\r\n"]
[1058.036, "o", "                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\r\n"]
[1058.046, "o", "            return True\r\n"]
[1058.056, "o", "\r\n"]
[1058.066, "o", "        # Early stopping heuristic due to lack of improvement on smoothed\r\n"]
[1058.076, "o", "        # cost function\r\n"]
[1058.086, "o", "        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\r\n"]
[1058.096, "o", "            self._no_improvement = 0\r\n"]
[1058.106, "o", "            self._ewa_cost_min = self._ewa_cost\r\n"]
[1058.116, "o", "        else:\r\n"]
[1058.126, "o", "            self._no_improvement += 1\r\n"]
[1058.136, "o", "\r\n"]
[1058.146, "o", "        if (\r\n"]
[1058.156, "o", "            self.max_no_improvement is not None\r\n"]
[1058.166, "o", "            and self._no_improvement >= self.max_no_improvement\r\n"]
[1058.176, "o", "        ):\r\n"]
[1058.186, "o", "            if self.verbose:\r\n"]
[1058.196, "o", "                print(\r\n"]
[1058.206, "o", "                    \"Converged (lack of improvement in objective function) \"\r\n"]
[1058.216, "o", "                    f\"at step {step}/{n_steps}\"\r\n"]
[1058.226, "o", "                )\r\n"]
[1058.236, "o", "            return True\r\n"]
[1058.246, "o", "\r\n"]
[1058.256, "o", "        return False\r\n"]
[1058.266, "o", "\r\n"]
[1058.276, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[1058.286, "o", "    def fit(self, X, y=None):\r\n"]
[1058.296, "o", "        \"\"\"Fit the model from data in X.\r\n"]
[1058.306, "o", "\r\n"]
[1058.316, "o", "        Parameters\r\n"]
[1058.326, "o", "        ----------\r\n"]
[1058.336, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[1058.346, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1058.356, "o", "            and `n_features` is the number of features.\r\n"]
[1058.366, "o", "\r\n"]
[1058.376, "o", "        y : Ignored\r\n"]
[1058.386, "o", "            Not used, present for API consistency by convention.\r\n"]
[1058.396, "o", "\r\n"]
[1058.406, "o", "        Returns\r\n"]
[1058.416, "o", "        -------\r\n"]
[1058.426, "o", "        self : object\r\n"]
[1058.436, "o", "            Returns the instance itself.\r\n"]
[1058.446, "o", "        \"\"\"\r\n"]
[1058.456, "o", "        X = self._validate_data(\r\n"]
[1058.466, "o", "            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n"]
[1058.476, "o", "        )\r\n"]
[1058.486, "o", "\r\n"]
[1058.496, "o", "        self._check_params(X)\r\n"]
[1058.506, "o", "\r\n"]
[1058.516, "o", "        if self.n_iter != \"deprecated\":\r\n"]
[1058.526, "o", "            warnings.warn(\r\n"]
[1058.536, "o", "                (\r\n"]
[1058.546, "o", "                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n"]
[1058.556, "o", "                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n"]
[1058.566, "o", "                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n"]
[1058.576, "o", "                    \"specified.\"\r\n"]
[1058.586, "o", "                ),\r\n"]
[1058.596, "o", "                FutureWarning,\r\n"]
[1058.606, "o", "            )\r\n"]
[1058.616, "o", "            n_iter = self.n_iter\r\n"]
[1058.626, "o", "\r\n"]
[1058.636, "o", "        self._random_state = check_random_state(self.random_state)\r\n"]
[1058.646, "o", "\r\n"]
[1058.656, "o", "        dictionary = self._initialize_dict(X, self._random_state)\r\n"]
[1058.666, "o", "        old_dict = dictionary.copy()\r\n"]
[1058.676, "o", "\r\n"]
[1058.686, "o", "        if self.shuffle:\r\n"]
[1058.696, "o", "            X_train = X.copy()\r\n"]
[1058.706, "o", "            self._random_state.shuffle(X_train)\r\n"]
[1058.716, "o", "        else:\r\n"]
[1058.726, "o", "            X_train = X\r\n"]
[1058.736, "o", "\r\n"]
[1058.746, "o", "        n_samples, n_features = X_train.shape\r\n"]
[1058.756, "o", "\r\n"]
[1058.766, "o", "        if self.verbose:\r\n"]
[1058.776, "o", "            print(\"[dict_learning]\")\r\n"]
[1058.786, "o", "\r\n"]
[1058.796, "o", "        # Inner stats\r\n"]
[1058.806, "o", "        self._A = np.zeros(\r\n"]
[1058.816, "o", "            (self._n_components, self._n_components), dtype=X_train.dtype\r\n"]
[1058.826, "o", "        )\r\n"]
[1058.836, "o", "        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n"]
[1058.846, "o", "\r\n"]
[1058.856, "o", "        if self.max_iter is not None:\r\n"]
[1058.866, "o", "            # Attributes to monitor the convergence\r\n"]
[1058.876, "o", "            self._ewa_cost = None\r\n"]
[1058.886, "o", "            self._ewa_cost_min = None\r\n"]
[1058.896, "o", "            self._no_improvement = 0\r\n"]
[1058.906, "o", "\r\n"]
[1058.916, "o", "            batches = gen_batches(n_samples, self._batch_size)\r\n"]
[1058.926, "o", "            batches = itertools.cycle(batches)\r\n"]
[1058.936, "o", "            n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\r\n"]
[1058.946, "o", "            n_steps = self.max_iter * n_steps_per_iter\r\n"]
[1058.956, "o", "\r\n"]
[1058.966, "o", "            i = -1  # to allow max_iter = 0\r\n"]
[1058.976, "o", "\r\n"]
[1058.986, "o", "            for i, batch in zip(range(n_steps), batches):\r\n"]
[1058.996, "o", "                X_batch = X_train[batch]\r\n"]
[1059.006, "o", "\r\n"]
[1059.016, "o", "                batch_cost = self._minibatch_step(\r\n"]
[1059.026, "o", "                    X_batch, dictionary, self._random_state, i\r\n"]
[1059.036, "o", "                )\r\n"]
[1059.046, "o", "\r\n"]
[1059.056, "o", "                if self._check_convergence(\r\n"]
[1059.066, "o", "                    X_batch, batch_cost, dictionary, old_dict, n_samples, i, n_steps\r\n"]
[1059.076, "o", "                ):\r\n"]
[1059.086, "o", "                    break\r\n"]
[1059.096, "o", "\r\n"]
[1059.106, "o", "                # XXX callback param added for backward compat in #18975 but a common\r\n"]
[1059.116, "o", "                # unified callback API should be preferred\r\n"]
[1059.126, "o", "                if self.callback is not None:\r\n"]
[1059.136, "o", "                    self.callback(locals())\r\n"]
[1059.146, "o", "\r\n"]
[1059.156, "o", "                old_dict[:] = dictionary\r\n"]
[1059.166, "o", "\r\n"]
[1059.176, "o", "            self.n_steps_ = i + 1\r\n"]
[1059.186, "o", "            self.n_iter_ = np.ceil(self.n_steps_ / n_steps_per_iter)\r\n"]
[1059.196, "o", "        else:\r\n"]
[1059.206, "o", "            # TODO remove this branch in 1.4\r\n"]
[1059.216, "o", "            n_iter = 1000 if self.n_iter == \"deprecated\" else self.n_iter\r\n"]
[1059.226, "o", "\r\n"]
[1059.236, "o", "            batches = gen_batches(n_samples, self._batch_size)\r\n"]
[1059.246, "o", "            batches = itertools.cycle(batches)\r\n"]
[1059.256, "o", "\r\n"]
[1059.266, "o", "            for i, batch in zip(range(n_iter), batches):\r\n"]
[1059.276, "o", "                self._minibatch_step(X_train[batch], dictionary, self._random_state, i)\r\n"]
[1059.286, "o", "\r\n"]
[1059.296, "o", "                trigger_verbose = self.verbose and i % ceil(100.0 / self.verbose) == 0\r\n"]
[1059.306, "o", "                if self.verbose > 10 or trigger_verbose:\r\n"]
[1059.316, "o", "                    print(f\"{i} batches processed.\")\r\n"]
[1059.326, "o", "\r\n"]
[1059.336, "o", "                if self.callback is not None:\r\n"]
[1059.346, "o", "                    self.callback(locals())\r\n"]
[1059.356, "o", "\r\n"]
[1059.366, "o", "            self.n_steps_ = n_iter\r\n"]
[1059.376, "o", "            self.n_iter_ = np.ceil(n_iter / int(np.ceil(n_samples / self._batch_size)))\r\n"]
[1059.386, "o", "\r\n"]
[1059.396, "o", "        self.components_ = dictionary\r\n"]
[1059.406, "o", "\r\n"]
[1059.416, "o", "        return self\r\n"]
[1059.426, "o", "\r\n"]
[1059.436, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[1059.446, "o", "    def partial_fit(self, X, y=None):\r\n"]
[1059.456, "o", "        \"\"\"Update the model using the data in X as a mini-batch.\r\n"]
[1059.466, "o", "\r\n"]
[1059.476, "o", "        Parameters\r\n"]
[1059.486, "o", "        ----------\r\n"]
[1059.496, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[1059.506, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1059.516, "o", "            and `n_features` is the number of features.\r\n"]
[1059.526, "o", "\r\n"]
[1059.536, "o", "        y : Ignored\r\n"]
[1059.546, "o", "            Not used, present for API consistency by convention.\r\n"]
[1059.556, "o", "\r\n"]
[1059.566, "o", "        Returns\r\n"]
[1059.576, "o", "        -------\r\n"]
[1059.586, "o", "        self : object\r\n"]
[1059.596, "o", "            Return the instance itself.\r\n"]
[1059.606, "o", "        \"\"\"\r\n"]
[1059.616, "o", "        has_components = hasattr(self, \"components_\")\r\n"]
[1059.626, "o", "\r\n"]
[1059.636, "o", "        X = self._validate_data(\r\n"]
[1059.646, "o", "            X, dtype=[np.float64, np.float32], order=\"C\", reset=not has_components\r\n"]
[1059.656, "o", "        )\r\n"]
[1059.666, "o", "\r\n"]
[1059.676, "o", "        if not has_components:\r\n"]
[1059.686, "o", "            # This instance has not been fitted yet (fit or partial_fit)\r\n"]
[1059.696, "o", "            self._check_params(X)\r\n"]
[1059.706, "o", "            self._random_state = check_random_state(self.random_state)\r\n"]
[1059.716, "o", "\r\n"]
[1059.726, "o", "            dictionary = self._initialize_dict(X, self._random_state)\r\n"]
[1059.736, "o", "\r\n"]
[1059.746, "o", "            self.n_steps_ = 0\r\n"]
[1059.756, "o", "\r\n"]
[1059.766, "o", "            self._A = np.zeros((self._n_components, self._n_components), dtype=X.dtype)\r\n"]
[1059.776, "o", "            self._B = np.zeros((X.shape[1], self._n_components), dtype=X.dtype)\r\n"]
[1059.786, "o", "        else:\r\n"]
[1059.796, "o", "            dictionary = self.components_\r\n"]
[1059.806, "o", "\r\n"]
[1059.816, "o", "        self._minibatch_step(X, dictionary, self._random_state, self.n_steps_)\r\n"]
[1059.826, "o", "\r\n"]
[1059.836, "o", "        self.components_ = dictionary\r\n"]
[1059.846, "o", "        self.n_steps_ += 1\r\n"]
[1059.856, "o", "\r\n"]
[1059.866, "o", "        return self\r\n"]
[1059.876, "o", "\r\n"]
[1059.886, "o", "    @property\r\n"]
[1059.896, "o", "    def _n_features_out(self):\r\n"]
[1059.906, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[1059.916, "o", "        return self.components_.shape[0]\r\n"]
[1059.926, "o", "\r\n"]
[1059.936, "o", "    def _more_tags(self):\r\n"]
[1060.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1060.002, "i", "pytest -q sklearn/decomposition/tests/test_dict_learning.py -k minibatch -q\r"]
[1060.004, "o", "pytest -q sklearn/decomposition/tests/test_dict_learning.py -k minibatch -q\r\n"]
[1062.478, "o", "\u001b[?2004l\r\n"]
[1065.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1065.002, "i", "cd asv_benchmarks\r"]
[1065.004, "o", "cd asv_benchmarks\r\n"]
[1065.006, "o", "\u001b[?2004l\r\n"]
[1070.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1070.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r"]
[1070.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r\n"]
[1070.315, "o", "stderr\r\n"]
[1070.624, "o", "\u001b[?2004l\r\n"]
[1070.933, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[1071.242, "o", "\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n"]
[1071.551, "o", "[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n"]
[1071.86, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n"]
[1072.169, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0m                                                                                     ok\r\n"]
[1072.478, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ============\u001b[0m\r\n"]
[1072.787, "o", "              --                 n_jobs   \r\n"]
[1073.096, "o", "              --------------- ------------\r\n"]
[1073.405, "o", "               fit_algorithm       1      \r\n"]
[1073.714, "o", "              =============== ============\r\n"]
[1074.023, "o", "                    lars       6.50\u00b10.03s \r\n"]
[1074.332, "o", "                     cd        1.50\u00b10.02s \r\n"]
[1074.641, "o", "              =============== ============\r\n"]
[1075.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1075.002, "i", "cd /workspace/repo\r"]
[1075.004, "o", "cd /workspace/repo\r\n"]
[1075.006, "o", "\u001b[?2004l\r\n"]
[1080.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1080.002, "i", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py\r"]
[1080.004, "o", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py\r\n"]
[1082.478, "o", "\u001b[?2004l\r\n"]
[1085.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1085.002, "i", "grep -n \"_sparse_encode_precomputed\\s*\\(\" sklearn/decomposition/_dict_learning.py || true\r"]
[1085.004, "o", "grep -n \"_sparse_encode_precomputed\\s*\\(\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[1087.478, "o", "\u001b[?2004l\r\n"]
[1090.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1090.002, "i", "grep -n \"sparse_encode\\s*\\(\" sklearn/decomposition/_dict_learning.py || true\r"]
[1090.004, "o", "grep -n \"sparse_encode\\s*\\(\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[1092.478, "o", "\u001b[?2004l\r\n"]
[1095.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1095.002, "i", "grep -n \"\\bgram\\s*=\\s|\\bcov\\s*=\\s\" sklearn/decomposition/_dict_learning.py || true\r"]
[1095.004, "o", "grep -n \"\\bgram\\s*=\\s|\\bcov\\s*=\\s\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[1095.006, "o", "\u001b[?2004l\r\n"]
[1100.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1100.002, "i", "sed -n '1,600p' sklearn/decomposition/_dict_learning.py\r"]
[1100.004, "o", "sed -n '1,600p' sklearn/decomposition/_dict_learning.py\r\n"]
[1100.016, "o", "\u001b[?2004l\r\n"]
[1100.026, "o", "\"\"\" Dictionary learning.\r\n"]
[1100.036, "o", "\"\"\"\r\n"]
[1100.046, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[1100.056, "o", "# License: BSD 3 clause\r\n"]
[1100.066, "o", "\r\n"]
[1100.076, "o", "import itertools\r\n"]
[1100.086, "o", "import sys\r\n"]
[1100.096, "o", "import time\r\n"]
[1100.106, "o", "import warnings\r\n"]
[1100.116, "o", "from math import ceil\r\n"]
[1100.126, "o", "from numbers import Integral, Real\r\n"]
[1100.136, "o", "\r\n"]
[1100.146, "o", "import numpy as np\r\n"]
[1100.156, "o", "from joblib import effective_n_jobs\r\n"]
[1100.166, "o", "from scipy import linalg\r\n"]
[1100.176, "o", "\r\n"]
[1100.186, "o", "from ..base import (\r\n"]
[1100.196, "o", "    BaseEstimator,\r\n"]
[1100.206, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[1100.216, "o", "    TransformerMixin,\r\n"]
[1100.226, "o", "    _fit_context,\r\n"]
[1100.236, "o", ")\r\n"]
[1100.246, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[1100.256, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[1100.266, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[1100.276, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[1100.286, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[1100.296, "o", "from ..utils.validation import check_is_fitted\r\n"]
[1100.306, "o", "\r\n"]
[1100.316, "o", "\r\n"]
[1100.326, "o", "def _check_positive_coding(method, positive):\r\n"]
[1100.336, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[1100.346, "o", "        raise ValueError(\r\n"]
[1100.356, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[1100.366, "o", "        )\r\n"]
[1100.376, "o", "\r\n"]
[1100.386, "o", "\r\n"]
[1100.396, "o", "def _sparse_encode_precomputed(\r\n"]
[1100.406, "o", "    X,\r\n"]
[1100.416, "o", "    dictionary,\r\n"]
[1100.426, "o", "    *,\r\n"]
[1100.436, "o", "    gram=None,\r\n"]
[1100.446, "o", "    cov=None,\r\n"]
[1100.456, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1100.466, "o", "    regularization=None,\r\n"]
[1100.476, "o", "    copy_cov=True,\r\n"]
[1100.486, "o", "    init=None,\r\n"]
[1100.496, "o", "    max_iter=1000,\r\n"]
[1100.506, "o", "    verbose=0,\r\n"]
[1100.516, "o", "    positive=False,\r\n"]
[1100.526, "o", "):\r\n"]
[1100.536, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[1100.546, "o", "\r\n"]
[1100.556, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[1100.566, "o", "\r\n"]
[1100.576, "o", "    Parameters\r\n"]
[1100.586, "o", "    ----------\r\n"]
[1100.596, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[1100.606, "o", "        Data matrix.\r\n"]
[1100.616, "o", "\r\n"]
[1100.626, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1100.636, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1100.646, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[1100.656, "o", "\r\n"]
[1100.666, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[1100.676, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[1100.686, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[1100.696, "o", "\r\n"]
[1100.706, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[1100.716, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[1100.726, "o", "\r\n"]
[1100.736, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1100.746, "o", "            default='lasso_lars'\r\n"]
[1100.756, "o", "        The algorithm used:\r\n"]
[1100.766, "o", "\r\n"]
[1100.776, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[1100.786, "o", "          (`linear_model.lars_path`);\r\n"]
[1100.796, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1100.806, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1100.816, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1100.826, "o", "          the estimated components are sparse;\r\n"]
[1100.836, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1100.846, "o", "          solution;\r\n"]
[1100.856, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1100.866, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[1100.876, "o", "\r\n"]
[1100.886, "o", "    regularization : int or float, default=None\r\n"]
[1100.896, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[1100.906, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[1100.916, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[1100.926, "o", "\r\n"]
[1100.936, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1100.946, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[1100.956, "o", "        `algorithm='lasso_cd'`.\r\n"]
[1100.966, "o", "\r\n"]
[1100.976, "o", "    max_iter : int, default=1000\r\n"]
[1100.986, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1100.996, "o", "        `'lasso_lars'`.\r\n"]
[1101.006, "o", "\r\n"]
[1101.016, "o", "    copy_cov : bool, default=True\r\n"]
[1101.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1101.036, "o", "        be overwritten.\r\n"]
[1101.046, "o", "\r\n"]
[1101.056, "o", "    verbose : int, default=0\r\n"]
[1101.066, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[1101.076, "o", "\r\n"]
[1101.086, "o", "    positive: bool, default=False\r\n"]
[1101.096, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[1101.106, "o", "\r\n"]
[1101.116, "o", "        .. versionadded:: 0.20\r\n"]
[1101.126, "o", "\r\n"]
[1101.136, "o", "    Returns\r\n"]
[1101.146, "o", "    -------\r\n"]
[1101.156, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[1101.166, "o", "        The sparse codes.\r\n"]
[1101.176, "o", "    \"\"\"\r\n"]
[1101.186, "o", "    n_samples, n_features = X.shape\r\n"]
[1101.196, "o", "    n_components = dictionary.shape[0]\r\n"]
[1101.206, "o", "\r\n"]
[1101.216, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[1101.226, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1101.236, "o", "        try:\r\n"]
[1101.246, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1101.256, "o", "\r\n"]
[1101.266, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1101.276, "o", "            # corrects the verbosity level.\r\n"]
[1101.286, "o", "            lasso_lars = LassoLars(\r\n"]
[1101.296, "o", "                alpha=alpha,\r\n"]
[1101.306, "o", "                fit_intercept=False,\r\n"]
[1101.316, "o", "                verbose=verbose,\r\n"]
[1101.326, "o", "                precompute=gram,\r\n"]
[1101.336, "o", "                fit_path=False,\r\n"]
[1101.346, "o", "                positive=positive,\r\n"]
[1101.356, "o", "                max_iter=max_iter,\r\n"]
[1101.366, "o", "            )\r\n"]
[1101.376, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1101.386, "o", "            new_code = lasso_lars.coef_\r\n"]
[1101.396, "o", "        finally:\r\n"]
[1101.406, "o", "            np.seterr(**err_mgt)\r\n"]
[1101.416, "o", "\r\n"]
[1101.426, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[1101.436, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1101.446, "o", "\r\n"]
[1101.456, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[1101.466, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[1101.476, "o", "        # argument that we could pass in from Lasso.\r\n"]
[1101.486, "o", "        clf = Lasso(\r\n"]
[1101.496, "o", "            alpha=alpha,\r\n"]
[1101.506, "o", "            fit_intercept=False,\r\n"]
[1101.516, "o", "            precompute=gram,\r\n"]
[1101.526, "o", "            max_iter=max_iter,\r\n"]
[1101.536, "o", "            warm_start=True,\r\n"]
[1101.546, "o", "            positive=positive,\r\n"]
[1101.556, "o", "        )\r\n"]
[1101.566, "o", "\r\n"]
[1101.576, "o", "        if init is not None:\r\n"]
[1101.586, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[1101.596, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[1101.606, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[1101.616, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[1101.626, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[1101.636, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[1101.646, "o", "                init = np.array(init)\r\n"]
[1101.656, "o", "            clf.coef_ = init\r\n"]
[1101.666, "o", "\r\n"]
[1101.676, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[1101.686, "o", "        new_code = clf.coef_\r\n"]
[1101.696, "o", "\r\n"]
[1101.706, "o", "    elif algorithm == \"lars\":\r\n"]
[1101.716, "o", "        try:\r\n"]
[1101.726, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1101.736, "o", "\r\n"]
[1101.746, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1101.756, "o", "            # corrects the verbosity level.\r\n"]
[1101.766, "o", "            lars = Lars(\r\n"]
[1101.776, "o", "                fit_intercept=False,\r\n"]
[1101.786, "o", "                verbose=verbose,\r\n"]
[1101.796, "o", "                precompute=gram,\r\n"]
[1101.806, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[1101.816, "o", "                fit_path=False,\r\n"]
[1101.826, "o", "            )\r\n"]
[1101.836, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1101.846, "o", "            new_code = lars.coef_\r\n"]
[1101.856, "o", "        finally:\r\n"]
[1101.866, "o", "            np.seterr(**err_mgt)\r\n"]
[1101.876, "o", "\r\n"]
[1101.886, "o", "    elif algorithm == \"threshold\":\r\n"]
[1101.896, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[1101.906, "o", "        if positive:\r\n"]
[1101.916, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[1101.926, "o", "\r\n"]
[1101.936, "o", "    elif algorithm == \"omp\":\r\n"]
[1101.946, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[1101.956, "o", "            Gram=gram,\r\n"]
[1101.966, "o", "            Xy=cov,\r\n"]
[1101.976, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[1101.986, "o", "            tol=None,\r\n"]
[1101.996, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[1102.006, "o", "            copy_Xy=copy_cov,\r\n"]
[1102.016, "o", "        ).T\r\n"]
[1102.026, "o", "\r\n"]
[1102.036, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[1102.046, "o", "\r\n"]
[1102.056, "o", "\r\n"]
[1102.066, "o", "@validate_params(\r\n"]
[1102.076, "o", "    {\r\n"]
[1102.086, "o", "        \"X\": [\"array-like\"],\r\n"]
[1102.096, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[1102.106, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[1102.116, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[1102.126, "o", "        \"algorithm\": [\r\n"]
[1102.136, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[1102.146, "o", "        ],\r\n"]
[1102.156, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1102.166, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[1102.176, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[1102.186, "o", "        \"init\": [\"array-like\", None],\r\n"]
[1102.196, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1102.206, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[1102.216, "o", "        \"check_input\": [\"boolean\"],\r\n"]
[1102.226, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[1102.236, "o", "        \"positive\": [\"boolean\"],\r\n"]
[1102.246, "o", "    },\r\n"]
[1102.256, "o", "    prefer_skip_nested_validation=True,\r\n"]
[1102.266, "o", ")\r\n"]
[1102.276, "o", "# XXX : could be moved to the linear_model module\r\n"]
[1102.286, "o", "def sparse_encode(\r\n"]
[1102.296, "o", "    X,\r\n"]
[1102.306, "o", "    dictionary,\r\n"]
[1102.316, "o", "    *,\r\n"]
[1102.326, "o", "    gram=None,\r\n"]
[1102.336, "o", "    cov=None,\r\n"]
[1102.346, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1102.356, "o", "    n_nonzero_coefs=None,\r\n"]
[1102.366, "o", "    alpha=None,\r\n"]
[1102.376, "o", "    copy_cov=True,\r\n"]
[1102.386, "o", "    init=None,\r\n"]
[1102.396, "o", "    max_iter=1000,\r\n"]
[1102.406, "o", "    n_jobs=None,\r\n"]
[1102.416, "o", "    check_input=True,\r\n"]
[1102.426, "o", "    verbose=0,\r\n"]
[1102.436, "o", "    positive=False,\r\n"]
[1102.446, "o", "):\r\n"]
[1102.456, "o", "    \"\"\"Sparse coding.\r\n"]
[1102.466, "o", "\r\n"]
[1102.476, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[1102.486, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[1102.496, "o", "\r\n"]
[1102.506, "o", "        X ~= code * dictionary\r\n"]
[1102.516, "o", "\r\n"]
[1102.526, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[1102.536, "o", "\r\n"]
[1102.546, "o", "    Parameters\r\n"]
[1102.556, "o", "    ----------\r\n"]
[1102.566, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[1102.576, "o", "        Data matrix.\r\n"]
[1102.586, "o", "\r\n"]
[1102.596, "o", "    dictionary : array-like of shape (n_components, n_features)\r\n"]
[1102.606, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1102.616, "o", "        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[1102.626, "o", "        output.\r\n"]
[1102.636, "o", "\r\n"]
[1102.646, "o", "    gram : array-like of shape (n_components, n_components), default=None\r\n"]
[1102.656, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n"]
[1102.666, "o", "\r\n"]
[1102.676, "o", "    cov : array-like of shape (n_components, n_samples), default=None\r\n"]
[1102.686, "o", "        Precomputed covariance, `dictionary' * X`.\r\n"]
[1102.696, "o", "\r\n"]
[1102.706, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1102.716, "o", "            default='lasso_lars'\r\n"]
[1102.726, "o", "        The algorithm used:\r\n"]
[1102.736, "o", "\r\n"]
[1102.746, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[1102.756, "o", "          (`linear_model.lars_path`);\r\n"]
[1102.766, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1102.776, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1102.786, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1102.796, "o", "          the estimated components are sparse;\r\n"]
[1102.806, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1102.816, "o", "          solution;\r\n"]
[1102.826, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1102.836, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[1102.846, "o", "\r\n"]
[1102.856, "o", "    n_nonzero_coefs : int, default=None\r\n"]
[1102.866, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1102.876, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[1102.886, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[1102.896, "o", "        `n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1102.906, "o", "\r\n"]
[1102.916, "o", "    alpha : float, default=None\r\n"]
[1102.926, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1102.936, "o", "        penalty applied to the L1 norm.\r\n"]
[1102.946, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1102.956, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1102.966, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[1102.976, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[1102.986, "o", "        `n_nonzero_coefs`.\r\n"]
[1102.996, "o", "        If `None`, default to 1.\r\n"]
[1103.006, "o", "\r\n"]
[1103.016, "o", "    copy_cov : bool, default=True\r\n"]
[1103.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1103.036, "o", "        be overwritten.\r\n"]
[1103.046, "o", "\r\n"]
[1103.056, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1103.066, "o", "        Initialization value of the sparse codes. Only used if\r\n"]
[1103.076, "o", "        `algorithm='lasso_cd'`.\r\n"]
[1103.086, "o", "\r\n"]
[1103.096, "o", "    max_iter : int, default=1000\r\n"]
[1103.106, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1103.116, "o", "        `'lasso_lars'`.\r\n"]
[1103.126, "o", "\r\n"]
[1103.136, "o", "    n_jobs : int, default=None\r\n"]
[1103.146, "o", "        Number of parallel jobs to run.\r\n"]
[1103.156, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1103.166, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1103.176, "o", "        for more details.\r\n"]
[1103.186, "o", "\r\n"]
[1103.196, "o", "    check_input : bool, default=True\r\n"]
[1103.206, "o", "        If `False`, the input arrays X and dictionary will not be checked.\r\n"]
[1103.216, "o", "\r\n"]
[1103.226, "o", "    verbose : int, default=0\r\n"]
[1103.236, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[1103.246, "o", "\r\n"]
[1103.256, "o", "    positive : bool, default=False\r\n"]
[1103.266, "o", "        Whether to enforce positivity when finding the encoding.\r\n"]
[1103.276, "o", "\r\n"]
[1103.286, "o", "        .. versionadded:: 0.20\r\n"]
[1103.296, "o", "\r\n"]
[1103.306, "o", "    Returns\r\n"]
[1103.316, "o", "    -------\r\n"]
[1103.326, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1103.336, "o", "        The sparse codes.\r\n"]
[1103.346, "o", "\r\n"]
[1103.356, "o", "    See Also\r\n"]
[1103.366, "o", "    --------\r\n"]
[1103.376, "o", "    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n"]
[1103.386, "o", "        path using LARS algorithm.\r\n"]
[1103.396, "o", "    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n"]
[1103.406, "o", "    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n"]
[1103.416, "o", "    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n"]
[1103.426, "o", "        dictionary.\r\n"]
[1103.436, "o", "    \"\"\"\r\n"]
[1103.446, "o", "    if check_input:\r\n"]
[1103.456, "o", "        if algorithm == \"lasso_cd\":\r\n"]
[1103.466, "o", "            dictionary = check_array(\r\n"]
[1103.476, "o", "                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n"]
[1103.486, "o", "            )\r\n"]
[1103.496, "o", "            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n"]
[1103.506, "o", "        else:\r\n"]
[1103.516, "o", "            dictionary = check_array(dictionary)\r\n"]
[1103.526, "o", "            X = check_array(X)\r\n"]
[1103.536, "o", "\r\n"]
[1103.546, "o", "    if dictionary.shape[1] != X.shape[1]:\r\n"]
[1103.556, "o", "        raise ValueError(\r\n"]
[1103.566, "o", "            \"Dictionary and X have different numbers of features:\"\r\n"]
[1103.576, "o", "            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n"]
[1103.586, "o", "        )\r\n"]
[1103.596, "o", "\r\n"]
[1103.606, "o", "    _check_positive_coding(algorithm, positive)\r\n"]
[1103.616, "o", "\r\n"]
[1103.626, "o", "    return _sparse_encode(\r\n"]
[1103.636, "o", "        X,\r\n"]
[1103.646, "o", "        dictionary,\r\n"]
[1103.656, "o", "        gram=gram,\r\n"]
[1103.666, "o", "        cov=cov,\r\n"]
[1103.676, "o", "        algorithm=algorithm,\r\n"]
[1103.686, "o", "        n_nonzero_coefs=n_nonzero_coefs,\r\n"]
[1103.696, "o", "        alpha=alpha,\r\n"]
[1103.706, "o", "        copy_cov=copy_cov,\r\n"]
[1103.716, "o", "        init=init,\r\n"]
[1103.726, "o", "        max_iter=max_iter,\r\n"]
[1103.736, "o", "        n_jobs=n_jobs,\r\n"]
[1103.746, "o", "        verbose=verbose,\r\n"]
[1103.756, "o", "        positive=positive,\r\n"]
[1103.766, "o", "    )\r\n"]
[1103.776, "o", "\r\n"]
[1103.786, "o", "\r\n"]
[1103.796, "o", "def _sparse_encode(\r\n"]
[1103.806, "o", "    X,\r\n"]
[1103.816, "o", "    dictionary,\r\n"]
[1103.826, "o", "    *,\r\n"]
[1103.836, "o", "    gram=None,\r\n"]
[1103.846, "o", "    cov=None,\r\n"]
[1103.856, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1103.866, "o", "    n_nonzero_coefs=None,\r\n"]
[1103.876, "o", "    alpha=None,\r\n"]
[1103.886, "o", "    copy_cov=True,\r\n"]
[1103.896, "o", "    init=None,\r\n"]
[1103.906, "o", "    max_iter=1000,\r\n"]
[1103.916, "o", "    n_jobs=None,\r\n"]
[1103.926, "o", "    verbose=0,\r\n"]
[1103.936, "o", "    positive=False,\r\n"]
[1103.946, "o", "):\r\n"]
[1103.956, "o", "    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[1103.966, "o", "\r\n"]
[1103.976, "o", "    n_samples, n_features = X.shape\r\n"]
[1103.986, "o", "    n_components = dictionary.shape[0]\r\n"]
[1103.996, "o", "\r\n"]
[1104.006, "o", "    if algorithm in (\"lars\", \"omp\"):\r\n"]
[1104.016, "o", "        regularization = n_nonzero_coefs\r\n"]
[1104.026, "o", "        if regularization is None:\r\n"]
[1104.036, "o", "            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[1104.046, "o", "    else:\r\n"]
[1104.056, "o", "        regularization = alpha\r\n"]
[1104.066, "o", "        if regularization is None:\r\n"]
[1104.076, "o", "            regularization = 1.0\r\n"]
[1104.086, "o", "\r\n"]
[1104.096, "o", "    if gram is None and algorithm != \"threshold\":\r\n"]
[1104.106, "o", "        gram = np.dot(dictionary, dictionary.T)\r\n"]
[1104.116, "o", "\r\n"]
[1104.126, "o", "    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[1104.136, "o", "        copy_cov = False\r\n"]
[1104.146, "o", "        cov = np.dot(dictionary, X.T)\r\n"]
[1104.156, "o", "\r\n"]
[1104.166, "o", "    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[1104.176, "o", "        code = _sparse_encode_precomputed(\r\n"]
[1104.186, "o", "            X,\r\n"]
[1104.196, "o", "            dictionary,\r\n"]
[1104.206, "o", "            gram=gram,\r\n"]
[1104.216, "o", "            cov=cov,\r\n"]
[1104.226, "o", "            algorithm=algorithm,\r\n"]
[1104.236, "o", "            regularization=regularization,\r\n"]
[1104.246, "o", "            copy_cov=copy_cov,\r\n"]
[1104.256, "o", "            init=init,\r\n"]
[1104.266, "o", "            max_iter=max_iter,\r\n"]
[1104.276, "o", "            verbose=verbose,\r\n"]
[1104.286, "o", "            positive=positive,\r\n"]
[1104.296, "o", "        )\r\n"]
[1104.306, "o", "        return code\r\n"]
[1104.316, "o", "\r\n"]
[1104.326, "o", "    # Enter parallel code block\r\n"]
[1104.336, "o", "    n_samples = X.shape[0]\r\n"]
[1104.346, "o", "    n_components = dictionary.shape[0]\r\n"]
[1104.356, "o", "    code = np.empty((n_samples, n_components))\r\n"]
[1104.366, "o", "    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[1104.376, "o", "\r\n"]
[1104.386, "o", "    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[1104.396, "o", "        delayed(_sparse_encode_precomputed)(\r\n"]
[1104.406, "o", "            X[this_slice],\r\n"]
[1104.416, "o", "            dictionary,\r\n"]
[1104.426, "o", "            gram=gram,\r\n"]
[1104.436, "o", "            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[1104.446, "o", "            algorithm=algorithm,\r\n"]
[1104.456, "o", "            regularization=regularization,\r\n"]
[1104.466, "o", "            copy_cov=copy_cov,\r\n"]
[1104.476, "o", "            init=init[this_slice] if init is not None else None,\r\n"]
[1104.486, "o", "            max_iter=max_iter,\r\n"]
[1104.496, "o", "            verbose=verbose,\r\n"]
[1104.506, "o", "            positive=positive,\r\n"]
[1104.516, "o", "        )\r\n"]
[1104.526, "o", "        for this_slice in slices\r\n"]
[1104.536, "o", "    )\r\n"]
[1104.546, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[1104.556, "o", "        code[this_slice] = this_view\r\n"]
[1104.566, "o", "    return code\r\n"]
[1104.576, "o", "\r\n"]
[1104.586, "o", "\r\n"]
[1104.596, "o", "def _update_dict(\r\n"]
[1104.606, "o", "    dictionary,\r\n"]
[1104.616, "o", "    Y,\r\n"]
[1104.626, "o", "    code,\r\n"]
[1104.636, "o", "    A=None,\r\n"]
[1104.646, "o", "    B=None,\r\n"]
[1104.656, "o", "    verbose=False,\r\n"]
[1104.666, "o", "    random_state=None,\r\n"]
[1104.676, "o", "    positive=False,\r\n"]
[1104.686, "o", "):\r\n"]
[1104.696, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[1104.706, "o", "\r\n"]
[1104.716, "o", "    Parameters\r\n"]
[1104.726, "o", "    ----------\r\n"]
[1104.736, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1104.746, "o", "        Value of the dictionary at the previous iteration.\r\n"]
[1104.756, "o", "\r\n"]
[1104.766, "o", "    Y : ndarray of shape (n_samples, n_features)\r\n"]
[1104.776, "o", "        Data matrix.\r\n"]
[1104.786, "o", "\r\n"]
[1104.796, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1104.806, "o", "        Sparse coding of the data against which to optimize the dictionary.\r\n"]
[1104.816, "o", "\r\n"]
[1104.826, "o", "    A : ndarray of shape (n_components, n_components), default=None\r\n"]
[1104.836, "o", "        Together with `B`, sufficient stats of the online model to update the\r\n"]
[1104.846, "o", "        dictionary.\r\n"]
[1104.856, "o", "\r\n"]
[1104.866, "o", "    B : ndarray of shape (n_features, n_components), default=None\r\n"]
[1104.876, "o", "        Together with `A`, sufficient stats of the online model to update the\r\n"]
[1104.886, "o", "        dictionary.\r\n"]
[1104.896, "o", "\r\n"]
[1104.906, "o", "    verbose: bool, default=False\r\n"]
[1104.916, "o", "        Degree of output the procedure will print.\r\n"]
[1104.926, "o", "\r\n"]
[1104.936, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1105.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1105.002, "i", "sed -n '600,1400p' sklearn/decomposition/_dict_learning.py\r"]
[1105.004, "o", "sed -n '600,1400p' sklearn/decomposition/_dict_learning.py\r\n"]
[1105.016, "o", "\u001b[?2004l\r\n"]
[1105.026, "o", "                % (ii, dt, dt / 60, current_cost)\r\n"]
[1105.036, "o", "            )\r\n"]
[1105.046, "o", "\r\n"]
[1105.056, "o", "        # Update code\r\n"]
[1105.066, "o", "        code = sparse_encode(\r\n"]
[1105.076, "o", "            X,\r\n"]
[1105.086, "o", "            dictionary,\r\n"]
[1105.096, "o", "            algorithm=method,\r\n"]
[1105.106, "o", "            alpha=alpha,\r\n"]
[1105.116, "o", "            init=code,\r\n"]
[1105.126, "o", "            n_jobs=n_jobs,\r\n"]
[1105.136, "o", "            positive=positive_code,\r\n"]
[1105.146, "o", "            max_iter=method_max_iter,\r\n"]
[1105.156, "o", "            verbose=verbose,\r\n"]
[1105.166, "o", "        )\r\n"]
[1105.176, "o", "\r\n"]
[1105.186, "o", "        # Update dictionary in place\r\n"]
[1105.196, "o", "        _update_dict(\r\n"]
[1105.206, "o", "            dictionary,\r\n"]
[1105.216, "o", "            X,\r\n"]
[1105.226, "o", "            code,\r\n"]
[1105.236, "o", "            verbose=verbose,\r\n"]
[1105.246, "o", "            random_state=random_state,\r\n"]
[1105.256, "o", "            positive=positive_dict,\r\n"]
[1105.266, "o", "        )\r\n"]
[1105.276, "o", "\r\n"]
[1105.286, "o", "        # Cost function\r\n"]
[1105.296, "o", "        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n"]
[1105.306, "o", "            np.abs(code)\r\n"]
[1105.316, "o", "        )\r\n"]
[1105.326, "o", "        errors.append(current_cost)\r\n"]
[1105.336, "o", "\r\n"]
[1105.346, "o", "        if ii > 0:\r\n"]
[1105.356, "o", "            dE = errors[-2] - errors[-1]\r\n"]
[1105.366, "o", "            # assert(dE >= -tol * errors[-1])\r\n"]
[1105.376, "o", "            if dE < tol * errors[-1]:\r\n"]
[1105.386, "o", "                if verbose == 1:\r\n"]
[1105.396, "o", "                    # A line return\r\n"]
[1105.406, "o", "                    print(\"\")\r\n"]
[1105.416, "o", "                elif verbose:\r\n"]
[1105.426, "o", "                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n"]
[1105.436, "o", "                break\r\n"]
[1105.446, "o", "        if ii % 5 == 0 and callback is not None:\r\n"]
[1105.456, "o", "            callback(locals())\r\n"]
[1105.466, "o", "\r\n"]
[1105.476, "o", "    if return_n_iter:\r\n"]
[1105.486, "o", "        return code, dictionary, errors, ii + 1\r\n"]
[1105.496, "o", "    else:\r\n"]
[1105.506, "o", "        return code, dictionary, errors\r\n"]
[1105.516, "o", "\r\n"]
[1105.526, "o", "\r\n"]
[1105.536, "o", "def _check_warn_deprecated(param, name, default, additional_message=None):\r\n"]
[1105.546, "o", "    if param != \"deprecated\":\r\n"]
[1105.556, "o", "        msg = (\r\n"]
[1105.566, "o", "            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n"]
[1105.576, "o", "        )\r\n"]
[1105.586, "o", "        if additional_message:\r\n"]
[1105.596, "o", "            msg += f\" {additional_message}\"\r\n"]
[1105.606, "o", "        warnings.warn(msg, FutureWarning)\r\n"]
[1105.616, "o", "        return param\r\n"]
[1105.626, "o", "    else:\r\n"]
[1105.636, "o", "        return default\r\n"]
[1105.646, "o", "\r\n"]
[1105.656, "o", "\r\n"]
[1105.666, "o", "def dict_learning_online(\r\n"]
[1105.676, "o", "    X,\r\n"]
[1105.686, "o", "    n_components=2,\r\n"]
[1105.696, "o", "    *,\r\n"]
[1105.706, "o", "    alpha=1,\r\n"]
[1105.716, "o", "    n_iter=\"deprecated\",\r\n"]
[1105.726, "o", "    max_iter=None,\r\n"]
[1105.736, "o", "    return_code=True,\r\n"]
[1105.746, "o", "    dict_init=None,\r\n"]
[1105.756, "o", "    callback=None,\r\n"]
[1105.766, "o", "    batch_size=256,\r\n"]
[1105.776, "o", "    verbose=False,\r\n"]
[1105.786, "o", "    shuffle=True,\r\n"]
[1105.796, "o", "    n_jobs=None,\r\n"]
[1105.806, "o", "    method=\"lars\",\r\n"]
[1105.816, "o", "    iter_offset=\"deprecated\",\r\n"]
[1105.826, "o", "    random_state=None,\r\n"]
[1105.836, "o", "    return_inner_stats=\"deprecated\",\r\n"]
[1105.846, "o", "    inner_stats=\"deprecated\",\r\n"]
[1105.856, "o", "    return_n_iter=\"deprecated\",\r\n"]
[1105.866, "o", "    positive_dict=False,\r\n"]
[1105.876, "o", "    positive_code=False,\r\n"]
[1105.886, "o", "    method_max_iter=1000,\r\n"]
[1105.896, "o", "    tol=1e-3,\r\n"]
[1105.906, "o", "    max_no_improvement=10,\r\n"]
[1105.916, "o", "):\r\n"]
[1105.926, "o", "    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n"]
[1105.936, "o", "\r\n"]
[1105.946, "o", "    Finds the best dictionary and the corresponding sparse code for\r\n"]
[1105.956, "o", "    approximating the data matrix X by solving::\r\n"]
[1105.966, "o", "\r\n"]
[1105.976, "o", "        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[1105.986, "o", "                     (U,V)\r\n"]
[1105.996, "o", "                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n"]
[1106.006, "o", "\r\n"]
[1106.016, "o", "    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n"]
[1106.026, "o", "    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n"]
[1106.036, "o", "    which is the sum of the absolute values of all the entries in the matrix.\r\n"]
[1106.046, "o", "    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n"]
[1106.056, "o", "    the input data.\r\n"]
[1106.066, "o", "\r\n"]
[1106.076, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[1106.086, "o", "\r\n"]
[1106.096, "o", "    Parameters\r\n"]
[1106.106, "o", "    ----------\r\n"]
[1106.116, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[1106.126, "o", "        Data matrix.\r\n"]
[1106.136, "o", "\r\n"]
[1106.146, "o", "    n_components : int or None, default=2\r\n"]
[1106.156, "o", "        Number of dictionary atoms to extract. If None, then ``n_components``\r\n"]
[1106.166, "o", "        is set to ``n_features``.\r\n"]
[1106.176, "o", "\r\n"]
[1106.186, "o", "    alpha : float, default=1\r\n"]
[1106.196, "o", "        Sparsity controlling parameter.\r\n"]
[1106.206, "o", "\r\n"]
[1106.216, "o", "    n_iter : int, default=100\r\n"]
[1106.226, "o", "        Number of mini-batch iterations to perform.\r\n"]
[1106.236, "o", "\r\n"]
[1106.246, "o", "        .. deprecated:: 1.1\r\n"]
[1106.256, "o", "           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[1106.266, "o", "           `max_iter` instead.\r\n"]
[1106.276, "o", "\r\n"]
[1106.286, "o", "    max_iter : int, default=None\r\n"]
[1106.296, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[1106.306, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[1106.316, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[1106.326, "o", "\r\n"]
[1106.336, "o", "        .. versionadded:: 1.1\r\n"]
[1106.346, "o", "\r\n"]
[1106.356, "o", "    return_code : bool, default=True\r\n"]
[1106.366, "o", "        Whether to also return the code U or just the dictionary `V`.\r\n"]
[1106.376, "o", "\r\n"]
[1106.386, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1106.396, "o", "        Initial values for the dictionary for warm restart scenarios.\r\n"]
[1106.406, "o", "        If `None`, the initial values for the dictionary are created\r\n"]
[1106.416, "o", "        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n"]
[1106.426, "o", "\r\n"]
[1106.436, "o", "    callback : callable, default=None\r\n"]
[1106.446, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[1106.456, "o", "\r\n"]
[1106.466, "o", "    batch_size : int, default=256\r\n"]
[1106.476, "o", "        The number of samples to take in each batch.\r\n"]
[1106.486, "o", "\r\n"]
[1106.496, "o", "        .. versionchanged:: 1.3\r\n"]
[1106.506, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[1106.516, "o", "\r\n"]
[1106.526, "o", "    verbose : bool, default=False\r\n"]
[1106.536, "o", "        To control the verbosity of the procedure.\r\n"]
[1106.546, "o", "\r\n"]
[1106.556, "o", "    shuffle : bool, default=True\r\n"]
[1106.566, "o", "        Whether to shuffle the data before splitting it in batches.\r\n"]
[1106.576, "o", "\r\n"]
[1106.586, "o", "    n_jobs : int, default=None\r\n"]
[1106.596, "o", "        Number of parallel jobs to run.\r\n"]
[1106.606, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1106.616, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1106.626, "o", "        for more details.\r\n"]
[1106.636, "o", "\r\n"]
[1106.646, "o", "    method : {'lars', 'cd'}, default='lars'\r\n"]
[1106.656, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[1106.666, "o", "          problem (`linear_model.lars_path`);\r\n"]
[1106.676, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[1106.686, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[1106.696, "o", "          the estimated components are sparse.\r\n"]
[1106.706, "o", "\r\n"]
[1106.716, "o", "    iter_offset : int, default=0\r\n"]
[1106.726, "o", "        Number of previous iterations completed on the dictionary used for\r\n"]
[1106.736, "o", "        initialization.\r\n"]
[1106.746, "o", "\r\n"]
[1106.756, "o", "        .. deprecated:: 1.1\r\n"]
[1106.766, "o", "           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n"]
[1106.776, "o", "\r\n"]
[1106.786, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1106.796, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[1106.806, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[1106.816, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[1106.826, "o", "        results across multiple function calls.\r\n"]
[1106.836, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1106.846, "o", "\r\n"]
[1106.856, "o", "    return_inner_stats : bool, default=False\r\n"]
[1106.866, "o", "        Return the inner statistics A (dictionary covariance) and B\r\n"]
[1106.876, "o", "        (data approximation). Useful to restart the algorithm in an\r\n"]
[1106.886, "o", "        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n"]
[1106.896, "o", "        ignored.\r\n"]
[1106.906, "o", "\r\n"]
[1106.916, "o", "        .. deprecated:: 1.1\r\n"]
[1106.926, "o", "           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[1106.936, "o", "\r\n"]
[1106.946, "o", "    inner_stats : tuple of (A, B) ndarrays, default=None\r\n"]
[1106.956, "o", "        Inner sufficient statistics that are kept by the algorithm.\r\n"]
[1106.966, "o", "        Passing them at initialization is useful in online settings, to\r\n"]
[1106.976, "o", "        avoid losing the history of the evolution.\r\n"]
[1106.986, "o", "        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n"]
[1106.996, "o", "        `B` `(n_features, n_components)` is the data approximation matrix.\r\n"]
[1107.006, "o", "\r\n"]
[1107.016, "o", "        .. deprecated:: 1.1\r\n"]
[1107.026, "o", "           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[1107.036, "o", "\r\n"]
[1107.046, "o", "    return_n_iter : bool, default=False\r\n"]
[1107.056, "o", "        Whether or not to return the number of iterations.\r\n"]
[1107.066, "o", "\r\n"]
[1107.076, "o", "        .. deprecated:: 1.1\r\n"]
[1107.086, "o", "           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n"]
[1107.096, "o", "\r\n"]
[1107.106, "o", "    positive_dict : bool, default=False\r\n"]
[1107.116, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1107.126, "o", "\r\n"]
[1107.136, "o", "        .. versionadded:: 0.20\r\n"]
[1107.146, "o", "\r\n"]
[1107.156, "o", "    positive_code : bool, default=False\r\n"]
[1107.166, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1107.176, "o", "\r\n"]
[1107.186, "o", "        .. versionadded:: 0.20\r\n"]
[1107.196, "o", "\r\n"]
[1107.206, "o", "    method_max_iter : int, default=1000\r\n"]
[1107.216, "o", "        Maximum number of iterations to perform when solving the lasso problem.\r\n"]
[1107.226, "o", "\r\n"]
[1107.236, "o", "        .. versionadded:: 0.22\r\n"]
[1107.246, "o", "\r\n"]
[1107.256, "o", "    tol : float, default=1e-3\r\n"]
[1107.266, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[1107.276, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[1107.286, "o", "\r\n"]
[1107.296, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[1107.306, "o", "        `tol` to 0.0.\r\n"]
[1107.316, "o", "\r\n"]
[1107.326, "o", "        .. versionadded:: 1.1\r\n"]
[1107.336, "o", "\r\n"]
[1107.346, "o", "    max_no_improvement : int, default=10\r\n"]
[1107.356, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[1107.366, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[1107.376, "o", "        `max_iter` is not None.\r\n"]
[1107.386, "o", "\r\n"]
[1107.396, "o", "        To disable convergence detection based on cost function, set\r\n"]
[1107.406, "o", "        `max_no_improvement` to None.\r\n"]
[1107.416, "o", "\r\n"]
[1107.426, "o", "        .. versionadded:: 1.1\r\n"]
[1107.436, "o", "\r\n"]
[1107.446, "o", "    Returns\r\n"]
[1107.456, "o", "    -------\r\n"]
[1107.466, "o", "    code : ndarray of shape (n_samples, n_components),\r\n"]
[1107.476, "o", "        The sparse code (only returned if `return_code=True`).\r\n"]
[1107.486, "o", "\r\n"]
[1107.496, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[1107.506, "o", "        The solutions to the dictionary learning problem.\r\n"]
[1107.516, "o", "\r\n"]
[1107.526, "o", "    n_iter : int\r\n"]
[1107.536, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[1107.546, "o", "        set to `True`.\r\n"]
[1107.556, "o", "\r\n"]
[1107.566, "o", "    See Also\r\n"]
[1107.576, "o", "    --------\r\n"]
[1107.586, "o", "    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n"]
[1107.596, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1107.606, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n"]
[1107.616, "o", "        learning algorithm.\r\n"]
[1107.626, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1107.636, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1107.646, "o", "    \"\"\"\r\n"]
[1107.656, "o", "    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n"]
[1107.666, "o", "    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n"]
[1107.676, "o", "        raise ValueError(\r\n"]
[1107.686, "o", "            \"The following arguments are incompatible with 'max_iter': \"\r\n"]
[1107.696, "o", "            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n"]
[1107.706, "o", "        )\r\n"]
[1107.716, "o", "\r\n"]
[1107.726, "o", "    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n"]
[1107.736, "o", "    return_inner_stats = _check_warn_deprecated(\r\n"]
[1107.746, "o", "        return_inner_stats,\r\n"]
[1107.756, "o", "        \"return_inner_stats\",\r\n"]
[1107.766, "o", "        default=False,\r\n"]
[1107.776, "o", "        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n"]
[1107.786, "o", "    )\r\n"]
[1107.796, "o", "    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n"]
[1107.806, "o", "    return_n_iter = _check_warn_deprecated(\r\n"]
[1107.816, "o", "        return_n_iter,\r\n"]
[1107.826, "o", "        \"return_n_iter\",\r\n"]
[1107.836, "o", "        default=False,\r\n"]
[1107.846, "o", "        additional_message=(\r\n"]
[1107.856, "o", "            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n"]
[1107.866, "o", "            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n"]
[1107.876, "o", "        ),\r\n"]
[1107.886, "o", "    )\r\n"]
[1107.896, "o", "\r\n"]
[1107.906, "o", "    if max_iter is not None:\r\n"]
[1107.916, "o", "        transform_algorithm = \"lasso_\" + method\r\n"]
[1107.926, "o", "\r\n"]
[1107.936, "o", "        est = MiniBatchDictionaryLearning(\r\n"]
[1107.946, "o", "            n_components=n_components,\r\n"]
[1107.956, "o", "            alpha=alpha,\r\n"]
[1107.966, "o", "            n_iter=n_iter,\r\n"]
[1107.976, "o", "            n_jobs=n_jobs,\r\n"]
[1107.986, "o", "            fit_algorithm=method,\r\n"]
[1107.996, "o", "            batch_size=batch_size,\r\n"]
[1108.006, "o", "            shuffle=shuffle,\r\n"]
[1108.016, "o", "            dict_init=dict_init,\r\n"]
[1108.026, "o", "            random_state=random_state,\r\n"]
[1108.036, "o", "            transform_algorithm=transform_algorithm,\r\n"]
[1108.046, "o", "            transform_alpha=alpha,\r\n"]
[1108.056, "o", "            positive_code=positive_code,\r\n"]
[1108.066, "o", "            positive_dict=positive_dict,\r\n"]
[1108.076, "o", "            transform_max_iter=method_max_iter,\r\n"]
[1108.086, "o", "            verbose=verbose,\r\n"]
[1108.096, "o", "            callback=callback,\r\n"]
[1108.106, "o", "            tol=tol,\r\n"]
[1108.116, "o", "            max_no_improvement=max_no_improvement,\r\n"]
[1108.126, "o", "        ).fit(X)\r\n"]
[1108.136, "o", "\r\n"]
[1108.146, "o", "        if not return_code:\r\n"]
[1108.156, "o", "            return est.components_\r\n"]
[1108.166, "o", "        else:\r\n"]
[1108.176, "o", "            code = est.transform(X)\r\n"]
[1108.186, "o", "            return code, est.components_\r\n"]
[1108.196, "o", "\r\n"]
[1108.206, "o", "    # TODO(1.4) remove the whole old behavior\r\n"]
[1108.216, "o", "    # Fallback to old behavior\r\n"]
[1108.226, "o", "\r\n"]
[1108.236, "o", "    n_iter = _check_warn_deprecated(\r\n"]
[1108.246, "o", "        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n"]
[1108.256, "o", "    )\r\n"]
[1108.266, "o", "\r\n"]
[1108.276, "o", "    if n_components is None:\r\n"]
[1108.286, "o", "        n_components = X.shape[1]\r\n"]
[1108.296, "o", "\r\n"]
[1108.306, "o", "    if method not in (\"lars\", \"cd\"):\r\n"]
[1108.316, "o", "        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n"]
[1108.326, "o", "\r\n"]
[1108.336, "o", "    _check_positive_coding(method, positive_code)\r\n"]
[1108.346, "o", "\r\n"]
[1108.356, "o", "    method = \"lasso_\" + method\r\n"]
[1108.366, "o", "\r\n"]
[1108.376, "o", "    t0 = time.time()\r\n"]
[1108.386, "o", "    n_samples, n_features = X.shape\r\n"]
[1108.396, "o", "    # Avoid integer division problems\r\n"]
[1108.406, "o", "    alpha = float(alpha)\r\n"]
[1108.416, "o", "    random_state = check_random_state(random_state)\r\n"]
[1108.426, "o", "\r\n"]
[1108.436, "o", "    # Init V with SVD of X\r\n"]
[1108.446, "o", "    if dict_init is not None:\r\n"]
[1108.456, "o", "        dictionary = dict_init\r\n"]
[1108.466, "o", "    else:\r\n"]
[1108.476, "o", "        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n"]
[1108.486, "o", "        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[1108.496, "o", "    r = len(dictionary)\r\n"]
[1108.506, "o", "    if n_components <= r:\r\n"]
[1108.516, "o", "        dictionary = dictionary[:n_components, :]\r\n"]
[1108.526, "o", "    else:\r\n"]
[1108.536, "o", "        dictionary = np.r_[\r\n"]
[1108.546, "o", "            dictionary,\r\n"]
[1108.556, "o", "            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n"]
[1108.566, "o", "        ]\r\n"]
[1108.576, "o", "\r\n"]
[1108.586, "o", "    if verbose == 1:\r\n"]
[1108.596, "o", "        print(\"[dict_learning]\", end=\" \")\r\n"]
[1108.606, "o", "\r\n"]
[1108.616, "o", "    if shuffle:\r\n"]
[1108.626, "o", "        X_train = X.copy()\r\n"]
[1108.636, "o", "        random_state.shuffle(X_train)\r\n"]
[1108.646, "o", "    else:\r\n"]
[1108.656, "o", "        X_train = X\r\n"]
[1108.666, "o", "\r\n"]
[1108.676, "o", "    X_train = check_array(\r\n"]
[1108.686, "o", "        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n"]
[1108.696, "o", "    )\r\n"]
[1108.706, "o", "\r\n"]
[1108.716, "o", "    # Fortran-order dict better suited for the sparse coding which is the\r\n"]
[1108.726, "o", "    # bottleneck of this algorithm.\r\n"]
[1108.736, "o", "    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n"]
[1108.746, "o", "    dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[1108.756, "o", "\r\n"]
[1108.766, "o", "    batches = gen_batches(n_samples, batch_size)\r\n"]
[1108.776, "o", "    batches = itertools.cycle(batches)\r\n"]
[1108.786, "o", "\r\n"]
[1108.796, "o", "    # The covariance of the dictionary\r\n"]
[1108.806, "o", "    if inner_stats is None:\r\n"]
[1108.816, "o", "        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n"]
[1108.826, "o", "        # The data approximation\r\n"]
[1108.836, "o", "        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n"]
[1108.846, "o", "    else:\r\n"]
[1108.856, "o", "        A = inner_stats[0].copy()\r\n"]
[1108.866, "o", "        B = inner_stats[1].copy()\r\n"]
[1108.876, "o", "\r\n"]
[1108.886, "o", "    # If n_iter is zero, we need to return zero.\r\n"]
[1108.896, "o", "    ii = iter_offset - 1\r\n"]
[1108.906, "o", "\r\n"]
[1108.916, "o", "    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n"]
[1108.926, "o", "        this_X = X_train[batch]\r\n"]
[1108.936, "o", "        dt = time.time() - t0\r\n"]
[1108.946, "o", "        if verbose == 1:\r\n"]
[1108.956, "o", "            sys.stdout.write(\".\")\r\n"]
[1108.966, "o", "            sys.stdout.flush()\r\n"]
[1108.976, "o", "        elif verbose:\r\n"]
[1108.986, "o", "            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n"]
[1108.996, "o", "                print(\r\n"]
[1109.006, "o", "                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n"]
[1109.016, "o", "                )\r\n"]
[1109.026, "o", "\r\n"]
[1109.036, "o", "        this_code = sparse_encode(\r\n"]
[1109.046, "o", "            this_X,\r\n"]
[1109.056, "o", "            dictionary,\r\n"]
[1109.066, "o", "            algorithm=method,\r\n"]
[1109.076, "o", "            alpha=alpha,\r\n"]
[1109.086, "o", "            n_jobs=n_jobs,\r\n"]
[1109.096, "o", "            check_input=False,\r\n"]
[1109.106, "o", "            positive=positive_code,\r\n"]
[1109.116, "o", "            max_iter=method_max_iter,\r\n"]
[1109.126, "o", "            verbose=verbose,\r\n"]
[1109.136, "o", "        )\r\n"]
[1109.146, "o", "\r\n"]
[1109.156, "o", "        # Update the auxiliary variables\r\n"]
[1109.166, "o", "        if ii < batch_size - 1:\r\n"]
[1109.176, "o", "            theta = float((ii + 1) * batch_size)\r\n"]
[1109.186, "o", "        else:\r\n"]
[1109.196, "o", "            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n"]
[1109.206, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[1109.216, "o", "\r\n"]
[1109.226, "o", "        A *= beta\r\n"]
[1109.236, "o", "        A += np.dot(this_code.T, this_code)\r\n"]
[1109.246, "o", "        B *= beta\r\n"]
[1109.256, "o", "        B += np.dot(this_X.T, this_code)\r\n"]
[1109.266, "o", "\r\n"]
[1109.276, "o", "        # Update dictionary in place\r\n"]
[1109.286, "o", "        _update_dict(\r\n"]
[1109.296, "o", "            dictionary,\r\n"]
[1109.306, "o", "            this_X,\r\n"]
[1109.316, "o", "            this_code,\r\n"]
[1109.326, "o", "            A,\r\n"]
[1109.336, "o", "            B,\r\n"]
[1109.346, "o", "            verbose=verbose,\r\n"]
[1109.356, "o", "            random_state=random_state,\r\n"]
[1109.366, "o", "            positive=positive_dict,\r\n"]
[1109.376, "o", "        )\r\n"]
[1109.386, "o", "\r\n"]
[1109.396, "o", "        # Maybe we need a stopping criteria based on the amount of\r\n"]
[1109.406, "o", "        # modification in the dictionary\r\n"]
[1109.416, "o", "        if callback is not None:\r\n"]
[1109.426, "o", "            callback(locals())\r\n"]
[1109.436, "o", "\r\n"]
[1109.446, "o", "    if return_inner_stats:\r\n"]
[1109.456, "o", "        if return_n_iter:\r\n"]
[1109.466, "o", "            return dictionary, (A, B), ii - iter_offset + 1\r\n"]
[1109.476, "o", "        else:\r\n"]
[1109.486, "o", "            return dictionary, (A, B)\r\n"]
[1109.496, "o", "    if return_code:\r\n"]
[1109.506, "o", "        if verbose > 1:\r\n"]
[1109.516, "o", "            print(\"Learning code...\", end=\" \")\r\n"]
[1109.526, "o", "        elif verbose == 1:\r\n"]
[1109.536, "o", "            print(\"|\", end=\" \")\r\n"]
[1109.546, "o", "        code = sparse_encode(\r\n"]
[1109.556, "o", "            X,\r\n"]
[1109.566, "o", "            dictionary,\r\n"]
[1109.576, "o", "            algorithm=method,\r\n"]
[1109.586, "o", "            alpha=alpha,\r\n"]
[1109.596, "o", "            n_jobs=n_jobs,\r\n"]
[1109.606, "o", "            check_input=False,\r\n"]
[1109.616, "o", "            positive=positive_code,\r\n"]
[1109.626, "o", "            max_iter=method_max_iter,\r\n"]
[1109.636, "o", "            verbose=verbose,\r\n"]
[1109.646, "o", "        )\r\n"]
[1109.656, "o", "        if verbose > 1:\r\n"]
[1109.666, "o", "            dt = time.time() - t0\r\n"]
[1109.676, "o", "            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n"]
[1109.686, "o", "        if return_n_iter:\r\n"]
[1109.696, "o", "            return code, dictionary, ii - iter_offset + 1\r\n"]
[1109.706, "o", "        else:\r\n"]
[1109.716, "o", "            return code, dictionary\r\n"]
[1109.726, "o", "\r\n"]
[1109.736, "o", "    if return_n_iter:\r\n"]
[1109.746, "o", "        return dictionary, ii - iter_offset + 1\r\n"]
[1109.756, "o", "    else:\r\n"]
[1109.766, "o", "        return dictionary\r\n"]
[1109.776, "o", "\r\n"]
[1109.786, "o", "\r\n"]
[1109.796, "o", "@validate_params(\r\n"]
[1109.806, "o", "    {\r\n"]
[1109.816, "o", "        \"X\": [\"array-like\"],\r\n"]
[1109.826, "o", "        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[1109.836, "o", "        \"return_n_iter\": [\"boolean\"],\r\n"]
[1109.846, "o", "        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1109.856, "o", "    },\r\n"]
[1109.866, "o", "    prefer_skip_nested_validation=False,\r\n"]
[1109.876, "o", ")\r\n"]
[1109.886, "o", "def dict_learning(\r\n"]
[1109.896, "o", "    X,\r\n"]
[1109.906, "o", "    n_components,\r\n"]
[1109.916, "o", "    *,\r\n"]
[1109.926, "o", "    alpha,\r\n"]
[1109.936, "o", "    max_iter=100,\r\n"]
[1110.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1110.002, "i", "sed -n '1400,2200p' sklearn/decomposition/_dict_learning.py\r"]
[1110.004, "o", "sed -n '1400,2200p' sklearn/decomposition/_dict_learning.py\r\n"]
[1110.016, "o", "\u001b[?2004l\r\n"]
[1110.026, "o", "        .. versionadded:: 1.0\r\n"]
[1110.036, "o", "\r\n"]
[1110.046, "o", "    See Also\r\n"]
[1110.056, "o", "    --------\r\n"]
[1110.066, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1110.076, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n"]
[1110.086, "o", "        dictionary learning algorithm.\r\n"]
[1110.096, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1110.106, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1110.116, "o", "    sparse_encode : Sparse coding where each row of the result is the solution\r\n"]
[1110.126, "o", "        to a sparse coding problem.\r\n"]
[1110.136, "o", "\r\n"]
[1110.146, "o", "    Examples\r\n"]
[1110.156, "o", "    --------\r\n"]
[1110.166, "o", "    >>> import numpy as np\r\n"]
[1110.176, "o", "    >>> from sklearn.decomposition import SparseCoder\r\n"]
[1110.186, "o", "    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n"]
[1110.196, "o", "    >>> dictionary = np.array(\r\n"]
[1110.206, "o", "    ...     [[0, 1, 0],\r\n"]
[1110.216, "o", "    ...      [-1, -1, 2],\r\n"]
[1110.226, "o", "    ...      [1, 1, 1],\r\n"]
[1110.236, "o", "    ...      [0, 1, 1],\r\n"]
[1110.246, "o", "    ...      [0, 2, 1]],\r\n"]
[1110.256, "o", "    ...    dtype=np.float64\r\n"]
[1110.266, "o", "    ... )\r\n"]
[1110.276, "o", "    >>> coder = SparseCoder(\r\n"]
[1110.286, "o", "    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n"]
[1110.296, "o", "    ...     transform_alpha=1e-10,\r\n"]
[1110.306, "o", "    ... )\r\n"]
[1110.316, "o", "    >>> coder.transform(X)\r\n"]
[1110.326, "o", "    array([[ 0.,  0., -1.,  0.,  0.],\r\n"]
[1110.336, "o", "           [ 0.,  1.,  1.,  0.,  0.]])\r\n"]
[1110.346, "o", "    \"\"\"\r\n"]
[1110.356, "o", "\r\n"]
[1110.366, "o", "    _required_parameters = [\"dictionary\"]\r\n"]
[1110.376, "o", "\r\n"]
[1110.386, "o", "    def __init__(\r\n"]
[1110.396, "o", "        self,\r\n"]
[1110.406, "o", "        dictionary,\r\n"]
[1110.416, "o", "        *,\r\n"]
[1110.426, "o", "        transform_algorithm=\"omp\",\r\n"]
[1110.436, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[1110.446, "o", "        transform_alpha=None,\r\n"]
[1110.456, "o", "        split_sign=False,\r\n"]
[1110.466, "o", "        n_jobs=None,\r\n"]
[1110.476, "o", "        positive_code=False,\r\n"]
[1110.486, "o", "        transform_max_iter=1000,\r\n"]
[1110.496, "o", "    ):\r\n"]
[1110.506, "o", "        super().__init__(\r\n"]
[1110.516, "o", "            transform_algorithm,\r\n"]
[1110.526, "o", "            transform_n_nonzero_coefs,\r\n"]
[1110.536, "o", "            transform_alpha,\r\n"]
[1110.546, "o", "            split_sign,\r\n"]
[1110.556, "o", "            n_jobs,\r\n"]
[1110.566, "o", "            positive_code,\r\n"]
[1110.576, "o", "            transform_max_iter,\r\n"]
[1110.586, "o", "        )\r\n"]
[1110.596, "o", "        self.dictionary = dictionary\r\n"]
[1110.606, "o", "\r\n"]
[1110.616, "o", "    def fit(self, X, y=None):\r\n"]
[1110.626, "o", "        \"\"\"Do nothing and return the estimator unchanged.\r\n"]
[1110.636, "o", "\r\n"]
[1110.646, "o", "        This method is just there to implement the usual API and hence\r\n"]
[1110.656, "o", "        work in pipelines.\r\n"]
[1110.666, "o", "\r\n"]
[1110.676, "o", "        Parameters\r\n"]
[1110.686, "o", "        ----------\r\n"]
[1110.696, "o", "        X : Ignored\r\n"]
[1110.706, "o", "            Not used, present for API consistency by convention.\r\n"]
[1110.716, "o", "\r\n"]
[1110.726, "o", "        y : Ignored\r\n"]
[1110.736, "o", "            Not used, present for API consistency by convention.\r\n"]
[1110.746, "o", "\r\n"]
[1110.756, "o", "        Returns\r\n"]
[1110.766, "o", "        -------\r\n"]
[1110.776, "o", "        self : object\r\n"]
[1110.786, "o", "            Returns the instance itself.\r\n"]
[1110.796, "o", "        \"\"\"\r\n"]
[1110.806, "o", "        return self\r\n"]
[1110.816, "o", "\r\n"]
[1110.826, "o", "    def transform(self, X, y=None):\r\n"]
[1110.836, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[1110.846, "o", "\r\n"]
[1110.856, "o", "        Coding method is determined by the object parameter\r\n"]
[1110.866, "o", "        `transform_algorithm`.\r\n"]
[1110.876, "o", "\r\n"]
[1110.886, "o", "        Parameters\r\n"]
[1110.896, "o", "        ----------\r\n"]
[1110.906, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[1110.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1110.926, "o", "            and `n_features` is the number of features.\r\n"]
[1110.936, "o", "\r\n"]
[1110.946, "o", "        y : Ignored\r\n"]
[1110.956, "o", "            Not used, present for API consistency by convention.\r\n"]
[1110.966, "o", "\r\n"]
[1110.976, "o", "        Returns\r\n"]
[1110.986, "o", "        -------\r\n"]
[1110.996, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[1111.006, "o", "            Transformed data.\r\n"]
[1111.016, "o", "        \"\"\"\r\n"]
[1111.026, "o", "        return super()._transform(X, self.dictionary)\r\n"]
[1111.036, "o", "\r\n"]
[1111.046, "o", "    def _more_tags(self):\r\n"]
[1111.056, "o", "        return {\r\n"]
[1111.066, "o", "            \"requires_fit\": False,\r\n"]
[1111.076, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[1111.086, "o", "        }\r\n"]
[1111.096, "o", "\r\n"]
[1111.106, "o", "    @property\r\n"]
[1111.116, "o", "    def n_components_(self):\r\n"]
[1111.126, "o", "        \"\"\"Number of atoms.\"\"\"\r\n"]
[1111.136, "o", "        return self.dictionary.shape[0]\r\n"]
[1111.146, "o", "\r\n"]
[1111.156, "o", "    @property\r\n"]
[1111.166, "o", "    def n_features_in_(self):\r\n"]
[1111.176, "o", "        \"\"\"Number of features seen during `fit`.\"\"\"\r\n"]
[1111.186, "o", "        return self.dictionary.shape[1]\r\n"]
[1111.196, "o", "\r\n"]
[1111.206, "o", "    @property\r\n"]
[1111.216, "o", "    def _n_features_out(self):\r\n"]
[1111.226, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[1111.236, "o", "        return self.n_components_\r\n"]
[1111.246, "o", "\r\n"]
[1111.256, "o", "\r\n"]
[1111.266, "o", "class DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[1111.276, "o", "    \"\"\"Dictionary learning.\r\n"]
[1111.286, "o", "\r\n"]
[1111.296, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[1111.306, "o", "    encoding the fitted data.\r\n"]
[1111.316, "o", "\r\n"]
[1111.326, "o", "    Solves the optimization problem::\r\n"]
[1111.336, "o", "\r\n"]
[1111.346, "o", "        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[1111.356, "o", "                    (U,V)\r\n"]
[1111.366, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[1111.376, "o", "\r\n"]
[1111.386, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[1111.396, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[1111.406, "o", "    of all the entries in the matrix.\r\n"]
[1111.416, "o", "\r\n"]
[1111.426, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[1111.436, "o", "\r\n"]
[1111.446, "o", "    Parameters\r\n"]
[1111.456, "o", "    ----------\r\n"]
[1111.466, "o", "    n_components : int, default=None\r\n"]
[1111.476, "o", "        Number of dictionary elements to extract. If None, then ``n_components``\r\n"]
[1111.486, "o", "        is set to ``n_features``.\r\n"]
[1111.496, "o", "\r\n"]
[1111.506, "o", "    alpha : float, default=1.0\r\n"]
[1111.516, "o", "        Sparsity controlling parameter.\r\n"]
[1111.526, "o", "\r\n"]
[1111.536, "o", "    max_iter : int, default=1000\r\n"]
[1111.546, "o", "        Maximum number of iterations to perform.\r\n"]
[1111.556, "o", "\r\n"]
[1111.566, "o", "    tol : float, default=1e-8\r\n"]
[1111.576, "o", "        Tolerance for numerical error.\r\n"]
[1111.586, "o", "\r\n"]
[1111.596, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[1111.606, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[1111.616, "o", "          problem (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[1111.626, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[1111.636, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n"]
[1111.646, "o", "          faster if the estimated components are sparse.\r\n"]
[1111.656, "o", "\r\n"]
[1111.666, "o", "        .. versionadded:: 0.17\r\n"]
[1111.676, "o", "           *cd* coordinate descent method to improve speed.\r\n"]
[1111.686, "o", "\r\n"]
[1111.696, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[1111.706, "o", "            'threshold'}, default='omp'\r\n"]
[1111.716, "o", "        Algorithm used to transform the data:\r\n"]
[1111.726, "o", "\r\n"]
[1111.736, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[1111.746, "o", "          (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[1111.756, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[1111.766, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1111.776, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n"]
[1111.786, "o", "          will be faster if the estimated components are sparse.\r\n"]
[1111.796, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1111.806, "o", "          solution.\r\n"]
[1111.816, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[1111.826, "o", "          the projection ``dictionary * X'``.\r\n"]
[1111.836, "o", "\r\n"]
[1111.846, "o", "        .. versionadded:: 0.17\r\n"]
[1111.856, "o", "           *lasso_cd* coordinate descent method to improve speed.\r\n"]
[1111.866, "o", "\r\n"]
[1111.876, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[1111.886, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1111.896, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[1111.906, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[1111.916, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1111.926, "o", "\r\n"]
[1111.936, "o", "    transform_alpha : float, default=None\r\n"]
[1111.946, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1111.956, "o", "        penalty applied to the L1 norm.\r\n"]
[1111.966, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1111.976, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1111.986, "o", "        If `None`, defaults to `alpha`.\r\n"]
[1111.996, "o", "\r\n"]
[1112.006, "o", "        .. versionchanged:: 1.2\r\n"]
[1112.016, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[1112.026, "o", "\r\n"]
[1112.036, "o", "    n_jobs : int or None, default=None\r\n"]
[1112.046, "o", "        Number of parallel jobs to run.\r\n"]
[1112.056, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1112.066, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1112.076, "o", "        for more details.\r\n"]
[1112.086, "o", "\r\n"]
[1112.096, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1112.106, "o", "        Initial value for the code, for warm restart. Only used if `code_init`\r\n"]
[1112.116, "o", "        and `dict_init` are not None.\r\n"]
[1112.126, "o", "\r\n"]
[1112.136, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1112.146, "o", "        Initial values for the dictionary, for warm restart. Only used if\r\n"]
[1112.156, "o", "        `code_init` and `dict_init` are not None.\r\n"]
[1112.166, "o", "\r\n"]
[1112.176, "o", "    callback : callable, default=None\r\n"]
[1112.186, "o", "        Callable that gets invoked every five iterations.\r\n"]
[1112.196, "o", "\r\n"]
[1112.206, "o", "        .. versionadded:: 1.3\r\n"]
[1112.216, "o", "\r\n"]
[1112.226, "o", "    verbose : bool, default=False\r\n"]
[1112.236, "o", "        To control the verbosity of the procedure.\r\n"]
[1112.246, "o", "\r\n"]
[1112.256, "o", "    split_sign : bool, default=False\r\n"]
[1112.266, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[1112.276, "o", "        its negative part and its positive part. This can improve the\r\n"]
[1112.286, "o", "        performance of downstream classifiers.\r\n"]
[1112.296, "o", "\r\n"]
[1112.306, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1112.316, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[1112.326, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[1112.336, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[1112.346, "o", "        results across multiple function calls.\r\n"]
[1112.356, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1112.366, "o", "\r\n"]
[1112.376, "o", "    positive_code : bool, default=False\r\n"]
[1112.386, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1112.396, "o", "\r\n"]
[1112.406, "o", "        .. versionadded:: 0.20\r\n"]
[1112.416, "o", "\r\n"]
[1112.426, "o", "    positive_dict : bool, default=False\r\n"]
[1112.436, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1112.446, "o", "\r\n"]
[1112.456, "o", "        .. versionadded:: 0.20\r\n"]
[1112.466, "o", "\r\n"]
[1112.476, "o", "    transform_max_iter : int, default=1000\r\n"]
[1112.486, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1112.496, "o", "        `'lasso_lars'`.\r\n"]
[1112.506, "o", "\r\n"]
[1112.516, "o", "        .. versionadded:: 0.22\r\n"]
[1112.526, "o", "\r\n"]
[1112.536, "o", "    Attributes\r\n"]
[1112.546, "o", "    ----------\r\n"]
[1112.556, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[1112.566, "o", "        dictionary atoms extracted from the data\r\n"]
[1112.576, "o", "\r\n"]
[1112.586, "o", "    error_ : array\r\n"]
[1112.596, "o", "        vector of errors at each iteration\r\n"]
[1112.606, "o", "\r\n"]
[1112.616, "o", "    n_features_in_ : int\r\n"]
[1112.626, "o", "        Number of features seen during :term:`fit`.\r\n"]
[1112.636, "o", "\r\n"]
[1112.646, "o", "        .. versionadded:: 0.24\r\n"]
[1112.656, "o", "\r\n"]
[1112.666, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[1112.676, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[1112.686, "o", "        has feature names that are all strings.\r\n"]
[1112.696, "o", "\r\n"]
[1112.706, "o", "        .. versionadded:: 1.0\r\n"]
[1112.716, "o", "\r\n"]
[1112.726, "o", "    n_iter_ : int\r\n"]
[1112.736, "o", "        Number of iterations run.\r\n"]
[1112.746, "o", "\r\n"]
[1112.756, "o", "    See Also\r\n"]
[1112.766, "o", "    --------\r\n"]
[1112.776, "o", "    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n"]
[1112.786, "o", "        dictionary learning algorithm.\r\n"]
[1112.796, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1112.806, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[1112.816, "o", "        precomputed dictionary.\r\n"]
[1112.826, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1112.836, "o", "\r\n"]
[1112.846, "o", "    References\r\n"]
[1112.856, "o", "    ----------\r\n"]
[1112.866, "o", "\r\n"]
[1112.876, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[1112.886, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[1112.896, "o", "\r\n"]
[1112.906, "o", "    Examples\r\n"]
[1112.916, "o", "    --------\r\n"]
[1112.926, "o", "    >>> import numpy as np\r\n"]
[1112.936, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[1112.946, "o", "    >>> from sklearn.decomposition import DictionaryLearning\r\n"]
[1112.956, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[1112.966, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[1112.976, "o", "    ...     random_state=42,\r\n"]
[1112.986, "o", "    ... )\r\n"]
[1112.996, "o", "    >>> dict_learner = DictionaryLearning(\r\n"]
[1113.006, "o", "    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n"]
[1113.016, "o", "    ...     random_state=42,\r\n"]
[1113.026, "o", "    ... )\r\n"]
[1113.036, "o", "    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n"]
[1113.046, "o", "\r\n"]
[1113.056, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[1113.066, "o", "\r\n"]
[1113.076, "o", "    >>> np.mean(X_transformed == 0)\r\n"]
[1113.086, "o", "    0.41...\r\n"]
[1113.096, "o", "\r\n"]
[1113.106, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[1113.116, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[1113.126, "o", "    the original signal:\r\n"]
[1113.136, "o", "\r\n"]
[1113.146, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[1113.156, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[1113.166, "o", "    0.07...\r\n"]
[1113.176, "o", "    \"\"\"\r\n"]
[1113.186, "o", "\r\n"]
[1113.196, "o", "    _parameter_constraints: dict = {\r\n"]
[1113.206, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1113.216, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1113.226, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1113.236, "o", "        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1113.246, "o", "        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[1113.256, "o", "        \"transform_algorithm\": [\r\n"]
[1113.266, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[1113.276, "o", "        ],\r\n"]
[1113.286, "o", "        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1113.296, "o", "        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[1113.306, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[1113.316, "o", "        \"code_init\": [np.ndarray, None],\r\n"]
[1113.326, "o", "        \"dict_init\": [np.ndarray, None],\r\n"]
[1113.336, "o", "        \"callback\": [callable, None],\r\n"]
[1113.346, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[1113.356, "o", "        \"split_sign\": [\"boolean\"],\r\n"]
[1113.366, "o", "        \"random_state\": [\"random_state\"],\r\n"]
[1113.376, "o", "        \"positive_code\": [\"boolean\"],\r\n"]
[1113.386, "o", "        \"positive_dict\": [\"boolean\"],\r\n"]
[1113.396, "o", "        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1113.406, "o", "    }\r\n"]
[1113.416, "o", "\r\n"]
[1113.426, "o", "    def __init__(\r\n"]
[1113.436, "o", "        self,\r\n"]
[1113.446, "o", "        n_components=None,\r\n"]
[1113.456, "o", "        *,\r\n"]
[1113.466, "o", "        alpha=1,\r\n"]
[1113.476, "o", "        max_iter=1000,\r\n"]
[1113.486, "o", "        tol=1e-8,\r\n"]
[1113.496, "o", "        fit_algorithm=\"lars\",\r\n"]
[1113.506, "o", "        transform_algorithm=\"omp\",\r\n"]
[1113.516, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[1113.526, "o", "        transform_alpha=None,\r\n"]
[1113.536, "o", "        n_jobs=None,\r\n"]
[1113.546, "o", "        code_init=None,\r\n"]
[1113.556, "o", "        dict_init=None,\r\n"]
[1113.566, "o", "        callback=None,\r\n"]
[1113.576, "o", "        verbose=False,\r\n"]
[1113.586, "o", "        split_sign=False,\r\n"]
[1113.596, "o", "        random_state=None,\r\n"]
[1113.606, "o", "        positive_code=False,\r\n"]
[1113.616, "o", "        positive_dict=False,\r\n"]
[1113.626, "o", "        transform_max_iter=1000,\r\n"]
[1113.636, "o", "    ):\r\n"]
[1113.646, "o", "        super().__init__(\r\n"]
[1113.656, "o", "            transform_algorithm,\r\n"]
[1113.666, "o", "            transform_n_nonzero_coefs,\r\n"]
[1113.676, "o", "            transform_alpha,\r\n"]
[1113.686, "o", "            split_sign,\r\n"]
[1113.696, "o", "            n_jobs,\r\n"]
[1113.706, "o", "            positive_code,\r\n"]
[1113.716, "o", "            transform_max_iter,\r\n"]
[1113.726, "o", "        )\r\n"]
[1113.736, "o", "        self.n_components = n_components\r\n"]
[1113.746, "o", "        self.alpha = alpha\r\n"]
[1113.756, "o", "        self.max_iter = max_iter\r\n"]
[1113.766, "o", "        self.tol = tol\r\n"]
[1113.776, "o", "        self.fit_algorithm = fit_algorithm\r\n"]
[1113.786, "o", "        self.code_init = code_init\r\n"]
[1113.796, "o", "        self.dict_init = dict_init\r\n"]
[1113.806, "o", "        self.callback = callback\r\n"]
[1113.816, "o", "        self.verbose = verbose\r\n"]
[1113.826, "o", "        self.random_state = random_state\r\n"]
[1113.836, "o", "        self.positive_dict = positive_dict\r\n"]
[1113.846, "o", "\r\n"]
[1113.856, "o", "    def fit(self, X, y=None):\r\n"]
[1113.866, "o", "        \"\"\"Fit the model from data in X.\r\n"]
[1113.876, "o", "\r\n"]
[1113.886, "o", "        Parameters\r\n"]
[1113.896, "o", "        ----------\r\n"]
[1113.906, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[1113.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1113.926, "o", "            and `n_features` is the number of features.\r\n"]
[1113.936, "o", "\r\n"]
[1113.946, "o", "        y : Ignored\r\n"]
[1113.956, "o", "            Not used, present for API consistency by convention.\r\n"]
[1113.966, "o", "\r\n"]
[1113.976, "o", "        Returns\r\n"]
[1113.986, "o", "        -------\r\n"]
[1113.996, "o", "        self : object\r\n"]
[1114.006, "o", "            Returns the instance itself.\r\n"]
[1114.016, "o", "        \"\"\"\r\n"]
[1114.026, "o", "        self.fit_transform(X)\r\n"]
[1114.036, "o", "        return self\r\n"]
[1114.046, "o", "\r\n"]
[1114.056, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[1114.066, "o", "    def fit_transform(self, X, y=None):\r\n"]
[1114.076, "o", "        \"\"\"Fit the model from data in X and return the transformed data.\r\n"]
[1114.086, "o", "\r\n"]
[1114.096, "o", "        Parameters\r\n"]
[1114.106, "o", "        ----------\r\n"]
[1114.116, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[1114.126, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1114.136, "o", "            and `n_features` is the number of features.\r\n"]
[1114.146, "o", "\r\n"]
[1114.156, "o", "        y : Ignored\r\n"]
[1114.166, "o", "            Not used, present for API consistency by convention.\r\n"]
[1114.176, "o", "\r\n"]
[1114.186, "o", "        Returns\r\n"]
[1114.196, "o", "        -------\r\n"]
[1114.206, "o", "        V : ndarray of shape (n_samples, n_components)\r\n"]
[1114.216, "o", "            Transformed data.\r\n"]
[1114.226, "o", "        \"\"\"\r\n"]
[1114.236, "o", "        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n"]
[1114.246, "o", "\r\n"]
[1114.256, "o", "        method = \"lasso_\" + self.fit_algorithm\r\n"]
[1114.266, "o", "\r\n"]
[1114.276, "o", "        random_state = check_random_state(self.random_state)\r\n"]
[1114.286, "o", "        X = self._validate_data(X)\r\n"]
[1114.296, "o", "\r\n"]
[1114.306, "o", "        if self.n_components is None:\r\n"]
[1114.316, "o", "            n_components = X.shape[1]\r\n"]
[1114.326, "o", "        else:\r\n"]
[1114.336, "o", "            n_components = self.n_components\r\n"]
[1114.346, "o", "\r\n"]
[1114.356, "o", "        V, U, E, self.n_iter_ = _dict_learning(\r\n"]
[1114.366, "o", "            X,\r\n"]
[1114.376, "o", "            n_components,\r\n"]
[1114.386, "o", "            alpha=self.alpha,\r\n"]
[1114.396, "o", "            tol=self.tol,\r\n"]
[1114.406, "o", "            max_iter=self.max_iter,\r\n"]
[1114.416, "o", "            method=method,\r\n"]
[1114.426, "o", "            method_max_iter=self.transform_max_iter,\r\n"]
[1114.436, "o", "            n_jobs=self.n_jobs,\r\n"]
[1114.446, "o", "            code_init=self.code_init,\r\n"]
[1114.456, "o", "            dict_init=self.dict_init,\r\n"]
[1114.466, "o", "            callback=self.callback,\r\n"]
[1114.476, "o", "            verbose=self.verbose,\r\n"]
[1114.486, "o", "            random_state=random_state,\r\n"]
[1114.496, "o", "            return_n_iter=True,\r\n"]
[1114.506, "o", "            positive_dict=self.positive_dict,\r\n"]
[1114.516, "o", "            positive_code=self.positive_code,\r\n"]
[1114.526, "o", "        )\r\n"]
[1114.536, "o", "        self.components_ = U\r\n"]
[1114.546, "o", "        self.error_ = E\r\n"]
[1114.556, "o", "\r\n"]
[1114.566, "o", "        return V\r\n"]
[1114.576, "o", "\r\n"]
[1114.586, "o", "    @property\r\n"]
[1114.596, "o", "    def _n_features_out(self):\r\n"]
[1114.606, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[1114.616, "o", "        return self.components_.shape[0]\r\n"]
[1114.626, "o", "\r\n"]
[1114.636, "o", "    def _more_tags(self):\r\n"]
[1114.646, "o", "        return {\r\n"]
[1114.656, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[1114.666, "o", "        }\r\n"]
[1114.676, "o", "\r\n"]
[1114.686, "o", "\r\n"]
[1114.696, "o", "class MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[1114.706, "o", "    \"\"\"Mini-batch dictionary learning.\r\n"]
[1114.716, "o", "\r\n"]
[1114.726, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[1114.736, "o", "    encoding the fitted data.\r\n"]
[1114.746, "o", "\r\n"]
[1114.756, "o", "    Solves the optimization problem::\r\n"]
[1114.766, "o", "\r\n"]
[1114.776, "o", "       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[1114.786, "o", "                    (U,V)\r\n"]
[1114.796, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[1114.806, "o", "\r\n"]
[1114.816, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[1114.826, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[1114.836, "o", "    of all the entries in the matrix.\r\n"]
[1114.846, "o", "\r\n"]
[1114.856, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[1114.866, "o", "\r\n"]
[1114.876, "o", "    Parameters\r\n"]
[1114.886, "o", "    ----------\r\n"]
[1114.896, "o", "    n_components : int, default=None\r\n"]
[1114.906, "o", "        Number of dictionary elements to extract.\r\n"]
[1114.916, "o", "\r\n"]
[1114.926, "o", "    alpha : float, default=1\r\n"]
[1114.936, "o", "        Sparsity controlling parameter.\r\n"]
[1115.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1115.002, "i", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py || grep -n \"_minibatch_step\\(\" -n sklearn/dec\r"]
[1115.004, "o", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py || grep -n \"_minibatch_step\\(\" -n sklearn/dec\r\n"]
[1116.654, "o", "composition/_dict_learning.py\r\n"]
[1118.302, "o", "\u001b[?2004l\r\n"]
[1120.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1120.002, "i", "grep -n \"_sparse_encode_precomputed\\s*\\(\" sklearn/decomposition/_dict_learning.py || true\r"]
[1120.004, "o", "grep -n \"_sparse_encode_precomputed\\s*\\(\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[1122.478, "o", "\u001b[?2004l\r\n"]
[1125.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1125.002, "i", "grep -n \"sparse_encode\\s*\\(\" sklearn/decomposition/_dict_learning.py || true\r"]
[1125.004, "o", "grep -n \"sparse_encode\\s*\\(\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[1127.478, "o", "\u001b[?2004l\r\n"]
[1130.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1130.002, "i", "grep -n \"\\bgram\\s*=\\s|\\bcov\\s*=\\s\" sklearn/decomposition/_dict_learning.py || true\r"]
[1130.004, "o", "grep -n \"\\bgram\\s*=\\s|\\bcov\\s*=\\s\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[1130.006, "o", "\u001b[?2004l\r\n"]
[1135.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1135.002, "i", "sed -n '1,600p' sklearn/decomposition/_dict_learning.py\r"]
[1135.004, "o", "sed -n '1,600p' sklearn/decomposition/_dict_learning.py\r\n"]
[1135.016, "o", "\u001b[?2004l\r\n"]
[1135.026, "o", "\"\"\" Dictionary learning.\r\n"]
[1135.036, "o", "\"\"\"\r\n"]
[1135.046, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[1135.056, "o", "# License: BSD 3 clause\r\n"]
[1135.066, "o", "\r\n"]
[1135.076, "o", "import itertools\r\n"]
[1135.086, "o", "import sys\r\n"]
[1135.096, "o", "import time\r\n"]
[1135.106, "o", "import warnings\r\n"]
[1135.116, "o", "from math import ceil\r\n"]
[1135.126, "o", "from numbers import Integral, Real\r\n"]
[1135.136, "o", "\r\n"]
[1135.146, "o", "import numpy as np\r\n"]
[1135.156, "o", "from joblib import effective_n_jobs\r\n"]
[1135.166, "o", "from scipy import linalg\r\n"]
[1135.176, "o", "\r\n"]
[1135.186, "o", "from ..base import (\r\n"]
[1135.196, "o", "    BaseEstimator,\r\n"]
[1135.206, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[1135.216, "o", "    TransformerMixin,\r\n"]
[1135.226, "o", "    _fit_context,\r\n"]
[1135.236, "o", ")\r\n"]
[1135.246, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[1135.256, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[1135.266, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[1135.276, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[1135.286, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[1135.296, "o", "from ..utils.validation import check_is_fitted\r\n"]
[1135.306, "o", "\r\n"]
[1135.316, "o", "\r\n"]
[1135.326, "o", "def _check_positive_coding(method, positive):\r\n"]
[1135.336, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[1135.346, "o", "        raise ValueError(\r\n"]
[1135.356, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[1135.366, "o", "        )\r\n"]
[1135.376, "o", "\r\n"]
[1135.386, "o", "\r\n"]
[1135.396, "o", "def _sparse_encode_precomputed(\r\n"]
[1135.406, "o", "    X,\r\n"]
[1135.416, "o", "    dictionary,\r\n"]
[1135.426, "o", "    *,\r\n"]
[1135.436, "o", "    gram=None,\r\n"]
[1135.446, "o", "    cov=None,\r\n"]
[1135.456, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1135.466, "o", "    regularization=None,\r\n"]
[1135.476, "o", "    copy_cov=True,\r\n"]
[1135.486, "o", "    init=None,\r\n"]
[1135.496, "o", "    max_iter=1000,\r\n"]
[1135.506, "o", "    verbose=0,\r\n"]
[1135.516, "o", "    positive=False,\r\n"]
[1135.526, "o", "):\r\n"]
[1135.536, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[1135.546, "o", "\r\n"]
[1135.556, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[1135.566, "o", "\r\n"]
[1135.576, "o", "    Parameters\r\n"]
[1135.586, "o", "    ----------\r\n"]
[1135.596, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[1135.606, "o", "        Data matrix.\r\n"]
[1135.616, "o", "\r\n"]
[1135.626, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1135.636, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1135.646, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[1135.656, "o", "\r\n"]
[1135.666, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[1135.676, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[1135.686, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[1135.696, "o", "\r\n"]
[1135.706, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[1135.716, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[1135.726, "o", "\r\n"]
[1135.736, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1135.746, "o", "            default='lasso_lars'\r\n"]
[1135.756, "o", "        The algorithm used:\r\n"]
[1135.766, "o", "\r\n"]
[1135.776, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[1135.786, "o", "          (`linear_model.lars_path`);\r\n"]
[1135.796, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1135.806, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1135.816, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1135.826, "o", "          the estimated components are sparse;\r\n"]
[1135.836, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1135.846, "o", "          solution;\r\n"]
[1135.856, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1135.866, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[1135.876, "o", "\r\n"]
[1135.886, "o", "    regularization : int or float, default=None\r\n"]
[1135.896, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[1135.906, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[1135.916, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[1135.926, "o", "\r\n"]
[1135.936, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1135.946, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[1135.956, "o", "        `algorithm='lasso_cd'`.\r\n"]
[1135.966, "o", "\r\n"]
[1135.976, "o", "    max_iter : int, default=1000\r\n"]
[1135.986, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1135.996, "o", "        `'lasso_lars'`.\r\n"]
[1136.006, "o", "\r\n"]
[1136.016, "o", "    copy_cov : bool, default=True\r\n"]
[1136.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1136.036, "o", "        be overwritten.\r\n"]
[1136.046, "o", "\r\n"]
[1136.056, "o", "    verbose : int, default=0\r\n"]
[1136.066, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[1136.076, "o", "\r\n"]
[1136.086, "o", "    positive: bool, default=False\r\n"]
[1136.096, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[1136.106, "o", "\r\n"]
[1136.116, "o", "        .. versionadded:: 0.20\r\n"]
[1136.126, "o", "\r\n"]
[1136.136, "o", "    Returns\r\n"]
[1136.146, "o", "    -------\r\n"]
[1136.156, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[1136.166, "o", "        The sparse codes.\r\n"]
[1136.176, "o", "    \"\"\"\r\n"]
[1136.186, "o", "    n_samples, n_features = X.shape\r\n"]
[1136.196, "o", "    n_components = dictionary.shape[0]\r\n"]
[1136.206, "o", "\r\n"]
[1136.216, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[1136.226, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1136.236, "o", "        try:\r\n"]
[1136.246, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1136.256, "o", "\r\n"]
[1136.266, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1136.276, "o", "            # corrects the verbosity level.\r\n"]
[1136.286, "o", "            lasso_lars = LassoLars(\r\n"]
[1136.296, "o", "                alpha=alpha,\r\n"]
[1136.306, "o", "                fit_intercept=False,\r\n"]
[1136.316, "o", "                verbose=verbose,\r\n"]
[1136.326, "o", "                precompute=gram,\r\n"]
[1136.336, "o", "                fit_path=False,\r\n"]
[1136.346, "o", "                positive=positive,\r\n"]
[1136.356, "o", "                max_iter=max_iter,\r\n"]
[1136.366, "o", "            )\r\n"]
[1136.376, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1136.386, "o", "            new_code = lasso_lars.coef_\r\n"]
[1136.396, "o", "        finally:\r\n"]
[1136.406, "o", "            np.seterr(**err_mgt)\r\n"]
[1136.416, "o", "\r\n"]
[1136.426, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[1136.436, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1136.446, "o", "\r\n"]
[1136.456, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[1136.466, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[1136.476, "o", "        # argument that we could pass in from Lasso.\r\n"]
[1136.486, "o", "        clf = Lasso(\r\n"]
[1136.496, "o", "            alpha=alpha,\r\n"]
[1136.506, "o", "            fit_intercept=False,\r\n"]
[1136.516, "o", "            precompute=gram,\r\n"]
[1136.526, "o", "            max_iter=max_iter,\r\n"]
[1136.536, "o", "            warm_start=True,\r\n"]
[1136.546, "o", "            positive=positive,\r\n"]
[1136.556, "o", "        )\r\n"]
[1136.566, "o", "\r\n"]
[1136.576, "o", "        if init is not None:\r\n"]
[1136.586, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[1136.596, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[1136.606, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[1136.616, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[1136.626, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[1136.636, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[1136.646, "o", "                init = np.array(init)\r\n"]
[1136.656, "o", "            clf.coef_ = init\r\n"]
[1136.666, "o", "\r\n"]
[1136.676, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[1136.686, "o", "        new_code = clf.coef_\r\n"]
[1136.696, "o", "\r\n"]
[1136.706, "o", "    elif algorithm == \"lars\":\r\n"]
[1136.716, "o", "        try:\r\n"]
[1136.726, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1136.736, "o", "\r\n"]
[1136.746, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1136.756, "o", "            # corrects the verbosity level.\r\n"]
[1136.766, "o", "            lars = Lars(\r\n"]
[1136.776, "o", "                fit_intercept=False,\r\n"]
[1136.786, "o", "                verbose=verbose,\r\n"]
[1136.796, "o", "                precompute=gram,\r\n"]
[1136.806, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[1136.816, "o", "                fit_path=False,\r\n"]
[1136.826, "o", "            )\r\n"]
[1136.836, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1136.846, "o", "            new_code = lars.coef_\r\n"]
[1136.856, "o", "        finally:\r\n"]
[1136.866, "o", "            np.seterr(**err_mgt)\r\n"]
[1136.876, "o", "\r\n"]
[1136.886, "o", "    elif algorithm == \"threshold\":\r\n"]
[1136.896, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[1136.906, "o", "        if positive:\r\n"]
[1136.916, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[1136.926, "o", "\r\n"]
[1136.936, "o", "    elif algorithm == \"omp\":\r\n"]
[1136.946, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[1136.956, "o", "            Gram=gram,\r\n"]
[1136.966, "o", "            Xy=cov,\r\n"]
[1136.976, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[1136.986, "o", "            tol=None,\r\n"]
[1136.996, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[1137.006, "o", "            copy_Xy=copy_cov,\r\n"]
[1137.016, "o", "        ).T\r\n"]
[1137.026, "o", "\r\n"]
[1137.036, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[1137.046, "o", "\r\n"]
[1137.056, "o", "\r\n"]
[1137.066, "o", "@validate_params(\r\n"]
[1137.076, "o", "    {\r\n"]
[1137.086, "o", "        \"X\": [\"array-like\"],\r\n"]
[1137.096, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[1137.106, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[1137.116, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[1137.126, "o", "        \"algorithm\": [\r\n"]
[1137.136, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[1137.146, "o", "        ],\r\n"]
[1137.156, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1137.166, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[1137.176, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[1137.186, "o", "        \"init\": [\"array-like\", None],\r\n"]
[1137.196, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1137.206, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[1137.216, "o", "        \"check_input\": [\"boolean\"],\r\n"]
[1137.226, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[1137.236, "o", "        \"positive\": [\"boolean\"],\r\n"]
[1137.246, "o", "    },\r\n"]
[1137.256, "o", "    prefer_skip_nested_validation=True,\r\n"]
[1137.266, "o", ")\r\n"]
[1137.276, "o", "# XXX : could be moved to the linear_model module\r\n"]
[1137.286, "o", "def sparse_encode(\r\n"]
[1137.296, "o", "    X,\r\n"]
[1137.306, "o", "    dictionary,\r\n"]
[1137.316, "o", "    *,\r\n"]
[1137.326, "o", "    gram=None,\r\n"]
[1137.336, "o", "    cov=None,\r\n"]
[1137.346, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1137.356, "o", "    n_nonzero_coefs=None,\r\n"]
[1137.366, "o", "    alpha=None,\r\n"]
[1137.376, "o", "    copy_cov=True,\r\n"]
[1137.386, "o", "    init=None,\r\n"]
[1137.396, "o", "    max_iter=1000,\r\n"]
[1137.406, "o", "    n_jobs=None,\r\n"]
[1137.416, "o", "    check_input=True,\r\n"]
[1137.426, "o", "    verbose=0,\r\n"]
[1137.436, "o", "    positive=False,\r\n"]
[1137.446, "o", "):\r\n"]
[1137.456, "o", "    \"\"\"Sparse coding.\r\n"]
[1137.466, "o", "\r\n"]
[1137.476, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[1137.486, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[1137.496, "o", "\r\n"]
[1137.506, "o", "        X ~= code * dictionary\r\n"]
[1137.516, "o", "\r\n"]
[1137.526, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[1137.536, "o", "\r\n"]
[1137.546, "o", "    Parameters\r\n"]
[1137.556, "o", "    ----------\r\n"]
[1137.566, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[1137.576, "o", "        Data matrix.\r\n"]
[1137.586, "o", "\r\n"]
[1137.596, "o", "    dictionary : array-like of shape (n_components, n_features)\r\n"]
[1137.606, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1137.616, "o", "        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[1137.626, "o", "        output.\r\n"]
[1137.636, "o", "\r\n"]
[1137.646, "o", "    gram : array-like of shape (n_components, n_components), default=None\r\n"]
[1137.656, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n"]
[1137.666, "o", "\r\n"]
[1137.676, "o", "    cov : array-like of shape (n_components, n_samples), default=None\r\n"]
[1137.686, "o", "        Precomputed covariance, `dictionary' * X`.\r\n"]
[1137.696, "o", "\r\n"]
[1137.706, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1137.716, "o", "            default='lasso_lars'\r\n"]
[1137.726, "o", "        The algorithm used:\r\n"]
[1137.736, "o", "\r\n"]
[1137.746, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[1137.756, "o", "          (`linear_model.lars_path`);\r\n"]
[1137.766, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1137.776, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1137.786, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1137.796, "o", "          the estimated components are sparse;\r\n"]
[1137.806, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1137.816, "o", "          solution;\r\n"]
[1137.826, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1137.836, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[1137.846, "o", "\r\n"]
[1137.856, "o", "    n_nonzero_coefs : int, default=None\r\n"]
[1137.866, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1137.876, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[1137.886, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[1137.896, "o", "        `n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1137.906, "o", "\r\n"]
[1137.916, "o", "    alpha : float, default=None\r\n"]
[1137.926, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1137.936, "o", "        penalty applied to the L1 norm.\r\n"]
[1137.946, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1137.956, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1137.966, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[1137.976, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[1137.986, "o", "        `n_nonzero_coefs`.\r\n"]
[1137.996, "o", "        If `None`, default to 1.\r\n"]
[1138.006, "o", "\r\n"]
[1138.016, "o", "    copy_cov : bool, default=True\r\n"]
[1138.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1138.036, "o", "        be overwritten.\r\n"]
[1138.046, "o", "\r\n"]
[1138.056, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1138.066, "o", "        Initialization value of the sparse codes. Only used if\r\n"]
[1138.076, "o", "        `algorithm='lasso_cd'`.\r\n"]
[1138.086, "o", "\r\n"]
[1138.096, "o", "    max_iter : int, default=1000\r\n"]
[1138.106, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1138.116, "o", "        `'lasso_lars'`.\r\n"]
[1138.126, "o", "\r\n"]
[1138.136, "o", "    n_jobs : int, default=None\r\n"]
[1138.146, "o", "        Number of parallel jobs to run.\r\n"]
[1138.156, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1138.166, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1138.176, "o", "        for more details.\r\n"]
[1138.186, "o", "\r\n"]
[1138.196, "o", "    check_input : bool, default=True\r\n"]
[1138.206, "o", "        If `False`, the input arrays X and dictionary will not be checked.\r\n"]
[1138.216, "o", "\r\n"]
[1138.226, "o", "    verbose : int, default=0\r\n"]
[1138.236, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[1138.246, "o", "\r\n"]
[1138.256, "o", "    positive : bool, default=False\r\n"]
[1138.266, "o", "        Whether to enforce positivity when finding the encoding.\r\n"]
[1138.276, "o", "\r\n"]
[1138.286, "o", "        .. versionadded:: 0.20\r\n"]
[1138.296, "o", "\r\n"]
[1138.306, "o", "    Returns\r\n"]
[1138.316, "o", "    -------\r\n"]
[1138.326, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1138.336, "o", "        The sparse codes.\r\n"]
[1138.346, "o", "\r\n"]
[1138.356, "o", "    See Also\r\n"]
[1138.366, "o", "    --------\r\n"]
[1138.376, "o", "    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n"]
[1138.386, "o", "        path using LARS algorithm.\r\n"]
[1138.396, "o", "    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n"]
[1138.406, "o", "    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n"]
[1138.416, "o", "    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n"]
[1138.426, "o", "        dictionary.\r\n"]
[1138.436, "o", "    \"\"\"\r\n"]
[1138.446, "o", "    if check_input:\r\n"]
[1138.456, "o", "        if algorithm == \"lasso_cd\":\r\n"]
[1138.466, "o", "            dictionary = check_array(\r\n"]
[1138.476, "o", "                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n"]
[1138.486, "o", "            )\r\n"]
[1138.496, "o", "            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n"]
[1138.506, "o", "        else:\r\n"]
[1138.516, "o", "            dictionary = check_array(dictionary)\r\n"]
[1138.526, "o", "            X = check_array(X)\r\n"]
[1138.536, "o", "\r\n"]
[1138.546, "o", "    if dictionary.shape[1] != X.shape[1]:\r\n"]
[1138.556, "o", "        raise ValueError(\r\n"]
[1138.566, "o", "            \"Dictionary and X have different numbers of features:\"\r\n"]
[1138.576, "o", "            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n"]
[1138.586, "o", "        )\r\n"]
[1138.596, "o", "\r\n"]
[1138.606, "o", "    _check_positive_coding(algorithm, positive)\r\n"]
[1138.616, "o", "\r\n"]
[1138.626, "o", "    return _sparse_encode(\r\n"]
[1138.636, "o", "        X,\r\n"]
[1138.646, "o", "        dictionary,\r\n"]
[1138.656, "o", "        gram=gram,\r\n"]
[1138.666, "o", "        cov=cov,\r\n"]
[1138.676, "o", "        algorithm=algorithm,\r\n"]
[1138.686, "o", "        n_nonzero_coefs=n_nonzero_coefs,\r\n"]
[1138.696, "o", "        alpha=alpha,\r\n"]
[1138.706, "o", "        copy_cov=copy_cov,\r\n"]
[1138.716, "o", "        init=init,\r\n"]
[1138.726, "o", "        max_iter=max_iter,\r\n"]
[1138.736, "o", "        n_jobs=n_jobs,\r\n"]
[1138.746, "o", "        verbose=verbose,\r\n"]
[1138.756, "o", "        positive=positive,\r\n"]
[1138.766, "o", "    )\r\n"]
[1138.776, "o", "\r\n"]
[1138.786, "o", "\r\n"]
[1138.796, "o", "def _sparse_encode(\r\n"]
[1138.806, "o", "    X,\r\n"]
[1138.816, "o", "    dictionary,\r\n"]
[1138.826, "o", "    *,\r\n"]
[1138.836, "o", "    gram=None,\r\n"]
[1138.846, "o", "    cov=None,\r\n"]
[1138.856, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1138.866, "o", "    n_nonzero_coefs=None,\r\n"]
[1138.876, "o", "    alpha=None,\r\n"]
[1138.886, "o", "    copy_cov=True,\r\n"]
[1138.896, "o", "    init=None,\r\n"]
[1138.906, "o", "    max_iter=1000,\r\n"]
[1138.916, "o", "    n_jobs=None,\r\n"]
[1138.926, "o", "    verbose=0,\r\n"]
[1138.936, "o", "    positive=False,\r\n"]
[1138.946, "o", "):\r\n"]
[1138.956, "o", "    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[1138.966, "o", "\r\n"]
[1138.976, "o", "    n_samples, n_features = X.shape\r\n"]
[1138.986, "o", "    n_components = dictionary.shape[0]\r\n"]
[1138.996, "o", "\r\n"]
[1139.006, "o", "    if algorithm in (\"lars\", \"omp\"):\r\n"]
[1139.016, "o", "        regularization = n_nonzero_coefs\r\n"]
[1139.026, "o", "        if regularization is None:\r\n"]
[1139.036, "o", "            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[1139.046, "o", "    else:\r\n"]
[1139.056, "o", "        regularization = alpha\r\n"]
[1139.066, "o", "        if regularization is None:\r\n"]
[1139.076, "o", "            regularization = 1.0\r\n"]
[1139.086, "o", "\r\n"]
[1139.096, "o", "    if gram is None and algorithm != \"threshold\":\r\n"]
[1139.106, "o", "        gram = np.dot(dictionary, dictionary.T)\r\n"]
[1139.116, "o", "\r\n"]
[1139.126, "o", "    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[1139.136, "o", "        copy_cov = False\r\n"]
[1139.146, "o", "        cov = np.dot(dictionary, X.T)\r\n"]
[1139.156, "o", "\r\n"]
[1139.166, "o", "    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[1139.176, "o", "        code = _sparse_encode_precomputed(\r\n"]
[1139.186, "o", "            X,\r\n"]
[1139.196, "o", "            dictionary,\r\n"]
[1139.206, "o", "            gram=gram,\r\n"]
[1139.216, "o", "            cov=cov,\r\n"]
[1139.226, "o", "            algorithm=algorithm,\r\n"]
[1139.236, "o", "            regularization=regularization,\r\n"]
[1139.246, "o", "            copy_cov=copy_cov,\r\n"]
[1139.256, "o", "            init=init,\r\n"]
[1139.266, "o", "            max_iter=max_iter,\r\n"]
[1139.276, "o", "            verbose=verbose,\r\n"]
[1139.286, "o", "            positive=positive,\r\n"]
[1139.296, "o", "        )\r\n"]
[1139.306, "o", "        return code\r\n"]
[1139.316, "o", "\r\n"]
[1139.326, "o", "    # Enter parallel code block\r\n"]
[1139.336, "o", "    n_samples = X.shape[0]\r\n"]
[1139.346, "o", "    n_components = dictionary.shape[0]\r\n"]
[1139.356, "o", "    code = np.empty((n_samples, n_components))\r\n"]
[1139.366, "o", "    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[1139.376, "o", "\r\n"]
[1139.386, "o", "    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[1139.396, "o", "        delayed(_sparse_encode_precomputed)(\r\n"]
[1139.406, "o", "            X[this_slice],\r\n"]
[1139.416, "o", "            dictionary,\r\n"]
[1139.426, "o", "            gram=gram,\r\n"]
[1139.436, "o", "            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[1139.446, "o", "            algorithm=algorithm,\r\n"]
[1139.456, "o", "            regularization=regularization,\r\n"]
[1139.466, "o", "            copy_cov=copy_cov,\r\n"]
[1139.476, "o", "            init=init[this_slice] if init is not None else None,\r\n"]
[1139.486, "o", "            max_iter=max_iter,\r\n"]
[1139.496, "o", "            verbose=verbose,\r\n"]
[1139.506, "o", "            positive=positive,\r\n"]
[1139.516, "o", "        )\r\n"]
[1139.526, "o", "        for this_slice in slices\r\n"]
[1139.536, "o", "    )\r\n"]
[1139.546, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[1139.556, "o", "        code[this_slice] = this_view\r\n"]
[1139.566, "o", "    return code\r\n"]
[1139.576, "o", "\r\n"]
[1139.586, "o", "\r\n"]
[1139.596, "o", "def _update_dict(\r\n"]
[1139.606, "o", "    dictionary,\r\n"]
[1139.616, "o", "    Y,\r\n"]
[1139.626, "o", "    code,\r\n"]
[1139.636, "o", "    A=None,\r\n"]
[1139.646, "o", "    B=None,\r\n"]
[1139.656, "o", "    verbose=False,\r\n"]
[1139.666, "o", "    random_state=None,\r\n"]
[1139.676, "o", "    positive=False,\r\n"]
[1139.686, "o", "):\r\n"]
[1139.696, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[1139.706, "o", "\r\n"]
[1139.716, "o", "    Parameters\r\n"]
[1139.726, "o", "    ----------\r\n"]
[1139.736, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1139.746, "o", "        Value of the dictionary at the previous iteration.\r\n"]
[1139.756, "o", "\r\n"]
[1139.766, "o", "    Y : ndarray of shape (n_samples, n_features)\r\n"]
[1139.776, "o", "        Data matrix.\r\n"]
[1139.786, "o", "\r\n"]
[1139.796, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1139.806, "o", "        Sparse coding of the data against which to optimize the dictionary.\r\n"]
[1139.816, "o", "\r\n"]
[1139.826, "o", "    A : ndarray of shape (n_components, n_components), default=None\r\n"]
[1139.836, "o", "        Together with `B`, sufficient stats of the online model to update the\r\n"]
[1139.846, "o", "        dictionary.\r\n"]
[1139.856, "o", "\r\n"]
[1139.866, "o", "    B : ndarray of shape (n_features, n_components), default=None\r\n"]
[1139.876, "o", "        Together with `A`, sufficient stats of the online model to update the\r\n"]
[1139.886, "o", "        dictionary.\r\n"]
[1139.896, "o", "\r\n"]
[1139.906, "o", "    verbose: bool, default=False\r\n"]
[1139.916, "o", "        Degree of output the procedure will print.\r\n"]
[1139.926, "o", "\r\n"]
[1139.936, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1140.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1140.002, "i", "sed -n '600,1400p' sklearn/decomposition/_dict_learning.py\r"]
[1140.004, "o", "sed -n '600,1400p' sklearn/decomposition/_dict_learning.py\r\n"]
[1140.016, "o", "\u001b[?2004l\r\n"]
[1140.026, "o", "                % (ii, dt, dt / 60, current_cost)\r\n"]
[1140.036, "o", "            )\r\n"]
[1140.046, "o", "\r\n"]
[1140.056, "o", "        # Update code\r\n"]
[1140.066, "o", "        code = sparse_encode(\r\n"]
[1140.076, "o", "            X,\r\n"]
[1140.086, "o", "            dictionary,\r\n"]
[1140.096, "o", "            algorithm=method,\r\n"]
[1140.106, "o", "            alpha=alpha,\r\n"]
[1140.116, "o", "            init=code,\r\n"]
[1140.126, "o", "            n_jobs=n_jobs,\r\n"]
[1140.136, "o", "            positive=positive_code,\r\n"]
[1140.146, "o", "            max_iter=method_max_iter,\r\n"]
[1140.156, "o", "            verbose=verbose,\r\n"]
[1140.166, "o", "        )\r\n"]
[1140.176, "o", "\r\n"]
[1140.186, "o", "        # Update dictionary in place\r\n"]
[1140.196, "o", "        _update_dict(\r\n"]
[1140.206, "o", "            dictionary,\r\n"]
[1140.216, "o", "            X,\r\n"]
[1140.226, "o", "            code,\r\n"]
[1140.236, "o", "            verbose=verbose,\r\n"]
[1140.246, "o", "            random_state=random_state,\r\n"]
[1140.256, "o", "            positive=positive_dict,\r\n"]
[1140.266, "o", "        )\r\n"]
[1140.276, "o", "\r\n"]
[1140.286, "o", "        # Cost function\r\n"]
[1140.296, "o", "        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n"]
[1140.306, "o", "            np.abs(code)\r\n"]
[1140.316, "o", "        )\r\n"]
[1140.326, "o", "        errors.append(current_cost)\r\n"]
[1140.336, "o", "\r\n"]
[1140.346, "o", "        if ii > 0:\r\n"]
[1140.356, "o", "            dE = errors[-2] - errors[-1]\r\n"]
[1140.366, "o", "            # assert(dE >= -tol * errors[-1])\r\n"]
[1140.376, "o", "            if dE < tol * errors[-1]:\r\n"]
[1140.386, "o", "                if verbose == 1:\r\n"]
[1140.396, "o", "                    # A line return\r\n"]
[1140.406, "o", "                    print(\"\")\r\n"]
[1140.416, "o", "                elif verbose:\r\n"]
[1140.426, "o", "                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n"]
[1140.436, "o", "                break\r\n"]
[1140.446, "o", "        if ii % 5 == 0 and callback is not None:\r\n"]
[1140.456, "o", "            callback(locals())\r\n"]
[1140.466, "o", "\r\n"]
[1140.476, "o", "    if return_n_iter:\r\n"]
[1140.486, "o", "        return code, dictionary, errors, ii + 1\r\n"]
[1140.496, "o", "    else:\r\n"]
[1140.506, "o", "        return code, dictionary, errors\r\n"]
[1140.516, "o", "\r\n"]
[1140.526, "o", "\r\n"]
[1140.536, "o", "def _check_warn_deprecated(param, name, default, additional_message=None):\r\n"]
[1140.546, "o", "    if param != \"deprecated\":\r\n"]
[1140.556, "o", "        msg = (\r\n"]
[1140.566, "o", "            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n"]
[1140.576, "o", "        )\r\n"]
[1140.586, "o", "        if additional_message:\r\n"]
[1140.596, "o", "            msg += f\" {additional_message}\"\r\n"]
[1140.606, "o", "        warnings.warn(msg, FutureWarning)\r\n"]
[1140.616, "o", "        return param\r\n"]
[1140.626, "o", "    else:\r\n"]
[1140.636, "o", "        return default\r\n"]
[1140.646, "o", "\r\n"]
[1140.656, "o", "\r\n"]
[1140.666, "o", "def dict_learning_online(\r\n"]
[1140.676, "o", "    X,\r\n"]
[1140.686, "o", "    n_components=2,\r\n"]
[1140.696, "o", "    *,\r\n"]
[1140.706, "o", "    alpha=1,\r\n"]
[1140.716, "o", "    n_iter=\"deprecated\",\r\n"]
[1140.726, "o", "    max_iter=None,\r\n"]
[1140.736, "o", "    return_code=True,\r\n"]
[1140.746, "o", "    dict_init=None,\r\n"]
[1140.756, "o", "    callback=None,\r\n"]
[1140.766, "o", "    batch_size=256,\r\n"]
[1140.776, "o", "    verbose=False,\r\n"]
[1140.786, "o", "    shuffle=True,\r\n"]
[1140.796, "o", "    n_jobs=None,\r\n"]
[1140.806, "o", "    method=\"lars\",\r\n"]
[1140.816, "o", "    iter_offset=\"deprecated\",\r\n"]
[1140.826, "o", "    random_state=None,\r\n"]
[1140.836, "o", "    return_inner_stats=\"deprecated\",\r\n"]
[1140.846, "o", "    inner_stats=\"deprecated\",\r\n"]
[1140.856, "o", "    return_n_iter=\"deprecated\",\r\n"]
[1140.866, "o", "    positive_dict=False,\r\n"]
[1140.876, "o", "    positive_code=False,\r\n"]
[1140.886, "o", "    method_max_iter=1000,\r\n"]
[1140.896, "o", "    tol=1e-3,\r\n"]
[1140.906, "o", "    max_no_improvement=10,\r\n"]
[1140.916, "o", "):\r\n"]
[1140.926, "o", "    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n"]
[1140.936, "o", "\r\n"]
[1140.946, "o", "    Finds the best dictionary and the corresponding sparse code for\r\n"]
[1140.956, "o", "    approximating the data matrix X by solving::\r\n"]
[1140.966, "o", "\r\n"]
[1140.976, "o", "        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[1140.986, "o", "                     (U,V)\r\n"]
[1140.996, "o", "                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n"]
[1141.006, "o", "\r\n"]
[1141.016, "o", "    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n"]
[1141.026, "o", "    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n"]
[1141.036, "o", "    which is the sum of the absolute values of all the entries in the matrix.\r\n"]
[1141.046, "o", "    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n"]
[1141.056, "o", "    the input data.\r\n"]
[1141.066, "o", "\r\n"]
[1141.076, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[1141.086, "o", "\r\n"]
[1141.096, "o", "    Parameters\r\n"]
[1141.106, "o", "    ----------\r\n"]
[1141.116, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[1141.126, "o", "        Data matrix.\r\n"]
[1141.136, "o", "\r\n"]
[1141.146, "o", "    n_components : int or None, default=2\r\n"]
[1141.156, "o", "        Number of dictionary atoms to extract. If None, then ``n_components``\r\n"]
[1141.166, "o", "        is set to ``n_features``.\r\n"]
[1141.176, "o", "\r\n"]
[1141.186, "o", "    alpha : float, default=1\r\n"]
[1141.196, "o", "        Sparsity controlling parameter.\r\n"]
[1141.206, "o", "\r\n"]
[1141.216, "o", "    n_iter : int, default=100\r\n"]
[1141.226, "o", "        Number of mini-batch iterations to perform.\r\n"]
[1141.236, "o", "\r\n"]
[1141.246, "o", "        .. deprecated:: 1.1\r\n"]
[1141.256, "o", "           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[1141.266, "o", "           `max_iter` instead.\r\n"]
[1141.276, "o", "\r\n"]
[1141.286, "o", "    max_iter : int, default=None\r\n"]
[1141.296, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[1141.306, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[1141.316, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[1141.326, "o", "\r\n"]
[1141.336, "o", "        .. versionadded:: 1.1\r\n"]
[1141.346, "o", "\r\n"]
[1141.356, "o", "    return_code : bool, default=True\r\n"]
[1141.366, "o", "        Whether to also return the code U or just the dictionary `V`.\r\n"]
[1141.376, "o", "\r\n"]
[1141.386, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1141.396, "o", "        Initial values for the dictionary for warm restart scenarios.\r\n"]
[1141.406, "o", "        If `None`, the initial values for the dictionary are created\r\n"]
[1141.416, "o", "        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n"]
[1141.426, "o", "\r\n"]
[1141.436, "o", "    callback : callable, default=None\r\n"]
[1141.446, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[1141.456, "o", "\r\n"]
[1141.466, "o", "    batch_size : int, default=256\r\n"]
[1141.476, "o", "        The number of samples to take in each batch.\r\n"]
[1141.486, "o", "\r\n"]
[1141.496, "o", "        .. versionchanged:: 1.3\r\n"]
[1141.506, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[1141.516, "o", "\r\n"]
[1141.526, "o", "    verbose : bool, default=False\r\n"]
[1141.536, "o", "        To control the verbosity of the procedure.\r\n"]
[1141.546, "o", "\r\n"]
[1141.556, "o", "    shuffle : bool, default=True\r\n"]
[1141.566, "o", "        Whether to shuffle the data before splitting it in batches.\r\n"]
[1141.576, "o", "\r\n"]
[1141.586, "o", "    n_jobs : int, default=None\r\n"]
[1141.596, "o", "        Number of parallel jobs to run.\r\n"]
[1141.606, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1141.616, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1141.626, "o", "        for more details.\r\n"]
[1141.636, "o", "\r\n"]
[1141.646, "o", "    method : {'lars', 'cd'}, default='lars'\r\n"]
[1141.656, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[1141.666, "o", "          problem (`linear_model.lars_path`);\r\n"]
[1141.676, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[1141.686, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[1141.696, "o", "          the estimated components are sparse.\r\n"]
[1141.706, "o", "\r\n"]
[1141.716, "o", "    iter_offset : int, default=0\r\n"]
[1141.726, "o", "        Number of previous iterations completed on the dictionary used for\r\n"]
[1141.736, "o", "        initialization.\r\n"]
[1141.746, "o", "\r\n"]
[1141.756, "o", "        .. deprecated:: 1.1\r\n"]
[1141.766, "o", "           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n"]
[1141.776, "o", "\r\n"]
[1141.786, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1141.796, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[1141.806, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[1141.816, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[1141.826, "o", "        results across multiple function calls.\r\n"]
[1141.836, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1141.846, "o", "\r\n"]
[1141.856, "o", "    return_inner_stats : bool, default=False\r\n"]
[1141.866, "o", "        Return the inner statistics A (dictionary covariance) and B\r\n"]
[1141.876, "o", "        (data approximation). Useful to restart the algorithm in an\r\n"]
[1141.886, "o", "        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n"]
[1141.896, "o", "        ignored.\r\n"]
[1141.906, "o", "\r\n"]
[1141.916, "o", "        .. deprecated:: 1.1\r\n"]
[1141.926, "o", "           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[1141.936, "o", "\r\n"]
[1141.946, "o", "    inner_stats : tuple of (A, B) ndarrays, default=None\r\n"]
[1141.956, "o", "        Inner sufficient statistics that are kept by the algorithm.\r\n"]
[1141.966, "o", "        Passing them at initialization is useful in online settings, to\r\n"]
[1141.976, "o", "        avoid losing the history of the evolution.\r\n"]
[1141.986, "o", "        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n"]
[1141.996, "o", "        `B` `(n_features, n_components)` is the data approximation matrix.\r\n"]
[1142.006, "o", "\r\n"]
[1142.016, "o", "        .. deprecated:: 1.1\r\n"]
[1142.026, "o", "           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[1142.036, "o", "\r\n"]
[1142.046, "o", "    return_n_iter : bool, default=False\r\n"]
[1142.056, "o", "        Whether or not to return the number of iterations.\r\n"]
[1142.066, "o", "\r\n"]
[1142.076, "o", "        .. deprecated:: 1.1\r\n"]
[1142.086, "o", "           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n"]
[1142.096, "o", "\r\n"]
[1142.106, "o", "    positive_dict : bool, default=False\r\n"]
[1142.116, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1142.126, "o", "\r\n"]
[1142.136, "o", "        .. versionadded:: 0.20\r\n"]
[1142.146, "o", "\r\n"]
[1142.156, "o", "    positive_code : bool, default=False\r\n"]
[1142.166, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1142.176, "o", "\r\n"]
[1142.186, "o", "        .. versionadded:: 0.20\r\n"]
[1142.196, "o", "\r\n"]
[1142.206, "o", "    method_max_iter : int, default=1000\r\n"]
[1142.216, "o", "        Maximum number of iterations to perform when solving the lasso problem.\r\n"]
[1142.226, "o", "\r\n"]
[1142.236, "o", "        .. versionadded:: 0.22\r\n"]
[1142.246, "o", "\r\n"]
[1142.256, "o", "    tol : float, default=1e-3\r\n"]
[1142.266, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[1142.276, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[1142.286, "o", "\r\n"]
[1142.296, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[1142.306, "o", "        `tol` to 0.0.\r\n"]
[1142.316, "o", "\r\n"]
[1142.326, "o", "        .. versionadded:: 1.1\r\n"]
[1142.336, "o", "\r\n"]
[1142.346, "o", "    max_no_improvement : int, default=10\r\n"]
[1142.356, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[1142.366, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[1142.376, "o", "        `max_iter` is not None.\r\n"]
[1142.386, "o", "\r\n"]
[1142.396, "o", "        To disable convergence detection based on cost function, set\r\n"]
[1142.406, "o", "        `max_no_improvement` to None.\r\n"]
[1142.416, "o", "\r\n"]
[1142.426, "o", "        .. versionadded:: 1.1\r\n"]
[1142.436, "o", "\r\n"]
[1142.446, "o", "    Returns\r\n"]
[1142.456, "o", "    -------\r\n"]
[1142.466, "o", "    code : ndarray of shape (n_samples, n_components),\r\n"]
[1142.476, "o", "        The sparse code (only returned if `return_code=True`).\r\n"]
[1142.486, "o", "\r\n"]
[1142.496, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[1142.506, "o", "        The solutions to the dictionary learning problem.\r\n"]
[1142.516, "o", "\r\n"]
[1142.526, "o", "    n_iter : int\r\n"]
[1142.536, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[1142.546, "o", "        set to `True`.\r\n"]
[1142.556, "o", "\r\n"]
[1142.566, "o", "    See Also\r\n"]
[1142.576, "o", "    --------\r\n"]
[1142.586, "o", "    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n"]
[1142.596, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1142.606, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n"]
[1142.616, "o", "        learning algorithm.\r\n"]
[1142.626, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1142.636, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1142.646, "o", "    \"\"\"\r\n"]
[1142.656, "o", "    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n"]
[1142.666, "o", "    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n"]
[1142.676, "o", "        raise ValueError(\r\n"]
[1142.686, "o", "            \"The following arguments are incompatible with 'max_iter': \"\r\n"]
[1142.696, "o", "            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n"]
[1142.706, "o", "        )\r\n"]
[1142.716, "o", "\r\n"]
[1142.726, "o", "    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n"]
[1142.736, "o", "    return_inner_stats = _check_warn_deprecated(\r\n"]
[1142.746, "o", "        return_inner_stats,\r\n"]
[1142.756, "o", "        \"return_inner_stats\",\r\n"]
[1142.766, "o", "        default=False,\r\n"]
[1142.776, "o", "        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n"]
[1142.786, "o", "    )\r\n"]
[1142.796, "o", "    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n"]
[1142.806, "o", "    return_n_iter = _check_warn_deprecated(\r\n"]
[1142.816, "o", "        return_n_iter,\r\n"]
[1142.826, "o", "        \"return_n_iter\",\r\n"]
[1142.836, "o", "        default=False,\r\n"]
[1142.846, "o", "        additional_message=(\r\n"]
[1142.856, "o", "            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n"]
[1142.866, "o", "            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n"]
[1142.876, "o", "        ),\r\n"]
[1142.886, "o", "    )\r\n"]
[1142.896, "o", "\r\n"]
[1142.906, "o", "    if max_iter is not None:\r\n"]
[1142.916, "o", "        transform_algorithm = \"lasso_\" + method\r\n"]
[1142.926, "o", "\r\n"]
[1142.936, "o", "        est = MiniBatchDictionaryLearning(\r\n"]
[1142.946, "o", "            n_components=n_components,\r\n"]
[1142.956, "o", "            alpha=alpha,\r\n"]
[1142.966, "o", "            n_iter=n_iter,\r\n"]
[1142.976, "o", "            n_jobs=n_jobs,\r\n"]
[1142.986, "o", "            fit_algorithm=method,\r\n"]
[1142.996, "o", "            batch_size=batch_size,\r\n"]
[1143.006, "o", "            shuffle=shuffle,\r\n"]
[1143.016, "o", "            dict_init=dict_init,\r\n"]
[1143.026, "o", "            random_state=random_state,\r\n"]
[1143.036, "o", "            transform_algorithm=transform_algorithm,\r\n"]
[1143.046, "o", "            transform_alpha=alpha,\r\n"]
[1143.056, "o", "            positive_code=positive_code,\r\n"]
[1143.066, "o", "            positive_dict=positive_dict,\r\n"]
[1143.076, "o", "            transform_max_iter=method_max_iter,\r\n"]
[1143.086, "o", "            verbose=verbose,\r\n"]
[1143.096, "o", "            callback=callback,\r\n"]
[1143.106, "o", "            tol=tol,\r\n"]
[1143.116, "o", "            max_no_improvement=max_no_improvement,\r\n"]
[1143.126, "o", "        ).fit(X)\r\n"]
[1143.136, "o", "\r\n"]
[1143.146, "o", "        if not return_code:\r\n"]
[1143.156, "o", "            return est.components_\r\n"]
[1143.166, "o", "        else:\r\n"]
[1143.176, "o", "            code = est.transform(X)\r\n"]
[1143.186, "o", "            return code, est.components_\r\n"]
[1143.196, "o", "\r\n"]
[1143.206, "o", "    # TODO(1.4) remove the whole old behavior\r\n"]
[1143.216, "o", "    # Fallback to old behavior\r\n"]
[1143.226, "o", "\r\n"]
[1143.236, "o", "    n_iter = _check_warn_deprecated(\r\n"]
[1143.246, "o", "        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n"]
[1143.256, "o", "    )\r\n"]
[1143.266, "o", "\r\n"]
[1143.276, "o", "    if n_components is None:\r\n"]
[1143.286, "o", "        n_components = X.shape[1]\r\n"]
[1143.296, "o", "\r\n"]
[1143.306, "o", "    if method not in (\"lars\", \"cd\"):\r\n"]
[1143.316, "o", "        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n"]
[1143.326, "o", "\r\n"]
[1143.336, "o", "    _check_positive_coding(method, positive_code)\r\n"]
[1143.346, "o", "\r\n"]
[1143.356, "o", "    method = \"lasso_\" + method\r\n"]
[1143.366, "o", "\r\n"]
[1143.376, "o", "    t0 = time.time()\r\n"]
[1143.386, "o", "    n_samples, n_features = X.shape\r\n"]
[1143.396, "o", "    # Avoid integer division problems\r\n"]
[1143.406, "o", "    alpha = float(alpha)\r\n"]
[1143.416, "o", "    random_state = check_random_state(random_state)\r\n"]
[1143.426, "o", "\r\n"]
[1143.436, "o", "    # Init V with SVD of X\r\n"]
[1143.446, "o", "    if dict_init is not None:\r\n"]
[1143.456, "o", "        dictionary = dict_init\r\n"]
[1143.466, "o", "    else:\r\n"]
[1143.476, "o", "        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n"]
[1143.486, "o", "        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[1143.496, "o", "    r = len(dictionary)\r\n"]
[1143.506, "o", "    if n_components <= r:\r\n"]
[1143.516, "o", "        dictionary = dictionary[:n_components, :]\r\n"]
[1143.526, "o", "    else:\r\n"]
[1143.536, "o", "        dictionary = np.r_[\r\n"]
[1143.546, "o", "            dictionary,\r\n"]
[1143.556, "o", "            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n"]
[1143.566, "o", "        ]\r\n"]
[1143.576, "o", "\r\n"]
[1143.586, "o", "    if verbose == 1:\r\n"]
[1143.596, "o", "        print(\"[dict_learning]\", end=\" \")\r\n"]
[1143.606, "o", "\r\n"]
[1143.616, "o", "    if shuffle:\r\n"]
[1143.626, "o", "        X_train = X.copy()\r\n"]
[1143.636, "o", "        random_state.shuffle(X_train)\r\n"]
[1143.646, "o", "    else:\r\n"]
[1143.656, "o", "        X_train = X\r\n"]
[1143.666, "o", "\r\n"]
[1143.676, "o", "    X_train = check_array(\r\n"]
[1143.686, "o", "        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n"]
[1143.696, "o", "    )\r\n"]
[1143.706, "o", "\r\n"]
[1143.716, "o", "    # Fortran-order dict better suited for the sparse coding which is the\r\n"]
[1143.726, "o", "    # bottleneck of this algorithm.\r\n"]
[1143.736, "o", "    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n"]
[1143.746, "o", "    dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[1143.756, "o", "\r\n"]
[1143.766, "o", "    batches = gen_batches(n_samples, batch_size)\r\n"]
[1143.776, "o", "    batches = itertools.cycle(batches)\r\n"]
[1143.786, "o", "\r\n"]
[1143.796, "o", "    # The covariance of the dictionary\r\n"]
[1143.806, "o", "    if inner_stats is None:\r\n"]
[1143.816, "o", "        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n"]
[1143.826, "o", "        # The data approximation\r\n"]
[1143.836, "o", "        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n"]
[1143.846, "o", "    else:\r\n"]
[1143.856, "o", "        A = inner_stats[0].copy()\r\n"]
[1143.866, "o", "        B = inner_stats[1].copy()\r\n"]
[1143.876, "o", "\r\n"]
[1143.886, "o", "    # If n_iter is zero, we need to return zero.\r\n"]
[1143.896, "o", "    ii = iter_offset - 1\r\n"]
[1143.906, "o", "\r\n"]
[1143.916, "o", "    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n"]
[1143.926, "o", "        this_X = X_train[batch]\r\n"]
[1143.936, "o", "        dt = time.time() - t0\r\n"]
[1143.946, "o", "        if verbose == 1:\r\n"]
[1143.956, "o", "            sys.stdout.write(\".\")\r\n"]
[1143.966, "o", "            sys.stdout.flush()\r\n"]
[1143.976, "o", "        elif verbose:\r\n"]
[1143.986, "o", "            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n"]
[1143.996, "o", "                print(\r\n"]
[1144.006, "o", "                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n"]
[1144.016, "o", "                )\r\n"]
[1144.026, "o", "\r\n"]
[1144.036, "o", "        this_code = sparse_encode(\r\n"]
[1144.046, "o", "            this_X,\r\n"]
[1144.056, "o", "            dictionary,\r\n"]
[1144.066, "o", "            algorithm=method,\r\n"]
[1144.076, "o", "            alpha=alpha,\r\n"]
[1144.086, "o", "            n_jobs=n_jobs,\r\n"]
[1144.096, "o", "            check_input=False,\r\n"]
[1144.106, "o", "            positive=positive_code,\r\n"]
[1144.116, "o", "            max_iter=method_max_iter,\r\n"]
[1144.126, "o", "            verbose=verbose,\r\n"]
[1144.136, "o", "        )\r\n"]
[1144.146, "o", "\r\n"]
[1144.156, "o", "        # Update the auxiliary variables\r\n"]
[1144.166, "o", "        if ii < batch_size - 1:\r\n"]
[1144.176, "o", "            theta = float((ii + 1) * batch_size)\r\n"]
[1144.186, "o", "        else:\r\n"]
[1144.196, "o", "            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n"]
[1144.206, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[1144.216, "o", "\r\n"]
[1144.226, "o", "        A *= beta\r\n"]
[1144.236, "o", "        A += np.dot(this_code.T, this_code)\r\n"]
[1144.246, "o", "        B *= beta\r\n"]
[1144.256, "o", "        B += np.dot(this_X.T, this_code)\r\n"]
[1144.266, "o", "\r\n"]
[1144.276, "o", "        # Update dictionary in place\r\n"]
[1144.286, "o", "        _update_dict(\r\n"]
[1144.296, "o", "            dictionary,\r\n"]
[1144.306, "o", "            this_X,\r\n"]
[1144.316, "o", "            this_code,\r\n"]
[1144.326, "o", "            A,\r\n"]
[1144.336, "o", "            B,\r\n"]
[1144.346, "o", "            verbose=verbose,\r\n"]
[1144.356, "o", "            random_state=random_state,\r\n"]
[1144.366, "o", "            positive=positive_dict,\r\n"]
[1144.376, "o", "        )\r\n"]
[1144.386, "o", "\r\n"]
[1144.396, "o", "        # Maybe we need a stopping criteria based on the amount of\r\n"]
[1144.406, "o", "        # modification in the dictionary\r\n"]
[1144.416, "o", "        if callback is not None:\r\n"]
[1144.426, "o", "            callback(locals())\r\n"]
[1144.436, "o", "\r\n"]
[1144.446, "o", "    if return_inner_stats:\r\n"]
[1144.456, "o", "        if return_n_iter:\r\n"]
[1144.466, "o", "            return dictionary, (A, B), ii - iter_offset + 1\r\n"]
[1144.476, "o", "        else:\r\n"]
[1144.486, "o", "            return dictionary, (A, B)\r\n"]
[1144.496, "o", "    if return_code:\r\n"]
[1144.506, "o", "        if verbose > 1:\r\n"]
[1144.516, "o", "            print(\"Learning code...\", end=\" \")\r\n"]
[1144.526, "o", "        elif verbose == 1:\r\n"]
[1144.536, "o", "            print(\"|\", end=\" \")\r\n"]
[1144.546, "o", "        code = sparse_encode(\r\n"]
[1144.556, "o", "            X,\r\n"]
[1144.566, "o", "            dictionary,\r\n"]
[1144.576, "o", "            algorithm=method,\r\n"]
[1144.586, "o", "            alpha=alpha,\r\n"]
[1144.596, "o", "            n_jobs=n_jobs,\r\n"]
[1144.606, "o", "            check_input=False,\r\n"]
[1144.616, "o", "            positive=positive_code,\r\n"]
[1144.626, "o", "            max_iter=method_max_iter,\r\n"]
[1144.636, "o", "            verbose=verbose,\r\n"]
[1144.646, "o", "        )\r\n"]
[1144.656, "o", "        if verbose > 1:\r\n"]
[1144.666, "o", "            dt = time.time() - t0\r\n"]
[1144.676, "o", "            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n"]
[1144.686, "o", "        if return_n_iter:\r\n"]
[1144.696, "o", "            return code, dictionary, ii - iter_offset + 1\r\n"]
[1144.706, "o", "        else:\r\n"]
[1144.716, "o", "            return code, dictionary\r\n"]
[1144.726, "o", "\r\n"]
[1144.736, "o", "    if return_n_iter:\r\n"]
[1144.746, "o", "        return dictionary, ii - iter_offset + 1\r\n"]
[1144.756, "o", "    else:\r\n"]
[1144.766, "o", "        return dictionary\r\n"]
[1144.776, "o", "\r\n"]
[1144.786, "o", "\r\n"]
[1144.796, "o", "@validate_params(\r\n"]
[1144.806, "o", "    {\r\n"]
[1144.816, "o", "        \"X\": [\"array-like\"],\r\n"]
[1144.826, "o", "        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[1144.836, "o", "        \"return_n_iter\": [\"boolean\"],\r\n"]
[1144.846, "o", "        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1144.856, "o", "    },\r\n"]
[1144.866, "o", "    prefer_skip_nested_validation=False,\r\n"]
[1144.876, "o", ")\r\n"]
[1144.886, "o", "def dict_learning(\r\n"]
[1144.896, "o", "    X,\r\n"]
[1144.906, "o", "    n_components,\r\n"]
[1144.916, "o", "    *,\r\n"]
[1144.926, "o", "    alpha,\r\n"]
[1144.936, "o", "    max_iter=100,\r\n"]
[1145.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1145.002, "i", "sed -n '1400,2200p' sklearn/decomposition/_dict_learning.py\r"]
[1145.004, "o", "sed -n '1400,2200p' sklearn/decomposition/_dict_learning.py\r\n"]
[1145.016, "o", "\u001b[?2004l\r\n"]
[1145.026, "o", "        .. versionadded:: 1.0\r\n"]
[1145.036, "o", "\r\n"]
[1145.046, "o", "    See Also\r\n"]
[1145.056, "o", "    --------\r\n"]
[1145.066, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1145.076, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n"]
[1145.086, "o", "        dictionary learning algorithm.\r\n"]
[1145.096, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1145.106, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1145.116, "o", "    sparse_encode : Sparse coding where each row of the result is the solution\r\n"]
[1145.126, "o", "        to a sparse coding problem.\r\n"]
[1145.136, "o", "\r\n"]
[1145.146, "o", "    Examples\r\n"]
[1145.156, "o", "    --------\r\n"]
[1145.166, "o", "    >>> import numpy as np\r\n"]
[1145.176, "o", "    >>> from sklearn.decomposition import SparseCoder\r\n"]
[1145.186, "o", "    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n"]
[1145.196, "o", "    >>> dictionary = np.array(\r\n"]
[1145.206, "o", "    ...     [[0, 1, 0],\r\n"]
[1145.216, "o", "    ...      [-1, -1, 2],\r\n"]
[1145.226, "o", "    ...      [1, 1, 1],\r\n"]
[1145.236, "o", "    ...      [0, 1, 1],\r\n"]
[1145.246, "o", "    ...      [0, 2, 1]],\r\n"]
[1145.256, "o", "    ...    dtype=np.float64\r\n"]
[1145.266, "o", "    ... )\r\n"]
[1145.276, "o", "    >>> coder = SparseCoder(\r\n"]
[1145.286, "o", "    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n"]
[1145.296, "o", "    ...     transform_alpha=1e-10,\r\n"]
[1145.306, "o", "    ... )\r\n"]
[1145.316, "o", "    >>> coder.transform(X)\r\n"]
[1145.326, "o", "    array([[ 0.,  0., -1.,  0.,  0.],\r\n"]
[1145.336, "o", "           [ 0.,  1.,  1.,  0.,  0.]])\r\n"]
[1145.346, "o", "    \"\"\"\r\n"]
[1145.356, "o", "\r\n"]
[1145.366, "o", "    _required_parameters = [\"dictionary\"]\r\n"]
[1145.376, "o", "\r\n"]
[1145.386, "o", "    def __init__(\r\n"]
[1145.396, "o", "        self,\r\n"]
[1145.406, "o", "        dictionary,\r\n"]
[1145.416, "o", "        *,\r\n"]
[1145.426, "o", "        transform_algorithm=\"omp\",\r\n"]
[1145.436, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[1145.446, "o", "        transform_alpha=None,\r\n"]
[1145.456, "o", "        split_sign=False,\r\n"]
[1145.466, "o", "        n_jobs=None,\r\n"]
[1145.476, "o", "        positive_code=False,\r\n"]
[1145.486, "o", "        transform_max_iter=1000,\r\n"]
[1145.496, "o", "    ):\r\n"]
[1145.506, "o", "        super().__init__(\r\n"]
[1145.516, "o", "            transform_algorithm,\r\n"]
[1145.526, "o", "            transform_n_nonzero_coefs,\r\n"]
[1145.536, "o", "            transform_alpha,\r\n"]
[1145.546, "o", "            split_sign,\r\n"]
[1145.556, "o", "            n_jobs,\r\n"]
[1145.566, "o", "            positive_code,\r\n"]
[1145.576, "o", "            transform_max_iter,\r\n"]
[1145.586, "o", "        )\r\n"]
[1145.596, "o", "        self.dictionary = dictionary\r\n"]
[1145.606, "o", "\r\n"]
[1145.616, "o", "    def fit(self, X, y=None):\r\n"]
[1145.626, "o", "        \"\"\"Do nothing and return the estimator unchanged.\r\n"]
[1145.636, "o", "\r\n"]
[1145.646, "o", "        This method is just there to implement the usual API and hence\r\n"]
[1145.656, "o", "        work in pipelines.\r\n"]
[1145.666, "o", "\r\n"]
[1145.676, "o", "        Parameters\r\n"]
[1145.686, "o", "        ----------\r\n"]
[1145.696, "o", "        X : Ignored\r\n"]
[1145.706, "o", "            Not used, present for API consistency by convention.\r\n"]
[1145.716, "o", "\r\n"]
[1145.726, "o", "        y : Ignored\r\n"]
[1145.736, "o", "            Not used, present for API consistency by convention.\r\n"]
[1145.746, "o", "\r\n"]
[1145.756, "o", "        Returns\r\n"]
[1145.766, "o", "        -------\r\n"]
[1145.776, "o", "        self : object\r\n"]
[1145.786, "o", "            Returns the instance itself.\r\n"]
[1145.796, "o", "        \"\"\"\r\n"]
[1145.806, "o", "        return self\r\n"]
[1145.816, "o", "\r\n"]
[1145.826, "o", "    def transform(self, X, y=None):\r\n"]
[1145.836, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[1145.846, "o", "\r\n"]
[1145.856, "o", "        Coding method is determined by the object parameter\r\n"]
[1145.866, "o", "        `transform_algorithm`.\r\n"]
[1145.876, "o", "\r\n"]
[1145.886, "o", "        Parameters\r\n"]
[1145.896, "o", "        ----------\r\n"]
[1145.906, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[1145.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1145.926, "o", "            and `n_features` is the number of features.\r\n"]
[1145.936, "o", "\r\n"]
[1145.946, "o", "        y : Ignored\r\n"]
[1145.956, "o", "            Not used, present for API consistency by convention.\r\n"]
[1145.966, "o", "\r\n"]
[1145.976, "o", "        Returns\r\n"]
[1145.986, "o", "        -------\r\n"]
[1145.996, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[1146.006, "o", "            Transformed data.\r\n"]
[1146.016, "o", "        \"\"\"\r\n"]
[1146.026, "o", "        return super()._transform(X, self.dictionary)\r\n"]
[1146.036, "o", "\r\n"]
[1146.046, "o", "    def _more_tags(self):\r\n"]
[1146.056, "o", "        return {\r\n"]
[1146.066, "o", "            \"requires_fit\": False,\r\n"]
[1146.076, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[1146.086, "o", "        }\r\n"]
[1146.096, "o", "\r\n"]
[1146.106, "o", "    @property\r\n"]
[1146.116, "o", "    def n_components_(self):\r\n"]
[1146.126, "o", "        \"\"\"Number of atoms.\"\"\"\r\n"]
[1146.136, "o", "        return self.dictionary.shape[0]\r\n"]
[1146.146, "o", "\r\n"]
[1146.156, "o", "    @property\r\n"]
[1146.166, "o", "    def n_features_in_(self):\r\n"]
[1146.176, "o", "        \"\"\"Number of features seen during `fit`.\"\"\"\r\n"]
[1146.186, "o", "        return self.dictionary.shape[1]\r\n"]
[1146.196, "o", "\r\n"]
[1146.206, "o", "    @property\r\n"]
[1146.216, "o", "    def _n_features_out(self):\r\n"]
[1146.226, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[1146.236, "o", "        return self.n_components_\r\n"]
[1146.246, "o", "\r\n"]
[1146.256, "o", "\r\n"]
[1146.266, "o", "class DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[1146.276, "o", "    \"\"\"Dictionary learning.\r\n"]
[1146.286, "o", "\r\n"]
[1146.296, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[1146.306, "o", "    encoding the fitted data.\r\n"]
[1146.316, "o", "\r\n"]
[1146.326, "o", "    Solves the optimization problem::\r\n"]
[1146.336, "o", "\r\n"]
[1146.346, "o", "        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[1146.356, "o", "                    (U,V)\r\n"]
[1146.366, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[1146.376, "o", "\r\n"]
[1146.386, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[1146.396, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[1146.406, "o", "    of all the entries in the matrix.\r\n"]
[1146.416, "o", "\r\n"]
[1146.426, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[1146.436, "o", "\r\n"]
[1146.446, "o", "    Parameters\r\n"]
[1146.456, "o", "    ----------\r\n"]
[1146.466, "o", "    n_components : int, default=None\r\n"]
[1146.476, "o", "        Number of dictionary elements to extract. If None, then ``n_components``\r\n"]
[1146.486, "o", "        is set to ``n_features``.\r\n"]
[1146.496, "o", "\r\n"]
[1146.506, "o", "    alpha : float, default=1.0\r\n"]
[1146.516, "o", "        Sparsity controlling parameter.\r\n"]
[1146.526, "o", "\r\n"]
[1146.536, "o", "    max_iter : int, default=1000\r\n"]
[1146.546, "o", "        Maximum number of iterations to perform.\r\n"]
[1146.556, "o", "\r\n"]
[1146.566, "o", "    tol : float, default=1e-8\r\n"]
[1146.576, "o", "        Tolerance for numerical error.\r\n"]
[1146.586, "o", "\r\n"]
[1146.596, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[1146.606, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[1146.616, "o", "          problem (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[1146.626, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[1146.636, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n"]
[1146.646, "o", "          faster if the estimated components are sparse.\r\n"]
[1146.656, "o", "\r\n"]
[1146.666, "o", "        .. versionadded:: 0.17\r\n"]
[1146.676, "o", "           *cd* coordinate descent method to improve speed.\r\n"]
[1146.686, "o", "\r\n"]
[1146.696, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[1146.706, "o", "            'threshold'}, default='omp'\r\n"]
[1146.716, "o", "        Algorithm used to transform the data:\r\n"]
[1146.726, "o", "\r\n"]
[1146.736, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[1146.746, "o", "          (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[1146.756, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[1146.766, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1146.776, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n"]
[1146.786, "o", "          will be faster if the estimated components are sparse.\r\n"]
[1146.796, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1146.806, "o", "          solution.\r\n"]
[1146.816, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[1146.826, "o", "          the projection ``dictionary * X'``.\r\n"]
[1146.836, "o", "\r\n"]
[1146.846, "o", "        .. versionadded:: 0.17\r\n"]
[1146.856, "o", "           *lasso_cd* coordinate descent method to improve speed.\r\n"]
[1146.866, "o", "\r\n"]
[1146.876, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[1146.886, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1146.896, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[1146.906, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[1146.916, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1146.926, "o", "\r\n"]
[1146.936, "o", "    transform_alpha : float, default=None\r\n"]
[1146.946, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1146.956, "o", "        penalty applied to the L1 norm.\r\n"]
[1146.966, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1146.976, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1146.986, "o", "        If `None`, defaults to `alpha`.\r\n"]
[1146.996, "o", "\r\n"]
[1147.006, "o", "        .. versionchanged:: 1.2\r\n"]
[1147.016, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[1147.026, "o", "\r\n"]
[1147.036, "o", "    n_jobs : int or None, default=None\r\n"]
[1147.046, "o", "        Number of parallel jobs to run.\r\n"]
[1147.056, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1147.066, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1147.076, "o", "        for more details.\r\n"]
[1147.086, "o", "\r\n"]
[1147.096, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1147.106, "o", "        Initial value for the code, for warm restart. Only used if `code_init`\r\n"]
[1147.116, "o", "        and `dict_init` are not None.\r\n"]
[1147.126, "o", "\r\n"]
[1147.136, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1147.146, "o", "        Initial values for the dictionary, for warm restart. Only used if\r\n"]
[1147.156, "o", "        `code_init` and `dict_init` are not None.\r\n"]
[1147.166, "o", "\r\n"]
[1147.176, "o", "    callback : callable, default=None\r\n"]
[1147.186, "o", "        Callable that gets invoked every five iterations.\r\n"]
[1147.196, "o", "\r\n"]
[1147.206, "o", "        .. versionadded:: 1.3\r\n"]
[1147.216, "o", "\r\n"]
[1147.226, "o", "    verbose : bool, default=False\r\n"]
[1147.236, "o", "        To control the verbosity of the procedure.\r\n"]
[1147.246, "o", "\r\n"]
[1147.256, "o", "    split_sign : bool, default=False\r\n"]
[1147.266, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[1147.276, "o", "        its negative part and its positive part. This can improve the\r\n"]
[1147.286, "o", "        performance of downstream classifiers.\r\n"]
[1147.296, "o", "\r\n"]
[1147.306, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1147.316, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[1147.326, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[1147.336, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[1147.346, "o", "        results across multiple function calls.\r\n"]
[1147.356, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1147.366, "o", "\r\n"]
[1147.376, "o", "    positive_code : bool, default=False\r\n"]
[1147.386, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1147.396, "o", "\r\n"]
[1147.406, "o", "        .. versionadded:: 0.20\r\n"]
[1147.416, "o", "\r\n"]
[1147.426, "o", "    positive_dict : bool, default=False\r\n"]
[1147.436, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1147.446, "o", "\r\n"]
[1147.456, "o", "        .. versionadded:: 0.20\r\n"]
[1147.466, "o", "\r\n"]
[1147.476, "o", "    transform_max_iter : int, default=1000\r\n"]
[1147.486, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1147.496, "o", "        `'lasso_lars'`.\r\n"]
[1147.506, "o", "\r\n"]
[1147.516, "o", "        .. versionadded:: 0.22\r\n"]
[1147.526, "o", "\r\n"]
[1147.536, "o", "    Attributes\r\n"]
[1147.546, "o", "    ----------\r\n"]
[1147.556, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[1147.566, "o", "        dictionary atoms extracted from the data\r\n"]
[1147.576, "o", "\r\n"]
[1147.586, "o", "    error_ : array\r\n"]
[1147.596, "o", "        vector of errors at each iteration\r\n"]
[1147.606, "o", "\r\n"]
[1147.616, "o", "    n_features_in_ : int\r\n"]
[1147.626, "o", "        Number of features seen during :term:`fit`.\r\n"]
[1147.636, "o", "\r\n"]
[1147.646, "o", "        .. versionadded:: 0.24\r\n"]
[1147.656, "o", "\r\n"]
[1147.666, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[1147.676, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[1147.686, "o", "        has feature names that are all strings.\r\n"]
[1147.696, "o", "\r\n"]
[1147.706, "o", "        .. versionadded:: 1.0\r\n"]
[1147.716, "o", "\r\n"]
[1147.726, "o", "    n_iter_ : int\r\n"]
[1147.736, "o", "        Number of iterations run.\r\n"]
[1147.746, "o", "\r\n"]
[1147.756, "o", "    See Also\r\n"]
[1147.766, "o", "    --------\r\n"]
[1147.776, "o", "    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n"]
[1147.786, "o", "        dictionary learning algorithm.\r\n"]
[1147.796, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1147.806, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[1147.816, "o", "        precomputed dictionary.\r\n"]
[1147.826, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1147.836, "o", "\r\n"]
[1147.846, "o", "    References\r\n"]
[1147.856, "o", "    ----------\r\n"]
[1147.866, "o", "\r\n"]
[1147.876, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[1147.886, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[1147.896, "o", "\r\n"]
[1147.906, "o", "    Examples\r\n"]
[1147.916, "o", "    --------\r\n"]
[1147.926, "o", "    >>> import numpy as np\r\n"]
[1147.936, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[1147.946, "o", "    >>> from sklearn.decomposition import DictionaryLearning\r\n"]
[1147.956, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[1147.966, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[1147.976, "o", "    ...     random_state=42,\r\n"]
[1147.986, "o", "    ... )\r\n"]
[1147.996, "o", "    >>> dict_learner = DictionaryLearning(\r\n"]
[1148.006, "o", "    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n"]
[1148.016, "o", "    ...     random_state=42,\r\n"]
[1148.026, "o", "    ... )\r\n"]
[1148.036, "o", "    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n"]
[1148.046, "o", "\r\n"]
[1148.056, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[1148.066, "o", "\r\n"]
[1148.076, "o", "    >>> np.mean(X_transformed == 0)\r\n"]
[1148.086, "o", "    0.41...\r\n"]
[1148.096, "o", "\r\n"]
[1148.106, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[1148.116, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[1148.126, "o", "    the original signal:\r\n"]
[1148.136, "o", "\r\n"]
[1148.146, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[1148.156, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[1148.166, "o", "    0.07...\r\n"]
[1148.176, "o", "    \"\"\"\r\n"]
[1148.186, "o", "\r\n"]
[1148.196, "o", "    _parameter_constraints: dict = {\r\n"]
[1148.206, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1148.216, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1148.226, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1148.236, "o", "        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1148.246, "o", "        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[1148.256, "o", "        \"transform_algorithm\": [\r\n"]
[1148.266, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[1148.276, "o", "        ],\r\n"]
[1148.286, "o", "        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1148.296, "o", "        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[1148.306, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[1148.316, "o", "        \"code_init\": [np.ndarray, None],\r\n"]
[1148.326, "o", "        \"dict_init\": [np.ndarray, None],\r\n"]
[1148.336, "o", "        \"callback\": [callable, None],\r\n"]
[1148.346, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[1148.356, "o", "        \"split_sign\": [\"boolean\"],\r\n"]
[1148.366, "o", "        \"random_state\": [\"random_state\"],\r\n"]
[1148.376, "o", "        \"positive_code\": [\"boolean\"],\r\n"]
[1148.386, "o", "        \"positive_dict\": [\"boolean\"],\r\n"]
[1148.396, "o", "        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1148.406, "o", "    }\r\n"]
[1148.416, "o", "\r\n"]
[1148.426, "o", "    def __init__(\r\n"]
[1148.436, "o", "        self,\r\n"]
[1148.446, "o", "        n_components=None,\r\n"]
[1148.456, "o", "        *,\r\n"]
[1148.466, "o", "        alpha=1,\r\n"]
[1148.476, "o", "        max_iter=1000,\r\n"]
[1148.486, "o", "        tol=1e-8,\r\n"]
[1148.496, "o", "        fit_algorithm=\"lars\",\r\n"]
[1148.506, "o", "        transform_algorithm=\"omp\",\r\n"]
[1148.516, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[1148.526, "o", "        transform_alpha=None,\r\n"]
[1148.536, "o", "        n_jobs=None,\r\n"]
[1148.546, "o", "        code_init=None,\r\n"]
[1148.556, "o", "        dict_init=None,\r\n"]
[1148.566, "o", "        callback=None,\r\n"]
[1148.576, "o", "        verbose=False,\r\n"]
[1148.586, "o", "        split_sign=False,\r\n"]
[1148.596, "o", "        random_state=None,\r\n"]
[1148.606, "o", "        positive_code=False,\r\n"]
[1148.616, "o", "        positive_dict=False,\r\n"]
[1148.626, "o", "        transform_max_iter=1000,\r\n"]
[1148.636, "o", "    ):\r\n"]
[1148.646, "o", "        super().__init__(\r\n"]
[1148.656, "o", "            transform_algorithm,\r\n"]
[1148.666, "o", "            transform_n_nonzero_coefs,\r\n"]
[1148.676, "o", "            transform_alpha,\r\n"]
[1148.686, "o", "            split_sign,\r\n"]
[1148.696, "o", "            n_jobs,\r\n"]
[1148.706, "o", "            positive_code,\r\n"]
[1148.716, "o", "            transform_max_iter,\r\n"]
[1148.726, "o", "        )\r\n"]
[1148.736, "o", "        self.n_components = n_components\r\n"]
[1148.746, "o", "        self.alpha = alpha\r\n"]
[1148.756, "o", "        self.max_iter = max_iter\r\n"]
[1148.766, "o", "        self.tol = tol\r\n"]
[1148.776, "o", "        self.fit_algorithm = fit_algorithm\r\n"]
[1148.786, "o", "        self.code_init = code_init\r\n"]
[1148.796, "o", "        self.dict_init = dict_init\r\n"]
[1148.806, "o", "        self.callback = callback\r\n"]
[1148.816, "o", "        self.verbose = verbose\r\n"]
[1148.826, "o", "        self.random_state = random_state\r\n"]
[1148.836, "o", "        self.positive_dict = positive_dict\r\n"]
[1148.846, "o", "\r\n"]
[1148.856, "o", "    def fit(self, X, y=None):\r\n"]
[1148.866, "o", "        \"\"\"Fit the model from data in X.\r\n"]
[1148.876, "o", "\r\n"]
[1148.886, "o", "        Parameters\r\n"]
[1148.896, "o", "        ----------\r\n"]
[1148.906, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[1148.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1148.926, "o", "            and `n_features` is the number of features.\r\n"]
[1148.936, "o", "\r\n"]
[1148.946, "o", "        y : Ignored\r\n"]
[1148.956, "o", "            Not used, present for API consistency by convention.\r\n"]
[1148.966, "o", "\r\n"]
[1148.976, "o", "        Returns\r\n"]
[1148.986, "o", "        -------\r\n"]
[1148.996, "o", "        self : object\r\n"]
[1149.006, "o", "            Returns the instance itself.\r\n"]
[1149.016, "o", "        \"\"\"\r\n"]
[1149.026, "o", "        self.fit_transform(X)\r\n"]
[1149.036, "o", "        return self\r\n"]
[1149.046, "o", "\r\n"]
[1149.056, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[1149.066, "o", "    def fit_transform(self, X, y=None):\r\n"]
[1149.076, "o", "        \"\"\"Fit the model from data in X and return the transformed data.\r\n"]
[1149.086, "o", "\r\n"]
[1149.096, "o", "        Parameters\r\n"]
[1149.106, "o", "        ----------\r\n"]
[1149.116, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[1149.126, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1149.136, "o", "            and `n_features` is the number of features.\r\n"]
[1149.146, "o", "\r\n"]
[1149.156, "o", "        y : Ignored\r\n"]
[1149.166, "o", "            Not used, present for API consistency by convention.\r\n"]
[1149.176, "o", "\r\n"]
[1149.186, "o", "        Returns\r\n"]
[1149.196, "o", "        -------\r\n"]
[1149.206, "o", "        V : ndarray of shape (n_samples, n_components)\r\n"]
[1149.216, "o", "            Transformed data.\r\n"]
[1149.226, "o", "        \"\"\"\r\n"]
[1149.236, "o", "        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n"]
[1149.246, "o", "\r\n"]
[1149.256, "o", "        method = \"lasso_\" + self.fit_algorithm\r\n"]
[1149.266, "o", "\r\n"]
[1149.276, "o", "        random_state = check_random_state(self.random_state)\r\n"]
[1149.286, "o", "        X = self._validate_data(X)\r\n"]
[1149.296, "o", "\r\n"]
[1149.306, "o", "        if self.n_components is None:\r\n"]
[1149.316, "o", "            n_components = X.shape[1]\r\n"]
[1149.326, "o", "        else:\r\n"]
[1149.336, "o", "            n_components = self.n_components\r\n"]
[1149.346, "o", "\r\n"]
[1149.356, "o", "        V, U, E, self.n_iter_ = _dict_learning(\r\n"]
[1149.366, "o", "            X,\r\n"]
[1149.376, "o", "            n_components,\r\n"]
[1149.386, "o", "            alpha=self.alpha,\r\n"]
[1149.396, "o", "            tol=self.tol,\r\n"]
[1149.406, "o", "            max_iter=self.max_iter,\r\n"]
[1149.416, "o", "            method=method,\r\n"]
[1149.426, "o", "            method_max_iter=self.transform_max_iter,\r\n"]
[1149.436, "o", "            n_jobs=self.n_jobs,\r\n"]
[1149.446, "o", "            code_init=self.code_init,\r\n"]
[1149.456, "o", "            dict_init=self.dict_init,\r\n"]
[1149.466, "o", "            callback=self.callback,\r\n"]
[1149.476, "o", "            verbose=self.verbose,\r\n"]
[1149.486, "o", "            random_state=random_state,\r\n"]
[1149.496, "o", "            return_n_iter=True,\r\n"]
[1149.506, "o", "            positive_dict=self.positive_dict,\r\n"]
[1149.516, "o", "            positive_code=self.positive_code,\r\n"]
[1149.526, "o", "        )\r\n"]
[1149.536, "o", "        self.components_ = U\r\n"]
[1149.546, "o", "        self.error_ = E\r\n"]
[1149.556, "o", "\r\n"]
[1149.566, "o", "        return V\r\n"]
[1149.576, "o", "\r\n"]
[1149.586, "o", "    @property\r\n"]
[1149.596, "o", "    def _n_features_out(self):\r\n"]
[1149.606, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[1149.616, "o", "        return self.components_.shape[0]\r\n"]
[1149.626, "o", "\r\n"]
[1149.636, "o", "    def _more_tags(self):\r\n"]
[1149.646, "o", "        return {\r\n"]
[1149.656, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[1149.666, "o", "        }\r\n"]
[1149.676, "o", "\r\n"]
[1149.686, "o", "\r\n"]
[1149.696, "o", "class MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[1149.706, "o", "    \"\"\"Mini-batch dictionary learning.\r\n"]
[1149.716, "o", "\r\n"]
[1149.726, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[1149.736, "o", "    encoding the fitted data.\r\n"]
[1149.746, "o", "\r\n"]
[1149.756, "o", "    Solves the optimization problem::\r\n"]
[1149.766, "o", "\r\n"]
[1149.776, "o", "       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[1149.786, "o", "                    (U,V)\r\n"]
[1149.796, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[1149.806, "o", "\r\n"]
[1149.816, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[1149.826, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[1149.836, "o", "    of all the entries in the matrix.\r\n"]
[1149.846, "o", "\r\n"]
[1149.856, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[1149.866, "o", "\r\n"]
[1149.876, "o", "    Parameters\r\n"]
[1149.886, "o", "    ----------\r\n"]
[1149.896, "o", "    n_components : int, default=None\r\n"]
[1149.906, "o", "        Number of dictionary elements to extract.\r\n"]
[1149.916, "o", "\r\n"]
[1149.926, "o", "    alpha : float, default=1\r\n"]
[1149.936, "o", "        Sparsity controlling parameter.\r\n"]
[1150.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1150.002, "i", "pytest -q sklearn/decomposition/tests/test_dict_learning.py -k minibatch -q\r"]
[1150.004, "o", "pytest -q sklearn/decomposition/tests/test_dict_learning.py -k minibatch -q\r\n"]
[1152.478, "o", "\u001b[?2004l\r\n"]
[1155.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1155.002, "i", "cd asv_benchmarks\r"]
[1155.004, "o", "cd asv_benchmarks\r\n"]
[1155.006, "o", "\u001b[?2004l\r\n"]
[1160.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1160.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r"]
[1160.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r\n"]
[1160.315, "o", "stderr\r\n"]
[1160.624, "o", "\u001b[?2004l\r\n"]
[1160.933, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[1161.242, "o", "\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n"]
[1161.551, "o", "[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n"]
[1161.86, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n"]
[1162.169, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0m                                                                                     ok\r\n"]
[1162.478, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ============\u001b[0m\r\n"]
[1162.787, "o", "              --                 n_jobs   \r\n"]
[1163.096, "o", "              --------------- ------------\r\n"]
[1163.405, "o", "               fit_algorithm       1      \r\n"]
[1163.714, "o", "              =============== ============\r\n"]
[1164.023, "o", "                    lars       6.52\u00b10.04s \r\n"]
[1164.332, "o", "                     cd        1.54\u00b10.02s \r\n"]
[1164.641, "o", "              =============== ============\r\n"]
[1165.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1165.002, "i", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py || true\r"]
[1165.004, "o", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[1167.478, "o", "\u001b[?2004l\r\n"]
[1170.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1170.002, "i", "grep -n \"_sparse_encode_precomputed\\s*\\(\" sklearn/decomposition/_dict_learning.py || true\r"]
[1170.004, "o", "grep -n \"_sparse_encode_precomputed\\s*\\(\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[1172.478, "o", "\u001b[?2004l\r\n"]
[1175.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1175.002, "i", "grep -n \"sparse_encode\\s*\\(\" sklearn/decomposition/_dict_learning.py || true\r"]
[1175.004, "o", "grep -n \"sparse_encode\\s*\\(\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[1177.478, "o", "\u001b[?2004l\r\n"]
[1180.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1180.002, "i", "sed -n '1,600p' sklearn/decomposition/_dict_learning.py\r"]
[1180.004, "o", "sed -n '1,600p' sklearn/decomposition/_dict_learning.py\r\n"]
[1182.478, "o", "\u001b[?2004l\r\n"]
[1185.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1185.002, "i", "sed -n '600,1400p' sklearn/decomposition/_dict_learning.py\r"]
[1185.004, "o", "sed -n '600,1400p' sklearn/decomposition/_dict_learning.py\r\n"]
[1187.478, "o", "\u001b[?2004l\r\n"]
[1190.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1190.002, "i", "sed -n '1400,2200p' sklearn/decomposition/_dict_learning.py\r"]
[1190.004, "o", "sed -n '1400,2200p' sklearn/decomposition/_dict_learning.py\r\n"]
[1192.478, "o", "\u001b[?2004l\r\n"]
[1195.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1195.002, "i", "cd /workspace/repo\r"]
[1195.004, "o", "cd /workspace/repo\r\n"]
[1195.006, "o", "\u001b[?2004l\r\n"]
[1200.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1200.002, "i", "grep -n \"def _minibatch_step\\|_minibatch_step\\(\" sklearn/decomposition/_dict_learning.py\r"]
[1200.004, "o", "grep -n \"def _minibatch_step\\|_minibatch_step\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1202.478, "o", "\u001b[?2004l\r\n"]
[1205.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1205.002, "i", "grep -n \"_sparse_encode_precomputed\\s*\\(|sparse_encode\\s*\\(\" sklearn/decomposition/_dict_learning.py\r"]
[1205.004, "o", "grep -n \"_sparse_encode_precomputed\\s*\\(|sparse_encode\\s*\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1207.478, "o", "\u001b[?2004l\r\n"]
[1210.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1210.002, "i", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r"]
[1210.004, "o", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r\n"]
[1210.016, "o", "\u001b[?2004l\r\n"]
[1210.026, "o", "\"\"\" Dictionary learning.\r\n"]
[1210.036, "o", "\"\"\"\r\n"]
[1210.046, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[1210.056, "o", "# License: BSD 3 clause\r\n"]
[1210.066, "o", "\r\n"]
[1210.076, "o", "import itertools\r\n"]
[1210.086, "o", "import sys\r\n"]
[1210.096, "o", "import time\r\n"]
[1210.106, "o", "import warnings\r\n"]
[1210.116, "o", "from math import ceil\r\n"]
[1210.126, "o", "from numbers import Integral, Real\r\n"]
[1210.136, "o", "\r\n"]
[1210.146, "o", "import numpy as np\r\n"]
[1210.156, "o", "from joblib import effective_n_jobs\r\n"]
[1210.166, "o", "from scipy import linalg\r\n"]
[1210.176, "o", "\r\n"]
[1210.186, "o", "from ..base import (\r\n"]
[1210.196, "o", "    BaseEstimator,\r\n"]
[1210.206, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[1210.216, "o", "    TransformerMixin,\r\n"]
[1210.226, "o", "    _fit_context,\r\n"]
[1210.236, "o", ")\r\n"]
[1210.246, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[1210.256, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[1210.266, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[1210.276, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[1210.286, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[1210.296, "o", "from ..utils.validation import check_is_fitted\r\n"]
[1210.306, "o", "\r\n"]
[1210.316, "o", "\r\n"]
[1210.326, "o", "def _check_positive_coding(method, positive):\r\n"]
[1210.336, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[1210.346, "o", "        raise ValueError(\r\n"]
[1210.356, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[1210.366, "o", "        )\r\n"]
[1210.376, "o", "\r\n"]
[1210.386, "o", "\r\n"]
[1210.396, "o", "def _sparse_encode_precomputed(\r\n"]
[1210.406, "o", "    X,\r\n"]
[1210.416, "o", "    dictionary,\r\n"]
[1210.426, "o", "    *,\r\n"]
[1210.436, "o", "    gram=None,\r\n"]
[1210.446, "o", "    cov=None,\r\n"]
[1210.456, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1210.466, "o", "    regularization=None,\r\n"]
[1210.476, "o", "    copy_cov=True,\r\n"]
[1210.486, "o", "    init=None,\r\n"]
[1210.496, "o", "    max_iter=1000,\r\n"]
[1210.506, "o", "    verbose=0,\r\n"]
[1210.516, "o", "    positive=False,\r\n"]
[1210.526, "o", "):\r\n"]
[1210.536, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[1210.546, "o", "\r\n"]
[1210.556, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[1210.566, "o", "\r\n"]
[1210.576, "o", "    Parameters\r\n"]
[1210.586, "o", "    ----------\r\n"]
[1210.596, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[1210.606, "o", "        Data matrix.\r\n"]
[1210.616, "o", "\r\n"]
[1210.626, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1210.636, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1210.646, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[1210.656, "o", "\r\n"]
[1210.666, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[1210.676, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[1210.686, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[1210.696, "o", "\r\n"]
[1210.706, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[1210.716, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[1210.726, "o", "\r\n"]
[1210.736, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1210.746, "o", "            default='lasso_lars'\r\n"]
[1210.756, "o", "        The algorithm used:\r\n"]
[1210.766, "o", "\r\n"]
[1210.776, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[1210.786, "o", "          (`linear_model.lars_path`);\r\n"]
[1210.796, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1210.806, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1210.816, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1210.826, "o", "          the estimated components are sparse;\r\n"]
[1210.836, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1210.846, "o", "          solution;\r\n"]
[1210.856, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1210.866, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[1210.876, "o", "\r\n"]
[1210.886, "o", "    regularization : int or float, default=None\r\n"]
[1210.896, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[1210.906, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[1210.916, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[1210.926, "o", "\r\n"]
[1210.936, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1210.946, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[1210.956, "o", "        `algorithm='lasso_cd'`.\r\n"]
[1210.966, "o", "\r\n"]
[1210.976, "o", "    max_iter : int, default=1000\r\n"]
[1210.986, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1210.996, "o", "        `'lasso_lars'`.\r\n"]
[1211.006, "o", "\r\n"]
[1211.016, "o", "    copy_cov : bool, default=True\r\n"]
[1211.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1211.036, "o", "        be overwritten.\r\n"]
[1211.046, "o", "\r\n"]
[1211.056, "o", "    verbose : int, default=0\r\n"]
[1211.066, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[1211.076, "o", "\r\n"]
[1211.086, "o", "    positive: bool, default=False\r\n"]
[1211.096, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[1211.106, "o", "\r\n"]
[1211.116, "o", "        .. versionadded:: 0.20\r\n"]
[1211.126, "o", "\r\n"]
[1211.136, "o", "    Returns\r\n"]
[1211.146, "o", "    -------\r\n"]
[1211.156, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[1211.166, "o", "        The sparse codes.\r\n"]
[1211.176, "o", "    \"\"\"\r\n"]
[1211.186, "o", "    n_samples, n_features = X.shape\r\n"]
[1211.196, "o", "    n_components = dictionary.shape[0]\r\n"]
[1211.206, "o", "\r\n"]
[1211.216, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[1211.226, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1211.236, "o", "        try:\r\n"]
[1211.246, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1211.256, "o", "\r\n"]
[1211.266, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1211.276, "o", "            # corrects the verbosity level.\r\n"]
[1211.286, "o", "            lasso_lars = LassoLars(\r\n"]
[1211.296, "o", "                alpha=alpha,\r\n"]
[1211.306, "o", "                fit_intercept=False,\r\n"]
[1211.316, "o", "                verbose=verbose,\r\n"]
[1211.326, "o", "                precompute=gram,\r\n"]
[1211.336, "o", "                fit_path=False,\r\n"]
[1211.346, "o", "                positive=positive,\r\n"]
[1211.356, "o", "                max_iter=max_iter,\r\n"]
[1211.366, "o", "            )\r\n"]
[1211.376, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1211.386, "o", "            new_code = lasso_lars.coef_\r\n"]
[1211.396, "o", "        finally:\r\n"]
[1211.406, "o", "            np.seterr(**err_mgt)\r\n"]
[1211.416, "o", "\r\n"]
[1211.426, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[1211.436, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1211.446, "o", "\r\n"]
[1211.456, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[1211.466, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[1211.476, "o", "        # argument that we could pass in from Lasso.\r\n"]
[1211.486, "o", "        clf = Lasso(\r\n"]
[1211.496, "o", "            alpha=alpha,\r\n"]
[1211.506, "o", "            fit_intercept=False,\r\n"]
[1211.516, "o", "            precompute=gram,\r\n"]
[1211.526, "o", "            max_iter=max_iter,\r\n"]
[1211.536, "o", "            warm_start=True,\r\n"]
[1211.546, "o", "            positive=positive,\r\n"]
[1211.556, "o", "        )\r\n"]
[1211.566, "o", "\r\n"]
[1211.576, "o", "        if init is not None:\r\n"]
[1211.586, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[1211.596, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[1211.606, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[1211.616, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[1211.626, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[1211.636, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[1211.646, "o", "                init = np.array(init)\r\n"]
[1211.656, "o", "            clf.coef_ = init\r\n"]
[1211.666, "o", "\r\n"]
[1211.676, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[1211.686, "o", "        new_code = clf.coef_\r\n"]
[1211.696, "o", "\r\n"]
[1211.706, "o", "    elif algorithm == \"lars\":\r\n"]
[1211.716, "o", "        try:\r\n"]
[1211.726, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1211.736, "o", "\r\n"]
[1211.746, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1211.756, "o", "            # corrects the verbosity level.\r\n"]
[1211.766, "o", "            lars = Lars(\r\n"]
[1211.776, "o", "                fit_intercept=False,\r\n"]
[1211.786, "o", "                verbose=verbose,\r\n"]
[1211.796, "o", "                precompute=gram,\r\n"]
[1211.806, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[1211.816, "o", "                fit_path=False,\r\n"]
[1211.826, "o", "            )\r\n"]
[1211.836, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1211.846, "o", "            new_code = lars.coef_\r\n"]
[1211.856, "o", "        finally:\r\n"]
[1211.866, "o", "            np.seterr(**err_mgt)\r\n"]
[1211.876, "o", "\r\n"]
[1211.886, "o", "    elif algorithm == \"threshold\":\r\n"]
[1211.896, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[1211.906, "o", "        if positive:\r\n"]
[1211.916, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[1211.926, "o", "\r\n"]
[1211.936, "o", "    elif algorithm == \"omp\":\r\n"]
[1211.946, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[1211.956, "o", "            Gram=gram,\r\n"]
[1211.966, "o", "            Xy=cov,\r\n"]
[1211.976, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[1211.986, "o", "            tol=None,\r\n"]
[1211.996, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[1212.006, "o", "            copy_Xy=copy_cov,\r\n"]
[1212.016, "o", "        ).T\r\n"]
[1212.026, "o", "\r\n"]
[1212.036, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[1212.046, "o", "\r\n"]
[1212.056, "o", "\r\n"]
[1212.066, "o", "@validate_params(\r\n"]
[1212.076, "o", "    {\r\n"]
[1212.086, "o", "        \"X\": [\"array-like\"],\r\n"]
[1212.096, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[1212.106, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[1212.116, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[1212.126, "o", "        \"algorithm\": [\r\n"]
[1212.136, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[1212.146, "o", "        ],\r\n"]
[1212.156, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1212.166, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[1212.176, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[1212.186, "o", "        \"init\": [\"array-like\", None],\r\n"]
[1212.196, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1212.206, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[1212.216, "o", "        \"check_input\": [\"boolean\"],\r\n"]
[1212.226, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[1212.236, "o", "        \"positive\": [\"boolean\"],\r\n"]
[1212.246, "o", "    },\r\n"]
[1212.256, "o", "    prefer_skip_nested_validation=True,\r\n"]
[1212.266, "o", ")\r\n"]
[1212.276, "o", "# XXX : could be moved to the linear_model module\r\n"]
[1212.286, "o", "def sparse_encode(\r\n"]
[1212.296, "o", "    X,\r\n"]
[1212.306, "o", "    dictionary,\r\n"]
[1212.316, "o", "    *,\r\n"]
[1212.326, "o", "    gram=None,\r\n"]
[1212.336, "o", "    cov=None,\r\n"]
[1212.346, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1212.356, "o", "    n_nonzero_coefs=None,\r\n"]
[1212.366, "o", "    alpha=None,\r\n"]
[1212.376, "o", "    copy_cov=True,\r\n"]
[1212.386, "o", "    init=None,\r\n"]
[1212.396, "o", "    max_iter=1000,\r\n"]
[1212.406, "o", "    n_jobs=None,\r\n"]
[1212.416, "o", "    check_input=True,\r\n"]
[1212.426, "o", "    verbose=0,\r\n"]
[1212.436, "o", "    positive=False,\r\n"]
[1212.446, "o", "):\r\n"]
[1212.456, "o", "    \"\"\"Sparse coding.\r\n"]
[1212.466, "o", "\r\n"]
[1212.476, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[1212.486, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[1212.496, "o", "\r\n"]
[1212.506, "o", "        X ~= code * dictionary\r\n"]
[1212.516, "o", "\r\n"]
[1212.526, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[1212.536, "o", "\r\n"]
[1212.546, "o", "    Parameters\r\n"]
[1212.556, "o", "    ----------\r\n"]
[1212.566, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[1212.576, "o", "        Data matrix.\r\n"]
[1212.586, "o", "\r\n"]
[1212.596, "o", "    dictionary : array-like of shape (n_components, n_features)\r\n"]
[1212.606, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1212.616, "o", "        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[1212.626, "o", "        output.\r\n"]
[1212.636, "o", "\r\n"]
[1212.646, "o", "    gram : array-like of shape (n_components, n_components), default=None\r\n"]
[1212.656, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n"]
[1212.666, "o", "\r\n"]
[1212.676, "o", "    cov : array-like of shape (n_components, n_samples), default=None\r\n"]
[1212.686, "o", "        Precomputed covariance, `dictionary' * X`.\r\n"]
[1212.696, "o", "\r\n"]
[1212.706, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1212.716, "o", "            default='lasso_lars'\r\n"]
[1212.726, "o", "        The algorithm used:\r\n"]
[1212.736, "o", "\r\n"]
[1212.746, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[1212.756, "o", "          (`linear_model.lars_path`);\r\n"]
[1212.766, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1212.776, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1212.786, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1212.796, "o", "          the estimated components are sparse;\r\n"]
[1212.806, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1212.816, "o", "          solution;\r\n"]
[1212.826, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1212.836, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[1212.846, "o", "\r\n"]
[1212.856, "o", "    n_nonzero_coefs : int, default=None\r\n"]
[1212.866, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1212.876, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[1212.886, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[1212.896, "o", "        `n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1212.906, "o", "\r\n"]
[1212.916, "o", "    alpha : float, default=None\r\n"]
[1212.926, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1212.936, "o", "        penalty applied to the L1 norm.\r\n"]
[1212.946, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1212.956, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1212.966, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[1212.976, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[1212.986, "o", "        `n_nonzero_coefs`.\r\n"]
[1212.996, "o", "        If `None`, default to 1.\r\n"]
[1213.006, "o", "\r\n"]
[1213.016, "o", "    copy_cov : bool, default=True\r\n"]
[1213.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1213.036, "o", "        be overwritten.\r\n"]
[1213.046, "o", "\r\n"]
[1213.056, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1213.066, "o", "        Initialization value of the sparse codes. Only used if\r\n"]
[1213.076, "o", "        `algorithm='lasso_cd'`.\r\n"]
[1213.086, "o", "\r\n"]
[1213.096, "o", "    max_iter : int, default=1000\r\n"]
[1213.106, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1213.116, "o", "        `'lasso_lars'`.\r\n"]
[1213.126, "o", "\r\n"]
[1213.136, "o", "    n_jobs : int, default=None\r\n"]
[1213.146, "o", "        Number of parallel jobs to run.\r\n"]
[1213.156, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1213.166, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1213.176, "o", "        for more details.\r\n"]
[1213.186, "o", "\r\n"]
[1213.196, "o", "    check_input : bool, default=True\r\n"]
[1213.206, "o", "        If `False`, the input arrays X and dictionary will not be checked.\r\n"]
[1213.216, "o", "\r\n"]
[1213.226, "o", "    verbose : int, default=0\r\n"]
[1213.236, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[1213.246, "o", "\r\n"]
[1213.256, "o", "    positive : bool, default=False\r\n"]
[1213.266, "o", "        Whether to enforce positivity when finding the encoding.\r\n"]
[1213.276, "o", "\r\n"]
[1213.286, "o", "        .. versionadded:: 0.20\r\n"]
[1213.296, "o", "\r\n"]
[1213.306, "o", "    Returns\r\n"]
[1213.316, "o", "    -------\r\n"]
[1213.326, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1213.336, "o", "        The sparse codes.\r\n"]
[1213.346, "o", "\r\n"]
[1213.356, "o", "    See Also\r\n"]
[1213.366, "o", "    --------\r\n"]
[1213.376, "o", "    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n"]
[1213.386, "o", "        path using LARS algorithm.\r\n"]
[1213.396, "o", "    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n"]
[1213.406, "o", "    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n"]
[1213.416, "o", "    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n"]
[1213.426, "o", "        dictionary.\r\n"]
[1213.436, "o", "    \"\"\"\r\n"]
[1213.446, "o", "    if check_input:\r\n"]
[1213.456, "o", "        if algorithm == \"lasso_cd\":\r\n"]
[1213.466, "o", "            dictionary = check_array(\r\n"]
[1213.476, "o", "                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n"]
[1213.486, "o", "            )\r\n"]
[1213.496, "o", "            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n"]
[1213.506, "o", "        else:\r\n"]
[1213.516, "o", "            dictionary = check_array(dictionary)\r\n"]
[1213.526, "o", "            X = check_array(X)\r\n"]
[1213.536, "o", "\r\n"]
[1213.546, "o", "    if dictionary.shape[1] != X.shape[1]:\r\n"]
[1213.556, "o", "        raise ValueError(\r\n"]
[1213.566, "o", "            \"Dictionary and X have different numbers of features:\"\r\n"]
[1213.576, "o", "            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n"]
[1213.586, "o", "        )\r\n"]
[1213.596, "o", "\r\n"]
[1213.606, "o", "    _check_positive_coding(algorithm, positive)\r\n"]
[1213.616, "o", "\r\n"]
[1213.626, "o", "    return _sparse_encode(\r\n"]
[1213.636, "o", "        X,\r\n"]
[1213.646, "o", "        dictionary,\r\n"]
[1213.656, "o", "        gram=gram,\r\n"]
[1213.666, "o", "        cov=cov,\r\n"]
[1213.676, "o", "        algorithm=algorithm,\r\n"]
[1213.686, "o", "        n_nonzero_coefs=n_nonzero_coefs,\r\n"]
[1213.696, "o", "        alpha=alpha,\r\n"]
[1213.706, "o", "        copy_cov=copy_cov,\r\n"]
[1213.716, "o", "        init=init,\r\n"]
[1213.726, "o", "        max_iter=max_iter,\r\n"]
[1213.736, "o", "        n_jobs=n_jobs,\r\n"]
[1213.746, "o", "        verbose=verbose,\r\n"]
[1213.756, "o", "        positive=positive,\r\n"]
[1213.766, "o", "    )\r\n"]
[1213.776, "o", "\r\n"]
[1213.786, "o", "\r\n"]
[1213.796, "o", "def _sparse_encode(\r\n"]
[1213.806, "o", "    X,\r\n"]
[1213.816, "o", "    dictionary,\r\n"]
[1213.826, "o", "    *,\r\n"]
[1213.836, "o", "    gram=None,\r\n"]
[1213.846, "o", "    cov=None,\r\n"]
[1213.856, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1213.866, "o", "    n_nonzero_coefs=None,\r\n"]
[1213.876, "o", "    alpha=None,\r\n"]
[1213.886, "o", "    copy_cov=True,\r\n"]
[1213.896, "o", "    init=None,\r\n"]
[1213.906, "o", "    max_iter=1000,\r\n"]
[1213.916, "o", "    n_jobs=None,\r\n"]
[1213.926, "o", "    verbose=0,\r\n"]
[1213.936, "o", "    positive=False,\r\n"]
[1213.946, "o", "):\r\n"]
[1213.956, "o", "    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[1213.966, "o", "\r\n"]
[1213.976, "o", "    n_samples, n_features = X.shape\r\n"]
[1213.986, "o", "    n_components = dictionary.shape[0]\r\n"]
[1213.996, "o", "\r\n"]
[1214.006, "o", "    if algorithm in (\"lars\", \"omp\"):\r\n"]
[1214.016, "o", "        regularization = n_nonzero_coefs\r\n"]
[1214.026, "o", "        if regularization is None:\r\n"]
[1214.036, "o", "            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[1214.046, "o", "    else:\r\n"]
[1214.056, "o", "        regularization = alpha\r\n"]
[1214.066, "o", "        if regularization is None:\r\n"]
[1214.076, "o", "            regularization = 1.0\r\n"]
[1214.086, "o", "\r\n"]
[1214.096, "o", "    if gram is None and algorithm != \"threshold\":\r\n"]
[1214.106, "o", "        gram = np.dot(dictionary, dictionary.T)\r\n"]
[1214.116, "o", "\r\n"]
[1214.126, "o", "    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[1214.136, "o", "        copy_cov = False\r\n"]
[1214.146, "o", "        cov = np.dot(dictionary, X.T)\r\n"]
[1214.156, "o", "\r\n"]
[1214.166, "o", "    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[1214.176, "o", "        code = _sparse_encode_precomputed(\r\n"]
[1214.186, "o", "            X,\r\n"]
[1214.196, "o", "            dictionary,\r\n"]
[1214.206, "o", "            gram=gram,\r\n"]
[1214.216, "o", "            cov=cov,\r\n"]
[1214.226, "o", "            algorithm=algorithm,\r\n"]
[1214.236, "o", "            regularization=regularization,\r\n"]
[1214.246, "o", "            copy_cov=copy_cov,\r\n"]
[1214.256, "o", "            init=init,\r\n"]
[1214.266, "o", "            max_iter=max_iter,\r\n"]
[1214.276, "o", "            verbose=verbose,\r\n"]
[1214.286, "o", "            positive=positive,\r\n"]
[1214.296, "o", "        )\r\n"]
[1214.306, "o", "        return code\r\n"]
[1214.316, "o", "\r\n"]
[1214.326, "o", "    # Enter parallel code block\r\n"]
[1214.336, "o", "    n_samples = X.shape[0]\r\n"]
[1214.346, "o", "    n_components = dictionary.shape[0]\r\n"]
[1214.356, "o", "    code = np.empty((n_samples, n_components))\r\n"]
[1214.366, "o", "    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[1214.376, "o", "\r\n"]
[1214.386, "o", "    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[1214.396, "o", "        delayed(_sparse_encode_precomputed)(\r\n"]
[1214.406, "o", "            X[this_slice],\r\n"]
[1214.416, "o", "            dictionary,\r\n"]
[1214.426, "o", "            gram=gram,\r\n"]
[1214.436, "o", "            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[1214.446, "o", "            algorithm=algorithm,\r\n"]
[1214.456, "o", "            regularization=regularization,\r\n"]
[1214.466, "o", "            copy_cov=copy_cov,\r\n"]
[1214.476, "o", "            init=init[this_slice] if init is not None else None,\r\n"]
[1214.486, "o", "            max_iter=max_iter,\r\n"]
[1214.496, "o", "            verbose=verbose,\r\n"]
[1214.506, "o", "            positive=positive,\r\n"]
[1214.516, "o", "        )\r\n"]
[1214.526, "o", "        for this_slice in slices\r\n"]
[1214.536, "o", "    )\r\n"]
[1214.546, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[1214.556, "o", "        code[this_slice] = this_view\r\n"]
[1214.566, "o", "    return code\r\n"]
[1214.576, "o", "\r\n"]
[1214.586, "o", "\r\n"]
[1214.596, "o", "def _update_dict(\r\n"]
[1214.606, "o", "    dictionary,\r\n"]
[1214.616, "o", "    Y,\r\n"]
[1214.626, "o", "    code,\r\n"]
[1214.636, "o", "    A=None,\r\n"]
[1214.646, "o", "    B=None,\r\n"]
[1214.656, "o", "    verbose=False,\r\n"]
[1214.666, "o", "    random_state=None,\r\n"]
[1214.676, "o", "    positive=False,\r\n"]
[1214.686, "o", "):\r\n"]
[1214.696, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[1214.706, "o", "\r\n"]
[1214.716, "o", "    Parameters\r\n"]
[1214.726, "o", "    ----------\r\n"]
[1214.736, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1214.746, "o", "        Value of the dictionary at the previous iteration.\r\n"]
[1214.756, "o", "\r\n"]
[1214.766, "o", "    Y : ndarray of shape (n_samples, n_features)\r\n"]
[1214.776, "o", "        Data matrix.\r\n"]
[1214.786, "o", "\r\n"]
[1214.796, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1214.806, "o", "        Sparse coding of the data against which to optimize the dictionary.\r\n"]
[1214.816, "o", "\r\n"]
[1214.826, "o", "    A : ndarray of shape (n_components, n_components), default=None\r\n"]
[1214.836, "o", "        Together with `B`, sufficient stats of the online model to update the\r\n"]
[1214.846, "o", "        dictionary.\r\n"]
[1214.856, "o", "\r\n"]
[1214.866, "o", "    B : ndarray of shape (n_features, n_components), default=None\r\n"]
[1214.876, "o", "        Together with `A`, sufficient stats of the online model to update the\r\n"]
[1214.886, "o", "        dictionary.\r\n"]
[1214.896, "o", "\r\n"]
[1214.906, "o", "    verbose: bool, default=False\r\n"]
[1214.916, "o", "        Degree of output the procedure will print.\r\n"]
[1214.926, "o", "\r\n"]
[1214.936, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1215.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1215.002, "i", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r"]
[1215.004, "o", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r\n"]
[1215.016, "o", "\u001b[?2004l\r\n"]
[1215.026, "o", "           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[1215.036, "o", "\r\n"]
[1215.046, "o", "    return_n_iter : bool, default=False\r\n"]
[1215.056, "o", "        Whether or not to return the number of iterations.\r\n"]
[1215.066, "o", "\r\n"]
[1215.076, "o", "        .. deprecated:: 1.1\r\n"]
[1215.086, "o", "           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n"]
[1215.096, "o", "\r\n"]
[1215.106, "o", "    positive_dict : bool, default=False\r\n"]
[1215.116, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1215.126, "o", "\r\n"]
[1215.136, "o", "        .. versionadded:: 0.20\r\n"]
[1215.146, "o", "\r\n"]
[1215.156, "o", "    positive_code : bool, default=False\r\n"]
[1215.166, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1215.176, "o", "\r\n"]
[1215.186, "o", "        .. versionadded:: 0.20\r\n"]
[1215.196, "o", "\r\n"]
[1215.206, "o", "    method_max_iter : int, default=1000\r\n"]
[1215.216, "o", "        Maximum number of iterations to perform when solving the lasso problem.\r\n"]
[1215.226, "o", "\r\n"]
[1215.236, "o", "        .. versionadded:: 0.22\r\n"]
[1215.246, "o", "\r\n"]
[1215.256, "o", "    tol : float, default=1e-3\r\n"]
[1215.266, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[1215.276, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[1215.286, "o", "\r\n"]
[1215.296, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[1215.306, "o", "        `tol` to 0.0.\r\n"]
[1215.316, "o", "\r\n"]
[1215.326, "o", "        .. versionadded:: 1.1\r\n"]
[1215.336, "o", "\r\n"]
[1215.346, "o", "    max_no_improvement : int, default=10\r\n"]
[1215.356, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[1215.366, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[1215.376, "o", "        `max_iter` is not None.\r\n"]
[1215.386, "o", "\r\n"]
[1215.396, "o", "        To disable convergence detection based on cost function, set\r\n"]
[1215.406, "o", "        `max_no_improvement` to None.\r\n"]
[1215.416, "o", "\r\n"]
[1215.426, "o", "        .. versionadded:: 1.1\r\n"]
[1215.436, "o", "\r\n"]
[1215.446, "o", "    Returns\r\n"]
[1215.456, "o", "    -------\r\n"]
[1215.466, "o", "    code : ndarray of shape (n_samples, n_components),\r\n"]
[1215.476, "o", "        The sparse code (only returned if `return_code=True`).\r\n"]
[1215.486, "o", "\r\n"]
[1215.496, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[1215.506, "o", "        The solutions to the dictionary learning problem.\r\n"]
[1215.516, "o", "\r\n"]
[1215.526, "o", "    n_iter : int\r\n"]
[1215.536, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[1215.546, "o", "        set to `True`.\r\n"]
[1215.556, "o", "\r\n"]
[1215.566, "o", "    See Also\r\n"]
[1215.576, "o", "    --------\r\n"]
[1215.586, "o", "    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n"]
[1215.596, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1215.606, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n"]
[1215.616, "o", "        learning algorithm.\r\n"]
[1215.626, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1215.636, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1215.646, "o", "    \"\"\"\r\n"]
[1215.656, "o", "    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n"]
[1215.666, "o", "    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n"]
[1215.676, "o", "        raise ValueError(\r\n"]
[1215.686, "o", "            \"The following arguments are incompatible with 'max_iter': \"\r\n"]
[1215.696, "o", "            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n"]
[1215.706, "o", "        )\r\n"]
[1215.716, "o", "\r\n"]
[1215.726, "o", "    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n"]
[1215.736, "o", "    return_inner_stats = _check_warn_deprecated(\r\n"]
[1215.746, "o", "        return_inner_stats,\r\n"]
[1215.756, "o", "        \"return_inner_stats\",\r\n"]
[1215.766, "o", "        default=False,\r\n"]
[1215.776, "o", "        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n"]
[1215.786, "o", "    )\r\n"]
[1215.796, "o", "    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n"]
[1215.806, "o", "    return_n_iter = _check_warn_deprecated(\r\n"]
[1215.816, "o", "        return_n_iter,\r\n"]
[1215.826, "o", "        \"return_n_iter\",\r\n"]
[1215.836, "o", "        default=False,\r\n"]
[1215.846, "o", "        additional_message=(\r\n"]
[1215.856, "o", "            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n"]
[1215.866, "o", "            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n"]
[1215.876, "o", "        ),\r\n"]
[1215.886, "o", "    )\r\n"]
[1215.896, "o", "\r\n"]
[1215.906, "o", "    if max_iter is not None:\r\n"]
[1215.916, "o", "        transform_algorithm = \"lasso_\" + method\r\n"]
[1215.926, "o", "\r\n"]
[1215.936, "o", "        est = MiniBatchDictionaryLearning(\r\n"]
[1215.946, "o", "            n_components=n_components,\r\n"]
[1215.956, "o", "            alpha=alpha,\r\n"]
[1215.966, "o", "            n_iter=n_iter,\r\n"]
[1215.976, "o", "            n_jobs=n_jobs,\r\n"]
[1215.986, "o", "            fit_algorithm=method,\r\n"]
[1215.996, "o", "            batch_size=batch_size,\r\n"]
[1216.006, "o", "            shuffle=shuffle,\r\n"]
[1216.016, "o", "            dict_init=dict_init,\r\n"]
[1216.026, "o", "            random_state=random_state,\r\n"]
[1216.036, "o", "            transform_algorithm=transform_algorithm,\r\n"]
[1216.046, "o", "            transform_alpha=alpha,\r\n"]
[1216.056, "o", "            positive_code=positive_code,\r\n"]
[1216.066, "o", "            positive_dict=positive_dict,\r\n"]
[1216.076, "o", "            transform_max_iter=method_max_iter,\r\n"]
[1216.086, "o", "            verbose=verbose,\r\n"]
[1216.096, "o", "            callback=callback,\r\n"]
[1216.106, "o", "            tol=tol,\r\n"]
[1216.116, "o", "            max_no_improvement=max_no_improvement,\r\n"]
[1216.126, "o", "        ).fit(X)\r\n"]
[1216.136, "o", "\r\n"]
[1216.146, "o", "        if not return_code:\r\n"]
[1216.156, "o", "            return est.components_\r\n"]
[1216.166, "o", "        else:\r\n"]
[1216.176, "o", "            code = est.transform(X)\r\n"]
[1216.186, "o", "            return code, est.components_\r\n"]
[1216.196, "o", "\r\n"]
[1216.206, "o", "    # TODO(1.4) remove the whole old behavior\r\n"]
[1216.216, "o", "    # Fallback to old behavior\r\n"]
[1216.226, "o", "\r\n"]
[1216.236, "o", "    n_iter = _check_warn_deprecated(\r\n"]
[1216.246, "o", "        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n"]
[1216.256, "o", "    )\r\n"]
[1216.266, "o", "\r\n"]
[1216.276, "o", "    if n_components is None:\r\n"]
[1216.286, "o", "        n_components = X.shape[1]\r\n"]
[1216.296, "o", "\r\n"]
[1216.306, "o", "    if method not in (\"lars\", \"cd\"):\r\n"]
[1216.316, "o", "        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n"]
[1216.326, "o", "\r\n"]
[1216.336, "o", "    _check_positive_coding(method, positive_code)\r\n"]
[1216.346, "o", "\r\n"]
[1216.356, "o", "    method = \"lasso_\" + method\r\n"]
[1216.366, "o", "\r\n"]
[1216.376, "o", "    t0 = time.time()\r\n"]
[1216.386, "o", "    n_samples, n_features = X.shape\r\n"]
[1216.396, "o", "    # Avoid integer division problems\r\n"]
[1216.406, "o", "    alpha = float(alpha)\r\n"]
[1216.416, "o", "    random_state = check_random_state(random_state)\r\n"]
[1216.426, "o", "\r\n"]
[1216.436, "o", "    # Init V with SVD of X\r\n"]
[1216.446, "o", "    if dict_init is not None:\r\n"]
[1216.456, "o", "        dictionary = dict_init\r\n"]
[1216.466, "o", "    else:\r\n"]
[1216.476, "o", "        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n"]
[1216.486, "o", "        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[1216.496, "o", "    r = len(dictionary)\r\n"]
[1216.506, "o", "    if n_components <= r:\r\n"]
[1216.516, "o", "        dictionary = dictionary[:n_components, :]\r\n"]
[1216.526, "o", "    else:\r\n"]
[1216.536, "o", "        dictionary = np.r_[\r\n"]
[1216.546, "o", "            dictionary,\r\n"]
[1216.556, "o", "            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n"]
[1216.566, "o", "        ]\r\n"]
[1216.576, "o", "\r\n"]
[1216.586, "o", "    if verbose == 1:\r\n"]
[1216.596, "o", "        print(\"[dict_learning]\", end=\" \")\r\n"]
[1216.606, "o", "\r\n"]
[1216.616, "o", "    if shuffle:\r\n"]
[1216.626, "o", "        X_train = X.copy()\r\n"]
[1216.636, "o", "        random_state.shuffle(X_train)\r\n"]
[1216.646, "o", "    else:\r\n"]
[1216.656, "o", "        X_train = X\r\n"]
[1216.666, "o", "\r\n"]
[1216.676, "o", "    X_train = check_array(\r\n"]
[1216.686, "o", "        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n"]
[1216.696, "o", "    )\r\n"]
[1216.706, "o", "\r\n"]
[1216.716, "o", "    # Fortran-order dict better suited for the sparse coding which is the\r\n"]
[1216.726, "o", "    # bottleneck of this algorithm.\r\n"]
[1216.736, "o", "    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n"]
[1216.746, "o", "    dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[1216.756, "o", "\r\n"]
[1216.766, "o", "    batches = gen_batches(n_samples, batch_size)\r\n"]
[1216.776, "o", "    batches = itertools.cycle(batches)\r\n"]
[1216.786, "o", "\r\n"]
[1216.796, "o", "    # The covariance of the dictionary\r\n"]
[1216.806, "o", "    if inner_stats is None:\r\n"]
[1216.816, "o", "        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n"]
[1216.826, "o", "        # The data approximation\r\n"]
[1216.836, "o", "        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n"]
[1216.846, "o", "    else:\r\n"]
[1216.856, "o", "        A = inner_stats[0].copy()\r\n"]
[1216.866, "o", "        B = inner_stats[1].copy()\r\n"]
[1216.876, "o", "\r\n"]
[1216.886, "o", "    # If n_iter is zero, we need to return zero.\r\n"]
[1216.896, "o", "    ii = iter_offset - 1\r\n"]
[1216.906, "o", "\r\n"]
[1216.916, "o", "    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n"]
[1216.926, "o", "        this_X = X_train[batch]\r\n"]
[1216.936, "o", "        dt = time.time() - t0\r\n"]
[1216.946, "o", "        if verbose == 1:\r\n"]
[1216.956, "o", "            sys.stdout.write(\".\")\r\n"]
[1216.966, "o", "            sys.stdout.flush()\r\n"]
[1216.976, "o", "        elif verbose:\r\n"]
[1216.986, "o", "            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n"]
[1216.996, "o", "                print(\r\n"]
[1217.006, "o", "                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n"]
[1217.016, "o", "                )\r\n"]
[1217.026, "o", "\r\n"]
[1217.036, "o", "        this_code = sparse_encode(\r\n"]
[1217.046, "o", "            this_X,\r\n"]
[1217.056, "o", "            dictionary,\r\n"]
[1217.066, "o", "            algorithm=method,\r\n"]
[1217.076, "o", "            alpha=alpha,\r\n"]
[1217.086, "o", "            n_jobs=n_jobs,\r\n"]
[1217.096, "o", "            check_input=False,\r\n"]
[1217.106, "o", "            positive=positive_code,\r\n"]
[1217.116, "o", "            max_iter=method_max_iter,\r\n"]
[1217.126, "o", "            verbose=verbose,\r\n"]
[1217.136, "o", "        )\r\n"]
[1217.146, "o", "\r\n"]
[1217.156, "o", "        # Update the auxiliary variables\r\n"]
[1217.166, "o", "        if ii < batch_size - 1:\r\n"]
[1217.176, "o", "            theta = float((ii + 1) * batch_size)\r\n"]
[1217.186, "o", "        else:\r\n"]
[1217.196, "o", "            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n"]
[1217.206, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[1217.216, "o", "\r\n"]
[1217.226, "o", "        A *= beta\r\n"]
[1217.236, "o", "        A += np.dot(this_code.T, this_code)\r\n"]
[1217.246, "o", "        B *= beta\r\n"]
[1217.256, "o", "        B += np.dot(this_X.T, this_code)\r\n"]
[1217.266, "o", "\r\n"]
[1217.276, "o", "        # Update dictionary in place\r\n"]
[1217.286, "o", "        _update_dict(\r\n"]
[1217.296, "o", "            dictionary,\r\n"]
[1217.306, "o", "            this_X,\r\n"]
[1217.316, "o", "            this_code,\r\n"]
[1217.326, "o", "            A,\r\n"]
[1217.336, "o", "            B,\r\n"]
[1217.346, "o", "            verbose=verbose,\r\n"]
[1217.356, "o", "            random_state=random_state,\r\n"]
[1217.366, "o", "            positive=positive_dict,\r\n"]
[1217.376, "o", "        )\r\n"]
[1217.386, "o", "\r\n"]
[1217.396, "o", "        # Maybe we need a stopping criteria based on the amount of\r\n"]
[1217.406, "o", "        # modification in the dictionary\r\n"]
[1217.416, "o", "        if callback is not None:\r\n"]
[1217.426, "o", "            callback(locals())\r\n"]
[1217.436, "o", "\r\n"]
[1217.446, "o", "    if return_inner_stats:\r\n"]
[1217.456, "o", "        if return_n_iter:\r\n"]
[1217.466, "o", "            return dictionary, (A, B), ii - iter_offset + 1\r\n"]
[1217.476, "o", "        else:\r\n"]
[1217.486, "o", "            return dictionary, (A, B)\r\n"]
[1217.496, "o", "    if return_code:\r\n"]
[1217.506, "o", "        if verbose > 1:\r\n"]
[1217.516, "o", "            print(\"Learning code...\", end=\" \")\r\n"]
[1217.526, "o", "        elif verbose == 1:\r\n"]
[1217.536, "o", "            print(\"|\", end=\" \")\r\n"]
[1217.546, "o", "        code = sparse_encode(\r\n"]
[1217.556, "o", "            X,\r\n"]
[1217.566, "o", "            dictionary,\r\n"]
[1217.576, "o", "            algorithm=method,\r\n"]
[1217.586, "o", "            alpha=alpha,\r\n"]
[1217.596, "o", "            n_jobs=n_jobs,\r\n"]
[1217.606, "o", "            check_input=False,\r\n"]
[1217.616, "o", "            positive=positive_code,\r\n"]
[1217.626, "o", "            max_iter=method_max_iter,\r\n"]
[1217.636, "o", "            verbose=verbose,\r\n"]
[1217.646, "o", "        )\r\n"]
[1217.656, "o", "        if verbose > 1:\r\n"]
[1217.666, "o", "            dt = time.time() - t0\r\n"]
[1217.676, "o", "            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n"]
[1217.686, "o", "        if return_n_iter:\r\n"]
[1217.696, "o", "            return code, dictionary, ii - iter_offset + 1\r\n"]
[1217.706, "o", "        else:\r\n"]
[1217.716, "o", "            return code, dictionary\r\n"]
[1217.726, "o", "\r\n"]
[1217.736, "o", "    if return_n_iter:\r\n"]
[1217.746, "o", "        return dictionary, ii - iter_offset + 1\r\n"]
[1217.756, "o", "    else:\r\n"]
[1217.766, "o", "        return dictionary\r\n"]
[1217.776, "o", "\r\n"]
[1217.786, "o", "\r\n"]
[1217.796, "o", "@validate_params(\r\n"]
[1217.806, "o", "    {\r\n"]
[1217.816, "o", "        \"X\": [\"array-like\"],\r\n"]
[1217.826, "o", "        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[1217.836, "o", "        \"return_n_iter\": [\"boolean\"],\r\n"]
[1217.846, "o", "        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1217.856, "o", "    },\r\n"]
[1217.866, "o", "    prefer_skip_nested_validation=False,\r\n"]
[1217.876, "o", ")\r\n"]
[1217.886, "o", "def dict_learning(\r\n"]
[1217.896, "o", "    X,\r\n"]
[1217.906, "o", "    n_components,\r\n"]
[1217.916, "o", "    *,\r\n"]
[1217.926, "o", "    alpha,\r\n"]
[1217.936, "o", "    max_iter=100,\r\n"]
[1217.946, "o", "    tol=1e-8,\r\n"]
[1217.956, "o", "    method=\"lars\",\r\n"]
[1217.966, "o", "    n_jobs=None,\r\n"]
[1217.976, "o", "    dict_init=None,\r\n"]
[1217.986, "o", "    code_init=None,\r\n"]
[1217.996, "o", "    callback=None,\r\n"]
[1218.006, "o", "    verbose=False,\r\n"]
[1218.016, "o", "    random_state=None,\r\n"]
[1218.026, "o", "    return_n_iter=False,\r\n"]
[1218.036, "o", "    positive_dict=False,\r\n"]
[1218.046, "o", "    positive_code=False,\r\n"]
[1218.056, "o", "    method_max_iter=1000,\r\n"]
[1218.066, "o", "):\r\n"]
[1218.076, "o", "    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n"]
[1218.086, "o", "\r\n"]
[1218.096, "o", "    Finds the best dictionary and the corresponding sparse code for\r\n"]
[1218.106, "o", "    approximating the data matrix X by solving::\r\n"]
[1218.116, "o", "\r\n"]
[1218.126, "o", "        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[1218.136, "o", "                     (U,V)\r\n"]
[1218.146, "o", "                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n"]
[1218.156, "o", "\r\n"]
[1218.166, "o", "    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n"]
[1218.176, "o", "    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n"]
[1218.186, "o", "    which is the sum of the absolute values of all the entries in the matrix.\r\n"]
[1218.196, "o", "\r\n"]
[1218.206, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[1218.216, "o", "\r\n"]
[1218.226, "o", "    Parameters\r\n"]
[1218.236, "o", "    ----------\r\n"]
[1218.246, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[1218.256, "o", "        Data matrix.\r\n"]
[1218.266, "o", "\r\n"]
[1218.276, "o", "    n_components : int\r\n"]
[1218.286, "o", "        Number of dictionary atoms to extract.\r\n"]
[1218.296, "o", "\r\n"]
[1218.306, "o", "    alpha : int or float\r\n"]
[1218.316, "o", "        Sparsity controlling parameter.\r\n"]
[1218.326, "o", "\r\n"]
[1218.336, "o", "    max_iter : int, default=100\r\n"]
[1218.346, "o", "        Maximum number of iterations to perform.\r\n"]
[1218.356, "o", "\r\n"]
[1218.366, "o", "    tol : float, default=1e-8\r\n"]
[1218.376, "o", "        Tolerance for the stopping condition.\r\n"]
[1218.386, "o", "\r\n"]
[1218.396, "o", "    method : {'lars', 'cd'}, default='lars'\r\n"]
[1218.406, "o", "        The method used:\r\n"]
[1218.416, "o", "\r\n"]
[1218.426, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[1218.436, "o", "           problem (`linear_model.lars_path`);\r\n"]
[1218.446, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[1218.456, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[1218.466, "o", "          the estimated components are sparse.\r\n"]
[1218.476, "o", "\r\n"]
[1218.486, "o", "    n_jobs : int, default=None\r\n"]
[1218.496, "o", "        Number of parallel jobs to run.\r\n"]
[1218.506, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1218.516, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1218.526, "o", "        for more details.\r\n"]
[1218.536, "o", "\r\n"]
[1218.546, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1218.556, "o", "        Initial value for the dictionary for warm restart scenarios. Only used\r\n"]
[1218.566, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[1218.576, "o", "\r\n"]
[1218.586, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1218.596, "o", "        Initial value for the sparse code for warm restart scenarios. Only used\r\n"]
[1218.606, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[1218.616, "o", "\r\n"]
[1218.626, "o", "    callback : callable, default=None\r\n"]
[1218.636, "o", "        Callable that gets invoked every five iterations.\r\n"]
[1218.646, "o", "\r\n"]
[1218.656, "o", "    verbose : bool, default=False\r\n"]
[1218.666, "o", "        To control the verbosity of the procedure.\r\n"]
[1218.676, "o", "\r\n"]
[1218.686, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1218.696, "o", "        Used for randomly initializing the dictionary. Pass an int for\r\n"]
[1218.706, "o", "        reproducible results across multiple function calls.\r\n"]
[1218.716, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1218.726, "o", "\r\n"]
[1218.736, "o", "    return_n_iter : bool, default=False\r\n"]
[1218.746, "o", "        Whether or not to return the number of iterations.\r\n"]
[1218.756, "o", "\r\n"]
[1218.766, "o", "    positive_dict : bool, default=False\r\n"]
[1218.776, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1218.786, "o", "\r\n"]
[1218.796, "o", "        .. versionadded:: 0.20\r\n"]
[1218.806, "o", "\r\n"]
[1218.816, "o", "    positive_code : bool, default=False\r\n"]
[1218.826, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1218.836, "o", "\r\n"]
[1218.846, "o", "        .. versionadded:: 0.20\r\n"]
[1218.856, "o", "\r\n"]
[1218.866, "o", "    method_max_iter : int, default=1000\r\n"]
[1218.876, "o", "        Maximum number of iterations to perform.\r\n"]
[1218.886, "o", "\r\n"]
[1218.896, "o", "        .. versionadded:: 0.22\r\n"]
[1218.906, "o", "\r\n"]
[1218.916, "o", "    Returns\r\n"]
[1218.926, "o", "    -------\r\n"]
[1218.936, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1218.946, "o", "        The sparse code factor in the matrix factorization.\r\n"]
[1218.956, "o", "\r\n"]
[1218.966, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[1218.976, "o", "        The dictionary factor in the matrix factorization.\r\n"]
[1218.986, "o", "\r\n"]
[1218.996, "o", "    errors : array\r\n"]
[1219.006, "o", "        Vector of errors at each iteration.\r\n"]
[1219.016, "o", "\r\n"]
[1219.026, "o", "    n_iter : int\r\n"]
[1219.036, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[1219.046, "o", "        set to True.\r\n"]
[1219.056, "o", "\r\n"]
[1219.066, "o", "    See Also\r\n"]
[1219.076, "o", "    --------\r\n"]
[1219.086, "o", "    dict_learning_online : Solve a dictionary learning matrix factorization\r\n"]
[1219.096, "o", "        problem online.\r\n"]
[1219.106, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1219.116, "o", "    MiniBatchDictionaryLearning : A faster, less accurate version\r\n"]
[1219.126, "o", "        of the dictionary learning algorithm.\r\n"]
[1219.136, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1219.146, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1219.156, "o", "    \"\"\"\r\n"]
[1219.166, "o", "    estimator = DictionaryLearning(\r\n"]
[1219.176, "o", "        n_components=n_components,\r\n"]
[1219.186, "o", "        alpha=alpha,\r\n"]
[1219.196, "o", "        max_iter=max_iter,\r\n"]
[1219.206, "o", "        tol=tol,\r\n"]
[1219.216, "o", "        fit_algorithm=method,\r\n"]
[1219.226, "o", "        n_jobs=n_jobs,\r\n"]
[1219.236, "o", "        dict_init=dict_init,\r\n"]
[1219.246, "o", "        callback=callback,\r\n"]
[1219.256, "o", "        code_init=code_init,\r\n"]
[1219.266, "o", "        verbose=verbose,\r\n"]
[1219.276, "o", "        random_state=random_state,\r\n"]
[1219.286, "o", "        positive_code=positive_code,\r\n"]
[1219.296, "o", "        positive_dict=positive_dict,\r\n"]
[1219.306, "o", "        transform_max_iter=method_max_iter,\r\n"]
[1219.316, "o", "    )\r\n"]
[1219.326, "o", "    code = estimator.fit_transform(X)\r\n"]
[1219.336, "o", "    if return_n_iter:\r\n"]
[1219.346, "o", "        return (\r\n"]
[1219.356, "o", "            code,\r\n"]
[1219.366, "o", "            estimator.components_,\r\n"]
[1219.376, "o", "            estimator.error_,\r\n"]
[1219.386, "o", "            estimator.n_iter_,\r\n"]
[1219.396, "o", "        )\r\n"]
[1219.406, "o", "    return code, estimator.components_, estimator.error_\r\n"]
[1219.416, "o", "\r\n"]
[1219.426, "o", "\r\n"]
[1219.436, "o", "class _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n"]
[1219.446, "o", "    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n"]
[1219.456, "o", "\r\n"]
[1219.466, "o", "    def __init__(\r\n"]
[1219.476, "o", "        self,\r\n"]
[1219.486, "o", "        transform_algorithm,\r\n"]
[1219.496, "o", "        transform_n_nonzero_coefs,\r\n"]
[1219.506, "o", "        transform_alpha,\r\n"]
[1219.516, "o", "        split_sign,\r\n"]
[1219.526, "o", "        n_jobs,\r\n"]
[1219.536, "o", "        positive_code,\r\n"]
[1219.546, "o", "        transform_max_iter,\r\n"]
[1219.556, "o", "    ):\r\n"]
[1219.566, "o", "        self.transform_algorithm = transform_algorithm\r\n"]
[1219.576, "o", "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n"]
[1219.586, "o", "        self.transform_alpha = transform_alpha\r\n"]
[1219.596, "o", "        self.transform_max_iter = transform_max_iter\r\n"]
[1219.606, "o", "        self.split_sign = split_sign\r\n"]
[1219.616, "o", "        self.n_jobs = n_jobs\r\n"]
[1219.626, "o", "        self.positive_code = positive_code\r\n"]
[1219.636, "o", "\r\n"]
[1219.646, "o", "    def _transform(self, X, dictionary):\r\n"]
[1219.656, "o", "        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n"]
[1219.666, "o", "        SparseCoder.\"\"\"\r\n"]
[1219.676, "o", "        X = self._validate_data(X, reset=False)\r\n"]
[1219.686, "o", "\r\n"]
[1219.696, "o", "        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n"]
[1219.706, "o", "            transform_alpha = self.alpha\r\n"]
[1219.716, "o", "        else:\r\n"]
[1219.726, "o", "            transform_alpha = self.transform_alpha\r\n"]
[1219.736, "o", "\r\n"]
[1219.746, "o", "        code = sparse_encode(\r\n"]
[1219.756, "o", "            X,\r\n"]
[1219.766, "o", "            dictionary,\r\n"]
[1219.776, "o", "            algorithm=self.transform_algorithm,\r\n"]
[1219.786, "o", "            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n"]
[1219.796, "o", "            alpha=transform_alpha,\r\n"]
[1219.806, "o", "            max_iter=self.transform_max_iter,\r\n"]
[1219.816, "o", "            n_jobs=self.n_jobs,\r\n"]
[1219.826, "o", "            positive=self.positive_code,\r\n"]
[1219.836, "o", "        )\r\n"]
[1219.846, "o", "\r\n"]
[1219.856, "o", "        if self.split_sign:\r\n"]
[1219.866, "o", "            # feature vector is split into a positive and negative side\r\n"]
[1219.876, "o", "            n_samples, n_features = code.shape\r\n"]
[1219.886, "o", "            split_code = np.empty((n_samples, 2 * n_features))\r\n"]
[1219.896, "o", "            split_code[:, :n_features] = np.maximum(code, 0)\r\n"]
[1219.906, "o", "            split_code[:, n_features:] = -np.minimum(code, 0)\r\n"]
[1219.916, "o", "            code = split_code\r\n"]
[1219.926, "o", "\r\n"]
[1219.936, "o", "        return code\r\n"]
[1220.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1220.002, "i", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py\r"]
[1220.004, "o", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py\r\n"]
[1220.016, "o", "\u001b[?2004l\r\n"]
[1220.026, "o", "\r\n"]
[1220.036, "o", "    n_jobs : int or None, default=None\r\n"]
[1220.046, "o", "        Number of parallel jobs to run.\r\n"]
[1220.056, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1220.066, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1220.076, "o", "        for more details.\r\n"]
[1220.086, "o", "\r\n"]
[1220.096, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1220.106, "o", "        Initial value for the code, for warm restart. Only used if `code_init`\r\n"]
[1220.116, "o", "        and `dict_init` are not None.\r\n"]
[1220.126, "o", "\r\n"]
[1220.136, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1220.146, "o", "        Initial values for the dictionary, for warm restart. Only used if\r\n"]
[1220.156, "o", "        `code_init` and `dict_init` are not None.\r\n"]
[1220.166, "o", "\r\n"]
[1220.176, "o", "    callback : callable, default=None\r\n"]
[1220.186, "o", "        Callable that gets invoked every five iterations.\r\n"]
[1220.196, "o", "\r\n"]
[1220.206, "o", "        .. versionadded:: 1.3\r\n"]
[1220.216, "o", "\r\n"]
[1220.226, "o", "    verbose : bool, default=False\r\n"]
[1220.236, "o", "        To control the verbosity of the procedure.\r\n"]
[1220.246, "o", "\r\n"]
[1220.256, "o", "    split_sign : bool, default=False\r\n"]
[1220.266, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[1220.276, "o", "        its negative part and its positive part. This can improve the\r\n"]
[1220.286, "o", "        performance of downstream classifiers.\r\n"]
[1220.296, "o", "\r\n"]
[1220.306, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1220.316, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[1220.326, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[1220.336, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[1220.346, "o", "        results across multiple function calls.\r\n"]
[1220.356, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1220.366, "o", "\r\n"]
[1220.376, "o", "    positive_code : bool, default=False\r\n"]
[1220.386, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1220.396, "o", "\r\n"]
[1220.406, "o", "        .. versionadded:: 0.20\r\n"]
[1220.416, "o", "\r\n"]
[1220.426, "o", "    positive_dict : bool, default=False\r\n"]
[1220.436, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1220.446, "o", "\r\n"]
[1220.456, "o", "        .. versionadded:: 0.20\r\n"]
[1220.466, "o", "\r\n"]
[1220.476, "o", "    transform_max_iter : int, default=1000\r\n"]
[1220.486, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1220.496, "o", "        `'lasso_lars'`.\r\n"]
[1220.506, "o", "\r\n"]
[1220.516, "o", "        .. versionadded:: 0.22\r\n"]
[1220.526, "o", "\r\n"]
[1220.536, "o", "    Attributes\r\n"]
[1220.546, "o", "    ----------\r\n"]
[1220.556, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[1220.566, "o", "        dictionary atoms extracted from the data\r\n"]
[1220.576, "o", "\r\n"]
[1220.586, "o", "    error_ : array\r\n"]
[1220.596, "o", "        vector of errors at each iteration\r\n"]
[1220.606, "o", "\r\n"]
[1220.616, "o", "    n_features_in_ : int\r\n"]
[1220.626, "o", "        Number of features seen during :term:`fit`.\r\n"]
[1220.636, "o", "\r\n"]
[1220.646, "o", "        .. versionadded:: 0.24\r\n"]
[1220.656, "o", "\r\n"]
[1220.666, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[1220.676, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[1220.686, "o", "        has feature names that are all strings.\r\n"]
[1220.696, "o", "\r\n"]
[1220.706, "o", "        .. versionadded:: 1.0\r\n"]
[1220.716, "o", "\r\n"]
[1220.726, "o", "    n_iter_ : int\r\n"]
[1220.736, "o", "        Number of iterations run.\r\n"]
[1220.746, "o", "\r\n"]
[1220.756, "o", "    See Also\r\n"]
[1220.766, "o", "    --------\r\n"]
[1220.776, "o", "    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n"]
[1220.786, "o", "        dictionary learning algorithm.\r\n"]
[1220.796, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1220.806, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[1220.816, "o", "        precomputed dictionary.\r\n"]
[1220.826, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1220.836, "o", "\r\n"]
[1220.846, "o", "    References\r\n"]
[1220.856, "o", "    ----------\r\n"]
[1220.866, "o", "\r\n"]
[1220.876, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[1220.886, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[1220.896, "o", "\r\n"]
[1220.906, "o", "    Examples\r\n"]
[1220.916, "o", "    --------\r\n"]
[1220.926, "o", "    >>> import numpy as np\r\n"]
[1220.936, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[1220.946, "o", "    >>> from sklearn.decomposition import DictionaryLearning\r\n"]
[1220.956, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[1220.966, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[1220.976, "o", "    ...     random_state=42,\r\n"]
[1220.986, "o", "    ... )\r\n"]
[1220.996, "o", "    >>> dict_learner = DictionaryLearning(\r\n"]
[1221.006, "o", "    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n"]
[1221.016, "o", "    ...     random_state=42,\r\n"]
[1221.026, "o", "    ... )\r\n"]
[1221.036, "o", "    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n"]
[1221.046, "o", "\r\n"]
[1221.056, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[1221.066, "o", "\r\n"]
[1221.076, "o", "    >>> np.mean(X_transformed == 0)\r\n"]
[1221.086, "o", "    0.41...\r\n"]
[1221.096, "o", "\r\n"]
[1221.106, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[1221.116, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[1221.126, "o", "    the original signal:\r\n"]
[1221.136, "o", "\r\n"]
[1221.146, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[1221.156, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[1221.166, "o", "    0.07...\r\n"]
[1221.176, "o", "    \"\"\"\r\n"]
[1221.186, "o", "\r\n"]
[1221.196, "o", "    _parameter_constraints: dict = {\r\n"]
[1221.206, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1221.216, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1221.226, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1221.236, "o", "        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1221.246, "o", "        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[1221.256, "o", "        \"transform_algorithm\": [\r\n"]
[1221.266, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[1221.276, "o", "        ],\r\n"]
[1221.286, "o", "        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1221.296, "o", "        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[1221.306, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[1221.316, "o", "        \"code_init\": [np.ndarray, None],\r\n"]
[1221.326, "o", "        \"dict_init\": [np.ndarray, None],\r\n"]
[1221.336, "o", "        \"callback\": [callable, None],\r\n"]
[1221.346, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[1221.356, "o", "        \"split_sign\": [\"boolean\"],\r\n"]
[1221.366, "o", "        \"random_state\": [\"random_state\"],\r\n"]
[1221.376, "o", "        \"positive_code\": [\"boolean\"],\r\n"]
[1221.386, "o", "        \"positive_dict\": [\"boolean\"],\r\n"]
[1221.396, "o", "        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1221.406, "o", "    }\r\n"]
[1221.416, "o", "\r\n"]
[1221.426, "o", "    def __init__(\r\n"]
[1221.436, "o", "        self,\r\n"]
[1221.446, "o", "        n_components=None,\r\n"]
[1221.456, "o", "        *,\r\n"]
[1221.466, "o", "        alpha=1,\r\n"]
[1221.476, "o", "        max_iter=1000,\r\n"]
[1221.486, "o", "        tol=1e-8,\r\n"]
[1221.496, "o", "        fit_algorithm=\"lars\",\r\n"]
[1221.506, "o", "        transform_algorithm=\"omp\",\r\n"]
[1221.516, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[1221.526, "o", "        transform_alpha=None,\r\n"]
[1221.536, "o", "        n_jobs=None,\r\n"]
[1221.546, "o", "        code_init=None,\r\n"]
[1221.556, "o", "        dict_init=None,\r\n"]
[1221.566, "o", "        callback=None,\r\n"]
[1221.576, "o", "        verbose=False,\r\n"]
[1221.586, "o", "        split_sign=False,\r\n"]
[1221.596, "o", "        random_state=None,\r\n"]
[1221.606, "o", "        positive_code=False,\r\n"]
[1221.616, "o", "        positive_dict=False,\r\n"]
[1221.626, "o", "        transform_max_iter=1000,\r\n"]
[1221.636, "o", "    ):\r\n"]
[1221.646, "o", "        super().__init__(\r\n"]
[1221.656, "o", "            transform_algorithm,\r\n"]
[1221.666, "o", "            transform_n_nonzero_coefs,\r\n"]
[1221.676, "o", "            transform_alpha,\r\n"]
[1221.686, "o", "            split_sign,\r\n"]
[1221.696, "o", "            n_jobs,\r\n"]
[1221.706, "o", "            positive_code,\r\n"]
[1221.716, "o", "            transform_max_iter,\r\n"]
[1221.726, "o", "        )\r\n"]
[1221.736, "o", "        self.n_components = n_components\r\n"]
[1221.746, "o", "        self.alpha = alpha\r\n"]
[1221.756, "o", "        self.max_iter = max_iter\r\n"]
[1221.766, "o", "        self.tol = tol\r\n"]
[1221.776, "o", "        self.fit_algorithm = fit_algorithm\r\n"]
[1221.786, "o", "        self.code_init = code_init\r\n"]
[1221.796, "o", "        self.dict_init = dict_init\r\n"]
[1221.806, "o", "        self.callback = callback\r\n"]
[1221.816, "o", "        self.verbose = verbose\r\n"]
[1221.826, "o", "        self.random_state = random_state\r\n"]
[1221.836, "o", "        self.positive_dict = positive_dict\r\n"]
[1221.846, "o", "\r\n"]
[1221.856, "o", "    def fit(self, X, y=None):\r\n"]
[1221.866, "o", "        \"\"\"Fit the model from data in X.\r\n"]
[1221.876, "o", "\r\n"]
[1221.886, "o", "        Parameters\r\n"]
[1221.896, "o", "        ----------\r\n"]
[1221.906, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[1221.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1221.926, "o", "            and `n_features` is the number of features.\r\n"]
[1221.936, "o", "\r\n"]
[1221.946, "o", "        y : Ignored\r\n"]
[1221.956, "o", "            Not used, present for API consistency by convention.\r\n"]
[1221.966, "o", "\r\n"]
[1221.976, "o", "        Returns\r\n"]
[1221.986, "o", "        -------\r\n"]
[1221.996, "o", "        self : object\r\n"]
[1222.006, "o", "            Returns the instance itself.\r\n"]
[1222.016, "o", "        \"\"\"\r\n"]
[1222.026, "o", "        self.fit_transform(X)\r\n"]
[1222.036, "o", "        return self\r\n"]
[1222.046, "o", "\r\n"]
[1222.056, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[1222.066, "o", "    def fit_transform(self, X, y=None):\r\n"]
[1222.076, "o", "        \"\"\"Fit the model from data in X and return the transformed data.\r\n"]
[1222.086, "o", "\r\n"]
[1222.096, "o", "        Parameters\r\n"]
[1222.106, "o", "        ----------\r\n"]
[1222.116, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[1222.126, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1222.136, "o", "            and `n_features` is the number of features.\r\n"]
[1222.146, "o", "\r\n"]
[1222.156, "o", "        y : Ignored\r\n"]
[1222.166, "o", "            Not used, present for API consistency by convention.\r\n"]
[1222.176, "o", "\r\n"]
[1222.186, "o", "        Returns\r\n"]
[1222.196, "o", "        -------\r\n"]
[1222.206, "o", "        V : ndarray of shape (n_samples, n_components)\r\n"]
[1222.216, "o", "            Transformed data.\r\n"]
[1222.226, "o", "        \"\"\"\r\n"]
[1222.236, "o", "        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n"]
[1222.246, "o", "\r\n"]
[1222.256, "o", "        method = \"lasso_\" + self.fit_algorithm\r\n"]
[1222.266, "o", "\r\n"]
[1222.276, "o", "        random_state = check_random_state(self.random_state)\r\n"]
[1222.286, "o", "        X = self._validate_data(X)\r\n"]
[1222.296, "o", "\r\n"]
[1222.306, "o", "        if self.n_components is None:\r\n"]
[1222.316, "o", "            n_components = X.shape[1]\r\n"]
[1222.326, "o", "        else:\r\n"]
[1222.336, "o", "            n_components = self.n_components\r\n"]
[1222.346, "o", "\r\n"]
[1222.356, "o", "        V, U, E, self.n_iter_ = _dict_learning(\r\n"]
[1222.366, "o", "            X,\r\n"]
[1222.376, "o", "            n_components,\r\n"]
[1222.386, "o", "            alpha=self.alpha,\r\n"]
[1222.396, "o", "            tol=self.tol,\r\n"]
[1222.406, "o", "            max_iter=self.max_iter,\r\n"]
[1222.416, "o", "            method=method,\r\n"]
[1222.426, "o", "            method_max_iter=self.transform_max_iter,\r\n"]
[1222.436, "o", "            n_jobs=self.n_jobs,\r\n"]
[1222.446, "o", "            code_init=self.code_init,\r\n"]
[1222.456, "o", "            dict_init=self.dict_init,\r\n"]
[1222.466, "o", "            callback=self.callback,\r\n"]
[1222.476, "o", "            verbose=self.verbose,\r\n"]
[1222.486, "o", "            random_state=random_state,\r\n"]
[1222.496, "o", "            return_n_iter=True,\r\n"]
[1222.506, "o", "            positive_dict=self.positive_dict,\r\n"]
[1222.516, "o", "            positive_code=self.positive_code,\r\n"]
[1222.526, "o", "        )\r\n"]
[1222.536, "o", "        self.components_ = U\r\n"]
[1222.546, "o", "        self.error_ = E\r\n"]
[1222.556, "o", "\r\n"]
[1222.566, "o", "        return V\r\n"]
[1222.576, "o", "\r\n"]
[1222.586, "o", "    @property\r\n"]
[1222.596, "o", "    def _n_features_out(self):\r\n"]
[1222.606, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[1222.616, "o", "        return self.components_.shape[0]\r\n"]
[1222.626, "o", "\r\n"]
[1222.636, "o", "    def _more_tags(self):\r\n"]
[1222.646, "o", "        return {\r\n"]
[1222.656, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[1222.666, "o", "        }\r\n"]
[1222.676, "o", "\r\n"]
[1222.686, "o", "\r\n"]
[1222.696, "o", "class MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[1222.706, "o", "    \"\"\"Mini-batch dictionary learning.\r\n"]
[1222.716, "o", "\r\n"]
[1222.726, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[1222.736, "o", "    encoding the fitted data.\r\n"]
[1222.746, "o", "\r\n"]
[1222.756, "o", "    Solves the optimization problem::\r\n"]
[1222.766, "o", "\r\n"]
[1222.776, "o", "       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[1222.786, "o", "                    (U,V)\r\n"]
[1222.796, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[1222.806, "o", "\r\n"]
[1222.816, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[1222.826, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[1222.836, "o", "    of all the entries in the matrix.\r\n"]
[1222.846, "o", "\r\n"]
[1222.856, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[1222.866, "o", "\r\n"]
[1222.876, "o", "    Parameters\r\n"]
[1222.886, "o", "    ----------\r\n"]
[1222.896, "o", "    n_components : int, default=None\r\n"]
[1222.906, "o", "        Number of dictionary elements to extract.\r\n"]
[1222.916, "o", "\r\n"]
[1222.926, "o", "    alpha : float, default=1\r\n"]
[1222.936, "o", "        Sparsity controlling parameter.\r\n"]
[1222.946, "o", "\r\n"]
[1222.956, "o", "    n_iter : int, default=1000\r\n"]
[1222.966, "o", "        Total number of iterations over data batches to perform.\r\n"]
[1222.976, "o", "\r\n"]
[1222.986, "o", "        .. deprecated:: 1.1\r\n"]
[1222.996, "o", "           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[1223.006, "o", "           ``max_iter`` instead.\r\n"]
[1223.016, "o", "\r\n"]
[1223.026, "o", "    max_iter : int, default=None\r\n"]
[1223.036, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[1223.046, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[1223.056, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[1223.066, "o", "\r\n"]
[1223.076, "o", "        .. versionadded:: 1.1\r\n"]
[1223.086, "o", "\r\n"]
[1223.096, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[1223.106, "o", "        The algorithm used:\r\n"]
[1223.116, "o", "\r\n"]
[1223.126, "o", "        - `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[1223.136, "o", "          problem (`linear_model.lars_path`)\r\n"]
[1223.146, "o", "        - `'cd'`: uses the coordinate descent method to compute the\r\n"]
[1223.156, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[1223.166, "o", "          the estimated components are sparse.\r\n"]
[1223.176, "o", "\r\n"]
[1223.186, "o", "    n_jobs : int, default=None\r\n"]
[1223.196, "o", "        Number of parallel jobs to run.\r\n"]
[1223.206, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1223.216, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1223.226, "o", "        for more details.\r\n"]
[1223.236, "o", "\r\n"]
[1223.246, "o", "    batch_size : int, default=256\r\n"]
[1223.256, "o", "        Number of samples in each mini-batch.\r\n"]
[1223.266, "o", "\r\n"]
[1223.276, "o", "        .. versionchanged:: 1.3\r\n"]
[1223.286, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[1223.296, "o", "\r\n"]
[1223.306, "o", "    shuffle : bool, default=True\r\n"]
[1223.316, "o", "        Whether to shuffle the samples before forming batches.\r\n"]
[1223.326, "o", "\r\n"]
[1223.336, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1223.346, "o", "        Initial value of the dictionary for warm restart scenarios.\r\n"]
[1223.356, "o", "\r\n"]
[1223.366, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[1223.376, "o", "            'threshold'}, default='omp'\r\n"]
[1223.386, "o", "        Algorithm used to transform the data:\r\n"]
[1223.396, "o", "\r\n"]
[1223.406, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[1223.416, "o", "          (`linear_model.lars_path`);\r\n"]
[1223.426, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[1223.436, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1223.446, "o", "          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n"]
[1223.456, "o", "          if the estimated components are sparse.\r\n"]
[1223.466, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1223.476, "o", "          solution.\r\n"]
[1223.486, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[1223.496, "o", "          the projection ``dictionary * X'``.\r\n"]
[1223.506, "o", "\r\n"]
[1223.516, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[1223.526, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1223.536, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[1223.546, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[1223.556, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1223.566, "o", "\r\n"]
[1223.576, "o", "    transform_alpha : float, default=None\r\n"]
[1223.586, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1223.596, "o", "        penalty applied to the L1 norm.\r\n"]
[1223.606, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1223.616, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1223.626, "o", "        If `None`, defaults to `alpha`.\r\n"]
[1223.636, "o", "\r\n"]
[1223.646, "o", "        .. versionchanged:: 1.2\r\n"]
[1223.656, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[1223.666, "o", "\r\n"]
[1223.676, "o", "    verbose : bool or int, default=False\r\n"]
[1223.686, "o", "        To control the verbosity of the procedure.\r\n"]
[1223.696, "o", "\r\n"]
[1223.706, "o", "    split_sign : bool, default=False\r\n"]
[1223.716, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[1223.726, "o", "        its negative part and its positive part. This can improve the\r\n"]
[1223.736, "o", "        performance of downstream classifiers.\r\n"]
[1223.746, "o", "\r\n"]
[1223.756, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1223.766, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[1223.776, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[1223.786, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[1223.796, "o", "        results across multiple function calls.\r\n"]
[1223.806, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1223.816, "o", "\r\n"]
[1223.826, "o", "    positive_code : bool, default=False\r\n"]
[1223.836, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1223.846, "o", "\r\n"]
[1223.856, "o", "        .. versionadded:: 0.20\r\n"]
[1223.866, "o", "\r\n"]
[1223.876, "o", "    positive_dict : bool, default=False\r\n"]
[1223.886, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1223.896, "o", "\r\n"]
[1223.906, "o", "        .. versionadded:: 0.20\r\n"]
[1223.916, "o", "\r\n"]
[1223.926, "o", "    transform_max_iter : int, default=1000\r\n"]
[1223.936, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1223.946, "o", "        `'lasso_lars'`.\r\n"]
[1223.956, "o", "\r\n"]
[1223.966, "o", "        .. versionadded:: 0.22\r\n"]
[1223.976, "o", "\r\n"]
[1223.986, "o", "    callback : callable, default=None\r\n"]
[1223.996, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[1224.006, "o", "\r\n"]
[1224.016, "o", "        .. versionadded:: 1.1\r\n"]
[1224.026, "o", "\r\n"]
[1224.036, "o", "    tol : float, default=1e-3\r\n"]
[1224.046, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[1224.056, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[1224.066, "o", "\r\n"]
[1224.076, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[1224.086, "o", "        `tol` to 0.0.\r\n"]
[1224.096, "o", "\r\n"]
[1224.106, "o", "        .. versionadded:: 1.1\r\n"]
[1224.116, "o", "\r\n"]
[1224.126, "o", "    max_no_improvement : int, default=10\r\n"]
[1224.136, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[1224.146, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[1224.156, "o", "        `max_iter` is not None.\r\n"]
[1224.166, "o", "\r\n"]
[1224.176, "o", "        To disable convergence detection based on cost function, set\r\n"]
[1224.186, "o", "        `max_no_improvement` to None.\r\n"]
[1224.196, "o", "\r\n"]
[1224.206, "o", "        .. versionadded:: 1.1\r\n"]
[1224.216, "o", "\r\n"]
[1224.226, "o", "    Attributes\r\n"]
[1224.236, "o", "    ----------\r\n"]
[1224.246, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[1224.256, "o", "        Components extracted from the data.\r\n"]
[1224.266, "o", "\r\n"]
[1224.276, "o", "    n_features_in_ : int\r\n"]
[1224.286, "o", "        Number of features seen during :term:`fit`.\r\n"]
[1224.296, "o", "\r\n"]
[1224.306, "o", "        .. versionadded:: 0.24\r\n"]
[1224.316, "o", "\r\n"]
[1224.326, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[1224.336, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[1224.346, "o", "        has feature names that are all strings.\r\n"]
[1224.356, "o", "\r\n"]
[1224.366, "o", "        .. versionadded:: 1.0\r\n"]
[1224.376, "o", "\r\n"]
[1224.386, "o", "    n_iter_ : int\r\n"]
[1224.396, "o", "        Number of iterations over the full dataset.\r\n"]
[1224.406, "o", "\r\n"]
[1224.416, "o", "    n_steps_ : int\r\n"]
[1224.426, "o", "        Number of mini-batches processed.\r\n"]
[1224.436, "o", "\r\n"]
[1224.446, "o", "        .. versionadded:: 1.1\r\n"]
[1224.456, "o", "\r\n"]
[1224.466, "o", "    See Also\r\n"]
[1224.476, "o", "    --------\r\n"]
[1224.486, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1224.496, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1224.506, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[1224.516, "o", "        precomputed dictionary.\r\n"]
[1224.526, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1224.536, "o", "\r\n"]
[1224.546, "o", "    References\r\n"]
[1224.556, "o", "    ----------\r\n"]
[1224.566, "o", "\r\n"]
[1224.576, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[1224.586, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[1224.596, "o", "\r\n"]
[1224.606, "o", "    Examples\r\n"]
[1224.616, "o", "    --------\r\n"]
[1224.626, "o", "    >>> import numpy as np\r\n"]
[1224.636, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[1224.646, "o", "    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n"]
[1224.656, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[1224.666, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[1224.676, "o", "    ...     random_state=42)\r\n"]
[1224.686, "o", "    >>> dict_learner = MiniBatchDictionaryLearning(\r\n"]
[1224.696, "o", "    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n"]
[1224.706, "o", "    ...     transform_alpha=0.1, random_state=42)\r\n"]
[1224.716, "o", "    >>> X_transformed = dict_learner.fit_transform(X)\r\n"]
[1224.726, "o", "\r\n"]
[1224.736, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[1224.746, "o", "\r\n"]
[1224.756, "o", "    >>> np.mean(X_transformed == 0) < 0.5\r\n"]
[1224.766, "o", "    True\r\n"]
[1224.776, "o", "\r\n"]
[1224.786, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[1224.796, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[1224.806, "o", "    the original signal:\r\n"]
[1224.816, "o", "\r\n"]
[1224.826, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[1224.836, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[1224.846, "o", "    0.057...\r\n"]
[1224.856, "o", "    \"\"\"\r\n"]
[1224.866, "o", "\r\n"]
[1224.876, "o", "    _parameter_constraints: dict = {\r\n"]
[1224.886, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1224.896, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1224.906, "o", "        \"n_iter\": [\r\n"]
[1224.916, "o", "            Interval(Integral, 0, None, closed=\"left\"),\r\n"]
[1224.926, "o", "            Hidden(StrOptions({\"deprecated\"})),\r\n"]
[1224.936, "o", "        ],\r\n"]
[1225.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1225.002, "i", "pytest -q sklearn/decomposition/tests/test_dict_learning.py -k minibatch -q\r"]
[1225.004, "o", "pytest -q sklearn/decomposition/tests/test_dict_learning.py -k minibatch -q\r\n"]
[1227.478, "o", "\u001b[?2004l\r\n"]
[1230.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1230.002, "i", "cd asv_benchmarks\r"]
[1230.004, "o", "cd asv_benchmarks\r\n"]
[1230.006, "o", "\u001b[?2004l\r\n"]
[1235.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1235.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r"]
[1235.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r\n"]
[1235.315, "o", "stderr\r\n"]
[1235.624, "o", "\u001b[?2004l\r\n"]
[1235.933, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[1236.242, "o", "\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n"]
[1236.551, "o", "[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n"]
[1236.86, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n"]
[1237.169, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0m                                                                                     ok\r\n"]
[1237.478, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ============\u001b[0m\r\n"]
[1237.787, "o", "              --                 n_jobs   \r\n"]
[1238.096, "o", "              --------------- ------------\r\n"]
[1238.405, "o", "               fit_algorithm       1      \r\n"]
[1238.714, "o", "              =============== ============\r\n"]
[1239.023, "o", "                    lars       6.69\u00b10.06s \r\n"]
[1239.332, "o", "                     cd        1.64\u00b10.02s \r\n"]
[1239.641, "o", "              =============== ============\r\n"]
[1240.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1240.002, "i", "cd /workspace/repo\r"]
[1240.004, "o", "cd /workspace/repo\r\n"]
[1240.006, "o", "\u001b[?2004l\r\n"]
[1245.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1245.002, "i", "grep -n \"def _minibatch_step\\|_minibatch_step\\(\" sklearn/decomposition/_dict_learning.py\r"]
[1245.004, "o", "grep -n \"def _minibatch_step\\|_minibatch_step\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1247.478, "o", "\u001b[?2004l\r\n"]
[1250.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1250.002, "i", "grep -n \"_sparse_encode_precomputed\\s*(\\|sparse_encode\\s*(\" sklearn/decomposition/_dict_learning.py\r"]
[1250.004, "o", "grep -n \"_sparse_encode_precomputed\\s*(\\|sparse_encode\\s*(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1250.455455, "o", "\u001b[?2004l\r\n"]
[1250.904909, "o", "\u001b[32m\u001b[K38\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdef \u001b[01;31m\u001b[K_sparse_encode_precomputed(\u001b[m\u001b[K\r\n"]
[1251.354364, "o", "\u001b[32m\u001b[K227\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdef \u001b[01;31m\u001b[Ksparse_encode(\u001b[m\u001b[K\r\n"]
[1251.803818, "o", "\u001b[32m\u001b[K361\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    return _\u001b[01;31m\u001b[Ksparse_encode(\u001b[m\u001b[K\r\n"]
[1252.253273, "o", "\u001b[32m\u001b[K378\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdef _\u001b[01;31m\u001b[Ksparse_encode(\u001b[m\u001b[K\r\n"]
[1252.702727, "o", "\u001b[32m\u001b[K416\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        code = \u001b[01;31m\u001b[K_sparse_encode_precomputed(\u001b[m\u001b[K\r\n"]
[1253.152182, "o", "\u001b[32m\u001b[K604\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        code = \u001b[01;31m\u001b[Ksparse_encode(\u001b[m\u001b[K\r\n"]
[1253.601636, "o", "\u001b[32m\u001b[K1001\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        this_code = \u001b[01;31m\u001b[Ksparse_encode(\u001b[m\u001b[K\r\n"]
[1254.051091, "o", "\u001b[32m\u001b[K1052\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        code = \u001b[01;31m\u001b[Ksparse_encode(\u001b[m\u001b[K\r\n"]
[1254.500545, "o", "\u001b[32m\u001b[K1272\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        code = \u001b[01;31m\u001b[Ksparse_encode(\u001b[m\u001b[K\r\n"]
[1255.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1255.002, "i", "sed -n '1,900p' sklearn/decomposition/_dict_learning.py\r"]
[1255.004, "o", "sed -n '1,900p' sklearn/decomposition/_dict_learning.py\r\n"]
[1255.016, "o", "\u001b[?2004l\r\n"]
[1255.026, "o", "\"\"\" Dictionary learning.\r\n"]
[1255.036, "o", "\"\"\"\r\n"]
[1255.046, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[1255.056, "o", "# License: BSD 3 clause\r\n"]
[1255.066, "o", "\r\n"]
[1255.076, "o", "import itertools\r\n"]
[1255.086, "o", "import sys\r\n"]
[1255.096, "o", "import time\r\n"]
[1255.106, "o", "import warnings\r\n"]
[1255.116, "o", "from math import ceil\r\n"]
[1255.126, "o", "from numbers import Integral, Real\r\n"]
[1255.136, "o", "\r\n"]
[1255.146, "o", "import numpy as np\r\n"]
[1255.156, "o", "from joblib import effective_n_jobs\r\n"]
[1255.166, "o", "from scipy import linalg\r\n"]
[1255.176, "o", "\r\n"]
[1255.186, "o", "from ..base import (\r\n"]
[1255.196, "o", "    BaseEstimator,\r\n"]
[1255.206, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[1255.216, "o", "    TransformerMixin,\r\n"]
[1255.226, "o", "    _fit_context,\r\n"]
[1255.236, "o", ")\r\n"]
[1255.246, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[1255.256, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[1255.266, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[1255.276, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[1255.286, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[1255.296, "o", "from ..utils.validation import check_is_fitted\r\n"]
[1255.306, "o", "\r\n"]
[1255.316, "o", "\r\n"]
[1255.326, "o", "def _check_positive_coding(method, positive):\r\n"]
[1255.336, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[1255.346, "o", "        raise ValueError(\r\n"]
[1255.356, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[1255.366, "o", "        )\r\n"]
[1255.376, "o", "\r\n"]
[1255.386, "o", "\r\n"]
[1255.396, "o", "def _sparse_encode_precomputed(\r\n"]
[1255.406, "o", "    X,\r\n"]
[1255.416, "o", "    dictionary,\r\n"]
[1255.426, "o", "    *,\r\n"]
[1255.436, "o", "    gram=None,\r\n"]
[1255.446, "o", "    cov=None,\r\n"]
[1255.456, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1255.466, "o", "    regularization=None,\r\n"]
[1255.476, "o", "    copy_cov=True,\r\n"]
[1255.486, "o", "    init=None,\r\n"]
[1255.496, "o", "    max_iter=1000,\r\n"]
[1255.506, "o", "    verbose=0,\r\n"]
[1255.516, "o", "    positive=False,\r\n"]
[1255.526, "o", "):\r\n"]
[1255.536, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[1255.546, "o", "\r\n"]
[1255.556, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[1255.566, "o", "\r\n"]
[1255.576, "o", "    Parameters\r\n"]
[1255.586, "o", "    ----------\r\n"]
[1255.596, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[1255.606, "o", "        Data matrix.\r\n"]
[1255.616, "o", "\r\n"]
[1255.626, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1255.636, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1255.646, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[1255.656, "o", "\r\n"]
[1255.666, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[1255.676, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[1255.686, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[1255.696, "o", "\r\n"]
[1255.706, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[1255.716, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[1255.726, "o", "\r\n"]
[1255.736, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1255.746, "o", "            default='lasso_lars'\r\n"]
[1255.756, "o", "        The algorithm used:\r\n"]
[1255.766, "o", "\r\n"]
[1255.776, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[1255.786, "o", "          (`linear_model.lars_path`);\r\n"]
[1255.796, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1255.806, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1255.816, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1255.826, "o", "          the estimated components are sparse;\r\n"]
[1255.836, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1255.846, "o", "          solution;\r\n"]
[1255.856, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1255.866, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[1255.876, "o", "\r\n"]
[1255.886, "o", "    regularization : int or float, default=None\r\n"]
[1255.896, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[1255.906, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[1255.916, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[1255.926, "o", "\r\n"]
[1255.936, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1255.946, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[1255.956, "o", "        `algorithm='lasso_cd'`.\r\n"]
[1255.966, "o", "\r\n"]
[1255.976, "o", "    max_iter : int, default=1000\r\n"]
[1255.986, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1255.996, "o", "        `'lasso_lars'`.\r\n"]
[1256.006, "o", "\r\n"]
[1256.016, "o", "    copy_cov : bool, default=True\r\n"]
[1256.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1256.036, "o", "        be overwritten.\r\n"]
[1256.046, "o", "\r\n"]
[1256.056, "o", "    verbose : int, default=0\r\n"]
[1256.066, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[1256.076, "o", "\r\n"]
[1256.086, "o", "    positive: bool, default=False\r\n"]
[1256.096, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[1256.106, "o", "\r\n"]
[1256.116, "o", "        .. versionadded:: 0.20\r\n"]
[1256.126, "o", "\r\n"]
[1256.136, "o", "    Returns\r\n"]
[1256.146, "o", "    -------\r\n"]
[1256.156, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[1256.166, "o", "        The sparse codes.\r\n"]
[1256.176, "o", "    \"\"\"\r\n"]
[1256.186, "o", "    n_samples, n_features = X.shape\r\n"]
[1256.196, "o", "    n_components = dictionary.shape[0]\r\n"]
[1256.206, "o", "\r\n"]
[1256.216, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[1256.226, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1256.236, "o", "        try:\r\n"]
[1256.246, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1256.256, "o", "\r\n"]
[1256.266, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1256.276, "o", "            # corrects the verbosity level.\r\n"]
[1256.286, "o", "            lasso_lars = LassoLars(\r\n"]
[1256.296, "o", "                alpha=alpha,\r\n"]
[1256.306, "o", "                fit_intercept=False,\r\n"]
[1256.316, "o", "                verbose=verbose,\r\n"]
[1256.326, "o", "                precompute=gram,\r\n"]
[1256.336, "o", "                fit_path=False,\r\n"]
[1256.346, "o", "                positive=positive,\r\n"]
[1256.356, "o", "                max_iter=max_iter,\r\n"]
[1256.366, "o", "            )\r\n"]
[1256.376, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1256.386, "o", "            new_code = lasso_lars.coef_\r\n"]
[1256.396, "o", "        finally:\r\n"]
[1256.406, "o", "            np.seterr(**err_mgt)\r\n"]
[1256.416, "o", "\r\n"]
[1256.426, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[1256.436, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1256.446, "o", "\r\n"]
[1256.456, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[1256.466, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[1256.476, "o", "        # argument that we could pass in from Lasso.\r\n"]
[1256.486, "o", "        clf = Lasso(\r\n"]
[1256.496, "o", "            alpha=alpha,\r\n"]
[1256.506, "o", "            fit_intercept=False,\r\n"]
[1256.516, "o", "            precompute=gram,\r\n"]
[1256.526, "o", "            max_iter=max_iter,\r\n"]
[1256.536, "o", "            warm_start=True,\r\n"]
[1256.546, "o", "            positive=positive,\r\n"]
[1256.556, "o", "        )\r\n"]
[1256.566, "o", "\r\n"]
[1256.576, "o", "        if init is not None:\r\n"]
[1256.586, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[1256.596, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[1256.606, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[1256.616, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[1256.626, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[1256.636, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[1256.646, "o", "                init = np.array(init)\r\n"]
[1256.656, "o", "            clf.coef_ = init\r\n"]
[1256.666, "o", "\r\n"]
[1256.676, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[1256.686, "o", "        new_code = clf.coef_\r\n"]
[1256.696, "o", "\r\n"]
[1256.706, "o", "    elif algorithm == \"lars\":\r\n"]
[1256.716, "o", "        try:\r\n"]
[1256.726, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1256.736, "o", "\r\n"]
[1256.746, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1256.756, "o", "            # corrects the verbosity level.\r\n"]
[1256.766, "o", "            lars = Lars(\r\n"]
[1256.776, "o", "                fit_intercept=False,\r\n"]
[1256.786, "o", "                verbose=verbose,\r\n"]
[1256.796, "o", "                precompute=gram,\r\n"]
[1256.806, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[1256.816, "o", "                fit_path=False,\r\n"]
[1256.826, "o", "            )\r\n"]
[1256.836, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1256.846, "o", "            new_code = lars.coef_\r\n"]
[1256.856, "o", "        finally:\r\n"]
[1256.866, "o", "            np.seterr(**err_mgt)\r\n"]
[1256.876, "o", "\r\n"]
[1256.886, "o", "    elif algorithm == \"threshold\":\r\n"]
[1256.896, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[1256.906, "o", "        if positive:\r\n"]
[1256.916, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[1256.926, "o", "\r\n"]
[1256.936, "o", "    elif algorithm == \"omp\":\r\n"]
[1256.946, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[1256.956, "o", "            Gram=gram,\r\n"]
[1256.966, "o", "            Xy=cov,\r\n"]
[1256.976, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[1256.986, "o", "            tol=None,\r\n"]
[1256.996, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[1257.006, "o", "            copy_Xy=copy_cov,\r\n"]
[1257.016, "o", "        ).T\r\n"]
[1257.026, "o", "\r\n"]
[1257.036, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[1257.046, "o", "\r\n"]
[1257.056, "o", "\r\n"]
[1257.066, "o", "@validate_params(\r\n"]
[1257.076, "o", "    {\r\n"]
[1257.086, "o", "        \"X\": [\"array-like\"],\r\n"]
[1257.096, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[1257.106, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[1257.116, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[1257.126, "o", "        \"algorithm\": [\r\n"]
[1257.136, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[1257.146, "o", "        ],\r\n"]
[1257.156, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1257.166, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[1257.176, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[1257.186, "o", "        \"init\": [\"array-like\", None],\r\n"]
[1257.196, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1257.206, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[1257.216, "o", "        \"check_input\": [\"boolean\"],\r\n"]
[1257.226, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[1257.236, "o", "        \"positive\": [\"boolean\"],\r\n"]
[1257.246, "o", "    },\r\n"]
[1257.256, "o", "    prefer_skip_nested_validation=True,\r\n"]
[1257.266, "o", ")\r\n"]
[1257.276, "o", "# XXX : could be moved to the linear_model module\r\n"]
[1257.286, "o", "def sparse_encode(\r\n"]
[1257.296, "o", "    X,\r\n"]
[1257.306, "o", "    dictionary,\r\n"]
[1257.316, "o", "    *,\r\n"]
[1257.326, "o", "    gram=None,\r\n"]
[1257.336, "o", "    cov=None,\r\n"]
[1257.346, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1257.356, "o", "    n_nonzero_coefs=None,\r\n"]
[1257.366, "o", "    alpha=None,\r\n"]
[1257.376, "o", "    copy_cov=True,\r\n"]
[1257.386, "o", "    init=None,\r\n"]
[1257.396, "o", "    max_iter=1000,\r\n"]
[1257.406, "o", "    n_jobs=None,\r\n"]
[1257.416, "o", "    check_input=True,\r\n"]
[1257.426, "o", "    verbose=0,\r\n"]
[1257.436, "o", "    positive=False,\r\n"]
[1257.446, "o", "):\r\n"]
[1257.456, "o", "    \"\"\"Sparse coding.\r\n"]
[1257.466, "o", "\r\n"]
[1257.476, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[1257.486, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[1257.496, "o", "\r\n"]
[1257.506, "o", "        X ~= code * dictionary\r\n"]
[1257.516, "o", "\r\n"]
[1257.526, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[1257.536, "o", "\r\n"]
[1257.546, "o", "    Parameters\r\n"]
[1257.556, "o", "    ----------\r\n"]
[1257.566, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[1257.576, "o", "        Data matrix.\r\n"]
[1257.586, "o", "\r\n"]
[1257.596, "o", "    dictionary : array-like of shape (n_components, n_features)\r\n"]
[1257.606, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1257.616, "o", "        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[1257.626, "o", "        output.\r\n"]
[1257.636, "o", "\r\n"]
[1257.646, "o", "    gram : array-like of shape (n_components, n_components), default=None\r\n"]
[1257.656, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n"]
[1257.666, "o", "\r\n"]
[1257.676, "o", "    cov : array-like of shape (n_components, n_samples), default=None\r\n"]
[1257.686, "o", "        Precomputed covariance, `dictionary' * X`.\r\n"]
[1257.696, "o", "\r\n"]
[1257.706, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1257.716, "o", "            default='lasso_lars'\r\n"]
[1257.726, "o", "        The algorithm used:\r\n"]
[1257.736, "o", "\r\n"]
[1257.746, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[1257.756, "o", "          (`linear_model.lars_path`);\r\n"]
[1257.766, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1257.776, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1257.786, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1257.796, "o", "          the estimated components are sparse;\r\n"]
[1257.806, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1257.816, "o", "          solution;\r\n"]
[1257.826, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1257.836, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[1257.846, "o", "\r\n"]
[1257.856, "o", "    n_nonzero_coefs : int, default=None\r\n"]
[1257.866, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1257.876, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[1257.886, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[1257.896, "o", "        `n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1257.906, "o", "\r\n"]
[1257.916, "o", "    alpha : float, default=None\r\n"]
[1257.926, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1257.936, "o", "        penalty applied to the L1 norm.\r\n"]
[1257.946, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1257.956, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1257.966, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[1257.976, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[1257.986, "o", "        `n_nonzero_coefs`.\r\n"]
[1257.996, "o", "        If `None`, default to 1.\r\n"]
[1258.006, "o", "\r\n"]
[1258.016, "o", "    copy_cov : bool, default=True\r\n"]
[1258.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1258.036, "o", "        be overwritten.\r\n"]
[1258.046, "o", "\r\n"]
[1258.056, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1258.066, "o", "        Initialization value of the sparse codes. Only used if\r\n"]
[1258.076, "o", "        `algorithm='lasso_cd'`.\r\n"]
[1258.086, "o", "\r\n"]
[1258.096, "o", "    max_iter : int, default=1000\r\n"]
[1258.106, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1258.116, "o", "        `'lasso_lars'`.\r\n"]
[1258.126, "o", "\r\n"]
[1258.136, "o", "    n_jobs : int, default=None\r\n"]
[1258.146, "o", "        Number of parallel jobs to run.\r\n"]
[1258.156, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1258.166, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1258.176, "o", "        for more details.\r\n"]
[1258.186, "o", "\r\n"]
[1258.196, "o", "    check_input : bool, default=True\r\n"]
[1258.206, "o", "        If `False`, the input arrays X and dictionary will not be checked.\r\n"]
[1258.216, "o", "\r\n"]
[1258.226, "o", "    verbose : int, default=0\r\n"]
[1258.236, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[1258.246, "o", "\r\n"]
[1258.256, "o", "    positive : bool, default=False\r\n"]
[1258.266, "o", "        Whether to enforce positivity when finding the encoding.\r\n"]
[1258.276, "o", "\r\n"]
[1258.286, "o", "        .. versionadded:: 0.20\r\n"]
[1258.296, "o", "\r\n"]
[1258.306, "o", "    Returns\r\n"]
[1258.316, "o", "    -------\r\n"]
[1258.326, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1258.336, "o", "        The sparse codes.\r\n"]
[1258.346, "o", "\r\n"]
[1258.356, "o", "    See Also\r\n"]
[1258.366, "o", "    --------\r\n"]
[1258.376, "o", "    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n"]
[1258.386, "o", "        path using LARS algorithm.\r\n"]
[1258.396, "o", "    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n"]
[1258.406, "o", "    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n"]
[1258.416, "o", "    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n"]
[1258.426, "o", "        dictionary.\r\n"]
[1258.436, "o", "    \"\"\"\r\n"]
[1258.446, "o", "    if check_input:\r\n"]
[1258.456, "o", "        if algorithm == \"lasso_cd\":\r\n"]
[1258.466, "o", "            dictionary = check_array(\r\n"]
[1258.476, "o", "                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n"]
[1258.486, "o", "            )\r\n"]
[1258.496, "o", "            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n"]
[1258.506, "o", "        else:\r\n"]
[1258.516, "o", "            dictionary = check_array(dictionary)\r\n"]
[1258.526, "o", "            X = check_array(X)\r\n"]
[1258.536, "o", "\r\n"]
[1258.546, "o", "    if dictionary.shape[1] != X.shape[1]:\r\n"]
[1258.556, "o", "        raise ValueError(\r\n"]
[1258.566, "o", "            \"Dictionary and X have different numbers of features:\"\r\n"]
[1258.576, "o", "            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n"]
[1258.586, "o", "        )\r\n"]
[1258.596, "o", "\r\n"]
[1258.606, "o", "    _check_positive_coding(algorithm, positive)\r\n"]
[1258.616, "o", "\r\n"]
[1258.626, "o", "    return _sparse_encode(\r\n"]
[1258.636, "o", "        X,\r\n"]
[1258.646, "o", "        dictionary,\r\n"]
[1258.656, "o", "        gram=gram,\r\n"]
[1258.666, "o", "        cov=cov,\r\n"]
[1258.676, "o", "        algorithm=algorithm,\r\n"]
[1258.686, "o", "        n_nonzero_coefs=n_nonzero_coefs,\r\n"]
[1258.696, "o", "        alpha=alpha,\r\n"]
[1258.706, "o", "        copy_cov=copy_cov,\r\n"]
[1258.716, "o", "        init=init,\r\n"]
[1258.726, "o", "        max_iter=max_iter,\r\n"]
[1258.736, "o", "        n_jobs=n_jobs,\r\n"]
[1258.746, "o", "        verbose=verbose,\r\n"]
[1258.756, "o", "        positive=positive,\r\n"]
[1258.766, "o", "    )\r\n"]
[1258.776, "o", "\r\n"]
[1258.786, "o", "\r\n"]
[1258.796, "o", "def _sparse_encode(\r\n"]
[1258.806, "o", "    X,\r\n"]
[1258.816, "o", "    dictionary,\r\n"]
[1258.826, "o", "    *,\r\n"]
[1258.836, "o", "    gram=None,\r\n"]
[1258.846, "o", "    cov=None,\r\n"]
[1258.856, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1258.866, "o", "    n_nonzero_coefs=None,\r\n"]
[1258.876, "o", "    alpha=None,\r\n"]
[1258.886, "o", "    copy_cov=True,\r\n"]
[1258.896, "o", "    init=None,\r\n"]
[1258.906, "o", "    max_iter=1000,\r\n"]
[1258.916, "o", "    n_jobs=None,\r\n"]
[1258.926, "o", "    verbose=0,\r\n"]
[1258.936, "o", "    positive=False,\r\n"]
[1258.946, "o", "):\r\n"]
[1258.956, "o", "    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[1258.966, "o", "\r\n"]
[1258.976, "o", "    n_samples, n_features = X.shape\r\n"]
[1258.986, "o", "    n_components = dictionary.shape[0]\r\n"]
[1258.996, "o", "\r\n"]
[1259.006, "o", "    if algorithm in (\"lars\", \"omp\"):\r\n"]
[1259.016, "o", "        regularization = n_nonzero_coefs\r\n"]
[1259.026, "o", "        if regularization is None:\r\n"]
[1259.036, "o", "            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[1259.046, "o", "    else:\r\n"]
[1259.056, "o", "        regularization = alpha\r\n"]
[1259.066, "o", "        if regularization is None:\r\n"]
[1259.076, "o", "            regularization = 1.0\r\n"]
[1259.086, "o", "\r\n"]
[1259.096, "o", "    if gram is None and algorithm != \"threshold\":\r\n"]
[1259.106, "o", "        gram = np.dot(dictionary, dictionary.T)\r\n"]
[1259.116, "o", "\r\n"]
[1259.126, "o", "    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[1259.136, "o", "        copy_cov = False\r\n"]
[1259.146, "o", "        cov = np.dot(dictionary, X.T)\r\n"]
[1259.156, "o", "\r\n"]
[1259.166, "o", "    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[1259.176, "o", "        code = _sparse_encode_precomputed(\r\n"]
[1259.186, "o", "            X,\r\n"]
[1259.196, "o", "            dictionary,\r\n"]
[1259.206, "o", "            gram=gram,\r\n"]
[1259.216, "o", "            cov=cov,\r\n"]
[1259.226, "o", "            algorithm=algorithm,\r\n"]
[1259.236, "o", "            regularization=regularization,\r\n"]
[1259.246, "o", "            copy_cov=copy_cov,\r\n"]
[1259.256, "o", "            init=init,\r\n"]
[1259.266, "o", "            max_iter=max_iter,\r\n"]
[1259.276, "o", "            verbose=verbose,\r\n"]
[1259.286, "o", "            positive=positive,\r\n"]
[1259.296, "o", "        )\r\n"]
[1259.306, "o", "        return code\r\n"]
[1259.316, "o", "\r\n"]
[1259.326, "o", "    # Enter parallel code block\r\n"]
[1259.336, "o", "    n_samples = X.shape[0]\r\n"]
[1259.346, "o", "    n_components = dictionary.shape[0]\r\n"]
[1259.356, "o", "    code = np.empty((n_samples, n_components))\r\n"]
[1259.366, "o", "    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[1259.376, "o", "\r\n"]
[1259.386, "o", "    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[1259.396, "o", "        delayed(_sparse_encode_precomputed)(\r\n"]
[1259.406, "o", "            X[this_slice],\r\n"]
[1259.416, "o", "            dictionary,\r\n"]
[1259.426, "o", "            gram=gram,\r\n"]
[1259.436, "o", "            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[1259.446, "o", "            algorithm=algorithm,\r\n"]
[1259.456, "o", "            regularization=regularization,\r\n"]
[1259.466, "o", "            copy_cov=copy_cov,\r\n"]
[1259.476, "o", "            init=init[this_slice] if init is not None else None,\r\n"]
[1259.486, "o", "            max_iter=max_iter,\r\n"]
[1259.496, "o", "            verbose=verbose,\r\n"]
[1259.506, "o", "            positive=positive,\r\n"]
[1259.516, "o", "        )\r\n"]
[1259.526, "o", "        for this_slice in slices\r\n"]
[1259.536, "o", "    )\r\n"]
[1259.546, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[1259.556, "o", "        code[this_slice] = this_view\r\n"]
[1259.566, "o", "    return code\r\n"]
[1259.576, "o", "\r\n"]
[1259.586, "o", "\r\n"]
[1259.596, "o", "def _update_dict(\r\n"]
[1259.606, "o", "    dictionary,\r\n"]
[1259.616, "o", "    Y,\r\n"]
[1259.626, "o", "    code,\r\n"]
[1259.636, "o", "    A=None,\r\n"]
[1259.646, "o", "    B=None,\r\n"]
[1259.656, "o", "    verbose=False,\r\n"]
[1259.666, "o", "    random_state=None,\r\n"]
[1259.676, "o", "    positive=False,\r\n"]
[1259.686, "o", "):\r\n"]
[1259.696, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[1259.706, "o", "\r\n"]
[1259.716, "o", "    Parameters\r\n"]
[1259.726, "o", "    ----------\r\n"]
[1259.736, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1259.746, "o", "        Value of the dictionary at the previous iteration.\r\n"]
[1259.756, "o", "\r\n"]
[1259.766, "o", "    Y : ndarray of shape (n_samples, n_features)\r\n"]
[1259.776, "o", "        Data matrix.\r\n"]
[1259.786, "o", "\r\n"]
[1259.796, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1259.806, "o", "        Sparse coding of the data against which to optimize the dictionary.\r\n"]
[1259.816, "o", "\r\n"]
[1259.826, "o", "    A : ndarray of shape (n_components, n_components), default=None\r\n"]
[1259.836, "o", "        Together with `B`, sufficient stats of the online model to update the\r\n"]
[1259.846, "o", "        dictionary.\r\n"]
[1259.856, "o", "\r\n"]
[1259.866, "o", "    B : ndarray of shape (n_features, n_components), default=None\r\n"]
[1259.876, "o", "        Together with `A`, sufficient stats of the online model to update the\r\n"]
[1259.886, "o", "        dictionary.\r\n"]
[1259.896, "o", "\r\n"]
[1259.906, "o", "    verbose: bool, default=False\r\n"]
[1259.916, "o", "        Degree of output the procedure will print.\r\n"]
[1259.926, "o", "\r\n"]
[1259.936, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1260.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1260.002, "i", "sed -n '900,1800p' sklearn/decomposition/_dict_learning.py\r"]
[1260.004, "o", "sed -n '900,1800p' sklearn/decomposition/_dict_learning.py\r\n"]
[1260.016, "o", "\u001b[?2004l\r\n"]
[1260.026, "o", "            random_state=random_state,\r\n"]
[1260.036, "o", "            transform_algorithm=transform_algorithm,\r\n"]
[1260.046, "o", "            transform_alpha=alpha,\r\n"]
[1260.056, "o", "            positive_code=positive_code,\r\n"]
[1260.066, "o", "            positive_dict=positive_dict,\r\n"]
[1260.076, "o", "            transform_max_iter=method_max_iter,\r\n"]
[1260.086, "o", "            verbose=verbose,\r\n"]
[1260.096, "o", "            callback=callback,\r\n"]
[1260.106, "o", "            tol=tol,\r\n"]
[1260.116, "o", "            max_no_improvement=max_no_improvement,\r\n"]
[1260.126, "o", "        ).fit(X)\r\n"]
[1260.136, "o", "\r\n"]
[1260.146, "o", "        if not return_code:\r\n"]
[1260.156, "o", "            return est.components_\r\n"]
[1260.166, "o", "        else:\r\n"]
[1260.176, "o", "            code = est.transform(X)\r\n"]
[1260.186, "o", "            return code, est.components_\r\n"]
[1260.196, "o", "\r\n"]
[1260.206, "o", "    # TODO(1.4) remove the whole old behavior\r\n"]
[1260.216, "o", "    # Fallback to old behavior\r\n"]
[1260.226, "o", "\r\n"]
[1260.236, "o", "    n_iter = _check_warn_deprecated(\r\n"]
[1260.246, "o", "        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n"]
[1260.256, "o", "    )\r\n"]
[1260.266, "o", "\r\n"]
[1260.276, "o", "    if n_components is None:\r\n"]
[1260.286, "o", "        n_components = X.shape[1]\r\n"]
[1260.296, "o", "\r\n"]
[1260.306, "o", "    if method not in (\"lars\", \"cd\"):\r\n"]
[1260.316, "o", "        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n"]
[1260.326, "o", "\r\n"]
[1260.336, "o", "    _check_positive_coding(method, positive_code)\r\n"]
[1260.346, "o", "\r\n"]
[1260.356, "o", "    method = \"lasso_\" + method\r\n"]
[1260.366, "o", "\r\n"]
[1260.376, "o", "    t0 = time.time()\r\n"]
[1260.386, "o", "    n_samples, n_features = X.shape\r\n"]
[1260.396, "o", "    # Avoid integer division problems\r\n"]
[1260.406, "o", "    alpha = float(alpha)\r\n"]
[1260.416, "o", "    random_state = check_random_state(random_state)\r\n"]
[1260.426, "o", "\r\n"]
[1260.436, "o", "    # Init V with SVD of X\r\n"]
[1260.446, "o", "    if dict_init is not None:\r\n"]
[1260.456, "o", "        dictionary = dict_init\r\n"]
[1260.466, "o", "    else:\r\n"]
[1260.476, "o", "        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n"]
[1260.486, "o", "        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[1260.496, "o", "    r = len(dictionary)\r\n"]
[1260.506, "o", "    if n_components <= r:\r\n"]
[1260.516, "o", "        dictionary = dictionary[:n_components, :]\r\n"]
[1260.526, "o", "    else:\r\n"]
[1260.536, "o", "        dictionary = np.r_[\r\n"]
[1260.546, "o", "            dictionary,\r\n"]
[1260.556, "o", "            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n"]
[1260.566, "o", "        ]\r\n"]
[1260.576, "o", "\r\n"]
[1260.586, "o", "    if verbose == 1:\r\n"]
[1260.596, "o", "        print(\"[dict_learning]\", end=\" \")\r\n"]
[1260.606, "o", "\r\n"]
[1260.616, "o", "    if shuffle:\r\n"]
[1260.626, "o", "        X_train = X.copy()\r\n"]
[1260.636, "o", "        random_state.shuffle(X_train)\r\n"]
[1260.646, "o", "    else:\r\n"]
[1260.656, "o", "        X_train = X\r\n"]
[1260.666, "o", "\r\n"]
[1260.676, "o", "    X_train = check_array(\r\n"]
[1260.686, "o", "        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n"]
[1260.696, "o", "    )\r\n"]
[1260.706, "o", "\r\n"]
[1260.716, "o", "    # Fortran-order dict better suited for the sparse coding which is the\r\n"]
[1260.726, "o", "    # bottleneck of this algorithm.\r\n"]
[1260.736, "o", "    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n"]
[1260.746, "o", "    dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[1260.756, "o", "\r\n"]
[1260.766, "o", "    batches = gen_batches(n_samples, batch_size)\r\n"]
[1260.776, "o", "    batches = itertools.cycle(batches)\r\n"]
[1260.786, "o", "\r\n"]
[1260.796, "o", "    # The covariance of the dictionary\r\n"]
[1260.806, "o", "    if inner_stats is None:\r\n"]
[1260.816, "o", "        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n"]
[1260.826, "o", "        # The data approximation\r\n"]
[1260.836, "o", "        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n"]
[1260.846, "o", "    else:\r\n"]
[1260.856, "o", "        A = inner_stats[0].copy()\r\n"]
[1260.866, "o", "        B = inner_stats[1].copy()\r\n"]
[1260.876, "o", "\r\n"]
[1260.886, "o", "    # If n_iter is zero, we need to return zero.\r\n"]
[1260.896, "o", "    ii = iter_offset - 1\r\n"]
[1260.906, "o", "\r\n"]
[1260.916, "o", "    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n"]
[1260.926, "o", "        this_X = X_train[batch]\r\n"]
[1260.936, "o", "        dt = time.time() - t0\r\n"]
[1260.946, "o", "        if verbose == 1:\r\n"]
[1260.956, "o", "            sys.stdout.write(\".\")\r\n"]
[1260.966, "o", "            sys.stdout.flush()\r\n"]
[1260.976, "o", "        elif verbose:\r\n"]
[1260.986, "o", "            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n"]
[1260.996, "o", "                print(\r\n"]
[1261.006, "o", "                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n"]
[1261.016, "o", "                )\r\n"]
[1261.026, "o", "\r\n"]
[1261.036, "o", "        this_code = sparse_encode(\r\n"]
[1261.046, "o", "            this_X,\r\n"]
[1261.056, "o", "            dictionary,\r\n"]
[1261.066, "o", "            algorithm=method,\r\n"]
[1261.076, "o", "            alpha=alpha,\r\n"]
[1261.086, "o", "            n_jobs=n_jobs,\r\n"]
[1261.096, "o", "            check_input=False,\r\n"]
[1261.106, "o", "            positive=positive_code,\r\n"]
[1261.116, "o", "            max_iter=method_max_iter,\r\n"]
[1261.126, "o", "            verbose=verbose,\r\n"]
[1261.136, "o", "        )\r\n"]
[1261.146, "o", "\r\n"]
[1261.156, "o", "        # Update the auxiliary variables\r\n"]
[1261.166, "o", "        if ii < batch_size - 1:\r\n"]
[1261.176, "o", "            theta = float((ii + 1) * batch_size)\r\n"]
[1261.186, "o", "        else:\r\n"]
[1261.196, "o", "            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n"]
[1261.206, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[1261.216, "o", "\r\n"]
[1261.226, "o", "        A *= beta\r\n"]
[1261.236, "o", "        A += np.dot(this_code.T, this_code)\r\n"]
[1261.246, "o", "        B *= beta\r\n"]
[1261.256, "o", "        B += np.dot(this_X.T, this_code)\r\n"]
[1261.266, "o", "\r\n"]
[1261.276, "o", "        # Update dictionary in place\r\n"]
[1261.286, "o", "        _update_dict(\r\n"]
[1261.296, "o", "            dictionary,\r\n"]
[1261.306, "o", "            this_X,\r\n"]
[1261.316, "o", "            this_code,\r\n"]
[1261.326, "o", "            A,\r\n"]
[1261.336, "o", "            B,\r\n"]
[1261.346, "o", "            verbose=verbose,\r\n"]
[1261.356, "o", "            random_state=random_state,\r\n"]
[1261.366, "o", "            positive=positive_dict,\r\n"]
[1261.376, "o", "        )\r\n"]
[1261.386, "o", "\r\n"]
[1261.396, "o", "        # Maybe we need a stopping criteria based on the amount of\r\n"]
[1261.406, "o", "        # modification in the dictionary\r\n"]
[1261.416, "o", "        if callback is not None:\r\n"]
[1261.426, "o", "            callback(locals())\r\n"]
[1261.436, "o", "\r\n"]
[1261.446, "o", "    if return_inner_stats:\r\n"]
[1261.456, "o", "        if return_n_iter:\r\n"]
[1261.466, "o", "            return dictionary, (A, B), ii - iter_offset + 1\r\n"]
[1261.476, "o", "        else:\r\n"]
[1261.486, "o", "            return dictionary, (A, B)\r\n"]
[1261.496, "o", "    if return_code:\r\n"]
[1261.506, "o", "        if verbose > 1:\r\n"]
[1261.516, "o", "            print(\"Learning code...\", end=\" \")\r\n"]
[1261.526, "o", "        elif verbose == 1:\r\n"]
[1261.536, "o", "            print(\"|\", end=\" \")\r\n"]
[1261.546, "o", "        code = sparse_encode(\r\n"]
[1261.556, "o", "            X,\r\n"]
[1261.566, "o", "            dictionary,\r\n"]
[1261.576, "o", "            algorithm=method,\r\n"]
[1261.586, "o", "            alpha=alpha,\r\n"]
[1261.596, "o", "            n_jobs=n_jobs,\r\n"]
[1261.606, "o", "            check_input=False,\r\n"]
[1261.616, "o", "            positive=positive_code,\r\n"]
[1261.626, "o", "            max_iter=method_max_iter,\r\n"]
[1261.636, "o", "            verbose=verbose,\r\n"]
[1261.646, "o", "        )\r\n"]
[1261.656, "o", "        if verbose > 1:\r\n"]
[1261.666, "o", "            dt = time.time() - t0\r\n"]
[1261.676, "o", "            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n"]
[1261.686, "o", "        if return_n_iter:\r\n"]
[1261.696, "o", "            return code, dictionary, ii - iter_offset + 1\r\n"]
[1261.706, "o", "        else:\r\n"]
[1261.716, "o", "            return code, dictionary\r\n"]
[1261.726, "o", "\r\n"]
[1261.736, "o", "    if return_n_iter:\r\n"]
[1261.746, "o", "        return dictionary, ii - iter_offset + 1\r\n"]
[1261.756, "o", "    else:\r\n"]
[1261.766, "o", "        return dictionary\r\n"]
[1261.776, "o", "\r\n"]
[1261.786, "o", "\r\n"]
[1261.796, "o", "@validate_params(\r\n"]
[1261.806, "o", "    {\r\n"]
[1261.816, "o", "        \"X\": [\"array-like\"],\r\n"]
[1261.826, "o", "        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[1261.836, "o", "        \"return_n_iter\": [\"boolean\"],\r\n"]
[1261.846, "o", "        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1261.856, "o", "    },\r\n"]
[1261.866, "o", "    prefer_skip_nested_validation=False,\r\n"]
[1261.876, "o", ")\r\n"]
[1261.886, "o", "def dict_learning(\r\n"]
[1261.896, "o", "    X,\r\n"]
[1261.906, "o", "    n_components,\r\n"]
[1261.916, "o", "    *,\r\n"]
[1261.926, "o", "    alpha,\r\n"]
[1261.936, "o", "    max_iter=100,\r\n"]
[1261.946, "o", "    tol=1e-8,\r\n"]
[1261.956, "o", "    method=\"lars\",\r\n"]
[1261.966, "o", "    n_jobs=None,\r\n"]
[1261.976, "o", "    dict_init=None,\r\n"]
[1261.986, "o", "    code_init=None,\r\n"]
[1261.996, "o", "    callback=None,\r\n"]
[1262.006, "o", "    verbose=False,\r\n"]
[1262.016, "o", "    random_state=None,\r\n"]
[1262.026, "o", "    return_n_iter=False,\r\n"]
[1262.036, "o", "    positive_dict=False,\r\n"]
[1262.046, "o", "    positive_code=False,\r\n"]
[1262.056, "o", "    method_max_iter=1000,\r\n"]
[1262.066, "o", "):\r\n"]
[1262.076, "o", "    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n"]
[1262.086, "o", "\r\n"]
[1262.096, "o", "    Finds the best dictionary and the corresponding sparse code for\r\n"]
[1262.106, "o", "    approximating the data matrix X by solving::\r\n"]
[1262.116, "o", "\r\n"]
[1262.126, "o", "        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[1262.136, "o", "                     (U,V)\r\n"]
[1262.146, "o", "                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n"]
[1262.156, "o", "\r\n"]
[1262.166, "o", "    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n"]
[1262.176, "o", "    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n"]
[1262.186, "o", "    which is the sum of the absolute values of all the entries in the matrix.\r\n"]
[1262.196, "o", "\r\n"]
[1262.206, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[1262.216, "o", "\r\n"]
[1262.226, "o", "    Parameters\r\n"]
[1262.236, "o", "    ----------\r\n"]
[1262.246, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[1262.256, "o", "        Data matrix.\r\n"]
[1262.266, "o", "\r\n"]
[1262.276, "o", "    n_components : int\r\n"]
[1262.286, "o", "        Number of dictionary atoms to extract.\r\n"]
[1262.296, "o", "\r\n"]
[1262.306, "o", "    alpha : int or float\r\n"]
[1262.316, "o", "        Sparsity controlling parameter.\r\n"]
[1262.326, "o", "\r\n"]
[1262.336, "o", "    max_iter : int, default=100\r\n"]
[1262.346, "o", "        Maximum number of iterations to perform.\r\n"]
[1262.356, "o", "\r\n"]
[1262.366, "o", "    tol : float, default=1e-8\r\n"]
[1262.376, "o", "        Tolerance for the stopping condition.\r\n"]
[1262.386, "o", "\r\n"]
[1262.396, "o", "    method : {'lars', 'cd'}, default='lars'\r\n"]
[1262.406, "o", "        The method used:\r\n"]
[1262.416, "o", "\r\n"]
[1262.426, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[1262.436, "o", "           problem (`linear_model.lars_path`);\r\n"]
[1262.446, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[1262.456, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[1262.466, "o", "          the estimated components are sparse.\r\n"]
[1262.476, "o", "\r\n"]
[1262.486, "o", "    n_jobs : int, default=None\r\n"]
[1262.496, "o", "        Number of parallel jobs to run.\r\n"]
[1262.506, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1262.516, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1262.526, "o", "        for more details.\r\n"]
[1262.536, "o", "\r\n"]
[1262.546, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1262.556, "o", "        Initial value for the dictionary for warm restart scenarios. Only used\r\n"]
[1262.566, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[1262.576, "o", "\r\n"]
[1262.586, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1262.596, "o", "        Initial value for the sparse code for warm restart scenarios. Only used\r\n"]
[1262.606, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[1262.616, "o", "\r\n"]
[1262.626, "o", "    callback : callable, default=None\r\n"]
[1262.636, "o", "        Callable that gets invoked every five iterations.\r\n"]
[1262.646, "o", "\r\n"]
[1262.656, "o", "    verbose : bool, default=False\r\n"]
[1262.666, "o", "        To control the verbosity of the procedure.\r\n"]
[1262.676, "o", "\r\n"]
[1262.686, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1262.696, "o", "        Used for randomly initializing the dictionary. Pass an int for\r\n"]
[1262.706, "o", "        reproducible results across multiple function calls.\r\n"]
[1262.716, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1262.726, "o", "\r\n"]
[1262.736, "o", "    return_n_iter : bool, default=False\r\n"]
[1262.746, "o", "        Whether or not to return the number of iterations.\r\n"]
[1262.756, "o", "\r\n"]
[1262.766, "o", "    positive_dict : bool, default=False\r\n"]
[1262.776, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1262.786, "o", "\r\n"]
[1262.796, "o", "        .. versionadded:: 0.20\r\n"]
[1262.806, "o", "\r\n"]
[1262.816, "o", "    positive_code : bool, default=False\r\n"]
[1262.826, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1262.836, "o", "\r\n"]
[1262.846, "o", "        .. versionadded:: 0.20\r\n"]
[1262.856, "o", "\r\n"]
[1262.866, "o", "    method_max_iter : int, default=1000\r\n"]
[1262.876, "o", "        Maximum number of iterations to perform.\r\n"]
[1262.886, "o", "\r\n"]
[1262.896, "o", "        .. versionadded:: 0.22\r\n"]
[1262.906, "o", "\r\n"]
[1262.916, "o", "    Returns\r\n"]
[1262.926, "o", "    -------\r\n"]
[1262.936, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1262.946, "o", "        The sparse code factor in the matrix factorization.\r\n"]
[1262.956, "o", "\r\n"]
[1262.966, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[1262.976, "o", "        The dictionary factor in the matrix factorization.\r\n"]
[1262.986, "o", "\r\n"]
[1262.996, "o", "    errors : array\r\n"]
[1263.006, "o", "        Vector of errors at each iteration.\r\n"]
[1263.016, "o", "\r\n"]
[1263.026, "o", "    n_iter : int\r\n"]
[1263.036, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[1263.046, "o", "        set to True.\r\n"]
[1263.056, "o", "\r\n"]
[1263.066, "o", "    See Also\r\n"]
[1263.076, "o", "    --------\r\n"]
[1263.086, "o", "    dict_learning_online : Solve a dictionary learning matrix factorization\r\n"]
[1263.096, "o", "        problem online.\r\n"]
[1263.106, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1263.116, "o", "    MiniBatchDictionaryLearning : A faster, less accurate version\r\n"]
[1263.126, "o", "        of the dictionary learning algorithm.\r\n"]
[1263.136, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1263.146, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1263.156, "o", "    \"\"\"\r\n"]
[1263.166, "o", "    estimator = DictionaryLearning(\r\n"]
[1263.176, "o", "        n_components=n_components,\r\n"]
[1263.186, "o", "        alpha=alpha,\r\n"]
[1263.196, "o", "        max_iter=max_iter,\r\n"]
[1263.206, "o", "        tol=tol,\r\n"]
[1263.216, "o", "        fit_algorithm=method,\r\n"]
[1263.226, "o", "        n_jobs=n_jobs,\r\n"]
[1263.236, "o", "        dict_init=dict_init,\r\n"]
[1263.246, "o", "        callback=callback,\r\n"]
[1263.256, "o", "        code_init=code_init,\r\n"]
[1263.266, "o", "        verbose=verbose,\r\n"]
[1263.276, "o", "        random_state=random_state,\r\n"]
[1263.286, "o", "        positive_code=positive_code,\r\n"]
[1263.296, "o", "        positive_dict=positive_dict,\r\n"]
[1263.306, "o", "        transform_max_iter=method_max_iter,\r\n"]
[1263.316, "o", "    )\r\n"]
[1263.326, "o", "    code = estimator.fit_transform(X)\r\n"]
[1263.336, "o", "    if return_n_iter:\r\n"]
[1263.346, "o", "        return (\r\n"]
[1263.356, "o", "            code,\r\n"]
[1263.366, "o", "            estimator.components_,\r\n"]
[1263.376, "o", "            estimator.error_,\r\n"]
[1263.386, "o", "            estimator.n_iter_,\r\n"]
[1263.396, "o", "        )\r\n"]
[1263.406, "o", "    return code, estimator.components_, estimator.error_\r\n"]
[1263.416, "o", "\r\n"]
[1263.426, "o", "\r\n"]
[1263.436, "o", "class _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n"]
[1263.446, "o", "    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n"]
[1263.456, "o", "\r\n"]
[1263.466, "o", "    def __init__(\r\n"]
[1263.476, "o", "        self,\r\n"]
[1263.486, "o", "        transform_algorithm,\r\n"]
[1263.496, "o", "        transform_n_nonzero_coefs,\r\n"]
[1263.506, "o", "        transform_alpha,\r\n"]
[1263.516, "o", "        split_sign,\r\n"]
[1263.526, "o", "        n_jobs,\r\n"]
[1263.536, "o", "        positive_code,\r\n"]
[1263.546, "o", "        transform_max_iter,\r\n"]
[1263.556, "o", "    ):\r\n"]
[1263.566, "o", "        self.transform_algorithm = transform_algorithm\r\n"]
[1263.576, "o", "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n"]
[1263.586, "o", "        self.transform_alpha = transform_alpha\r\n"]
[1263.596, "o", "        self.transform_max_iter = transform_max_iter\r\n"]
[1263.606, "o", "        self.split_sign = split_sign\r\n"]
[1263.616, "o", "        self.n_jobs = n_jobs\r\n"]
[1263.626, "o", "        self.positive_code = positive_code\r\n"]
[1263.636, "o", "\r\n"]
[1263.646, "o", "    def _transform(self, X, dictionary):\r\n"]
[1263.656, "o", "        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n"]
[1263.666, "o", "        SparseCoder.\"\"\"\r\n"]
[1263.676, "o", "        X = self._validate_data(X, reset=False)\r\n"]
[1263.686, "o", "\r\n"]
[1263.696, "o", "        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n"]
[1263.706, "o", "            transform_alpha = self.alpha\r\n"]
[1263.716, "o", "        else:\r\n"]
[1263.726, "o", "            transform_alpha = self.transform_alpha\r\n"]
[1263.736, "o", "\r\n"]
[1263.746, "o", "        code = sparse_encode(\r\n"]
[1263.756, "o", "            X,\r\n"]
[1263.766, "o", "            dictionary,\r\n"]
[1263.776, "o", "            algorithm=self.transform_algorithm,\r\n"]
[1263.786, "o", "            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n"]
[1263.796, "o", "            alpha=transform_alpha,\r\n"]
[1263.806, "o", "            max_iter=self.transform_max_iter,\r\n"]
[1263.816, "o", "            n_jobs=self.n_jobs,\r\n"]
[1263.826, "o", "            positive=self.positive_code,\r\n"]
[1263.836, "o", "        )\r\n"]
[1263.846, "o", "\r\n"]
[1263.856, "o", "        if self.split_sign:\r\n"]
[1263.866, "o", "            # feature vector is split into a positive and negative side\r\n"]
[1263.876, "o", "            n_samples, n_features = code.shape\r\n"]
[1263.886, "o", "            split_code = np.empty((n_samples, 2 * n_features))\r\n"]
[1263.896, "o", "            split_code[:, :n_features] = np.maximum(code, 0)\r\n"]
[1263.906, "o", "            split_code[:, n_features:] = -np.minimum(code, 0)\r\n"]
[1263.916, "o", "            code = split_code\r\n"]
[1263.926, "o", "\r\n"]
[1263.936, "o", "        return code\r\n"]
[1263.946, "o", "\r\n"]
[1263.956, "o", "    def transform(self, X):\r\n"]
[1263.966, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[1263.976, "o", "\r\n"]
[1263.986, "o", "        Coding method is determined by the object parameter\r\n"]
[1263.996, "o", "        `transform_algorithm`.\r\n"]
[1264.006, "o", "\r\n"]
[1264.016, "o", "        Parameters\r\n"]
[1264.026, "o", "        ----------\r\n"]
[1264.036, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[1264.046, "o", "            Test data to be transformed, must have the same number of\r\n"]
[1264.056, "o", "            features as the data used to train the model.\r\n"]
[1264.066, "o", "\r\n"]
[1264.076, "o", "        Returns\r\n"]
[1264.086, "o", "        -------\r\n"]
[1264.096, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[1264.106, "o", "            Transformed data.\r\n"]
[1264.116, "o", "        \"\"\"\r\n"]
[1264.126, "o", "        check_is_fitted(self)\r\n"]
[1264.136, "o", "        return self._transform(X, self.components_)\r\n"]
[1264.146, "o", "\r\n"]
[1264.156, "o", "\r\n"]
[1264.166, "o", "class SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n"]
[1264.176, "o", "    \"\"\"Sparse coding.\r\n"]
[1264.186, "o", "\r\n"]
[1264.196, "o", "    Finds a sparse representation of data against a fixed, precomputed\r\n"]
[1264.206, "o", "    dictionary.\r\n"]
[1264.216, "o", "\r\n"]
[1264.226, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[1264.236, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[1264.246, "o", "\r\n"]
[1264.256, "o", "        X ~= code * dictionary\r\n"]
[1264.266, "o", "\r\n"]
[1264.276, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[1264.286, "o", "\r\n"]
[1264.296, "o", "    Parameters\r\n"]
[1264.306, "o", "    ----------\r\n"]
[1264.316, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1264.326, "o", "        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n"]
[1264.336, "o", "        normalized to unit norm.\r\n"]
[1264.346, "o", "\r\n"]
[1264.356, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[1264.366, "o", "            'threshold'}, default='omp'\r\n"]
[1264.376, "o", "        Algorithm used to transform the data:\r\n"]
[1264.386, "o", "\r\n"]
[1264.396, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[1264.406, "o", "          (`linear_model.lars_path`);\r\n"]
[1264.416, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1264.426, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1264.436, "o", "          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n"]
[1264.446, "o", "          the estimated components are sparse;\r\n"]
[1264.456, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1264.466, "o", "          solution;\r\n"]
[1264.476, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[1264.486, "o", "          the projection ``dictionary * X'``.\r\n"]
[1264.496, "o", "\r\n"]
[1264.506, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[1264.516, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1264.526, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[1264.536, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[1264.546, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1264.556, "o", "\r\n"]
[1264.566, "o", "    transform_alpha : float, default=None\r\n"]
[1264.576, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1264.586, "o", "        penalty applied to the L1 norm.\r\n"]
[1264.596, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1264.606, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1264.616, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[1264.626, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[1264.636, "o", "        `n_nonzero_coefs`.\r\n"]
[1264.646, "o", "        If `None`, default to 1.\r\n"]
[1264.656, "o", "\r\n"]
[1264.666, "o", "    split_sign : bool, default=False\r\n"]
[1264.676, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[1264.686, "o", "        its negative part and its positive part. This can improve the\r\n"]
[1264.696, "o", "        performance of downstream classifiers.\r\n"]
[1264.706, "o", "\r\n"]
[1264.716, "o", "    n_jobs : int, default=None\r\n"]
[1264.726, "o", "        Number of parallel jobs to run.\r\n"]
[1264.736, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1264.746, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1264.756, "o", "        for more details.\r\n"]
[1264.766, "o", "\r\n"]
[1264.776, "o", "    positive_code : bool, default=False\r\n"]
[1264.786, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1264.796, "o", "\r\n"]
[1264.806, "o", "        .. versionadded:: 0.20\r\n"]
[1264.816, "o", "\r\n"]
[1264.826, "o", "    transform_max_iter : int, default=1000\r\n"]
[1264.836, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1264.846, "o", "        `lasso_lars`.\r\n"]
[1264.856, "o", "\r\n"]
[1264.866, "o", "        .. versionadded:: 0.22\r\n"]
[1264.876, "o", "\r\n"]
[1264.886, "o", "    Attributes\r\n"]
[1264.896, "o", "    ----------\r\n"]
[1264.906, "o", "    n_components_ : int\r\n"]
[1264.916, "o", "        Number of atoms.\r\n"]
[1264.926, "o", "\r\n"]
[1264.936, "o", "    n_features_in_ : int\r\n"]
[1265.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1265.002, "i", "sed -n '1800,2600p' sklearn/decomposition/_dict_learning.py\r"]
[1265.004, "o", "sed -n '1800,2600p' sklearn/decomposition/_dict_learning.py\r\n"]
[1265.016, "o", "\u001b[?2004l\r\n"]
[1265.026, "o", "        self.fit_transform(X)\r\n"]
[1265.036, "o", "        return self\r\n"]
[1265.046, "o", "\r\n"]
[1265.056, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[1265.066, "o", "    def fit_transform(self, X, y=None):\r\n"]
[1265.076, "o", "        \"\"\"Fit the model from data in X and return the transformed data.\r\n"]
[1265.086, "o", "\r\n"]
[1265.096, "o", "        Parameters\r\n"]
[1265.106, "o", "        ----------\r\n"]
[1265.116, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[1265.126, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1265.136, "o", "            and `n_features` is the number of features.\r\n"]
[1265.146, "o", "\r\n"]
[1265.156, "o", "        y : Ignored\r\n"]
[1265.166, "o", "            Not used, present for API consistency by convention.\r\n"]
[1265.176, "o", "\r\n"]
[1265.186, "o", "        Returns\r\n"]
[1265.196, "o", "        -------\r\n"]
[1265.206, "o", "        V : ndarray of shape (n_samples, n_components)\r\n"]
[1265.216, "o", "            Transformed data.\r\n"]
[1265.226, "o", "        \"\"\"\r\n"]
[1265.236, "o", "        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n"]
[1265.246, "o", "\r\n"]
[1265.256, "o", "        method = \"lasso_\" + self.fit_algorithm\r\n"]
[1265.266, "o", "\r\n"]
[1265.276, "o", "        random_state = check_random_state(self.random_state)\r\n"]
[1265.286, "o", "        X = self._validate_data(X)\r\n"]
[1265.296, "o", "\r\n"]
[1265.306, "o", "        if self.n_components is None:\r\n"]
[1265.316, "o", "            n_components = X.shape[1]\r\n"]
[1265.326, "o", "        else:\r\n"]
[1265.336, "o", "            n_components = self.n_components\r\n"]
[1265.346, "o", "\r\n"]
[1265.356, "o", "        V, U, E, self.n_iter_ = _dict_learning(\r\n"]
[1265.366, "o", "            X,\r\n"]
[1265.376, "o", "            n_components,\r\n"]
[1265.386, "o", "            alpha=self.alpha,\r\n"]
[1265.396, "o", "            tol=self.tol,\r\n"]
[1265.406, "o", "            max_iter=self.max_iter,\r\n"]
[1265.416, "o", "            method=method,\r\n"]
[1265.426, "o", "            method_max_iter=self.transform_max_iter,\r\n"]
[1265.436, "o", "            n_jobs=self.n_jobs,\r\n"]
[1265.446, "o", "            code_init=self.code_init,\r\n"]
[1265.456, "o", "            dict_init=self.dict_init,\r\n"]
[1265.466, "o", "            callback=self.callback,\r\n"]
[1265.476, "o", "            verbose=self.verbose,\r\n"]
[1265.486, "o", "            random_state=random_state,\r\n"]
[1265.496, "o", "            return_n_iter=True,\r\n"]
[1265.506, "o", "            positive_dict=self.positive_dict,\r\n"]
[1265.516, "o", "            positive_code=self.positive_code,\r\n"]
[1265.526, "o", "        )\r\n"]
[1265.536, "o", "        self.components_ = U\r\n"]
[1265.546, "o", "        self.error_ = E\r\n"]
[1265.556, "o", "\r\n"]
[1265.566, "o", "        return V\r\n"]
[1265.576, "o", "\r\n"]
[1265.586, "o", "    @property\r\n"]
[1265.596, "o", "    def _n_features_out(self):\r\n"]
[1265.606, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[1265.616, "o", "        return self.components_.shape[0]\r\n"]
[1265.626, "o", "\r\n"]
[1265.636, "o", "    def _more_tags(self):\r\n"]
[1265.646, "o", "        return {\r\n"]
[1265.656, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[1265.666, "o", "        }\r\n"]
[1265.676, "o", "\r\n"]
[1265.686, "o", "\r\n"]
[1265.696, "o", "class MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[1265.706, "o", "    \"\"\"Mini-batch dictionary learning.\r\n"]
[1265.716, "o", "\r\n"]
[1265.726, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[1265.736, "o", "    encoding the fitted data.\r\n"]
[1265.746, "o", "\r\n"]
[1265.756, "o", "    Solves the optimization problem::\r\n"]
[1265.766, "o", "\r\n"]
[1265.776, "o", "       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[1265.786, "o", "                    (U,V)\r\n"]
[1265.796, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[1265.806, "o", "\r\n"]
[1265.816, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[1265.826, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[1265.836, "o", "    of all the entries in the matrix.\r\n"]
[1265.846, "o", "\r\n"]
[1265.856, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[1265.866, "o", "\r\n"]
[1265.876, "o", "    Parameters\r\n"]
[1265.886, "o", "    ----------\r\n"]
[1265.896, "o", "    n_components : int, default=None\r\n"]
[1265.906, "o", "        Number of dictionary elements to extract.\r\n"]
[1265.916, "o", "\r\n"]
[1265.926, "o", "    alpha : float, default=1\r\n"]
[1265.936, "o", "        Sparsity controlling parameter.\r\n"]
[1265.946, "o", "\r\n"]
[1265.956, "o", "    n_iter : int, default=1000\r\n"]
[1265.966, "o", "        Total number of iterations over data batches to perform.\r\n"]
[1265.976, "o", "\r\n"]
[1265.986, "o", "        .. deprecated:: 1.1\r\n"]
[1265.996, "o", "           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[1266.006, "o", "           ``max_iter`` instead.\r\n"]
[1266.016, "o", "\r\n"]
[1266.026, "o", "    max_iter : int, default=None\r\n"]
[1266.036, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[1266.046, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[1266.056, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[1266.066, "o", "\r\n"]
[1266.076, "o", "        .. versionadded:: 1.1\r\n"]
[1266.086, "o", "\r\n"]
[1266.096, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[1266.106, "o", "        The algorithm used:\r\n"]
[1266.116, "o", "\r\n"]
[1266.126, "o", "        - `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[1266.136, "o", "          problem (`linear_model.lars_path`)\r\n"]
[1266.146, "o", "        - `'cd'`: uses the coordinate descent method to compute the\r\n"]
[1266.156, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[1266.166, "o", "          the estimated components are sparse.\r\n"]
[1266.176, "o", "\r\n"]
[1266.186, "o", "    n_jobs : int, default=None\r\n"]
[1266.196, "o", "        Number of parallel jobs to run.\r\n"]
[1266.206, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1266.216, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1266.226, "o", "        for more details.\r\n"]
[1266.236, "o", "\r\n"]
[1266.246, "o", "    batch_size : int, default=256\r\n"]
[1266.256, "o", "        Number of samples in each mini-batch.\r\n"]
[1266.266, "o", "\r\n"]
[1266.276, "o", "        .. versionchanged:: 1.3\r\n"]
[1266.286, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[1266.296, "o", "\r\n"]
[1266.306, "o", "    shuffle : bool, default=True\r\n"]
[1266.316, "o", "        Whether to shuffle the samples before forming batches.\r\n"]
[1266.326, "o", "\r\n"]
[1266.336, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1266.346, "o", "        Initial value of the dictionary for warm restart scenarios.\r\n"]
[1266.356, "o", "\r\n"]
[1266.366, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[1266.376, "o", "            'threshold'}, default='omp'\r\n"]
[1266.386, "o", "        Algorithm used to transform the data:\r\n"]
[1266.396, "o", "\r\n"]
[1266.406, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[1266.416, "o", "          (`linear_model.lars_path`);\r\n"]
[1266.426, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[1266.436, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1266.446, "o", "          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n"]
[1266.456, "o", "          if the estimated components are sparse.\r\n"]
[1266.466, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1266.476, "o", "          solution.\r\n"]
[1266.486, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[1266.496, "o", "          the projection ``dictionary * X'``.\r\n"]
[1266.506, "o", "\r\n"]
[1266.516, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[1266.526, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1266.536, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[1266.546, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[1266.556, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1266.566, "o", "\r\n"]
[1266.576, "o", "    transform_alpha : float, default=None\r\n"]
[1266.586, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1266.596, "o", "        penalty applied to the L1 norm.\r\n"]
[1266.606, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1266.616, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1266.626, "o", "        If `None`, defaults to `alpha`.\r\n"]
[1266.636, "o", "\r\n"]
[1266.646, "o", "        .. versionchanged:: 1.2\r\n"]
[1266.656, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[1266.666, "o", "\r\n"]
[1266.676, "o", "    verbose : bool or int, default=False\r\n"]
[1266.686, "o", "        To control the verbosity of the procedure.\r\n"]
[1266.696, "o", "\r\n"]
[1266.706, "o", "    split_sign : bool, default=False\r\n"]
[1266.716, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[1266.726, "o", "        its negative part and its positive part. This can improve the\r\n"]
[1266.736, "o", "        performance of downstream classifiers.\r\n"]
[1266.746, "o", "\r\n"]
[1266.756, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1266.766, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[1266.776, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[1266.786, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[1266.796, "o", "        results across multiple function calls.\r\n"]
[1266.806, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1266.816, "o", "\r\n"]
[1266.826, "o", "    positive_code : bool, default=False\r\n"]
[1266.836, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1266.846, "o", "\r\n"]
[1266.856, "o", "        .. versionadded:: 0.20\r\n"]
[1266.866, "o", "\r\n"]
[1266.876, "o", "    positive_dict : bool, default=False\r\n"]
[1266.886, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1266.896, "o", "\r\n"]
[1266.906, "o", "        .. versionadded:: 0.20\r\n"]
[1266.916, "o", "\r\n"]
[1266.926, "o", "    transform_max_iter : int, default=1000\r\n"]
[1266.936, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1266.946, "o", "        `'lasso_lars'`.\r\n"]
[1266.956, "o", "\r\n"]
[1266.966, "o", "        .. versionadded:: 0.22\r\n"]
[1266.976, "o", "\r\n"]
[1266.986, "o", "    callback : callable, default=None\r\n"]
[1266.996, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[1267.006, "o", "\r\n"]
[1267.016, "o", "        .. versionadded:: 1.1\r\n"]
[1267.026, "o", "\r\n"]
[1267.036, "o", "    tol : float, default=1e-3\r\n"]
[1267.046, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[1267.056, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[1267.066, "o", "\r\n"]
[1267.076, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[1267.086, "o", "        `tol` to 0.0.\r\n"]
[1267.096, "o", "\r\n"]
[1267.106, "o", "        .. versionadded:: 1.1\r\n"]
[1267.116, "o", "\r\n"]
[1267.126, "o", "    max_no_improvement : int, default=10\r\n"]
[1267.136, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[1267.146, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[1267.156, "o", "        `max_iter` is not None.\r\n"]
[1267.166, "o", "\r\n"]
[1267.176, "o", "        To disable convergence detection based on cost function, set\r\n"]
[1267.186, "o", "        `max_no_improvement` to None.\r\n"]
[1267.196, "o", "\r\n"]
[1267.206, "o", "        .. versionadded:: 1.1\r\n"]
[1267.216, "o", "\r\n"]
[1267.226, "o", "    Attributes\r\n"]
[1267.236, "o", "    ----------\r\n"]
[1267.246, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[1267.256, "o", "        Components extracted from the data.\r\n"]
[1267.266, "o", "\r\n"]
[1267.276, "o", "    n_features_in_ : int\r\n"]
[1267.286, "o", "        Number of features seen during :term:`fit`.\r\n"]
[1267.296, "o", "\r\n"]
[1267.306, "o", "        .. versionadded:: 0.24\r\n"]
[1267.316, "o", "\r\n"]
[1267.326, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[1267.336, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[1267.346, "o", "        has feature names that are all strings.\r\n"]
[1267.356, "o", "\r\n"]
[1267.366, "o", "        .. versionadded:: 1.0\r\n"]
[1267.376, "o", "\r\n"]
[1267.386, "o", "    n_iter_ : int\r\n"]
[1267.396, "o", "        Number of iterations over the full dataset.\r\n"]
[1267.406, "o", "\r\n"]
[1267.416, "o", "    n_steps_ : int\r\n"]
[1267.426, "o", "        Number of mini-batches processed.\r\n"]
[1267.436, "o", "\r\n"]
[1267.446, "o", "        .. versionadded:: 1.1\r\n"]
[1267.456, "o", "\r\n"]
[1267.466, "o", "    See Also\r\n"]
[1267.476, "o", "    --------\r\n"]
[1267.486, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1267.496, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1267.506, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[1267.516, "o", "        precomputed dictionary.\r\n"]
[1267.526, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1267.536, "o", "\r\n"]
[1267.546, "o", "    References\r\n"]
[1267.556, "o", "    ----------\r\n"]
[1267.566, "o", "\r\n"]
[1267.576, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[1267.586, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[1267.596, "o", "\r\n"]
[1267.606, "o", "    Examples\r\n"]
[1267.616, "o", "    --------\r\n"]
[1267.626, "o", "    >>> import numpy as np\r\n"]
[1267.636, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[1267.646, "o", "    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n"]
[1267.656, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[1267.666, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[1267.676, "o", "    ...     random_state=42)\r\n"]
[1267.686, "o", "    >>> dict_learner = MiniBatchDictionaryLearning(\r\n"]
[1267.696, "o", "    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n"]
[1267.706, "o", "    ...     transform_alpha=0.1, random_state=42)\r\n"]
[1267.716, "o", "    >>> X_transformed = dict_learner.fit_transform(X)\r\n"]
[1267.726, "o", "\r\n"]
[1267.736, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[1267.746, "o", "\r\n"]
[1267.756, "o", "    >>> np.mean(X_transformed == 0) < 0.5\r\n"]
[1267.766, "o", "    True\r\n"]
[1267.776, "o", "\r\n"]
[1267.786, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[1267.796, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[1267.806, "o", "    the original signal:\r\n"]
[1267.816, "o", "\r\n"]
[1267.826, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[1267.836, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[1267.846, "o", "    0.057...\r\n"]
[1267.856, "o", "    \"\"\"\r\n"]
[1267.866, "o", "\r\n"]
[1267.876, "o", "    _parameter_constraints: dict = {\r\n"]
[1267.886, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1267.896, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1267.906, "o", "        \"n_iter\": [\r\n"]
[1267.916, "o", "            Interval(Integral, 0, None, closed=\"left\"),\r\n"]
[1267.926, "o", "            Hidden(StrOptions({\"deprecated\"})),\r\n"]
[1267.936, "o", "        ],\r\n"]
[1267.946, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n"]
[1267.956, "o", "        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n"]
[1267.966, "o", "        \"n_jobs\": [None, Integral],\r\n"]
[1267.976, "o", "        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n"]
[1267.986, "o", "        \"shuffle\": [\"boolean\"],\r\n"]
[1267.996, "o", "        \"dict_init\": [None, np.ndarray],\r\n"]
[1268.006, "o", "        \"transform_algorithm\": [\r\n"]
[1268.016, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[1268.026, "o", "        ],\r\n"]
[1268.036, "o", "        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1268.046, "o", "        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[1268.056, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[1268.066, "o", "        \"split_sign\": [\"boolean\"],\r\n"]
[1268.076, "o", "        \"random_state\": [\"random_state\"],\r\n"]
[1268.086, "o", "        \"positive_code\": [\"boolean\"],\r\n"]
[1268.096, "o", "        \"positive_dict\": [\"boolean\"],\r\n"]
[1268.106, "o", "        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1268.116, "o", "        \"callback\": [None, callable],\r\n"]
[1268.126, "o", "        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1268.136, "o", "        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n"]
[1268.146, "o", "    }\r\n"]
[1268.156, "o", "\r\n"]
[1268.166, "o", "    def __init__(\r\n"]
[1268.176, "o", "        self,\r\n"]
[1268.186, "o", "        n_components=None,\r\n"]
[1268.196, "o", "        *,\r\n"]
[1268.206, "o", "        alpha=1,\r\n"]
[1268.216, "o", "        n_iter=\"deprecated\",\r\n"]
[1268.226, "o", "        max_iter=None,\r\n"]
[1268.236, "o", "        fit_algorithm=\"lars\",\r\n"]
[1268.246, "o", "        n_jobs=None,\r\n"]
[1268.256, "o", "        batch_size=256,\r\n"]
[1268.266, "o", "        shuffle=True,\r\n"]
[1268.276, "o", "        dict_init=None,\r\n"]
[1268.286, "o", "        transform_algorithm=\"omp\",\r\n"]
[1268.296, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[1268.306, "o", "        transform_alpha=None,\r\n"]
[1268.316, "o", "        verbose=False,\r\n"]
[1268.326, "o", "        split_sign=False,\r\n"]
[1268.336, "o", "        random_state=None,\r\n"]
[1268.346, "o", "        positive_code=False,\r\n"]
[1268.356, "o", "        positive_dict=False,\r\n"]
[1268.366, "o", "        transform_max_iter=1000,\r\n"]
[1268.376, "o", "        callback=None,\r\n"]
[1268.386, "o", "        tol=1e-3,\r\n"]
[1268.396, "o", "        max_no_improvement=10,\r\n"]
[1268.406, "o", "    ):\r\n"]
[1268.416, "o", "        super().__init__(\r\n"]
[1268.426, "o", "            transform_algorithm,\r\n"]
[1268.436, "o", "            transform_n_nonzero_coefs,\r\n"]
[1268.446, "o", "            transform_alpha,\r\n"]
[1268.456, "o", "            split_sign,\r\n"]
[1268.466, "o", "            n_jobs,\r\n"]
[1268.476, "o", "            positive_code,\r\n"]
[1268.486, "o", "            transform_max_iter,\r\n"]
[1268.496, "o", "        )\r\n"]
[1268.506, "o", "        self.n_components = n_components\r\n"]
[1268.516, "o", "        self.alpha = alpha\r\n"]
[1268.526, "o", "        self.n_iter = n_iter\r\n"]
[1268.536, "o", "        self.max_iter = max_iter\r\n"]
[1268.546, "o", "        self.fit_algorithm = fit_algorithm\r\n"]
[1268.556, "o", "        self.dict_init = dict_init\r\n"]
[1268.566, "o", "        self.verbose = verbose\r\n"]
[1268.576, "o", "        self.shuffle = shuffle\r\n"]
[1268.586, "o", "        self.batch_size = batch_size\r\n"]
[1268.596, "o", "        self.split_sign = split_sign\r\n"]
[1268.606, "o", "        self.random_state = random_state\r\n"]
[1268.616, "o", "        self.positive_dict = positive_dict\r\n"]
[1268.626, "o", "        self.callback = callback\r\n"]
[1268.636, "o", "        self.max_no_improvement = max_no_improvement\r\n"]
[1268.646, "o", "        self.tol = tol\r\n"]
[1268.656, "o", "\r\n"]
[1268.666, "o", "    def _check_params(self, X):\r\n"]
[1268.676, "o", "        # n_components\r\n"]
[1268.686, "o", "        self._n_components = self.n_components\r\n"]
[1268.696, "o", "        if self._n_components is None:\r\n"]
[1268.706, "o", "            self._n_components = X.shape[1]\r\n"]
[1268.716, "o", "\r\n"]
[1268.726, "o", "        # fit_algorithm\r\n"]
[1268.736, "o", "        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n"]
[1268.746, "o", "        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n"]
[1268.756, "o", "\r\n"]
[1268.766, "o", "        # batch_size\r\n"]
[1268.776, "o", "        self._batch_size = min(self.batch_size, X.shape[0])\r\n"]
[1268.786, "o", "\r\n"]
[1268.796, "o", "    def _initialize_dict(self, X, random_state):\r\n"]
[1268.806, "o", "        \"\"\"Initialization of the dictionary.\"\"\"\r\n"]
[1268.816, "o", "        if self.dict_init is not None:\r\n"]
[1268.826, "o", "            dictionary = self.dict_init\r\n"]
[1268.836, "o", "        else:\r\n"]
[1268.846, "o", "            # Init V with SVD of X\r\n"]
[1268.856, "o", "            _, S, dictionary = randomized_svd(\r\n"]
[1268.866, "o", "                X, self._n_components, random_state=random_state\r\n"]
[1268.876, "o", "            )\r\n"]
[1268.886, "o", "            dictionary = S[:, np.newaxis] * dictionary\r\n"]
[1268.896, "o", "\r\n"]
[1268.906, "o", "        if self._n_components <= len(dictionary):\r\n"]
[1268.916, "o", "            dictionary = dictionary[: self._n_components, :]\r\n"]
[1268.926, "o", "        else:\r\n"]
[1268.936, "o", "            dictionary = np.concatenate(\r\n"]
[1268.946, "o", "                (\r\n"]
[1268.956, "o", "                    dictionary,\r\n"]
[1268.966, "o", "                    np.zeros(\r\n"]
[1268.976, "o", "                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n"]
[1268.986, "o", "                        dtype=dictionary.dtype,\r\n"]
[1268.996, "o", "                    ),\r\n"]
[1269.006, "o", "                )\r\n"]
[1269.016, "o", "            )\r\n"]
[1269.026, "o", "\r\n"]
[1269.036, "o", "        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n"]
[1269.046, "o", "        dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[1269.056, "o", "\r\n"]
[1269.066, "o", "        return dictionary\r\n"]
[1269.076, "o", "\r\n"]
[1269.086, "o", "    def _update_inner_stats(self, X, code, batch_size, step):\r\n"]
[1269.096, "o", "        \"\"\"Update the inner stats inplace.\"\"\"\r\n"]
[1269.106, "o", "        if step < batch_size - 1:\r\n"]
[1269.116, "o", "            theta = (step + 1) * batch_size\r\n"]
[1269.126, "o", "        else:\r\n"]
[1269.136, "o", "            theta = batch_size**2 + step + 1 - batch_size\r\n"]
[1269.146, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[1269.156, "o", "\r\n"]
[1269.166, "o", "        self._A *= beta\r\n"]
[1269.176, "o", "        self._A += code.T @ code / batch_size\r\n"]
[1269.186, "o", "        self._B *= beta\r\n"]
[1269.196, "o", "        self._B += X.T @ code / batch_size\r\n"]
[1269.206, "o", "\r\n"]
[1269.216, "o", "    def _minibatch_step(self, X, dictionary, random_state, step):\r\n"]
[1269.226, "o", "        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n"]
[1269.236, "o", "        batch_size = X.shape[0]\r\n"]
[1269.246, "o", "\r\n"]
[1269.256, "o", "        # Compute code for this batch\r\n"]
[1269.266, "o", "        code = _sparse_encode(\r\n"]
[1269.276, "o", "            X,\r\n"]
[1269.286, "o", "            dictionary,\r\n"]
[1269.296, "o", "            algorithm=self._fit_algorithm,\r\n"]
[1269.306, "o", "            alpha=self.alpha,\r\n"]
[1269.316, "o", "            n_jobs=self.n_jobs,\r\n"]
[1269.326, "o", "            positive=self.positive_code,\r\n"]
[1269.336, "o", "            max_iter=self.transform_max_iter,\r\n"]
[1269.346, "o", "            verbose=self.verbose,\r\n"]
[1269.356, "o", "        )\r\n"]
[1269.366, "o", "\r\n"]
[1269.376, "o", "        batch_cost = (\r\n"]
[1269.386, "o", "            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n"]
[1269.396, "o", "            + self.alpha * np.sum(np.abs(code))\r\n"]
[1269.406, "o", "        ) / batch_size\r\n"]
[1269.416, "o", "\r\n"]
[1269.426, "o", "        # Update inner stats\r\n"]
[1269.436, "o", "        self._update_inner_stats(X, code, batch_size, step)\r\n"]
[1269.446, "o", "\r\n"]
[1269.456, "o", "        # Update dictionary\r\n"]
[1269.466, "o", "        _update_dict(\r\n"]
[1269.476, "o", "            dictionary,\r\n"]
[1269.486, "o", "            X,\r\n"]
[1269.496, "o", "            code,\r\n"]
[1269.506, "o", "            self._A,\r\n"]
[1269.516, "o", "            self._B,\r\n"]
[1269.526, "o", "            verbose=self.verbose,\r\n"]
[1269.536, "o", "            random_state=random_state,\r\n"]
[1269.546, "o", "            positive=self.positive_dict,\r\n"]
[1269.556, "o", "        )\r\n"]
[1269.566, "o", "\r\n"]
[1269.576, "o", "        return batch_cost\r\n"]
[1269.586, "o", "\r\n"]
[1269.596, "o", "    def _check_convergence(\r\n"]
[1269.606, "o", "        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n"]
[1269.616, "o", "    ):\r\n"]
[1269.626, "o", "        \"\"\"Helper function to encapsulate the early stopping logic.\r\n"]
[1269.636, "o", "\r\n"]
[1269.646, "o", "        Early stopping is based on two factors:\r\n"]
[1269.656, "o", "        - A small change of the dictionary between two minibatch updates. This is\r\n"]
[1269.666, "o", "          controlled by the tol parameter.\r\n"]
[1269.676, "o", "        - No more improvement on a smoothed estimate of the objective function for a\r\n"]
[1269.686, "o", "          a certain number of consecutive minibatch updates. This is controlled by\r\n"]
[1269.696, "o", "          the max_no_improvement parameter.\r\n"]
[1269.706, "o", "        \"\"\"\r\n"]
[1269.716, "o", "        batch_size = X.shape[0]\r\n"]
[1269.726, "o", "\r\n"]
[1269.736, "o", "        # counts steps starting from 1 for user friendly verbose mode.\r\n"]
[1269.746, "o", "        step = step + 1\r\n"]
[1269.756, "o", "\r\n"]
[1269.766, "o", "        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n"]
[1269.776, "o", "        # too bad value\r\n"]
[1269.786, "o", "        if step <= min(100, n_samples / batch_size):\r\n"]
[1269.796, "o", "            if self.verbose:\r\n"]
[1269.806, "o", "                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n"]
[1269.816, "o", "            return False\r\n"]
[1269.826, "o", "\r\n"]
[1269.836, "o", "        # Compute an Exponentially Weighted Average of the cost function to\r\n"]
[1269.846, "o", "        # monitor the convergence while discarding minibatch-local stochastic\r\n"]
[1269.856, "o", "        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n"]
[1269.866, "o", "        if self._ewa_cost is None:\r\n"]
[1269.876, "o", "            self._ewa_cost = batch_cost\r\n"]
[1269.886, "o", "        else:\r\n"]
[1269.896, "o", "            alpha = batch_size / (n_samples + 1)\r\n"]
[1269.906, "o", "            alpha = min(alpha, 1)\r\n"]
[1269.916, "o", "            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n"]
[1269.926, "o", "\r\n"]
[1269.936, "o", "        if self.verbose:\r\n"]
[1270.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1270.002, "i", "python - << 'PY'\r"]
[1270.004, "o", "python - << 'PY'\r\n"]
[1270.123714, "o", "\u001b[?2004l\r\n"]
[1270.241429, "o", "\u001b[?2004h> import re, sys\r\n"]
[1270.359143, "o", "\u001b[?2004l\r\n"]
[1270.476857, "o", "\u001b[?2004h> p='sklearn/decomposition/_dict_learning.py'\r\n"]
[1270.594571, "o", "\u001b[?2004l\r\n"]
[1270.712286, "o", "\u001b[?2004h> s=open(p,'r',encoding='utf-8').read()\r\n"]
[1270.83, "o", "\u001b[?2004l\r\n"]
[1270.947714, "o", "\u001b[?2004h> # Add copy_cov=False in calls to _sparse_encode_precomputed if not already present\r\n"]
[1271.065429, "o", "\u001b[?2004l\r\n"]
[1271.183143, "o", "\u001b[?2004h> pat=r\"_sparse_encode_precomputed\\((?P<args>[^\\)]*)\\)\"\r\n"]
[1271.300857, "o", "\u001b[?2004l\r\n"]
[1271.418571, "o", "\u001b[?2004h> \r\n"]
[1271.536286, "o", "\u001b[?2004l\r\n"]
[1271.654, "o", "\u001b[?2004h> def repl(m):\r\n"]
[1271.771714, "o", "\u001b[?2004l\r\n"]
[1271.889429, "o", "\u001b[?2004h>     args=m.group('args')\r\n"]
[1272.007143, "o", "\u001b[?2004l\r\n"]
[1272.124857, "o", "\u001b[?2004h>     if 'copy_cov' in args:\r\n"]
[1272.242571, "o", "\u001b[?2004l\r\n"]
[1272.360286, "o", "\u001b[?2004h>         return m.group(0)\r\n"]
[1272.478, "o", "\u001b[?2004l\r\n"]
[1272.595714, "o", "\u001b[?2004h>     # insert copy_cov=False at end before closing\r\n"]
[1272.713429, "o", "\u001b[?2004l\r\n"]
[1272.831143, "o", "\u001b[?2004h>     new=f\"_sparse_encode_precomputed({args}, copy_cov=False)\"\r\n"]
[1272.948857, "o", "\u001b[?2004l\r\n"]
[1273.066571, "o", "\u001b[?2004h>     return new\r\n"]
[1273.184286, "o", "\u001b[?2004l\r\n"]
[1273.302, "o", "\u001b[?2004h> ns=re.sub(pat,repl,s)\r\n"]
[1273.419714, "o", "\u001b[?2004l\r\n"]
[1273.537429, "o", "\u001b[?2004h> if ns!=s:\r\n"]
[1273.655143, "o", "\u001b[?2004l\r\n"]
[1273.772857, "o", "\u001b[?2004h>     open(p,'w',encoding='utf-8').write(ns)\r\n"]
[1273.890571, "o", "\u001b[?2004l\r\n"]
[1274.008286, "o", "\u001b[?2004h>     print('Patched _sparse_encode_precomputed call(s) with copy_cov=False')\r\n"]
[1274.126, "o", "\u001b[?2004l\r\n"]
[1274.243714, "o", "\u001b[?2004h> else:\r\n"]
[1274.361429, "o", "\u001b[?2004l\r\n"]
[1274.479143, "o", "\u001b[?2004h>     print('No patch applied (already present or no call found)')\r\n"]
[1274.596857, "o", "\u001b[?2004l\r\n"]
[1274.714571, "o", "\u001b[?2004h> PY\r\n"]
[1274.832286, "o", "\u001b[?2004l\r\n"]
[1275.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1275.002, "i", "git status -sb\r"]
[1275.004, "o", "git status -sb\r\n"]
[1277.478, "o", "\u001b[?2004l\r\n"]
[1280.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1280.002, "i", "git diff -- sklearn/decomposition/_dict_learning.py | sed -n '1,200p'\r"]
[1280.004, "o", "git diff -- sklearn/decomposition/_dict_learning.py | sed -n '1,200p'\r\n"]
[1280.006, "o", "\u001b[?2004l\r\n"]
[1285.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1285.002, "i", "pytest -q sklearn/decomposition/tests/test_dict_learning.py -k minibatch -q\r"]
[1285.004, "o", "pytest -q sklearn/decomposition/tests/test_dict_learning.py -k minibatch -q\r\n"]
[1287.478, "o", "\u001b[?2004l\r\n"]
[1290.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1290.002, "i", "cd asv_benchmarks\r"]
[1290.004, "o", "cd asv_benchmarks\r\n"]
[1290.006, "o", "\u001b[?2004l\r\n"]
[1295.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1295.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r"]
[1295.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r\n"]
[1295.315, "o", "stderr\r\n"]
[1295.624, "o", "\u001b[?2004l\r\n"]
[1295.933, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[1296.242, "o", "\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n"]
[1296.551, "o", "[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n"]
[1296.86, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n"]
[1297.169, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0m                                                                                     ok\r\n"]
[1297.478, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ============\u001b[0m\r\n"]
[1297.787, "o", "              --                 n_jobs   \r\n"]
[1298.096, "o", "              --------------- ------------\r\n"]
[1298.405, "o", "               fit_algorithm       1      \r\n"]
[1298.714, "o", "              =============== ============\r\n"]
[1299.023, "o", "                    lars       6.83\u00b10.02s \r\n"]
[1299.332, "o", "                     cd        1.64\u00b10.03s \r\n"]
[1299.641, "o", "              =============== ============\r\n"]
[1300.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1300.002, "i", "cd /workspace/repo\r"]
[1300.004, "o", "cd /workspace/repo\r\n"]
[1300.006, "o", "\u001b[?2004l\r\n"]
[1305.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1305.002, "i", "grep -n \"_minibatch_step\" sklearn/decomposition/_dict_learning.py || grep -n \"minibatch\" sklearn/decomposition/_dic\r"]
[1305.004, "o", "grep -n \"_minibatch_step\" sklearn/decomposition/_dict_learning.py || grep -n \"minibatch\" sklearn/decomposition/_dic\r\n"]
[1305.83, "o", "ct_learning.py\r\n"]
[1306.654, "o", "\u001b[?2004l\r\n"]
[1307.478, "o", "\u001b[32m\u001b[K2219\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    def \u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(self, X, dictionary, random_state, step):\r\n"]
[1308.302, "o", "\u001b[32m\u001b[K2399\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                batch_cost = self.\u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(\r\n"]
[1309.126, "o", "\u001b[32m\u001b[K2425\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                self.\u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(X_train[batch], dictionary, self._random_state, i)\r\n"]
[1310.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1310.002, "i", "grep -n \"def sparse_encode\\|_sparse_encode_precomputed\\(\" sklearn/decomposition/_dict_learning.py\r"]
[1310.004, "o", "grep -n \"def sparse_encode\\|_sparse_encode_precomputed\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1312.478, "o", "\u001b[?2004l\r\n"]
[1315.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1315.002, "i", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r"]
[1315.004, "o", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r\n"]
[1315.016, "o", "\u001b[?2004l\r\n"]
[1315.026, "o", "\"\"\" Dictionary learning.\r\n"]
[1315.036, "o", "\"\"\"\r\n"]
[1315.046, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[1315.056, "o", "# License: BSD 3 clause\r\n"]
[1315.066, "o", "\r\n"]
[1315.076, "o", "import itertools\r\n"]
[1315.086, "o", "import sys\r\n"]
[1315.096, "o", "import time\r\n"]
[1315.106, "o", "import warnings\r\n"]
[1315.116, "o", "from math import ceil\r\n"]
[1315.126, "o", "from numbers import Integral, Real\r\n"]
[1315.136, "o", "\r\n"]
[1315.146, "o", "import numpy as np\r\n"]
[1315.156, "o", "from joblib import effective_n_jobs\r\n"]
[1315.166, "o", "from scipy import linalg\r\n"]
[1315.176, "o", "\r\n"]
[1315.186, "o", "from ..base import (\r\n"]
[1315.196, "o", "    BaseEstimator,\r\n"]
[1315.206, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[1315.216, "o", "    TransformerMixin,\r\n"]
[1315.226, "o", "    _fit_context,\r\n"]
[1315.236, "o", ")\r\n"]
[1315.246, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[1315.256, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[1315.266, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[1315.276, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[1315.286, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[1315.296, "o", "from ..utils.validation import check_is_fitted\r\n"]
[1315.306, "o", "\r\n"]
[1315.316, "o", "\r\n"]
[1315.326, "o", "def _check_positive_coding(method, positive):\r\n"]
[1315.336, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[1315.346, "o", "        raise ValueError(\r\n"]
[1315.356, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[1315.366, "o", "        )\r\n"]
[1315.376, "o", "\r\n"]
[1315.386, "o", "\r\n"]
[1315.396, "o", "def _sparse_encode_precomputed(\r\n"]
[1315.406, "o", "    X,\r\n"]
[1315.416, "o", "    dictionary,\r\n"]
[1315.426, "o", "    *,\r\n"]
[1315.436, "o", "    gram=None,\r\n"]
[1315.446, "o", "    cov=None,\r\n"]
[1315.456, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1315.466, "o", "    regularization=None,\r\n"]
[1315.476, "o", "    copy_cov=True,\r\n"]
[1315.486, "o", "    init=None,\r\n"]
[1315.496, "o", "    max_iter=1000,\r\n"]
[1315.506, "o", "    verbose=0,\r\n"]
[1315.516, "o", "    positive=False,\r\n"]
[1315.526, "o", "):\r\n"]
[1315.536, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[1315.546, "o", "\r\n"]
[1315.556, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[1315.566, "o", "\r\n"]
[1315.576, "o", "    Parameters\r\n"]
[1315.586, "o", "    ----------\r\n"]
[1315.596, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[1315.606, "o", "        Data matrix.\r\n"]
[1315.616, "o", "\r\n"]
[1315.626, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1315.636, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1315.646, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[1315.656, "o", "\r\n"]
[1315.666, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[1315.676, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[1315.686, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[1315.696, "o", "\r\n"]
[1315.706, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[1315.716, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[1315.726, "o", "\r\n"]
[1315.736, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1315.746, "o", "            default='lasso_lars'\r\n"]
[1315.756, "o", "        The algorithm used:\r\n"]
[1315.766, "o", "\r\n"]
[1315.776, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[1315.786, "o", "          (`linear_model.lars_path`);\r\n"]
[1315.796, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1315.806, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1315.816, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1315.826, "o", "          the estimated components are sparse;\r\n"]
[1315.836, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1315.846, "o", "          solution;\r\n"]
[1315.856, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1315.866, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[1315.876, "o", "\r\n"]
[1315.886, "o", "    regularization : int or float, default=None\r\n"]
[1315.896, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[1315.906, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[1315.916, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[1315.926, "o", "\r\n"]
[1315.936, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1315.946, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[1315.956, "o", "        `algorithm='lasso_cd'`.\r\n"]
[1315.966, "o", "\r\n"]
[1315.976, "o", "    max_iter : int, default=1000\r\n"]
[1315.986, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1315.996, "o", "        `'lasso_lars'`.\r\n"]
[1316.006, "o", "\r\n"]
[1316.016, "o", "    copy_cov : bool, default=True\r\n"]
[1316.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1316.036, "o", "        be overwritten.\r\n"]
[1316.046, "o", "\r\n"]
[1316.056, "o", "    verbose : int, default=0\r\n"]
[1316.066, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[1316.076, "o", "\r\n"]
[1316.086, "o", "    positive: bool, default=False\r\n"]
[1316.096, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[1316.106, "o", "\r\n"]
[1316.116, "o", "        .. versionadded:: 0.20\r\n"]
[1316.126, "o", "\r\n"]
[1316.136, "o", "    Returns\r\n"]
[1316.146, "o", "    -------\r\n"]
[1316.156, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[1316.166, "o", "        The sparse codes.\r\n"]
[1316.176, "o", "    \"\"\"\r\n"]
[1316.186, "o", "    n_samples, n_features = X.shape\r\n"]
[1316.196, "o", "    n_components = dictionary.shape[0]\r\n"]
[1316.206, "o", "\r\n"]
[1316.216, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[1316.226, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1316.236, "o", "        try:\r\n"]
[1316.246, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1316.256, "o", "\r\n"]
[1316.266, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1316.276, "o", "            # corrects the verbosity level.\r\n"]
[1316.286, "o", "            lasso_lars = LassoLars(\r\n"]
[1316.296, "o", "                alpha=alpha,\r\n"]
[1316.306, "o", "                fit_intercept=False,\r\n"]
[1316.316, "o", "                verbose=verbose,\r\n"]
[1316.326, "o", "                precompute=gram,\r\n"]
[1316.336, "o", "                fit_path=False,\r\n"]
[1316.346, "o", "                positive=positive,\r\n"]
[1316.356, "o", "                max_iter=max_iter,\r\n"]
[1316.366, "o", "            )\r\n"]
[1316.376, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1316.386, "o", "            new_code = lasso_lars.coef_\r\n"]
[1316.396, "o", "        finally:\r\n"]
[1316.406, "o", "            np.seterr(**err_mgt)\r\n"]
[1316.416, "o", "\r\n"]
[1316.426, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[1316.436, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1316.446, "o", "\r\n"]
[1316.456, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[1316.466, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[1316.476, "o", "        # argument that we could pass in from Lasso.\r\n"]
[1316.486, "o", "        clf = Lasso(\r\n"]
[1316.496, "o", "            alpha=alpha,\r\n"]
[1316.506, "o", "            fit_intercept=False,\r\n"]
[1316.516, "o", "            precompute=gram,\r\n"]
[1316.526, "o", "            max_iter=max_iter,\r\n"]
[1316.536, "o", "            warm_start=True,\r\n"]
[1316.546, "o", "            positive=positive,\r\n"]
[1316.556, "o", "        )\r\n"]
[1316.566, "o", "\r\n"]
[1316.576, "o", "        if init is not None:\r\n"]
[1316.586, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[1316.596, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[1316.606, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[1316.616, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[1316.626, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[1316.636, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[1316.646, "o", "                init = np.array(init)\r\n"]
[1316.656, "o", "            clf.coef_ = init\r\n"]
[1316.666, "o", "\r\n"]
[1316.676, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[1316.686, "o", "        new_code = clf.coef_\r\n"]
[1316.696, "o", "\r\n"]
[1316.706, "o", "    elif algorithm == \"lars\":\r\n"]
[1316.716, "o", "        try:\r\n"]
[1316.726, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1316.736, "o", "\r\n"]
[1316.746, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1316.756, "o", "            # corrects the verbosity level.\r\n"]
[1316.766, "o", "            lars = Lars(\r\n"]
[1316.776, "o", "                fit_intercept=False,\r\n"]
[1316.786, "o", "                verbose=verbose,\r\n"]
[1316.796, "o", "                precompute=gram,\r\n"]
[1316.806, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[1316.816, "o", "                fit_path=False,\r\n"]
[1316.826, "o", "            )\r\n"]
[1316.836, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1316.846, "o", "            new_code = lars.coef_\r\n"]
[1316.856, "o", "        finally:\r\n"]
[1316.866, "o", "            np.seterr(**err_mgt)\r\n"]
[1316.876, "o", "\r\n"]
[1316.886, "o", "    elif algorithm == \"threshold\":\r\n"]
[1316.896, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[1316.906, "o", "        if positive:\r\n"]
[1316.916, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[1316.926, "o", "\r\n"]
[1316.936, "o", "    elif algorithm == \"omp\":\r\n"]
[1316.946, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[1316.956, "o", "            Gram=gram,\r\n"]
[1316.966, "o", "            Xy=cov,\r\n"]
[1316.976, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[1316.986, "o", "            tol=None,\r\n"]
[1316.996, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[1317.006, "o", "            copy_Xy=copy_cov,\r\n"]
[1317.016, "o", "        ).T\r\n"]
[1317.026, "o", "\r\n"]
[1317.036, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[1317.046, "o", "\r\n"]
[1317.056, "o", "\r\n"]
[1317.066, "o", "@validate_params(\r\n"]
[1317.076, "o", "    {\r\n"]
[1317.086, "o", "        \"X\": [\"array-like\"],\r\n"]
[1317.096, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[1317.106, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[1317.116, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[1317.126, "o", "        \"algorithm\": [\r\n"]
[1317.136, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[1317.146, "o", "        ],\r\n"]
[1317.156, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1317.166, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[1317.176, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[1317.186, "o", "        \"init\": [\"array-like\", None],\r\n"]
[1317.196, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1317.206, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[1317.216, "o", "        \"check_input\": [\"boolean\"],\r\n"]
[1317.226, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[1317.236, "o", "        \"positive\": [\"boolean\"],\r\n"]
[1317.246, "o", "    },\r\n"]
[1317.256, "o", "    prefer_skip_nested_validation=True,\r\n"]
[1317.266, "o", ")\r\n"]
[1317.276, "o", "# XXX : could be moved to the linear_model module\r\n"]
[1317.286, "o", "def sparse_encode(\r\n"]
[1317.296, "o", "    X,\r\n"]
[1317.306, "o", "    dictionary,\r\n"]
[1317.316, "o", "    *,\r\n"]
[1317.326, "o", "    gram=None,\r\n"]
[1317.336, "o", "    cov=None,\r\n"]
[1317.346, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1317.356, "o", "    n_nonzero_coefs=None,\r\n"]
[1317.366, "o", "    alpha=None,\r\n"]
[1317.376, "o", "    copy_cov=True,\r\n"]
[1317.386, "o", "    init=None,\r\n"]
[1317.396, "o", "    max_iter=1000,\r\n"]
[1317.406, "o", "    n_jobs=None,\r\n"]
[1317.416, "o", "    check_input=True,\r\n"]
[1317.426, "o", "    verbose=0,\r\n"]
[1317.436, "o", "    positive=False,\r\n"]
[1317.446, "o", "):\r\n"]
[1317.456, "o", "    \"\"\"Sparse coding.\r\n"]
[1317.466, "o", "\r\n"]
[1317.476, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[1317.486, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[1317.496, "o", "\r\n"]
[1317.506, "o", "        X ~= code * dictionary\r\n"]
[1317.516, "o", "\r\n"]
[1317.526, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[1317.536, "o", "\r\n"]
[1317.546, "o", "    Parameters\r\n"]
[1317.556, "o", "    ----------\r\n"]
[1317.566, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[1317.576, "o", "        Data matrix.\r\n"]
[1317.586, "o", "\r\n"]
[1317.596, "o", "    dictionary : array-like of shape (n_components, n_features)\r\n"]
[1317.606, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1317.616, "o", "        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[1317.626, "o", "        output.\r\n"]
[1317.636, "o", "\r\n"]
[1317.646, "o", "    gram : array-like of shape (n_components, n_components), default=None\r\n"]
[1317.656, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n"]
[1317.666, "o", "\r\n"]
[1317.676, "o", "    cov : array-like of shape (n_components, n_samples), default=None\r\n"]
[1317.686, "o", "        Precomputed covariance, `dictionary' * X`.\r\n"]
[1317.696, "o", "\r\n"]
[1317.706, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1317.716, "o", "            default='lasso_lars'\r\n"]
[1317.726, "o", "        The algorithm used:\r\n"]
[1317.736, "o", "\r\n"]
[1317.746, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[1317.756, "o", "          (`linear_model.lars_path`);\r\n"]
[1317.766, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1317.776, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1317.786, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1317.796, "o", "          the estimated components are sparse;\r\n"]
[1317.806, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1317.816, "o", "          solution;\r\n"]
[1317.826, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1317.836, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[1317.846, "o", "\r\n"]
[1317.856, "o", "    n_nonzero_coefs : int, default=None\r\n"]
[1317.866, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1317.876, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[1317.886, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[1317.896, "o", "        `n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1317.906, "o", "\r\n"]
[1317.916, "o", "    alpha : float, default=None\r\n"]
[1317.926, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1317.936, "o", "        penalty applied to the L1 norm.\r\n"]
[1317.946, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1317.956, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1317.966, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[1317.976, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[1317.986, "o", "        `n_nonzero_coefs`.\r\n"]
[1317.996, "o", "        If `None`, default to 1.\r\n"]
[1318.006, "o", "\r\n"]
[1318.016, "o", "    copy_cov : bool, default=True\r\n"]
[1318.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1318.036, "o", "        be overwritten.\r\n"]
[1318.046, "o", "\r\n"]
[1318.056, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1318.066, "o", "        Initialization value of the sparse codes. Only used if\r\n"]
[1318.076, "o", "        `algorithm='lasso_cd'`.\r\n"]
[1318.086, "o", "\r\n"]
[1318.096, "o", "    max_iter : int, default=1000\r\n"]
[1318.106, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1318.116, "o", "        `'lasso_lars'`.\r\n"]
[1318.126, "o", "\r\n"]
[1318.136, "o", "    n_jobs : int, default=None\r\n"]
[1318.146, "o", "        Number of parallel jobs to run.\r\n"]
[1318.156, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1318.166, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1318.176, "o", "        for more details.\r\n"]
[1318.186, "o", "\r\n"]
[1318.196, "o", "    check_input : bool, default=True\r\n"]
[1318.206, "o", "        If `False`, the input arrays X and dictionary will not be checked.\r\n"]
[1318.216, "o", "\r\n"]
[1318.226, "o", "    verbose : int, default=0\r\n"]
[1318.236, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[1318.246, "o", "\r\n"]
[1318.256, "o", "    positive : bool, default=False\r\n"]
[1318.266, "o", "        Whether to enforce positivity when finding the encoding.\r\n"]
[1318.276, "o", "\r\n"]
[1318.286, "o", "        .. versionadded:: 0.20\r\n"]
[1318.296, "o", "\r\n"]
[1318.306, "o", "    Returns\r\n"]
[1318.316, "o", "    -------\r\n"]
[1318.326, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1318.336, "o", "        The sparse codes.\r\n"]
[1318.346, "o", "\r\n"]
[1318.356, "o", "    See Also\r\n"]
[1318.366, "o", "    --------\r\n"]
[1318.376, "o", "    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n"]
[1318.386, "o", "        path using LARS algorithm.\r\n"]
[1318.396, "o", "    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n"]
[1318.406, "o", "    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n"]
[1318.416, "o", "    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n"]
[1318.426, "o", "        dictionary.\r\n"]
[1318.436, "o", "    \"\"\"\r\n"]
[1318.446, "o", "    if check_input:\r\n"]
[1318.456, "o", "        if algorithm == \"lasso_cd\":\r\n"]
[1318.466, "o", "            dictionary = check_array(\r\n"]
[1318.476, "o", "                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n"]
[1318.486, "o", "            )\r\n"]
[1318.496, "o", "            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n"]
[1318.506, "o", "        else:\r\n"]
[1318.516, "o", "            dictionary = check_array(dictionary)\r\n"]
[1318.526, "o", "            X = check_array(X)\r\n"]
[1318.536, "o", "\r\n"]
[1318.546, "o", "    if dictionary.shape[1] != X.shape[1]:\r\n"]
[1318.556, "o", "        raise ValueError(\r\n"]
[1318.566, "o", "            \"Dictionary and X have different numbers of features:\"\r\n"]
[1318.576, "o", "            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n"]
[1318.586, "o", "        )\r\n"]
[1318.596, "o", "\r\n"]
[1318.606, "o", "    _check_positive_coding(algorithm, positive)\r\n"]
[1318.616, "o", "\r\n"]
[1318.626, "o", "    return _sparse_encode(\r\n"]
[1318.636, "o", "        X,\r\n"]
[1318.646, "o", "        dictionary,\r\n"]
[1318.656, "o", "        gram=gram,\r\n"]
[1318.666, "o", "        cov=cov,\r\n"]
[1318.676, "o", "        algorithm=algorithm,\r\n"]
[1318.686, "o", "        n_nonzero_coefs=n_nonzero_coefs,\r\n"]
[1318.696, "o", "        alpha=alpha,\r\n"]
[1318.706, "o", "        copy_cov=copy_cov,\r\n"]
[1318.716, "o", "        init=init,\r\n"]
[1318.726, "o", "        max_iter=max_iter,\r\n"]
[1318.736, "o", "        n_jobs=n_jobs,\r\n"]
[1318.746, "o", "        verbose=verbose,\r\n"]
[1318.756, "o", "        positive=positive,\r\n"]
[1318.766, "o", "    )\r\n"]
[1318.776, "o", "\r\n"]
[1318.786, "o", "\r\n"]
[1318.796, "o", "def _sparse_encode(\r\n"]
[1318.806, "o", "    X,\r\n"]
[1318.816, "o", "    dictionary,\r\n"]
[1318.826, "o", "    *,\r\n"]
[1318.836, "o", "    gram=None,\r\n"]
[1318.846, "o", "    cov=None,\r\n"]
[1318.856, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1318.866, "o", "    n_nonzero_coefs=None,\r\n"]
[1318.876, "o", "    alpha=None,\r\n"]
[1318.886, "o", "    copy_cov=True,\r\n"]
[1318.896, "o", "    init=None,\r\n"]
[1318.906, "o", "    max_iter=1000,\r\n"]
[1318.916, "o", "    n_jobs=None,\r\n"]
[1318.926, "o", "    verbose=0,\r\n"]
[1318.936, "o", "    positive=False,\r\n"]
[1318.946, "o", "):\r\n"]
[1318.956, "o", "    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[1318.966, "o", "\r\n"]
[1318.976, "o", "    n_samples, n_features = X.shape\r\n"]
[1318.986, "o", "    n_components = dictionary.shape[0]\r\n"]
[1318.996, "o", "\r\n"]
[1319.006, "o", "    if algorithm in (\"lars\", \"omp\"):\r\n"]
[1319.016, "o", "        regularization = n_nonzero_coefs\r\n"]
[1319.026, "o", "        if regularization is None:\r\n"]
[1319.036, "o", "            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[1319.046, "o", "    else:\r\n"]
[1319.056, "o", "        regularization = alpha\r\n"]
[1319.066, "o", "        if regularization is None:\r\n"]
[1319.076, "o", "            regularization = 1.0\r\n"]
[1319.086, "o", "\r\n"]
[1319.096, "o", "    if gram is None and algorithm != \"threshold\":\r\n"]
[1319.106, "o", "        gram = np.dot(dictionary, dictionary.T)\r\n"]
[1319.116, "o", "\r\n"]
[1319.126, "o", "    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[1319.136, "o", "        copy_cov = False\r\n"]
[1319.146, "o", "        cov = np.dot(dictionary, X.T)\r\n"]
[1319.156, "o", "\r\n"]
[1319.166, "o", "    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[1319.176, "o", "        code = _sparse_encode_precomputed(\r\n"]
[1319.186, "o", "            X,\r\n"]
[1319.196, "o", "            dictionary,\r\n"]
[1319.206, "o", "            gram=gram,\r\n"]
[1319.216, "o", "            cov=cov,\r\n"]
[1319.226, "o", "            algorithm=algorithm,\r\n"]
[1319.236, "o", "            regularization=regularization,\r\n"]
[1319.246, "o", "            copy_cov=copy_cov,\r\n"]
[1319.256, "o", "            init=init,\r\n"]
[1319.266, "o", "            max_iter=max_iter,\r\n"]
[1319.276, "o", "            verbose=verbose,\r\n"]
[1319.286, "o", "            positive=positive,\r\n"]
[1319.296, "o", "        )\r\n"]
[1319.306, "o", "        return code\r\n"]
[1319.316, "o", "\r\n"]
[1319.326, "o", "    # Enter parallel code block\r\n"]
[1319.336, "o", "    n_samples = X.shape[0]\r\n"]
[1319.346, "o", "    n_components = dictionary.shape[0]\r\n"]
[1319.356, "o", "    code = np.empty((n_samples, n_components))\r\n"]
[1319.366, "o", "    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[1319.376, "o", "\r\n"]
[1319.386, "o", "    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[1319.396, "o", "        delayed(_sparse_encode_precomputed)(\r\n"]
[1319.406, "o", "            X[this_slice],\r\n"]
[1319.416, "o", "            dictionary,\r\n"]
[1319.426, "o", "            gram=gram,\r\n"]
[1319.436, "o", "            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[1319.446, "o", "            algorithm=algorithm,\r\n"]
[1319.456, "o", "            regularization=regularization,\r\n"]
[1319.466, "o", "            copy_cov=copy_cov,\r\n"]
[1319.476, "o", "            init=init[this_slice] if init is not None else None,\r\n"]
[1319.486, "o", "            max_iter=max_iter,\r\n"]
[1319.496, "o", "            verbose=verbose,\r\n"]
[1319.506, "o", "            positive=positive,\r\n"]
[1319.516, "o", "        )\r\n"]
[1319.526, "o", "        for this_slice in slices\r\n"]
[1319.536, "o", "    )\r\n"]
[1319.546, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[1319.556, "o", "        code[this_slice] = this_view\r\n"]
[1319.566, "o", "    return code\r\n"]
[1319.576, "o", "\r\n"]
[1319.586, "o", "\r\n"]
[1319.596, "o", "def _update_dict(\r\n"]
[1319.606, "o", "    dictionary,\r\n"]
[1319.616, "o", "    Y,\r\n"]
[1319.626, "o", "    code,\r\n"]
[1319.636, "o", "    A=None,\r\n"]
[1319.646, "o", "    B=None,\r\n"]
[1319.656, "o", "    verbose=False,\r\n"]
[1319.666, "o", "    random_state=None,\r\n"]
[1319.676, "o", "    positive=False,\r\n"]
[1319.686, "o", "):\r\n"]
[1319.696, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[1319.706, "o", "\r\n"]
[1319.716, "o", "    Parameters\r\n"]
[1319.726, "o", "    ----------\r\n"]
[1319.736, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1319.746, "o", "        Value of the dictionary at the previous iteration.\r\n"]
[1319.756, "o", "\r\n"]
[1319.766, "o", "    Y : ndarray of shape (n_samples, n_features)\r\n"]
[1319.776, "o", "        Data matrix.\r\n"]
[1319.786, "o", "\r\n"]
[1319.796, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1319.806, "o", "        Sparse coding of the data against which to optimize the dictionary.\r\n"]
[1319.816, "o", "\r\n"]
[1319.826, "o", "    A : ndarray of shape (n_components, n_components), default=None\r\n"]
[1319.836, "o", "        Together with `B`, sufficient stats of the online model to update the\r\n"]
[1319.846, "o", "        dictionary.\r\n"]
[1319.856, "o", "\r\n"]
[1319.866, "o", "    B : ndarray of shape (n_features, n_components), default=None\r\n"]
[1319.876, "o", "        Together with `A`, sufficient stats of the online model to update the\r\n"]
[1319.886, "o", "        dictionary.\r\n"]
[1319.896, "o", "\r\n"]
[1319.906, "o", "    verbose: bool, default=False\r\n"]
[1319.916, "o", "        Degree of output the procedure will print.\r\n"]
[1319.926, "o", "\r\n"]
[1319.936, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1320.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1320.002, "i", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r"]
[1320.004, "o", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r\n"]
[1320.016, "o", "\u001b[?2004l\r\n"]
[1320.026, "o", "           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[1320.036, "o", "\r\n"]
[1320.046, "o", "    return_n_iter : bool, default=False\r\n"]
[1320.056, "o", "        Whether or not to return the number of iterations.\r\n"]
[1320.066, "o", "\r\n"]
[1320.076, "o", "        .. deprecated:: 1.1\r\n"]
[1320.086, "o", "           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n"]
[1320.096, "o", "\r\n"]
[1320.106, "o", "    positive_dict : bool, default=False\r\n"]
[1320.116, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1320.126, "o", "\r\n"]
[1320.136, "o", "        .. versionadded:: 0.20\r\n"]
[1320.146, "o", "\r\n"]
[1320.156, "o", "    positive_code : bool, default=False\r\n"]
[1320.166, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1320.176, "o", "\r\n"]
[1320.186, "o", "        .. versionadded:: 0.20\r\n"]
[1320.196, "o", "\r\n"]
[1320.206, "o", "    method_max_iter : int, default=1000\r\n"]
[1320.216, "o", "        Maximum number of iterations to perform when solving the lasso problem.\r\n"]
[1320.226, "o", "\r\n"]
[1320.236, "o", "        .. versionadded:: 0.22\r\n"]
[1320.246, "o", "\r\n"]
[1320.256, "o", "    tol : float, default=1e-3\r\n"]
[1320.266, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[1320.276, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[1320.286, "o", "\r\n"]
[1320.296, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[1320.306, "o", "        `tol` to 0.0.\r\n"]
[1320.316, "o", "\r\n"]
[1320.326, "o", "        .. versionadded:: 1.1\r\n"]
[1320.336, "o", "\r\n"]
[1320.346, "o", "    max_no_improvement : int, default=10\r\n"]
[1320.356, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[1320.366, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[1320.376, "o", "        `max_iter` is not None.\r\n"]
[1320.386, "o", "\r\n"]
[1320.396, "o", "        To disable convergence detection based on cost function, set\r\n"]
[1320.406, "o", "        `max_no_improvement` to None.\r\n"]
[1320.416, "o", "\r\n"]
[1320.426, "o", "        .. versionadded:: 1.1\r\n"]
[1320.436, "o", "\r\n"]
[1320.446, "o", "    Returns\r\n"]
[1320.456, "o", "    -------\r\n"]
[1320.466, "o", "    code : ndarray of shape (n_samples, n_components),\r\n"]
[1320.476, "o", "        The sparse code (only returned if `return_code=True`).\r\n"]
[1320.486, "o", "\r\n"]
[1320.496, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[1320.506, "o", "        The solutions to the dictionary learning problem.\r\n"]
[1320.516, "o", "\r\n"]
[1320.526, "o", "    n_iter : int\r\n"]
[1320.536, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[1320.546, "o", "        set to `True`.\r\n"]
[1320.556, "o", "\r\n"]
[1320.566, "o", "    See Also\r\n"]
[1320.576, "o", "    --------\r\n"]
[1320.586, "o", "    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n"]
[1320.596, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1320.606, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n"]
[1320.616, "o", "        learning algorithm.\r\n"]
[1320.626, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1320.636, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1320.646, "o", "    \"\"\"\r\n"]
[1320.656, "o", "    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n"]
[1320.666, "o", "    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n"]
[1320.676, "o", "        raise ValueError(\r\n"]
[1320.686, "o", "            \"The following arguments are incompatible with 'max_iter': \"\r\n"]
[1320.696, "o", "            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n"]
[1320.706, "o", "        )\r\n"]
[1320.716, "o", "\r\n"]
[1320.726, "o", "    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n"]
[1320.736, "o", "    return_inner_stats = _check_warn_deprecated(\r\n"]
[1320.746, "o", "        return_inner_stats,\r\n"]
[1320.756, "o", "        \"return_inner_stats\",\r\n"]
[1320.766, "o", "        default=False,\r\n"]
[1320.776, "o", "        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n"]
[1320.786, "o", "    )\r\n"]
[1320.796, "o", "    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n"]
[1320.806, "o", "    return_n_iter = _check_warn_deprecated(\r\n"]
[1320.816, "o", "        return_n_iter,\r\n"]
[1320.826, "o", "        \"return_n_iter\",\r\n"]
[1320.836, "o", "        default=False,\r\n"]
[1320.846, "o", "        additional_message=(\r\n"]
[1320.856, "o", "            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n"]
[1320.866, "o", "            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n"]
[1320.876, "o", "        ),\r\n"]
[1320.886, "o", "    )\r\n"]
[1320.896, "o", "\r\n"]
[1320.906, "o", "    if max_iter is not None:\r\n"]
[1320.916, "o", "        transform_algorithm = \"lasso_\" + method\r\n"]
[1320.926, "o", "\r\n"]
[1320.936, "o", "        est = MiniBatchDictionaryLearning(\r\n"]
[1320.946, "o", "            n_components=n_components,\r\n"]
[1320.956, "o", "            alpha=alpha,\r\n"]
[1320.966, "o", "            n_iter=n_iter,\r\n"]
[1320.976, "o", "            n_jobs=n_jobs,\r\n"]
[1320.986, "o", "            fit_algorithm=method,\r\n"]
[1320.996, "o", "            batch_size=batch_size,\r\n"]
[1321.006, "o", "            shuffle=shuffle,\r\n"]
[1321.016, "o", "            dict_init=dict_init,\r\n"]
[1321.026, "o", "            random_state=random_state,\r\n"]
[1321.036, "o", "            transform_algorithm=transform_algorithm,\r\n"]
[1321.046, "o", "            transform_alpha=alpha,\r\n"]
[1321.056, "o", "            positive_code=positive_code,\r\n"]
[1321.066, "o", "            positive_dict=positive_dict,\r\n"]
[1321.076, "o", "            transform_max_iter=method_max_iter,\r\n"]
[1321.086, "o", "            verbose=verbose,\r\n"]
[1321.096, "o", "            callback=callback,\r\n"]
[1321.106, "o", "            tol=tol,\r\n"]
[1321.116, "o", "            max_no_improvement=max_no_improvement,\r\n"]
[1321.126, "o", "        ).fit(X)\r\n"]
[1321.136, "o", "\r\n"]
[1321.146, "o", "        if not return_code:\r\n"]
[1321.156, "o", "            return est.components_\r\n"]
[1321.166, "o", "        else:\r\n"]
[1321.176, "o", "            code = est.transform(X)\r\n"]
[1321.186, "o", "            return code, est.components_\r\n"]
[1321.196, "o", "\r\n"]
[1321.206, "o", "    # TODO(1.4) remove the whole old behavior\r\n"]
[1321.216, "o", "    # Fallback to old behavior\r\n"]
[1321.226, "o", "\r\n"]
[1321.236, "o", "    n_iter = _check_warn_deprecated(\r\n"]
[1321.246, "o", "        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n"]
[1321.256, "o", "    )\r\n"]
[1321.266, "o", "\r\n"]
[1321.276, "o", "    if n_components is None:\r\n"]
[1321.286, "o", "        n_components = X.shape[1]\r\n"]
[1321.296, "o", "\r\n"]
[1321.306, "o", "    if method not in (\"lars\", \"cd\"):\r\n"]
[1321.316, "o", "        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n"]
[1321.326, "o", "\r\n"]
[1321.336, "o", "    _check_positive_coding(method, positive_code)\r\n"]
[1321.346, "o", "\r\n"]
[1321.356, "o", "    method = \"lasso_\" + method\r\n"]
[1321.366, "o", "\r\n"]
[1321.376, "o", "    t0 = time.time()\r\n"]
[1321.386, "o", "    n_samples, n_features = X.shape\r\n"]
[1321.396, "o", "    # Avoid integer division problems\r\n"]
[1321.406, "o", "    alpha = float(alpha)\r\n"]
[1321.416, "o", "    random_state = check_random_state(random_state)\r\n"]
[1321.426, "o", "\r\n"]
[1321.436, "o", "    # Init V with SVD of X\r\n"]
[1321.446, "o", "    if dict_init is not None:\r\n"]
[1321.456, "o", "        dictionary = dict_init\r\n"]
[1321.466, "o", "    else:\r\n"]
[1321.476, "o", "        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n"]
[1321.486, "o", "        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[1321.496, "o", "    r = len(dictionary)\r\n"]
[1321.506, "o", "    if n_components <= r:\r\n"]
[1321.516, "o", "        dictionary = dictionary[:n_components, :]\r\n"]
[1321.526, "o", "    else:\r\n"]
[1321.536, "o", "        dictionary = np.r_[\r\n"]
[1321.546, "o", "            dictionary,\r\n"]
[1321.556, "o", "            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n"]
[1321.566, "o", "        ]\r\n"]
[1321.576, "o", "\r\n"]
[1321.586, "o", "    if verbose == 1:\r\n"]
[1321.596, "o", "        print(\"[dict_learning]\", end=\" \")\r\n"]
[1321.606, "o", "\r\n"]
[1321.616, "o", "    if shuffle:\r\n"]
[1321.626, "o", "        X_train = X.copy()\r\n"]
[1321.636, "o", "        random_state.shuffle(X_train)\r\n"]
[1321.646, "o", "    else:\r\n"]
[1321.656, "o", "        X_train = X\r\n"]
[1321.666, "o", "\r\n"]
[1321.676, "o", "    X_train = check_array(\r\n"]
[1321.686, "o", "        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n"]
[1321.696, "o", "    )\r\n"]
[1321.706, "o", "\r\n"]
[1321.716, "o", "    # Fortran-order dict better suited for the sparse coding which is the\r\n"]
[1321.726, "o", "    # bottleneck of this algorithm.\r\n"]
[1321.736, "o", "    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n"]
[1321.746, "o", "    dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[1321.756, "o", "\r\n"]
[1321.766, "o", "    batches = gen_batches(n_samples, batch_size)\r\n"]
[1321.776, "o", "    batches = itertools.cycle(batches)\r\n"]
[1321.786, "o", "\r\n"]
[1321.796, "o", "    # The covariance of the dictionary\r\n"]
[1321.806, "o", "    if inner_stats is None:\r\n"]
[1321.816, "o", "        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n"]
[1321.826, "o", "        # The data approximation\r\n"]
[1321.836, "o", "        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n"]
[1321.846, "o", "    else:\r\n"]
[1321.856, "o", "        A = inner_stats[0].copy()\r\n"]
[1321.866, "o", "        B = inner_stats[1].copy()\r\n"]
[1321.876, "o", "\r\n"]
[1321.886, "o", "    # If n_iter is zero, we need to return zero.\r\n"]
[1321.896, "o", "    ii = iter_offset - 1\r\n"]
[1321.906, "o", "\r\n"]
[1321.916, "o", "    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n"]
[1321.926, "o", "        this_X = X_train[batch]\r\n"]
[1321.936, "o", "        dt = time.time() - t0\r\n"]
[1321.946, "o", "        if verbose == 1:\r\n"]
[1321.956, "o", "            sys.stdout.write(\".\")\r\n"]
[1321.966, "o", "            sys.stdout.flush()\r\n"]
[1321.976, "o", "        elif verbose:\r\n"]
[1321.986, "o", "            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n"]
[1321.996, "o", "                print(\r\n"]
[1322.006, "o", "                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n"]
[1322.016, "o", "                )\r\n"]
[1322.026, "o", "\r\n"]
[1322.036, "o", "        this_code = sparse_encode(\r\n"]
[1322.046, "o", "            this_X,\r\n"]
[1322.056, "o", "            dictionary,\r\n"]
[1322.066, "o", "            algorithm=method,\r\n"]
[1322.076, "o", "            alpha=alpha,\r\n"]
[1322.086, "o", "            n_jobs=n_jobs,\r\n"]
[1322.096, "o", "            check_input=False,\r\n"]
[1322.106, "o", "            positive=positive_code,\r\n"]
[1322.116, "o", "            max_iter=method_max_iter,\r\n"]
[1322.126, "o", "            verbose=verbose,\r\n"]
[1322.136, "o", "        )\r\n"]
[1322.146, "o", "\r\n"]
[1322.156, "o", "        # Update the auxiliary variables\r\n"]
[1322.166, "o", "        if ii < batch_size - 1:\r\n"]
[1322.176, "o", "            theta = float((ii + 1) * batch_size)\r\n"]
[1322.186, "o", "        else:\r\n"]
[1322.196, "o", "            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n"]
[1322.206, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[1322.216, "o", "\r\n"]
[1322.226, "o", "        A *= beta\r\n"]
[1322.236, "o", "        A += np.dot(this_code.T, this_code)\r\n"]
[1322.246, "o", "        B *= beta\r\n"]
[1322.256, "o", "        B += np.dot(this_X.T, this_code)\r\n"]
[1322.266, "o", "\r\n"]
[1322.276, "o", "        # Update dictionary in place\r\n"]
[1322.286, "o", "        _update_dict(\r\n"]
[1322.296, "o", "            dictionary,\r\n"]
[1322.306, "o", "            this_X,\r\n"]
[1322.316, "o", "            this_code,\r\n"]
[1322.326, "o", "            A,\r\n"]
[1322.336, "o", "            B,\r\n"]
[1322.346, "o", "            verbose=verbose,\r\n"]
[1322.356, "o", "            random_state=random_state,\r\n"]
[1322.366, "o", "            positive=positive_dict,\r\n"]
[1322.376, "o", "        )\r\n"]
[1322.386, "o", "\r\n"]
[1322.396, "o", "        # Maybe we need a stopping criteria based on the amount of\r\n"]
[1322.406, "o", "        # modification in the dictionary\r\n"]
[1322.416, "o", "        if callback is not None:\r\n"]
[1322.426, "o", "            callback(locals())\r\n"]
[1322.436, "o", "\r\n"]
[1322.446, "o", "    if return_inner_stats:\r\n"]
[1322.456, "o", "        if return_n_iter:\r\n"]
[1322.466, "o", "            return dictionary, (A, B), ii - iter_offset + 1\r\n"]
[1322.476, "o", "        else:\r\n"]
[1322.486, "o", "            return dictionary, (A, B)\r\n"]
[1322.496, "o", "    if return_code:\r\n"]
[1322.506, "o", "        if verbose > 1:\r\n"]
[1322.516, "o", "            print(\"Learning code...\", end=\" \")\r\n"]
[1322.526, "o", "        elif verbose == 1:\r\n"]
[1322.536, "o", "            print(\"|\", end=\" \")\r\n"]
[1322.546, "o", "        code = sparse_encode(\r\n"]
[1322.556, "o", "            X,\r\n"]
[1322.566, "o", "            dictionary,\r\n"]
[1322.576, "o", "            algorithm=method,\r\n"]
[1322.586, "o", "            alpha=alpha,\r\n"]
[1322.596, "o", "            n_jobs=n_jobs,\r\n"]
[1322.606, "o", "            check_input=False,\r\n"]
[1322.616, "o", "            positive=positive_code,\r\n"]
[1322.626, "o", "            max_iter=method_max_iter,\r\n"]
[1322.636, "o", "            verbose=verbose,\r\n"]
[1322.646, "o", "        )\r\n"]
[1322.656, "o", "        if verbose > 1:\r\n"]
[1322.666, "o", "            dt = time.time() - t0\r\n"]
[1322.676, "o", "            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n"]
[1322.686, "o", "        if return_n_iter:\r\n"]
[1322.696, "o", "            return code, dictionary, ii - iter_offset + 1\r\n"]
[1322.706, "o", "        else:\r\n"]
[1322.716, "o", "            return code, dictionary\r\n"]
[1322.726, "o", "\r\n"]
[1322.736, "o", "    if return_n_iter:\r\n"]
[1322.746, "o", "        return dictionary, ii - iter_offset + 1\r\n"]
[1322.756, "o", "    else:\r\n"]
[1322.766, "o", "        return dictionary\r\n"]
[1322.776, "o", "\r\n"]
[1322.786, "o", "\r\n"]
[1322.796, "o", "@validate_params(\r\n"]
[1322.806, "o", "    {\r\n"]
[1322.816, "o", "        \"X\": [\"array-like\"],\r\n"]
[1322.826, "o", "        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[1322.836, "o", "        \"return_n_iter\": [\"boolean\"],\r\n"]
[1322.846, "o", "        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1322.856, "o", "    },\r\n"]
[1322.866, "o", "    prefer_skip_nested_validation=False,\r\n"]
[1322.876, "o", ")\r\n"]
[1322.886, "o", "def dict_learning(\r\n"]
[1322.896, "o", "    X,\r\n"]
[1322.906, "o", "    n_components,\r\n"]
[1322.916, "o", "    *,\r\n"]
[1322.926, "o", "    alpha,\r\n"]
[1322.936, "o", "    max_iter=100,\r\n"]
[1322.946, "o", "    tol=1e-8,\r\n"]
[1322.956, "o", "    method=\"lars\",\r\n"]
[1322.966, "o", "    n_jobs=None,\r\n"]
[1322.976, "o", "    dict_init=None,\r\n"]
[1322.986, "o", "    code_init=None,\r\n"]
[1322.996, "o", "    callback=None,\r\n"]
[1323.006, "o", "    verbose=False,\r\n"]
[1323.016, "o", "    random_state=None,\r\n"]
[1323.026, "o", "    return_n_iter=False,\r\n"]
[1323.036, "o", "    positive_dict=False,\r\n"]
[1323.046, "o", "    positive_code=False,\r\n"]
[1323.056, "o", "    method_max_iter=1000,\r\n"]
[1323.066, "o", "):\r\n"]
[1323.076, "o", "    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n"]
[1323.086, "o", "\r\n"]
[1323.096, "o", "    Finds the best dictionary and the corresponding sparse code for\r\n"]
[1323.106, "o", "    approximating the data matrix X by solving::\r\n"]
[1323.116, "o", "\r\n"]
[1323.126, "o", "        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[1323.136, "o", "                     (U,V)\r\n"]
[1323.146, "o", "                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n"]
[1323.156, "o", "\r\n"]
[1323.166, "o", "    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n"]
[1323.176, "o", "    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n"]
[1323.186, "o", "    which is the sum of the absolute values of all the entries in the matrix.\r\n"]
[1323.196, "o", "\r\n"]
[1323.206, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[1323.216, "o", "\r\n"]
[1323.226, "o", "    Parameters\r\n"]
[1323.236, "o", "    ----------\r\n"]
[1323.246, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[1323.256, "o", "        Data matrix.\r\n"]
[1323.266, "o", "\r\n"]
[1323.276, "o", "    n_components : int\r\n"]
[1323.286, "o", "        Number of dictionary atoms to extract.\r\n"]
[1323.296, "o", "\r\n"]
[1323.306, "o", "    alpha : int or float\r\n"]
[1323.316, "o", "        Sparsity controlling parameter.\r\n"]
[1323.326, "o", "\r\n"]
[1323.336, "o", "    max_iter : int, default=100\r\n"]
[1323.346, "o", "        Maximum number of iterations to perform.\r\n"]
[1323.356, "o", "\r\n"]
[1323.366, "o", "    tol : float, default=1e-8\r\n"]
[1323.376, "o", "        Tolerance for the stopping condition.\r\n"]
[1323.386, "o", "\r\n"]
[1323.396, "o", "    method : {'lars', 'cd'}, default='lars'\r\n"]
[1323.406, "o", "        The method used:\r\n"]
[1323.416, "o", "\r\n"]
[1323.426, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[1323.436, "o", "           problem (`linear_model.lars_path`);\r\n"]
[1323.446, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[1323.456, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[1323.466, "o", "          the estimated components are sparse.\r\n"]
[1323.476, "o", "\r\n"]
[1323.486, "o", "    n_jobs : int, default=None\r\n"]
[1323.496, "o", "        Number of parallel jobs to run.\r\n"]
[1323.506, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1323.516, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1323.526, "o", "        for more details.\r\n"]
[1323.536, "o", "\r\n"]
[1323.546, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1323.556, "o", "        Initial value for the dictionary for warm restart scenarios. Only used\r\n"]
[1323.566, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[1323.576, "o", "\r\n"]
[1323.586, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1323.596, "o", "        Initial value for the sparse code for warm restart scenarios. Only used\r\n"]
[1323.606, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[1323.616, "o", "\r\n"]
[1323.626, "o", "    callback : callable, default=None\r\n"]
[1323.636, "o", "        Callable that gets invoked every five iterations.\r\n"]
[1323.646, "o", "\r\n"]
[1323.656, "o", "    verbose : bool, default=False\r\n"]
[1323.666, "o", "        To control the verbosity of the procedure.\r\n"]
[1323.676, "o", "\r\n"]
[1323.686, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1323.696, "o", "        Used for randomly initializing the dictionary. Pass an int for\r\n"]
[1323.706, "o", "        reproducible results across multiple function calls.\r\n"]
[1323.716, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1323.726, "o", "\r\n"]
[1323.736, "o", "    return_n_iter : bool, default=False\r\n"]
[1323.746, "o", "        Whether or not to return the number of iterations.\r\n"]
[1323.756, "o", "\r\n"]
[1323.766, "o", "    positive_dict : bool, default=False\r\n"]
[1323.776, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1323.786, "o", "\r\n"]
[1323.796, "o", "        .. versionadded:: 0.20\r\n"]
[1323.806, "o", "\r\n"]
[1323.816, "o", "    positive_code : bool, default=False\r\n"]
[1323.826, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1323.836, "o", "\r\n"]
[1323.846, "o", "        .. versionadded:: 0.20\r\n"]
[1323.856, "o", "\r\n"]
[1323.866, "o", "    method_max_iter : int, default=1000\r\n"]
[1323.876, "o", "        Maximum number of iterations to perform.\r\n"]
[1323.886, "o", "\r\n"]
[1323.896, "o", "        .. versionadded:: 0.22\r\n"]
[1323.906, "o", "\r\n"]
[1323.916, "o", "    Returns\r\n"]
[1323.926, "o", "    -------\r\n"]
[1323.936, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1323.946, "o", "        The sparse code factor in the matrix factorization.\r\n"]
[1323.956, "o", "\r\n"]
[1323.966, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[1323.976, "o", "        The dictionary factor in the matrix factorization.\r\n"]
[1323.986, "o", "\r\n"]
[1323.996, "o", "    errors : array\r\n"]
[1324.006, "o", "        Vector of errors at each iteration.\r\n"]
[1324.016, "o", "\r\n"]
[1324.026, "o", "    n_iter : int\r\n"]
[1324.036, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[1324.046, "o", "        set to True.\r\n"]
[1324.056, "o", "\r\n"]
[1324.066, "o", "    See Also\r\n"]
[1324.076, "o", "    --------\r\n"]
[1324.086, "o", "    dict_learning_online : Solve a dictionary learning matrix factorization\r\n"]
[1324.096, "o", "        problem online.\r\n"]
[1324.106, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1324.116, "o", "    MiniBatchDictionaryLearning : A faster, less accurate version\r\n"]
[1324.126, "o", "        of the dictionary learning algorithm.\r\n"]
[1324.136, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1324.146, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1324.156, "o", "    \"\"\"\r\n"]
[1324.166, "o", "    estimator = DictionaryLearning(\r\n"]
[1324.176, "o", "        n_components=n_components,\r\n"]
[1324.186, "o", "        alpha=alpha,\r\n"]
[1324.196, "o", "        max_iter=max_iter,\r\n"]
[1324.206, "o", "        tol=tol,\r\n"]
[1324.216, "o", "        fit_algorithm=method,\r\n"]
[1324.226, "o", "        n_jobs=n_jobs,\r\n"]
[1324.236, "o", "        dict_init=dict_init,\r\n"]
[1324.246, "o", "        callback=callback,\r\n"]
[1324.256, "o", "        code_init=code_init,\r\n"]
[1324.266, "o", "        verbose=verbose,\r\n"]
[1324.276, "o", "        random_state=random_state,\r\n"]
[1324.286, "o", "        positive_code=positive_code,\r\n"]
[1324.296, "o", "        positive_dict=positive_dict,\r\n"]
[1324.306, "o", "        transform_max_iter=method_max_iter,\r\n"]
[1324.316, "o", "    )\r\n"]
[1324.326, "o", "    code = estimator.fit_transform(X)\r\n"]
[1324.336, "o", "    if return_n_iter:\r\n"]
[1324.346, "o", "        return (\r\n"]
[1324.356, "o", "            code,\r\n"]
[1324.366, "o", "            estimator.components_,\r\n"]
[1324.376, "o", "            estimator.error_,\r\n"]
[1324.386, "o", "            estimator.n_iter_,\r\n"]
[1324.396, "o", "        )\r\n"]
[1324.406, "o", "    return code, estimator.components_, estimator.error_\r\n"]
[1324.416, "o", "\r\n"]
[1324.426, "o", "\r\n"]
[1324.436, "o", "class _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n"]
[1324.446, "o", "    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n"]
[1324.456, "o", "\r\n"]
[1324.466, "o", "    def __init__(\r\n"]
[1324.476, "o", "        self,\r\n"]
[1324.486, "o", "        transform_algorithm,\r\n"]
[1324.496, "o", "        transform_n_nonzero_coefs,\r\n"]
[1324.506, "o", "        transform_alpha,\r\n"]
[1324.516, "o", "        split_sign,\r\n"]
[1324.526, "o", "        n_jobs,\r\n"]
[1324.536, "o", "        positive_code,\r\n"]
[1324.546, "o", "        transform_max_iter,\r\n"]
[1324.556, "o", "    ):\r\n"]
[1324.566, "o", "        self.transform_algorithm = transform_algorithm\r\n"]
[1324.576, "o", "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n"]
[1324.586, "o", "        self.transform_alpha = transform_alpha\r\n"]
[1324.596, "o", "        self.transform_max_iter = transform_max_iter\r\n"]
[1324.606, "o", "        self.split_sign = split_sign\r\n"]
[1324.616, "o", "        self.n_jobs = n_jobs\r\n"]
[1324.626, "o", "        self.positive_code = positive_code\r\n"]
[1324.636, "o", "\r\n"]
[1324.646, "o", "    def _transform(self, X, dictionary):\r\n"]
[1324.656, "o", "        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n"]
[1324.666, "o", "        SparseCoder.\"\"\"\r\n"]
[1324.676, "o", "        X = self._validate_data(X, reset=False)\r\n"]
[1324.686, "o", "\r\n"]
[1324.696, "o", "        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n"]
[1324.706, "o", "            transform_alpha = self.alpha\r\n"]
[1324.716, "o", "        else:\r\n"]
[1324.726, "o", "            transform_alpha = self.transform_alpha\r\n"]
[1324.736, "o", "\r\n"]
[1324.746, "o", "        code = sparse_encode(\r\n"]
[1324.756, "o", "            X,\r\n"]
[1324.766, "o", "            dictionary,\r\n"]
[1324.776, "o", "            algorithm=self.transform_algorithm,\r\n"]
[1324.786, "o", "            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n"]
[1324.796, "o", "            alpha=transform_alpha,\r\n"]
[1324.806, "o", "            max_iter=self.transform_max_iter,\r\n"]
[1324.816, "o", "            n_jobs=self.n_jobs,\r\n"]
[1324.826, "o", "            positive=self.positive_code,\r\n"]
[1324.836, "o", "        )\r\n"]
[1324.846, "o", "\r\n"]
[1324.856, "o", "        if self.split_sign:\r\n"]
[1324.866, "o", "            # feature vector is split into a positive and negative side\r\n"]
[1324.876, "o", "            n_samples, n_features = code.shape\r\n"]
[1324.886, "o", "            split_code = np.empty((n_samples, 2 * n_features))\r\n"]
[1324.896, "o", "            split_code[:, :n_features] = np.maximum(code, 0)\r\n"]
[1324.906, "o", "            split_code[:, n_features:] = -np.minimum(code, 0)\r\n"]
[1324.916, "o", "            code = split_code\r\n"]
[1324.926, "o", "\r\n"]
[1324.936, "o", "        return code\r\n"]
[1325.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1325.002, "i", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py\r"]
[1325.004, "o", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py\r\n"]
[1325.016, "o", "\u001b[?2004l\r\n"]
[1325.026, "o", "\r\n"]
[1325.036, "o", "    n_jobs : int or None, default=None\r\n"]
[1325.046, "o", "        Number of parallel jobs to run.\r\n"]
[1325.056, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1325.066, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1325.076, "o", "        for more details.\r\n"]
[1325.086, "o", "\r\n"]
[1325.096, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1325.106, "o", "        Initial value for the code, for warm restart. Only used if `code_init`\r\n"]
[1325.116, "o", "        and `dict_init` are not None.\r\n"]
[1325.126, "o", "\r\n"]
[1325.136, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1325.146, "o", "        Initial values for the dictionary, for warm restart. Only used if\r\n"]
[1325.156, "o", "        `code_init` and `dict_init` are not None.\r\n"]
[1325.166, "o", "\r\n"]
[1325.176, "o", "    callback : callable, default=None\r\n"]
[1325.186, "o", "        Callable that gets invoked every five iterations.\r\n"]
[1325.196, "o", "\r\n"]
[1325.206, "o", "        .. versionadded:: 1.3\r\n"]
[1325.216, "o", "\r\n"]
[1325.226, "o", "    verbose : bool, default=False\r\n"]
[1325.236, "o", "        To control the verbosity of the procedure.\r\n"]
[1325.246, "o", "\r\n"]
[1325.256, "o", "    split_sign : bool, default=False\r\n"]
[1325.266, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[1325.276, "o", "        its negative part and its positive part. This can improve the\r\n"]
[1325.286, "o", "        performance of downstream classifiers.\r\n"]
[1325.296, "o", "\r\n"]
[1325.306, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1325.316, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[1325.326, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[1325.336, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[1325.346, "o", "        results across multiple function calls.\r\n"]
[1325.356, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1325.366, "o", "\r\n"]
[1325.376, "o", "    positive_code : bool, default=False\r\n"]
[1325.386, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1325.396, "o", "\r\n"]
[1325.406, "o", "        .. versionadded:: 0.20\r\n"]
[1325.416, "o", "\r\n"]
[1325.426, "o", "    positive_dict : bool, default=False\r\n"]
[1325.436, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1325.446, "o", "\r\n"]
[1325.456, "o", "        .. versionadded:: 0.20\r\n"]
[1325.466, "o", "\r\n"]
[1325.476, "o", "    transform_max_iter : int, default=1000\r\n"]
[1325.486, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1325.496, "o", "        `'lasso_lars'`.\r\n"]
[1325.506, "o", "\r\n"]
[1325.516, "o", "        .. versionadded:: 0.22\r\n"]
[1325.526, "o", "\r\n"]
[1325.536, "o", "    Attributes\r\n"]
[1325.546, "o", "    ----------\r\n"]
[1325.556, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[1325.566, "o", "        dictionary atoms extracted from the data\r\n"]
[1325.576, "o", "\r\n"]
[1325.586, "o", "    error_ : array\r\n"]
[1325.596, "o", "        vector of errors at each iteration\r\n"]
[1325.606, "o", "\r\n"]
[1325.616, "o", "    n_features_in_ : int\r\n"]
[1325.626, "o", "        Number of features seen during :term:`fit`.\r\n"]
[1325.636, "o", "\r\n"]
[1325.646, "o", "        .. versionadded:: 0.24\r\n"]
[1325.656, "o", "\r\n"]
[1325.666, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[1325.676, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[1325.686, "o", "        has feature names that are all strings.\r\n"]
[1325.696, "o", "\r\n"]
[1325.706, "o", "        .. versionadded:: 1.0\r\n"]
[1325.716, "o", "\r\n"]
[1325.726, "o", "    n_iter_ : int\r\n"]
[1325.736, "o", "        Number of iterations run.\r\n"]
[1325.746, "o", "\r\n"]
[1325.756, "o", "    See Also\r\n"]
[1325.766, "o", "    --------\r\n"]
[1325.776, "o", "    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n"]
[1325.786, "o", "        dictionary learning algorithm.\r\n"]
[1325.796, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1325.806, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[1325.816, "o", "        precomputed dictionary.\r\n"]
[1325.826, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1325.836, "o", "\r\n"]
[1325.846, "o", "    References\r\n"]
[1325.856, "o", "    ----------\r\n"]
[1325.866, "o", "\r\n"]
[1325.876, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[1325.886, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[1325.896, "o", "\r\n"]
[1325.906, "o", "    Examples\r\n"]
[1325.916, "o", "    --------\r\n"]
[1325.926, "o", "    >>> import numpy as np\r\n"]
[1325.936, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[1325.946, "o", "    >>> from sklearn.decomposition import DictionaryLearning\r\n"]
[1325.956, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[1325.966, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[1325.976, "o", "    ...     random_state=42,\r\n"]
[1325.986, "o", "    ... )\r\n"]
[1325.996, "o", "    >>> dict_learner = DictionaryLearning(\r\n"]
[1326.006, "o", "    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n"]
[1326.016, "o", "    ...     random_state=42,\r\n"]
[1326.026, "o", "    ... )\r\n"]
[1326.036, "o", "    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n"]
[1326.046, "o", "\r\n"]
[1326.056, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[1326.066, "o", "\r\n"]
[1326.076, "o", "    >>> np.mean(X_transformed == 0)\r\n"]
[1326.086, "o", "    0.41...\r\n"]
[1326.096, "o", "\r\n"]
[1326.106, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[1326.116, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[1326.126, "o", "    the original signal:\r\n"]
[1326.136, "o", "\r\n"]
[1326.146, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[1326.156, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[1326.166, "o", "    0.07...\r\n"]
[1326.176, "o", "    \"\"\"\r\n"]
[1326.186, "o", "\r\n"]
[1326.196, "o", "    _parameter_constraints: dict = {\r\n"]
[1326.206, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1326.216, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1326.226, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1326.236, "o", "        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1326.246, "o", "        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[1326.256, "o", "        \"transform_algorithm\": [\r\n"]
[1326.266, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[1326.276, "o", "        ],\r\n"]
[1326.286, "o", "        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1326.296, "o", "        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[1326.306, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[1326.316, "o", "        \"code_init\": [np.ndarray, None],\r\n"]
[1326.326, "o", "        \"dict_init\": [np.ndarray, None],\r\n"]
[1326.336, "o", "        \"callback\": [callable, None],\r\n"]
[1326.346, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[1326.356, "o", "        \"split_sign\": [\"boolean\"],\r\n"]
[1326.366, "o", "        \"random_state\": [\"random_state\"],\r\n"]
[1326.376, "o", "        \"positive_code\": [\"boolean\"],\r\n"]
[1326.386, "o", "        \"positive_dict\": [\"boolean\"],\r\n"]
[1326.396, "o", "        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1326.406, "o", "    }\r\n"]
[1326.416, "o", "\r\n"]
[1326.426, "o", "    def __init__(\r\n"]
[1326.436, "o", "        self,\r\n"]
[1326.446, "o", "        n_components=None,\r\n"]
[1326.456, "o", "        *,\r\n"]
[1326.466, "o", "        alpha=1,\r\n"]
[1326.476, "o", "        max_iter=1000,\r\n"]
[1326.486, "o", "        tol=1e-8,\r\n"]
[1326.496, "o", "        fit_algorithm=\"lars\",\r\n"]
[1326.506, "o", "        transform_algorithm=\"omp\",\r\n"]
[1326.516, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[1326.526, "o", "        transform_alpha=None,\r\n"]
[1326.536, "o", "        n_jobs=None,\r\n"]
[1326.546, "o", "        code_init=None,\r\n"]
[1326.556, "o", "        dict_init=None,\r\n"]
[1326.566, "o", "        callback=None,\r\n"]
[1326.576, "o", "        verbose=False,\r\n"]
[1326.586, "o", "        split_sign=False,\r\n"]
[1326.596, "o", "        random_state=None,\r\n"]
[1326.606, "o", "        positive_code=False,\r\n"]
[1326.616, "o", "        positive_dict=False,\r\n"]
[1326.626, "o", "        transform_max_iter=1000,\r\n"]
[1326.636, "o", "    ):\r\n"]
[1326.646, "o", "        super().__init__(\r\n"]
[1326.656, "o", "            transform_algorithm,\r\n"]
[1326.666, "o", "            transform_n_nonzero_coefs,\r\n"]
[1326.676, "o", "            transform_alpha,\r\n"]
[1326.686, "o", "            split_sign,\r\n"]
[1326.696, "o", "            n_jobs,\r\n"]
[1326.706, "o", "            positive_code,\r\n"]
[1326.716, "o", "            transform_max_iter,\r\n"]
[1326.726, "o", "        )\r\n"]
[1326.736, "o", "        self.n_components = n_components\r\n"]
[1326.746, "o", "        self.alpha = alpha\r\n"]
[1326.756, "o", "        self.max_iter = max_iter\r\n"]
[1326.766, "o", "        self.tol = tol\r\n"]
[1326.776, "o", "        self.fit_algorithm = fit_algorithm\r\n"]
[1326.786, "o", "        self.code_init = code_init\r\n"]
[1326.796, "o", "        self.dict_init = dict_init\r\n"]
[1326.806, "o", "        self.callback = callback\r\n"]
[1326.816, "o", "        self.verbose = verbose\r\n"]
[1326.826, "o", "        self.random_state = random_state\r\n"]
[1326.836, "o", "        self.positive_dict = positive_dict\r\n"]
[1326.846, "o", "\r\n"]
[1326.856, "o", "    def fit(self, X, y=None):\r\n"]
[1326.866, "o", "        \"\"\"Fit the model from data in X.\r\n"]
[1326.876, "o", "\r\n"]
[1326.886, "o", "        Parameters\r\n"]
[1326.896, "o", "        ----------\r\n"]
[1326.906, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[1326.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1326.926, "o", "            and `n_features` is the number of features.\r\n"]
[1326.936, "o", "\r\n"]
[1326.946, "o", "        y : Ignored\r\n"]
[1326.956, "o", "            Not used, present for API consistency by convention.\r\n"]
[1326.966, "o", "\r\n"]
[1326.976, "o", "        Returns\r\n"]
[1326.986, "o", "        -------\r\n"]
[1326.996, "o", "        self : object\r\n"]
[1327.006, "o", "            Returns the instance itself.\r\n"]
[1327.016, "o", "        \"\"\"\r\n"]
[1327.026, "o", "        self.fit_transform(X)\r\n"]
[1327.036, "o", "        return self\r\n"]
[1327.046, "o", "\r\n"]
[1327.056, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[1327.066, "o", "    def fit_transform(self, X, y=None):\r\n"]
[1327.076, "o", "        \"\"\"Fit the model from data in X and return the transformed data.\r\n"]
[1327.086, "o", "\r\n"]
[1327.096, "o", "        Parameters\r\n"]
[1327.106, "o", "        ----------\r\n"]
[1327.116, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[1327.126, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1327.136, "o", "            and `n_features` is the number of features.\r\n"]
[1327.146, "o", "\r\n"]
[1327.156, "o", "        y : Ignored\r\n"]
[1327.166, "o", "            Not used, present for API consistency by convention.\r\n"]
[1327.176, "o", "\r\n"]
[1327.186, "o", "        Returns\r\n"]
[1327.196, "o", "        -------\r\n"]
[1327.206, "o", "        V : ndarray of shape (n_samples, n_components)\r\n"]
[1327.216, "o", "            Transformed data.\r\n"]
[1327.226, "o", "        \"\"\"\r\n"]
[1327.236, "o", "        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n"]
[1327.246, "o", "\r\n"]
[1327.256, "o", "        method = \"lasso_\" + self.fit_algorithm\r\n"]
[1327.266, "o", "\r\n"]
[1327.276, "o", "        random_state = check_random_state(self.random_state)\r\n"]
[1327.286, "o", "        X = self._validate_data(X)\r\n"]
[1327.296, "o", "\r\n"]
[1327.306, "o", "        if self.n_components is None:\r\n"]
[1327.316, "o", "            n_components = X.shape[1]\r\n"]
[1327.326, "o", "        else:\r\n"]
[1327.336, "o", "            n_components = self.n_components\r\n"]
[1327.346, "o", "\r\n"]
[1327.356, "o", "        V, U, E, self.n_iter_ = _dict_learning(\r\n"]
[1327.366, "o", "            X,\r\n"]
[1327.376, "o", "            n_components,\r\n"]
[1327.386, "o", "            alpha=self.alpha,\r\n"]
[1327.396, "o", "            tol=self.tol,\r\n"]
[1327.406, "o", "            max_iter=self.max_iter,\r\n"]
[1327.416, "o", "            method=method,\r\n"]
[1327.426, "o", "            method_max_iter=self.transform_max_iter,\r\n"]
[1327.436, "o", "            n_jobs=self.n_jobs,\r\n"]
[1327.446, "o", "            code_init=self.code_init,\r\n"]
[1327.456, "o", "            dict_init=self.dict_init,\r\n"]
[1327.466, "o", "            callback=self.callback,\r\n"]
[1327.476, "o", "            verbose=self.verbose,\r\n"]
[1327.486, "o", "            random_state=random_state,\r\n"]
[1327.496, "o", "            return_n_iter=True,\r\n"]
[1327.506, "o", "            positive_dict=self.positive_dict,\r\n"]
[1327.516, "o", "            positive_code=self.positive_code,\r\n"]
[1327.526, "o", "        )\r\n"]
[1327.536, "o", "        self.components_ = U\r\n"]
[1327.546, "o", "        self.error_ = E\r\n"]
[1327.556, "o", "\r\n"]
[1327.566, "o", "        return V\r\n"]
[1327.576, "o", "\r\n"]
[1327.586, "o", "    @property\r\n"]
[1327.596, "o", "    def _n_features_out(self):\r\n"]
[1327.606, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[1327.616, "o", "        return self.components_.shape[0]\r\n"]
[1327.626, "o", "\r\n"]
[1327.636, "o", "    def _more_tags(self):\r\n"]
[1327.646, "o", "        return {\r\n"]
[1327.656, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[1327.666, "o", "        }\r\n"]
[1327.676, "o", "\r\n"]
[1327.686, "o", "\r\n"]
[1327.696, "o", "class MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[1327.706, "o", "    \"\"\"Mini-batch dictionary learning.\r\n"]
[1327.716, "o", "\r\n"]
[1327.726, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[1327.736, "o", "    encoding the fitted data.\r\n"]
[1327.746, "o", "\r\n"]
[1327.756, "o", "    Solves the optimization problem::\r\n"]
[1327.766, "o", "\r\n"]
[1327.776, "o", "       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[1327.786, "o", "                    (U,V)\r\n"]
[1327.796, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[1327.806, "o", "\r\n"]
[1327.816, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[1327.826, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[1327.836, "o", "    of all the entries in the matrix.\r\n"]
[1327.846, "o", "\r\n"]
[1327.856, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[1327.866, "o", "\r\n"]
[1327.876, "o", "    Parameters\r\n"]
[1327.886, "o", "    ----------\r\n"]
[1327.896, "o", "    n_components : int, default=None\r\n"]
[1327.906, "o", "        Number of dictionary elements to extract.\r\n"]
[1327.916, "o", "\r\n"]
[1327.926, "o", "    alpha : float, default=1\r\n"]
[1327.936, "o", "        Sparsity controlling parameter.\r\n"]
[1327.946, "o", "\r\n"]
[1327.956, "o", "    n_iter : int, default=1000\r\n"]
[1327.966, "o", "        Total number of iterations over data batches to perform.\r\n"]
[1327.976, "o", "\r\n"]
[1327.986, "o", "        .. deprecated:: 1.1\r\n"]
[1327.996, "o", "           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[1328.006, "o", "           ``max_iter`` instead.\r\n"]
[1328.016, "o", "\r\n"]
[1328.026, "o", "    max_iter : int, default=None\r\n"]
[1328.036, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[1328.046, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[1328.056, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[1328.066, "o", "\r\n"]
[1328.076, "o", "        .. versionadded:: 1.1\r\n"]
[1328.086, "o", "\r\n"]
[1328.096, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[1328.106, "o", "        The algorithm used:\r\n"]
[1328.116, "o", "\r\n"]
[1328.126, "o", "        - `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[1328.136, "o", "          problem (`linear_model.lars_path`)\r\n"]
[1328.146, "o", "        - `'cd'`: uses the coordinate descent method to compute the\r\n"]
[1328.156, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[1328.166, "o", "          the estimated components are sparse.\r\n"]
[1328.176, "o", "\r\n"]
[1328.186, "o", "    n_jobs : int, default=None\r\n"]
[1328.196, "o", "        Number of parallel jobs to run.\r\n"]
[1328.206, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1328.216, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1328.226, "o", "        for more details.\r\n"]
[1328.236, "o", "\r\n"]
[1328.246, "o", "    batch_size : int, default=256\r\n"]
[1328.256, "o", "        Number of samples in each mini-batch.\r\n"]
[1328.266, "o", "\r\n"]
[1328.276, "o", "        .. versionchanged:: 1.3\r\n"]
[1328.286, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[1328.296, "o", "\r\n"]
[1328.306, "o", "    shuffle : bool, default=True\r\n"]
[1328.316, "o", "        Whether to shuffle the samples before forming batches.\r\n"]
[1328.326, "o", "\r\n"]
[1328.336, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1328.346, "o", "        Initial value of the dictionary for warm restart scenarios.\r\n"]
[1328.356, "o", "\r\n"]
[1328.366, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[1328.376, "o", "            'threshold'}, default='omp'\r\n"]
[1328.386, "o", "        Algorithm used to transform the data:\r\n"]
[1328.396, "o", "\r\n"]
[1328.406, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[1328.416, "o", "          (`linear_model.lars_path`);\r\n"]
[1328.426, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[1328.436, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1328.446, "o", "          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n"]
[1328.456, "o", "          if the estimated components are sparse.\r\n"]
[1328.466, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1328.476, "o", "          solution.\r\n"]
[1328.486, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[1328.496, "o", "          the projection ``dictionary * X'``.\r\n"]
[1328.506, "o", "\r\n"]
[1328.516, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[1328.526, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1328.536, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[1328.546, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[1328.556, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1328.566, "o", "\r\n"]
[1328.576, "o", "    transform_alpha : float, default=None\r\n"]
[1328.586, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1328.596, "o", "        penalty applied to the L1 norm.\r\n"]
[1328.606, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1328.616, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1328.626, "o", "        If `None`, defaults to `alpha`.\r\n"]
[1328.636, "o", "\r\n"]
[1328.646, "o", "        .. versionchanged:: 1.2\r\n"]
[1328.656, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[1328.666, "o", "\r\n"]
[1328.676, "o", "    verbose : bool or int, default=False\r\n"]
[1328.686, "o", "        To control the verbosity of the procedure.\r\n"]
[1328.696, "o", "\r\n"]
[1328.706, "o", "    split_sign : bool, default=False\r\n"]
[1328.716, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[1328.726, "o", "        its negative part and its positive part. This can improve the\r\n"]
[1328.736, "o", "        performance of downstream classifiers.\r\n"]
[1328.746, "o", "\r\n"]
[1328.756, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1328.766, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[1328.776, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[1328.786, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[1328.796, "o", "        results across multiple function calls.\r\n"]
[1328.806, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1328.816, "o", "\r\n"]
[1328.826, "o", "    positive_code : bool, default=False\r\n"]
[1328.836, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1328.846, "o", "\r\n"]
[1328.856, "o", "        .. versionadded:: 0.20\r\n"]
[1328.866, "o", "\r\n"]
[1328.876, "o", "    positive_dict : bool, default=False\r\n"]
[1328.886, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1328.896, "o", "\r\n"]
[1328.906, "o", "        .. versionadded:: 0.20\r\n"]
[1328.916, "o", "\r\n"]
[1328.926, "o", "    transform_max_iter : int, default=1000\r\n"]
[1328.936, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1328.946, "o", "        `'lasso_lars'`.\r\n"]
[1328.956, "o", "\r\n"]
[1328.966, "o", "        .. versionadded:: 0.22\r\n"]
[1328.976, "o", "\r\n"]
[1328.986, "o", "    callback : callable, default=None\r\n"]
[1328.996, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[1329.006, "o", "\r\n"]
[1329.016, "o", "        .. versionadded:: 1.1\r\n"]
[1329.026, "o", "\r\n"]
[1329.036, "o", "    tol : float, default=1e-3\r\n"]
[1329.046, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[1329.056, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[1329.066, "o", "\r\n"]
[1329.076, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[1329.086, "o", "        `tol` to 0.0.\r\n"]
[1329.096, "o", "\r\n"]
[1329.106, "o", "        .. versionadded:: 1.1\r\n"]
[1329.116, "o", "\r\n"]
[1329.126, "o", "    max_no_improvement : int, default=10\r\n"]
[1329.136, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[1329.146, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[1329.156, "o", "        `max_iter` is not None.\r\n"]
[1329.166, "o", "\r\n"]
[1329.176, "o", "        To disable convergence detection based on cost function, set\r\n"]
[1329.186, "o", "        `max_no_improvement` to None.\r\n"]
[1329.196, "o", "\r\n"]
[1329.206, "o", "        .. versionadded:: 1.1\r\n"]
[1329.216, "o", "\r\n"]
[1329.226, "o", "    Attributes\r\n"]
[1329.236, "o", "    ----------\r\n"]
[1329.246, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[1329.256, "o", "        Components extracted from the data.\r\n"]
[1329.266, "o", "\r\n"]
[1329.276, "o", "    n_features_in_ : int\r\n"]
[1329.286, "o", "        Number of features seen during :term:`fit`.\r\n"]
[1329.296, "o", "\r\n"]
[1329.306, "o", "        .. versionadded:: 0.24\r\n"]
[1329.316, "o", "\r\n"]
[1329.326, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[1329.336, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[1329.346, "o", "        has feature names that are all strings.\r\n"]
[1329.356, "o", "\r\n"]
[1329.366, "o", "        .. versionadded:: 1.0\r\n"]
[1329.376, "o", "\r\n"]
[1329.386, "o", "    n_iter_ : int\r\n"]
[1329.396, "o", "        Number of iterations over the full dataset.\r\n"]
[1329.406, "o", "\r\n"]
[1329.416, "o", "    n_steps_ : int\r\n"]
[1329.426, "o", "        Number of mini-batches processed.\r\n"]
[1329.436, "o", "\r\n"]
[1329.446, "o", "        .. versionadded:: 1.1\r\n"]
[1329.456, "o", "\r\n"]
[1329.466, "o", "    See Also\r\n"]
[1329.476, "o", "    --------\r\n"]
[1329.486, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1329.496, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1329.506, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[1329.516, "o", "        precomputed dictionary.\r\n"]
[1329.526, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1329.536, "o", "\r\n"]
[1329.546, "o", "    References\r\n"]
[1329.556, "o", "    ----------\r\n"]
[1329.566, "o", "\r\n"]
[1329.576, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[1329.586, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[1329.596, "o", "\r\n"]
[1329.606, "o", "    Examples\r\n"]
[1329.616, "o", "    --------\r\n"]
[1329.626, "o", "    >>> import numpy as np\r\n"]
[1329.636, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[1329.646, "o", "    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n"]
[1329.656, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[1329.666, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[1329.676, "o", "    ...     random_state=42)\r\n"]
[1329.686, "o", "    >>> dict_learner = MiniBatchDictionaryLearning(\r\n"]
[1329.696, "o", "    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n"]
[1329.706, "o", "    ...     transform_alpha=0.1, random_state=42)\r\n"]
[1329.716, "o", "    >>> X_transformed = dict_learner.fit_transform(X)\r\n"]
[1329.726, "o", "\r\n"]
[1329.736, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[1329.746, "o", "\r\n"]
[1329.756, "o", "    >>> np.mean(X_transformed == 0) < 0.5\r\n"]
[1329.766, "o", "    True\r\n"]
[1329.776, "o", "\r\n"]
[1329.786, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[1329.796, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[1329.806, "o", "    the original signal:\r\n"]
[1329.816, "o", "\r\n"]
[1329.826, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[1329.836, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[1329.846, "o", "    0.057...\r\n"]
[1329.856, "o", "    \"\"\"\r\n"]
[1329.866, "o", "\r\n"]
[1329.876, "o", "    _parameter_constraints: dict = {\r\n"]
[1329.886, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1329.896, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1329.906, "o", "        \"n_iter\": [\r\n"]
[1329.916, "o", "            Interval(Integral, 0, None, closed=\"left\"),\r\n"]
[1329.926, "o", "            Hidden(StrOptions({\"deprecated\"})),\r\n"]
[1329.936, "o", "        ],\r\n"]
[1330.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1330.002, "i", "cd /workspace/repo\r"]
[1330.004, "o", "cd /workspace/repo\r\n"]
[1330.006, "o", "\u001b[?2004l\r\n"]
[1335.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1335.002, "i", "grep -n \"def _minibatch_step\\|_minibatch_step\\(\" sklearn/decomposition/_dict_learning.py\r"]
[1335.004, "o", "grep -n \"def _minibatch_step\\|_minibatch_step\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1337.478, "o", "\u001b[?2004l\r\n"]
[1340.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1340.002, "i", "grep -n \"def _update_dict\\|def update_dict\\|def _update_dict\\(\" sklearn/decomposition/_dict_learning.py || true\r"]
[1340.004, "o", "grep -n \"def _update_dict\\|def update_dict\\|def _update_dict\\(\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[1342.478, "o", "\u001b[?2004l\r\n"]
[1345.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1345.002, "i", "grep -n \"_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.py\r"]
[1345.004, "o", "grep -n \"_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1347.478, "o", "\u001b[?2004l\r\n"]
[1350.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1350.002, "i", "sed -n '1,900p' sklearn/decomposition/_dict_learning.py\r"]
[1350.004, "o", "sed -n '1,900p' sklearn/decomposition/_dict_learning.py\r\n"]
[1350.016, "o", "\u001b[?2004l\r\n"]
[1350.026, "o", "\"\"\" Dictionary learning.\r\n"]
[1350.036, "o", "\"\"\"\r\n"]
[1350.046, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[1350.056, "o", "# License: BSD 3 clause\r\n"]
[1350.066, "o", "\r\n"]
[1350.076, "o", "import itertools\r\n"]
[1350.086, "o", "import sys\r\n"]
[1350.096, "o", "import time\r\n"]
[1350.106, "o", "import warnings\r\n"]
[1350.116, "o", "from math import ceil\r\n"]
[1350.126, "o", "from numbers import Integral, Real\r\n"]
[1350.136, "o", "\r\n"]
[1350.146, "o", "import numpy as np\r\n"]
[1350.156, "o", "from joblib import effective_n_jobs\r\n"]
[1350.166, "o", "from scipy import linalg\r\n"]
[1350.176, "o", "\r\n"]
[1350.186, "o", "from ..base import (\r\n"]
[1350.196, "o", "    BaseEstimator,\r\n"]
[1350.206, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[1350.216, "o", "    TransformerMixin,\r\n"]
[1350.226, "o", "    _fit_context,\r\n"]
[1350.236, "o", ")\r\n"]
[1350.246, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[1350.256, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[1350.266, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[1350.276, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[1350.286, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[1350.296, "o", "from ..utils.validation import check_is_fitted\r\n"]
[1350.306, "o", "\r\n"]
[1350.316, "o", "\r\n"]
[1350.326, "o", "def _check_positive_coding(method, positive):\r\n"]
[1350.336, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[1350.346, "o", "        raise ValueError(\r\n"]
[1350.356, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[1350.366, "o", "        )\r\n"]
[1350.376, "o", "\r\n"]
[1350.386, "o", "\r\n"]
[1350.396, "o", "def _sparse_encode_precomputed(\r\n"]
[1350.406, "o", "    X,\r\n"]
[1350.416, "o", "    dictionary,\r\n"]
[1350.426, "o", "    *,\r\n"]
[1350.436, "o", "    gram=None,\r\n"]
[1350.446, "o", "    cov=None,\r\n"]
[1350.456, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1350.466, "o", "    regularization=None,\r\n"]
[1350.476, "o", "    copy_cov=True,\r\n"]
[1350.486, "o", "    init=None,\r\n"]
[1350.496, "o", "    max_iter=1000,\r\n"]
[1350.506, "o", "    verbose=0,\r\n"]
[1350.516, "o", "    positive=False,\r\n"]
[1350.526, "o", "):\r\n"]
[1350.536, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[1350.546, "o", "\r\n"]
[1350.556, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[1350.566, "o", "\r\n"]
[1350.576, "o", "    Parameters\r\n"]
[1350.586, "o", "    ----------\r\n"]
[1350.596, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[1350.606, "o", "        Data matrix.\r\n"]
[1350.616, "o", "\r\n"]
[1350.626, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1350.636, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1350.646, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[1350.656, "o", "\r\n"]
[1350.666, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[1350.676, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[1350.686, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[1350.696, "o", "\r\n"]
[1350.706, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[1350.716, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[1350.726, "o", "\r\n"]
[1350.736, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1350.746, "o", "            default='lasso_lars'\r\n"]
[1350.756, "o", "        The algorithm used:\r\n"]
[1350.766, "o", "\r\n"]
[1350.776, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[1350.786, "o", "          (`linear_model.lars_path`);\r\n"]
[1350.796, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1350.806, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1350.816, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1350.826, "o", "          the estimated components are sparse;\r\n"]
[1350.836, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1350.846, "o", "          solution;\r\n"]
[1350.856, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1350.866, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[1350.876, "o", "\r\n"]
[1350.886, "o", "    regularization : int or float, default=None\r\n"]
[1350.896, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[1350.906, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[1350.916, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[1350.926, "o", "\r\n"]
[1350.936, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1350.946, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[1350.956, "o", "        `algorithm='lasso_cd'`.\r\n"]
[1350.966, "o", "\r\n"]
[1350.976, "o", "    max_iter : int, default=1000\r\n"]
[1350.986, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1350.996, "o", "        `'lasso_lars'`.\r\n"]
[1351.006, "o", "\r\n"]
[1351.016, "o", "    copy_cov : bool, default=True\r\n"]
[1351.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1351.036, "o", "        be overwritten.\r\n"]
[1351.046, "o", "\r\n"]
[1351.056, "o", "    verbose : int, default=0\r\n"]
[1351.066, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[1351.076, "o", "\r\n"]
[1351.086, "o", "    positive: bool, default=False\r\n"]
[1351.096, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[1351.106, "o", "\r\n"]
[1351.116, "o", "        .. versionadded:: 0.20\r\n"]
[1351.126, "o", "\r\n"]
[1351.136, "o", "    Returns\r\n"]
[1351.146, "o", "    -------\r\n"]
[1351.156, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[1351.166, "o", "        The sparse codes.\r\n"]
[1351.176, "o", "    \"\"\"\r\n"]
[1351.186, "o", "    n_samples, n_features = X.shape\r\n"]
[1351.196, "o", "    n_components = dictionary.shape[0]\r\n"]
[1351.206, "o", "\r\n"]
[1351.216, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[1351.226, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1351.236, "o", "        try:\r\n"]
[1351.246, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1351.256, "o", "\r\n"]
[1351.266, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1351.276, "o", "            # corrects the verbosity level.\r\n"]
[1351.286, "o", "            lasso_lars = LassoLars(\r\n"]
[1351.296, "o", "                alpha=alpha,\r\n"]
[1351.306, "o", "                fit_intercept=False,\r\n"]
[1351.316, "o", "                verbose=verbose,\r\n"]
[1351.326, "o", "                precompute=gram,\r\n"]
[1351.336, "o", "                fit_path=False,\r\n"]
[1351.346, "o", "                positive=positive,\r\n"]
[1351.356, "o", "                max_iter=max_iter,\r\n"]
[1351.366, "o", "            )\r\n"]
[1351.376, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1351.386, "o", "            new_code = lasso_lars.coef_\r\n"]
[1351.396, "o", "        finally:\r\n"]
[1351.406, "o", "            np.seterr(**err_mgt)\r\n"]
[1351.416, "o", "\r\n"]
[1351.426, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[1351.436, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1351.446, "o", "\r\n"]
[1351.456, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[1351.466, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[1351.476, "o", "        # argument that we could pass in from Lasso.\r\n"]
[1351.486, "o", "        clf = Lasso(\r\n"]
[1351.496, "o", "            alpha=alpha,\r\n"]
[1351.506, "o", "            fit_intercept=False,\r\n"]
[1351.516, "o", "            precompute=gram,\r\n"]
[1351.526, "o", "            max_iter=max_iter,\r\n"]
[1351.536, "o", "            warm_start=True,\r\n"]
[1351.546, "o", "            positive=positive,\r\n"]
[1351.556, "o", "        )\r\n"]
[1351.566, "o", "\r\n"]
[1351.576, "o", "        if init is not None:\r\n"]
[1351.586, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[1351.596, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[1351.606, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[1351.616, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[1351.626, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[1351.636, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[1351.646, "o", "                init = np.array(init)\r\n"]
[1351.656, "o", "            clf.coef_ = init\r\n"]
[1351.666, "o", "\r\n"]
[1351.676, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[1351.686, "o", "        new_code = clf.coef_\r\n"]
[1351.696, "o", "\r\n"]
[1351.706, "o", "    elif algorithm == \"lars\":\r\n"]
[1351.716, "o", "        try:\r\n"]
[1351.726, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1351.736, "o", "\r\n"]
[1351.746, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1351.756, "o", "            # corrects the verbosity level.\r\n"]
[1351.766, "o", "            lars = Lars(\r\n"]
[1351.776, "o", "                fit_intercept=False,\r\n"]
[1351.786, "o", "                verbose=verbose,\r\n"]
[1351.796, "o", "                precompute=gram,\r\n"]
[1351.806, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[1351.816, "o", "                fit_path=False,\r\n"]
[1351.826, "o", "            )\r\n"]
[1351.836, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1351.846, "o", "            new_code = lars.coef_\r\n"]
[1351.856, "o", "        finally:\r\n"]
[1351.866, "o", "            np.seterr(**err_mgt)\r\n"]
[1351.876, "o", "\r\n"]
[1351.886, "o", "    elif algorithm == \"threshold\":\r\n"]
[1351.896, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[1351.906, "o", "        if positive:\r\n"]
[1351.916, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[1351.926, "o", "\r\n"]
[1351.936, "o", "    elif algorithm == \"omp\":\r\n"]
[1351.946, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[1351.956, "o", "            Gram=gram,\r\n"]
[1351.966, "o", "            Xy=cov,\r\n"]
[1351.976, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[1351.986, "o", "            tol=None,\r\n"]
[1351.996, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[1352.006, "o", "            copy_Xy=copy_cov,\r\n"]
[1352.016, "o", "        ).T\r\n"]
[1352.026, "o", "\r\n"]
[1352.036, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[1352.046, "o", "\r\n"]
[1352.056, "o", "\r\n"]
[1352.066, "o", "@validate_params(\r\n"]
[1352.076, "o", "    {\r\n"]
[1352.086, "o", "        \"X\": [\"array-like\"],\r\n"]
[1352.096, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[1352.106, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[1352.116, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[1352.126, "o", "        \"algorithm\": [\r\n"]
[1352.136, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[1352.146, "o", "        ],\r\n"]
[1352.156, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1352.166, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[1352.176, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[1352.186, "o", "        \"init\": [\"array-like\", None],\r\n"]
[1352.196, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1352.206, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[1352.216, "o", "        \"check_input\": [\"boolean\"],\r\n"]
[1352.226, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[1352.236, "o", "        \"positive\": [\"boolean\"],\r\n"]
[1352.246, "o", "    },\r\n"]
[1352.256, "o", "    prefer_skip_nested_validation=True,\r\n"]
[1352.266, "o", ")\r\n"]
[1352.276, "o", "# XXX : could be moved to the linear_model module\r\n"]
[1352.286, "o", "def sparse_encode(\r\n"]
[1352.296, "o", "    X,\r\n"]
[1352.306, "o", "    dictionary,\r\n"]
[1352.316, "o", "    *,\r\n"]
[1352.326, "o", "    gram=None,\r\n"]
[1352.336, "o", "    cov=None,\r\n"]
[1352.346, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1352.356, "o", "    n_nonzero_coefs=None,\r\n"]
[1352.366, "o", "    alpha=None,\r\n"]
[1352.376, "o", "    copy_cov=True,\r\n"]
[1352.386, "o", "    init=None,\r\n"]
[1352.396, "o", "    max_iter=1000,\r\n"]
[1352.406, "o", "    n_jobs=None,\r\n"]
[1352.416, "o", "    check_input=True,\r\n"]
[1352.426, "o", "    verbose=0,\r\n"]
[1352.436, "o", "    positive=False,\r\n"]
[1352.446, "o", "):\r\n"]
[1352.456, "o", "    \"\"\"Sparse coding.\r\n"]
[1352.466, "o", "\r\n"]
[1352.476, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[1352.486, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[1352.496, "o", "\r\n"]
[1352.506, "o", "        X ~= code * dictionary\r\n"]
[1352.516, "o", "\r\n"]
[1352.526, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[1352.536, "o", "\r\n"]
[1352.546, "o", "    Parameters\r\n"]
[1352.556, "o", "    ----------\r\n"]
[1352.566, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[1352.576, "o", "        Data matrix.\r\n"]
[1352.586, "o", "\r\n"]
[1352.596, "o", "    dictionary : array-like of shape (n_components, n_features)\r\n"]
[1352.606, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1352.616, "o", "        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[1352.626, "o", "        output.\r\n"]
[1352.636, "o", "\r\n"]
[1352.646, "o", "    gram : array-like of shape (n_components, n_components), default=None\r\n"]
[1352.656, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n"]
[1352.666, "o", "\r\n"]
[1352.676, "o", "    cov : array-like of shape (n_components, n_samples), default=None\r\n"]
[1352.686, "o", "        Precomputed covariance, `dictionary' * X`.\r\n"]
[1352.696, "o", "\r\n"]
[1352.706, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1352.716, "o", "            default='lasso_lars'\r\n"]
[1352.726, "o", "        The algorithm used:\r\n"]
[1352.736, "o", "\r\n"]
[1352.746, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[1352.756, "o", "          (`linear_model.lars_path`);\r\n"]
[1352.766, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1352.776, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1352.786, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1352.796, "o", "          the estimated components are sparse;\r\n"]
[1352.806, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1352.816, "o", "          solution;\r\n"]
[1352.826, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1352.836, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[1352.846, "o", "\r\n"]
[1352.856, "o", "    n_nonzero_coefs : int, default=None\r\n"]
[1352.866, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1352.876, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[1352.886, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[1352.896, "o", "        `n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1352.906, "o", "\r\n"]
[1352.916, "o", "    alpha : float, default=None\r\n"]
[1352.926, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1352.936, "o", "        penalty applied to the L1 norm.\r\n"]
[1352.946, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1352.956, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1352.966, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[1352.976, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[1352.986, "o", "        `n_nonzero_coefs`.\r\n"]
[1352.996, "o", "        If `None`, default to 1.\r\n"]
[1353.006, "o", "\r\n"]
[1353.016, "o", "    copy_cov : bool, default=True\r\n"]
[1353.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1353.036, "o", "        be overwritten.\r\n"]
[1353.046, "o", "\r\n"]
[1353.056, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1353.066, "o", "        Initialization value of the sparse codes. Only used if\r\n"]
[1353.076, "o", "        `algorithm='lasso_cd'`.\r\n"]
[1353.086, "o", "\r\n"]
[1353.096, "o", "    max_iter : int, default=1000\r\n"]
[1353.106, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1353.116, "o", "        `'lasso_lars'`.\r\n"]
[1353.126, "o", "\r\n"]
[1353.136, "o", "    n_jobs : int, default=None\r\n"]
[1353.146, "o", "        Number of parallel jobs to run.\r\n"]
[1353.156, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1353.166, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1353.176, "o", "        for more details.\r\n"]
[1353.186, "o", "\r\n"]
[1353.196, "o", "    check_input : bool, default=True\r\n"]
[1353.206, "o", "        If `False`, the input arrays X and dictionary will not be checked.\r\n"]
[1353.216, "o", "\r\n"]
[1353.226, "o", "    verbose : int, default=0\r\n"]
[1353.236, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[1353.246, "o", "\r\n"]
[1353.256, "o", "    positive : bool, default=False\r\n"]
[1353.266, "o", "        Whether to enforce positivity when finding the encoding.\r\n"]
[1353.276, "o", "\r\n"]
[1353.286, "o", "        .. versionadded:: 0.20\r\n"]
[1353.296, "o", "\r\n"]
[1353.306, "o", "    Returns\r\n"]
[1353.316, "o", "    -------\r\n"]
[1353.326, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1353.336, "o", "        The sparse codes.\r\n"]
[1353.346, "o", "\r\n"]
[1353.356, "o", "    See Also\r\n"]
[1353.366, "o", "    --------\r\n"]
[1353.376, "o", "    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n"]
[1353.386, "o", "        path using LARS algorithm.\r\n"]
[1353.396, "o", "    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n"]
[1353.406, "o", "    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n"]
[1353.416, "o", "    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n"]
[1353.426, "o", "        dictionary.\r\n"]
[1353.436, "o", "    \"\"\"\r\n"]
[1353.446, "o", "    if check_input:\r\n"]
[1353.456, "o", "        if algorithm == \"lasso_cd\":\r\n"]
[1353.466, "o", "            dictionary = check_array(\r\n"]
[1353.476, "o", "                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n"]
[1353.486, "o", "            )\r\n"]
[1353.496, "o", "            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n"]
[1353.506, "o", "        else:\r\n"]
[1353.516, "o", "            dictionary = check_array(dictionary)\r\n"]
[1353.526, "o", "            X = check_array(X)\r\n"]
[1353.536, "o", "\r\n"]
[1353.546, "o", "    if dictionary.shape[1] != X.shape[1]:\r\n"]
[1353.556, "o", "        raise ValueError(\r\n"]
[1353.566, "o", "            \"Dictionary and X have different numbers of features:\"\r\n"]
[1353.576, "o", "            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n"]
[1353.586, "o", "        )\r\n"]
[1353.596, "o", "\r\n"]
[1353.606, "o", "    _check_positive_coding(algorithm, positive)\r\n"]
[1353.616, "o", "\r\n"]
[1353.626, "o", "    return _sparse_encode(\r\n"]
[1353.636, "o", "        X,\r\n"]
[1353.646, "o", "        dictionary,\r\n"]
[1353.656, "o", "        gram=gram,\r\n"]
[1353.666, "o", "        cov=cov,\r\n"]
[1353.676, "o", "        algorithm=algorithm,\r\n"]
[1353.686, "o", "        n_nonzero_coefs=n_nonzero_coefs,\r\n"]
[1353.696, "o", "        alpha=alpha,\r\n"]
[1353.706, "o", "        copy_cov=copy_cov,\r\n"]
[1353.716, "o", "        init=init,\r\n"]
[1353.726, "o", "        max_iter=max_iter,\r\n"]
[1353.736, "o", "        n_jobs=n_jobs,\r\n"]
[1353.746, "o", "        verbose=verbose,\r\n"]
[1353.756, "o", "        positive=positive,\r\n"]
[1353.766, "o", "    )\r\n"]
[1353.776, "o", "\r\n"]
[1353.786, "o", "\r\n"]
[1353.796, "o", "def _sparse_encode(\r\n"]
[1353.806, "o", "    X,\r\n"]
[1353.816, "o", "    dictionary,\r\n"]
[1353.826, "o", "    *,\r\n"]
[1353.836, "o", "    gram=None,\r\n"]
[1353.846, "o", "    cov=None,\r\n"]
[1353.856, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1353.866, "o", "    n_nonzero_coefs=None,\r\n"]
[1353.876, "o", "    alpha=None,\r\n"]
[1353.886, "o", "    copy_cov=True,\r\n"]
[1353.896, "o", "    init=None,\r\n"]
[1353.906, "o", "    max_iter=1000,\r\n"]
[1353.916, "o", "    n_jobs=None,\r\n"]
[1353.926, "o", "    verbose=0,\r\n"]
[1353.936, "o", "    positive=False,\r\n"]
[1353.946, "o", "):\r\n"]
[1353.956, "o", "    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[1353.966, "o", "\r\n"]
[1353.976, "o", "    n_samples, n_features = X.shape\r\n"]
[1353.986, "o", "    n_components = dictionary.shape[0]\r\n"]
[1353.996, "o", "\r\n"]
[1354.006, "o", "    if algorithm in (\"lars\", \"omp\"):\r\n"]
[1354.016, "o", "        regularization = n_nonzero_coefs\r\n"]
[1354.026, "o", "        if regularization is None:\r\n"]
[1354.036, "o", "            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[1354.046, "o", "    else:\r\n"]
[1354.056, "o", "        regularization = alpha\r\n"]
[1354.066, "o", "        if regularization is None:\r\n"]
[1354.076, "o", "            regularization = 1.0\r\n"]
[1354.086, "o", "\r\n"]
[1354.096, "o", "    if gram is None and algorithm != \"threshold\":\r\n"]
[1354.106, "o", "        gram = np.dot(dictionary, dictionary.T)\r\n"]
[1354.116, "o", "\r\n"]
[1354.126, "o", "    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[1354.136, "o", "        copy_cov = False\r\n"]
[1354.146, "o", "        cov = np.dot(dictionary, X.T)\r\n"]
[1354.156, "o", "\r\n"]
[1354.166, "o", "    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[1354.176, "o", "        code = _sparse_encode_precomputed(\r\n"]
[1354.186, "o", "            X,\r\n"]
[1354.196, "o", "            dictionary,\r\n"]
[1354.206, "o", "            gram=gram,\r\n"]
[1354.216, "o", "            cov=cov,\r\n"]
[1354.226, "o", "            algorithm=algorithm,\r\n"]
[1354.236, "o", "            regularization=regularization,\r\n"]
[1354.246, "o", "            copy_cov=copy_cov,\r\n"]
[1354.256, "o", "            init=init,\r\n"]
[1354.266, "o", "            max_iter=max_iter,\r\n"]
[1354.276, "o", "            verbose=verbose,\r\n"]
[1354.286, "o", "            positive=positive,\r\n"]
[1354.296, "o", "        )\r\n"]
[1354.306, "o", "        return code\r\n"]
[1354.316, "o", "\r\n"]
[1354.326, "o", "    # Enter parallel code block\r\n"]
[1354.336, "o", "    n_samples = X.shape[0]\r\n"]
[1354.346, "o", "    n_components = dictionary.shape[0]\r\n"]
[1354.356, "o", "    code = np.empty((n_samples, n_components))\r\n"]
[1354.366, "o", "    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[1354.376, "o", "\r\n"]
[1354.386, "o", "    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[1354.396, "o", "        delayed(_sparse_encode_precomputed)(\r\n"]
[1354.406, "o", "            X[this_slice],\r\n"]
[1354.416, "o", "            dictionary,\r\n"]
[1354.426, "o", "            gram=gram,\r\n"]
[1354.436, "o", "            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[1354.446, "o", "            algorithm=algorithm,\r\n"]
[1354.456, "o", "            regularization=regularization,\r\n"]
[1354.466, "o", "            copy_cov=copy_cov,\r\n"]
[1354.476, "o", "            init=init[this_slice] if init is not None else None,\r\n"]
[1354.486, "o", "            max_iter=max_iter,\r\n"]
[1354.496, "o", "            verbose=verbose,\r\n"]
[1354.506, "o", "            positive=positive,\r\n"]
[1354.516, "o", "        )\r\n"]
[1354.526, "o", "        for this_slice in slices\r\n"]
[1354.536, "o", "    )\r\n"]
[1354.546, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[1354.556, "o", "        code[this_slice] = this_view\r\n"]
[1354.566, "o", "    return code\r\n"]
[1354.576, "o", "\r\n"]
[1354.586, "o", "\r\n"]
[1354.596, "o", "def _update_dict(\r\n"]
[1354.606, "o", "    dictionary,\r\n"]
[1354.616, "o", "    Y,\r\n"]
[1354.626, "o", "    code,\r\n"]
[1354.636, "o", "    A=None,\r\n"]
[1354.646, "o", "    B=None,\r\n"]
[1354.656, "o", "    verbose=False,\r\n"]
[1354.666, "o", "    random_state=None,\r\n"]
[1354.676, "o", "    positive=False,\r\n"]
[1354.686, "o", "):\r\n"]
[1354.696, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[1354.706, "o", "\r\n"]
[1354.716, "o", "    Parameters\r\n"]
[1354.726, "o", "    ----------\r\n"]
[1354.736, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1354.746, "o", "        Value of the dictionary at the previous iteration.\r\n"]
[1354.756, "o", "\r\n"]
[1354.766, "o", "    Y : ndarray of shape (n_samples, n_features)\r\n"]
[1354.776, "o", "        Data matrix.\r\n"]
[1354.786, "o", "\r\n"]
[1354.796, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1354.806, "o", "        Sparse coding of the data against which to optimize the dictionary.\r\n"]
[1354.816, "o", "\r\n"]
[1354.826, "o", "    A : ndarray of shape (n_components, n_components), default=None\r\n"]
[1354.836, "o", "        Together with `B`, sufficient stats of the online model to update the\r\n"]
[1354.846, "o", "        dictionary.\r\n"]
[1354.856, "o", "\r\n"]
[1354.866, "o", "    B : ndarray of shape (n_features, n_components), default=None\r\n"]
[1354.876, "o", "        Together with `A`, sufficient stats of the online model to update the\r\n"]
[1354.886, "o", "        dictionary.\r\n"]
[1354.896, "o", "\r\n"]
[1354.906, "o", "    verbose: bool, default=False\r\n"]
[1354.916, "o", "        Degree of output the procedure will print.\r\n"]
[1354.926, "o", "\r\n"]
[1354.936, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1355.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1355.002, "i", "sed -n '900,1800p' sklearn/decomposition/_dict_learning.py\r"]
[1355.004, "o", "sed -n '900,1800p' sklearn/decomposition/_dict_learning.py\r\n"]
[1355.016, "o", "\u001b[?2004l\r\n"]
[1355.026, "o", "            random_state=random_state,\r\n"]
[1355.036, "o", "            transform_algorithm=transform_algorithm,\r\n"]
[1355.046, "o", "            transform_alpha=alpha,\r\n"]
[1355.056, "o", "            positive_code=positive_code,\r\n"]
[1355.066, "o", "            positive_dict=positive_dict,\r\n"]
[1355.076, "o", "            transform_max_iter=method_max_iter,\r\n"]
[1355.086, "o", "            verbose=verbose,\r\n"]
[1355.096, "o", "            callback=callback,\r\n"]
[1355.106, "o", "            tol=tol,\r\n"]
[1355.116, "o", "            max_no_improvement=max_no_improvement,\r\n"]
[1355.126, "o", "        ).fit(X)\r\n"]
[1355.136, "o", "\r\n"]
[1355.146, "o", "        if not return_code:\r\n"]
[1355.156, "o", "            return est.components_\r\n"]
[1355.166, "o", "        else:\r\n"]
[1355.176, "o", "            code = est.transform(X)\r\n"]
[1355.186, "o", "            return code, est.components_\r\n"]
[1355.196, "o", "\r\n"]
[1355.206, "o", "    # TODO(1.4) remove the whole old behavior\r\n"]
[1355.216, "o", "    # Fallback to old behavior\r\n"]
[1355.226, "o", "\r\n"]
[1355.236, "o", "    n_iter = _check_warn_deprecated(\r\n"]
[1355.246, "o", "        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n"]
[1355.256, "o", "    )\r\n"]
[1355.266, "o", "\r\n"]
[1355.276, "o", "    if n_components is None:\r\n"]
[1355.286, "o", "        n_components = X.shape[1]\r\n"]
[1355.296, "o", "\r\n"]
[1355.306, "o", "    if method not in (\"lars\", \"cd\"):\r\n"]
[1355.316, "o", "        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n"]
[1355.326, "o", "\r\n"]
[1355.336, "o", "    _check_positive_coding(method, positive_code)\r\n"]
[1355.346, "o", "\r\n"]
[1355.356, "o", "    method = \"lasso_\" + method\r\n"]
[1355.366, "o", "\r\n"]
[1355.376, "o", "    t0 = time.time()\r\n"]
[1355.386, "o", "    n_samples, n_features = X.shape\r\n"]
[1355.396, "o", "    # Avoid integer division problems\r\n"]
[1355.406, "o", "    alpha = float(alpha)\r\n"]
[1355.416, "o", "    random_state = check_random_state(random_state)\r\n"]
[1355.426, "o", "\r\n"]
[1355.436, "o", "    # Init V with SVD of X\r\n"]
[1355.446, "o", "    if dict_init is not None:\r\n"]
[1355.456, "o", "        dictionary = dict_init\r\n"]
[1355.466, "o", "    else:\r\n"]
[1355.476, "o", "        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n"]
[1355.486, "o", "        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[1355.496, "o", "    r = len(dictionary)\r\n"]
[1355.506, "o", "    if n_components <= r:\r\n"]
[1355.516, "o", "        dictionary = dictionary[:n_components, :]\r\n"]
[1355.526, "o", "    else:\r\n"]
[1355.536, "o", "        dictionary = np.r_[\r\n"]
[1355.546, "o", "            dictionary,\r\n"]
[1355.556, "o", "            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n"]
[1355.566, "o", "        ]\r\n"]
[1355.576, "o", "\r\n"]
[1355.586, "o", "    if verbose == 1:\r\n"]
[1355.596, "o", "        print(\"[dict_learning]\", end=\" \")\r\n"]
[1355.606, "o", "\r\n"]
[1355.616, "o", "    if shuffle:\r\n"]
[1355.626, "o", "        X_train = X.copy()\r\n"]
[1355.636, "o", "        random_state.shuffle(X_train)\r\n"]
[1355.646, "o", "    else:\r\n"]
[1355.656, "o", "        X_train = X\r\n"]
[1355.666, "o", "\r\n"]
[1355.676, "o", "    X_train = check_array(\r\n"]
[1355.686, "o", "        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n"]
[1355.696, "o", "    )\r\n"]
[1355.706, "o", "\r\n"]
[1355.716, "o", "    # Fortran-order dict better suited for the sparse coding which is the\r\n"]
[1355.726, "o", "    # bottleneck of this algorithm.\r\n"]
[1355.736, "o", "    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n"]
[1355.746, "o", "    dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[1355.756, "o", "\r\n"]
[1355.766, "o", "    batches = gen_batches(n_samples, batch_size)\r\n"]
[1355.776, "o", "    batches = itertools.cycle(batches)\r\n"]
[1355.786, "o", "\r\n"]
[1355.796, "o", "    # The covariance of the dictionary\r\n"]
[1355.806, "o", "    if inner_stats is None:\r\n"]
[1355.816, "o", "        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n"]
[1355.826, "o", "        # The data approximation\r\n"]
[1355.836, "o", "        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n"]
[1355.846, "o", "    else:\r\n"]
[1355.856, "o", "        A = inner_stats[0].copy()\r\n"]
[1355.866, "o", "        B = inner_stats[1].copy()\r\n"]
[1355.876, "o", "\r\n"]
[1355.886, "o", "    # If n_iter is zero, we need to return zero.\r\n"]
[1355.896, "o", "    ii = iter_offset - 1\r\n"]
[1355.906, "o", "\r\n"]
[1355.916, "o", "    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n"]
[1355.926, "o", "        this_X = X_train[batch]\r\n"]
[1355.936, "o", "        dt = time.time() - t0\r\n"]
[1355.946, "o", "        if verbose == 1:\r\n"]
[1355.956, "o", "            sys.stdout.write(\".\")\r\n"]
[1355.966, "o", "            sys.stdout.flush()\r\n"]
[1355.976, "o", "        elif verbose:\r\n"]
[1355.986, "o", "            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n"]
[1355.996, "o", "                print(\r\n"]
[1356.006, "o", "                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n"]
[1356.016, "o", "                )\r\n"]
[1356.026, "o", "\r\n"]
[1356.036, "o", "        this_code = sparse_encode(\r\n"]
[1356.046, "o", "            this_X,\r\n"]
[1356.056, "o", "            dictionary,\r\n"]
[1356.066, "o", "            algorithm=method,\r\n"]
[1356.076, "o", "            alpha=alpha,\r\n"]
[1356.086, "o", "            n_jobs=n_jobs,\r\n"]
[1356.096, "o", "            check_input=False,\r\n"]
[1356.106, "o", "            positive=positive_code,\r\n"]
[1356.116, "o", "            max_iter=method_max_iter,\r\n"]
[1356.126, "o", "            verbose=verbose,\r\n"]
[1356.136, "o", "        )\r\n"]
[1356.146, "o", "\r\n"]
[1356.156, "o", "        # Update the auxiliary variables\r\n"]
[1356.166, "o", "        if ii < batch_size - 1:\r\n"]
[1356.176, "o", "            theta = float((ii + 1) * batch_size)\r\n"]
[1356.186, "o", "        else:\r\n"]
[1356.196, "o", "            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n"]
[1356.206, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[1356.216, "o", "\r\n"]
[1356.226, "o", "        A *= beta\r\n"]
[1356.236, "o", "        A += np.dot(this_code.T, this_code)\r\n"]
[1356.246, "o", "        B *= beta\r\n"]
[1356.256, "o", "        B += np.dot(this_X.T, this_code)\r\n"]
[1356.266, "o", "\r\n"]
[1356.276, "o", "        # Update dictionary in place\r\n"]
[1356.286, "o", "        _update_dict(\r\n"]
[1356.296, "o", "            dictionary,\r\n"]
[1356.306, "o", "            this_X,\r\n"]
[1356.316, "o", "            this_code,\r\n"]
[1356.326, "o", "            A,\r\n"]
[1356.336, "o", "            B,\r\n"]
[1356.346, "o", "            verbose=verbose,\r\n"]
[1356.356, "o", "            random_state=random_state,\r\n"]
[1356.366, "o", "            positive=positive_dict,\r\n"]
[1356.376, "o", "        )\r\n"]
[1356.386, "o", "\r\n"]
[1356.396, "o", "        # Maybe we need a stopping criteria based on the amount of\r\n"]
[1356.406, "o", "        # modification in the dictionary\r\n"]
[1356.416, "o", "        if callback is not None:\r\n"]
[1356.426, "o", "            callback(locals())\r\n"]
[1356.436, "o", "\r\n"]
[1356.446, "o", "    if return_inner_stats:\r\n"]
[1356.456, "o", "        if return_n_iter:\r\n"]
[1356.466, "o", "            return dictionary, (A, B), ii - iter_offset + 1\r\n"]
[1356.476, "o", "        else:\r\n"]
[1356.486, "o", "            return dictionary, (A, B)\r\n"]
[1356.496, "o", "    if return_code:\r\n"]
[1356.506, "o", "        if verbose > 1:\r\n"]
[1356.516, "o", "            print(\"Learning code...\", end=\" \")\r\n"]
[1356.526, "o", "        elif verbose == 1:\r\n"]
[1356.536, "o", "            print(\"|\", end=\" \")\r\n"]
[1356.546, "o", "        code = sparse_encode(\r\n"]
[1356.556, "o", "            X,\r\n"]
[1356.566, "o", "            dictionary,\r\n"]
[1356.576, "o", "            algorithm=method,\r\n"]
[1356.586, "o", "            alpha=alpha,\r\n"]
[1356.596, "o", "            n_jobs=n_jobs,\r\n"]
[1356.606, "o", "            check_input=False,\r\n"]
[1356.616, "o", "            positive=positive_code,\r\n"]
[1356.626, "o", "            max_iter=method_max_iter,\r\n"]
[1356.636, "o", "            verbose=verbose,\r\n"]
[1356.646, "o", "        )\r\n"]
[1356.656, "o", "        if verbose > 1:\r\n"]
[1356.666, "o", "            dt = time.time() - t0\r\n"]
[1356.676, "o", "            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n"]
[1356.686, "o", "        if return_n_iter:\r\n"]
[1356.696, "o", "            return code, dictionary, ii - iter_offset + 1\r\n"]
[1356.706, "o", "        else:\r\n"]
[1356.716, "o", "            return code, dictionary\r\n"]
[1356.726, "o", "\r\n"]
[1356.736, "o", "    if return_n_iter:\r\n"]
[1356.746, "o", "        return dictionary, ii - iter_offset + 1\r\n"]
[1356.756, "o", "    else:\r\n"]
[1356.766, "o", "        return dictionary\r\n"]
[1356.776, "o", "\r\n"]
[1356.786, "o", "\r\n"]
[1356.796, "o", "@validate_params(\r\n"]
[1356.806, "o", "    {\r\n"]
[1356.816, "o", "        \"X\": [\"array-like\"],\r\n"]
[1356.826, "o", "        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[1356.836, "o", "        \"return_n_iter\": [\"boolean\"],\r\n"]
[1356.846, "o", "        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1356.856, "o", "    },\r\n"]
[1356.866, "o", "    prefer_skip_nested_validation=False,\r\n"]
[1356.876, "o", ")\r\n"]
[1356.886, "o", "def dict_learning(\r\n"]
[1356.896, "o", "    X,\r\n"]
[1356.906, "o", "    n_components,\r\n"]
[1356.916, "o", "    *,\r\n"]
[1356.926, "o", "    alpha,\r\n"]
[1356.936, "o", "    max_iter=100,\r\n"]
[1356.946, "o", "    tol=1e-8,\r\n"]
[1356.956, "o", "    method=\"lars\",\r\n"]
[1356.966, "o", "    n_jobs=None,\r\n"]
[1356.976, "o", "    dict_init=None,\r\n"]
[1356.986, "o", "    code_init=None,\r\n"]
[1356.996, "o", "    callback=None,\r\n"]
[1357.006, "o", "    verbose=False,\r\n"]
[1357.016, "o", "    random_state=None,\r\n"]
[1357.026, "o", "    return_n_iter=False,\r\n"]
[1357.036, "o", "    positive_dict=False,\r\n"]
[1357.046, "o", "    positive_code=False,\r\n"]
[1357.056, "o", "    method_max_iter=1000,\r\n"]
[1357.066, "o", "):\r\n"]
[1357.076, "o", "    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n"]
[1357.086, "o", "\r\n"]
[1357.096, "o", "    Finds the best dictionary and the corresponding sparse code for\r\n"]
[1357.106, "o", "    approximating the data matrix X by solving::\r\n"]
[1357.116, "o", "\r\n"]
[1357.126, "o", "        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[1357.136, "o", "                     (U,V)\r\n"]
[1357.146, "o", "                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n"]
[1357.156, "o", "\r\n"]
[1357.166, "o", "    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n"]
[1357.176, "o", "    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n"]
[1357.186, "o", "    which is the sum of the absolute values of all the entries in the matrix.\r\n"]
[1357.196, "o", "\r\n"]
[1357.206, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[1357.216, "o", "\r\n"]
[1357.226, "o", "    Parameters\r\n"]
[1357.236, "o", "    ----------\r\n"]
[1357.246, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[1357.256, "o", "        Data matrix.\r\n"]
[1357.266, "o", "\r\n"]
[1357.276, "o", "    n_components : int\r\n"]
[1357.286, "o", "        Number of dictionary atoms to extract.\r\n"]
[1357.296, "o", "\r\n"]
[1357.306, "o", "    alpha : int or float\r\n"]
[1357.316, "o", "        Sparsity controlling parameter.\r\n"]
[1357.326, "o", "\r\n"]
[1357.336, "o", "    max_iter : int, default=100\r\n"]
[1357.346, "o", "        Maximum number of iterations to perform.\r\n"]
[1357.356, "o", "\r\n"]
[1357.366, "o", "    tol : float, default=1e-8\r\n"]
[1357.376, "o", "        Tolerance for the stopping condition.\r\n"]
[1357.386, "o", "\r\n"]
[1357.396, "o", "    method : {'lars', 'cd'}, default='lars'\r\n"]
[1357.406, "o", "        The method used:\r\n"]
[1357.416, "o", "\r\n"]
[1357.426, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[1357.436, "o", "           problem (`linear_model.lars_path`);\r\n"]
[1357.446, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[1357.456, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[1357.466, "o", "          the estimated components are sparse.\r\n"]
[1357.476, "o", "\r\n"]
[1357.486, "o", "    n_jobs : int, default=None\r\n"]
[1357.496, "o", "        Number of parallel jobs to run.\r\n"]
[1357.506, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1357.516, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1357.526, "o", "        for more details.\r\n"]
[1357.536, "o", "\r\n"]
[1357.546, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1357.556, "o", "        Initial value for the dictionary for warm restart scenarios. Only used\r\n"]
[1357.566, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[1357.576, "o", "\r\n"]
[1357.586, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1357.596, "o", "        Initial value for the sparse code for warm restart scenarios. Only used\r\n"]
[1357.606, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[1357.616, "o", "\r\n"]
[1357.626, "o", "    callback : callable, default=None\r\n"]
[1357.636, "o", "        Callable that gets invoked every five iterations.\r\n"]
[1357.646, "o", "\r\n"]
[1357.656, "o", "    verbose : bool, default=False\r\n"]
[1357.666, "o", "        To control the verbosity of the procedure.\r\n"]
[1357.676, "o", "\r\n"]
[1357.686, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1357.696, "o", "        Used for randomly initializing the dictionary. Pass an int for\r\n"]
[1357.706, "o", "        reproducible results across multiple function calls.\r\n"]
[1357.716, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1357.726, "o", "\r\n"]
[1357.736, "o", "    return_n_iter : bool, default=False\r\n"]
[1357.746, "o", "        Whether or not to return the number of iterations.\r\n"]
[1357.756, "o", "\r\n"]
[1357.766, "o", "    positive_dict : bool, default=False\r\n"]
[1357.776, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1357.786, "o", "\r\n"]
[1357.796, "o", "        .. versionadded:: 0.20\r\n"]
[1357.806, "o", "\r\n"]
[1357.816, "o", "    positive_code : bool, default=False\r\n"]
[1357.826, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1357.836, "o", "\r\n"]
[1357.846, "o", "        .. versionadded:: 0.20\r\n"]
[1357.856, "o", "\r\n"]
[1357.866, "o", "    method_max_iter : int, default=1000\r\n"]
[1357.876, "o", "        Maximum number of iterations to perform.\r\n"]
[1357.886, "o", "\r\n"]
[1357.896, "o", "        .. versionadded:: 0.22\r\n"]
[1357.906, "o", "\r\n"]
[1357.916, "o", "    Returns\r\n"]
[1357.926, "o", "    -------\r\n"]
[1357.936, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1357.946, "o", "        The sparse code factor in the matrix factorization.\r\n"]
[1357.956, "o", "\r\n"]
[1357.966, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[1357.976, "o", "        The dictionary factor in the matrix factorization.\r\n"]
[1357.986, "o", "\r\n"]
[1357.996, "o", "    errors : array\r\n"]
[1358.006, "o", "        Vector of errors at each iteration.\r\n"]
[1358.016, "o", "\r\n"]
[1358.026, "o", "    n_iter : int\r\n"]
[1358.036, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[1358.046, "o", "        set to True.\r\n"]
[1358.056, "o", "\r\n"]
[1358.066, "o", "    See Also\r\n"]
[1358.076, "o", "    --------\r\n"]
[1358.086, "o", "    dict_learning_online : Solve a dictionary learning matrix factorization\r\n"]
[1358.096, "o", "        problem online.\r\n"]
[1358.106, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1358.116, "o", "    MiniBatchDictionaryLearning : A faster, less accurate version\r\n"]
[1358.126, "o", "        of the dictionary learning algorithm.\r\n"]
[1358.136, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1358.146, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1358.156, "o", "    \"\"\"\r\n"]
[1358.166, "o", "    estimator = DictionaryLearning(\r\n"]
[1358.176, "o", "        n_components=n_components,\r\n"]
[1358.186, "o", "        alpha=alpha,\r\n"]
[1358.196, "o", "        max_iter=max_iter,\r\n"]
[1358.206, "o", "        tol=tol,\r\n"]
[1358.216, "o", "        fit_algorithm=method,\r\n"]
[1358.226, "o", "        n_jobs=n_jobs,\r\n"]
[1358.236, "o", "        dict_init=dict_init,\r\n"]
[1358.246, "o", "        callback=callback,\r\n"]
[1358.256, "o", "        code_init=code_init,\r\n"]
[1358.266, "o", "        verbose=verbose,\r\n"]
[1358.276, "o", "        random_state=random_state,\r\n"]
[1358.286, "o", "        positive_code=positive_code,\r\n"]
[1358.296, "o", "        positive_dict=positive_dict,\r\n"]
[1358.306, "o", "        transform_max_iter=method_max_iter,\r\n"]
[1358.316, "o", "    )\r\n"]
[1358.326, "o", "    code = estimator.fit_transform(X)\r\n"]
[1358.336, "o", "    if return_n_iter:\r\n"]
[1358.346, "o", "        return (\r\n"]
[1358.356, "o", "            code,\r\n"]
[1358.366, "o", "            estimator.components_,\r\n"]
[1358.376, "o", "            estimator.error_,\r\n"]
[1358.386, "o", "            estimator.n_iter_,\r\n"]
[1358.396, "o", "        )\r\n"]
[1358.406, "o", "    return code, estimator.components_, estimator.error_\r\n"]
[1358.416, "o", "\r\n"]
[1358.426, "o", "\r\n"]
[1358.436, "o", "class _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n"]
[1358.446, "o", "    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n"]
[1358.456, "o", "\r\n"]
[1358.466, "o", "    def __init__(\r\n"]
[1358.476, "o", "        self,\r\n"]
[1358.486, "o", "        transform_algorithm,\r\n"]
[1358.496, "o", "        transform_n_nonzero_coefs,\r\n"]
[1358.506, "o", "        transform_alpha,\r\n"]
[1358.516, "o", "        split_sign,\r\n"]
[1358.526, "o", "        n_jobs,\r\n"]
[1358.536, "o", "        positive_code,\r\n"]
[1358.546, "o", "        transform_max_iter,\r\n"]
[1358.556, "o", "    ):\r\n"]
[1358.566, "o", "        self.transform_algorithm = transform_algorithm\r\n"]
[1358.576, "o", "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n"]
[1358.586, "o", "        self.transform_alpha = transform_alpha\r\n"]
[1358.596, "o", "        self.transform_max_iter = transform_max_iter\r\n"]
[1358.606, "o", "        self.split_sign = split_sign\r\n"]
[1358.616, "o", "        self.n_jobs = n_jobs\r\n"]
[1358.626, "o", "        self.positive_code = positive_code\r\n"]
[1358.636, "o", "\r\n"]
[1358.646, "o", "    def _transform(self, X, dictionary):\r\n"]
[1358.656, "o", "        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n"]
[1358.666, "o", "        SparseCoder.\"\"\"\r\n"]
[1358.676, "o", "        X = self._validate_data(X, reset=False)\r\n"]
[1358.686, "o", "\r\n"]
[1358.696, "o", "        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n"]
[1358.706, "o", "            transform_alpha = self.alpha\r\n"]
[1358.716, "o", "        else:\r\n"]
[1358.726, "o", "            transform_alpha = self.transform_alpha\r\n"]
[1358.736, "o", "\r\n"]
[1358.746, "o", "        code = sparse_encode(\r\n"]
[1358.756, "o", "            X,\r\n"]
[1358.766, "o", "            dictionary,\r\n"]
[1358.776, "o", "            algorithm=self.transform_algorithm,\r\n"]
[1358.786, "o", "            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n"]
[1358.796, "o", "            alpha=transform_alpha,\r\n"]
[1358.806, "o", "            max_iter=self.transform_max_iter,\r\n"]
[1358.816, "o", "            n_jobs=self.n_jobs,\r\n"]
[1358.826, "o", "            positive=self.positive_code,\r\n"]
[1358.836, "o", "        )\r\n"]
[1358.846, "o", "\r\n"]
[1358.856, "o", "        if self.split_sign:\r\n"]
[1358.866, "o", "            # feature vector is split into a positive and negative side\r\n"]
[1358.876, "o", "            n_samples, n_features = code.shape\r\n"]
[1358.886, "o", "            split_code = np.empty((n_samples, 2 * n_features))\r\n"]
[1358.896, "o", "            split_code[:, :n_features] = np.maximum(code, 0)\r\n"]
[1358.906, "o", "            split_code[:, n_features:] = -np.minimum(code, 0)\r\n"]
[1358.916, "o", "            code = split_code\r\n"]
[1358.926, "o", "\r\n"]
[1358.936, "o", "        return code\r\n"]
[1358.946, "o", "\r\n"]
[1358.956, "o", "    def transform(self, X):\r\n"]
[1358.966, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[1358.976, "o", "\r\n"]
[1358.986, "o", "        Coding method is determined by the object parameter\r\n"]
[1358.996, "o", "        `transform_algorithm`.\r\n"]
[1359.006, "o", "\r\n"]
[1359.016, "o", "        Parameters\r\n"]
[1359.026, "o", "        ----------\r\n"]
[1359.036, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[1359.046, "o", "            Test data to be transformed, must have the same number of\r\n"]
[1359.056, "o", "            features as the data used to train the model.\r\n"]
[1359.066, "o", "\r\n"]
[1359.076, "o", "        Returns\r\n"]
[1359.086, "o", "        -------\r\n"]
[1359.096, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[1359.106, "o", "            Transformed data.\r\n"]
[1359.116, "o", "        \"\"\"\r\n"]
[1359.126, "o", "        check_is_fitted(self)\r\n"]
[1359.136, "o", "        return self._transform(X, self.components_)\r\n"]
[1359.146, "o", "\r\n"]
[1359.156, "o", "\r\n"]
[1359.166, "o", "class SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n"]
[1359.176, "o", "    \"\"\"Sparse coding.\r\n"]
[1359.186, "o", "\r\n"]
[1359.196, "o", "    Finds a sparse representation of data against a fixed, precomputed\r\n"]
[1359.206, "o", "    dictionary.\r\n"]
[1359.216, "o", "\r\n"]
[1359.226, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[1359.236, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[1359.246, "o", "\r\n"]
[1359.256, "o", "        X ~= code * dictionary\r\n"]
[1359.266, "o", "\r\n"]
[1359.276, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[1359.286, "o", "\r\n"]
[1359.296, "o", "    Parameters\r\n"]
[1359.306, "o", "    ----------\r\n"]
[1359.316, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1359.326, "o", "        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n"]
[1359.336, "o", "        normalized to unit norm.\r\n"]
[1359.346, "o", "\r\n"]
[1359.356, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[1359.366, "o", "            'threshold'}, default='omp'\r\n"]
[1359.376, "o", "        Algorithm used to transform the data:\r\n"]
[1359.386, "o", "\r\n"]
[1359.396, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[1359.406, "o", "          (`linear_model.lars_path`);\r\n"]
[1359.416, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1359.426, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1359.436, "o", "          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n"]
[1359.446, "o", "          the estimated components are sparse;\r\n"]
[1359.456, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1359.466, "o", "          solution;\r\n"]
[1359.476, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[1359.486, "o", "          the projection ``dictionary * X'``.\r\n"]
[1359.496, "o", "\r\n"]
[1359.506, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[1359.516, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1359.526, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[1359.536, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[1359.546, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1359.556, "o", "\r\n"]
[1359.566, "o", "    transform_alpha : float, default=None\r\n"]
[1359.576, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1359.586, "o", "        penalty applied to the L1 norm.\r\n"]
[1359.596, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1359.606, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1359.616, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[1359.626, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[1359.636, "o", "        `n_nonzero_coefs`.\r\n"]
[1359.646, "o", "        If `None`, default to 1.\r\n"]
[1359.656, "o", "\r\n"]
[1359.666, "o", "    split_sign : bool, default=False\r\n"]
[1359.676, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[1359.686, "o", "        its negative part and its positive part. This can improve the\r\n"]
[1359.696, "o", "        performance of downstream classifiers.\r\n"]
[1359.706, "o", "\r\n"]
[1359.716, "o", "    n_jobs : int, default=None\r\n"]
[1359.726, "o", "        Number of parallel jobs to run.\r\n"]
[1359.736, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1359.746, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1359.756, "o", "        for more details.\r\n"]
[1359.766, "o", "\r\n"]
[1359.776, "o", "    positive_code : bool, default=False\r\n"]
[1359.786, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1359.796, "o", "\r\n"]
[1359.806, "o", "        .. versionadded:: 0.20\r\n"]
[1359.816, "o", "\r\n"]
[1359.826, "o", "    transform_max_iter : int, default=1000\r\n"]
[1359.836, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1359.846, "o", "        `lasso_lars`.\r\n"]
[1359.856, "o", "\r\n"]
[1359.866, "o", "        .. versionadded:: 0.22\r\n"]
[1359.876, "o", "\r\n"]
[1359.886, "o", "    Attributes\r\n"]
[1359.896, "o", "    ----------\r\n"]
[1359.906, "o", "    n_components_ : int\r\n"]
[1359.916, "o", "        Number of atoms.\r\n"]
[1359.926, "o", "\r\n"]
[1359.936, "o", "    n_features_in_ : int\r\n"]
[1360.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1360.002, "i", "sed -n '1800,2600p' sklearn/decomposition/_dict_learning.py\r"]
[1360.004, "o", "sed -n '1800,2600p' sklearn/decomposition/_dict_learning.py\r\n"]
[1360.016, "o", "\u001b[?2004l\r\n"]
[1360.026, "o", "        self.fit_transform(X)\r\n"]
[1360.036, "o", "        return self\r\n"]
[1360.046, "o", "\r\n"]
[1360.056, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[1360.066, "o", "    def fit_transform(self, X, y=None):\r\n"]
[1360.076, "o", "        \"\"\"Fit the model from data in X and return the transformed data.\r\n"]
[1360.086, "o", "\r\n"]
[1360.096, "o", "        Parameters\r\n"]
[1360.106, "o", "        ----------\r\n"]
[1360.116, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[1360.126, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1360.136, "o", "            and `n_features` is the number of features.\r\n"]
[1360.146, "o", "\r\n"]
[1360.156, "o", "        y : Ignored\r\n"]
[1360.166, "o", "            Not used, present for API consistency by convention.\r\n"]
[1360.176, "o", "\r\n"]
[1360.186, "o", "        Returns\r\n"]
[1360.196, "o", "        -------\r\n"]
[1360.206, "o", "        V : ndarray of shape (n_samples, n_components)\r\n"]
[1360.216, "o", "            Transformed data.\r\n"]
[1360.226, "o", "        \"\"\"\r\n"]
[1360.236, "o", "        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n"]
[1360.246, "o", "\r\n"]
[1360.256, "o", "        method = \"lasso_\" + self.fit_algorithm\r\n"]
[1360.266, "o", "\r\n"]
[1360.276, "o", "        random_state = check_random_state(self.random_state)\r\n"]
[1360.286, "o", "        X = self._validate_data(X)\r\n"]
[1360.296, "o", "\r\n"]
[1360.306, "o", "        if self.n_components is None:\r\n"]
[1360.316, "o", "            n_components = X.shape[1]\r\n"]
[1360.326, "o", "        else:\r\n"]
[1360.336, "o", "            n_components = self.n_components\r\n"]
[1360.346, "o", "\r\n"]
[1360.356, "o", "        V, U, E, self.n_iter_ = _dict_learning(\r\n"]
[1360.366, "o", "            X,\r\n"]
[1360.376, "o", "            n_components,\r\n"]
[1360.386, "o", "            alpha=self.alpha,\r\n"]
[1360.396, "o", "            tol=self.tol,\r\n"]
[1360.406, "o", "            max_iter=self.max_iter,\r\n"]
[1360.416, "o", "            method=method,\r\n"]
[1360.426, "o", "            method_max_iter=self.transform_max_iter,\r\n"]
[1360.436, "o", "            n_jobs=self.n_jobs,\r\n"]
[1360.446, "o", "            code_init=self.code_init,\r\n"]
[1360.456, "o", "            dict_init=self.dict_init,\r\n"]
[1360.466, "o", "            callback=self.callback,\r\n"]
[1360.476, "o", "            verbose=self.verbose,\r\n"]
[1360.486, "o", "            random_state=random_state,\r\n"]
[1360.496, "o", "            return_n_iter=True,\r\n"]
[1360.506, "o", "            positive_dict=self.positive_dict,\r\n"]
[1360.516, "o", "            positive_code=self.positive_code,\r\n"]
[1360.526, "o", "        )\r\n"]
[1360.536, "o", "        self.components_ = U\r\n"]
[1360.546, "o", "        self.error_ = E\r\n"]
[1360.556, "o", "\r\n"]
[1360.566, "o", "        return V\r\n"]
[1360.576, "o", "\r\n"]
[1360.586, "o", "    @property\r\n"]
[1360.596, "o", "    def _n_features_out(self):\r\n"]
[1360.606, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[1360.616, "o", "        return self.components_.shape[0]\r\n"]
[1360.626, "o", "\r\n"]
[1360.636, "o", "    def _more_tags(self):\r\n"]
[1360.646, "o", "        return {\r\n"]
[1360.656, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[1360.666, "o", "        }\r\n"]
[1360.676, "o", "\r\n"]
[1360.686, "o", "\r\n"]
[1360.696, "o", "class MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[1360.706, "o", "    \"\"\"Mini-batch dictionary learning.\r\n"]
[1360.716, "o", "\r\n"]
[1360.726, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[1360.736, "o", "    encoding the fitted data.\r\n"]
[1360.746, "o", "\r\n"]
[1360.756, "o", "    Solves the optimization problem::\r\n"]
[1360.766, "o", "\r\n"]
[1360.776, "o", "       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[1360.786, "o", "                    (U,V)\r\n"]
[1360.796, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[1360.806, "o", "\r\n"]
[1360.816, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[1360.826, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[1360.836, "o", "    of all the entries in the matrix.\r\n"]
[1360.846, "o", "\r\n"]
[1360.856, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[1360.866, "o", "\r\n"]
[1360.876, "o", "    Parameters\r\n"]
[1360.886, "o", "    ----------\r\n"]
[1360.896, "o", "    n_components : int, default=None\r\n"]
[1360.906, "o", "        Number of dictionary elements to extract.\r\n"]
[1360.916, "o", "\r\n"]
[1360.926, "o", "    alpha : float, default=1\r\n"]
[1360.936, "o", "        Sparsity controlling parameter.\r\n"]
[1360.946, "o", "\r\n"]
[1360.956, "o", "    n_iter : int, default=1000\r\n"]
[1360.966, "o", "        Total number of iterations over data batches to perform.\r\n"]
[1360.976, "o", "\r\n"]
[1360.986, "o", "        .. deprecated:: 1.1\r\n"]
[1360.996, "o", "           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[1361.006, "o", "           ``max_iter`` instead.\r\n"]
[1361.016, "o", "\r\n"]
[1361.026, "o", "    max_iter : int, default=None\r\n"]
[1361.036, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[1361.046, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[1361.056, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[1361.066, "o", "\r\n"]
[1361.076, "o", "        .. versionadded:: 1.1\r\n"]
[1361.086, "o", "\r\n"]
[1361.096, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[1361.106, "o", "        The algorithm used:\r\n"]
[1361.116, "o", "\r\n"]
[1361.126, "o", "        - `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[1361.136, "o", "          problem (`linear_model.lars_path`)\r\n"]
[1361.146, "o", "        - `'cd'`: uses the coordinate descent method to compute the\r\n"]
[1361.156, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[1361.166, "o", "          the estimated components are sparse.\r\n"]
[1361.176, "o", "\r\n"]
[1361.186, "o", "    n_jobs : int, default=None\r\n"]
[1361.196, "o", "        Number of parallel jobs to run.\r\n"]
[1361.206, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1361.216, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1361.226, "o", "        for more details.\r\n"]
[1361.236, "o", "\r\n"]
[1361.246, "o", "    batch_size : int, default=256\r\n"]
[1361.256, "o", "        Number of samples in each mini-batch.\r\n"]
[1361.266, "o", "\r\n"]
[1361.276, "o", "        .. versionchanged:: 1.3\r\n"]
[1361.286, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[1361.296, "o", "\r\n"]
[1361.306, "o", "    shuffle : bool, default=True\r\n"]
[1361.316, "o", "        Whether to shuffle the samples before forming batches.\r\n"]
[1361.326, "o", "\r\n"]
[1361.336, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1361.346, "o", "        Initial value of the dictionary for warm restart scenarios.\r\n"]
[1361.356, "o", "\r\n"]
[1361.366, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[1361.376, "o", "            'threshold'}, default='omp'\r\n"]
[1361.386, "o", "        Algorithm used to transform the data:\r\n"]
[1361.396, "o", "\r\n"]
[1361.406, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[1361.416, "o", "          (`linear_model.lars_path`);\r\n"]
[1361.426, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[1361.436, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1361.446, "o", "          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n"]
[1361.456, "o", "          if the estimated components are sparse.\r\n"]
[1361.466, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1361.476, "o", "          solution.\r\n"]
[1361.486, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[1361.496, "o", "          the projection ``dictionary * X'``.\r\n"]
[1361.506, "o", "\r\n"]
[1361.516, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[1361.526, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1361.536, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[1361.546, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[1361.556, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1361.566, "o", "\r\n"]
[1361.576, "o", "    transform_alpha : float, default=None\r\n"]
[1361.586, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1361.596, "o", "        penalty applied to the L1 norm.\r\n"]
[1361.606, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1361.616, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1361.626, "o", "        If `None`, defaults to `alpha`.\r\n"]
[1361.636, "o", "\r\n"]
[1361.646, "o", "        .. versionchanged:: 1.2\r\n"]
[1361.656, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[1361.666, "o", "\r\n"]
[1361.676, "o", "    verbose : bool or int, default=False\r\n"]
[1361.686, "o", "        To control the verbosity of the procedure.\r\n"]
[1361.696, "o", "\r\n"]
[1361.706, "o", "    split_sign : bool, default=False\r\n"]
[1361.716, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[1361.726, "o", "        its negative part and its positive part. This can improve the\r\n"]
[1361.736, "o", "        performance of downstream classifiers.\r\n"]
[1361.746, "o", "\r\n"]
[1361.756, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1361.766, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[1361.776, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[1361.786, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[1361.796, "o", "        results across multiple function calls.\r\n"]
[1361.806, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1361.816, "o", "\r\n"]
[1361.826, "o", "    positive_code : bool, default=False\r\n"]
[1361.836, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1361.846, "o", "\r\n"]
[1361.856, "o", "        .. versionadded:: 0.20\r\n"]
[1361.866, "o", "\r\n"]
[1361.876, "o", "    positive_dict : bool, default=False\r\n"]
[1361.886, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1361.896, "o", "\r\n"]
[1361.906, "o", "        .. versionadded:: 0.20\r\n"]
[1361.916, "o", "\r\n"]
[1361.926, "o", "    transform_max_iter : int, default=1000\r\n"]
[1361.936, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1361.946, "o", "        `'lasso_lars'`.\r\n"]
[1361.956, "o", "\r\n"]
[1361.966, "o", "        .. versionadded:: 0.22\r\n"]
[1361.976, "o", "\r\n"]
[1361.986, "o", "    callback : callable, default=None\r\n"]
[1361.996, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[1362.006, "o", "\r\n"]
[1362.016, "o", "        .. versionadded:: 1.1\r\n"]
[1362.026, "o", "\r\n"]
[1362.036, "o", "    tol : float, default=1e-3\r\n"]
[1362.046, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[1362.056, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[1362.066, "o", "\r\n"]
[1362.076, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[1362.086, "o", "        `tol` to 0.0.\r\n"]
[1362.096, "o", "\r\n"]
[1362.106, "o", "        .. versionadded:: 1.1\r\n"]
[1362.116, "o", "\r\n"]
[1362.126, "o", "    max_no_improvement : int, default=10\r\n"]
[1362.136, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[1362.146, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[1362.156, "o", "        `max_iter` is not None.\r\n"]
[1362.166, "o", "\r\n"]
[1362.176, "o", "        To disable convergence detection based on cost function, set\r\n"]
[1362.186, "o", "        `max_no_improvement` to None.\r\n"]
[1362.196, "o", "\r\n"]
[1362.206, "o", "        .. versionadded:: 1.1\r\n"]
[1362.216, "o", "\r\n"]
[1362.226, "o", "    Attributes\r\n"]
[1362.236, "o", "    ----------\r\n"]
[1362.246, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[1362.256, "o", "        Components extracted from the data.\r\n"]
[1362.266, "o", "\r\n"]
[1362.276, "o", "    n_features_in_ : int\r\n"]
[1362.286, "o", "        Number of features seen during :term:`fit`.\r\n"]
[1362.296, "o", "\r\n"]
[1362.306, "o", "        .. versionadded:: 0.24\r\n"]
[1362.316, "o", "\r\n"]
[1362.326, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[1362.336, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[1362.346, "o", "        has feature names that are all strings.\r\n"]
[1362.356, "o", "\r\n"]
[1362.366, "o", "        .. versionadded:: 1.0\r\n"]
[1362.376, "o", "\r\n"]
[1362.386, "o", "    n_iter_ : int\r\n"]
[1362.396, "o", "        Number of iterations over the full dataset.\r\n"]
[1362.406, "o", "\r\n"]
[1362.416, "o", "    n_steps_ : int\r\n"]
[1362.426, "o", "        Number of mini-batches processed.\r\n"]
[1362.436, "o", "\r\n"]
[1362.446, "o", "        .. versionadded:: 1.1\r\n"]
[1362.456, "o", "\r\n"]
[1362.466, "o", "    See Also\r\n"]
[1362.476, "o", "    --------\r\n"]
[1362.486, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1362.496, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1362.506, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[1362.516, "o", "        precomputed dictionary.\r\n"]
[1362.526, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1362.536, "o", "\r\n"]
[1362.546, "o", "    References\r\n"]
[1362.556, "o", "    ----------\r\n"]
[1362.566, "o", "\r\n"]
[1362.576, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[1362.586, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[1362.596, "o", "\r\n"]
[1362.606, "o", "    Examples\r\n"]
[1362.616, "o", "    --------\r\n"]
[1362.626, "o", "    >>> import numpy as np\r\n"]
[1362.636, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[1362.646, "o", "    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n"]
[1362.656, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[1362.666, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[1362.676, "o", "    ...     random_state=42)\r\n"]
[1362.686, "o", "    >>> dict_learner = MiniBatchDictionaryLearning(\r\n"]
[1362.696, "o", "    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n"]
[1362.706, "o", "    ...     transform_alpha=0.1, random_state=42)\r\n"]
[1362.716, "o", "    >>> X_transformed = dict_learner.fit_transform(X)\r\n"]
[1362.726, "o", "\r\n"]
[1362.736, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[1362.746, "o", "\r\n"]
[1362.756, "o", "    >>> np.mean(X_transformed == 0) < 0.5\r\n"]
[1362.766, "o", "    True\r\n"]
[1362.776, "o", "\r\n"]
[1362.786, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[1362.796, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[1362.806, "o", "    the original signal:\r\n"]
[1362.816, "o", "\r\n"]
[1362.826, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[1362.836, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[1362.846, "o", "    0.057...\r\n"]
[1362.856, "o", "    \"\"\"\r\n"]
[1362.866, "o", "\r\n"]
[1362.876, "o", "    _parameter_constraints: dict = {\r\n"]
[1362.886, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1362.896, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1362.906, "o", "        \"n_iter\": [\r\n"]
[1362.916, "o", "            Interval(Integral, 0, None, closed=\"left\"),\r\n"]
[1362.926, "o", "            Hidden(StrOptions({\"deprecated\"})),\r\n"]
[1362.936, "o", "        ],\r\n"]
[1362.946, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n"]
[1362.956, "o", "        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n"]
[1362.966, "o", "        \"n_jobs\": [None, Integral],\r\n"]
[1362.976, "o", "        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n"]
[1362.986, "o", "        \"shuffle\": [\"boolean\"],\r\n"]
[1362.996, "o", "        \"dict_init\": [None, np.ndarray],\r\n"]
[1363.006, "o", "        \"transform_algorithm\": [\r\n"]
[1363.016, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[1363.026, "o", "        ],\r\n"]
[1363.036, "o", "        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1363.046, "o", "        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[1363.056, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[1363.066, "o", "        \"split_sign\": [\"boolean\"],\r\n"]
[1363.076, "o", "        \"random_state\": [\"random_state\"],\r\n"]
[1363.086, "o", "        \"positive_code\": [\"boolean\"],\r\n"]
[1363.096, "o", "        \"positive_dict\": [\"boolean\"],\r\n"]
[1363.106, "o", "        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1363.116, "o", "        \"callback\": [None, callable],\r\n"]
[1363.126, "o", "        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1363.136, "o", "        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n"]
[1363.146, "o", "    }\r\n"]
[1363.156, "o", "\r\n"]
[1363.166, "o", "    def __init__(\r\n"]
[1363.176, "o", "        self,\r\n"]
[1363.186, "o", "        n_components=None,\r\n"]
[1363.196, "o", "        *,\r\n"]
[1363.206, "o", "        alpha=1,\r\n"]
[1363.216, "o", "        n_iter=\"deprecated\",\r\n"]
[1363.226, "o", "        max_iter=None,\r\n"]
[1363.236, "o", "        fit_algorithm=\"lars\",\r\n"]
[1363.246, "o", "        n_jobs=None,\r\n"]
[1363.256, "o", "        batch_size=256,\r\n"]
[1363.266, "o", "        shuffle=True,\r\n"]
[1363.276, "o", "        dict_init=None,\r\n"]
[1363.286, "o", "        transform_algorithm=\"omp\",\r\n"]
[1363.296, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[1363.306, "o", "        transform_alpha=None,\r\n"]
[1363.316, "o", "        verbose=False,\r\n"]
[1363.326, "o", "        split_sign=False,\r\n"]
[1363.336, "o", "        random_state=None,\r\n"]
[1363.346, "o", "        positive_code=False,\r\n"]
[1363.356, "o", "        positive_dict=False,\r\n"]
[1363.366, "o", "        transform_max_iter=1000,\r\n"]
[1363.376, "o", "        callback=None,\r\n"]
[1363.386, "o", "        tol=1e-3,\r\n"]
[1363.396, "o", "        max_no_improvement=10,\r\n"]
[1363.406, "o", "    ):\r\n"]
[1363.416, "o", "        super().__init__(\r\n"]
[1363.426, "o", "            transform_algorithm,\r\n"]
[1363.436, "o", "            transform_n_nonzero_coefs,\r\n"]
[1363.446, "o", "            transform_alpha,\r\n"]
[1363.456, "o", "            split_sign,\r\n"]
[1363.466, "o", "            n_jobs,\r\n"]
[1363.476, "o", "            positive_code,\r\n"]
[1363.486, "o", "            transform_max_iter,\r\n"]
[1363.496, "o", "        )\r\n"]
[1363.506, "o", "        self.n_components = n_components\r\n"]
[1363.516, "o", "        self.alpha = alpha\r\n"]
[1363.526, "o", "        self.n_iter = n_iter\r\n"]
[1363.536, "o", "        self.max_iter = max_iter\r\n"]
[1363.546, "o", "        self.fit_algorithm = fit_algorithm\r\n"]
[1363.556, "o", "        self.dict_init = dict_init\r\n"]
[1363.566, "o", "        self.verbose = verbose\r\n"]
[1363.576, "o", "        self.shuffle = shuffle\r\n"]
[1363.586, "o", "        self.batch_size = batch_size\r\n"]
[1363.596, "o", "        self.split_sign = split_sign\r\n"]
[1363.606, "o", "        self.random_state = random_state\r\n"]
[1363.616, "o", "        self.positive_dict = positive_dict\r\n"]
[1363.626, "o", "        self.callback = callback\r\n"]
[1363.636, "o", "        self.max_no_improvement = max_no_improvement\r\n"]
[1363.646, "o", "        self.tol = tol\r\n"]
[1363.656, "o", "\r\n"]
[1363.666, "o", "    def _check_params(self, X):\r\n"]
[1363.676, "o", "        # n_components\r\n"]
[1363.686, "o", "        self._n_components = self.n_components\r\n"]
[1363.696, "o", "        if self._n_components is None:\r\n"]
[1363.706, "o", "            self._n_components = X.shape[1]\r\n"]
[1363.716, "o", "\r\n"]
[1363.726, "o", "        # fit_algorithm\r\n"]
[1363.736, "o", "        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n"]
[1363.746, "o", "        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n"]
[1363.756, "o", "\r\n"]
[1363.766, "o", "        # batch_size\r\n"]
[1363.776, "o", "        self._batch_size = min(self.batch_size, X.shape[0])\r\n"]
[1363.786, "o", "\r\n"]
[1363.796, "o", "    def _initialize_dict(self, X, random_state):\r\n"]
[1363.806, "o", "        \"\"\"Initialization of the dictionary.\"\"\"\r\n"]
[1363.816, "o", "        if self.dict_init is not None:\r\n"]
[1363.826, "o", "            dictionary = self.dict_init\r\n"]
[1363.836, "o", "        else:\r\n"]
[1363.846, "o", "            # Init V with SVD of X\r\n"]
[1363.856, "o", "            _, S, dictionary = randomized_svd(\r\n"]
[1363.866, "o", "                X, self._n_components, random_state=random_state\r\n"]
[1363.876, "o", "            )\r\n"]
[1363.886, "o", "            dictionary = S[:, np.newaxis] * dictionary\r\n"]
[1363.896, "o", "\r\n"]
[1363.906, "o", "        if self._n_components <= len(dictionary):\r\n"]
[1363.916, "o", "            dictionary = dictionary[: self._n_components, :]\r\n"]
[1363.926, "o", "        else:\r\n"]
[1363.936, "o", "            dictionary = np.concatenate(\r\n"]
[1363.946, "o", "                (\r\n"]
[1363.956, "o", "                    dictionary,\r\n"]
[1363.966, "o", "                    np.zeros(\r\n"]
[1363.976, "o", "                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n"]
[1363.986, "o", "                        dtype=dictionary.dtype,\r\n"]
[1363.996, "o", "                    ),\r\n"]
[1364.006, "o", "                )\r\n"]
[1364.016, "o", "            )\r\n"]
[1364.026, "o", "\r\n"]
[1364.036, "o", "        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n"]
[1364.046, "o", "        dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[1364.056, "o", "\r\n"]
[1364.066, "o", "        return dictionary\r\n"]
[1364.076, "o", "\r\n"]
[1364.086, "o", "    def _update_inner_stats(self, X, code, batch_size, step):\r\n"]
[1364.096, "o", "        \"\"\"Update the inner stats inplace.\"\"\"\r\n"]
[1364.106, "o", "        if step < batch_size - 1:\r\n"]
[1364.116, "o", "            theta = (step + 1) * batch_size\r\n"]
[1364.126, "o", "        else:\r\n"]
[1364.136, "o", "            theta = batch_size**2 + step + 1 - batch_size\r\n"]
[1364.146, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[1364.156, "o", "\r\n"]
[1364.166, "o", "        self._A *= beta\r\n"]
[1364.176, "o", "        self._A += code.T @ code / batch_size\r\n"]
[1364.186, "o", "        self._B *= beta\r\n"]
[1364.196, "o", "        self._B += X.T @ code / batch_size\r\n"]
[1364.206, "o", "\r\n"]
[1364.216, "o", "    def _minibatch_step(self, X, dictionary, random_state, step):\r\n"]
[1364.226, "o", "        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n"]
[1364.236, "o", "        batch_size = X.shape[0]\r\n"]
[1364.246, "o", "\r\n"]
[1364.256, "o", "        # Compute code for this batch\r\n"]
[1364.266, "o", "        code = _sparse_encode(\r\n"]
[1364.276, "o", "            X,\r\n"]
[1364.286, "o", "            dictionary,\r\n"]
[1364.296, "o", "            algorithm=self._fit_algorithm,\r\n"]
[1364.306, "o", "            alpha=self.alpha,\r\n"]
[1364.316, "o", "            n_jobs=self.n_jobs,\r\n"]
[1364.326, "o", "            positive=self.positive_code,\r\n"]
[1364.336, "o", "            max_iter=self.transform_max_iter,\r\n"]
[1364.346, "o", "            verbose=self.verbose,\r\n"]
[1364.356, "o", "        )\r\n"]
[1364.366, "o", "\r\n"]
[1364.376, "o", "        batch_cost = (\r\n"]
[1364.386, "o", "            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n"]
[1364.396, "o", "            + self.alpha * np.sum(np.abs(code))\r\n"]
[1364.406, "o", "        ) / batch_size\r\n"]
[1364.416, "o", "\r\n"]
[1364.426, "o", "        # Update inner stats\r\n"]
[1364.436, "o", "        self._update_inner_stats(X, code, batch_size, step)\r\n"]
[1364.446, "o", "\r\n"]
[1364.456, "o", "        # Update dictionary\r\n"]
[1364.466, "o", "        _update_dict(\r\n"]
[1364.476, "o", "            dictionary,\r\n"]
[1364.486, "o", "            X,\r\n"]
[1364.496, "o", "            code,\r\n"]
[1364.506, "o", "            self._A,\r\n"]
[1364.516, "o", "            self._B,\r\n"]
[1364.526, "o", "            verbose=self.verbose,\r\n"]
[1364.536, "o", "            random_state=random_state,\r\n"]
[1364.546, "o", "            positive=self.positive_dict,\r\n"]
[1364.556, "o", "        )\r\n"]
[1364.566, "o", "\r\n"]
[1364.576, "o", "        return batch_cost\r\n"]
[1364.586, "o", "\r\n"]
[1364.596, "o", "    def _check_convergence(\r\n"]
[1364.606, "o", "        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n"]
[1364.616, "o", "    ):\r\n"]
[1364.626, "o", "        \"\"\"Helper function to encapsulate the early stopping logic.\r\n"]
[1364.636, "o", "\r\n"]
[1364.646, "o", "        Early stopping is based on two factors:\r\n"]
[1364.656, "o", "        - A small change of the dictionary between two minibatch updates. This is\r\n"]
[1364.666, "o", "          controlled by the tol parameter.\r\n"]
[1364.676, "o", "        - No more improvement on a smoothed estimate of the objective function for a\r\n"]
[1364.686, "o", "          a certain number of consecutive minibatch updates. This is controlled by\r\n"]
[1364.696, "o", "          the max_no_improvement parameter.\r\n"]
[1364.706, "o", "        \"\"\"\r\n"]
[1364.716, "o", "        batch_size = X.shape[0]\r\n"]
[1364.726, "o", "\r\n"]
[1364.736, "o", "        # counts steps starting from 1 for user friendly verbose mode.\r\n"]
[1364.746, "o", "        step = step + 1\r\n"]
[1364.756, "o", "\r\n"]
[1364.766, "o", "        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n"]
[1364.776, "o", "        # too bad value\r\n"]
[1364.786, "o", "        if step <= min(100, n_samples / batch_size):\r\n"]
[1364.796, "o", "            if self.verbose:\r\n"]
[1364.806, "o", "                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n"]
[1364.816, "o", "            return False\r\n"]
[1364.826, "o", "\r\n"]
[1364.836, "o", "        # Compute an Exponentially Weighted Average of the cost function to\r\n"]
[1364.846, "o", "        # monitor the convergence while discarding minibatch-local stochastic\r\n"]
[1364.856, "o", "        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n"]
[1364.866, "o", "        if self._ewa_cost is None:\r\n"]
[1364.876, "o", "            self._ewa_cost = batch_cost\r\n"]
[1364.886, "o", "        else:\r\n"]
[1364.896, "o", "            alpha = batch_size / (n_samples + 1)\r\n"]
[1364.906, "o", "            alpha = min(alpha, 1)\r\n"]
[1364.916, "o", "            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n"]
[1364.926, "o", "\r\n"]
[1364.936, "o", "        if self.verbose:\r\n"]
[1365.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1365.002, "i", "grep -n \"_minibatch_step\" sklearn/decomposition/_dict_learning.py || true\r"]
[1365.004, "o", "grep -n \"_minibatch_step\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[1365.9948, "o", "\u001b[?2004l\r\n"]
[1366.9836, "o", "\u001b[32m\u001b[K2219\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    def \u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(self, X, dictionary, random_state, step):\r\n"]
[1367.9724, "o", "\u001b[32m\u001b[K2399\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                batch_cost = self.\u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(\r\n"]
[1368.9612, "o", "\u001b[32m\u001b[K2425\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                self.\u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(X_train[batch], dictionary, self._random_state, i)\r\n"]
[1370.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1370.002, "i", "grep -n \"_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.py || true\r"]
[1370.004, "o", "grep -n \"_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[1372.478, "o", "\u001b[?2004l\r\n"]
[1375.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1375.002, "i", "grep -n \"def transform\\(|def fit\\(|def partial_fit\\(\" sklearn/decomposition/_dict_learning.py\r"]
[1375.004, "o", "grep -n \"def transform\\(|def fit\\(|def partial_fit\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1377.478, "o", "\u001b[?2004l\r\n"]
[1380.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1380.002, "i", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r"]
[1380.004, "o", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r\n"]
[1380.016, "o", "\u001b[?2004l\r\n"]
[1380.026, "o", "\"\"\" Dictionary learning.\r\n"]
[1380.036, "o", "\"\"\"\r\n"]
[1380.046, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[1380.056, "o", "# License: BSD 3 clause\r\n"]
[1380.066, "o", "\r\n"]
[1380.076, "o", "import itertools\r\n"]
[1380.086, "o", "import sys\r\n"]
[1380.096, "o", "import time\r\n"]
[1380.106, "o", "import warnings\r\n"]
[1380.116, "o", "from math import ceil\r\n"]
[1380.126, "o", "from numbers import Integral, Real\r\n"]
[1380.136, "o", "\r\n"]
[1380.146, "o", "import numpy as np\r\n"]
[1380.156, "o", "from joblib import effective_n_jobs\r\n"]
[1380.166, "o", "from scipy import linalg\r\n"]
[1380.176, "o", "\r\n"]
[1380.186, "o", "from ..base import (\r\n"]
[1380.196, "o", "    BaseEstimator,\r\n"]
[1380.206, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[1380.216, "o", "    TransformerMixin,\r\n"]
[1380.226, "o", "    _fit_context,\r\n"]
[1380.236, "o", ")\r\n"]
[1380.246, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[1380.256, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[1380.266, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[1380.276, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[1380.286, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[1380.296, "o", "from ..utils.validation import check_is_fitted\r\n"]
[1380.306, "o", "\r\n"]
[1380.316, "o", "\r\n"]
[1380.326, "o", "def _check_positive_coding(method, positive):\r\n"]
[1380.336, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[1380.346, "o", "        raise ValueError(\r\n"]
[1380.356, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[1380.366, "o", "        )\r\n"]
[1380.376, "o", "\r\n"]
[1380.386, "o", "\r\n"]
[1380.396, "o", "def _sparse_encode_precomputed(\r\n"]
[1380.406, "o", "    X,\r\n"]
[1380.416, "o", "    dictionary,\r\n"]
[1380.426, "o", "    *,\r\n"]
[1380.436, "o", "    gram=None,\r\n"]
[1380.446, "o", "    cov=None,\r\n"]
[1380.456, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1380.466, "o", "    regularization=None,\r\n"]
[1380.476, "o", "    copy_cov=True,\r\n"]
[1380.486, "o", "    init=None,\r\n"]
[1380.496, "o", "    max_iter=1000,\r\n"]
[1380.506, "o", "    verbose=0,\r\n"]
[1380.516, "o", "    positive=False,\r\n"]
[1380.526, "o", "):\r\n"]
[1380.536, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[1380.546, "o", "\r\n"]
[1380.556, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[1380.566, "o", "\r\n"]
[1380.576, "o", "    Parameters\r\n"]
[1380.586, "o", "    ----------\r\n"]
[1380.596, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[1380.606, "o", "        Data matrix.\r\n"]
[1380.616, "o", "\r\n"]
[1380.626, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1380.636, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1380.646, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[1380.656, "o", "\r\n"]
[1380.666, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[1380.676, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[1380.686, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[1380.696, "o", "\r\n"]
[1380.706, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[1380.716, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[1380.726, "o", "\r\n"]
[1380.736, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1380.746, "o", "            default='lasso_lars'\r\n"]
[1380.756, "o", "        The algorithm used:\r\n"]
[1380.766, "o", "\r\n"]
[1380.776, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[1380.786, "o", "          (`linear_model.lars_path`);\r\n"]
[1380.796, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1380.806, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1380.816, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1380.826, "o", "          the estimated components are sparse;\r\n"]
[1380.836, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1380.846, "o", "          solution;\r\n"]
[1380.856, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1380.866, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[1380.876, "o", "\r\n"]
[1380.886, "o", "    regularization : int or float, default=None\r\n"]
[1380.896, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[1380.906, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[1380.916, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[1380.926, "o", "\r\n"]
[1380.936, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1380.946, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[1380.956, "o", "        `algorithm='lasso_cd'`.\r\n"]
[1380.966, "o", "\r\n"]
[1380.976, "o", "    max_iter : int, default=1000\r\n"]
[1380.986, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1380.996, "o", "        `'lasso_lars'`.\r\n"]
[1381.006, "o", "\r\n"]
[1381.016, "o", "    copy_cov : bool, default=True\r\n"]
[1381.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1381.036, "o", "        be overwritten.\r\n"]
[1381.046, "o", "\r\n"]
[1381.056, "o", "    verbose : int, default=0\r\n"]
[1381.066, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[1381.076, "o", "\r\n"]
[1381.086, "o", "    positive: bool, default=False\r\n"]
[1381.096, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[1381.106, "o", "\r\n"]
[1381.116, "o", "        .. versionadded:: 0.20\r\n"]
[1381.126, "o", "\r\n"]
[1381.136, "o", "    Returns\r\n"]
[1381.146, "o", "    -------\r\n"]
[1381.156, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[1381.166, "o", "        The sparse codes.\r\n"]
[1381.176, "o", "    \"\"\"\r\n"]
[1381.186, "o", "    n_samples, n_features = X.shape\r\n"]
[1381.196, "o", "    n_components = dictionary.shape[0]\r\n"]
[1381.206, "o", "\r\n"]
[1381.216, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[1381.226, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1381.236, "o", "        try:\r\n"]
[1381.246, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1381.256, "o", "\r\n"]
[1381.266, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1381.276, "o", "            # corrects the verbosity level.\r\n"]
[1381.286, "o", "            lasso_lars = LassoLars(\r\n"]
[1381.296, "o", "                alpha=alpha,\r\n"]
[1381.306, "o", "                fit_intercept=False,\r\n"]
[1381.316, "o", "                verbose=verbose,\r\n"]
[1381.326, "o", "                precompute=gram,\r\n"]
[1381.336, "o", "                fit_path=False,\r\n"]
[1381.346, "o", "                positive=positive,\r\n"]
[1381.356, "o", "                max_iter=max_iter,\r\n"]
[1381.366, "o", "            )\r\n"]
[1381.376, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1381.386, "o", "            new_code = lasso_lars.coef_\r\n"]
[1381.396, "o", "        finally:\r\n"]
[1381.406, "o", "            np.seterr(**err_mgt)\r\n"]
[1381.416, "o", "\r\n"]
[1381.426, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[1381.436, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1381.446, "o", "\r\n"]
[1381.456, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[1381.466, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[1381.476, "o", "        # argument that we could pass in from Lasso.\r\n"]
[1381.486, "o", "        clf = Lasso(\r\n"]
[1381.496, "o", "            alpha=alpha,\r\n"]
[1381.506, "o", "            fit_intercept=False,\r\n"]
[1381.516, "o", "            precompute=gram,\r\n"]
[1381.526, "o", "            max_iter=max_iter,\r\n"]
[1381.536, "o", "            warm_start=True,\r\n"]
[1381.546, "o", "            positive=positive,\r\n"]
[1381.556, "o", "        )\r\n"]
[1381.566, "o", "\r\n"]
[1381.576, "o", "        if init is not None:\r\n"]
[1381.586, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[1381.596, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[1381.606, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[1381.616, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[1381.626, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[1381.636, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[1381.646, "o", "                init = np.array(init)\r\n"]
[1381.656, "o", "            clf.coef_ = init\r\n"]
[1381.666, "o", "\r\n"]
[1381.676, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[1381.686, "o", "        new_code = clf.coef_\r\n"]
[1381.696, "o", "\r\n"]
[1381.706, "o", "    elif algorithm == \"lars\":\r\n"]
[1381.716, "o", "        try:\r\n"]
[1381.726, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1381.736, "o", "\r\n"]
[1381.746, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1381.756, "o", "            # corrects the verbosity level.\r\n"]
[1381.766, "o", "            lars = Lars(\r\n"]
[1381.776, "o", "                fit_intercept=False,\r\n"]
[1381.786, "o", "                verbose=verbose,\r\n"]
[1381.796, "o", "                precompute=gram,\r\n"]
[1381.806, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[1381.816, "o", "                fit_path=False,\r\n"]
[1381.826, "o", "            )\r\n"]
[1381.836, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1381.846, "o", "            new_code = lars.coef_\r\n"]
[1381.856, "o", "        finally:\r\n"]
[1381.866, "o", "            np.seterr(**err_mgt)\r\n"]
[1381.876, "o", "\r\n"]
[1381.886, "o", "    elif algorithm == \"threshold\":\r\n"]
[1381.896, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[1381.906, "o", "        if positive:\r\n"]
[1381.916, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[1381.926, "o", "\r\n"]
[1381.936, "o", "    elif algorithm == \"omp\":\r\n"]
[1381.946, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[1381.956, "o", "            Gram=gram,\r\n"]
[1381.966, "o", "            Xy=cov,\r\n"]
[1381.976, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[1381.986, "o", "            tol=None,\r\n"]
[1381.996, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[1382.006, "o", "            copy_Xy=copy_cov,\r\n"]
[1382.016, "o", "        ).T\r\n"]
[1382.026, "o", "\r\n"]
[1382.036, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[1382.046, "o", "\r\n"]
[1382.056, "o", "\r\n"]
[1382.066, "o", "@validate_params(\r\n"]
[1382.076, "o", "    {\r\n"]
[1382.086, "o", "        \"X\": [\"array-like\"],\r\n"]
[1382.096, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[1382.106, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[1382.116, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[1382.126, "o", "        \"algorithm\": [\r\n"]
[1382.136, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[1382.146, "o", "        ],\r\n"]
[1382.156, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1382.166, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[1382.176, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[1382.186, "o", "        \"init\": [\"array-like\", None],\r\n"]
[1382.196, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1382.206, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[1382.216, "o", "        \"check_input\": [\"boolean\"],\r\n"]
[1382.226, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[1382.236, "o", "        \"positive\": [\"boolean\"],\r\n"]
[1382.246, "o", "    },\r\n"]
[1382.256, "o", "    prefer_skip_nested_validation=True,\r\n"]
[1382.266, "o", ")\r\n"]
[1382.276, "o", "# XXX : could be moved to the linear_model module\r\n"]
[1382.286, "o", "def sparse_encode(\r\n"]
[1382.296, "o", "    X,\r\n"]
[1382.306, "o", "    dictionary,\r\n"]
[1382.316, "o", "    *,\r\n"]
[1382.326, "o", "    gram=None,\r\n"]
[1382.336, "o", "    cov=None,\r\n"]
[1382.346, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1382.356, "o", "    n_nonzero_coefs=None,\r\n"]
[1382.366, "o", "    alpha=None,\r\n"]
[1382.376, "o", "    copy_cov=True,\r\n"]
[1382.386, "o", "    init=None,\r\n"]
[1382.396, "o", "    max_iter=1000,\r\n"]
[1382.406, "o", "    n_jobs=None,\r\n"]
[1382.416, "o", "    check_input=True,\r\n"]
[1382.426, "o", "    verbose=0,\r\n"]
[1382.436, "o", "    positive=False,\r\n"]
[1382.446, "o", "):\r\n"]
[1382.456, "o", "    \"\"\"Sparse coding.\r\n"]
[1382.466, "o", "\r\n"]
[1382.476, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[1382.486, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[1382.496, "o", "\r\n"]
[1382.506, "o", "        X ~= code * dictionary\r\n"]
[1382.516, "o", "\r\n"]
[1382.526, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[1382.536, "o", "\r\n"]
[1382.546, "o", "    Parameters\r\n"]
[1382.556, "o", "    ----------\r\n"]
[1382.566, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[1382.576, "o", "        Data matrix.\r\n"]
[1382.586, "o", "\r\n"]
[1382.596, "o", "    dictionary : array-like of shape (n_components, n_features)\r\n"]
[1382.606, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1382.616, "o", "        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[1382.626, "o", "        output.\r\n"]
[1382.636, "o", "\r\n"]
[1382.646, "o", "    gram : array-like of shape (n_components, n_components), default=None\r\n"]
[1382.656, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n"]
[1382.666, "o", "\r\n"]
[1382.676, "o", "    cov : array-like of shape (n_components, n_samples), default=None\r\n"]
[1382.686, "o", "        Precomputed covariance, `dictionary' * X`.\r\n"]
[1382.696, "o", "\r\n"]
[1382.706, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1382.716, "o", "            default='lasso_lars'\r\n"]
[1382.726, "o", "        The algorithm used:\r\n"]
[1382.736, "o", "\r\n"]
[1382.746, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[1382.756, "o", "          (`linear_model.lars_path`);\r\n"]
[1382.766, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1382.776, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1382.786, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1382.796, "o", "          the estimated components are sparse;\r\n"]
[1382.806, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1382.816, "o", "          solution;\r\n"]
[1382.826, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1382.836, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[1382.846, "o", "\r\n"]
[1382.856, "o", "    n_nonzero_coefs : int, default=None\r\n"]
[1382.866, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1382.876, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[1382.886, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[1382.896, "o", "        `n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1382.906, "o", "\r\n"]
[1382.916, "o", "    alpha : float, default=None\r\n"]
[1382.926, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1382.936, "o", "        penalty applied to the L1 norm.\r\n"]
[1382.946, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1382.956, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1382.966, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[1382.976, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[1382.986, "o", "        `n_nonzero_coefs`.\r\n"]
[1382.996, "o", "        If `None`, default to 1.\r\n"]
[1383.006, "o", "\r\n"]
[1383.016, "o", "    copy_cov : bool, default=True\r\n"]
[1383.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1383.036, "o", "        be overwritten.\r\n"]
[1383.046, "o", "\r\n"]
[1383.056, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1383.066, "o", "        Initialization value of the sparse codes. Only used if\r\n"]
[1383.076, "o", "        `algorithm='lasso_cd'`.\r\n"]
[1383.086, "o", "\r\n"]
[1383.096, "o", "    max_iter : int, default=1000\r\n"]
[1383.106, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1383.116, "o", "        `'lasso_lars'`.\r\n"]
[1383.126, "o", "\r\n"]
[1383.136, "o", "    n_jobs : int, default=None\r\n"]
[1383.146, "o", "        Number of parallel jobs to run.\r\n"]
[1383.156, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1383.166, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1383.176, "o", "        for more details.\r\n"]
[1383.186, "o", "\r\n"]
[1383.196, "o", "    check_input : bool, default=True\r\n"]
[1383.206, "o", "        If `False`, the input arrays X and dictionary will not be checked.\r\n"]
[1383.216, "o", "\r\n"]
[1383.226, "o", "    verbose : int, default=0\r\n"]
[1383.236, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[1383.246, "o", "\r\n"]
[1383.256, "o", "    positive : bool, default=False\r\n"]
[1383.266, "o", "        Whether to enforce positivity when finding the encoding.\r\n"]
[1383.276, "o", "\r\n"]
[1383.286, "o", "        .. versionadded:: 0.20\r\n"]
[1383.296, "o", "\r\n"]
[1383.306, "o", "    Returns\r\n"]
[1383.316, "o", "    -------\r\n"]
[1383.326, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1383.336, "o", "        The sparse codes.\r\n"]
[1383.346, "o", "\r\n"]
[1383.356, "o", "    See Also\r\n"]
[1383.366, "o", "    --------\r\n"]
[1383.376, "o", "    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n"]
[1383.386, "o", "        path using LARS algorithm.\r\n"]
[1383.396, "o", "    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n"]
[1383.406, "o", "    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n"]
[1383.416, "o", "    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n"]
[1383.426, "o", "        dictionary.\r\n"]
[1383.436, "o", "    \"\"\"\r\n"]
[1383.446, "o", "    if check_input:\r\n"]
[1383.456, "o", "        if algorithm == \"lasso_cd\":\r\n"]
[1383.466, "o", "            dictionary = check_array(\r\n"]
[1383.476, "o", "                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n"]
[1383.486, "o", "            )\r\n"]
[1383.496, "o", "            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n"]
[1383.506, "o", "        else:\r\n"]
[1383.516, "o", "            dictionary = check_array(dictionary)\r\n"]
[1383.526, "o", "            X = check_array(X)\r\n"]
[1383.536, "o", "\r\n"]
[1383.546, "o", "    if dictionary.shape[1] != X.shape[1]:\r\n"]
[1383.556, "o", "        raise ValueError(\r\n"]
[1383.566, "o", "            \"Dictionary and X have different numbers of features:\"\r\n"]
[1383.576, "o", "            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n"]
[1383.586, "o", "        )\r\n"]
[1383.596, "o", "\r\n"]
[1383.606, "o", "    _check_positive_coding(algorithm, positive)\r\n"]
[1383.616, "o", "\r\n"]
[1383.626, "o", "    return _sparse_encode(\r\n"]
[1383.636, "o", "        X,\r\n"]
[1383.646, "o", "        dictionary,\r\n"]
[1383.656, "o", "        gram=gram,\r\n"]
[1383.666, "o", "        cov=cov,\r\n"]
[1383.676, "o", "        algorithm=algorithm,\r\n"]
[1383.686, "o", "        n_nonzero_coefs=n_nonzero_coefs,\r\n"]
[1383.696, "o", "        alpha=alpha,\r\n"]
[1383.706, "o", "        copy_cov=copy_cov,\r\n"]
[1383.716, "o", "        init=init,\r\n"]
[1383.726, "o", "        max_iter=max_iter,\r\n"]
[1383.736, "o", "        n_jobs=n_jobs,\r\n"]
[1383.746, "o", "        verbose=verbose,\r\n"]
[1383.756, "o", "        positive=positive,\r\n"]
[1383.766, "o", "    )\r\n"]
[1383.776, "o", "\r\n"]
[1383.786, "o", "\r\n"]
[1383.796, "o", "def _sparse_encode(\r\n"]
[1383.806, "o", "    X,\r\n"]
[1383.816, "o", "    dictionary,\r\n"]
[1383.826, "o", "    *,\r\n"]
[1383.836, "o", "    gram=None,\r\n"]
[1383.846, "o", "    cov=None,\r\n"]
[1383.856, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1383.866, "o", "    n_nonzero_coefs=None,\r\n"]
[1383.876, "o", "    alpha=None,\r\n"]
[1383.886, "o", "    copy_cov=True,\r\n"]
[1383.896, "o", "    init=None,\r\n"]
[1383.906, "o", "    max_iter=1000,\r\n"]
[1383.916, "o", "    n_jobs=None,\r\n"]
[1383.926, "o", "    verbose=0,\r\n"]
[1383.936, "o", "    positive=False,\r\n"]
[1383.946, "o", "):\r\n"]
[1383.956, "o", "    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[1383.966, "o", "\r\n"]
[1383.976, "o", "    n_samples, n_features = X.shape\r\n"]
[1383.986, "o", "    n_components = dictionary.shape[0]\r\n"]
[1383.996, "o", "\r\n"]
[1384.006, "o", "    if algorithm in (\"lars\", \"omp\"):\r\n"]
[1384.016, "o", "        regularization = n_nonzero_coefs\r\n"]
[1384.026, "o", "        if regularization is None:\r\n"]
[1384.036, "o", "            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[1384.046, "o", "    else:\r\n"]
[1384.056, "o", "        regularization = alpha\r\n"]
[1384.066, "o", "        if regularization is None:\r\n"]
[1384.076, "o", "            regularization = 1.0\r\n"]
[1384.086, "o", "\r\n"]
[1384.096, "o", "    if gram is None and algorithm != \"threshold\":\r\n"]
[1384.106, "o", "        gram = np.dot(dictionary, dictionary.T)\r\n"]
[1384.116, "o", "\r\n"]
[1384.126, "o", "    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[1384.136, "o", "        copy_cov = False\r\n"]
[1384.146, "o", "        cov = np.dot(dictionary, X.T)\r\n"]
[1384.156, "o", "\r\n"]
[1384.166, "o", "    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[1384.176, "o", "        code = _sparse_encode_precomputed(\r\n"]
[1384.186, "o", "            X,\r\n"]
[1384.196, "o", "            dictionary,\r\n"]
[1384.206, "o", "            gram=gram,\r\n"]
[1384.216, "o", "            cov=cov,\r\n"]
[1384.226, "o", "            algorithm=algorithm,\r\n"]
[1384.236, "o", "            regularization=regularization,\r\n"]
[1384.246, "o", "            copy_cov=copy_cov,\r\n"]
[1384.256, "o", "            init=init,\r\n"]
[1384.266, "o", "            max_iter=max_iter,\r\n"]
[1384.276, "o", "            verbose=verbose,\r\n"]
[1384.286, "o", "            positive=positive,\r\n"]
[1384.296, "o", "        )\r\n"]
[1384.306, "o", "        return code\r\n"]
[1384.316, "o", "\r\n"]
[1384.326, "o", "    # Enter parallel code block\r\n"]
[1384.336, "o", "    n_samples = X.shape[0]\r\n"]
[1384.346, "o", "    n_components = dictionary.shape[0]\r\n"]
[1384.356, "o", "    code = np.empty((n_samples, n_components))\r\n"]
[1384.366, "o", "    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[1384.376, "o", "\r\n"]
[1384.386, "o", "    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[1384.396, "o", "        delayed(_sparse_encode_precomputed)(\r\n"]
[1384.406, "o", "            X[this_slice],\r\n"]
[1384.416, "o", "            dictionary,\r\n"]
[1384.426, "o", "            gram=gram,\r\n"]
[1384.436, "o", "            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[1384.446, "o", "            algorithm=algorithm,\r\n"]
[1384.456, "o", "            regularization=regularization,\r\n"]
[1384.466, "o", "            copy_cov=copy_cov,\r\n"]
[1384.476, "o", "            init=init[this_slice] if init is not None else None,\r\n"]
[1384.486, "o", "            max_iter=max_iter,\r\n"]
[1384.496, "o", "            verbose=verbose,\r\n"]
[1384.506, "o", "            positive=positive,\r\n"]
[1384.516, "o", "        )\r\n"]
[1384.526, "o", "        for this_slice in slices\r\n"]
[1384.536, "o", "    )\r\n"]
[1384.546, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[1384.556, "o", "        code[this_slice] = this_view\r\n"]
[1384.566, "o", "    return code\r\n"]
[1384.576, "o", "\r\n"]
[1384.586, "o", "\r\n"]
[1384.596, "o", "def _update_dict(\r\n"]
[1384.606, "o", "    dictionary,\r\n"]
[1384.616, "o", "    Y,\r\n"]
[1384.626, "o", "    code,\r\n"]
[1384.636, "o", "    A=None,\r\n"]
[1384.646, "o", "    B=None,\r\n"]
[1384.656, "o", "    verbose=False,\r\n"]
[1384.666, "o", "    random_state=None,\r\n"]
[1384.676, "o", "    positive=False,\r\n"]
[1384.686, "o", "):\r\n"]
[1384.696, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[1384.706, "o", "\r\n"]
[1384.716, "o", "    Parameters\r\n"]
[1384.726, "o", "    ----------\r\n"]
[1384.736, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1384.746, "o", "        Value of the dictionary at the previous iteration.\r\n"]
[1384.756, "o", "\r\n"]
[1384.766, "o", "    Y : ndarray of shape (n_samples, n_features)\r\n"]
[1384.776, "o", "        Data matrix.\r\n"]
[1384.786, "o", "\r\n"]
[1384.796, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1384.806, "o", "        Sparse coding of the data against which to optimize the dictionary.\r\n"]
[1384.816, "o", "\r\n"]
[1384.826, "o", "    A : ndarray of shape (n_components, n_components), default=None\r\n"]
[1384.836, "o", "        Together with `B`, sufficient stats of the online model to update the\r\n"]
[1384.846, "o", "        dictionary.\r\n"]
[1384.856, "o", "\r\n"]
[1384.866, "o", "    B : ndarray of shape (n_features, n_components), default=None\r\n"]
[1384.876, "o", "        Together with `A`, sufficient stats of the online model to update the\r\n"]
[1384.886, "o", "        dictionary.\r\n"]
[1384.896, "o", "\r\n"]
[1384.906, "o", "    verbose: bool, default=False\r\n"]
[1384.916, "o", "        Degree of output the procedure will print.\r\n"]
[1384.926, "o", "\r\n"]
[1384.936, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1385.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1385.002, "i", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r"]
[1385.004, "o", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r\n"]
[1385.016, "o", "\u001b[?2004l\r\n"]
[1385.026, "o", "           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[1385.036, "o", "\r\n"]
[1385.046, "o", "    return_n_iter : bool, default=False\r\n"]
[1385.056, "o", "        Whether or not to return the number of iterations.\r\n"]
[1385.066, "o", "\r\n"]
[1385.076, "o", "        .. deprecated:: 1.1\r\n"]
[1385.086, "o", "           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n"]
[1385.096, "o", "\r\n"]
[1385.106, "o", "    positive_dict : bool, default=False\r\n"]
[1385.116, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1385.126, "o", "\r\n"]
[1385.136, "o", "        .. versionadded:: 0.20\r\n"]
[1385.146, "o", "\r\n"]
[1385.156, "o", "    positive_code : bool, default=False\r\n"]
[1385.166, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1385.176, "o", "\r\n"]
[1385.186, "o", "        .. versionadded:: 0.20\r\n"]
[1385.196, "o", "\r\n"]
[1385.206, "o", "    method_max_iter : int, default=1000\r\n"]
[1385.216, "o", "        Maximum number of iterations to perform when solving the lasso problem.\r\n"]
[1385.226, "o", "\r\n"]
[1385.236, "o", "        .. versionadded:: 0.22\r\n"]
[1385.246, "o", "\r\n"]
[1385.256, "o", "    tol : float, default=1e-3\r\n"]
[1385.266, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[1385.276, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[1385.286, "o", "\r\n"]
[1385.296, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[1385.306, "o", "        `tol` to 0.0.\r\n"]
[1385.316, "o", "\r\n"]
[1385.326, "o", "        .. versionadded:: 1.1\r\n"]
[1385.336, "o", "\r\n"]
[1385.346, "o", "    max_no_improvement : int, default=10\r\n"]
[1385.356, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[1385.366, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[1385.376, "o", "        `max_iter` is not None.\r\n"]
[1385.386, "o", "\r\n"]
[1385.396, "o", "        To disable convergence detection based on cost function, set\r\n"]
[1385.406, "o", "        `max_no_improvement` to None.\r\n"]
[1385.416, "o", "\r\n"]
[1385.426, "o", "        .. versionadded:: 1.1\r\n"]
[1385.436, "o", "\r\n"]
[1385.446, "o", "    Returns\r\n"]
[1385.456, "o", "    -------\r\n"]
[1385.466, "o", "    code : ndarray of shape (n_samples, n_components),\r\n"]
[1385.476, "o", "        The sparse code (only returned if `return_code=True`).\r\n"]
[1385.486, "o", "\r\n"]
[1385.496, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[1385.506, "o", "        The solutions to the dictionary learning problem.\r\n"]
[1385.516, "o", "\r\n"]
[1385.526, "o", "    n_iter : int\r\n"]
[1385.536, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[1385.546, "o", "        set to `True`.\r\n"]
[1385.556, "o", "\r\n"]
[1385.566, "o", "    See Also\r\n"]
[1385.576, "o", "    --------\r\n"]
[1385.586, "o", "    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n"]
[1385.596, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1385.606, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n"]
[1385.616, "o", "        learning algorithm.\r\n"]
[1385.626, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1385.636, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1385.646, "o", "    \"\"\"\r\n"]
[1385.656, "o", "    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n"]
[1385.666, "o", "    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n"]
[1385.676, "o", "        raise ValueError(\r\n"]
[1385.686, "o", "            \"The following arguments are incompatible with 'max_iter': \"\r\n"]
[1385.696, "o", "            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n"]
[1385.706, "o", "        )\r\n"]
[1385.716, "o", "\r\n"]
[1385.726, "o", "    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n"]
[1385.736, "o", "    return_inner_stats = _check_warn_deprecated(\r\n"]
[1385.746, "o", "        return_inner_stats,\r\n"]
[1385.756, "o", "        \"return_inner_stats\",\r\n"]
[1385.766, "o", "        default=False,\r\n"]
[1385.776, "o", "        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n"]
[1385.786, "o", "    )\r\n"]
[1385.796, "o", "    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n"]
[1385.806, "o", "    return_n_iter = _check_warn_deprecated(\r\n"]
[1385.816, "o", "        return_n_iter,\r\n"]
[1385.826, "o", "        \"return_n_iter\",\r\n"]
[1385.836, "o", "        default=False,\r\n"]
[1385.846, "o", "        additional_message=(\r\n"]
[1385.856, "o", "            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n"]
[1385.866, "o", "            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n"]
[1385.876, "o", "        ),\r\n"]
[1385.886, "o", "    )\r\n"]
[1385.896, "o", "\r\n"]
[1385.906, "o", "    if max_iter is not None:\r\n"]
[1385.916, "o", "        transform_algorithm = \"lasso_\" + method\r\n"]
[1385.926, "o", "\r\n"]
[1385.936, "o", "        est = MiniBatchDictionaryLearning(\r\n"]
[1385.946, "o", "            n_components=n_components,\r\n"]
[1385.956, "o", "            alpha=alpha,\r\n"]
[1385.966, "o", "            n_iter=n_iter,\r\n"]
[1385.976, "o", "            n_jobs=n_jobs,\r\n"]
[1385.986, "o", "            fit_algorithm=method,\r\n"]
[1385.996, "o", "            batch_size=batch_size,\r\n"]
[1386.006, "o", "            shuffle=shuffle,\r\n"]
[1386.016, "o", "            dict_init=dict_init,\r\n"]
[1386.026, "o", "            random_state=random_state,\r\n"]
[1386.036, "o", "            transform_algorithm=transform_algorithm,\r\n"]
[1386.046, "o", "            transform_alpha=alpha,\r\n"]
[1386.056, "o", "            positive_code=positive_code,\r\n"]
[1386.066, "o", "            positive_dict=positive_dict,\r\n"]
[1386.076, "o", "            transform_max_iter=method_max_iter,\r\n"]
[1386.086, "o", "            verbose=verbose,\r\n"]
[1386.096, "o", "            callback=callback,\r\n"]
[1386.106, "o", "            tol=tol,\r\n"]
[1386.116, "o", "            max_no_improvement=max_no_improvement,\r\n"]
[1386.126, "o", "        ).fit(X)\r\n"]
[1386.136, "o", "\r\n"]
[1386.146, "o", "        if not return_code:\r\n"]
[1386.156, "o", "            return est.components_\r\n"]
[1386.166, "o", "        else:\r\n"]
[1386.176, "o", "            code = est.transform(X)\r\n"]
[1386.186, "o", "            return code, est.components_\r\n"]
[1386.196, "o", "\r\n"]
[1386.206, "o", "    # TODO(1.4) remove the whole old behavior\r\n"]
[1386.216, "o", "    # Fallback to old behavior\r\n"]
[1386.226, "o", "\r\n"]
[1386.236, "o", "    n_iter = _check_warn_deprecated(\r\n"]
[1386.246, "o", "        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n"]
[1386.256, "o", "    )\r\n"]
[1386.266, "o", "\r\n"]
[1386.276, "o", "    if n_components is None:\r\n"]
[1386.286, "o", "        n_components = X.shape[1]\r\n"]
[1386.296, "o", "\r\n"]
[1386.306, "o", "    if method not in (\"lars\", \"cd\"):\r\n"]
[1386.316, "o", "        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n"]
[1386.326, "o", "\r\n"]
[1386.336, "o", "    _check_positive_coding(method, positive_code)\r\n"]
[1386.346, "o", "\r\n"]
[1386.356, "o", "    method = \"lasso_\" + method\r\n"]
[1386.366, "o", "\r\n"]
[1386.376, "o", "    t0 = time.time()\r\n"]
[1386.386, "o", "    n_samples, n_features = X.shape\r\n"]
[1386.396, "o", "    # Avoid integer division problems\r\n"]
[1386.406, "o", "    alpha = float(alpha)\r\n"]
[1386.416, "o", "    random_state = check_random_state(random_state)\r\n"]
[1386.426, "o", "\r\n"]
[1386.436, "o", "    # Init V with SVD of X\r\n"]
[1386.446, "o", "    if dict_init is not None:\r\n"]
[1386.456, "o", "        dictionary = dict_init\r\n"]
[1386.466, "o", "    else:\r\n"]
[1386.476, "o", "        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n"]
[1386.486, "o", "        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[1386.496, "o", "    r = len(dictionary)\r\n"]
[1386.506, "o", "    if n_components <= r:\r\n"]
[1386.516, "o", "        dictionary = dictionary[:n_components, :]\r\n"]
[1386.526, "o", "    else:\r\n"]
[1386.536, "o", "        dictionary = np.r_[\r\n"]
[1386.546, "o", "            dictionary,\r\n"]
[1386.556, "o", "            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n"]
[1386.566, "o", "        ]\r\n"]
[1386.576, "o", "\r\n"]
[1386.586, "o", "    if verbose == 1:\r\n"]
[1386.596, "o", "        print(\"[dict_learning]\", end=\" \")\r\n"]
[1386.606, "o", "\r\n"]
[1386.616, "o", "    if shuffle:\r\n"]
[1386.626, "o", "        X_train = X.copy()\r\n"]
[1386.636, "o", "        random_state.shuffle(X_train)\r\n"]
[1386.646, "o", "    else:\r\n"]
[1386.656, "o", "        X_train = X\r\n"]
[1386.666, "o", "\r\n"]
[1386.676, "o", "    X_train = check_array(\r\n"]
[1386.686, "o", "        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n"]
[1386.696, "o", "    )\r\n"]
[1386.706, "o", "\r\n"]
[1386.716, "o", "    # Fortran-order dict better suited for the sparse coding which is the\r\n"]
[1386.726, "o", "    # bottleneck of this algorithm.\r\n"]
[1386.736, "o", "    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n"]
[1386.746, "o", "    dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[1386.756, "o", "\r\n"]
[1386.766, "o", "    batches = gen_batches(n_samples, batch_size)\r\n"]
[1386.776, "o", "    batches = itertools.cycle(batches)\r\n"]
[1386.786, "o", "\r\n"]
[1386.796, "o", "    # The covariance of the dictionary\r\n"]
[1386.806, "o", "    if inner_stats is None:\r\n"]
[1386.816, "o", "        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n"]
[1386.826, "o", "        # The data approximation\r\n"]
[1386.836, "o", "        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n"]
[1386.846, "o", "    else:\r\n"]
[1386.856, "o", "        A = inner_stats[0].copy()\r\n"]
[1386.866, "o", "        B = inner_stats[1].copy()\r\n"]
[1386.876, "o", "\r\n"]
[1386.886, "o", "    # If n_iter is zero, we need to return zero.\r\n"]
[1386.896, "o", "    ii = iter_offset - 1\r\n"]
[1386.906, "o", "\r\n"]
[1386.916, "o", "    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n"]
[1386.926, "o", "        this_X = X_train[batch]\r\n"]
[1386.936, "o", "        dt = time.time() - t0\r\n"]
[1386.946, "o", "        if verbose == 1:\r\n"]
[1386.956, "o", "            sys.stdout.write(\".\")\r\n"]
[1386.966, "o", "            sys.stdout.flush()\r\n"]
[1386.976, "o", "        elif verbose:\r\n"]
[1386.986, "o", "            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n"]
[1386.996, "o", "                print(\r\n"]
[1387.006, "o", "                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n"]
[1387.016, "o", "                )\r\n"]
[1387.026, "o", "\r\n"]
[1387.036, "o", "        this_code = sparse_encode(\r\n"]
[1387.046, "o", "            this_X,\r\n"]
[1387.056, "o", "            dictionary,\r\n"]
[1387.066, "o", "            algorithm=method,\r\n"]
[1387.076, "o", "            alpha=alpha,\r\n"]
[1387.086, "o", "            n_jobs=n_jobs,\r\n"]
[1387.096, "o", "            check_input=False,\r\n"]
[1387.106, "o", "            positive=positive_code,\r\n"]
[1387.116, "o", "            max_iter=method_max_iter,\r\n"]
[1387.126, "o", "            verbose=verbose,\r\n"]
[1387.136, "o", "        )\r\n"]
[1387.146, "o", "\r\n"]
[1387.156, "o", "        # Update the auxiliary variables\r\n"]
[1387.166, "o", "        if ii < batch_size - 1:\r\n"]
[1387.176, "o", "            theta = float((ii + 1) * batch_size)\r\n"]
[1387.186, "o", "        else:\r\n"]
[1387.196, "o", "            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n"]
[1387.206, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[1387.216, "o", "\r\n"]
[1387.226, "o", "        A *= beta\r\n"]
[1387.236, "o", "        A += np.dot(this_code.T, this_code)\r\n"]
[1387.246, "o", "        B *= beta\r\n"]
[1387.256, "o", "        B += np.dot(this_X.T, this_code)\r\n"]
[1387.266, "o", "\r\n"]
[1387.276, "o", "        # Update dictionary in place\r\n"]
[1387.286, "o", "        _update_dict(\r\n"]
[1387.296, "o", "            dictionary,\r\n"]
[1387.306, "o", "            this_X,\r\n"]
[1387.316, "o", "            this_code,\r\n"]
[1387.326, "o", "            A,\r\n"]
[1387.336, "o", "            B,\r\n"]
[1387.346, "o", "            verbose=verbose,\r\n"]
[1387.356, "o", "            random_state=random_state,\r\n"]
[1387.366, "o", "            positive=positive_dict,\r\n"]
[1387.376, "o", "        )\r\n"]
[1387.386, "o", "\r\n"]
[1387.396, "o", "        # Maybe we need a stopping criteria based on the amount of\r\n"]
[1387.406, "o", "        # modification in the dictionary\r\n"]
[1387.416, "o", "        if callback is not None:\r\n"]
[1387.426, "o", "            callback(locals())\r\n"]
[1387.436, "o", "\r\n"]
[1387.446, "o", "    if return_inner_stats:\r\n"]
[1387.456, "o", "        if return_n_iter:\r\n"]
[1387.466, "o", "            return dictionary, (A, B), ii - iter_offset + 1\r\n"]
[1387.476, "o", "        else:\r\n"]
[1387.486, "o", "            return dictionary, (A, B)\r\n"]
[1387.496, "o", "    if return_code:\r\n"]
[1387.506, "o", "        if verbose > 1:\r\n"]
[1387.516, "o", "            print(\"Learning code...\", end=\" \")\r\n"]
[1387.526, "o", "        elif verbose == 1:\r\n"]
[1387.536, "o", "            print(\"|\", end=\" \")\r\n"]
[1387.546, "o", "        code = sparse_encode(\r\n"]
[1387.556, "o", "            X,\r\n"]
[1387.566, "o", "            dictionary,\r\n"]
[1387.576, "o", "            algorithm=method,\r\n"]
[1387.586, "o", "            alpha=alpha,\r\n"]
[1387.596, "o", "            n_jobs=n_jobs,\r\n"]
[1387.606, "o", "            check_input=False,\r\n"]
[1387.616, "o", "            positive=positive_code,\r\n"]
[1387.626, "o", "            max_iter=method_max_iter,\r\n"]
[1387.636, "o", "            verbose=verbose,\r\n"]
[1387.646, "o", "        )\r\n"]
[1387.656, "o", "        if verbose > 1:\r\n"]
[1387.666, "o", "            dt = time.time() - t0\r\n"]
[1387.676, "o", "            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n"]
[1387.686, "o", "        if return_n_iter:\r\n"]
[1387.696, "o", "            return code, dictionary, ii - iter_offset + 1\r\n"]
[1387.706, "o", "        else:\r\n"]
[1387.716, "o", "            return code, dictionary\r\n"]
[1387.726, "o", "\r\n"]
[1387.736, "o", "    if return_n_iter:\r\n"]
[1387.746, "o", "        return dictionary, ii - iter_offset + 1\r\n"]
[1387.756, "o", "    else:\r\n"]
[1387.766, "o", "        return dictionary\r\n"]
[1387.776, "o", "\r\n"]
[1387.786, "o", "\r\n"]
[1387.796, "o", "@validate_params(\r\n"]
[1387.806, "o", "    {\r\n"]
[1387.816, "o", "        \"X\": [\"array-like\"],\r\n"]
[1387.826, "o", "        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[1387.836, "o", "        \"return_n_iter\": [\"boolean\"],\r\n"]
[1387.846, "o", "        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1387.856, "o", "    },\r\n"]
[1387.866, "o", "    prefer_skip_nested_validation=False,\r\n"]
[1387.876, "o", ")\r\n"]
[1387.886, "o", "def dict_learning(\r\n"]
[1387.896, "o", "    X,\r\n"]
[1387.906, "o", "    n_components,\r\n"]
[1387.916, "o", "    *,\r\n"]
[1387.926, "o", "    alpha,\r\n"]
[1387.936, "o", "    max_iter=100,\r\n"]
[1387.946, "o", "    tol=1e-8,\r\n"]
[1387.956, "o", "    method=\"lars\",\r\n"]
[1387.966, "o", "    n_jobs=None,\r\n"]
[1387.976, "o", "    dict_init=None,\r\n"]
[1387.986, "o", "    code_init=None,\r\n"]
[1387.996, "o", "    callback=None,\r\n"]
[1388.006, "o", "    verbose=False,\r\n"]
[1388.016, "o", "    random_state=None,\r\n"]
[1388.026, "o", "    return_n_iter=False,\r\n"]
[1388.036, "o", "    positive_dict=False,\r\n"]
[1388.046, "o", "    positive_code=False,\r\n"]
[1388.056, "o", "    method_max_iter=1000,\r\n"]
[1388.066, "o", "):\r\n"]
[1388.076, "o", "    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n"]
[1388.086, "o", "\r\n"]
[1388.096, "o", "    Finds the best dictionary and the corresponding sparse code for\r\n"]
[1388.106, "o", "    approximating the data matrix X by solving::\r\n"]
[1388.116, "o", "\r\n"]
[1388.126, "o", "        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[1388.136, "o", "                     (U,V)\r\n"]
[1388.146, "o", "                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n"]
[1388.156, "o", "\r\n"]
[1388.166, "o", "    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n"]
[1388.176, "o", "    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n"]
[1388.186, "o", "    which is the sum of the absolute values of all the entries in the matrix.\r\n"]
[1388.196, "o", "\r\n"]
[1388.206, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[1388.216, "o", "\r\n"]
[1388.226, "o", "    Parameters\r\n"]
[1388.236, "o", "    ----------\r\n"]
[1388.246, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[1388.256, "o", "        Data matrix.\r\n"]
[1388.266, "o", "\r\n"]
[1388.276, "o", "    n_components : int\r\n"]
[1388.286, "o", "        Number of dictionary atoms to extract.\r\n"]
[1388.296, "o", "\r\n"]
[1388.306, "o", "    alpha : int or float\r\n"]
[1388.316, "o", "        Sparsity controlling parameter.\r\n"]
[1388.326, "o", "\r\n"]
[1388.336, "o", "    max_iter : int, default=100\r\n"]
[1388.346, "o", "        Maximum number of iterations to perform.\r\n"]
[1388.356, "o", "\r\n"]
[1388.366, "o", "    tol : float, default=1e-8\r\n"]
[1388.376, "o", "        Tolerance for the stopping condition.\r\n"]
[1388.386, "o", "\r\n"]
[1388.396, "o", "    method : {'lars', 'cd'}, default='lars'\r\n"]
[1388.406, "o", "        The method used:\r\n"]
[1388.416, "o", "\r\n"]
[1388.426, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[1388.436, "o", "           problem (`linear_model.lars_path`);\r\n"]
[1388.446, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[1388.456, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[1388.466, "o", "          the estimated components are sparse.\r\n"]
[1388.476, "o", "\r\n"]
[1388.486, "o", "    n_jobs : int, default=None\r\n"]
[1388.496, "o", "        Number of parallel jobs to run.\r\n"]
[1388.506, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1388.516, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1388.526, "o", "        for more details.\r\n"]
[1388.536, "o", "\r\n"]
[1388.546, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1388.556, "o", "        Initial value for the dictionary for warm restart scenarios. Only used\r\n"]
[1388.566, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[1388.576, "o", "\r\n"]
[1388.586, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1388.596, "o", "        Initial value for the sparse code for warm restart scenarios. Only used\r\n"]
[1388.606, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[1388.616, "o", "\r\n"]
[1388.626, "o", "    callback : callable, default=None\r\n"]
[1388.636, "o", "        Callable that gets invoked every five iterations.\r\n"]
[1388.646, "o", "\r\n"]
[1388.656, "o", "    verbose : bool, default=False\r\n"]
[1388.666, "o", "        To control the verbosity of the procedure.\r\n"]
[1388.676, "o", "\r\n"]
[1388.686, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1388.696, "o", "        Used for randomly initializing the dictionary. Pass an int for\r\n"]
[1388.706, "o", "        reproducible results across multiple function calls.\r\n"]
[1388.716, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1388.726, "o", "\r\n"]
[1388.736, "o", "    return_n_iter : bool, default=False\r\n"]
[1388.746, "o", "        Whether or not to return the number of iterations.\r\n"]
[1388.756, "o", "\r\n"]
[1388.766, "o", "    positive_dict : bool, default=False\r\n"]
[1388.776, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1388.786, "o", "\r\n"]
[1388.796, "o", "        .. versionadded:: 0.20\r\n"]
[1388.806, "o", "\r\n"]
[1388.816, "o", "    positive_code : bool, default=False\r\n"]
[1388.826, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1388.836, "o", "\r\n"]
[1388.846, "o", "        .. versionadded:: 0.20\r\n"]
[1388.856, "o", "\r\n"]
[1388.866, "o", "    method_max_iter : int, default=1000\r\n"]
[1388.876, "o", "        Maximum number of iterations to perform.\r\n"]
[1388.886, "o", "\r\n"]
[1388.896, "o", "        .. versionadded:: 0.22\r\n"]
[1388.906, "o", "\r\n"]
[1388.916, "o", "    Returns\r\n"]
[1388.926, "o", "    -------\r\n"]
[1388.936, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1388.946, "o", "        The sparse code factor in the matrix factorization.\r\n"]
[1388.956, "o", "\r\n"]
[1388.966, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[1388.976, "o", "        The dictionary factor in the matrix factorization.\r\n"]
[1388.986, "o", "\r\n"]
[1388.996, "o", "    errors : array\r\n"]
[1389.006, "o", "        Vector of errors at each iteration.\r\n"]
[1389.016, "o", "\r\n"]
[1389.026, "o", "    n_iter : int\r\n"]
[1389.036, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[1389.046, "o", "        set to True.\r\n"]
[1389.056, "o", "\r\n"]
[1389.066, "o", "    See Also\r\n"]
[1389.076, "o", "    --------\r\n"]
[1389.086, "o", "    dict_learning_online : Solve a dictionary learning matrix factorization\r\n"]
[1389.096, "o", "        problem online.\r\n"]
[1389.106, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1389.116, "o", "    MiniBatchDictionaryLearning : A faster, less accurate version\r\n"]
[1389.126, "o", "        of the dictionary learning algorithm.\r\n"]
[1389.136, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1389.146, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1389.156, "o", "    \"\"\"\r\n"]
[1389.166, "o", "    estimator = DictionaryLearning(\r\n"]
[1389.176, "o", "        n_components=n_components,\r\n"]
[1389.186, "o", "        alpha=alpha,\r\n"]
[1389.196, "o", "        max_iter=max_iter,\r\n"]
[1389.206, "o", "        tol=tol,\r\n"]
[1389.216, "o", "        fit_algorithm=method,\r\n"]
[1389.226, "o", "        n_jobs=n_jobs,\r\n"]
[1389.236, "o", "        dict_init=dict_init,\r\n"]
[1389.246, "o", "        callback=callback,\r\n"]
[1389.256, "o", "        code_init=code_init,\r\n"]
[1389.266, "o", "        verbose=verbose,\r\n"]
[1389.276, "o", "        random_state=random_state,\r\n"]
[1389.286, "o", "        positive_code=positive_code,\r\n"]
[1389.296, "o", "        positive_dict=positive_dict,\r\n"]
[1389.306, "o", "        transform_max_iter=method_max_iter,\r\n"]
[1389.316, "o", "    )\r\n"]
[1389.326, "o", "    code = estimator.fit_transform(X)\r\n"]
[1389.336, "o", "    if return_n_iter:\r\n"]
[1389.346, "o", "        return (\r\n"]
[1389.356, "o", "            code,\r\n"]
[1389.366, "o", "            estimator.components_,\r\n"]
[1389.376, "o", "            estimator.error_,\r\n"]
[1389.386, "o", "            estimator.n_iter_,\r\n"]
[1389.396, "o", "        )\r\n"]
[1389.406, "o", "    return code, estimator.components_, estimator.error_\r\n"]
[1389.416, "o", "\r\n"]
[1389.426, "o", "\r\n"]
[1389.436, "o", "class _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n"]
[1389.446, "o", "    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n"]
[1389.456, "o", "\r\n"]
[1389.466, "o", "    def __init__(\r\n"]
[1389.476, "o", "        self,\r\n"]
[1389.486, "o", "        transform_algorithm,\r\n"]
[1389.496, "o", "        transform_n_nonzero_coefs,\r\n"]
[1389.506, "o", "        transform_alpha,\r\n"]
[1389.516, "o", "        split_sign,\r\n"]
[1389.526, "o", "        n_jobs,\r\n"]
[1389.536, "o", "        positive_code,\r\n"]
[1389.546, "o", "        transform_max_iter,\r\n"]
[1389.556, "o", "    ):\r\n"]
[1389.566, "o", "        self.transform_algorithm = transform_algorithm\r\n"]
[1389.576, "o", "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n"]
[1389.586, "o", "        self.transform_alpha = transform_alpha\r\n"]
[1389.596, "o", "        self.transform_max_iter = transform_max_iter\r\n"]
[1389.606, "o", "        self.split_sign = split_sign\r\n"]
[1389.616, "o", "        self.n_jobs = n_jobs\r\n"]
[1389.626, "o", "        self.positive_code = positive_code\r\n"]
[1389.636, "o", "\r\n"]
[1389.646, "o", "    def _transform(self, X, dictionary):\r\n"]
[1389.656, "o", "        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n"]
[1389.666, "o", "        SparseCoder.\"\"\"\r\n"]
[1389.676, "o", "        X = self._validate_data(X, reset=False)\r\n"]
[1389.686, "o", "\r\n"]
[1389.696, "o", "        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n"]
[1389.706, "o", "            transform_alpha = self.alpha\r\n"]
[1389.716, "o", "        else:\r\n"]
[1389.726, "o", "            transform_alpha = self.transform_alpha\r\n"]
[1389.736, "o", "\r\n"]
[1389.746, "o", "        code = sparse_encode(\r\n"]
[1389.756, "o", "            X,\r\n"]
[1389.766, "o", "            dictionary,\r\n"]
[1389.776, "o", "            algorithm=self.transform_algorithm,\r\n"]
[1389.786, "o", "            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n"]
[1389.796, "o", "            alpha=transform_alpha,\r\n"]
[1389.806, "o", "            max_iter=self.transform_max_iter,\r\n"]
[1389.816, "o", "            n_jobs=self.n_jobs,\r\n"]
[1389.826, "o", "            positive=self.positive_code,\r\n"]
[1389.836, "o", "        )\r\n"]
[1389.846, "o", "\r\n"]
[1389.856, "o", "        if self.split_sign:\r\n"]
[1389.866, "o", "            # feature vector is split into a positive and negative side\r\n"]
[1389.876, "o", "            n_samples, n_features = code.shape\r\n"]
[1389.886, "o", "            split_code = np.empty((n_samples, 2 * n_features))\r\n"]
[1389.896, "o", "            split_code[:, :n_features] = np.maximum(code, 0)\r\n"]
[1389.906, "o", "            split_code[:, n_features:] = -np.minimum(code, 0)\r\n"]
[1389.916, "o", "            code = split_code\r\n"]
[1389.926, "o", "\r\n"]
[1389.936, "o", "        return code\r\n"]
[1390.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1390.002, "i", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py\r"]
[1390.004, "o", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py\r\n"]
[1390.016, "o", "\u001b[?2004l\r\n"]
[1390.026, "o", "\r\n"]
[1390.036, "o", "    n_jobs : int or None, default=None\r\n"]
[1390.046, "o", "        Number of parallel jobs to run.\r\n"]
[1390.056, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1390.066, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1390.076, "o", "        for more details.\r\n"]
[1390.086, "o", "\r\n"]
[1390.096, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1390.106, "o", "        Initial value for the code, for warm restart. Only used if `code_init`\r\n"]
[1390.116, "o", "        and `dict_init` are not None.\r\n"]
[1390.126, "o", "\r\n"]
[1390.136, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1390.146, "o", "        Initial values for the dictionary, for warm restart. Only used if\r\n"]
[1390.156, "o", "        `code_init` and `dict_init` are not None.\r\n"]
[1390.166, "o", "\r\n"]
[1390.176, "o", "    callback : callable, default=None\r\n"]
[1390.186, "o", "        Callable that gets invoked every five iterations.\r\n"]
[1390.196, "o", "\r\n"]
[1390.206, "o", "        .. versionadded:: 1.3\r\n"]
[1390.216, "o", "\r\n"]
[1390.226, "o", "    verbose : bool, default=False\r\n"]
[1390.236, "o", "        To control the verbosity of the procedure.\r\n"]
[1390.246, "o", "\r\n"]
[1390.256, "o", "    split_sign : bool, default=False\r\n"]
[1390.266, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[1390.276, "o", "        its negative part and its positive part. This can improve the\r\n"]
[1390.286, "o", "        performance of downstream classifiers.\r\n"]
[1390.296, "o", "\r\n"]
[1390.306, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1390.316, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[1390.326, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[1390.336, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[1390.346, "o", "        results across multiple function calls.\r\n"]
[1390.356, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1390.366, "o", "\r\n"]
[1390.376, "o", "    positive_code : bool, default=False\r\n"]
[1390.386, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1390.396, "o", "\r\n"]
[1390.406, "o", "        .. versionadded:: 0.20\r\n"]
[1390.416, "o", "\r\n"]
[1390.426, "o", "    positive_dict : bool, default=False\r\n"]
[1390.436, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1390.446, "o", "\r\n"]
[1390.456, "o", "        .. versionadded:: 0.20\r\n"]
[1390.466, "o", "\r\n"]
[1390.476, "o", "    transform_max_iter : int, default=1000\r\n"]
[1390.486, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1390.496, "o", "        `'lasso_lars'`.\r\n"]
[1390.506, "o", "\r\n"]
[1390.516, "o", "        .. versionadded:: 0.22\r\n"]
[1390.526, "o", "\r\n"]
[1390.536, "o", "    Attributes\r\n"]
[1390.546, "o", "    ----------\r\n"]
[1390.556, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[1390.566, "o", "        dictionary atoms extracted from the data\r\n"]
[1390.576, "o", "\r\n"]
[1390.586, "o", "    error_ : array\r\n"]
[1390.596, "o", "        vector of errors at each iteration\r\n"]
[1390.606, "o", "\r\n"]
[1390.616, "o", "    n_features_in_ : int\r\n"]
[1390.626, "o", "        Number of features seen during :term:`fit`.\r\n"]
[1390.636, "o", "\r\n"]
[1390.646, "o", "        .. versionadded:: 0.24\r\n"]
[1390.656, "o", "\r\n"]
[1390.666, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[1390.676, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[1390.686, "o", "        has feature names that are all strings.\r\n"]
[1390.696, "o", "\r\n"]
[1390.706, "o", "        .. versionadded:: 1.0\r\n"]
[1390.716, "o", "\r\n"]
[1390.726, "o", "    n_iter_ : int\r\n"]
[1390.736, "o", "        Number of iterations run.\r\n"]
[1390.746, "o", "\r\n"]
[1390.756, "o", "    See Also\r\n"]
[1390.766, "o", "    --------\r\n"]
[1390.776, "o", "    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n"]
[1390.786, "o", "        dictionary learning algorithm.\r\n"]
[1390.796, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1390.806, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[1390.816, "o", "        precomputed dictionary.\r\n"]
[1390.826, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1390.836, "o", "\r\n"]
[1390.846, "o", "    References\r\n"]
[1390.856, "o", "    ----------\r\n"]
[1390.866, "o", "\r\n"]
[1390.876, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[1390.886, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[1390.896, "o", "\r\n"]
[1390.906, "o", "    Examples\r\n"]
[1390.916, "o", "    --------\r\n"]
[1390.926, "o", "    >>> import numpy as np\r\n"]
[1390.936, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[1390.946, "o", "    >>> from sklearn.decomposition import DictionaryLearning\r\n"]
[1390.956, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[1390.966, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[1390.976, "o", "    ...     random_state=42,\r\n"]
[1390.986, "o", "    ... )\r\n"]
[1390.996, "o", "    >>> dict_learner = DictionaryLearning(\r\n"]
[1391.006, "o", "    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n"]
[1391.016, "o", "    ...     random_state=42,\r\n"]
[1391.026, "o", "    ... )\r\n"]
[1391.036, "o", "    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n"]
[1391.046, "o", "\r\n"]
[1391.056, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[1391.066, "o", "\r\n"]
[1391.076, "o", "    >>> np.mean(X_transformed == 0)\r\n"]
[1391.086, "o", "    0.41...\r\n"]
[1391.096, "o", "\r\n"]
[1391.106, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[1391.116, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[1391.126, "o", "    the original signal:\r\n"]
[1391.136, "o", "\r\n"]
[1391.146, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[1391.156, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[1391.166, "o", "    0.07...\r\n"]
[1391.176, "o", "    \"\"\"\r\n"]
[1391.186, "o", "\r\n"]
[1391.196, "o", "    _parameter_constraints: dict = {\r\n"]
[1391.206, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1391.216, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1391.226, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1391.236, "o", "        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1391.246, "o", "        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[1391.256, "o", "        \"transform_algorithm\": [\r\n"]
[1391.266, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[1391.276, "o", "        ],\r\n"]
[1391.286, "o", "        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1391.296, "o", "        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[1391.306, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[1391.316, "o", "        \"code_init\": [np.ndarray, None],\r\n"]
[1391.326, "o", "        \"dict_init\": [np.ndarray, None],\r\n"]
[1391.336, "o", "        \"callback\": [callable, None],\r\n"]
[1391.346, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[1391.356, "o", "        \"split_sign\": [\"boolean\"],\r\n"]
[1391.366, "o", "        \"random_state\": [\"random_state\"],\r\n"]
[1391.376, "o", "        \"positive_code\": [\"boolean\"],\r\n"]
[1391.386, "o", "        \"positive_dict\": [\"boolean\"],\r\n"]
[1391.396, "o", "        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1391.406, "o", "    }\r\n"]
[1391.416, "o", "\r\n"]
[1391.426, "o", "    def __init__(\r\n"]
[1391.436, "o", "        self,\r\n"]
[1391.446, "o", "        n_components=None,\r\n"]
[1391.456, "o", "        *,\r\n"]
[1391.466, "o", "        alpha=1,\r\n"]
[1391.476, "o", "        max_iter=1000,\r\n"]
[1391.486, "o", "        tol=1e-8,\r\n"]
[1391.496, "o", "        fit_algorithm=\"lars\",\r\n"]
[1391.506, "o", "        transform_algorithm=\"omp\",\r\n"]
[1391.516, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[1391.526, "o", "        transform_alpha=None,\r\n"]
[1391.536, "o", "        n_jobs=None,\r\n"]
[1391.546, "o", "        code_init=None,\r\n"]
[1391.556, "o", "        dict_init=None,\r\n"]
[1391.566, "o", "        callback=None,\r\n"]
[1391.576, "o", "        verbose=False,\r\n"]
[1391.586, "o", "        split_sign=False,\r\n"]
[1391.596, "o", "        random_state=None,\r\n"]
[1391.606, "o", "        positive_code=False,\r\n"]
[1391.616, "o", "        positive_dict=False,\r\n"]
[1391.626, "o", "        transform_max_iter=1000,\r\n"]
[1391.636, "o", "    ):\r\n"]
[1391.646, "o", "        super().__init__(\r\n"]
[1391.656, "o", "            transform_algorithm,\r\n"]
[1391.666, "o", "            transform_n_nonzero_coefs,\r\n"]
[1391.676, "o", "            transform_alpha,\r\n"]
[1391.686, "o", "            split_sign,\r\n"]
[1391.696, "o", "            n_jobs,\r\n"]
[1391.706, "o", "            positive_code,\r\n"]
[1391.716, "o", "            transform_max_iter,\r\n"]
[1391.726, "o", "        )\r\n"]
[1391.736, "o", "        self.n_components = n_components\r\n"]
[1391.746, "o", "        self.alpha = alpha\r\n"]
[1391.756, "o", "        self.max_iter = max_iter\r\n"]
[1391.766, "o", "        self.tol = tol\r\n"]
[1391.776, "o", "        self.fit_algorithm = fit_algorithm\r\n"]
[1391.786, "o", "        self.code_init = code_init\r\n"]
[1391.796, "o", "        self.dict_init = dict_init\r\n"]
[1391.806, "o", "        self.callback = callback\r\n"]
[1391.816, "o", "        self.verbose = verbose\r\n"]
[1391.826, "o", "        self.random_state = random_state\r\n"]
[1391.836, "o", "        self.positive_dict = positive_dict\r\n"]
[1391.846, "o", "\r\n"]
[1391.856, "o", "    def fit(self, X, y=None):\r\n"]
[1391.866, "o", "        \"\"\"Fit the model from data in X.\r\n"]
[1391.876, "o", "\r\n"]
[1391.886, "o", "        Parameters\r\n"]
[1391.896, "o", "        ----------\r\n"]
[1391.906, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[1391.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1391.926, "o", "            and `n_features` is the number of features.\r\n"]
[1391.936, "o", "\r\n"]
[1391.946, "o", "        y : Ignored\r\n"]
[1391.956, "o", "            Not used, present for API consistency by convention.\r\n"]
[1391.966, "o", "\r\n"]
[1391.976, "o", "        Returns\r\n"]
[1391.986, "o", "        -------\r\n"]
[1391.996, "o", "        self : object\r\n"]
[1392.006, "o", "            Returns the instance itself.\r\n"]
[1392.016, "o", "        \"\"\"\r\n"]
[1392.026, "o", "        self.fit_transform(X)\r\n"]
[1392.036, "o", "        return self\r\n"]
[1392.046, "o", "\r\n"]
[1392.056, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[1392.066, "o", "    def fit_transform(self, X, y=None):\r\n"]
[1392.076, "o", "        \"\"\"Fit the model from data in X and return the transformed data.\r\n"]
[1392.086, "o", "\r\n"]
[1392.096, "o", "        Parameters\r\n"]
[1392.106, "o", "        ----------\r\n"]
[1392.116, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[1392.126, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1392.136, "o", "            and `n_features` is the number of features.\r\n"]
[1392.146, "o", "\r\n"]
[1392.156, "o", "        y : Ignored\r\n"]
[1392.166, "o", "            Not used, present for API consistency by convention.\r\n"]
[1392.176, "o", "\r\n"]
[1392.186, "o", "        Returns\r\n"]
[1392.196, "o", "        -------\r\n"]
[1392.206, "o", "        V : ndarray of shape (n_samples, n_components)\r\n"]
[1392.216, "o", "            Transformed data.\r\n"]
[1392.226, "o", "        \"\"\"\r\n"]
[1392.236, "o", "        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n"]
[1392.246, "o", "\r\n"]
[1392.256, "o", "        method = \"lasso_\" + self.fit_algorithm\r\n"]
[1392.266, "o", "\r\n"]
[1392.276, "o", "        random_state = check_random_state(self.random_state)\r\n"]
[1392.286, "o", "        X = self._validate_data(X)\r\n"]
[1392.296, "o", "\r\n"]
[1392.306, "o", "        if self.n_components is None:\r\n"]
[1392.316, "o", "            n_components = X.shape[1]\r\n"]
[1392.326, "o", "        else:\r\n"]
[1392.336, "o", "            n_components = self.n_components\r\n"]
[1392.346, "o", "\r\n"]
[1392.356, "o", "        V, U, E, self.n_iter_ = _dict_learning(\r\n"]
[1392.366, "o", "            X,\r\n"]
[1392.376, "o", "            n_components,\r\n"]
[1392.386, "o", "            alpha=self.alpha,\r\n"]
[1392.396, "o", "            tol=self.tol,\r\n"]
[1392.406, "o", "            max_iter=self.max_iter,\r\n"]
[1392.416, "o", "            method=method,\r\n"]
[1392.426, "o", "            method_max_iter=self.transform_max_iter,\r\n"]
[1392.436, "o", "            n_jobs=self.n_jobs,\r\n"]
[1392.446, "o", "            code_init=self.code_init,\r\n"]
[1392.456, "o", "            dict_init=self.dict_init,\r\n"]
[1392.466, "o", "            callback=self.callback,\r\n"]
[1392.476, "o", "            verbose=self.verbose,\r\n"]
[1392.486, "o", "            random_state=random_state,\r\n"]
[1392.496, "o", "            return_n_iter=True,\r\n"]
[1392.506, "o", "            positive_dict=self.positive_dict,\r\n"]
[1392.516, "o", "            positive_code=self.positive_code,\r\n"]
[1392.526, "o", "        )\r\n"]
[1392.536, "o", "        self.components_ = U\r\n"]
[1392.546, "o", "        self.error_ = E\r\n"]
[1392.556, "o", "\r\n"]
[1392.566, "o", "        return V\r\n"]
[1392.576, "o", "\r\n"]
[1392.586, "o", "    @property\r\n"]
[1392.596, "o", "    def _n_features_out(self):\r\n"]
[1392.606, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[1392.616, "o", "        return self.components_.shape[0]\r\n"]
[1392.626, "o", "\r\n"]
[1392.636, "o", "    def _more_tags(self):\r\n"]
[1392.646, "o", "        return {\r\n"]
[1392.656, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[1392.666, "o", "        }\r\n"]
[1392.676, "o", "\r\n"]
[1392.686, "o", "\r\n"]
[1392.696, "o", "class MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[1392.706, "o", "    \"\"\"Mini-batch dictionary learning.\r\n"]
[1392.716, "o", "\r\n"]
[1392.726, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[1392.736, "o", "    encoding the fitted data.\r\n"]
[1392.746, "o", "\r\n"]
[1392.756, "o", "    Solves the optimization problem::\r\n"]
[1392.766, "o", "\r\n"]
[1392.776, "o", "       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[1392.786, "o", "                    (U,V)\r\n"]
[1392.796, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[1392.806, "o", "\r\n"]
[1392.816, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[1392.826, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[1392.836, "o", "    of all the entries in the matrix.\r\n"]
[1392.846, "o", "\r\n"]
[1392.856, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[1392.866, "o", "\r\n"]
[1392.876, "o", "    Parameters\r\n"]
[1392.886, "o", "    ----------\r\n"]
[1392.896, "o", "    n_components : int, default=None\r\n"]
[1392.906, "o", "        Number of dictionary elements to extract.\r\n"]
[1392.916, "o", "\r\n"]
[1392.926, "o", "    alpha : float, default=1\r\n"]
[1392.936, "o", "        Sparsity controlling parameter.\r\n"]
[1392.946, "o", "\r\n"]
[1392.956, "o", "    n_iter : int, default=1000\r\n"]
[1392.966, "o", "        Total number of iterations over data batches to perform.\r\n"]
[1392.976, "o", "\r\n"]
[1392.986, "o", "        .. deprecated:: 1.1\r\n"]
[1392.996, "o", "           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[1393.006, "o", "           ``max_iter`` instead.\r\n"]
[1393.016, "o", "\r\n"]
[1393.026, "o", "    max_iter : int, default=None\r\n"]
[1393.036, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[1393.046, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[1393.056, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[1393.066, "o", "\r\n"]
[1393.076, "o", "        .. versionadded:: 1.1\r\n"]
[1393.086, "o", "\r\n"]
[1393.096, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[1393.106, "o", "        The algorithm used:\r\n"]
[1393.116, "o", "\r\n"]
[1393.126, "o", "        - `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[1393.136, "o", "          problem (`linear_model.lars_path`)\r\n"]
[1393.146, "o", "        - `'cd'`: uses the coordinate descent method to compute the\r\n"]
[1393.156, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[1393.166, "o", "          the estimated components are sparse.\r\n"]
[1393.176, "o", "\r\n"]
[1393.186, "o", "    n_jobs : int, default=None\r\n"]
[1393.196, "o", "        Number of parallel jobs to run.\r\n"]
[1393.206, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1393.216, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1393.226, "o", "        for more details.\r\n"]
[1393.236, "o", "\r\n"]
[1393.246, "o", "    batch_size : int, default=256\r\n"]
[1393.256, "o", "        Number of samples in each mini-batch.\r\n"]
[1393.266, "o", "\r\n"]
[1393.276, "o", "        .. versionchanged:: 1.3\r\n"]
[1393.286, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[1393.296, "o", "\r\n"]
[1393.306, "o", "    shuffle : bool, default=True\r\n"]
[1393.316, "o", "        Whether to shuffle the samples before forming batches.\r\n"]
[1393.326, "o", "\r\n"]
[1393.336, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1393.346, "o", "        Initial value of the dictionary for warm restart scenarios.\r\n"]
[1393.356, "o", "\r\n"]
[1393.366, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[1393.376, "o", "            'threshold'}, default='omp'\r\n"]
[1393.386, "o", "        Algorithm used to transform the data:\r\n"]
[1393.396, "o", "\r\n"]
[1393.406, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[1393.416, "o", "          (`linear_model.lars_path`);\r\n"]
[1393.426, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[1393.436, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1393.446, "o", "          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n"]
[1393.456, "o", "          if the estimated components are sparse.\r\n"]
[1393.466, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1393.476, "o", "          solution.\r\n"]
[1393.486, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[1393.496, "o", "          the projection ``dictionary * X'``.\r\n"]
[1393.506, "o", "\r\n"]
[1393.516, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[1393.526, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1393.536, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[1393.546, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[1393.556, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1393.566, "o", "\r\n"]
[1393.576, "o", "    transform_alpha : float, default=None\r\n"]
[1393.586, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1393.596, "o", "        penalty applied to the L1 norm.\r\n"]
[1393.606, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1393.616, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1393.626, "o", "        If `None`, defaults to `alpha`.\r\n"]
[1393.636, "o", "\r\n"]
[1393.646, "o", "        .. versionchanged:: 1.2\r\n"]
[1393.656, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[1393.666, "o", "\r\n"]
[1393.676, "o", "    verbose : bool or int, default=False\r\n"]
[1393.686, "o", "        To control the verbosity of the procedure.\r\n"]
[1393.696, "o", "\r\n"]
[1393.706, "o", "    split_sign : bool, default=False\r\n"]
[1393.716, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[1393.726, "o", "        its negative part and its positive part. This can improve the\r\n"]
[1393.736, "o", "        performance of downstream classifiers.\r\n"]
[1393.746, "o", "\r\n"]
[1393.756, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1393.766, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[1393.776, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[1393.786, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[1393.796, "o", "        results across multiple function calls.\r\n"]
[1393.806, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1393.816, "o", "\r\n"]
[1393.826, "o", "    positive_code : bool, default=False\r\n"]
[1393.836, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1393.846, "o", "\r\n"]
[1393.856, "o", "        .. versionadded:: 0.20\r\n"]
[1393.866, "o", "\r\n"]
[1393.876, "o", "    positive_dict : bool, default=False\r\n"]
[1393.886, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1393.896, "o", "\r\n"]
[1393.906, "o", "        .. versionadded:: 0.20\r\n"]
[1393.916, "o", "\r\n"]
[1393.926, "o", "    transform_max_iter : int, default=1000\r\n"]
[1393.936, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1393.946, "o", "        `'lasso_lars'`.\r\n"]
[1393.956, "o", "\r\n"]
[1393.966, "o", "        .. versionadded:: 0.22\r\n"]
[1393.976, "o", "\r\n"]
[1393.986, "o", "    callback : callable, default=None\r\n"]
[1393.996, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[1394.006, "o", "\r\n"]
[1394.016, "o", "        .. versionadded:: 1.1\r\n"]
[1394.026, "o", "\r\n"]
[1394.036, "o", "    tol : float, default=1e-3\r\n"]
[1394.046, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[1394.056, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[1394.066, "o", "\r\n"]
[1394.076, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[1394.086, "o", "        `tol` to 0.0.\r\n"]
[1394.096, "o", "\r\n"]
[1394.106, "o", "        .. versionadded:: 1.1\r\n"]
[1394.116, "o", "\r\n"]
[1394.126, "o", "    max_no_improvement : int, default=10\r\n"]
[1394.136, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[1394.146, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[1394.156, "o", "        `max_iter` is not None.\r\n"]
[1394.166, "o", "\r\n"]
[1394.176, "o", "        To disable convergence detection based on cost function, set\r\n"]
[1394.186, "o", "        `max_no_improvement` to None.\r\n"]
[1394.196, "o", "\r\n"]
[1394.206, "o", "        .. versionadded:: 1.1\r\n"]
[1394.216, "o", "\r\n"]
[1394.226, "o", "    Attributes\r\n"]
[1394.236, "o", "    ----------\r\n"]
[1394.246, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[1394.256, "o", "        Components extracted from the data.\r\n"]
[1394.266, "o", "\r\n"]
[1394.276, "o", "    n_features_in_ : int\r\n"]
[1394.286, "o", "        Number of features seen during :term:`fit`.\r\n"]
[1394.296, "o", "\r\n"]
[1394.306, "o", "        .. versionadded:: 0.24\r\n"]
[1394.316, "o", "\r\n"]
[1394.326, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[1394.336, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[1394.346, "o", "        has feature names that are all strings.\r\n"]
[1394.356, "o", "\r\n"]
[1394.366, "o", "        .. versionadded:: 1.0\r\n"]
[1394.376, "o", "\r\n"]
[1394.386, "o", "    n_iter_ : int\r\n"]
[1394.396, "o", "        Number of iterations over the full dataset.\r\n"]
[1394.406, "o", "\r\n"]
[1394.416, "o", "    n_steps_ : int\r\n"]
[1394.426, "o", "        Number of mini-batches processed.\r\n"]
[1394.436, "o", "\r\n"]
[1394.446, "o", "        .. versionadded:: 1.1\r\n"]
[1394.456, "o", "\r\n"]
[1394.466, "o", "    See Also\r\n"]
[1394.476, "o", "    --------\r\n"]
[1394.486, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1394.496, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1394.506, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[1394.516, "o", "        precomputed dictionary.\r\n"]
[1394.526, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1394.536, "o", "\r\n"]
[1394.546, "o", "    References\r\n"]
[1394.556, "o", "    ----------\r\n"]
[1394.566, "o", "\r\n"]
[1394.576, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[1394.586, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[1394.596, "o", "\r\n"]
[1394.606, "o", "    Examples\r\n"]
[1394.616, "o", "    --------\r\n"]
[1394.626, "o", "    >>> import numpy as np\r\n"]
[1394.636, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[1394.646, "o", "    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n"]
[1394.656, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[1394.666, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[1394.676, "o", "    ...     random_state=42)\r\n"]
[1394.686, "o", "    >>> dict_learner = MiniBatchDictionaryLearning(\r\n"]
[1394.696, "o", "    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n"]
[1394.706, "o", "    ...     transform_alpha=0.1, random_state=42)\r\n"]
[1394.716, "o", "    >>> X_transformed = dict_learner.fit_transform(X)\r\n"]
[1394.726, "o", "\r\n"]
[1394.736, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[1394.746, "o", "\r\n"]
[1394.756, "o", "    >>> np.mean(X_transformed == 0) < 0.5\r\n"]
[1394.766, "o", "    True\r\n"]
[1394.776, "o", "\r\n"]
[1394.786, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[1394.796, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[1394.806, "o", "    the original signal:\r\n"]
[1394.816, "o", "\r\n"]
[1394.826, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[1394.836, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[1394.846, "o", "    0.057...\r\n"]
[1394.856, "o", "    \"\"\"\r\n"]
[1394.866, "o", "\r\n"]
[1394.876, "o", "    _parameter_constraints: dict = {\r\n"]
[1394.886, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1394.896, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1394.906, "o", "        \"n_iter\": [\r\n"]
[1394.916, "o", "            Interval(Integral, 0, None, closed=\"left\"),\r\n"]
[1394.926, "o", "            Hidden(StrOptions({\"deprecated\"})),\r\n"]
[1394.936, "o", "        ],\r\n"]
[1395.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1395.002, "i", "cd /workspace/repo\r"]
[1395.004, "o", "cd /workspace/repo\r\n"]
[1395.006, "o", "\u001b[?2004l\r\n"]
[1400.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1400.002, "i", "grep -R \"_sparse_encode_precomputed\\s*\\(\" -n sklearn || true\r"]
[1400.004, "o", "grep -R \"_sparse_encode_precomputed\\s*\\(\" -n sklearn || true\r\n"]
[1402.478, "o", "\u001b[?2004l\r\n"]
[1405.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1405.002, "i", "grep -R \"sparse_encode\\s*\\(\" -n sklearn/decomposition || true\r"]
[1405.004, "o", "grep -R \"sparse_encode\\s*\\(\" -n sklearn/decomposition || true\r\n"]
[1407.478, "o", "\u001b[?2004l\r\n"]
[1410.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1410.002, "i", "sed -n '1,2600p' sklearn/decomposition/_dict_learning.py | nl -ba | sed -n '1,260p'\r"]
[1410.004, "o", "sed -n '1,2600p' sklearn/decomposition/_dict_learning.py | nl -ba | sed -n '1,260p'\r\n"]
[1410.024943, "o", "\u001b[?2004l\r\n"]
[1410.043885, "o", "     1\t\"\"\" Dictionary learning.\r\n"]
[1410.062828, "o", "     2\t\"\"\"\r\n"]
[1410.08177, "o", "     3\t# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[1410.100713, "o", "     4\t# License: BSD 3 clause\r\n"]
[1410.119655, "o", "     5\t\r\n"]
[1410.138598, "o", "     6\timport itertools\r\n"]
[1410.15754, "o", "     7\timport sys\r\n"]
[1410.176483, "o", "     8\timport time\r\n"]
[1410.195425, "o", "     9\timport warnings\r\n"]
[1410.214368, "o", "    10\tfrom math import ceil\r\n"]
[1410.23331, "o", "    11\tfrom numbers import Integral, Real\r\n"]
[1410.252253, "o", "    12\t\r\n"]
[1410.271195, "o", "    13\timport numpy as np\r\n"]
[1410.290138, "o", "    14\tfrom joblib import effective_n_jobs\r\n"]
[1410.30908, "o", "    15\tfrom scipy import linalg\r\n"]
[1410.328023, "o", "    16\t\r\n"]
[1410.346966, "o", "    17\tfrom ..base import (\r\n"]
[1410.365908, "o", "    18\t    BaseEstimator,\r\n"]
[1410.384851, "o", "    19\t    ClassNamePrefixFeaturesOutMixin,\r\n"]
[1410.403793, "o", "    20\t    TransformerMixin,\r\n"]
[1410.422736, "o", "    21\t    _fit_context,\r\n"]
[1410.441678, "o", "    22\t)\r\n"]
[1410.460621, "o", "    23\tfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[1410.479563, "o", "    24\tfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[1410.498506, "o", "    25\tfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[1410.517448, "o", "    26\tfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[1410.536391, "o", "    27\tfrom ..utils.parallel import Parallel, delayed\r\n"]
[1410.555333, "o", "    28\tfrom ..utils.validation import check_is_fitted\r\n"]
[1410.574276, "o", "    29\t\r\n"]
[1410.593218, "o", "    30\t\r\n"]
[1410.612161, "o", "    31\tdef _check_positive_coding(method, positive):\r\n"]
[1410.631103, "o", "    32\t    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[1410.650046, "o", "    33\t        raise ValueError(\r\n"]
[1410.668989, "o", "    34\t            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[1410.687931, "o", "    35\t        )\r\n"]
[1410.706874, "o", "    36\t\r\n"]
[1410.725816, "o", "    37\t\r\n"]
[1410.744759, "o", "    38\tdef _sparse_encode_precomputed(\r\n"]
[1410.763701, "o", "    39\t    X,\r\n"]
[1410.782644, "o", "    40\t    dictionary,\r\n"]
[1410.801586, "o", "    41\t    *,\r\n"]
[1410.820529, "o", "    42\t    gram=None,\r\n"]
[1410.839471, "o", "    43\t    cov=None,\r\n"]
[1410.858414, "o", "    44\t    algorithm=\"lasso_lars\",\r\n"]
[1410.877356, "o", "    45\t    regularization=None,\r\n"]
[1410.896299, "o", "    46\t    copy_cov=True,\r\n"]
[1410.915241, "o", "    47\t    init=None,\r\n"]
[1410.934184, "o", "    48\t    max_iter=1000,\r\n"]
[1410.953126, "o", "    49\t    verbose=0,\r\n"]
[1410.972069, "o", "    50\t    positive=False,\r\n"]
[1410.991011, "o", "    51\t):\r\n"]
[1411.009954, "o", "    52\t    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[1411.028897, "o", "    53\t\r\n"]
[1411.047839, "o", "    54\t    Each row of the result is the solution to a Lasso problem.\r\n"]
[1411.066782, "o", "    55\t\r\n"]
[1411.085724, "o", "    56\t    Parameters\r\n"]
[1411.104667, "o", "    57\t    ----------\r\n"]
[1411.123609, "o", "    58\t    X : ndarray of shape (n_samples, n_features)\r\n"]
[1411.142552, "o", "    59\t        Data matrix.\r\n"]
[1411.161494, "o", "    60\t\r\n"]
[1411.180437, "o", "    61\t    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1411.199379, "o", "    62\t        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1411.218322, "o", "    63\t        the data. Some of the algorithms assume normalized rows.\r\n"]
[1411.237264, "o", "    64\t\r\n"]
[1411.256207, "o", "    65\t    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[1411.275149, "o", "    66\t        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[1411.294092, "o", "    67\t        gram can be `None` if method is 'threshold'.\r\n"]
[1411.313034, "o", "    68\t\r\n"]
[1411.331977, "o", "    69\t    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[1411.35092, "o", "    70\t        Precomputed covariance, `dictionary * X'`.\r\n"]
[1411.369862, "o", "    71\t\r\n"]
[1411.388805, "o", "    72\t    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1411.407747, "o", "    73\t            default='lasso_lars'\r\n"]
[1411.42669, "o", "    74\t        The algorithm used:\r\n"]
[1411.445632, "o", "    75\t\r\n"]
[1411.464575, "o", "    76\t        * `'lars'`: uses the least angle regression method\r\n"]
[1411.483517, "o", "    77\t          (`linear_model.lars_path`);\r\n"]
[1411.50246, "o", "    78\t        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1411.521402, "o", "    79\t        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1411.540345, "o", "    80\t          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1411.559287, "o", "    81\t          the estimated components are sparse;\r\n"]
[1411.57823, "o", "    82\t        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1411.597172, "o", "    83\t          solution;\r\n"]
[1411.616115, "o", "    84\t        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1411.635057, "o", "    85\t          regularization from the projection `dictionary * data'`.\r\n"]
[1411.654, "o", "    86\t\r\n"]
[1411.672943, "o", "    87\t    regularization : int or float, default=None\r\n"]
[1411.691885, "o", "    88\t        The regularization parameter. It corresponds to alpha when\r\n"]
[1411.710828, "o", "    89\t        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[1411.72977, "o", "    90\t        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[1411.748713, "o", "    91\t\r\n"]
[1411.767655, "o", "    92\t    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1411.786598, "o", "    93\t        Initialization value of the sparse code. Only used if\r\n"]
[1411.80554, "o", "    94\t        `algorithm='lasso_cd'`.\r\n"]
[1411.824483, "o", "    95\t\r\n"]
[1411.843425, "o", "    96\t    max_iter : int, default=1000\r\n"]
[1411.862368, "o", "    97\t        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1411.88131, "o", "    98\t        `'lasso_lars'`.\r\n"]
[1411.900253, "o", "    99\t\r\n"]
[1411.919195, "o", "   100\t    copy_cov : bool, default=True\r\n"]
[1411.938138, "o", "   101\t        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1411.95708, "o", "   102\t        be overwritten.\r\n"]
[1411.976023, "o", "   103\t\r\n"]
[1411.994966, "o", "   104\t    verbose : int, default=0\r\n"]
[1412.013908, "o", "   105\t        Controls the verbosity; the higher, the more messages.\r\n"]
[1412.032851, "o", "   106\t\r\n"]
[1412.051793, "o", "   107\t    positive: bool, default=False\r\n"]
[1412.070736, "o", "   108\t        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[1412.089678, "o", "   109\t\r\n"]
[1412.108621, "o", "   110\t        .. versionadded:: 0.20\r\n"]
[1412.127563, "o", "   111\t\r\n"]
[1412.146506, "o", "   112\t    Returns\r\n"]
[1412.165448, "o", "   113\t    -------\r\n"]
[1412.184391, "o", "   114\t    code : ndarray of shape (n_components, n_features)\r\n"]
[1412.203333, "o", "   115\t        The sparse codes.\r\n"]
[1412.222276, "o", "   116\t    \"\"\"\r\n"]
[1412.241218, "o", "   117\t    n_samples, n_features = X.shape\r\n"]
[1412.260161, "o", "   118\t    n_components = dictionary.shape[0]\r\n"]
[1412.279103, "o", "   119\t\r\n"]
[1412.298046, "o", "   120\t    if algorithm == \"lasso_lars\":\r\n"]
[1412.316989, "o", "   121\t        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1412.335931, "o", "   122\t        try:\r\n"]
[1412.354874, "o", "   123\t            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1412.373816, "o", "   124\t\r\n"]
[1412.392759, "o", "   125\t            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1412.411701, "o", "   126\t            # corrects the verbosity level.\r\n"]
[1412.430644, "o", "   127\t            lasso_lars = LassoLars(\r\n"]
[1412.449586, "o", "   128\t                alpha=alpha,\r\n"]
[1412.468529, "o", "   129\t                fit_intercept=False,\r\n"]
[1412.487471, "o", "   130\t                verbose=verbose,\r\n"]
[1412.506414, "o", "   131\t                precompute=gram,\r\n"]
[1412.525356, "o", "   132\t                fit_path=False,\r\n"]
[1412.544299, "o", "   133\t                positive=positive,\r\n"]
[1412.563241, "o", "   134\t                max_iter=max_iter,\r\n"]
[1412.582184, "o", "   135\t            )\r\n"]
[1412.601126, "o", "   136\t            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1412.620069, "o", "   137\t            new_code = lasso_lars.coef_\r\n"]
[1412.639011, "o", "   138\t        finally:\r\n"]
[1412.657954, "o", "   139\t            np.seterr(**err_mgt)\r\n"]
[1412.676897, "o", "   140\t\r\n"]
[1412.695839, "o", "   141\t    elif algorithm == \"lasso_cd\":\r\n"]
[1412.714782, "o", "   142\t        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1412.733724, "o", "   143\t\r\n"]
[1412.752667, "o", "   144\t        # TODO: Make verbosity argument for Lasso?\r\n"]
[1412.771609, "o", "   145\t        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[1412.790552, "o", "   146\t        # argument that we could pass in from Lasso.\r\n"]
[1412.809494, "o", "   147\t        clf = Lasso(\r\n"]
[1412.828437, "o", "   148\t            alpha=alpha,\r\n"]
[1412.847379, "o", "   149\t            fit_intercept=False,\r\n"]
[1412.866322, "o", "   150\t            precompute=gram,\r\n"]
[1412.885264, "o", "   151\t            max_iter=max_iter,\r\n"]
[1412.904207, "o", "   152\t            warm_start=True,\r\n"]
[1412.923149, "o", "   153\t            positive=positive,\r\n"]
[1412.942092, "o", "   154\t        )\r\n"]
[1412.961034, "o", "   155\t\r\n"]
[1412.979977, "o", "   156\t        if init is not None:\r\n"]
[1412.99892, "o", "   157\t            # In some workflows using coordinate descent algorithms:\r\n"]
[1413.017862, "o", "   158\t            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[1413.036805, "o", "   159\t            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[1413.055747, "o", "   160\t            # TODO: move this handling (which is currently too broad)\r\n"]
[1413.07469, "o", "   161\t            # closer to the actual private function which need buffers to be writable.\r\n"]
[1413.093632, "o", "   162\t            if not init.flags[\"WRITEABLE\"]:\r\n"]
[1413.112575, "o", "   163\t                init = np.array(init)\r\n"]
[1413.131517, "o", "   164\t            clf.coef_ = init\r\n"]
[1413.15046, "o", "   165\t\r\n"]
[1413.169402, "o", "   166\t        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[1413.188345, "o", "   167\t        new_code = clf.coef_\r\n"]
[1413.207287, "o", "   168\t\r\n"]
[1413.22623, "o", "   169\t    elif algorithm == \"lars\":\r\n"]
[1413.245172, "o", "   170\t        try:\r\n"]
[1413.264115, "o", "   171\t            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1413.283057, "o", "   172\t\r\n"]
[1413.302, "o", "   173\t            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1413.320943, "o", "   174\t            # corrects the verbosity level.\r\n"]
[1413.339885, "o", "   175\t            lars = Lars(\r\n"]
[1413.358828, "o", "   176\t                fit_intercept=False,\r\n"]
[1413.37777, "o", "   177\t                verbose=verbose,\r\n"]
[1413.396713, "o", "   178\t                precompute=gram,\r\n"]
[1413.415655, "o", "   179\t                n_nonzero_coefs=int(regularization),\r\n"]
[1413.434598, "o", "   180\t                fit_path=False,\r\n"]
[1413.45354, "o", "   181\t            )\r\n"]
[1413.472483, "o", "   182\t            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1413.491425, "o", "   183\t            new_code = lars.coef_\r\n"]
[1413.510368, "o", "   184\t        finally:\r\n"]
[1413.52931, "o", "   185\t            np.seterr(**err_mgt)\r\n"]
[1413.548253, "o", "   186\t\r\n"]
[1413.567195, "o", "   187\t    elif algorithm == \"threshold\":\r\n"]
[1413.586138, "o", "   188\t        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[1413.60508, "o", "   189\t        if positive:\r\n"]
[1413.624023, "o", "   190\t            np.clip(new_code, 0, None, out=new_code)\r\n"]
[1413.642966, "o", "   191\t\r\n"]
[1413.661908, "o", "   192\t    elif algorithm == \"omp\":\r\n"]
[1413.680851, "o", "   193\t        new_code = orthogonal_mp_gram(\r\n"]
[1413.699793, "o", "   194\t            Gram=gram,\r\n"]
[1413.718736, "o", "   195\t            Xy=cov,\r\n"]
[1413.737678, "o", "   196\t            n_nonzero_coefs=int(regularization),\r\n"]
[1413.756621, "o", "   197\t            tol=None,\r\n"]
[1413.775563, "o", "   198\t            norms_squared=row_norms(X, squared=True),\r\n"]
[1413.794506, "o", "   199\t            copy_Xy=copy_cov,\r\n"]
[1413.813448, "o", "   200\t        ).T\r\n"]
[1413.832391, "o", "   201\t\r\n"]
[1413.851333, "o", "   202\t    return new_code.reshape(n_samples, n_components)\r\n"]
[1413.870276, "o", "   203\t\r\n"]
[1413.889218, "o", "   204\t\r\n"]
[1413.908161, "o", "   205\t@validate_params(\r\n"]
[1413.927103, "o", "   206\t    {\r\n"]
[1413.946046, "o", "   207\t        \"X\": [\"array-like\"],\r\n"]
[1413.964989, "o", "   208\t        \"dictionary\": [\"array-like\"],\r\n"]
[1413.983931, "o", "   209\t        \"gram\": [\"array-like\", None],\r\n"]
[1414.002874, "o", "   210\t        \"cov\": [\"array-like\", None],\r\n"]
[1414.021816, "o", "   211\t        \"algorithm\": [\r\n"]
[1414.040759, "o", "   212\t            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[1414.059701, "o", "   213\t        ],\r\n"]
[1414.078644, "o", "   214\t        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1414.097586, "o", "   215\t        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[1414.116529, "o", "   216\t        \"copy_cov\": [\"boolean\"],\r\n"]
[1414.135471, "o", "   217\t        \"init\": [\"array-like\", None],\r\n"]
[1414.154414, "o", "   218\t        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1414.173356, "o", "   219\t        \"n_jobs\": [Integral, None],\r\n"]
[1414.192299, "o", "   220\t        \"check_input\": [\"boolean\"],\r\n"]
[1414.211241, "o", "   221\t        \"verbose\": [\"verbose\"],\r\n"]
[1414.230184, "o", "   222\t        \"positive\": [\"boolean\"],\r\n"]
[1414.249126, "o", "   223\t    },\r\n"]
[1414.268069, "o", "   224\t    prefer_skip_nested_validation=True,\r\n"]
[1414.287011, "o", "   225\t)\r\n"]
[1414.305954, "o", "   226\t# XXX : could be moved to the linear_model module\r\n"]
[1414.324897, "o", "   227\tdef sparse_encode(\r\n"]
[1414.343839, "o", "   228\t    X,\r\n"]
[1414.362782, "o", "   229\t    dictionary,\r\n"]
[1414.381724, "o", "   230\t    *,\r\n"]
[1414.400667, "o", "   231\t    gram=None,\r\n"]
[1414.419609, "o", "   232\t    cov=None,\r\n"]
[1414.438552, "o", "   233\t    algorithm=\"lasso_lars\",\r\n"]
[1414.457494, "o", "   234\t    n_nonzero_coefs=None,\r\n"]
[1414.476437, "o", "   235\t    alpha=None,\r\n"]
[1414.495379, "o", "   236\t    copy_cov=True,\r\n"]
[1414.514322, "o", "   237\t    init=None,\r\n"]
[1414.533264, "o", "   238\t    max_iter=1000,\r\n"]
[1414.552207, "o", "   239\t    n_jobs=None,\r\n"]
[1414.571149, "o", "   240\t    check_input=True,\r\n"]
[1414.590092, "o", "   241\t    verbose=0,\r\n"]
[1414.609034, "o", "   242\t    positive=False,\r\n"]
[1414.627977, "o", "   243\t):\r\n"]
[1414.64692, "o", "   244\t    \"\"\"Sparse coding.\r\n"]
[1414.665862, "o", "   245\t\r\n"]
[1414.684805, "o", "   246\t    Each row of the result is the solution to a sparse coding problem.\r\n"]
[1414.703747, "o", "   247\t    The goal is to find a sparse array `code` such that::\r\n"]
[1414.72269, "o", "   248\t\r\n"]
[1414.741632, "o", "   249\t        X ~= code * dictionary\r\n"]
[1414.760575, "o", "   250\t\r\n"]
[1414.779517, "o", "   251\t    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[1414.79846, "o", "   252\t\r\n"]
[1414.817402, "o", "   253\t    Parameters\r\n"]
[1414.836345, "o", "   254\t    ----------\r\n"]
[1414.855287, "o", "   255\t    X : array-like of shape (n_samples, n_features)\r\n"]
[1414.87423, "o", "   256\t        Data matrix.\r\n"]
[1414.893172, "o", "   257\t\r\n"]
[1414.912115, "o", "   258\t    dictionary : array-like of shape (n_components, n_features)\r\n"]
[1414.931057, "o", "   259\t        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1415.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1415.002, "i", "sed -n '260,800p' sklearn/decomposition/_dict_learning.py | nl -ba | sed -n '1,260p'\r"]
[1415.004, "o", "sed -n '260,800p' sklearn/decomposition/_dict_learning.py | nl -ba | sed -n '1,260p'\r\n"]
[1415.024943, "o", "\u001b[?2004l\r\n"]
[1415.043885, "o", "     1\t        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[1415.062828, "o", "     2\t        output.\r\n"]
[1415.08177, "o", "     3\t\r\n"]
[1415.100713, "o", "     4\t    gram : array-like of shape (n_components, n_components), default=None\r\n"]
[1415.119655, "o", "     5\t        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n"]
[1415.138598, "o", "     6\t\r\n"]
[1415.15754, "o", "     7\t    cov : array-like of shape (n_components, n_samples), default=None\r\n"]
[1415.176483, "o", "     8\t        Precomputed covariance, `dictionary' * X`.\r\n"]
[1415.195425, "o", "     9\t\r\n"]
[1415.214368, "o", "    10\t    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1415.23331, "o", "    11\t            default='lasso_lars'\r\n"]
[1415.252253, "o", "    12\t        The algorithm used:\r\n"]
[1415.271195, "o", "    13\t\r\n"]
[1415.290138, "o", "    14\t        * `'lars'`: uses the least angle regression method\r\n"]
[1415.30908, "o", "    15\t          (`linear_model.lars_path`);\r\n"]
[1415.328023, "o", "    16\t        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1415.346966, "o", "    17\t        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1415.365908, "o", "    18\t          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1415.384851, "o", "    19\t          the estimated components are sparse;\r\n"]
[1415.403793, "o", "    20\t        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1415.422736, "o", "    21\t          solution;\r\n"]
[1415.441678, "o", "    22\t        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1415.460621, "o", "    23\t          regularization from the projection `dictionary * data'`.\r\n"]
[1415.479563, "o", "    24\t\r\n"]
[1415.498506, "o", "    25\t    n_nonzero_coefs : int, default=None\r\n"]
[1415.517448, "o", "    26\t        Number of nonzero coefficients to target in each column of the\r\n"]
[1415.536391, "o", "    27\t        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[1415.555333, "o", "    28\t        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[1415.574276, "o", "    29\t        `n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1415.593218, "o", "    30\t\r\n"]
[1415.612161, "o", "    31\t    alpha : float, default=None\r\n"]
[1415.631103, "o", "    32\t        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1415.650046, "o", "    33\t        penalty applied to the L1 norm.\r\n"]
[1415.668989, "o", "    34\t        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1415.687931, "o", "    35\t        threshold below which coefficients will be squashed to zero.\r\n"]
[1415.706874, "o", "    36\t        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[1415.725816, "o", "    37\t        the reconstruction error targeted. In this case, it overrides\r\n"]
[1415.744759, "o", "    38\t        `n_nonzero_coefs`.\r\n"]
[1415.763701, "o", "    39\t        If `None`, default to 1.\r\n"]
[1415.782644, "o", "    40\t\r\n"]
[1415.801586, "o", "    41\t    copy_cov : bool, default=True\r\n"]
[1415.820529, "o", "    42\t        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1415.839471, "o", "    43\t        be overwritten.\r\n"]
[1415.858414, "o", "    44\t\r\n"]
[1415.877356, "o", "    45\t    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1415.896299, "o", "    46\t        Initialization value of the sparse codes. Only used if\r\n"]
[1415.915241, "o", "    47\t        `algorithm='lasso_cd'`.\r\n"]
[1415.934184, "o", "    48\t\r\n"]
[1415.953126, "o", "    49\t    max_iter : int, default=1000\r\n"]
[1415.972069, "o", "    50\t        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1415.991011, "o", "    51\t        `'lasso_lars'`.\r\n"]
[1416.009954, "o", "    52\t\r\n"]
[1416.028897, "o", "    53\t    n_jobs : int, default=None\r\n"]
[1416.047839, "o", "    54\t        Number of parallel jobs to run.\r\n"]
[1416.066782, "o", "    55\t        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1416.085724, "o", "    56\t        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1416.104667, "o", "    57\t        for more details.\r\n"]
[1416.123609, "o", "    58\t\r\n"]
[1416.142552, "o", "    59\t    check_input : bool, default=True\r\n"]
[1416.161494, "o", "    60\t        If `False`, the input arrays X and dictionary will not be checked.\r\n"]
[1416.180437, "o", "    61\t\r\n"]
[1416.199379, "o", "    62\t    verbose : int, default=0\r\n"]
[1416.218322, "o", "    63\t        Controls the verbosity; the higher, the more messages.\r\n"]
[1416.237264, "o", "    64\t\r\n"]
[1416.256207, "o", "    65\t    positive : bool, default=False\r\n"]
[1416.275149, "o", "    66\t        Whether to enforce positivity when finding the encoding.\r\n"]
[1416.294092, "o", "    67\t\r\n"]
[1416.313034, "o", "    68\t        .. versionadded:: 0.20\r\n"]
[1416.331977, "o", "    69\t\r\n"]
[1416.35092, "o", "    70\t    Returns\r\n"]
[1416.369862, "o", "    71\t    -------\r\n"]
[1416.388805, "o", "    72\t    code : ndarray of shape (n_samples, n_components)\r\n"]
[1416.407747, "o", "    73\t        The sparse codes.\r\n"]
[1416.42669, "o", "    74\t\r\n"]
[1416.445632, "o", "    75\t    See Also\r\n"]
[1416.464575, "o", "    76\t    --------\r\n"]
[1416.483517, "o", "    77\t    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n"]
[1416.50246, "o", "    78\t        path using LARS algorithm.\r\n"]
[1416.521402, "o", "    79\t    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n"]
[1416.540345, "o", "    80\t    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n"]
[1416.559287, "o", "    81\t    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n"]
[1416.57823, "o", "    82\t        dictionary.\r\n"]
[1416.597172, "o", "    83\t    \"\"\"\r\n"]
[1416.616115, "o", "    84\t    if check_input:\r\n"]
[1416.635057, "o", "    85\t        if algorithm == \"lasso_cd\":\r\n"]
[1416.654, "o", "    86\t            dictionary = check_array(\r\n"]
[1416.672943, "o", "    87\t                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n"]
[1416.691885, "o", "    88\t            )\r\n"]
[1416.710828, "o", "    89\t            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n"]
[1416.72977, "o", "    90\t        else:\r\n"]
[1416.748713, "o", "    91\t            dictionary = check_array(dictionary)\r\n"]
[1416.767655, "o", "    92\t            X = check_array(X)\r\n"]
[1416.786598, "o", "    93\t\r\n"]
[1416.80554, "o", "    94\t    if dictionary.shape[1] != X.shape[1]:\r\n"]
[1416.824483, "o", "    95\t        raise ValueError(\r\n"]
[1416.843425, "o", "    96\t            \"Dictionary and X have different numbers of features:\"\r\n"]
[1416.862368, "o", "    97\t            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n"]
[1416.88131, "o", "    98\t        )\r\n"]
[1416.900253, "o", "    99\t\r\n"]
[1416.919195, "o", "   100\t    _check_positive_coding(algorithm, positive)\r\n"]
[1416.938138, "o", "   101\t\r\n"]
[1416.95708, "o", "   102\t    return _sparse_encode(\r\n"]
[1416.976023, "o", "   103\t        X,\r\n"]
[1416.994966, "o", "   104\t        dictionary,\r\n"]
[1417.013908, "o", "   105\t        gram=gram,\r\n"]
[1417.032851, "o", "   106\t        cov=cov,\r\n"]
[1417.051793, "o", "   107\t        algorithm=algorithm,\r\n"]
[1417.070736, "o", "   108\t        n_nonzero_coefs=n_nonzero_coefs,\r\n"]
[1417.089678, "o", "   109\t        alpha=alpha,\r\n"]
[1417.108621, "o", "   110\t        copy_cov=copy_cov,\r\n"]
[1417.127563, "o", "   111\t        init=init,\r\n"]
[1417.146506, "o", "   112\t        max_iter=max_iter,\r\n"]
[1417.165448, "o", "   113\t        n_jobs=n_jobs,\r\n"]
[1417.184391, "o", "   114\t        verbose=verbose,\r\n"]
[1417.203333, "o", "   115\t        positive=positive,\r\n"]
[1417.222276, "o", "   116\t    )\r\n"]
[1417.241218, "o", "   117\t\r\n"]
[1417.260161, "o", "   118\t\r\n"]
[1417.279103, "o", "   119\tdef _sparse_encode(\r\n"]
[1417.298046, "o", "   120\t    X,\r\n"]
[1417.316989, "o", "   121\t    dictionary,\r\n"]
[1417.335931, "o", "   122\t    *,\r\n"]
[1417.354874, "o", "   123\t    gram=None,\r\n"]
[1417.373816, "o", "   124\t    cov=None,\r\n"]
[1417.392759, "o", "   125\t    algorithm=\"lasso_lars\",\r\n"]
[1417.411701, "o", "   126\t    n_nonzero_coefs=None,\r\n"]
[1417.430644, "o", "   127\t    alpha=None,\r\n"]
[1417.449586, "o", "   128\t    copy_cov=True,\r\n"]
[1417.468529, "o", "   129\t    init=None,\r\n"]
[1417.487471, "o", "   130\t    max_iter=1000,\r\n"]
[1417.506414, "o", "   131\t    n_jobs=None,\r\n"]
[1417.525356, "o", "   132\t    verbose=0,\r\n"]
[1417.544299, "o", "   133\t    positive=False,\r\n"]
[1417.563241, "o", "   134\t):\r\n"]
[1417.582184, "o", "   135\t    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[1417.601126, "o", "   136\t\r\n"]
[1417.620069, "o", "   137\t    n_samples, n_features = X.shape\r\n"]
[1417.639011, "o", "   138\t    n_components = dictionary.shape[0]\r\n"]
[1417.657954, "o", "   139\t\r\n"]
[1417.676897, "o", "   140\t    if algorithm in (\"lars\", \"omp\"):\r\n"]
[1417.695839, "o", "   141\t        regularization = n_nonzero_coefs\r\n"]
[1417.714782, "o", "   142\t        if regularization is None:\r\n"]
[1417.733724, "o", "   143\t            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[1417.752667, "o", "   144\t    else:\r\n"]
[1417.771609, "o", "   145\t        regularization = alpha\r\n"]
[1417.790552, "o", "   146\t        if regularization is None:\r\n"]
[1417.809494, "o", "   147\t            regularization = 1.0\r\n"]
[1417.828437, "o", "   148\t\r\n"]
[1417.847379, "o", "   149\t    if gram is None and algorithm != \"threshold\":\r\n"]
[1417.866322, "o", "   150\t        gram = np.dot(dictionary, dictionary.T)\r\n"]
[1417.885264, "o", "   151\t\r\n"]
[1417.904207, "o", "   152\t    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[1417.923149, "o", "   153\t        copy_cov = False\r\n"]
[1417.942092, "o", "   154\t        cov = np.dot(dictionary, X.T)\r\n"]
[1417.961034, "o", "   155\t\r\n"]
[1417.979977, "o", "   156\t    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[1417.99892, "o", "   157\t        code = _sparse_encode_precomputed(\r\n"]
[1418.017862, "o", "   158\t            X,\r\n"]
[1418.036805, "o", "   159\t            dictionary,\r\n"]
[1418.055747, "o", "   160\t            gram=gram,\r\n"]
[1418.07469, "o", "   161\t            cov=cov,\r\n"]
[1418.093632, "o", "   162\t            algorithm=algorithm,\r\n"]
[1418.112575, "o", "   163\t            regularization=regularization,\r\n"]
[1418.131517, "o", "   164\t            copy_cov=copy_cov,\r\n"]
[1418.15046, "o", "   165\t            init=init,\r\n"]
[1418.169402, "o", "   166\t            max_iter=max_iter,\r\n"]
[1418.188345, "o", "   167\t            verbose=verbose,\r\n"]
[1418.207287, "o", "   168\t            positive=positive,\r\n"]
[1418.22623, "o", "   169\t        )\r\n"]
[1418.245172, "o", "   170\t        return code\r\n"]
[1418.264115, "o", "   171\t\r\n"]
[1418.283057, "o", "   172\t    # Enter parallel code block\r\n"]
[1418.302, "o", "   173\t    n_samples = X.shape[0]\r\n"]
[1418.320943, "o", "   174\t    n_components = dictionary.shape[0]\r\n"]
[1418.339885, "o", "   175\t    code = np.empty((n_samples, n_components))\r\n"]
[1418.358828, "o", "   176\t    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[1418.37777, "o", "   177\t\r\n"]
[1418.396713, "o", "   178\t    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[1418.415655, "o", "   179\t        delayed(_sparse_encode_precomputed)(\r\n"]
[1418.434598, "o", "   180\t            X[this_slice],\r\n"]
[1418.45354, "o", "   181\t            dictionary,\r\n"]
[1418.472483, "o", "   182\t            gram=gram,\r\n"]
[1418.491425, "o", "   183\t            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[1418.510368, "o", "   184\t            algorithm=algorithm,\r\n"]
[1418.52931, "o", "   185\t            regularization=regularization,\r\n"]
[1418.548253, "o", "   186\t            copy_cov=copy_cov,\r\n"]
[1418.567195, "o", "   187\t            init=init[this_slice] if init is not None else None,\r\n"]
[1418.586138, "o", "   188\t            max_iter=max_iter,\r\n"]
[1418.60508, "o", "   189\t            verbose=verbose,\r\n"]
[1418.624023, "o", "   190\t            positive=positive,\r\n"]
[1418.642966, "o", "   191\t        )\r\n"]
[1418.661908, "o", "   192\t        for this_slice in slices\r\n"]
[1418.680851, "o", "   193\t    )\r\n"]
[1418.699793, "o", "   194\t    for this_slice, this_view in zip(slices, code_views):\r\n"]
[1418.718736, "o", "   195\t        code[this_slice] = this_view\r\n"]
[1418.737678, "o", "   196\t    return code\r\n"]
[1418.756621, "o", "   197\t\r\n"]
[1418.775563, "o", "   198\t\r\n"]
[1418.794506, "o", "   199\tdef _update_dict(\r\n"]
[1418.813448, "o", "   200\t    dictionary,\r\n"]
[1418.832391, "o", "   201\t    Y,\r\n"]
[1418.851333, "o", "   202\t    code,\r\n"]
[1418.870276, "o", "   203\t    A=None,\r\n"]
[1418.889218, "o", "   204\t    B=None,\r\n"]
[1418.908161, "o", "   205\t    verbose=False,\r\n"]
[1418.927103, "o", "   206\t    random_state=None,\r\n"]
[1418.946046, "o", "   207\t    positive=False,\r\n"]
[1418.964989, "o", "   208\t):\r\n"]
[1418.983931, "o", "   209\t    \"\"\"Update the dense dictionary factor in place.\r\n"]
[1419.002874, "o", "   210\t\r\n"]
[1419.021816, "o", "   211\t    Parameters\r\n"]
[1419.040759, "o", "   212\t    ----------\r\n"]
[1419.059701, "o", "   213\t    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1419.078644, "o", "   214\t        Value of the dictionary at the previous iteration.\r\n"]
[1419.097586, "o", "   215\t\r\n"]
[1419.116529, "o", "   216\t    Y : ndarray of shape (n_samples, n_features)\r\n"]
[1419.135471, "o", "   217\t        Data matrix.\r\n"]
[1419.154414, "o", "   218\t\r\n"]
[1419.173356, "o", "   219\t    code : ndarray of shape (n_samples, n_components)\r\n"]
[1419.192299, "o", "   220\t        Sparse coding of the data against which to optimize the dictionary.\r\n"]
[1419.211241, "o", "   221\t\r\n"]
[1419.230184, "o", "   222\t    A : ndarray of shape (n_components, n_components), default=None\r\n"]
[1419.249126, "o", "   223\t        Together with `B`, sufficient stats of the online model to update the\r\n"]
[1419.268069, "o", "   224\t        dictionary.\r\n"]
[1419.287011, "o", "   225\t\r\n"]
[1419.305954, "o", "   226\t    B : ndarray of shape (n_features, n_components), default=None\r\n"]
[1419.324897, "o", "   227\t        Together with `A`, sufficient stats of the online model to update the\r\n"]
[1419.343839, "o", "   228\t        dictionary.\r\n"]
[1419.362782, "o", "   229\t\r\n"]
[1419.381724, "o", "   230\t    verbose: bool, default=False\r\n"]
[1419.400667, "o", "   231\t        Degree of output the procedure will print.\r\n"]
[1419.419609, "o", "   232\t\r\n"]
[1419.438552, "o", "   233\t    random_state : int, RandomState instance or None, default=None\r\n"]
[1419.457494, "o", "   234\t        Used for randomly initializing the dictionary. Pass an int for\r\n"]
[1419.476437, "o", "   235\t        reproducible results across multiple function calls.\r\n"]
[1419.495379, "o", "   236\t        See :term:`Glossary <random_state>`.\r\n"]
[1419.514322, "o", "   237\t\r\n"]
[1419.533264, "o", "   238\t    positive : bool, default=False\r\n"]
[1419.552207, "o", "   239\t        Whether to enforce positivity when finding the dictionary.\r\n"]
[1419.571149, "o", "   240\t\r\n"]
[1419.590092, "o", "   241\t        .. versionadded:: 0.20\r\n"]
[1419.609034, "o", "   242\t    \"\"\"\r\n"]
[1419.627977, "o", "   243\t    n_samples, n_components = code.shape\r\n"]
[1419.64692, "o", "   244\t    random_state = check_random_state(random_state)\r\n"]
[1419.665862, "o", "   245\t\r\n"]
[1419.684805, "o", "   246\t    if A is None:\r\n"]
[1419.703747, "o", "   247\t        A = code.T @ code\r\n"]
[1419.72269, "o", "   248\t    if B is None:\r\n"]
[1419.741632, "o", "   249\t        B = Y.T @ code\r\n"]
[1419.760575, "o", "   250\t\r\n"]
[1419.779517, "o", "   251\t    n_unused = 0\r\n"]
[1419.79846, "o", "   252\t\r\n"]
[1419.817402, "o", "   253\t    for k in range(n_components):\r\n"]
[1419.836345, "o", "   254\t        if A[k, k] > 1e-6:\r\n"]
[1419.855287, "o", "   255\t            # 1e-6 is arbitrary but consistent with the spams implementation\r\n"]
[1419.87423, "o", "   256\t            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n"]
[1419.893172, "o", "   257\t        else:\r\n"]
[1419.912115, "o", "   258\t            # kth atom is almost never used -> sample a new one from the data\r\n"]
[1419.931057, "o", "   259\t            newd = Y[random_state.choice(n_samples)]\r\n"]
[1420.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1420.002, "i", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py | nl -ba | sed -n '1,260p'\r"]
[1420.004, "o", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py | nl -ba | sed -n '1,260p'\r\n"]
[1420.024943, "o", "\u001b[?2004l\r\n"]
[1420.043885, "o", "     1\t           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[1420.062828, "o", "     2\t\r\n"]
[1420.08177, "o", "     3\t    return_n_iter : bool, default=False\r\n"]
[1420.100713, "o", "     4\t        Whether or not to return the number of iterations.\r\n"]
[1420.119655, "o", "     5\t\r\n"]
[1420.138598, "o", "     6\t        .. deprecated:: 1.1\r\n"]
[1420.15754, "o", "     7\t           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n"]
[1420.176483, "o", "     8\t\r\n"]
[1420.195425, "o", "     9\t    positive_dict : bool, default=False\r\n"]
[1420.214368, "o", "    10\t        Whether to enforce positivity when finding the dictionary.\r\n"]
[1420.23331, "o", "    11\t\r\n"]
[1420.252253, "o", "    12\t        .. versionadded:: 0.20\r\n"]
[1420.271195, "o", "    13\t\r\n"]
[1420.290138, "o", "    14\t    positive_code : bool, default=False\r\n"]
[1420.30908, "o", "    15\t        Whether to enforce positivity when finding the code.\r\n"]
[1420.328023, "o", "    16\t\r\n"]
[1420.346966, "o", "    17\t        .. versionadded:: 0.20\r\n"]
[1420.365908, "o", "    18\t\r\n"]
[1420.384851, "o", "    19\t    method_max_iter : int, default=1000\r\n"]
[1420.403793, "o", "    20\t        Maximum number of iterations to perform when solving the lasso problem.\r\n"]
[1420.422736, "o", "    21\t\r\n"]
[1420.441678, "o", "    22\t        .. versionadded:: 0.22\r\n"]
[1420.460621, "o", "    23\t\r\n"]
[1420.479563, "o", "    24\t    tol : float, default=1e-3\r\n"]
[1420.498506, "o", "    25\t        Control early stopping based on the norm of the differences in the\r\n"]
[1420.517448, "o", "    26\t        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[1420.536391, "o", "    27\t\r\n"]
[1420.555333, "o", "    28\t        To disable early stopping based on changes in the dictionary, set\r\n"]
[1420.574276, "o", "    29\t        `tol` to 0.0.\r\n"]
[1420.593218, "o", "    30\t\r\n"]
[1420.612161, "o", "    31\t        .. versionadded:: 1.1\r\n"]
[1420.631103, "o", "    32\t\r\n"]
[1420.650046, "o", "    33\t    max_no_improvement : int, default=10\r\n"]
[1420.668989, "o", "    34\t        Control early stopping based on the consecutive number of mini batches\r\n"]
[1420.687931, "o", "    35\t        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[1420.706874, "o", "    36\t        `max_iter` is not None.\r\n"]
[1420.725816, "o", "    37\t\r\n"]
[1420.744759, "o", "    38\t        To disable convergence detection based on cost function, set\r\n"]
[1420.763701, "o", "    39\t        `max_no_improvement` to None.\r\n"]
[1420.782644, "o", "    40\t\r\n"]
[1420.801586, "o", "    41\t        .. versionadded:: 1.1\r\n"]
[1420.820529, "o", "    42\t\r\n"]
[1420.839471, "o", "    43\t    Returns\r\n"]
[1420.858414, "o", "    44\t    -------\r\n"]
[1420.877356, "o", "    45\t    code : ndarray of shape (n_samples, n_components),\r\n"]
[1420.896299, "o", "    46\t        The sparse code (only returned if `return_code=True`).\r\n"]
[1420.915241, "o", "    47\t\r\n"]
[1420.934184, "o", "    48\t    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[1420.953126, "o", "    49\t        The solutions to the dictionary learning problem.\r\n"]
[1420.972069, "o", "    50\t\r\n"]
[1420.991011, "o", "    51\t    n_iter : int\r\n"]
[1421.009954, "o", "    52\t        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[1421.028897, "o", "    53\t        set to `True`.\r\n"]
[1421.047839, "o", "    54\t\r\n"]
[1421.066782, "o", "    55\t    See Also\r\n"]
[1421.085724, "o", "    56\t    --------\r\n"]
[1421.104667, "o", "    57\t    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n"]
[1421.123609, "o", "    58\t    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1421.142552, "o", "    59\t    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n"]
[1421.161494, "o", "    60\t        learning algorithm.\r\n"]
[1421.180437, "o", "    61\t    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1421.199379, "o", "    62\t    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1421.218322, "o", "    63\t    \"\"\"\r\n"]
[1421.237264, "o", "    64\t    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n"]
[1421.256207, "o", "    65\t    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n"]
[1421.275149, "o", "    66\t        raise ValueError(\r\n"]
[1421.294092, "o", "    67\t            \"The following arguments are incompatible with 'max_iter': \"\r\n"]
[1421.313034, "o", "    68\t            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n"]
[1421.331977, "o", "    69\t        )\r\n"]
[1421.35092, "o", "    70\t\r\n"]
[1421.369862, "o", "    71\t    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n"]
[1421.388805, "o", "    72\t    return_inner_stats = _check_warn_deprecated(\r\n"]
[1421.407747, "o", "    73\t        return_inner_stats,\r\n"]
[1421.42669, "o", "    74\t        \"return_inner_stats\",\r\n"]
[1421.445632, "o", "    75\t        default=False,\r\n"]
[1421.464575, "o", "    76\t        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n"]
[1421.483517, "o", "    77\t    )\r\n"]
[1421.50246, "o", "    78\t    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n"]
[1421.521402, "o", "    79\t    return_n_iter = _check_warn_deprecated(\r\n"]
[1421.540345, "o", "    80\t        return_n_iter,\r\n"]
[1421.559287, "o", "    81\t        \"return_n_iter\",\r\n"]
[1421.57823, "o", "    82\t        default=False,\r\n"]
[1421.597172, "o", "    83\t        additional_message=(\r\n"]
[1421.616115, "o", "    84\t            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n"]
[1421.635057, "o", "    85\t            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n"]
[1421.654, "o", "    86\t        ),\r\n"]
[1421.672943, "o", "    87\t    )\r\n"]
[1421.691885, "o", "    88\t\r\n"]
[1421.710828, "o", "    89\t    if max_iter is not None:\r\n"]
[1421.72977, "o", "    90\t        transform_algorithm = \"lasso_\" + method\r\n"]
[1421.748713, "o", "    91\t\r\n"]
[1421.767655, "o", "    92\t        est = MiniBatchDictionaryLearning(\r\n"]
[1421.786598, "o", "    93\t            n_components=n_components,\r\n"]
[1421.80554, "o", "    94\t            alpha=alpha,\r\n"]
[1421.824483, "o", "    95\t            n_iter=n_iter,\r\n"]
[1421.843425, "o", "    96\t            n_jobs=n_jobs,\r\n"]
[1421.862368, "o", "    97\t            fit_algorithm=method,\r\n"]
[1421.88131, "o", "    98\t            batch_size=batch_size,\r\n"]
[1421.900253, "o", "    99\t            shuffle=shuffle,\r\n"]
[1421.919195, "o", "   100\t            dict_init=dict_init,\r\n"]
[1421.938138, "o", "   101\t            random_state=random_state,\r\n"]
[1421.95708, "o", "   102\t            transform_algorithm=transform_algorithm,\r\n"]
[1421.976023, "o", "   103\t            transform_alpha=alpha,\r\n"]
[1421.994966, "o", "   104\t            positive_code=positive_code,\r\n"]
[1422.013908, "o", "   105\t            positive_dict=positive_dict,\r\n"]
[1422.032851, "o", "   106\t            transform_max_iter=method_max_iter,\r\n"]
[1422.051793, "o", "   107\t            verbose=verbose,\r\n"]
[1422.070736, "o", "   108\t            callback=callback,\r\n"]
[1422.089678, "o", "   109\t            tol=tol,\r\n"]
[1422.108621, "o", "   110\t            max_no_improvement=max_no_improvement,\r\n"]
[1422.127563, "o", "   111\t        ).fit(X)\r\n"]
[1422.146506, "o", "   112\t\r\n"]
[1422.165448, "o", "   113\t        if not return_code:\r\n"]
[1422.184391, "o", "   114\t            return est.components_\r\n"]
[1422.203333, "o", "   115\t        else:\r\n"]
[1422.222276, "o", "   116\t            code = est.transform(X)\r\n"]
[1422.241218, "o", "   117\t            return code, est.components_\r\n"]
[1422.260161, "o", "   118\t\r\n"]
[1422.279103, "o", "   119\t    # TODO(1.4) remove the whole old behavior\r\n"]
[1422.298046, "o", "   120\t    # Fallback to old behavior\r\n"]
[1422.316989, "o", "   121\t\r\n"]
[1422.335931, "o", "   122\t    n_iter = _check_warn_deprecated(\r\n"]
[1422.354874, "o", "   123\t        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n"]
[1422.373816, "o", "   124\t    )\r\n"]
[1422.392759, "o", "   125\t\r\n"]
[1422.411701, "o", "   126\t    if n_components is None:\r\n"]
[1422.430644, "o", "   127\t        n_components = X.shape[1]\r\n"]
[1422.449586, "o", "   128\t\r\n"]
[1422.468529, "o", "   129\t    if method not in (\"lars\", \"cd\"):\r\n"]
[1422.487471, "o", "   130\t        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n"]
[1422.506414, "o", "   131\t\r\n"]
[1422.525356, "o", "   132\t    _check_positive_coding(method, positive_code)\r\n"]
[1422.544299, "o", "   133\t\r\n"]
[1422.563241, "o", "   134\t    method = \"lasso_\" + method\r\n"]
[1422.582184, "o", "   135\t\r\n"]
[1422.601126, "o", "   136\t    t0 = time.time()\r\n"]
[1422.620069, "o", "   137\t    n_samples, n_features = X.shape\r\n"]
[1422.639011, "o", "   138\t    # Avoid integer division problems\r\n"]
[1422.657954, "o", "   139\t    alpha = float(alpha)\r\n"]
[1422.676897, "o", "   140\t    random_state = check_random_state(random_state)\r\n"]
[1422.695839, "o", "   141\t\r\n"]
[1422.714782, "o", "   142\t    # Init V with SVD of X\r\n"]
[1422.733724, "o", "   143\t    if dict_init is not None:\r\n"]
[1422.752667, "o", "   144\t        dictionary = dict_init\r\n"]
[1422.771609, "o", "   145\t    else:\r\n"]
[1422.790552, "o", "   146\t        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n"]
[1422.809494, "o", "   147\t        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[1422.828437, "o", "   148\t    r = len(dictionary)\r\n"]
[1422.847379, "o", "   149\t    if n_components <= r:\r\n"]
[1422.866322, "o", "   150\t        dictionary = dictionary[:n_components, :]\r\n"]
[1422.885264, "o", "   151\t    else:\r\n"]
[1422.904207, "o", "   152\t        dictionary = np.r_[\r\n"]
[1422.923149, "o", "   153\t            dictionary,\r\n"]
[1422.942092, "o", "   154\t            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n"]
[1422.961034, "o", "   155\t        ]\r\n"]
[1422.979977, "o", "   156\t\r\n"]
[1422.99892, "o", "   157\t    if verbose == 1:\r\n"]
[1423.017862, "o", "   158\t        print(\"[dict_learning]\", end=\" \")\r\n"]
[1423.036805, "o", "   159\t\r\n"]
[1423.055747, "o", "   160\t    if shuffle:\r\n"]
[1423.07469, "o", "   161\t        X_train = X.copy()\r\n"]
[1423.093632, "o", "   162\t        random_state.shuffle(X_train)\r\n"]
[1423.112575, "o", "   163\t    else:\r\n"]
[1423.131517, "o", "   164\t        X_train = X\r\n"]
[1423.15046, "o", "   165\t\r\n"]
[1423.169402, "o", "   166\t    X_train = check_array(\r\n"]
[1423.188345, "o", "   167\t        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n"]
[1423.207287, "o", "   168\t    )\r\n"]
[1423.22623, "o", "   169\t\r\n"]
[1423.245172, "o", "   170\t    # Fortran-order dict better suited for the sparse coding which is the\r\n"]
[1423.264115, "o", "   171\t    # bottleneck of this algorithm.\r\n"]
[1423.283057, "o", "   172\t    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n"]
[1423.302, "o", "   173\t    dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[1423.320943, "o", "   174\t\r\n"]
[1423.339885, "o", "   175\t    batches = gen_batches(n_samples, batch_size)\r\n"]
[1423.358828, "o", "   176\t    batches = itertools.cycle(batches)\r\n"]
[1423.37777, "o", "   177\t\r\n"]
[1423.396713, "o", "   178\t    # The covariance of the dictionary\r\n"]
[1423.415655, "o", "   179\t    if inner_stats is None:\r\n"]
[1423.434598, "o", "   180\t        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n"]
[1423.45354, "o", "   181\t        # The data approximation\r\n"]
[1423.472483, "o", "   182\t        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n"]
[1423.491425, "o", "   183\t    else:\r\n"]
[1423.510368, "o", "   184\t        A = inner_stats[0].copy()\r\n"]
[1423.52931, "o", "   185\t        B = inner_stats[1].copy()\r\n"]
[1423.548253, "o", "   186\t\r\n"]
[1423.567195, "o", "   187\t    # If n_iter is zero, we need to return zero.\r\n"]
[1423.586138, "o", "   188\t    ii = iter_offset - 1\r\n"]
[1423.60508, "o", "   189\t\r\n"]
[1423.624023, "o", "   190\t    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n"]
[1423.642966, "o", "   191\t        this_X = X_train[batch]\r\n"]
[1423.661908, "o", "   192\t        dt = time.time() - t0\r\n"]
[1423.680851, "o", "   193\t        if verbose == 1:\r\n"]
[1423.699793, "o", "   194\t            sys.stdout.write(\".\")\r\n"]
[1423.718736, "o", "   195\t            sys.stdout.flush()\r\n"]
[1423.737678, "o", "   196\t        elif verbose:\r\n"]
[1423.756621, "o", "   197\t            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n"]
[1423.775563, "o", "   198\t                print(\r\n"]
[1423.794506, "o", "   199\t                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n"]
[1423.813448, "o", "   200\t                )\r\n"]
[1423.832391, "o", "   201\t\r\n"]
[1423.851333, "o", "   202\t        this_code = sparse_encode(\r\n"]
[1423.870276, "o", "   203\t            this_X,\r\n"]
[1423.889218, "o", "   204\t            dictionary,\r\n"]
[1423.908161, "o", "   205\t            algorithm=method,\r\n"]
[1423.927103, "o", "   206\t            alpha=alpha,\r\n"]
[1423.946046, "o", "   207\t            n_jobs=n_jobs,\r\n"]
[1423.964989, "o", "   208\t            check_input=False,\r\n"]
[1423.983931, "o", "   209\t            positive=positive_code,\r\n"]
[1424.002874, "o", "   210\t            max_iter=method_max_iter,\r\n"]
[1424.021816, "o", "   211\t            verbose=verbose,\r\n"]
[1424.040759, "o", "   212\t        )\r\n"]
[1424.059701, "o", "   213\t\r\n"]
[1424.078644, "o", "   214\t        # Update the auxiliary variables\r\n"]
[1424.097586, "o", "   215\t        if ii < batch_size - 1:\r\n"]
[1424.116529, "o", "   216\t            theta = float((ii + 1) * batch_size)\r\n"]
[1424.135471, "o", "   217\t        else:\r\n"]
[1424.154414, "o", "   218\t            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n"]
[1424.173356, "o", "   219\t        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[1424.192299, "o", "   220\t\r\n"]
[1424.211241, "o", "   221\t        A *= beta\r\n"]
[1424.230184, "o", "   222\t        A += np.dot(this_code.T, this_code)\r\n"]
[1424.249126, "o", "   223\t        B *= beta\r\n"]
[1424.268069, "o", "   224\t        B += np.dot(this_X.T, this_code)\r\n"]
[1424.287011, "o", "   225\t\r\n"]
[1424.305954, "o", "   226\t        # Update dictionary in place\r\n"]
[1424.324897, "o", "   227\t        _update_dict(\r\n"]
[1424.343839, "o", "   228\t            dictionary,\r\n"]
[1424.362782, "o", "   229\t            this_X,\r\n"]
[1424.381724, "o", "   230\t            this_code,\r\n"]
[1424.400667, "o", "   231\t            A,\r\n"]
[1424.419609, "o", "   232\t            B,\r\n"]
[1424.438552, "o", "   233\t            verbose=verbose,\r\n"]
[1424.457494, "o", "   234\t            random_state=random_state,\r\n"]
[1424.476437, "o", "   235\t            positive=positive_dict,\r\n"]
[1424.495379, "o", "   236\t        )\r\n"]
[1424.514322, "o", "   237\t\r\n"]
[1424.533264, "o", "   238\t        # Maybe we need a stopping criteria based on the amount of\r\n"]
[1424.552207, "o", "   239\t        # modification in the dictionary\r\n"]
[1424.571149, "o", "   240\t        if callback is not None:\r\n"]
[1424.590092, "o", "   241\t            callback(locals())\r\n"]
[1424.609034, "o", "   242\t\r\n"]
[1424.627977, "o", "   243\t    if return_inner_stats:\r\n"]
[1424.64692, "o", "   244\t        if return_n_iter:\r\n"]
[1424.665862, "o", "   245\t            return dictionary, (A, B), ii - iter_offset + 1\r\n"]
[1424.684805, "o", "   246\t        else:\r\n"]
[1424.703747, "o", "   247\t            return dictionary, (A, B)\r\n"]
[1424.72269, "o", "   248\t    if return_code:\r\n"]
[1424.741632, "o", "   249\t        if verbose > 1:\r\n"]
[1424.760575, "o", "   250\t            print(\"Learning code...\", end=\" \")\r\n"]
[1424.779517, "o", "   251\t        elif verbose == 1:\r\n"]
[1424.79846, "o", "   252\t            print(\"|\", end=\" \")\r\n"]
[1424.817402, "o", "   253\t        code = sparse_encode(\r\n"]
[1424.836345, "o", "   254\t            X,\r\n"]
[1424.855287, "o", "   255\t            dictionary,\r\n"]
[1424.87423, "o", "   256\t            algorithm=method,\r\n"]
[1424.893172, "o", "   257\t            alpha=alpha,\r\n"]
[1424.912115, "o", "   258\t            n_jobs=n_jobs,\r\n"]
[1424.931057, "o", "   259\t            check_input=False,\r\n"]
[1425.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1425.002, "i", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py | nl -ba | sed -n '1,260p'\r"]
[1425.004, "o", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py | nl -ba | sed -n '1,260p'\r\n"]
[1425.024943, "o", "\u001b[?2004l\r\n"]
[1425.043885, "o", "     1\t\r\n"]
[1425.062828, "o", "     2\t    n_jobs : int or None, default=None\r\n"]
[1425.08177, "o", "     3\t        Number of parallel jobs to run.\r\n"]
[1425.100713, "o", "     4\t        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1425.119655, "o", "     5\t        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1425.138598, "o", "     6\t        for more details.\r\n"]
[1425.15754, "o", "     7\t\r\n"]
[1425.176483, "o", "     8\t    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1425.195425, "o", "     9\t        Initial value for the code, for warm restart. Only used if `code_init`\r\n"]
[1425.214368, "o", "    10\t        and `dict_init` are not None.\r\n"]
[1425.23331, "o", "    11\t\r\n"]
[1425.252253, "o", "    12\t    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1425.271195, "o", "    13\t        Initial values for the dictionary, for warm restart. Only used if\r\n"]
[1425.290138, "o", "    14\t        `code_init` and `dict_init` are not None.\r\n"]
[1425.30908, "o", "    15\t\r\n"]
[1425.328023, "o", "    16\t    callback : callable, default=None\r\n"]
[1425.346966, "o", "    17\t        Callable that gets invoked every five iterations.\r\n"]
[1425.365908, "o", "    18\t\r\n"]
[1425.384851, "o", "    19\t        .. versionadded:: 1.3\r\n"]
[1425.403793, "o", "    20\t\r\n"]
[1425.422736, "o", "    21\t    verbose : bool, default=False\r\n"]
[1425.441678, "o", "    22\t        To control the verbosity of the procedure.\r\n"]
[1425.460621, "o", "    23\t\r\n"]
[1425.479563, "o", "    24\t    split_sign : bool, default=False\r\n"]
[1425.498506, "o", "    25\t        Whether to split the sparse feature vector into the concatenation of\r\n"]
[1425.517448, "o", "    26\t        its negative part and its positive part. This can improve the\r\n"]
[1425.536391, "o", "    27\t        performance of downstream classifiers.\r\n"]
[1425.555333, "o", "    28\t\r\n"]
[1425.574276, "o", "    29\t    random_state : int, RandomState instance or None, default=None\r\n"]
[1425.593218, "o", "    30\t        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[1425.612161, "o", "    31\t        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[1425.631103, "o", "    32\t        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[1425.650046, "o", "    33\t        results across multiple function calls.\r\n"]
[1425.668989, "o", "    34\t        See :term:`Glossary <random_state>`.\r\n"]
[1425.687931, "o", "    35\t\r\n"]
[1425.706874, "o", "    36\t    positive_code : bool, default=False\r\n"]
[1425.725816, "o", "    37\t        Whether to enforce positivity when finding the code.\r\n"]
[1425.744759, "o", "    38\t\r\n"]
[1425.763701, "o", "    39\t        .. versionadded:: 0.20\r\n"]
[1425.782644, "o", "    40\t\r\n"]
[1425.801586, "o", "    41\t    positive_dict : bool, default=False\r\n"]
[1425.820529, "o", "    42\t        Whether to enforce positivity when finding the dictionary.\r\n"]
[1425.839471, "o", "    43\t\r\n"]
[1425.858414, "o", "    44\t        .. versionadded:: 0.20\r\n"]
[1425.877356, "o", "    45\t\r\n"]
[1425.896299, "o", "    46\t    transform_max_iter : int, default=1000\r\n"]
[1425.915241, "o", "    47\t        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1425.934184, "o", "    48\t        `'lasso_lars'`.\r\n"]
[1425.953126, "o", "    49\t\r\n"]
[1425.972069, "o", "    50\t        .. versionadded:: 0.22\r\n"]
[1425.991011, "o", "    51\t\r\n"]
[1426.009954, "o", "    52\t    Attributes\r\n"]
[1426.028897, "o", "    53\t    ----------\r\n"]
[1426.047839, "o", "    54\t    components_ : ndarray of shape (n_components, n_features)\r\n"]
[1426.066782, "o", "    55\t        dictionary atoms extracted from the data\r\n"]
[1426.085724, "o", "    56\t\r\n"]
[1426.104667, "o", "    57\t    error_ : array\r\n"]
[1426.123609, "o", "    58\t        vector of errors at each iteration\r\n"]
[1426.142552, "o", "    59\t\r\n"]
[1426.161494, "o", "    60\t    n_features_in_ : int\r\n"]
[1426.180437, "o", "    61\t        Number of features seen during :term:`fit`.\r\n"]
[1426.199379, "o", "    62\t\r\n"]
[1426.218322, "o", "    63\t        .. versionadded:: 0.24\r\n"]
[1426.237264, "o", "    64\t\r\n"]
[1426.256207, "o", "    65\t    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[1426.275149, "o", "    66\t        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[1426.294092, "o", "    67\t        has feature names that are all strings.\r\n"]
[1426.313034, "o", "    68\t\r\n"]
[1426.331977, "o", "    69\t        .. versionadded:: 1.0\r\n"]
[1426.35092, "o", "    70\t\r\n"]
[1426.369862, "o", "    71\t    n_iter_ : int\r\n"]
[1426.388805, "o", "    72\t        Number of iterations run.\r\n"]
[1426.407747, "o", "    73\t\r\n"]
[1426.42669, "o", "    74\t    See Also\r\n"]
[1426.445632, "o", "    75\t    --------\r\n"]
[1426.464575, "o", "    76\t    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n"]
[1426.483517, "o", "    77\t        dictionary learning algorithm.\r\n"]
[1426.50246, "o", "    78\t    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1426.521402, "o", "    79\t    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[1426.540345, "o", "    80\t        precomputed dictionary.\r\n"]
[1426.559287, "o", "    81\t    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1426.57823, "o", "    82\t\r\n"]
[1426.597172, "o", "    83\t    References\r\n"]
[1426.616115, "o", "    84\t    ----------\r\n"]
[1426.635057, "o", "    85\t\r\n"]
[1426.654, "o", "    86\t    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[1426.672943, "o", "    87\t    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[1426.691885, "o", "    88\t\r\n"]
[1426.710828, "o", "    89\t    Examples\r\n"]
[1426.72977, "o", "    90\t    --------\r\n"]
[1426.748713, "o", "    91\t    >>> import numpy as np\r\n"]
[1426.767655, "o", "    92\t    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[1426.786598, "o", "    93\t    >>> from sklearn.decomposition import DictionaryLearning\r\n"]
[1426.80554, "o", "    94\t    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[1426.824483, "o", "    95\t    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[1426.843425, "o", "    96\t    ...     random_state=42,\r\n"]
[1426.862368, "o", "    97\t    ... )\r\n"]
[1426.88131, "o", "    98\t    >>> dict_learner = DictionaryLearning(\r\n"]
[1426.900253, "o", "    99\t    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n"]
[1426.919195, "o", "   100\t    ...     random_state=42,\r\n"]
[1426.938138, "o", "   101\t    ... )\r\n"]
[1426.95708, "o", "   102\t    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n"]
[1426.976023, "o", "   103\t\r\n"]
[1426.994966, "o", "   104\t    We can check the level of sparsity of `X_transformed`:\r\n"]
[1427.013908, "o", "   105\t\r\n"]
[1427.032851, "o", "   106\t    >>> np.mean(X_transformed == 0)\r\n"]
[1427.051793, "o", "   107\t    0.41...\r\n"]
[1427.070736, "o", "   108\t\r\n"]
[1427.089678, "o", "   109\t    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[1427.108621, "o", "   110\t    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[1427.127563, "o", "   111\t    the original signal:\r\n"]
[1427.146506, "o", "   112\t\r\n"]
[1427.165448, "o", "   113\t    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[1427.184391, "o", "   114\t    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[1427.203333, "o", "   115\t    0.07...\r\n"]
[1427.222276, "o", "   116\t    \"\"\"\r\n"]
[1427.241218, "o", "   117\t\r\n"]
[1427.260161, "o", "   118\t    _parameter_constraints: dict = {\r\n"]
[1427.279103, "o", "   119\t        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1427.298046, "o", "   120\t        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1427.316989, "o", "   121\t        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1427.335931, "o", "   122\t        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1427.354874, "o", "   123\t        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[1427.373816, "o", "   124\t        \"transform_algorithm\": [\r\n"]
[1427.392759, "o", "   125\t            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[1427.411701, "o", "   126\t        ],\r\n"]
[1427.430644, "o", "   127\t        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1427.449586, "o", "   128\t        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[1427.468529, "o", "   129\t        \"n_jobs\": [Integral, None],\r\n"]
[1427.487471, "o", "   130\t        \"code_init\": [np.ndarray, None],\r\n"]
[1427.506414, "o", "   131\t        \"dict_init\": [np.ndarray, None],\r\n"]
[1427.525356, "o", "   132\t        \"callback\": [callable, None],\r\n"]
[1427.544299, "o", "   133\t        \"verbose\": [\"verbose\"],\r\n"]
[1427.563241, "o", "   134\t        \"split_sign\": [\"boolean\"],\r\n"]
[1427.582184, "o", "   135\t        \"random_state\": [\"random_state\"],\r\n"]
[1427.601126, "o", "   136\t        \"positive_code\": [\"boolean\"],\r\n"]
[1427.620069, "o", "   137\t        \"positive_dict\": [\"boolean\"],\r\n"]
[1427.639011, "o", "   138\t        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1427.657954, "o", "   139\t    }\r\n"]
[1427.676897, "o", "   140\t\r\n"]
[1427.695839, "o", "   141\t    def __init__(\r\n"]
[1427.714782, "o", "   142\t        self,\r\n"]
[1427.733724, "o", "   143\t        n_components=None,\r\n"]
[1427.752667, "o", "   144\t        *,\r\n"]
[1427.771609, "o", "   145\t        alpha=1,\r\n"]
[1427.790552, "o", "   146\t        max_iter=1000,\r\n"]
[1427.809494, "o", "   147\t        tol=1e-8,\r\n"]
[1427.828437, "o", "   148\t        fit_algorithm=\"lars\",\r\n"]
[1427.847379, "o", "   149\t        transform_algorithm=\"omp\",\r\n"]
[1427.866322, "o", "   150\t        transform_n_nonzero_coefs=None,\r\n"]
[1427.885264, "o", "   151\t        transform_alpha=None,\r\n"]
[1427.904207, "o", "   152\t        n_jobs=None,\r\n"]
[1427.923149, "o", "   153\t        code_init=None,\r\n"]
[1427.942092, "o", "   154\t        dict_init=None,\r\n"]
[1427.961034, "o", "   155\t        callback=None,\r\n"]
[1427.979977, "o", "   156\t        verbose=False,\r\n"]
[1427.99892, "o", "   157\t        split_sign=False,\r\n"]
[1428.017862, "o", "   158\t        random_state=None,\r\n"]
[1428.036805, "o", "   159\t        positive_code=False,\r\n"]
[1428.055747, "o", "   160\t        positive_dict=False,\r\n"]
[1428.07469, "o", "   161\t        transform_max_iter=1000,\r\n"]
[1428.093632, "o", "   162\t    ):\r\n"]
[1428.112575, "o", "   163\t        super().__init__(\r\n"]
[1428.131517, "o", "   164\t            transform_algorithm,\r\n"]
[1428.15046, "o", "   165\t            transform_n_nonzero_coefs,\r\n"]
[1428.169402, "o", "   166\t            transform_alpha,\r\n"]
[1428.188345, "o", "   167\t            split_sign,\r\n"]
[1428.207287, "o", "   168\t            n_jobs,\r\n"]
[1428.22623, "o", "   169\t            positive_code,\r\n"]
[1428.245172, "o", "   170\t            transform_max_iter,\r\n"]
[1428.264115, "o", "   171\t        )\r\n"]
[1428.283057, "o", "   172\t        self.n_components = n_components\r\n"]
[1428.302, "o", "   173\t        self.alpha = alpha\r\n"]
[1428.320943, "o", "   174\t        self.max_iter = max_iter\r\n"]
[1428.339885, "o", "   175\t        self.tol = tol\r\n"]
[1428.358828, "o", "   176\t        self.fit_algorithm = fit_algorithm\r\n"]
[1428.37777, "o", "   177\t        self.code_init = code_init\r\n"]
[1428.396713, "o", "   178\t        self.dict_init = dict_init\r\n"]
[1428.415655, "o", "   179\t        self.callback = callback\r\n"]
[1428.434598, "o", "   180\t        self.verbose = verbose\r\n"]
[1428.45354, "o", "   181\t        self.random_state = random_state\r\n"]
[1428.472483, "o", "   182\t        self.positive_dict = positive_dict\r\n"]
[1428.491425, "o", "   183\t\r\n"]
[1428.510368, "o", "   184\t    def fit(self, X, y=None):\r\n"]
[1428.52931, "o", "   185\t        \"\"\"Fit the model from data in X.\r\n"]
[1428.548253, "o", "   186\t\r\n"]
[1428.567195, "o", "   187\t        Parameters\r\n"]
[1428.586138, "o", "   188\t        ----------\r\n"]
[1428.60508, "o", "   189\t        X : array-like of shape (n_samples, n_features)\r\n"]
[1428.624023, "o", "   190\t            Training vector, where `n_samples` is the number of samples\r\n"]
[1428.642966, "o", "   191\t            and `n_features` is the number of features.\r\n"]
[1428.661908, "o", "   192\t\r\n"]
[1428.680851, "o", "   193\t        y : Ignored\r\n"]
[1428.699793, "o", "   194\t            Not used, present for API consistency by convention.\r\n"]
[1428.718736, "o", "   195\t\r\n"]
[1428.737678, "o", "   196\t        Returns\r\n"]
[1428.756621, "o", "   197\t        -------\r\n"]
[1428.775563, "o", "   198\t        self : object\r\n"]
[1428.794506, "o", "   199\t            Returns the instance itself.\r\n"]
[1428.813448, "o", "   200\t        \"\"\"\r\n"]
[1428.832391, "o", "   201\t        self.fit_transform(X)\r\n"]
[1428.851333, "o", "   202\t        return self\r\n"]
[1428.870276, "o", "   203\t\r\n"]
[1428.889218, "o", "   204\t    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[1428.908161, "o", "   205\t    def fit_transform(self, X, y=None):\r\n"]
[1428.927103, "o", "   206\t        \"\"\"Fit the model from data in X and return the transformed data.\r\n"]
[1428.946046, "o", "   207\t\r\n"]
[1428.964989, "o", "   208\t        Parameters\r\n"]
[1428.983931, "o", "   209\t        ----------\r\n"]
[1429.002874, "o", "   210\t        X : array-like of shape (n_samples, n_features)\r\n"]
[1429.021816, "o", "   211\t            Training vector, where `n_samples` is the number of samples\r\n"]
[1429.040759, "o", "   212\t            and `n_features` is the number of features.\r\n"]
[1429.059701, "o", "   213\t\r\n"]
[1429.078644, "o", "   214\t        y : Ignored\r\n"]
[1429.097586, "o", "   215\t            Not used, present for API consistency by convention.\r\n"]
[1429.116529, "o", "   216\t\r\n"]
[1429.135471, "o", "   217\t        Returns\r\n"]
[1429.154414, "o", "   218\t        -------\r\n"]
[1429.173356, "o", "   219\t        V : ndarray of shape (n_samples, n_components)\r\n"]
[1429.192299, "o", "   220\t            Transformed data.\r\n"]
[1429.211241, "o", "   221\t        \"\"\"\r\n"]
[1429.230184, "o", "   222\t        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n"]
[1429.249126, "o", "   223\t\r\n"]
[1429.268069, "o", "   224\t        method = \"lasso_\" + self.fit_algorithm\r\n"]
[1429.287011, "o", "   225\t\r\n"]
[1429.305954, "o", "   226\t        random_state = check_random_state(self.random_state)\r\n"]
[1429.324897, "o", "   227\t        X = self._validate_data(X)\r\n"]
[1429.343839, "o", "   228\t\r\n"]
[1429.362782, "o", "   229\t        if self.n_components is None:\r\n"]
[1429.381724, "o", "   230\t            n_components = X.shape[1]\r\n"]
[1429.400667, "o", "   231\t        else:\r\n"]
[1429.419609, "o", "   232\t            n_components = self.n_components\r\n"]
[1429.438552, "o", "   233\t\r\n"]
[1429.457494, "o", "   234\t        V, U, E, self.n_iter_ = _dict_learning(\r\n"]
[1429.476437, "o", "   235\t            X,\r\n"]
[1429.495379, "o", "   236\t            n_components,\r\n"]
[1429.514322, "o", "   237\t            alpha=self.alpha,\r\n"]
[1429.533264, "o", "   238\t            tol=self.tol,\r\n"]
[1429.552207, "o", "   239\t            max_iter=self.max_iter,\r\n"]
[1429.571149, "o", "   240\t            method=method,\r\n"]
[1429.590092, "o", "   241\t            method_max_iter=self.transform_max_iter,\r\n"]
[1429.609034, "o", "   242\t            n_jobs=self.n_jobs,\r\n"]
[1429.627977, "o", "   243\t            code_init=self.code_init,\r\n"]
[1429.64692, "o", "   244\t            dict_init=self.dict_init,\r\n"]
[1429.665862, "o", "   245\t            callback=self.callback,\r\n"]
[1429.684805, "o", "   246\t            verbose=self.verbose,\r\n"]
[1429.703747, "o", "   247\t            random_state=random_state,\r\n"]
[1429.72269, "o", "   248\t            return_n_iter=True,\r\n"]
[1429.741632, "o", "   249\t            positive_dict=self.positive_dict,\r\n"]
[1429.760575, "o", "   250\t            positive_code=self.positive_code,\r\n"]
[1429.779517, "o", "   251\t        )\r\n"]
[1429.79846, "o", "   252\t        self.components_ = U\r\n"]
[1429.817402, "o", "   253\t        self.error_ = E\r\n"]
[1429.836345, "o", "   254\t\r\n"]
[1429.855287, "o", "   255\t        return V\r\n"]
[1429.87423, "o", "   256\t\r\n"]
[1429.893172, "o", "   257\t    @property\r\n"]
[1429.912115, "o", "   258\t    def _n_features_out(self):\r\n"]
[1429.931057, "o", "   259\t        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[1430.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1430.002, "i", "python - << 'PY'\r"]
[1430.004, "o", "python - << 'PY'\r\n"]
[1430.094286, "o", "\u001b[?2004l\r\n"]
[1430.182571, "o", "\u001b[?2004h> import io,sys,re\r\n"]
[1430.270857, "o", "\u001b[?2004l\r\n"]
[1430.359143, "o", "\u001b[?2004h> p='sklearn/decomposition/_dict_learning.py'\r\n"]
[1430.447429, "o", "\u001b[?2004l\r\n"]
[1430.535714, "o", "\u001b[?2004h> s=open(p,'r',encoding='utf-8').read()\r\n"]
[1430.624, "o", "\u001b[?2004l\r\n"]
[1430.712286, "o", "\u001b[?2004h> # Add copy_cov=False at call sites of _sparse_encode_precomputed if not already present\r\n"]
[1430.800571, "o", "\u001b[?2004l\r\n"]
[1430.888857, "o", "\u001b[?2004h> pat=r\"(_sparse_encode_precomputed\\([^\\)]*)\\)\"\r\n"]
[1430.977143, "o", "\u001b[?2004l\r\n"]
[1431.065429, "o", "\u001b[?2004h> changed=False\r\n"]
[1431.153714, "o", "\u001b[?2004l\r\n"]
[1431.242, "o", "\u001b[?2004h> out=[]\r\n"]
[1431.330286, "o", "\u001b[?2004l\r\n"]
[1431.418571, "o", "\u001b[?2004h> i=0\r\n"]
[1431.506857, "o", "\u001b[?2004l\r\n"]
[1431.595143, "o", "\u001b[?2004h> while True:\r\n"]
[1431.683429, "o", "\u001b[?2004l\r\n"]
[1431.771714, "o", "\u001b[?2004h>     m=re.search(pat,s[i:])\r\n"]
[1431.86, "o", "\u001b[?2004l\r\n"]
[1431.948286, "o", "\u001b[?2004h>     if not m:\r\n"]
[1432.036571, "o", "\u001b[?2004l\r\n"]
[1432.124857, "o", "\u001b[?2004h>         break\r\n"]
[1432.213143, "o", "\u001b[?2004l\r\n"]
[1432.301429, "o", "\u001b[?2004h>     a=i+m.start(1); b=i+m.end(1)\r\n"]
[1432.389714, "o", "\u001b[?2004l\r\n"]
[1432.478, "o", "\u001b[?2004h>     call=s[a:b]\r\n"]
[1432.566286, "o", "\u001b[?2004l\r\n"]
[1432.654571, "o", "\u001b[?2004h>     if 'copy_cov' not in call:\r\n"]
[1432.742857, "o", "\u001b[?2004l\r\n"]
[1432.831143, "o", "\u001b[?2004h>         call=call[:-1]+', copy_cov=False)'\r\n"]
[1432.919429, "o", "\u001b[?2004l\r\n"]
[1433.007714, "o", "\u001b[?2004h>         s=s[:a]+call+s[b:]\r\n"]
[1433.096, "o", "\u001b[?2004l\r\n"]
[1433.184286, "o", "\u001b[?2004h>         changed=True\r\n"]
[1433.272571, "o", "\u001b[?2004l\r\n"]
[1433.360857, "o", "\u001b[?2004h>         i=a+len(call)\r\n"]
[1433.449143, "o", "\u001b[?2004l\r\n"]
[1433.537429, "o", "\u001b[?2004h>     else:\r\n"]
[1433.625714, "o", "\u001b[?2004l\r\n"]
[1433.714, "o", "\u001b[?2004h>         i=b\r\n"]
[1433.802286, "o", "\u001b[?2004l\r\n"]
[1433.890571, "o", "\u001b[?2004h> if changed:\r\n"]
[1433.978857, "o", "\u001b[?2004l\r\n"]
[1434.067143, "o", "\u001b[?2004h>     open(p,'w',encoding='utf-8').write(s)\r\n"]
[1434.155429, "o", "\u001b[?2004l\r\n"]
[1434.243714, "o", "\u001b[?2004h>     print('Patched call sites to add copy_cov=False')\r\n"]
[1434.332, "o", "\u001b[?2004l\r\n"]
[1434.420286, "o", "\u001b[?2004h> else:\r\n"]
[1434.508571, "o", "\u001b[?2004l\r\n"]
[1434.596857, "o", "\u001b[?2004h>     print('No patch applied (already present or no call found)')\r\n"]
[1434.685143, "o", "\u001b[?2004l\r\n"]
[1434.773429, "o", "\u001b[?2004h> PY\r\n"]
[1434.861714, "o", "\u001b[?2004l\r\n"]
[1435.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1435.002, "i", "git status -sb\r"]
[1435.004, "o", "git status -sb\r\n"]
[1437.478, "o", "\u001b[?2004l\r\n"]
[1440.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1440.002, "i", "git diff -- sklearn/decomposition/_dict_learning.py | sed -n '1,200p'\r"]
[1440.004, "o", "git diff -- sklearn/decomposition/_dict_learning.py | sed -n '1,200p'\r\n"]
[1440.006, "o", "\u001b[?2004l\r\n"]
[1445.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1445.002, "i", "pytest -q sklearn/decomposition/tests/test_dict_learning.py -k minibatch -q\r"]
[1445.004, "o", "pytest -q sklearn/decomposition/tests/test_dict_learning.py -k minibatch -q\r\n"]
[1447.478, "o", "\u001b[?2004l\r\n"]
[1450.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1450.002, "i", "cd asv_benchmarks\r"]
[1450.004, "o", "cd asv_benchmarks\r\n"]
[1450.006, "o", "\u001b[?2004l\r\n"]
[1455.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1455.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r"]
[1455.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r\n"]
[1455.315, "o", "stderr\r\n"]
[1455.624, "o", "\u001b[?2004l\r\n"]
[1455.933, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[1456.242, "o", "\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n"]
[1456.551, "o", "[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n"]
[1456.86, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n"]
[1457.169, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0m                                                                                     ok\r\n"]
[1457.478, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ============\u001b[0m\r\n"]
[1457.787, "o", "              --                 n_jobs   \r\n"]
[1458.096, "o", "              --------------- ------------\r\n"]
[1458.405, "o", "               fit_algorithm       1      \r\n"]
[1458.714, "o", "              =============== ============\r\n"]
[1459.023, "o", "                    lars       6.69\u00b10.07s \r\n"]
[1459.332, "o", "                     cd        1.61\u00b10.02s \r\n"]
[1459.641, "o", "              =============== ============\r\n"]
[1460.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1460.002, "i", "cd /workspace/repo\r"]
[1460.004, "o", "cd /workspace/repo\r\n"]
[1460.006, "o", "\u001b[?2004l\r\n"]
[1465.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1465.002, "i", "grep -n \"def _minibatch_step\\|_minibatch_step\\(\" sklearn/decomposition/_dict_learning.py\r"]
[1465.004, "o", "grep -n \"def _minibatch_step\\|_minibatch_step\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1467.478, "o", "\u001b[?2004l\r\n"]
[1470.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1470.002, "i", "grep -n \"_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.py\r"]
[1470.004, "o", "grep -n \"_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1472.478, "o", "\u001b[?2004l\r\n"]
[1475.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1475.002, "i", "sed -n '1,900p' sklearn/decomposition/_dict_learning.py\r"]
[1475.004, "o", "sed -n '1,900p' sklearn/decomposition/_dict_learning.py\r\n"]
[1475.016, "o", "\u001b[?2004l\r\n"]
[1475.026, "o", "\"\"\" Dictionary learning.\r\n"]
[1475.036, "o", "\"\"\"\r\n"]
[1475.046, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[1475.056, "o", "# License: BSD 3 clause\r\n"]
[1475.066, "o", "\r\n"]
[1475.076, "o", "import itertools\r\n"]
[1475.086, "o", "import sys\r\n"]
[1475.096, "o", "import time\r\n"]
[1475.106, "o", "import warnings\r\n"]
[1475.116, "o", "from math import ceil\r\n"]
[1475.126, "o", "from numbers import Integral, Real\r\n"]
[1475.136, "o", "\r\n"]
[1475.146, "o", "import numpy as np\r\n"]
[1475.156, "o", "from joblib import effective_n_jobs\r\n"]
[1475.166, "o", "from scipy import linalg\r\n"]
[1475.176, "o", "\r\n"]
[1475.186, "o", "from ..base import (\r\n"]
[1475.196, "o", "    BaseEstimator,\r\n"]
[1475.206, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[1475.216, "o", "    TransformerMixin,\r\n"]
[1475.226, "o", "    _fit_context,\r\n"]
[1475.236, "o", ")\r\n"]
[1475.246, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[1475.256, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[1475.266, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[1475.276, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[1475.286, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[1475.296, "o", "from ..utils.validation import check_is_fitted\r\n"]
[1475.306, "o", "\r\n"]
[1475.316, "o", "\r\n"]
[1475.326, "o", "def _check_positive_coding(method, positive):\r\n"]
[1475.336, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[1475.346, "o", "        raise ValueError(\r\n"]
[1475.356, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[1475.366, "o", "        )\r\n"]
[1475.376, "o", "\r\n"]
[1475.386, "o", "\r\n"]
[1475.396, "o", "def _sparse_encode_precomputed(\r\n"]
[1475.406, "o", "    X,\r\n"]
[1475.416, "o", "    dictionary,\r\n"]
[1475.426, "o", "    *,\r\n"]
[1475.436, "o", "    gram=None,\r\n"]
[1475.446, "o", "    cov=None,\r\n"]
[1475.456, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1475.466, "o", "    regularization=None,\r\n"]
[1475.476, "o", "    copy_cov=True,\r\n"]
[1475.486, "o", "    init=None,\r\n"]
[1475.496, "o", "    max_iter=1000,\r\n"]
[1475.506, "o", "    verbose=0,\r\n"]
[1475.516, "o", "    positive=False,\r\n"]
[1475.526, "o", "):\r\n"]
[1475.536, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[1475.546, "o", "\r\n"]
[1475.556, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[1475.566, "o", "\r\n"]
[1475.576, "o", "    Parameters\r\n"]
[1475.586, "o", "    ----------\r\n"]
[1475.596, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[1475.606, "o", "        Data matrix.\r\n"]
[1475.616, "o", "\r\n"]
[1475.626, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1475.636, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1475.646, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[1475.656, "o", "\r\n"]
[1475.666, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[1475.676, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[1475.686, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[1475.696, "o", "\r\n"]
[1475.706, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[1475.716, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[1475.726, "o", "\r\n"]
[1475.736, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1475.746, "o", "            default='lasso_lars'\r\n"]
[1475.756, "o", "        The algorithm used:\r\n"]
[1475.766, "o", "\r\n"]
[1475.776, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[1475.786, "o", "          (`linear_model.lars_path`);\r\n"]
[1475.796, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1475.806, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1475.816, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1475.826, "o", "          the estimated components are sparse;\r\n"]
[1475.836, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1475.846, "o", "          solution;\r\n"]
[1475.856, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1475.866, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[1475.876, "o", "\r\n"]
[1475.886, "o", "    regularization : int or float, default=None\r\n"]
[1475.896, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[1475.906, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[1475.916, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[1475.926, "o", "\r\n"]
[1475.936, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1475.946, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[1475.956, "o", "        `algorithm='lasso_cd'`.\r\n"]
[1475.966, "o", "\r\n"]
[1475.976, "o", "    max_iter : int, default=1000\r\n"]
[1475.986, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1475.996, "o", "        `'lasso_lars'`.\r\n"]
[1476.006, "o", "\r\n"]
[1476.016, "o", "    copy_cov : bool, default=True\r\n"]
[1476.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1476.036, "o", "        be overwritten.\r\n"]
[1476.046, "o", "\r\n"]
[1476.056, "o", "    verbose : int, default=0\r\n"]
[1476.066, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[1476.076, "o", "\r\n"]
[1476.086, "o", "    positive: bool, default=False\r\n"]
[1476.096, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[1476.106, "o", "\r\n"]
[1476.116, "o", "        .. versionadded:: 0.20\r\n"]
[1476.126, "o", "\r\n"]
[1476.136, "o", "    Returns\r\n"]
[1476.146, "o", "    -------\r\n"]
[1476.156, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[1476.166, "o", "        The sparse codes.\r\n"]
[1476.176, "o", "    \"\"\"\r\n"]
[1476.186, "o", "    n_samples, n_features = X.shape\r\n"]
[1476.196, "o", "    n_components = dictionary.shape[0]\r\n"]
[1476.206, "o", "\r\n"]
[1476.216, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[1476.226, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1476.236, "o", "        try:\r\n"]
[1476.246, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1476.256, "o", "\r\n"]
[1476.266, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1476.276, "o", "            # corrects the verbosity level.\r\n"]
[1476.286, "o", "            lasso_lars = LassoLars(\r\n"]
[1476.296, "o", "                alpha=alpha,\r\n"]
[1476.306, "o", "                fit_intercept=False,\r\n"]
[1476.316, "o", "                verbose=verbose,\r\n"]
[1476.326, "o", "                precompute=gram,\r\n"]
[1476.336, "o", "                fit_path=False,\r\n"]
[1476.346, "o", "                positive=positive,\r\n"]
[1476.356, "o", "                max_iter=max_iter,\r\n"]
[1476.366, "o", "            )\r\n"]
[1476.376, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1476.386, "o", "            new_code = lasso_lars.coef_\r\n"]
[1476.396, "o", "        finally:\r\n"]
[1476.406, "o", "            np.seterr(**err_mgt)\r\n"]
[1476.416, "o", "\r\n"]
[1476.426, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[1476.436, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1476.446, "o", "\r\n"]
[1476.456, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[1476.466, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[1476.476, "o", "        # argument that we could pass in from Lasso.\r\n"]
[1476.486, "o", "        clf = Lasso(\r\n"]
[1476.496, "o", "            alpha=alpha,\r\n"]
[1476.506, "o", "            fit_intercept=False,\r\n"]
[1476.516, "o", "            precompute=gram,\r\n"]
[1476.526, "o", "            max_iter=max_iter,\r\n"]
[1476.536, "o", "            warm_start=True,\r\n"]
[1476.546, "o", "            positive=positive,\r\n"]
[1476.556, "o", "        )\r\n"]
[1476.566, "o", "\r\n"]
[1476.576, "o", "        if init is not None:\r\n"]
[1476.586, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[1476.596, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[1476.606, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[1476.616, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[1476.626, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[1476.636, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[1476.646, "o", "                init = np.array(init)\r\n"]
[1476.656, "o", "            clf.coef_ = init\r\n"]
[1476.666, "o", "\r\n"]
[1476.676, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[1476.686, "o", "        new_code = clf.coef_\r\n"]
[1476.696, "o", "\r\n"]
[1476.706, "o", "    elif algorithm == \"lars\":\r\n"]
[1476.716, "o", "        try:\r\n"]
[1476.726, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1476.736, "o", "\r\n"]
[1476.746, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1476.756, "o", "            # corrects the verbosity level.\r\n"]
[1476.766, "o", "            lars = Lars(\r\n"]
[1476.776, "o", "                fit_intercept=False,\r\n"]
[1476.786, "o", "                verbose=verbose,\r\n"]
[1476.796, "o", "                precompute=gram,\r\n"]
[1476.806, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[1476.816, "o", "                fit_path=False,\r\n"]
[1476.826, "o", "            )\r\n"]
[1476.836, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1476.846, "o", "            new_code = lars.coef_\r\n"]
[1476.856, "o", "        finally:\r\n"]
[1476.866, "o", "            np.seterr(**err_mgt)\r\n"]
[1476.876, "o", "\r\n"]
[1476.886, "o", "    elif algorithm == \"threshold\":\r\n"]
[1476.896, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[1476.906, "o", "        if positive:\r\n"]
[1476.916, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[1476.926, "o", "\r\n"]
[1476.936, "o", "    elif algorithm == \"omp\":\r\n"]
[1476.946, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[1476.956, "o", "            Gram=gram,\r\n"]
[1476.966, "o", "            Xy=cov,\r\n"]
[1476.976, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[1476.986, "o", "            tol=None,\r\n"]
[1476.996, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[1477.006, "o", "            copy_Xy=copy_cov,\r\n"]
[1477.016, "o", "        ).T\r\n"]
[1477.026, "o", "\r\n"]
[1477.036, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[1477.046, "o", "\r\n"]
[1477.056, "o", "\r\n"]
[1477.066, "o", "@validate_params(\r\n"]
[1477.076, "o", "    {\r\n"]
[1477.086, "o", "        \"X\": [\"array-like\"],\r\n"]
[1477.096, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[1477.106, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[1477.116, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[1477.126, "o", "        \"algorithm\": [\r\n"]
[1477.136, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[1477.146, "o", "        ],\r\n"]
[1477.156, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1477.166, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[1477.176, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[1477.186, "o", "        \"init\": [\"array-like\", None],\r\n"]
[1477.196, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1477.206, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[1477.216, "o", "        \"check_input\": [\"boolean\"],\r\n"]
[1477.226, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[1477.236, "o", "        \"positive\": [\"boolean\"],\r\n"]
[1477.246, "o", "    },\r\n"]
[1477.256, "o", "    prefer_skip_nested_validation=True,\r\n"]
[1477.266, "o", ")\r\n"]
[1477.276, "o", "# XXX : could be moved to the linear_model module\r\n"]
[1477.286, "o", "def sparse_encode(\r\n"]
[1477.296, "o", "    X,\r\n"]
[1477.306, "o", "    dictionary,\r\n"]
[1477.316, "o", "    *,\r\n"]
[1477.326, "o", "    gram=None,\r\n"]
[1477.336, "o", "    cov=None,\r\n"]
[1477.346, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1477.356, "o", "    n_nonzero_coefs=None,\r\n"]
[1477.366, "o", "    alpha=None,\r\n"]
[1477.376, "o", "    copy_cov=True,\r\n"]
[1477.386, "o", "    init=None,\r\n"]
[1477.396, "o", "    max_iter=1000,\r\n"]
[1477.406, "o", "    n_jobs=None,\r\n"]
[1477.416, "o", "    check_input=True,\r\n"]
[1477.426, "o", "    verbose=0,\r\n"]
[1477.436, "o", "    positive=False,\r\n"]
[1477.446, "o", "):\r\n"]
[1477.456, "o", "    \"\"\"Sparse coding.\r\n"]
[1477.466, "o", "\r\n"]
[1477.476, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[1477.486, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[1477.496, "o", "\r\n"]
[1477.506, "o", "        X ~= code * dictionary\r\n"]
[1477.516, "o", "\r\n"]
[1477.526, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[1477.536, "o", "\r\n"]
[1477.546, "o", "    Parameters\r\n"]
[1477.556, "o", "    ----------\r\n"]
[1477.566, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[1477.576, "o", "        Data matrix.\r\n"]
[1477.586, "o", "\r\n"]
[1477.596, "o", "    dictionary : array-like of shape (n_components, n_features)\r\n"]
[1477.606, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1477.616, "o", "        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[1477.626, "o", "        output.\r\n"]
[1477.636, "o", "\r\n"]
[1477.646, "o", "    gram : array-like of shape (n_components, n_components), default=None\r\n"]
[1477.656, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n"]
[1477.666, "o", "\r\n"]
[1477.676, "o", "    cov : array-like of shape (n_components, n_samples), default=None\r\n"]
[1477.686, "o", "        Precomputed covariance, `dictionary' * X`.\r\n"]
[1477.696, "o", "\r\n"]
[1477.706, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1477.716, "o", "            default='lasso_lars'\r\n"]
[1477.726, "o", "        The algorithm used:\r\n"]
[1477.736, "o", "\r\n"]
[1477.746, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[1477.756, "o", "          (`linear_model.lars_path`);\r\n"]
[1477.766, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1477.776, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1477.786, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1477.796, "o", "          the estimated components are sparse;\r\n"]
[1477.806, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1477.816, "o", "          solution;\r\n"]
[1477.826, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1477.836, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[1477.846, "o", "\r\n"]
[1477.856, "o", "    n_nonzero_coefs : int, default=None\r\n"]
[1477.866, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1477.876, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[1477.886, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[1477.896, "o", "        `n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1477.906, "o", "\r\n"]
[1477.916, "o", "    alpha : float, default=None\r\n"]
[1477.926, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1477.936, "o", "        penalty applied to the L1 norm.\r\n"]
[1477.946, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1477.956, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1477.966, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[1477.976, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[1477.986, "o", "        `n_nonzero_coefs`.\r\n"]
[1477.996, "o", "        If `None`, default to 1.\r\n"]
[1478.006, "o", "\r\n"]
[1478.016, "o", "    copy_cov : bool, default=True\r\n"]
[1478.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1478.036, "o", "        be overwritten.\r\n"]
[1478.046, "o", "\r\n"]
[1478.056, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1478.066, "o", "        Initialization value of the sparse codes. Only used if\r\n"]
[1478.076, "o", "        `algorithm='lasso_cd'`.\r\n"]
[1478.086, "o", "\r\n"]
[1478.096, "o", "    max_iter : int, default=1000\r\n"]
[1478.106, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1478.116, "o", "        `'lasso_lars'`.\r\n"]
[1478.126, "o", "\r\n"]
[1478.136, "o", "    n_jobs : int, default=None\r\n"]
[1478.146, "o", "        Number of parallel jobs to run.\r\n"]
[1478.156, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1478.166, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1478.176, "o", "        for more details.\r\n"]
[1478.186, "o", "\r\n"]
[1478.196, "o", "    check_input : bool, default=True\r\n"]
[1478.206, "o", "        If `False`, the input arrays X and dictionary will not be checked.\r\n"]
[1478.216, "o", "\r\n"]
[1478.226, "o", "    verbose : int, default=0\r\n"]
[1478.236, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[1478.246, "o", "\r\n"]
[1478.256, "o", "    positive : bool, default=False\r\n"]
[1478.266, "o", "        Whether to enforce positivity when finding the encoding.\r\n"]
[1478.276, "o", "\r\n"]
[1478.286, "o", "        .. versionadded:: 0.20\r\n"]
[1478.296, "o", "\r\n"]
[1478.306, "o", "    Returns\r\n"]
[1478.316, "o", "    -------\r\n"]
[1478.326, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1478.336, "o", "        The sparse codes.\r\n"]
[1478.346, "o", "\r\n"]
[1478.356, "o", "    See Also\r\n"]
[1478.366, "o", "    --------\r\n"]
[1478.376, "o", "    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n"]
[1478.386, "o", "        path using LARS algorithm.\r\n"]
[1478.396, "o", "    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n"]
[1478.406, "o", "    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n"]
[1478.416, "o", "    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n"]
[1478.426, "o", "        dictionary.\r\n"]
[1478.436, "o", "    \"\"\"\r\n"]
[1478.446, "o", "    if check_input:\r\n"]
[1478.456, "o", "        if algorithm == \"lasso_cd\":\r\n"]
[1478.466, "o", "            dictionary = check_array(\r\n"]
[1478.476, "o", "                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n"]
[1478.486, "o", "            )\r\n"]
[1478.496, "o", "            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n"]
[1478.506, "o", "        else:\r\n"]
[1478.516, "o", "            dictionary = check_array(dictionary)\r\n"]
[1478.526, "o", "            X = check_array(X)\r\n"]
[1478.536, "o", "\r\n"]
[1478.546, "o", "    if dictionary.shape[1] != X.shape[1]:\r\n"]
[1478.556, "o", "        raise ValueError(\r\n"]
[1478.566, "o", "            \"Dictionary and X have different numbers of features:\"\r\n"]
[1478.576, "o", "            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n"]
[1478.586, "o", "        )\r\n"]
[1478.596, "o", "\r\n"]
[1478.606, "o", "    _check_positive_coding(algorithm, positive)\r\n"]
[1478.616, "o", "\r\n"]
[1478.626, "o", "    return _sparse_encode(\r\n"]
[1478.636, "o", "        X,\r\n"]
[1478.646, "o", "        dictionary,\r\n"]
[1478.656, "o", "        gram=gram,\r\n"]
[1478.666, "o", "        cov=cov,\r\n"]
[1478.676, "o", "        algorithm=algorithm,\r\n"]
[1478.686, "o", "        n_nonzero_coefs=n_nonzero_coefs,\r\n"]
[1478.696, "o", "        alpha=alpha,\r\n"]
[1478.706, "o", "        copy_cov=copy_cov,\r\n"]
[1478.716, "o", "        init=init,\r\n"]
[1478.726, "o", "        max_iter=max_iter,\r\n"]
[1478.736, "o", "        n_jobs=n_jobs,\r\n"]
[1478.746, "o", "        verbose=verbose,\r\n"]
[1478.756, "o", "        positive=positive,\r\n"]
[1478.766, "o", "    )\r\n"]
[1478.776, "o", "\r\n"]
[1478.786, "o", "\r\n"]
[1478.796, "o", "def _sparse_encode(\r\n"]
[1478.806, "o", "    X,\r\n"]
[1478.816, "o", "    dictionary,\r\n"]
[1478.826, "o", "    *,\r\n"]
[1478.836, "o", "    gram=None,\r\n"]
[1478.846, "o", "    cov=None,\r\n"]
[1478.856, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1478.866, "o", "    n_nonzero_coefs=None,\r\n"]
[1478.876, "o", "    alpha=None,\r\n"]
[1478.886, "o", "    copy_cov=True,\r\n"]
[1478.896, "o", "    init=None,\r\n"]
[1478.906, "o", "    max_iter=1000,\r\n"]
[1478.916, "o", "    n_jobs=None,\r\n"]
[1478.926, "o", "    verbose=0,\r\n"]
[1478.936, "o", "    positive=False,\r\n"]
[1478.946, "o", "):\r\n"]
[1478.956, "o", "    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[1478.966, "o", "\r\n"]
[1478.976, "o", "    n_samples, n_features = X.shape\r\n"]
[1478.986, "o", "    n_components = dictionary.shape[0]\r\n"]
[1478.996, "o", "\r\n"]
[1479.006, "o", "    if algorithm in (\"lars\", \"omp\"):\r\n"]
[1479.016, "o", "        regularization = n_nonzero_coefs\r\n"]
[1479.026, "o", "        if regularization is None:\r\n"]
[1479.036, "o", "            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[1479.046, "o", "    else:\r\n"]
[1479.056, "o", "        regularization = alpha\r\n"]
[1479.066, "o", "        if regularization is None:\r\n"]
[1479.076, "o", "            regularization = 1.0\r\n"]
[1479.086, "o", "\r\n"]
[1479.096, "o", "    if gram is None and algorithm != \"threshold\":\r\n"]
[1479.106, "o", "        gram = np.dot(dictionary, dictionary.T)\r\n"]
[1479.116, "o", "\r\n"]
[1479.126, "o", "    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[1479.136, "o", "        copy_cov = False\r\n"]
[1479.146, "o", "        cov = np.dot(dictionary, X.T)\r\n"]
[1479.156, "o", "\r\n"]
[1479.166, "o", "    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[1479.176, "o", "        code = _sparse_encode_precomputed(\r\n"]
[1479.186, "o", "            X,\r\n"]
[1479.196, "o", "            dictionary,\r\n"]
[1479.206, "o", "            gram=gram,\r\n"]
[1479.216, "o", "            cov=cov,\r\n"]
[1479.226, "o", "            algorithm=algorithm,\r\n"]
[1479.236, "o", "            regularization=regularization,\r\n"]
[1479.246, "o", "            copy_cov=copy_cov,\r\n"]
[1479.256, "o", "            init=init,\r\n"]
[1479.266, "o", "            max_iter=max_iter,\r\n"]
[1479.276, "o", "            verbose=verbose,\r\n"]
[1479.286, "o", "            positive=positive,\r\n"]
[1479.296, "o", "        )\r\n"]
[1479.306, "o", "        return code\r\n"]
[1479.316, "o", "\r\n"]
[1479.326, "o", "    # Enter parallel code block\r\n"]
[1479.336, "o", "    n_samples = X.shape[0]\r\n"]
[1479.346, "o", "    n_components = dictionary.shape[0]\r\n"]
[1479.356, "o", "    code = np.empty((n_samples, n_components))\r\n"]
[1479.366, "o", "    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[1479.376, "o", "\r\n"]
[1479.386, "o", "    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[1479.396, "o", "        delayed(_sparse_encode_precomputed)(\r\n"]
[1479.406, "o", "            X[this_slice],\r\n"]
[1479.416, "o", "            dictionary,\r\n"]
[1479.426, "o", "            gram=gram,\r\n"]
[1479.436, "o", "            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[1479.446, "o", "            algorithm=algorithm,\r\n"]
[1479.456, "o", "            regularization=regularization,\r\n"]
[1479.466, "o", "            copy_cov=copy_cov,\r\n"]
[1479.476, "o", "            init=init[this_slice] if init is not None else None,\r\n"]
[1479.486, "o", "            max_iter=max_iter,\r\n"]
[1479.496, "o", "            verbose=verbose,\r\n"]
[1479.506, "o", "            positive=positive,\r\n"]
[1479.516, "o", "        )\r\n"]
[1479.526, "o", "        for this_slice in slices\r\n"]
[1479.536, "o", "    )\r\n"]
[1479.546, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[1479.556, "o", "        code[this_slice] = this_view\r\n"]
[1479.566, "o", "    return code\r\n"]
[1479.576, "o", "\r\n"]
[1479.586, "o", "\r\n"]
[1479.596, "o", "def _update_dict(\r\n"]
[1479.606, "o", "    dictionary,\r\n"]
[1479.616, "o", "    Y,\r\n"]
[1479.626, "o", "    code,\r\n"]
[1479.636, "o", "    A=None,\r\n"]
[1479.646, "o", "    B=None,\r\n"]
[1479.656, "o", "    verbose=False,\r\n"]
[1479.666, "o", "    random_state=None,\r\n"]
[1479.676, "o", "    positive=False,\r\n"]
[1479.686, "o", "):\r\n"]
[1479.696, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[1479.706, "o", "\r\n"]
[1479.716, "o", "    Parameters\r\n"]
[1479.726, "o", "    ----------\r\n"]
[1479.736, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1479.746, "o", "        Value of the dictionary at the previous iteration.\r\n"]
[1479.756, "o", "\r\n"]
[1479.766, "o", "    Y : ndarray of shape (n_samples, n_features)\r\n"]
[1479.776, "o", "        Data matrix.\r\n"]
[1479.786, "o", "\r\n"]
[1479.796, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1479.806, "o", "        Sparse coding of the data against which to optimize the dictionary.\r\n"]
[1479.816, "o", "\r\n"]
[1479.826, "o", "    A : ndarray of shape (n_components, n_components), default=None\r\n"]
[1479.836, "o", "        Together with `B`, sufficient stats of the online model to update the\r\n"]
[1479.846, "o", "        dictionary.\r\n"]
[1479.856, "o", "\r\n"]
[1479.866, "o", "    B : ndarray of shape (n_features, n_components), default=None\r\n"]
[1479.876, "o", "        Together with `A`, sufficient stats of the online model to update the\r\n"]
[1479.886, "o", "        dictionary.\r\n"]
[1479.896, "o", "\r\n"]
[1479.906, "o", "    verbose: bool, default=False\r\n"]
[1479.916, "o", "        Degree of output the procedure will print.\r\n"]
[1479.926, "o", "\r\n"]
[1479.936, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1480.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1480.002, "i", "sed -n '900,1800p' sklearn/decomposition/_dict_learning.py\r"]
[1480.004, "o", "sed -n '900,1800p' sklearn/decomposition/_dict_learning.py\r\n"]
[1480.016, "o", "\u001b[?2004l\r\n"]
[1480.026, "o", "            random_state=random_state,\r\n"]
[1480.036, "o", "            transform_algorithm=transform_algorithm,\r\n"]
[1480.046, "o", "            transform_alpha=alpha,\r\n"]
[1480.056, "o", "            positive_code=positive_code,\r\n"]
[1480.066, "o", "            positive_dict=positive_dict,\r\n"]
[1480.076, "o", "            transform_max_iter=method_max_iter,\r\n"]
[1480.086, "o", "            verbose=verbose,\r\n"]
[1480.096, "o", "            callback=callback,\r\n"]
[1480.106, "o", "            tol=tol,\r\n"]
[1480.116, "o", "            max_no_improvement=max_no_improvement,\r\n"]
[1480.126, "o", "        ).fit(X)\r\n"]
[1480.136, "o", "\r\n"]
[1480.146, "o", "        if not return_code:\r\n"]
[1480.156, "o", "            return est.components_\r\n"]
[1480.166, "o", "        else:\r\n"]
[1480.176, "o", "            code = est.transform(X)\r\n"]
[1480.186, "o", "            return code, est.components_\r\n"]
[1480.196, "o", "\r\n"]
[1480.206, "o", "    # TODO(1.4) remove the whole old behavior\r\n"]
[1480.216, "o", "    # Fallback to old behavior\r\n"]
[1480.226, "o", "\r\n"]
[1480.236, "o", "    n_iter = _check_warn_deprecated(\r\n"]
[1480.246, "o", "        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n"]
[1480.256, "o", "    )\r\n"]
[1480.266, "o", "\r\n"]
[1480.276, "o", "    if n_components is None:\r\n"]
[1480.286, "o", "        n_components = X.shape[1]\r\n"]
[1480.296, "o", "\r\n"]
[1480.306, "o", "    if method not in (\"lars\", \"cd\"):\r\n"]
[1480.316, "o", "        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n"]
[1480.326, "o", "\r\n"]
[1480.336, "o", "    _check_positive_coding(method, positive_code)\r\n"]
[1480.346, "o", "\r\n"]
[1480.356, "o", "    method = \"lasso_\" + method\r\n"]
[1480.366, "o", "\r\n"]
[1480.376, "o", "    t0 = time.time()\r\n"]
[1480.386, "o", "    n_samples, n_features = X.shape\r\n"]
[1480.396, "o", "    # Avoid integer division problems\r\n"]
[1480.406, "o", "    alpha = float(alpha)\r\n"]
[1480.416, "o", "    random_state = check_random_state(random_state)\r\n"]
[1480.426, "o", "\r\n"]
[1480.436, "o", "    # Init V with SVD of X\r\n"]
[1480.446, "o", "    if dict_init is not None:\r\n"]
[1480.456, "o", "        dictionary = dict_init\r\n"]
[1480.466, "o", "    else:\r\n"]
[1480.476, "o", "        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n"]
[1480.486, "o", "        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[1480.496, "o", "    r = len(dictionary)\r\n"]
[1480.506, "o", "    if n_components <= r:\r\n"]
[1480.516, "o", "        dictionary = dictionary[:n_components, :]\r\n"]
[1480.526, "o", "    else:\r\n"]
[1480.536, "o", "        dictionary = np.r_[\r\n"]
[1480.546, "o", "            dictionary,\r\n"]
[1480.556, "o", "            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n"]
[1480.566, "o", "        ]\r\n"]
[1480.576, "o", "\r\n"]
[1480.586, "o", "    if verbose == 1:\r\n"]
[1480.596, "o", "        print(\"[dict_learning]\", end=\" \")\r\n"]
[1480.606, "o", "\r\n"]
[1480.616, "o", "    if shuffle:\r\n"]
[1480.626, "o", "        X_train = X.copy()\r\n"]
[1480.636, "o", "        random_state.shuffle(X_train)\r\n"]
[1480.646, "o", "    else:\r\n"]
[1480.656, "o", "        X_train = X\r\n"]
[1480.666, "o", "\r\n"]
[1480.676, "o", "    X_train = check_array(\r\n"]
[1480.686, "o", "        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n"]
[1480.696, "o", "    )\r\n"]
[1480.706, "o", "\r\n"]
[1480.716, "o", "    # Fortran-order dict better suited for the sparse coding which is the\r\n"]
[1480.726, "o", "    # bottleneck of this algorithm.\r\n"]
[1480.736, "o", "    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n"]
[1480.746, "o", "    dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[1480.756, "o", "\r\n"]
[1480.766, "o", "    batches = gen_batches(n_samples, batch_size)\r\n"]
[1480.776, "o", "    batches = itertools.cycle(batches)\r\n"]
[1480.786, "o", "\r\n"]
[1480.796, "o", "    # The covariance of the dictionary\r\n"]
[1480.806, "o", "    if inner_stats is None:\r\n"]
[1480.816, "o", "        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n"]
[1480.826, "o", "        # The data approximation\r\n"]
[1480.836, "o", "        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n"]
[1480.846, "o", "    else:\r\n"]
[1480.856, "o", "        A = inner_stats[0].copy()\r\n"]
[1480.866, "o", "        B = inner_stats[1].copy()\r\n"]
[1480.876, "o", "\r\n"]
[1480.886, "o", "    # If n_iter is zero, we need to return zero.\r\n"]
[1480.896, "o", "    ii = iter_offset - 1\r\n"]
[1480.906, "o", "\r\n"]
[1480.916, "o", "    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n"]
[1480.926, "o", "        this_X = X_train[batch]\r\n"]
[1480.936, "o", "        dt = time.time() - t0\r\n"]
[1480.946, "o", "        if verbose == 1:\r\n"]
[1480.956, "o", "            sys.stdout.write(\".\")\r\n"]
[1480.966, "o", "            sys.stdout.flush()\r\n"]
[1480.976, "o", "        elif verbose:\r\n"]
[1480.986, "o", "            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n"]
[1480.996, "o", "                print(\r\n"]
[1481.006, "o", "                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n"]
[1481.016, "o", "                )\r\n"]
[1481.026, "o", "\r\n"]
[1481.036, "o", "        this_code = sparse_encode(\r\n"]
[1481.046, "o", "            this_X,\r\n"]
[1481.056, "o", "            dictionary,\r\n"]
[1481.066, "o", "            algorithm=method,\r\n"]
[1481.076, "o", "            alpha=alpha,\r\n"]
[1481.086, "o", "            n_jobs=n_jobs,\r\n"]
[1481.096, "o", "            check_input=False,\r\n"]
[1481.106, "o", "            positive=positive_code,\r\n"]
[1481.116, "o", "            max_iter=method_max_iter,\r\n"]
[1481.126, "o", "            verbose=verbose,\r\n"]
[1481.136, "o", "        )\r\n"]
[1481.146, "o", "\r\n"]
[1481.156, "o", "        # Update the auxiliary variables\r\n"]
[1481.166, "o", "        if ii < batch_size - 1:\r\n"]
[1481.176, "o", "            theta = float((ii + 1) * batch_size)\r\n"]
[1481.186, "o", "        else:\r\n"]
[1481.196, "o", "            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n"]
[1481.206, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[1481.216, "o", "\r\n"]
[1481.226, "o", "        A *= beta\r\n"]
[1481.236, "o", "        A += np.dot(this_code.T, this_code)\r\n"]
[1481.246, "o", "        B *= beta\r\n"]
[1481.256, "o", "        B += np.dot(this_X.T, this_code)\r\n"]
[1481.266, "o", "\r\n"]
[1481.276, "o", "        # Update dictionary in place\r\n"]
[1481.286, "o", "        _update_dict(\r\n"]
[1481.296, "o", "            dictionary,\r\n"]
[1481.306, "o", "            this_X,\r\n"]
[1481.316, "o", "            this_code,\r\n"]
[1481.326, "o", "            A,\r\n"]
[1481.336, "o", "            B,\r\n"]
[1481.346, "o", "            verbose=verbose,\r\n"]
[1481.356, "o", "            random_state=random_state,\r\n"]
[1481.366, "o", "            positive=positive_dict,\r\n"]
[1481.376, "o", "        )\r\n"]
[1481.386, "o", "\r\n"]
[1481.396, "o", "        # Maybe we need a stopping criteria based on the amount of\r\n"]
[1481.406, "o", "        # modification in the dictionary\r\n"]
[1481.416, "o", "        if callback is not None:\r\n"]
[1481.426, "o", "            callback(locals())\r\n"]
[1481.436, "o", "\r\n"]
[1481.446, "o", "    if return_inner_stats:\r\n"]
[1481.456, "o", "        if return_n_iter:\r\n"]
[1481.466, "o", "            return dictionary, (A, B), ii - iter_offset + 1\r\n"]
[1481.476, "o", "        else:\r\n"]
[1481.486, "o", "            return dictionary, (A, B)\r\n"]
[1481.496, "o", "    if return_code:\r\n"]
[1481.506, "o", "        if verbose > 1:\r\n"]
[1481.516, "o", "            print(\"Learning code...\", end=\" \")\r\n"]
[1481.526, "o", "        elif verbose == 1:\r\n"]
[1481.536, "o", "            print(\"|\", end=\" \")\r\n"]
[1481.546, "o", "        code = sparse_encode(\r\n"]
[1481.556, "o", "            X,\r\n"]
[1481.566, "o", "            dictionary,\r\n"]
[1481.576, "o", "            algorithm=method,\r\n"]
[1481.586, "o", "            alpha=alpha,\r\n"]
[1481.596, "o", "            n_jobs=n_jobs,\r\n"]
[1481.606, "o", "            check_input=False,\r\n"]
[1481.616, "o", "            positive=positive_code,\r\n"]
[1481.626, "o", "            max_iter=method_max_iter,\r\n"]
[1481.636, "o", "            verbose=verbose,\r\n"]
[1481.646, "o", "        )\r\n"]
[1481.656, "o", "        if verbose > 1:\r\n"]
[1481.666, "o", "            dt = time.time() - t0\r\n"]
[1481.676, "o", "            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n"]
[1481.686, "o", "        if return_n_iter:\r\n"]
[1481.696, "o", "            return code, dictionary, ii - iter_offset + 1\r\n"]
[1481.706, "o", "        else:\r\n"]
[1481.716, "o", "            return code, dictionary\r\n"]
[1481.726, "o", "\r\n"]
[1481.736, "o", "    if return_n_iter:\r\n"]
[1481.746, "o", "        return dictionary, ii - iter_offset + 1\r\n"]
[1481.756, "o", "    else:\r\n"]
[1481.766, "o", "        return dictionary\r\n"]
[1481.776, "o", "\r\n"]
[1481.786, "o", "\r\n"]
[1481.796, "o", "@validate_params(\r\n"]
[1481.806, "o", "    {\r\n"]
[1481.816, "o", "        \"X\": [\"array-like\"],\r\n"]
[1481.826, "o", "        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[1481.836, "o", "        \"return_n_iter\": [\"boolean\"],\r\n"]
[1481.846, "o", "        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1481.856, "o", "    },\r\n"]
[1481.866, "o", "    prefer_skip_nested_validation=False,\r\n"]
[1481.876, "o", ")\r\n"]
[1481.886, "o", "def dict_learning(\r\n"]
[1481.896, "o", "    X,\r\n"]
[1481.906, "o", "    n_components,\r\n"]
[1481.916, "o", "    *,\r\n"]
[1481.926, "o", "    alpha,\r\n"]
[1481.936, "o", "    max_iter=100,\r\n"]
[1481.946, "o", "    tol=1e-8,\r\n"]
[1481.956, "o", "    method=\"lars\",\r\n"]
[1481.966, "o", "    n_jobs=None,\r\n"]
[1481.976, "o", "    dict_init=None,\r\n"]
[1481.986, "o", "    code_init=None,\r\n"]
[1481.996, "o", "    callback=None,\r\n"]
[1482.006, "o", "    verbose=False,\r\n"]
[1482.016, "o", "    random_state=None,\r\n"]
[1482.026, "o", "    return_n_iter=False,\r\n"]
[1482.036, "o", "    positive_dict=False,\r\n"]
[1482.046, "o", "    positive_code=False,\r\n"]
[1482.056, "o", "    method_max_iter=1000,\r\n"]
[1482.066, "o", "):\r\n"]
[1482.076, "o", "    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n"]
[1482.086, "o", "\r\n"]
[1482.096, "o", "    Finds the best dictionary and the corresponding sparse code for\r\n"]
[1482.106, "o", "    approximating the data matrix X by solving::\r\n"]
[1482.116, "o", "\r\n"]
[1482.126, "o", "        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[1482.136, "o", "                     (U,V)\r\n"]
[1482.146, "o", "                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n"]
[1482.156, "o", "\r\n"]
[1482.166, "o", "    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n"]
[1482.176, "o", "    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n"]
[1482.186, "o", "    which is the sum of the absolute values of all the entries in the matrix.\r\n"]
[1482.196, "o", "\r\n"]
[1482.206, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[1482.216, "o", "\r\n"]
[1482.226, "o", "    Parameters\r\n"]
[1482.236, "o", "    ----------\r\n"]
[1482.246, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[1482.256, "o", "        Data matrix.\r\n"]
[1482.266, "o", "\r\n"]
[1482.276, "o", "    n_components : int\r\n"]
[1482.286, "o", "        Number of dictionary atoms to extract.\r\n"]
[1482.296, "o", "\r\n"]
[1482.306, "o", "    alpha : int or float\r\n"]
[1482.316, "o", "        Sparsity controlling parameter.\r\n"]
[1482.326, "o", "\r\n"]
[1482.336, "o", "    max_iter : int, default=100\r\n"]
[1482.346, "o", "        Maximum number of iterations to perform.\r\n"]
[1482.356, "o", "\r\n"]
[1482.366, "o", "    tol : float, default=1e-8\r\n"]
[1482.376, "o", "        Tolerance for the stopping condition.\r\n"]
[1482.386, "o", "\r\n"]
[1482.396, "o", "    method : {'lars', 'cd'}, default='lars'\r\n"]
[1482.406, "o", "        The method used:\r\n"]
[1482.416, "o", "\r\n"]
[1482.426, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[1482.436, "o", "           problem (`linear_model.lars_path`);\r\n"]
[1482.446, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[1482.456, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[1482.466, "o", "          the estimated components are sparse.\r\n"]
[1482.476, "o", "\r\n"]
[1482.486, "o", "    n_jobs : int, default=None\r\n"]
[1482.496, "o", "        Number of parallel jobs to run.\r\n"]
[1482.506, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1482.516, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1482.526, "o", "        for more details.\r\n"]
[1482.536, "o", "\r\n"]
[1482.546, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1482.556, "o", "        Initial value for the dictionary for warm restart scenarios. Only used\r\n"]
[1482.566, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[1482.576, "o", "\r\n"]
[1482.586, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1482.596, "o", "        Initial value for the sparse code for warm restart scenarios. Only used\r\n"]
[1482.606, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[1482.616, "o", "\r\n"]
[1482.626, "o", "    callback : callable, default=None\r\n"]
[1482.636, "o", "        Callable that gets invoked every five iterations.\r\n"]
[1482.646, "o", "\r\n"]
[1482.656, "o", "    verbose : bool, default=False\r\n"]
[1482.666, "o", "        To control the verbosity of the procedure.\r\n"]
[1482.676, "o", "\r\n"]
[1482.686, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1482.696, "o", "        Used for randomly initializing the dictionary. Pass an int for\r\n"]
[1482.706, "o", "        reproducible results across multiple function calls.\r\n"]
[1482.716, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1482.726, "o", "\r\n"]
[1482.736, "o", "    return_n_iter : bool, default=False\r\n"]
[1482.746, "o", "        Whether or not to return the number of iterations.\r\n"]
[1482.756, "o", "\r\n"]
[1482.766, "o", "    positive_dict : bool, default=False\r\n"]
[1482.776, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1482.786, "o", "\r\n"]
[1482.796, "o", "        .. versionadded:: 0.20\r\n"]
[1482.806, "o", "\r\n"]
[1482.816, "o", "    positive_code : bool, default=False\r\n"]
[1482.826, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1482.836, "o", "\r\n"]
[1482.846, "o", "        .. versionadded:: 0.20\r\n"]
[1482.856, "o", "\r\n"]
[1482.866, "o", "    method_max_iter : int, default=1000\r\n"]
[1482.876, "o", "        Maximum number of iterations to perform.\r\n"]
[1482.886, "o", "\r\n"]
[1482.896, "o", "        .. versionadded:: 0.22\r\n"]
[1482.906, "o", "\r\n"]
[1482.916, "o", "    Returns\r\n"]
[1482.926, "o", "    -------\r\n"]
[1482.936, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1482.946, "o", "        The sparse code factor in the matrix factorization.\r\n"]
[1482.956, "o", "\r\n"]
[1482.966, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[1482.976, "o", "        The dictionary factor in the matrix factorization.\r\n"]
[1482.986, "o", "\r\n"]
[1482.996, "o", "    errors : array\r\n"]
[1483.006, "o", "        Vector of errors at each iteration.\r\n"]
[1483.016, "o", "\r\n"]
[1483.026, "o", "    n_iter : int\r\n"]
[1483.036, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[1483.046, "o", "        set to True.\r\n"]
[1483.056, "o", "\r\n"]
[1483.066, "o", "    See Also\r\n"]
[1483.076, "o", "    --------\r\n"]
[1483.086, "o", "    dict_learning_online : Solve a dictionary learning matrix factorization\r\n"]
[1483.096, "o", "        problem online.\r\n"]
[1483.106, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1483.116, "o", "    MiniBatchDictionaryLearning : A faster, less accurate version\r\n"]
[1483.126, "o", "        of the dictionary learning algorithm.\r\n"]
[1483.136, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1483.146, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1483.156, "o", "    \"\"\"\r\n"]
[1483.166, "o", "    estimator = DictionaryLearning(\r\n"]
[1483.176, "o", "        n_components=n_components,\r\n"]
[1483.186, "o", "        alpha=alpha,\r\n"]
[1483.196, "o", "        max_iter=max_iter,\r\n"]
[1483.206, "o", "        tol=tol,\r\n"]
[1483.216, "o", "        fit_algorithm=method,\r\n"]
[1483.226, "o", "        n_jobs=n_jobs,\r\n"]
[1483.236, "o", "        dict_init=dict_init,\r\n"]
[1483.246, "o", "        callback=callback,\r\n"]
[1483.256, "o", "        code_init=code_init,\r\n"]
[1483.266, "o", "        verbose=verbose,\r\n"]
[1483.276, "o", "        random_state=random_state,\r\n"]
[1483.286, "o", "        positive_code=positive_code,\r\n"]
[1483.296, "o", "        positive_dict=positive_dict,\r\n"]
[1483.306, "o", "        transform_max_iter=method_max_iter,\r\n"]
[1483.316, "o", "    )\r\n"]
[1483.326, "o", "    code = estimator.fit_transform(X)\r\n"]
[1483.336, "o", "    if return_n_iter:\r\n"]
[1483.346, "o", "        return (\r\n"]
[1483.356, "o", "            code,\r\n"]
[1483.366, "o", "            estimator.components_,\r\n"]
[1483.376, "o", "            estimator.error_,\r\n"]
[1483.386, "o", "            estimator.n_iter_,\r\n"]
[1483.396, "o", "        )\r\n"]
[1483.406, "o", "    return code, estimator.components_, estimator.error_\r\n"]
[1483.416, "o", "\r\n"]
[1483.426, "o", "\r\n"]
[1483.436, "o", "class _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n"]
[1483.446, "o", "    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n"]
[1483.456, "o", "\r\n"]
[1483.466, "o", "    def __init__(\r\n"]
[1483.476, "o", "        self,\r\n"]
[1483.486, "o", "        transform_algorithm,\r\n"]
[1483.496, "o", "        transform_n_nonzero_coefs,\r\n"]
[1483.506, "o", "        transform_alpha,\r\n"]
[1483.516, "o", "        split_sign,\r\n"]
[1483.526, "o", "        n_jobs,\r\n"]
[1483.536, "o", "        positive_code,\r\n"]
[1483.546, "o", "        transform_max_iter,\r\n"]
[1483.556, "o", "    ):\r\n"]
[1483.566, "o", "        self.transform_algorithm = transform_algorithm\r\n"]
[1483.576, "o", "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n"]
[1483.586, "o", "        self.transform_alpha = transform_alpha\r\n"]
[1483.596, "o", "        self.transform_max_iter = transform_max_iter\r\n"]
[1483.606, "o", "        self.split_sign = split_sign\r\n"]
[1483.616, "o", "        self.n_jobs = n_jobs\r\n"]
[1483.626, "o", "        self.positive_code = positive_code\r\n"]
[1483.636, "o", "\r\n"]
[1483.646, "o", "    def _transform(self, X, dictionary):\r\n"]
[1483.656, "o", "        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n"]
[1483.666, "o", "        SparseCoder.\"\"\"\r\n"]
[1483.676, "o", "        X = self._validate_data(X, reset=False)\r\n"]
[1483.686, "o", "\r\n"]
[1483.696, "o", "        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n"]
[1483.706, "o", "            transform_alpha = self.alpha\r\n"]
[1483.716, "o", "        else:\r\n"]
[1483.726, "o", "            transform_alpha = self.transform_alpha\r\n"]
[1483.736, "o", "\r\n"]
[1483.746, "o", "        code = sparse_encode(\r\n"]
[1483.756, "o", "            X,\r\n"]
[1483.766, "o", "            dictionary,\r\n"]
[1483.776, "o", "            algorithm=self.transform_algorithm,\r\n"]
[1483.786, "o", "            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n"]
[1483.796, "o", "            alpha=transform_alpha,\r\n"]
[1483.806, "o", "            max_iter=self.transform_max_iter,\r\n"]
[1483.816, "o", "            n_jobs=self.n_jobs,\r\n"]
[1483.826, "o", "            positive=self.positive_code,\r\n"]
[1483.836, "o", "        )\r\n"]
[1483.846, "o", "\r\n"]
[1483.856, "o", "        if self.split_sign:\r\n"]
[1483.866, "o", "            # feature vector is split into a positive and negative side\r\n"]
[1483.876, "o", "            n_samples, n_features = code.shape\r\n"]
[1483.886, "o", "            split_code = np.empty((n_samples, 2 * n_features))\r\n"]
[1483.896, "o", "            split_code[:, :n_features] = np.maximum(code, 0)\r\n"]
[1483.906, "o", "            split_code[:, n_features:] = -np.minimum(code, 0)\r\n"]
[1483.916, "o", "            code = split_code\r\n"]
[1483.926, "o", "\r\n"]
[1483.936, "o", "        return code\r\n"]
[1483.946, "o", "\r\n"]
[1483.956, "o", "    def transform(self, X):\r\n"]
[1483.966, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[1483.976, "o", "\r\n"]
[1483.986, "o", "        Coding method is determined by the object parameter\r\n"]
[1483.996, "o", "        `transform_algorithm`.\r\n"]
[1484.006, "o", "\r\n"]
[1484.016, "o", "        Parameters\r\n"]
[1484.026, "o", "        ----------\r\n"]
[1484.036, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[1484.046, "o", "            Test data to be transformed, must have the same number of\r\n"]
[1484.056, "o", "            features as the data used to train the model.\r\n"]
[1484.066, "o", "\r\n"]
[1484.076, "o", "        Returns\r\n"]
[1484.086, "o", "        -------\r\n"]
[1484.096, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[1484.106, "o", "            Transformed data.\r\n"]
[1484.116, "o", "        \"\"\"\r\n"]
[1484.126, "o", "        check_is_fitted(self)\r\n"]
[1484.136, "o", "        return self._transform(X, self.components_)\r\n"]
[1484.146, "o", "\r\n"]
[1484.156, "o", "\r\n"]
[1484.166, "o", "class SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n"]
[1484.176, "o", "    \"\"\"Sparse coding.\r\n"]
[1484.186, "o", "\r\n"]
[1484.196, "o", "    Finds a sparse representation of data against a fixed, precomputed\r\n"]
[1484.206, "o", "    dictionary.\r\n"]
[1484.216, "o", "\r\n"]
[1484.226, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[1484.236, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[1484.246, "o", "\r\n"]
[1484.256, "o", "        X ~= code * dictionary\r\n"]
[1484.266, "o", "\r\n"]
[1484.276, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[1484.286, "o", "\r\n"]
[1484.296, "o", "    Parameters\r\n"]
[1484.306, "o", "    ----------\r\n"]
[1484.316, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1484.326, "o", "        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n"]
[1484.336, "o", "        normalized to unit norm.\r\n"]
[1484.346, "o", "\r\n"]
[1484.356, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[1484.366, "o", "            'threshold'}, default='omp'\r\n"]
[1484.376, "o", "        Algorithm used to transform the data:\r\n"]
[1484.386, "o", "\r\n"]
[1484.396, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[1484.406, "o", "          (`linear_model.lars_path`);\r\n"]
[1484.416, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1484.426, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1484.436, "o", "          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n"]
[1484.446, "o", "          the estimated components are sparse;\r\n"]
[1484.456, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1484.466, "o", "          solution;\r\n"]
[1484.476, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[1484.486, "o", "          the projection ``dictionary * X'``.\r\n"]
[1484.496, "o", "\r\n"]
[1484.506, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[1484.516, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1484.526, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[1484.536, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[1484.546, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1484.556, "o", "\r\n"]
[1484.566, "o", "    transform_alpha : float, default=None\r\n"]
[1484.576, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1484.586, "o", "        penalty applied to the L1 norm.\r\n"]
[1484.596, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1484.606, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1484.616, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[1484.626, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[1484.636, "o", "        `n_nonzero_coefs`.\r\n"]
[1484.646, "o", "        If `None`, default to 1.\r\n"]
[1484.656, "o", "\r\n"]
[1484.666, "o", "    split_sign : bool, default=False\r\n"]
[1484.676, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[1484.686, "o", "        its negative part and its positive part. This can improve the\r\n"]
[1484.696, "o", "        performance of downstream classifiers.\r\n"]
[1484.706, "o", "\r\n"]
[1484.716, "o", "    n_jobs : int, default=None\r\n"]
[1484.726, "o", "        Number of parallel jobs to run.\r\n"]
[1484.736, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1484.746, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1484.756, "o", "        for more details.\r\n"]
[1484.766, "o", "\r\n"]
[1484.776, "o", "    positive_code : bool, default=False\r\n"]
[1484.786, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1484.796, "o", "\r\n"]
[1484.806, "o", "        .. versionadded:: 0.20\r\n"]
[1484.816, "o", "\r\n"]
[1484.826, "o", "    transform_max_iter : int, default=1000\r\n"]
[1484.836, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1484.846, "o", "        `lasso_lars`.\r\n"]
[1484.856, "o", "\r\n"]
[1484.866, "o", "        .. versionadded:: 0.22\r\n"]
[1484.876, "o", "\r\n"]
[1484.886, "o", "    Attributes\r\n"]
[1484.896, "o", "    ----------\r\n"]
[1484.906, "o", "    n_components_ : int\r\n"]
[1484.916, "o", "        Number of atoms.\r\n"]
[1484.926, "o", "\r\n"]
[1484.936, "o", "    n_features_in_ : int\r\n"]
[1485.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1485.002, "i", "sed -n '1800,2600p' sklearn/decomposition/_dict_learning.py\r"]
[1485.004, "o", "sed -n '1800,2600p' sklearn/decomposition/_dict_learning.py\r\n"]
[1485.016, "o", "\u001b[?2004l\r\n"]
[1485.026, "o", "        self.fit_transform(X)\r\n"]
[1485.036, "o", "        return self\r\n"]
[1485.046, "o", "\r\n"]
[1485.056, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[1485.066, "o", "    def fit_transform(self, X, y=None):\r\n"]
[1485.076, "o", "        \"\"\"Fit the model from data in X and return the transformed data.\r\n"]
[1485.086, "o", "\r\n"]
[1485.096, "o", "        Parameters\r\n"]
[1485.106, "o", "        ----------\r\n"]
[1485.116, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[1485.126, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1485.136, "o", "            and `n_features` is the number of features.\r\n"]
[1485.146, "o", "\r\n"]
[1485.156, "o", "        y : Ignored\r\n"]
[1485.166, "o", "            Not used, present for API consistency by convention.\r\n"]
[1485.176, "o", "\r\n"]
[1485.186, "o", "        Returns\r\n"]
[1485.196, "o", "        -------\r\n"]
[1485.206, "o", "        V : ndarray of shape (n_samples, n_components)\r\n"]
[1485.216, "o", "            Transformed data.\r\n"]
[1485.226, "o", "        \"\"\"\r\n"]
[1485.236, "o", "        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n"]
[1485.246, "o", "\r\n"]
[1485.256, "o", "        method = \"lasso_\" + self.fit_algorithm\r\n"]
[1485.266, "o", "\r\n"]
[1485.276, "o", "        random_state = check_random_state(self.random_state)\r\n"]
[1485.286, "o", "        X = self._validate_data(X)\r\n"]
[1485.296, "o", "\r\n"]
[1485.306, "o", "        if self.n_components is None:\r\n"]
[1485.316, "o", "            n_components = X.shape[1]\r\n"]
[1485.326, "o", "        else:\r\n"]
[1485.336, "o", "            n_components = self.n_components\r\n"]
[1485.346, "o", "\r\n"]
[1485.356, "o", "        V, U, E, self.n_iter_ = _dict_learning(\r\n"]
[1485.366, "o", "            X,\r\n"]
[1485.376, "o", "            n_components,\r\n"]
[1485.386, "o", "            alpha=self.alpha,\r\n"]
[1485.396, "o", "            tol=self.tol,\r\n"]
[1485.406, "o", "            max_iter=self.max_iter,\r\n"]
[1485.416, "o", "            method=method,\r\n"]
[1485.426, "o", "            method_max_iter=self.transform_max_iter,\r\n"]
[1485.436, "o", "            n_jobs=self.n_jobs,\r\n"]
[1485.446, "o", "            code_init=self.code_init,\r\n"]
[1485.456, "o", "            dict_init=self.dict_init,\r\n"]
[1485.466, "o", "            callback=self.callback,\r\n"]
[1485.476, "o", "            verbose=self.verbose,\r\n"]
[1485.486, "o", "            random_state=random_state,\r\n"]
[1485.496, "o", "            return_n_iter=True,\r\n"]
[1485.506, "o", "            positive_dict=self.positive_dict,\r\n"]
[1485.516, "o", "            positive_code=self.positive_code,\r\n"]
[1485.526, "o", "        )\r\n"]
[1485.536, "o", "        self.components_ = U\r\n"]
[1485.546, "o", "        self.error_ = E\r\n"]
[1485.556, "o", "\r\n"]
[1485.566, "o", "        return V\r\n"]
[1485.576, "o", "\r\n"]
[1485.586, "o", "    @property\r\n"]
[1485.596, "o", "    def _n_features_out(self):\r\n"]
[1485.606, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[1485.616, "o", "        return self.components_.shape[0]\r\n"]
[1485.626, "o", "\r\n"]
[1485.636, "o", "    def _more_tags(self):\r\n"]
[1485.646, "o", "        return {\r\n"]
[1485.656, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[1485.666, "o", "        }\r\n"]
[1485.676, "o", "\r\n"]
[1485.686, "o", "\r\n"]
[1485.696, "o", "class MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[1485.706, "o", "    \"\"\"Mini-batch dictionary learning.\r\n"]
[1485.716, "o", "\r\n"]
[1485.726, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[1485.736, "o", "    encoding the fitted data.\r\n"]
[1485.746, "o", "\r\n"]
[1485.756, "o", "    Solves the optimization problem::\r\n"]
[1485.766, "o", "\r\n"]
[1485.776, "o", "       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[1485.786, "o", "                    (U,V)\r\n"]
[1485.796, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[1485.806, "o", "\r\n"]
[1485.816, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[1485.826, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[1485.836, "o", "    of all the entries in the matrix.\r\n"]
[1485.846, "o", "\r\n"]
[1485.856, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[1485.866, "o", "\r\n"]
[1485.876, "o", "    Parameters\r\n"]
[1485.886, "o", "    ----------\r\n"]
[1485.896, "o", "    n_components : int, default=None\r\n"]
[1485.906, "o", "        Number of dictionary elements to extract.\r\n"]
[1485.916, "o", "\r\n"]
[1485.926, "o", "    alpha : float, default=1\r\n"]
[1485.936, "o", "        Sparsity controlling parameter.\r\n"]
[1485.946, "o", "\r\n"]
[1485.956, "o", "    n_iter : int, default=1000\r\n"]
[1485.966, "o", "        Total number of iterations over data batches to perform.\r\n"]
[1485.976, "o", "\r\n"]
[1485.986, "o", "        .. deprecated:: 1.1\r\n"]
[1485.996, "o", "           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[1486.006, "o", "           ``max_iter`` instead.\r\n"]
[1486.016, "o", "\r\n"]
[1486.026, "o", "    max_iter : int, default=None\r\n"]
[1486.036, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[1486.046, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[1486.056, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[1486.066, "o", "\r\n"]
[1486.076, "o", "        .. versionadded:: 1.1\r\n"]
[1486.086, "o", "\r\n"]
[1486.096, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[1486.106, "o", "        The algorithm used:\r\n"]
[1486.116, "o", "\r\n"]
[1486.126, "o", "        - `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[1486.136, "o", "          problem (`linear_model.lars_path`)\r\n"]
[1486.146, "o", "        - `'cd'`: uses the coordinate descent method to compute the\r\n"]
[1486.156, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[1486.166, "o", "          the estimated components are sparse.\r\n"]
[1486.176, "o", "\r\n"]
[1486.186, "o", "    n_jobs : int, default=None\r\n"]
[1486.196, "o", "        Number of parallel jobs to run.\r\n"]
[1486.206, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1486.216, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1486.226, "o", "        for more details.\r\n"]
[1486.236, "o", "\r\n"]
[1486.246, "o", "    batch_size : int, default=256\r\n"]
[1486.256, "o", "        Number of samples in each mini-batch.\r\n"]
[1486.266, "o", "\r\n"]
[1486.276, "o", "        .. versionchanged:: 1.3\r\n"]
[1486.286, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[1486.296, "o", "\r\n"]
[1486.306, "o", "    shuffle : bool, default=True\r\n"]
[1486.316, "o", "        Whether to shuffle the samples before forming batches.\r\n"]
[1486.326, "o", "\r\n"]
[1486.336, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1486.346, "o", "        Initial value of the dictionary for warm restart scenarios.\r\n"]
[1486.356, "o", "\r\n"]
[1486.366, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[1486.376, "o", "            'threshold'}, default='omp'\r\n"]
[1486.386, "o", "        Algorithm used to transform the data:\r\n"]
[1486.396, "o", "\r\n"]
[1486.406, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[1486.416, "o", "          (`linear_model.lars_path`);\r\n"]
[1486.426, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[1486.436, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1486.446, "o", "          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n"]
[1486.456, "o", "          if the estimated components are sparse.\r\n"]
[1486.466, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1486.476, "o", "          solution.\r\n"]
[1486.486, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[1486.496, "o", "          the projection ``dictionary * X'``.\r\n"]
[1486.506, "o", "\r\n"]
[1486.516, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[1486.526, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1486.536, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[1486.546, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[1486.556, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1486.566, "o", "\r\n"]
[1486.576, "o", "    transform_alpha : float, default=None\r\n"]
[1486.586, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1486.596, "o", "        penalty applied to the L1 norm.\r\n"]
[1486.606, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1486.616, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1486.626, "o", "        If `None`, defaults to `alpha`.\r\n"]
[1486.636, "o", "\r\n"]
[1486.646, "o", "        .. versionchanged:: 1.2\r\n"]
[1486.656, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[1486.666, "o", "\r\n"]
[1486.676, "o", "    verbose : bool or int, default=False\r\n"]
[1486.686, "o", "        To control the verbosity of the procedure.\r\n"]
[1486.696, "o", "\r\n"]
[1486.706, "o", "    split_sign : bool, default=False\r\n"]
[1486.716, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[1486.726, "o", "        its negative part and its positive part. This can improve the\r\n"]
[1486.736, "o", "        performance of downstream classifiers.\r\n"]
[1486.746, "o", "\r\n"]
[1486.756, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1486.766, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[1486.776, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[1486.786, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[1486.796, "o", "        results across multiple function calls.\r\n"]
[1486.806, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1486.816, "o", "\r\n"]
[1486.826, "o", "    positive_code : bool, default=False\r\n"]
[1486.836, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1486.846, "o", "\r\n"]
[1486.856, "o", "        .. versionadded:: 0.20\r\n"]
[1486.866, "o", "\r\n"]
[1486.876, "o", "    positive_dict : bool, default=False\r\n"]
[1486.886, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1486.896, "o", "\r\n"]
[1486.906, "o", "        .. versionadded:: 0.20\r\n"]
[1486.916, "o", "\r\n"]
[1486.926, "o", "    transform_max_iter : int, default=1000\r\n"]
[1486.936, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1486.946, "o", "        `'lasso_lars'`.\r\n"]
[1486.956, "o", "\r\n"]
[1486.966, "o", "        .. versionadded:: 0.22\r\n"]
[1486.976, "o", "\r\n"]
[1486.986, "o", "    callback : callable, default=None\r\n"]
[1486.996, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[1487.006, "o", "\r\n"]
[1487.016, "o", "        .. versionadded:: 1.1\r\n"]
[1487.026, "o", "\r\n"]
[1487.036, "o", "    tol : float, default=1e-3\r\n"]
[1487.046, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[1487.056, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[1487.066, "o", "\r\n"]
[1487.076, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[1487.086, "o", "        `tol` to 0.0.\r\n"]
[1487.096, "o", "\r\n"]
[1487.106, "o", "        .. versionadded:: 1.1\r\n"]
[1487.116, "o", "\r\n"]
[1487.126, "o", "    max_no_improvement : int, default=10\r\n"]
[1487.136, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[1487.146, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[1487.156, "o", "        `max_iter` is not None.\r\n"]
[1487.166, "o", "\r\n"]
[1487.176, "o", "        To disable convergence detection based on cost function, set\r\n"]
[1487.186, "o", "        `max_no_improvement` to None.\r\n"]
[1487.196, "o", "\r\n"]
[1487.206, "o", "        .. versionadded:: 1.1\r\n"]
[1487.216, "o", "\r\n"]
[1487.226, "o", "    Attributes\r\n"]
[1487.236, "o", "    ----------\r\n"]
[1487.246, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[1487.256, "o", "        Components extracted from the data.\r\n"]
[1487.266, "o", "\r\n"]
[1487.276, "o", "    n_features_in_ : int\r\n"]
[1487.286, "o", "        Number of features seen during :term:`fit`.\r\n"]
[1487.296, "o", "\r\n"]
[1487.306, "o", "        .. versionadded:: 0.24\r\n"]
[1487.316, "o", "\r\n"]
[1487.326, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[1487.336, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[1487.346, "o", "        has feature names that are all strings.\r\n"]
[1487.356, "o", "\r\n"]
[1487.366, "o", "        .. versionadded:: 1.0\r\n"]
[1487.376, "o", "\r\n"]
[1487.386, "o", "    n_iter_ : int\r\n"]
[1487.396, "o", "        Number of iterations over the full dataset.\r\n"]
[1487.406, "o", "\r\n"]
[1487.416, "o", "    n_steps_ : int\r\n"]
[1487.426, "o", "        Number of mini-batches processed.\r\n"]
[1487.436, "o", "\r\n"]
[1487.446, "o", "        .. versionadded:: 1.1\r\n"]
[1487.456, "o", "\r\n"]
[1487.466, "o", "    See Also\r\n"]
[1487.476, "o", "    --------\r\n"]
[1487.486, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1487.496, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1487.506, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[1487.516, "o", "        precomputed dictionary.\r\n"]
[1487.526, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1487.536, "o", "\r\n"]
[1487.546, "o", "    References\r\n"]
[1487.556, "o", "    ----------\r\n"]
[1487.566, "o", "\r\n"]
[1487.576, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[1487.586, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[1487.596, "o", "\r\n"]
[1487.606, "o", "    Examples\r\n"]
[1487.616, "o", "    --------\r\n"]
[1487.626, "o", "    >>> import numpy as np\r\n"]
[1487.636, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[1487.646, "o", "    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n"]
[1487.656, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[1487.666, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[1487.676, "o", "    ...     random_state=42)\r\n"]
[1487.686, "o", "    >>> dict_learner = MiniBatchDictionaryLearning(\r\n"]
[1487.696, "o", "    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n"]
[1487.706, "o", "    ...     transform_alpha=0.1, random_state=42)\r\n"]
[1487.716, "o", "    >>> X_transformed = dict_learner.fit_transform(X)\r\n"]
[1487.726, "o", "\r\n"]
[1487.736, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[1487.746, "o", "\r\n"]
[1487.756, "o", "    >>> np.mean(X_transformed == 0) < 0.5\r\n"]
[1487.766, "o", "    True\r\n"]
[1487.776, "o", "\r\n"]
[1487.786, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[1487.796, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[1487.806, "o", "    the original signal:\r\n"]
[1487.816, "o", "\r\n"]
[1487.826, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[1487.836, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[1487.846, "o", "    0.057...\r\n"]
[1487.856, "o", "    \"\"\"\r\n"]
[1487.866, "o", "\r\n"]
[1487.876, "o", "    _parameter_constraints: dict = {\r\n"]
[1487.886, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1487.896, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1487.906, "o", "        \"n_iter\": [\r\n"]
[1487.916, "o", "            Interval(Integral, 0, None, closed=\"left\"),\r\n"]
[1487.926, "o", "            Hidden(StrOptions({\"deprecated\"})),\r\n"]
[1487.936, "o", "        ],\r\n"]
[1487.946, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n"]
[1487.956, "o", "        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n"]
[1487.966, "o", "        \"n_jobs\": [None, Integral],\r\n"]
[1487.976, "o", "        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n"]
[1487.986, "o", "        \"shuffle\": [\"boolean\"],\r\n"]
[1487.996, "o", "        \"dict_init\": [None, np.ndarray],\r\n"]
[1488.006, "o", "        \"transform_algorithm\": [\r\n"]
[1488.016, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[1488.026, "o", "        ],\r\n"]
[1488.036, "o", "        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1488.046, "o", "        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[1488.056, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[1488.066, "o", "        \"split_sign\": [\"boolean\"],\r\n"]
[1488.076, "o", "        \"random_state\": [\"random_state\"],\r\n"]
[1488.086, "o", "        \"positive_code\": [\"boolean\"],\r\n"]
[1488.096, "o", "        \"positive_dict\": [\"boolean\"],\r\n"]
[1488.106, "o", "        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1488.116, "o", "        \"callback\": [None, callable],\r\n"]
[1488.126, "o", "        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1488.136, "o", "        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n"]
[1488.146, "o", "    }\r\n"]
[1488.156, "o", "\r\n"]
[1488.166, "o", "    def __init__(\r\n"]
[1488.176, "o", "        self,\r\n"]
[1488.186, "o", "        n_components=None,\r\n"]
[1488.196, "o", "        *,\r\n"]
[1488.206, "o", "        alpha=1,\r\n"]
[1488.216, "o", "        n_iter=\"deprecated\",\r\n"]
[1488.226, "o", "        max_iter=None,\r\n"]
[1488.236, "o", "        fit_algorithm=\"lars\",\r\n"]
[1488.246, "o", "        n_jobs=None,\r\n"]
[1488.256, "o", "        batch_size=256,\r\n"]
[1488.266, "o", "        shuffle=True,\r\n"]
[1488.276, "o", "        dict_init=None,\r\n"]
[1488.286, "o", "        transform_algorithm=\"omp\",\r\n"]
[1488.296, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[1488.306, "o", "        transform_alpha=None,\r\n"]
[1488.316, "o", "        verbose=False,\r\n"]
[1488.326, "o", "        split_sign=False,\r\n"]
[1488.336, "o", "        random_state=None,\r\n"]
[1488.346, "o", "        positive_code=False,\r\n"]
[1488.356, "o", "        positive_dict=False,\r\n"]
[1488.366, "o", "        transform_max_iter=1000,\r\n"]
[1488.376, "o", "        callback=None,\r\n"]
[1488.386, "o", "        tol=1e-3,\r\n"]
[1488.396, "o", "        max_no_improvement=10,\r\n"]
[1488.406, "o", "    ):\r\n"]
[1488.416, "o", "        super().__init__(\r\n"]
[1488.426, "o", "            transform_algorithm,\r\n"]
[1488.436, "o", "            transform_n_nonzero_coefs,\r\n"]
[1488.446, "o", "            transform_alpha,\r\n"]
[1488.456, "o", "            split_sign,\r\n"]
[1488.466, "o", "            n_jobs,\r\n"]
[1488.476, "o", "            positive_code,\r\n"]
[1488.486, "o", "            transform_max_iter,\r\n"]
[1488.496, "o", "        )\r\n"]
[1488.506, "o", "        self.n_components = n_components\r\n"]
[1488.516, "o", "        self.alpha = alpha\r\n"]
[1488.526, "o", "        self.n_iter = n_iter\r\n"]
[1488.536, "o", "        self.max_iter = max_iter\r\n"]
[1488.546, "o", "        self.fit_algorithm = fit_algorithm\r\n"]
[1488.556, "o", "        self.dict_init = dict_init\r\n"]
[1488.566, "o", "        self.verbose = verbose\r\n"]
[1488.576, "o", "        self.shuffle = shuffle\r\n"]
[1488.586, "o", "        self.batch_size = batch_size\r\n"]
[1488.596, "o", "        self.split_sign = split_sign\r\n"]
[1488.606, "o", "        self.random_state = random_state\r\n"]
[1488.616, "o", "        self.positive_dict = positive_dict\r\n"]
[1488.626, "o", "        self.callback = callback\r\n"]
[1488.636, "o", "        self.max_no_improvement = max_no_improvement\r\n"]
[1488.646, "o", "        self.tol = tol\r\n"]
[1488.656, "o", "\r\n"]
[1488.666, "o", "    def _check_params(self, X):\r\n"]
[1488.676, "o", "        # n_components\r\n"]
[1488.686, "o", "        self._n_components = self.n_components\r\n"]
[1488.696, "o", "        if self._n_components is None:\r\n"]
[1488.706, "o", "            self._n_components = X.shape[1]\r\n"]
[1488.716, "o", "\r\n"]
[1488.726, "o", "        # fit_algorithm\r\n"]
[1488.736, "o", "        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n"]
[1488.746, "o", "        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n"]
[1488.756, "o", "\r\n"]
[1488.766, "o", "        # batch_size\r\n"]
[1488.776, "o", "        self._batch_size = min(self.batch_size, X.shape[0])\r\n"]
[1488.786, "o", "\r\n"]
[1488.796, "o", "    def _initialize_dict(self, X, random_state):\r\n"]
[1488.806, "o", "        \"\"\"Initialization of the dictionary.\"\"\"\r\n"]
[1488.816, "o", "        if self.dict_init is not None:\r\n"]
[1488.826, "o", "            dictionary = self.dict_init\r\n"]
[1488.836, "o", "        else:\r\n"]
[1488.846, "o", "            # Init V with SVD of X\r\n"]
[1488.856, "o", "            _, S, dictionary = randomized_svd(\r\n"]
[1488.866, "o", "                X, self._n_components, random_state=random_state\r\n"]
[1488.876, "o", "            )\r\n"]
[1488.886, "o", "            dictionary = S[:, np.newaxis] * dictionary\r\n"]
[1488.896, "o", "\r\n"]
[1488.906, "o", "        if self._n_components <= len(dictionary):\r\n"]
[1488.916, "o", "            dictionary = dictionary[: self._n_components, :]\r\n"]
[1488.926, "o", "        else:\r\n"]
[1488.936, "o", "            dictionary = np.concatenate(\r\n"]
[1488.946, "o", "                (\r\n"]
[1488.956, "o", "                    dictionary,\r\n"]
[1488.966, "o", "                    np.zeros(\r\n"]
[1488.976, "o", "                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n"]
[1488.986, "o", "                        dtype=dictionary.dtype,\r\n"]
[1488.996, "o", "                    ),\r\n"]
[1489.006, "o", "                )\r\n"]
[1489.016, "o", "            )\r\n"]
[1489.026, "o", "\r\n"]
[1489.036, "o", "        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n"]
[1489.046, "o", "        dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[1489.056, "o", "\r\n"]
[1489.066, "o", "        return dictionary\r\n"]
[1489.076, "o", "\r\n"]
[1489.086, "o", "    def _update_inner_stats(self, X, code, batch_size, step):\r\n"]
[1489.096, "o", "        \"\"\"Update the inner stats inplace.\"\"\"\r\n"]
[1489.106, "o", "        if step < batch_size - 1:\r\n"]
[1489.116, "o", "            theta = (step + 1) * batch_size\r\n"]
[1489.126, "o", "        else:\r\n"]
[1489.136, "o", "            theta = batch_size**2 + step + 1 - batch_size\r\n"]
[1489.146, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[1489.156, "o", "\r\n"]
[1489.166, "o", "        self._A *= beta\r\n"]
[1489.176, "o", "        self._A += code.T @ code / batch_size\r\n"]
[1489.186, "o", "        self._B *= beta\r\n"]
[1489.196, "o", "        self._B += X.T @ code / batch_size\r\n"]
[1489.206, "o", "\r\n"]
[1489.216, "o", "    def _minibatch_step(self, X, dictionary, random_state, step):\r\n"]
[1489.226, "o", "        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n"]
[1489.236, "o", "        batch_size = X.shape[0]\r\n"]
[1489.246, "o", "\r\n"]
[1489.256, "o", "        # Compute code for this batch\r\n"]
[1489.266, "o", "        code = _sparse_encode(\r\n"]
[1489.276, "o", "            X,\r\n"]
[1489.286, "o", "            dictionary,\r\n"]
[1489.296, "o", "            algorithm=self._fit_algorithm,\r\n"]
[1489.306, "o", "            alpha=self.alpha,\r\n"]
[1489.316, "o", "            n_jobs=self.n_jobs,\r\n"]
[1489.326, "o", "            positive=self.positive_code,\r\n"]
[1489.336, "o", "            max_iter=self.transform_max_iter,\r\n"]
[1489.346, "o", "            verbose=self.verbose,\r\n"]
[1489.356, "o", "        )\r\n"]
[1489.366, "o", "\r\n"]
[1489.376, "o", "        batch_cost = (\r\n"]
[1489.386, "o", "            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n"]
[1489.396, "o", "            + self.alpha * np.sum(np.abs(code))\r\n"]
[1489.406, "o", "        ) / batch_size\r\n"]
[1489.416, "o", "\r\n"]
[1489.426, "o", "        # Update inner stats\r\n"]
[1489.436, "o", "        self._update_inner_stats(X, code, batch_size, step)\r\n"]
[1489.446, "o", "\r\n"]
[1489.456, "o", "        # Update dictionary\r\n"]
[1489.466, "o", "        _update_dict(\r\n"]
[1489.476, "o", "            dictionary,\r\n"]
[1489.486, "o", "            X,\r\n"]
[1489.496, "o", "            code,\r\n"]
[1489.506, "o", "            self._A,\r\n"]
[1489.516, "o", "            self._B,\r\n"]
[1489.526, "o", "            verbose=self.verbose,\r\n"]
[1489.536, "o", "            random_state=random_state,\r\n"]
[1489.546, "o", "            positive=self.positive_dict,\r\n"]
[1489.556, "o", "        )\r\n"]
[1489.566, "o", "\r\n"]
[1489.576, "o", "        return batch_cost\r\n"]
[1489.586, "o", "\r\n"]
[1489.596, "o", "    def _check_convergence(\r\n"]
[1489.606, "o", "        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n"]
[1489.616, "o", "    ):\r\n"]
[1489.626, "o", "        \"\"\"Helper function to encapsulate the early stopping logic.\r\n"]
[1489.636, "o", "\r\n"]
[1489.646, "o", "        Early stopping is based on two factors:\r\n"]
[1489.656, "o", "        - A small change of the dictionary between two minibatch updates. This is\r\n"]
[1489.666, "o", "          controlled by the tol parameter.\r\n"]
[1489.676, "o", "        - No more improvement on a smoothed estimate of the objective function for a\r\n"]
[1489.686, "o", "          a certain number of consecutive minibatch updates. This is controlled by\r\n"]
[1489.696, "o", "          the max_no_improvement parameter.\r\n"]
[1489.706, "o", "        \"\"\"\r\n"]
[1489.716, "o", "        batch_size = X.shape[0]\r\n"]
[1489.726, "o", "\r\n"]
[1489.736, "o", "        # counts steps starting from 1 for user friendly verbose mode.\r\n"]
[1489.746, "o", "        step = step + 1\r\n"]
[1489.756, "o", "\r\n"]
[1489.766, "o", "        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n"]
[1489.776, "o", "        # too bad value\r\n"]
[1489.786, "o", "        if step <= min(100, n_samples / batch_size):\r\n"]
[1489.796, "o", "            if self.verbose:\r\n"]
[1489.806, "o", "                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n"]
[1489.816, "o", "            return False\r\n"]
[1489.826, "o", "\r\n"]
[1489.836, "o", "        # Compute an Exponentially Weighted Average of the cost function to\r\n"]
[1489.846, "o", "        # monitor the convergence while discarding minibatch-local stochastic\r\n"]
[1489.856, "o", "        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n"]
[1489.866, "o", "        if self._ewa_cost is None:\r\n"]
[1489.876, "o", "            self._ewa_cost = batch_cost\r\n"]
[1489.886, "o", "        else:\r\n"]
[1489.896, "o", "            alpha = batch_size / (n_samples + 1)\r\n"]
[1489.906, "o", "            alpha = min(alpha, 1)\r\n"]
[1489.916, "o", "            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n"]
[1489.926, "o", "\r\n"]
[1489.936, "o", "        if self.verbose:\r\n"]
[1490.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1490.002, "i", "grep -R \"_minibatch_step\\s*\\(\" -n sklearn || true\r"]
[1490.004, "o", "grep -R \"_minibatch_step\\s*\\(\" -n sklearn || true\r\n"]
[1492.478, "o", "\u001b[?2004l\r\n"]
[1495.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1495.002, "i", "grep -R \"_sparse_encode_precomputed\\s*\\(\" -n sklearn || true\r"]
[1495.004, "o", "grep -R \"_sparse_encode_precomputed\\s*\\(\" -n sklearn || true\r\n"]
[1497.478, "o", "\u001b[?2004l\r\n"]
[1500.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1500.002, "i", "grep -R \"sparse_encode\\s*\\(\" -n sklearn/decomposition || true\r"]
[1500.004, "o", "grep -R \"sparse_encode\\s*\\(\" -n sklearn/decomposition || true\r\n"]
[1502.478, "o", "\u001b[?2004l\r\n"]
[1505.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1505.002, "i", "sed -n '1,400p' sklearn/decomposition/_dict_learning.py\r"]
[1505.004, "o", "sed -n '1,400p' sklearn/decomposition/_dict_learning.py\r\n"]
[1505.018329, "o", "\u001b[?2004l\r\n"]
[1505.030658, "o", "\"\"\" Dictionary learning.\r\n"]
[1505.042988, "o", "\"\"\"\r\n"]
[1505.055317, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[1505.067646, "o", "# License: BSD 3 clause\r\n"]
[1505.079975, "o", "\r\n"]
[1505.092304, "o", "import itertools\r\n"]
[1505.104633, "o", "import sys\r\n"]
[1505.116963, "o", "import time\r\n"]
[1505.129292, "o", "import warnings\r\n"]
[1505.141621, "o", "from math import ceil\r\n"]
[1505.15395, "o", "from numbers import Integral, Real\r\n"]
[1505.166279, "o", "\r\n"]
[1505.178608, "o", "import numpy as np\r\n"]
[1505.190938, "o", "from joblib import effective_n_jobs\r\n"]
[1505.203267, "o", "from scipy import linalg\r\n"]
[1505.215596, "o", "\r\n"]
[1505.227925, "o", "from ..base import (\r\n"]
[1505.240254, "o", "    BaseEstimator,\r\n"]
[1505.252584, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[1505.264913, "o", "    TransformerMixin,\r\n"]
[1505.277242, "o", "    _fit_context,\r\n"]
[1505.289571, "o", ")\r\n"]
[1505.3019, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[1505.314229, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[1505.326559, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[1505.338888, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[1505.351217, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[1505.363546, "o", "from ..utils.validation import check_is_fitted\r\n"]
[1505.375875, "o", "\r\n"]
[1505.388204, "o", "\r\n"]
[1505.400534, "o", "def _check_positive_coding(method, positive):\r\n"]
[1505.412863, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[1505.425192, "o", "        raise ValueError(\r\n"]
[1505.437521, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[1505.44985, "o", "        )\r\n"]
[1505.46218, "o", "\r\n"]
[1505.474509, "o", "\r\n"]
[1505.486838, "o", "def _sparse_encode_precomputed(\r\n"]
[1505.499167, "o", "    X,\r\n"]
[1505.511496, "o", "    dictionary,\r\n"]
[1505.523825, "o", "    *,\r\n"]
[1505.536155, "o", "    gram=None,\r\n"]
[1505.548484, "o", "    cov=None,\r\n"]
[1505.560813, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1505.573142, "o", "    regularization=None,\r\n"]
[1505.585471, "o", "    copy_cov=True,\r\n"]
[1505.5978, "o", "    init=None,\r\n"]
[1505.61013, "o", "    max_iter=1000,\r\n"]
[1505.622459, "o", "    verbose=0,\r\n"]
[1505.634788, "o", "    positive=False,\r\n"]
[1505.647117, "o", "):\r\n"]
[1505.659446, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[1505.671776, "o", "\r\n"]
[1505.684105, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[1505.696434, "o", "\r\n"]
[1505.708763, "o", "    Parameters\r\n"]
[1505.721092, "o", "    ----------\r\n"]
[1505.733421, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[1505.745751, "o", "        Data matrix.\r\n"]
[1505.75808, "o", "\r\n"]
[1505.770409, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1505.782738, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1505.795067, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[1505.807397, "o", "\r\n"]
[1505.819726, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[1505.832055, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[1505.844384, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[1505.856713, "o", "\r\n"]
[1505.869042, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[1505.881372, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[1505.893701, "o", "\r\n"]
[1505.90603, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1505.918359, "o", "            default='lasso_lars'\r\n"]
[1505.930688, "o", "        The algorithm used:\r\n"]
[1505.943017, "o", "\r\n"]
[1505.955347, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[1505.967676, "o", "          (`linear_model.lars_path`);\r\n"]
[1505.980005, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1505.992334, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1506.004663, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1506.016993, "o", "          the estimated components are sparse;\r\n"]
[1506.029322, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1506.041651, "o", "          solution;\r\n"]
[1506.05398, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1506.066309, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[1506.078638, "o", "\r\n"]
[1506.090968, "o", "    regularization : int or float, default=None\r\n"]
[1506.103297, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[1506.115626, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[1506.127955, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[1506.140284, "o", "\r\n"]
[1506.152613, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1506.164943, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[1506.177272, "o", "        `algorithm='lasso_cd'`.\r\n"]
[1506.189601, "o", "\r\n"]
[1506.20193, "o", "    max_iter : int, default=1000\r\n"]
[1506.214259, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1506.226589, "o", "        `'lasso_lars'`.\r\n"]
[1506.238918, "o", "\r\n"]
[1506.251247, "o", "    copy_cov : bool, default=True\r\n"]
[1506.263576, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1506.275905, "o", "        be overwritten.\r\n"]
[1506.288234, "o", "\r\n"]
[1506.300564, "o", "    verbose : int, default=0\r\n"]
[1506.312893, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[1506.325222, "o", "\r\n"]
[1506.337551, "o", "    positive: bool, default=False\r\n"]
[1506.34988, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[1506.362209, "o", "\r\n"]
[1506.374539, "o", "        .. versionadded:: 0.20\r\n"]
[1506.386868, "o", "\r\n"]
[1506.399197, "o", "    Returns\r\n"]
[1506.411526, "o", "    -------\r\n"]
[1506.423855, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[1506.436185, "o", "        The sparse codes.\r\n"]
[1506.448514, "o", "    \"\"\"\r\n"]
[1506.460843, "o", "    n_samples, n_features = X.shape\r\n"]
[1506.473172, "o", "    n_components = dictionary.shape[0]\r\n"]
[1506.485501, "o", "\r\n"]
[1506.49783, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[1506.51016, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1506.522489, "o", "        try:\r\n"]
[1506.534818, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1506.547147, "o", "\r\n"]
[1506.559476, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1506.571805, "o", "            # corrects the verbosity level.\r\n"]
[1506.584135, "o", "            lasso_lars = LassoLars(\r\n"]
[1506.596464, "o", "                alpha=alpha,\r\n"]
[1506.608793, "o", "                fit_intercept=False,\r\n"]
[1506.621122, "o", "                verbose=verbose,\r\n"]
[1506.633451, "o", "                precompute=gram,\r\n"]
[1506.645781, "o", "                fit_path=False,\r\n"]
[1506.65811, "o", "                positive=positive,\r\n"]
[1506.670439, "o", "                max_iter=max_iter,\r\n"]
[1506.682768, "o", "            )\r\n"]
[1506.695097, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1506.707426, "o", "            new_code = lasso_lars.coef_\r\n"]
[1506.719756, "o", "        finally:\r\n"]
[1506.732085, "o", "            np.seterr(**err_mgt)\r\n"]
[1506.744414, "o", "\r\n"]
[1506.756743, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[1506.769072, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1506.781401, "o", "\r\n"]
[1506.793731, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[1506.80606, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[1506.818389, "o", "        # argument that we could pass in from Lasso.\r\n"]
[1506.830718, "o", "        clf = Lasso(\r\n"]
[1506.843047, "o", "            alpha=alpha,\r\n"]
[1506.855377, "o", "            fit_intercept=False,\r\n"]
[1506.867706, "o", "            precompute=gram,\r\n"]
[1506.880035, "o", "            max_iter=max_iter,\r\n"]
[1506.892364, "o", "            warm_start=True,\r\n"]
[1506.904693, "o", "            positive=positive,\r\n"]
[1506.917022, "o", "        )\r\n"]
[1506.929352, "o", "\r\n"]
[1506.941681, "o", "        if init is not None:\r\n"]
[1506.95401, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[1506.966339, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[1506.978668, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[1506.990998, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[1507.003327, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[1507.015656, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[1507.027985, "o", "                init = np.array(init)\r\n"]
[1507.040314, "o", "            clf.coef_ = init\r\n"]
[1507.052643, "o", "\r\n"]
[1507.064973, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[1507.077302, "o", "        new_code = clf.coef_\r\n"]
[1507.089631, "o", "\r\n"]
[1507.10196, "o", "    elif algorithm == \"lars\":\r\n"]
[1507.114289, "o", "        try:\r\n"]
[1507.126618, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1507.138948, "o", "\r\n"]
[1507.151277, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1507.163606, "o", "            # corrects the verbosity level.\r\n"]
[1507.175935, "o", "            lars = Lars(\r\n"]
[1507.188264, "o", "                fit_intercept=False,\r\n"]
[1507.200594, "o", "                verbose=verbose,\r\n"]
[1507.212923, "o", "                precompute=gram,\r\n"]
[1507.225252, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[1507.237581, "o", "                fit_path=False,\r\n"]
[1507.24991, "o", "            )\r\n"]
[1507.262239, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1507.274569, "o", "            new_code = lars.coef_\r\n"]
[1507.286898, "o", "        finally:\r\n"]
[1507.299227, "o", "            np.seterr(**err_mgt)\r\n"]
[1507.311556, "o", "\r\n"]
[1507.323885, "o", "    elif algorithm == \"threshold\":\r\n"]
[1507.336214, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[1507.348544, "o", "        if positive:\r\n"]
[1507.360873, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[1507.373202, "o", "\r\n"]
[1507.385531, "o", "    elif algorithm == \"omp\":\r\n"]
[1507.39786, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[1507.41019, "o", "            Gram=gram,\r\n"]
[1507.422519, "o", "            Xy=cov,\r\n"]
[1507.434848, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[1507.447177, "o", "            tol=None,\r\n"]
[1507.459506, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[1507.471835, "o", "            copy_Xy=copy_cov,\r\n"]
[1507.484165, "o", "        ).T\r\n"]
[1507.496494, "o", "\r\n"]
[1507.508823, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[1507.521152, "o", "\r\n"]
[1507.533481, "o", "\r\n"]
[1507.54581, "o", "@validate_params(\r\n"]
[1507.55814, "o", "    {\r\n"]
[1507.570469, "o", "        \"X\": [\"array-like\"],\r\n"]
[1507.582798, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[1507.595127, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[1507.607456, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[1507.619786, "o", "        \"algorithm\": [\r\n"]
[1507.632115, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[1507.644444, "o", "        ],\r\n"]
[1507.656773, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1507.669102, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[1507.681431, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[1507.693761, "o", "        \"init\": [\"array-like\", None],\r\n"]
[1507.70609, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1507.718419, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[1507.730748, "o", "        \"check_input\": [\"boolean\"],\r\n"]
[1507.743077, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[1507.755406, "o", "        \"positive\": [\"boolean\"],\r\n"]
[1507.767736, "o", "    },\r\n"]
[1507.780065, "o", "    prefer_skip_nested_validation=True,\r\n"]
[1507.792394, "o", ")\r\n"]
[1507.804723, "o", "# XXX : could be moved to the linear_model module\r\n"]
[1507.817052, "o", "def sparse_encode(\r\n"]
[1507.829382, "o", "    X,\r\n"]
[1507.841711, "o", "    dictionary,\r\n"]
[1507.85404, "o", "    *,\r\n"]
[1507.866369, "o", "    gram=None,\r\n"]
[1507.878698, "o", "    cov=None,\r\n"]
[1507.891027, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1507.903357, "o", "    n_nonzero_coefs=None,\r\n"]
[1507.915686, "o", "    alpha=None,\r\n"]
[1507.928015, "o", "    copy_cov=True,\r\n"]
[1507.940344, "o", "    init=None,\r\n"]
[1507.952673, "o", "    max_iter=1000,\r\n"]
[1507.965002, "o", "    n_jobs=None,\r\n"]
[1507.977332, "o", "    check_input=True,\r\n"]
[1507.989661, "o", "    verbose=0,\r\n"]
[1508.00199, "o", "    positive=False,\r\n"]
[1508.014319, "o", "):\r\n"]
[1508.026648, "o", "    \"\"\"Sparse coding.\r\n"]
[1508.038978, "o", "\r\n"]
[1508.051307, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[1508.063636, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[1508.075965, "o", "\r\n"]
[1508.088294, "o", "        X ~= code * dictionary\r\n"]
[1508.100623, "o", "\r\n"]
[1508.112953, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[1508.125282, "o", "\r\n"]
[1508.137611, "o", "    Parameters\r\n"]
[1508.14994, "o", "    ----------\r\n"]
[1508.162269, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[1508.174599, "o", "        Data matrix.\r\n"]
[1508.186928, "o", "\r\n"]
[1508.199257, "o", "    dictionary : array-like of shape (n_components, n_features)\r\n"]
[1508.211586, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1508.223915, "o", "        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[1508.236244, "o", "        output.\r\n"]
[1508.248574, "o", "\r\n"]
[1508.260903, "o", "    gram : array-like of shape (n_components, n_components), default=None\r\n"]
[1508.273232, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n"]
[1508.285561, "o", "\r\n"]
[1508.29789, "o", "    cov : array-like of shape (n_components, n_samples), default=None\r\n"]
[1508.310219, "o", "        Precomputed covariance, `dictionary' * X`.\r\n"]
[1508.322549, "o", "\r\n"]
[1508.334878, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1508.347207, "o", "            default='lasso_lars'\r\n"]
[1508.359536, "o", "        The algorithm used:\r\n"]
[1508.371865, "o", "\r\n"]
[1508.384195, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[1508.396524, "o", "          (`linear_model.lars_path`);\r\n"]
[1508.408853, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1508.421182, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1508.433511, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1508.44584, "o", "          the estimated components are sparse;\r\n"]
[1508.45817, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1508.470499, "o", "          solution;\r\n"]
[1508.482828, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1508.495157, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[1508.507486, "o", "\r\n"]
[1508.519815, "o", "    n_nonzero_coefs : int, default=None\r\n"]
[1508.532145, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1508.544474, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[1508.556803, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[1508.569132, "o", "        `n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1508.581461, "o", "\r\n"]
[1508.593791, "o", "    alpha : float, default=None\r\n"]
[1508.60612, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1508.618449, "o", "        penalty applied to the L1 norm.\r\n"]
[1508.630778, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1508.643107, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1508.655436, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[1508.667766, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[1508.680095, "o", "        `n_nonzero_coefs`.\r\n"]
[1508.692424, "o", "        If `None`, default to 1.\r\n"]
[1508.704753, "o", "\r\n"]
[1508.717082, "o", "    copy_cov : bool, default=True\r\n"]
[1508.729411, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1508.741741, "o", "        be overwritten.\r\n"]
[1508.75407, "o", "\r\n"]
[1508.766399, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1508.778728, "o", "        Initialization value of the sparse codes. Only used if\r\n"]
[1508.791057, "o", "        `algorithm='lasso_cd'`.\r\n"]
[1508.803387, "o", "\r\n"]
[1508.815716, "o", "    max_iter : int, default=1000\r\n"]
[1508.828045, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1508.840374, "o", "        `'lasso_lars'`.\r\n"]
[1508.852703, "o", "\r\n"]
[1508.865032, "o", "    n_jobs : int, default=None\r\n"]
[1508.877362, "o", "        Number of parallel jobs to run.\r\n"]
[1508.889691, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1508.90202, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1508.914349, "o", "        for more details.\r\n"]
[1508.926678, "o", "\r\n"]
[1508.939007, "o", "    check_input : bool, default=True\r\n"]
[1508.951337, "o", "        If `False`, the input arrays X and dictionary will not be checked.\r\n"]
[1508.963666, "o", "\r\n"]
[1508.975995, "o", "    verbose : int, default=0\r\n"]
[1508.988324, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[1509.000653, "o", "\r\n"]
[1509.012983, "o", "    positive : bool, default=False\r\n"]
[1509.025312, "o", "        Whether to enforce positivity when finding the encoding.\r\n"]
[1509.037641, "o", "\r\n"]
[1509.04997, "o", "        .. versionadded:: 0.20\r\n"]
[1509.062299, "o", "\r\n"]
[1509.074628, "o", "    Returns\r\n"]
[1509.086958, "o", "    -------\r\n"]
[1509.099287, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1509.111616, "o", "        The sparse codes.\r\n"]
[1509.123945, "o", "\r\n"]
[1509.136274, "o", "    See Also\r\n"]
[1509.148603, "o", "    --------\r\n"]
[1509.160933, "o", "    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n"]
[1509.173262, "o", "        path using LARS algorithm.\r\n"]
[1509.185591, "o", "    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n"]
[1509.19792, "o", "    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n"]
[1509.210249, "o", "    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n"]
[1509.222579, "o", "        dictionary.\r\n"]
[1509.234908, "o", "    \"\"\"\r\n"]
[1509.247237, "o", "    if check_input:\r\n"]
[1509.259566, "o", "        if algorithm == \"lasso_cd\":\r\n"]
[1509.271895, "o", "            dictionary = check_array(\r\n"]
[1509.284224, "o", "                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n"]
[1509.296554, "o", "            )\r\n"]
[1509.308883, "o", "            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n"]
[1509.321212, "o", "        else:\r\n"]
[1509.333541, "o", "            dictionary = check_array(dictionary)\r\n"]
[1509.34587, "o", "            X = check_array(X)\r\n"]
[1509.3582, "o", "\r\n"]
[1509.370529, "o", "    if dictionary.shape[1] != X.shape[1]:\r\n"]
[1509.382858, "o", "        raise ValueError(\r\n"]
[1509.395187, "o", "            \"Dictionary and X have different numbers of features:\"\r\n"]
[1509.407516, "o", "            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n"]
[1509.419845, "o", "        )\r\n"]
[1509.432175, "o", "\r\n"]
[1509.444504, "o", "    _check_positive_coding(algorithm, positive)\r\n"]
[1509.456833, "o", "\r\n"]
[1509.469162, "o", "    return _sparse_encode(\r\n"]
[1509.481491, "o", "        X,\r\n"]
[1509.49382, "o", "        dictionary,\r\n"]
[1509.50615, "o", "        gram=gram,\r\n"]
[1509.518479, "o", "        cov=cov,\r\n"]
[1509.530808, "o", "        algorithm=algorithm,\r\n"]
[1509.543137, "o", "        n_nonzero_coefs=n_nonzero_coefs,\r\n"]
[1509.555466, "o", "        alpha=alpha,\r\n"]
[1509.567796, "o", "        copy_cov=copy_cov,\r\n"]
[1509.580125, "o", "        init=init,\r\n"]
[1509.592454, "o", "        max_iter=max_iter,\r\n"]
[1509.604783, "o", "        n_jobs=n_jobs,\r\n"]
[1509.617112, "o", "        verbose=verbose,\r\n"]
[1509.629441, "o", "        positive=positive,\r\n"]
[1509.641771, "o", "    )\r\n"]
[1509.6541, "o", "\r\n"]
[1509.666429, "o", "\r\n"]
[1509.678758, "o", "def _sparse_encode(\r\n"]
[1509.691087, "o", "    X,\r\n"]
[1509.703416, "o", "    dictionary,\r\n"]
[1509.715746, "o", "    *,\r\n"]
[1509.728075, "o", "    gram=None,\r\n"]
[1509.740404, "o", "    cov=None,\r\n"]
[1509.752733, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1509.765062, "o", "    n_nonzero_coefs=None,\r\n"]
[1509.777392, "o", "    alpha=None,\r\n"]
[1509.789721, "o", "    copy_cov=True,\r\n"]
[1509.80205, "o", "    init=None,\r\n"]
[1509.814379, "o", "    max_iter=1000,\r\n"]
[1509.826708, "o", "    n_jobs=None,\r\n"]
[1509.839037, "o", "    verbose=0,\r\n"]
[1509.851367, "o", "    positive=False,\r\n"]
[1509.863696, "o", "):\r\n"]
[1509.876025, "o", "    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[1509.888354, "o", "\r\n"]
[1509.900683, "o", "    n_samples, n_features = X.shape\r\n"]
[1509.913012, "o", "    n_components = dictionary.shape[0]\r\n"]
[1509.925342, "o", "\r\n"]
[1509.937671, "o", "    if algorithm in (\"lars\", \"omp\"):\r\n"]
[1510.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1510.002, "i", "sed -n '400,1200p' sklearn/decomposition/_dict_learning.py\r"]
[1510.004, "o", "sed -n '400,1200p' sklearn/decomposition/_dict_learning.py\r\n"]
[1510.016, "o", "\u001b[?2004l\r\n"]
[1510.026, "o", "        regularization = n_nonzero_coefs\r\n"]
[1510.036, "o", "        if regularization is None:\r\n"]
[1510.046, "o", "            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[1510.056, "o", "    else:\r\n"]
[1510.066, "o", "        regularization = alpha\r\n"]
[1510.076, "o", "        if regularization is None:\r\n"]
[1510.086, "o", "            regularization = 1.0\r\n"]
[1510.096, "o", "\r\n"]
[1510.106, "o", "    if gram is None and algorithm != \"threshold\":\r\n"]
[1510.116, "o", "        gram = np.dot(dictionary, dictionary.T)\r\n"]
[1510.126, "o", "\r\n"]
[1510.136, "o", "    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[1510.146, "o", "        copy_cov = False\r\n"]
[1510.156, "o", "        cov = np.dot(dictionary, X.T)\r\n"]
[1510.166, "o", "\r\n"]
[1510.176, "o", "    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[1510.186, "o", "        code = _sparse_encode_precomputed(\r\n"]
[1510.196, "o", "            X,\r\n"]
[1510.206, "o", "            dictionary,\r\n"]
[1510.216, "o", "            gram=gram,\r\n"]
[1510.226, "o", "            cov=cov,\r\n"]
[1510.236, "o", "            algorithm=algorithm,\r\n"]
[1510.246, "o", "            regularization=regularization,\r\n"]
[1510.256, "o", "            copy_cov=copy_cov,\r\n"]
[1510.266, "o", "            init=init,\r\n"]
[1510.276, "o", "            max_iter=max_iter,\r\n"]
[1510.286, "o", "            verbose=verbose,\r\n"]
[1510.296, "o", "            positive=positive,\r\n"]
[1510.306, "o", "        )\r\n"]
[1510.316, "o", "        return code\r\n"]
[1510.326, "o", "\r\n"]
[1510.336, "o", "    # Enter parallel code block\r\n"]
[1510.346, "o", "    n_samples = X.shape[0]\r\n"]
[1510.356, "o", "    n_components = dictionary.shape[0]\r\n"]
[1510.366, "o", "    code = np.empty((n_samples, n_components))\r\n"]
[1510.376, "o", "    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[1510.386, "o", "\r\n"]
[1510.396, "o", "    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[1510.406, "o", "        delayed(_sparse_encode_precomputed)(\r\n"]
[1510.416, "o", "            X[this_slice],\r\n"]
[1510.426, "o", "            dictionary,\r\n"]
[1510.436, "o", "            gram=gram,\r\n"]
[1510.446, "o", "            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[1510.456, "o", "            algorithm=algorithm,\r\n"]
[1510.466, "o", "            regularization=regularization,\r\n"]
[1510.476, "o", "            copy_cov=copy_cov,\r\n"]
[1510.486, "o", "            init=init[this_slice] if init is not None else None,\r\n"]
[1510.496, "o", "            max_iter=max_iter,\r\n"]
[1510.506, "o", "            verbose=verbose,\r\n"]
[1510.516, "o", "            positive=positive,\r\n"]
[1510.526, "o", "        )\r\n"]
[1510.536, "o", "        for this_slice in slices\r\n"]
[1510.546, "o", "    )\r\n"]
[1510.556, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[1510.566, "o", "        code[this_slice] = this_view\r\n"]
[1510.576, "o", "    return code\r\n"]
[1510.586, "o", "\r\n"]
[1510.596, "o", "\r\n"]
[1510.606, "o", "def _update_dict(\r\n"]
[1510.616, "o", "    dictionary,\r\n"]
[1510.626, "o", "    Y,\r\n"]
[1510.636, "o", "    code,\r\n"]
[1510.646, "o", "    A=None,\r\n"]
[1510.656, "o", "    B=None,\r\n"]
[1510.666, "o", "    verbose=False,\r\n"]
[1510.676, "o", "    random_state=None,\r\n"]
[1510.686, "o", "    positive=False,\r\n"]
[1510.696, "o", "):\r\n"]
[1510.706, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[1510.716, "o", "\r\n"]
[1510.726, "o", "    Parameters\r\n"]
[1510.736, "o", "    ----------\r\n"]
[1510.746, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1510.756, "o", "        Value of the dictionary at the previous iteration.\r\n"]
[1510.766, "o", "\r\n"]
[1510.776, "o", "    Y : ndarray of shape (n_samples, n_features)\r\n"]
[1510.786, "o", "        Data matrix.\r\n"]
[1510.796, "o", "\r\n"]
[1510.806, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1510.816, "o", "        Sparse coding of the data against which to optimize the dictionary.\r\n"]
[1510.826, "o", "\r\n"]
[1510.836, "o", "    A : ndarray of shape (n_components, n_components), default=None\r\n"]
[1510.846, "o", "        Together with `B`, sufficient stats of the online model to update the\r\n"]
[1510.856, "o", "        dictionary.\r\n"]
[1510.866, "o", "\r\n"]
[1510.876, "o", "    B : ndarray of shape (n_features, n_components), default=None\r\n"]
[1510.886, "o", "        Together with `A`, sufficient stats of the online model to update the\r\n"]
[1510.896, "o", "        dictionary.\r\n"]
[1510.906, "o", "\r\n"]
[1510.916, "o", "    verbose: bool, default=False\r\n"]
[1510.926, "o", "        Degree of output the procedure will print.\r\n"]
[1510.936, "o", "\r\n"]
[1510.946, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1510.956, "o", "        Used for randomly initializing the dictionary. Pass an int for\r\n"]
[1510.966, "o", "        reproducible results across multiple function calls.\r\n"]
[1510.976, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1510.986, "o", "\r\n"]
[1510.996, "o", "    positive : bool, default=False\r\n"]
[1511.006, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1511.016, "o", "\r\n"]
[1511.026, "o", "        .. versionadded:: 0.20\r\n"]
[1511.036, "o", "    \"\"\"\r\n"]
[1511.046, "o", "    n_samples, n_components = code.shape\r\n"]
[1511.056, "o", "    random_state = check_random_state(random_state)\r\n"]
[1511.066, "o", "\r\n"]
[1511.076, "o", "    if A is None:\r\n"]
[1511.086, "o", "        A = code.T @ code\r\n"]
[1511.096, "o", "    if B is None:\r\n"]
[1511.106, "o", "        B = Y.T @ code\r\n"]
[1511.116, "o", "\r\n"]
[1511.126, "o", "    n_unused = 0\r\n"]
[1511.136, "o", "\r\n"]
[1511.146, "o", "    for k in range(n_components):\r\n"]
[1511.156, "o", "        if A[k, k] > 1e-6:\r\n"]
[1511.166, "o", "            # 1e-6 is arbitrary but consistent with the spams implementation\r\n"]
[1511.176, "o", "            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n"]
[1511.186, "o", "        else:\r\n"]
[1511.196, "o", "            # kth atom is almost never used -> sample a new one from the data\r\n"]
[1511.206, "o", "            newd = Y[random_state.choice(n_samples)]\r\n"]
[1511.216, "o", "\r\n"]
[1511.226, "o", "            # add small noise to avoid making the sparse coding ill conditioned\r\n"]
[1511.236, "o", "            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n"]
[1511.246, "o", "            noise = random_state.normal(0, noise_level, size=len(newd))\r\n"]
[1511.256, "o", "\r\n"]
[1511.266, "o", "            dictionary[k] = newd + noise\r\n"]
[1511.276, "o", "            code[:, k] = 0\r\n"]
[1511.286, "o", "            n_unused += 1\r\n"]
[1511.296, "o", "\r\n"]
[1511.306, "o", "        if positive:\r\n"]
[1511.316, "o", "            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n"]
[1511.326, "o", "\r\n"]
[1511.336, "o", "        # Projection on the constraint set ||V_k|| <= 1\r\n"]
[1511.346, "o", "        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n"]
[1511.356, "o", "\r\n"]
[1511.366, "o", "    if verbose and n_unused > 0:\r\n"]
[1511.376, "o", "        print(f\"{n_unused} unused atoms resampled.\")\r\n"]
[1511.386, "o", "\r\n"]
[1511.396, "o", "\r\n"]
[1511.406, "o", "def _dict_learning(\r\n"]
[1511.416, "o", "    X,\r\n"]
[1511.426, "o", "    n_components,\r\n"]
[1511.436, "o", "    *,\r\n"]
[1511.446, "o", "    alpha,\r\n"]
[1511.456, "o", "    max_iter,\r\n"]
[1511.466, "o", "    tol,\r\n"]
[1511.476, "o", "    method,\r\n"]
[1511.486, "o", "    n_jobs,\r\n"]
[1511.496, "o", "    dict_init,\r\n"]
[1511.506, "o", "    code_init,\r\n"]
[1511.516, "o", "    callback,\r\n"]
[1511.526, "o", "    verbose,\r\n"]
[1511.536, "o", "    random_state,\r\n"]
[1511.546, "o", "    return_n_iter,\r\n"]
[1511.556, "o", "    positive_dict,\r\n"]
[1511.566, "o", "    positive_code,\r\n"]
[1511.576, "o", "    method_max_iter,\r\n"]
[1511.586, "o", "):\r\n"]
[1511.596, "o", "    \"\"\"Main dictionary learning algorithm\"\"\"\r\n"]
[1511.606, "o", "    t0 = time.time()\r\n"]
[1511.616, "o", "    # Init the code and the dictionary with SVD of Y\r\n"]
[1511.626, "o", "    if code_init is not None and dict_init is not None:\r\n"]
[1511.636, "o", "        code = np.array(code_init, order=\"F\")\r\n"]
[1511.646, "o", "        # Don't copy V, it will happen below\r\n"]
[1511.656, "o", "        dictionary = dict_init\r\n"]
[1511.666, "o", "    else:\r\n"]
[1511.676, "o", "        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n"]
[1511.686, "o", "        # flip the initial code's sign to enforce deterministic output\r\n"]
[1511.696, "o", "        code, dictionary = svd_flip(code, dictionary)\r\n"]
[1511.706, "o", "        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[1511.716, "o", "    r = len(dictionary)\r\n"]
[1511.726, "o", "    if n_components <= r:  # True even if n_components=None\r\n"]
[1511.736, "o", "        code = code[:, :n_components]\r\n"]
[1511.746, "o", "        dictionary = dictionary[:n_components, :]\r\n"]
[1511.756, "o", "    else:\r\n"]
[1511.766, "o", "        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n"]
[1511.776, "o", "        dictionary = np.r_[\r\n"]
[1511.786, "o", "            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n"]
[1511.796, "o", "        ]\r\n"]
[1511.806, "o", "\r\n"]
[1511.816, "o", "    # Fortran-order dict better suited for the sparse coding which is the\r\n"]
[1511.826, "o", "    # bottleneck of this algorithm.\r\n"]
[1511.836, "o", "    dictionary = np.asfortranarray(dictionary)\r\n"]
[1511.846, "o", "\r\n"]
[1511.856, "o", "    errors = []\r\n"]
[1511.866, "o", "    current_cost = np.nan\r\n"]
[1511.876, "o", "\r\n"]
[1511.886, "o", "    if verbose == 1:\r\n"]
[1511.896, "o", "        print(\"[dict_learning]\", end=\" \")\r\n"]
[1511.906, "o", "\r\n"]
[1511.916, "o", "    # If max_iter is 0, number of iterations returned should be zero\r\n"]
[1511.926, "o", "    ii = -1\r\n"]
[1511.936, "o", "\r\n"]
[1511.946, "o", "    for ii in range(max_iter):\r\n"]
[1511.956, "o", "        dt = time.time() - t0\r\n"]
[1511.966, "o", "        if verbose == 1:\r\n"]
[1511.976, "o", "            sys.stdout.write(\".\")\r\n"]
[1511.986, "o", "            sys.stdout.flush()\r\n"]
[1511.996, "o", "        elif verbose:\r\n"]
[1512.006, "o", "            print(\r\n"]
[1512.016, "o", "                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n"]
[1512.026, "o", "                % (ii, dt, dt / 60, current_cost)\r\n"]
[1512.036, "o", "            )\r\n"]
[1512.046, "o", "\r\n"]
[1512.056, "o", "        # Update code\r\n"]
[1512.066, "o", "        code = sparse_encode(\r\n"]
[1512.076, "o", "            X,\r\n"]
[1512.086, "o", "            dictionary,\r\n"]
[1512.096, "o", "            algorithm=method,\r\n"]
[1512.106, "o", "            alpha=alpha,\r\n"]
[1512.116, "o", "            init=code,\r\n"]
[1512.126, "o", "            n_jobs=n_jobs,\r\n"]
[1512.136, "o", "            positive=positive_code,\r\n"]
[1512.146, "o", "            max_iter=method_max_iter,\r\n"]
[1512.156, "o", "            verbose=verbose,\r\n"]
[1512.166, "o", "        )\r\n"]
[1512.176, "o", "\r\n"]
[1512.186, "o", "        # Update dictionary in place\r\n"]
[1512.196, "o", "        _update_dict(\r\n"]
[1512.206, "o", "            dictionary,\r\n"]
[1512.216, "o", "            X,\r\n"]
[1512.226, "o", "            code,\r\n"]
[1512.236, "o", "            verbose=verbose,\r\n"]
[1512.246, "o", "            random_state=random_state,\r\n"]
[1512.256, "o", "            positive=positive_dict,\r\n"]
[1512.266, "o", "        )\r\n"]
[1512.276, "o", "\r\n"]
[1512.286, "o", "        # Cost function\r\n"]
[1512.296, "o", "        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n"]
[1512.306, "o", "            np.abs(code)\r\n"]
[1512.316, "o", "        )\r\n"]
[1512.326, "o", "        errors.append(current_cost)\r\n"]
[1512.336, "o", "\r\n"]
[1512.346, "o", "        if ii > 0:\r\n"]
[1512.356, "o", "            dE = errors[-2] - errors[-1]\r\n"]
[1512.366, "o", "            # assert(dE >= -tol * errors[-1])\r\n"]
[1512.376, "o", "            if dE < tol * errors[-1]:\r\n"]
[1512.386, "o", "                if verbose == 1:\r\n"]
[1512.396, "o", "                    # A line return\r\n"]
[1512.406, "o", "                    print(\"\")\r\n"]
[1512.416, "o", "                elif verbose:\r\n"]
[1512.426, "o", "                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n"]
[1512.436, "o", "                break\r\n"]
[1512.446, "o", "        if ii % 5 == 0 and callback is not None:\r\n"]
[1512.456, "o", "            callback(locals())\r\n"]
[1512.466, "o", "\r\n"]
[1512.476, "o", "    if return_n_iter:\r\n"]
[1512.486, "o", "        return code, dictionary, errors, ii + 1\r\n"]
[1512.496, "o", "    else:\r\n"]
[1512.506, "o", "        return code, dictionary, errors\r\n"]
[1512.516, "o", "\r\n"]
[1512.526, "o", "\r\n"]
[1512.536, "o", "def _check_warn_deprecated(param, name, default, additional_message=None):\r\n"]
[1512.546, "o", "    if param != \"deprecated\":\r\n"]
[1512.556, "o", "        msg = (\r\n"]
[1512.566, "o", "            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n"]
[1512.576, "o", "        )\r\n"]
[1512.586, "o", "        if additional_message:\r\n"]
[1512.596, "o", "            msg += f\" {additional_message}\"\r\n"]
[1512.606, "o", "        warnings.warn(msg, FutureWarning)\r\n"]
[1512.616, "o", "        return param\r\n"]
[1512.626, "o", "    else:\r\n"]
[1512.636, "o", "        return default\r\n"]
[1512.646, "o", "\r\n"]
[1512.656, "o", "\r\n"]
[1512.666, "o", "def dict_learning_online(\r\n"]
[1512.676, "o", "    X,\r\n"]
[1512.686, "o", "    n_components=2,\r\n"]
[1512.696, "o", "    *,\r\n"]
[1512.706, "o", "    alpha=1,\r\n"]
[1512.716, "o", "    n_iter=\"deprecated\",\r\n"]
[1512.726, "o", "    max_iter=None,\r\n"]
[1512.736, "o", "    return_code=True,\r\n"]
[1512.746, "o", "    dict_init=None,\r\n"]
[1512.756, "o", "    callback=None,\r\n"]
[1512.766, "o", "    batch_size=256,\r\n"]
[1512.776, "o", "    verbose=False,\r\n"]
[1512.786, "o", "    shuffle=True,\r\n"]
[1512.796, "o", "    n_jobs=None,\r\n"]
[1512.806, "o", "    method=\"lars\",\r\n"]
[1512.816, "o", "    iter_offset=\"deprecated\",\r\n"]
[1512.826, "o", "    random_state=None,\r\n"]
[1512.836, "o", "    return_inner_stats=\"deprecated\",\r\n"]
[1512.846, "o", "    inner_stats=\"deprecated\",\r\n"]
[1512.856, "o", "    return_n_iter=\"deprecated\",\r\n"]
[1512.866, "o", "    positive_dict=False,\r\n"]
[1512.876, "o", "    positive_code=False,\r\n"]
[1512.886, "o", "    method_max_iter=1000,\r\n"]
[1512.896, "o", "    tol=1e-3,\r\n"]
[1512.906, "o", "    max_no_improvement=10,\r\n"]
[1512.916, "o", "):\r\n"]
[1512.926, "o", "    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n"]
[1512.936, "o", "\r\n"]
[1512.946, "o", "    Finds the best dictionary and the corresponding sparse code for\r\n"]
[1512.956, "o", "    approximating the data matrix X by solving::\r\n"]
[1512.966, "o", "\r\n"]
[1512.976, "o", "        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[1512.986, "o", "                     (U,V)\r\n"]
[1512.996, "o", "                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n"]
[1513.006, "o", "\r\n"]
[1513.016, "o", "    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n"]
[1513.026, "o", "    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n"]
[1513.036, "o", "    which is the sum of the absolute values of all the entries in the matrix.\r\n"]
[1513.046, "o", "    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n"]
[1513.056, "o", "    the input data.\r\n"]
[1513.066, "o", "\r\n"]
[1513.076, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[1513.086, "o", "\r\n"]
[1513.096, "o", "    Parameters\r\n"]
[1513.106, "o", "    ----------\r\n"]
[1513.116, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[1513.126, "o", "        Data matrix.\r\n"]
[1513.136, "o", "\r\n"]
[1513.146, "o", "    n_components : int or None, default=2\r\n"]
[1513.156, "o", "        Number of dictionary atoms to extract. If None, then ``n_components``\r\n"]
[1513.166, "o", "        is set to ``n_features``.\r\n"]
[1513.176, "o", "\r\n"]
[1513.186, "o", "    alpha : float, default=1\r\n"]
[1513.196, "o", "        Sparsity controlling parameter.\r\n"]
[1513.206, "o", "\r\n"]
[1513.216, "o", "    n_iter : int, default=100\r\n"]
[1513.226, "o", "        Number of mini-batch iterations to perform.\r\n"]
[1513.236, "o", "\r\n"]
[1513.246, "o", "        .. deprecated:: 1.1\r\n"]
[1513.256, "o", "           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[1513.266, "o", "           `max_iter` instead.\r\n"]
[1513.276, "o", "\r\n"]
[1513.286, "o", "    max_iter : int, default=None\r\n"]
[1513.296, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[1513.306, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[1513.316, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[1513.326, "o", "\r\n"]
[1513.336, "o", "        .. versionadded:: 1.1\r\n"]
[1513.346, "o", "\r\n"]
[1513.356, "o", "    return_code : bool, default=True\r\n"]
[1513.366, "o", "        Whether to also return the code U or just the dictionary `V`.\r\n"]
[1513.376, "o", "\r\n"]
[1513.386, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1513.396, "o", "        Initial values for the dictionary for warm restart scenarios.\r\n"]
[1513.406, "o", "        If `None`, the initial values for the dictionary are created\r\n"]
[1513.416, "o", "        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n"]
[1513.426, "o", "\r\n"]
[1513.436, "o", "    callback : callable, default=None\r\n"]
[1513.446, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[1513.456, "o", "\r\n"]
[1513.466, "o", "    batch_size : int, default=256\r\n"]
[1513.476, "o", "        The number of samples to take in each batch.\r\n"]
[1513.486, "o", "\r\n"]
[1513.496, "o", "        .. versionchanged:: 1.3\r\n"]
[1513.506, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[1513.516, "o", "\r\n"]
[1513.526, "o", "    verbose : bool, default=False\r\n"]
[1513.536, "o", "        To control the verbosity of the procedure.\r\n"]
[1513.546, "o", "\r\n"]
[1513.556, "o", "    shuffle : bool, default=True\r\n"]
[1513.566, "o", "        Whether to shuffle the data before splitting it in batches.\r\n"]
[1513.576, "o", "\r\n"]
[1513.586, "o", "    n_jobs : int, default=None\r\n"]
[1513.596, "o", "        Number of parallel jobs to run.\r\n"]
[1513.606, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1513.616, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1513.626, "o", "        for more details.\r\n"]
[1513.636, "o", "\r\n"]
[1513.646, "o", "    method : {'lars', 'cd'}, default='lars'\r\n"]
[1513.656, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[1513.666, "o", "          problem (`linear_model.lars_path`);\r\n"]
[1513.676, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[1513.686, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[1513.696, "o", "          the estimated components are sparse.\r\n"]
[1513.706, "o", "\r\n"]
[1513.716, "o", "    iter_offset : int, default=0\r\n"]
[1513.726, "o", "        Number of previous iterations completed on the dictionary used for\r\n"]
[1513.736, "o", "        initialization.\r\n"]
[1513.746, "o", "\r\n"]
[1513.756, "o", "        .. deprecated:: 1.1\r\n"]
[1513.766, "o", "           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n"]
[1513.776, "o", "\r\n"]
[1513.786, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1513.796, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[1513.806, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[1513.816, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[1513.826, "o", "        results across multiple function calls.\r\n"]
[1513.836, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1513.846, "o", "\r\n"]
[1513.856, "o", "    return_inner_stats : bool, default=False\r\n"]
[1513.866, "o", "        Return the inner statistics A (dictionary covariance) and B\r\n"]
[1513.876, "o", "        (data approximation). Useful to restart the algorithm in an\r\n"]
[1513.886, "o", "        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n"]
[1513.896, "o", "        ignored.\r\n"]
[1513.906, "o", "\r\n"]
[1513.916, "o", "        .. deprecated:: 1.1\r\n"]
[1513.926, "o", "           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[1513.936, "o", "\r\n"]
[1513.946, "o", "    inner_stats : tuple of (A, B) ndarrays, default=None\r\n"]
[1513.956, "o", "        Inner sufficient statistics that are kept by the algorithm.\r\n"]
[1513.966, "o", "        Passing them at initialization is useful in online settings, to\r\n"]
[1513.976, "o", "        avoid losing the history of the evolution.\r\n"]
[1513.986, "o", "        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n"]
[1513.996, "o", "        `B` `(n_features, n_components)` is the data approximation matrix.\r\n"]
[1514.006, "o", "\r\n"]
[1514.016, "o", "        .. deprecated:: 1.1\r\n"]
[1514.026, "o", "           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[1514.036, "o", "\r\n"]
[1514.046, "o", "    return_n_iter : bool, default=False\r\n"]
[1514.056, "o", "        Whether or not to return the number of iterations.\r\n"]
[1514.066, "o", "\r\n"]
[1514.076, "o", "        .. deprecated:: 1.1\r\n"]
[1514.086, "o", "           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n"]
[1514.096, "o", "\r\n"]
[1514.106, "o", "    positive_dict : bool, default=False\r\n"]
[1514.116, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1514.126, "o", "\r\n"]
[1514.136, "o", "        .. versionadded:: 0.20\r\n"]
[1514.146, "o", "\r\n"]
[1514.156, "o", "    positive_code : bool, default=False\r\n"]
[1514.166, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1514.176, "o", "\r\n"]
[1514.186, "o", "        .. versionadded:: 0.20\r\n"]
[1514.196, "o", "\r\n"]
[1514.206, "o", "    method_max_iter : int, default=1000\r\n"]
[1514.216, "o", "        Maximum number of iterations to perform when solving the lasso problem.\r\n"]
[1514.226, "o", "\r\n"]
[1514.236, "o", "        .. versionadded:: 0.22\r\n"]
[1514.246, "o", "\r\n"]
[1514.256, "o", "    tol : float, default=1e-3\r\n"]
[1514.266, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[1514.276, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[1514.286, "o", "\r\n"]
[1514.296, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[1514.306, "o", "        `tol` to 0.0.\r\n"]
[1514.316, "o", "\r\n"]
[1514.326, "o", "        .. versionadded:: 1.1\r\n"]
[1514.336, "o", "\r\n"]
[1514.346, "o", "    max_no_improvement : int, default=10\r\n"]
[1514.356, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[1514.366, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[1514.376, "o", "        `max_iter` is not None.\r\n"]
[1514.386, "o", "\r\n"]
[1514.396, "o", "        To disable convergence detection based on cost function, set\r\n"]
[1514.406, "o", "        `max_no_improvement` to None.\r\n"]
[1514.416, "o", "\r\n"]
[1514.426, "o", "        .. versionadded:: 1.1\r\n"]
[1514.436, "o", "\r\n"]
[1514.446, "o", "    Returns\r\n"]
[1514.456, "o", "    -------\r\n"]
[1514.466, "o", "    code : ndarray of shape (n_samples, n_components),\r\n"]
[1514.476, "o", "        The sparse code (only returned if `return_code=True`).\r\n"]
[1514.486, "o", "\r\n"]
[1514.496, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[1514.506, "o", "        The solutions to the dictionary learning problem.\r\n"]
[1514.516, "o", "\r\n"]
[1514.526, "o", "    n_iter : int\r\n"]
[1514.536, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[1514.546, "o", "        set to `True`.\r\n"]
[1514.556, "o", "\r\n"]
[1514.566, "o", "    See Also\r\n"]
[1514.576, "o", "    --------\r\n"]
[1514.586, "o", "    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n"]
[1514.596, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1514.606, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n"]
[1514.616, "o", "        learning algorithm.\r\n"]
[1514.626, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1514.636, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1514.646, "o", "    \"\"\"\r\n"]
[1514.656, "o", "    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n"]
[1514.666, "o", "    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n"]
[1514.676, "o", "        raise ValueError(\r\n"]
[1514.686, "o", "            \"The following arguments are incompatible with 'max_iter': \"\r\n"]
[1514.696, "o", "            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n"]
[1514.706, "o", "        )\r\n"]
[1514.716, "o", "\r\n"]
[1514.726, "o", "    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n"]
[1514.736, "o", "    return_inner_stats = _check_warn_deprecated(\r\n"]
[1514.746, "o", "        return_inner_stats,\r\n"]
[1514.756, "o", "        \"return_inner_stats\",\r\n"]
[1514.766, "o", "        default=False,\r\n"]
[1514.776, "o", "        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n"]
[1514.786, "o", "    )\r\n"]
[1514.796, "o", "    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n"]
[1514.806, "o", "    return_n_iter = _check_warn_deprecated(\r\n"]
[1514.816, "o", "        return_n_iter,\r\n"]
[1514.826, "o", "        \"return_n_iter\",\r\n"]
[1514.836, "o", "        default=False,\r\n"]
[1514.846, "o", "        additional_message=(\r\n"]
[1514.856, "o", "            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n"]
[1514.866, "o", "            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n"]
[1514.876, "o", "        ),\r\n"]
[1514.886, "o", "    )\r\n"]
[1514.896, "o", "\r\n"]
[1514.906, "o", "    if max_iter is not None:\r\n"]
[1514.916, "o", "        transform_algorithm = \"lasso_\" + method\r\n"]
[1514.926, "o", "\r\n"]
[1514.936, "o", "        est = MiniBatchDictionaryLearning(\r\n"]
[1515.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1515.002, "i", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r"]
[1515.004, "o", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r\n"]
[1515.016, "o", "\u001b[?2004l\r\n"]
[1515.026, "o", "    n_iter : int\r\n"]
[1515.036, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[1515.046, "o", "        set to True.\r\n"]
[1515.056, "o", "\r\n"]
[1515.066, "o", "    See Also\r\n"]
[1515.076, "o", "    --------\r\n"]
[1515.086, "o", "    dict_learning_online : Solve a dictionary learning matrix factorization\r\n"]
[1515.096, "o", "        problem online.\r\n"]
[1515.106, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1515.116, "o", "    MiniBatchDictionaryLearning : A faster, less accurate version\r\n"]
[1515.126, "o", "        of the dictionary learning algorithm.\r\n"]
[1515.136, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1515.146, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1515.156, "o", "    \"\"\"\r\n"]
[1515.166, "o", "    estimator = DictionaryLearning(\r\n"]
[1515.176, "o", "        n_components=n_components,\r\n"]
[1515.186, "o", "        alpha=alpha,\r\n"]
[1515.196, "o", "        max_iter=max_iter,\r\n"]
[1515.206, "o", "        tol=tol,\r\n"]
[1515.216, "o", "        fit_algorithm=method,\r\n"]
[1515.226, "o", "        n_jobs=n_jobs,\r\n"]
[1515.236, "o", "        dict_init=dict_init,\r\n"]
[1515.246, "o", "        callback=callback,\r\n"]
[1515.256, "o", "        code_init=code_init,\r\n"]
[1515.266, "o", "        verbose=verbose,\r\n"]
[1515.276, "o", "        random_state=random_state,\r\n"]
[1515.286, "o", "        positive_code=positive_code,\r\n"]
[1515.296, "o", "        positive_dict=positive_dict,\r\n"]
[1515.306, "o", "        transform_max_iter=method_max_iter,\r\n"]
[1515.316, "o", "    )\r\n"]
[1515.326, "o", "    code = estimator.fit_transform(X)\r\n"]
[1515.336, "o", "    if return_n_iter:\r\n"]
[1515.346, "o", "        return (\r\n"]
[1515.356, "o", "            code,\r\n"]
[1515.366, "o", "            estimator.components_,\r\n"]
[1515.376, "o", "            estimator.error_,\r\n"]
[1515.386, "o", "            estimator.n_iter_,\r\n"]
[1515.396, "o", "        )\r\n"]
[1515.406, "o", "    return code, estimator.components_, estimator.error_\r\n"]
[1515.416, "o", "\r\n"]
[1515.426, "o", "\r\n"]
[1515.436, "o", "class _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n"]
[1515.446, "o", "    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n"]
[1515.456, "o", "\r\n"]
[1515.466, "o", "    def __init__(\r\n"]
[1515.476, "o", "        self,\r\n"]
[1515.486, "o", "        transform_algorithm,\r\n"]
[1515.496, "o", "        transform_n_nonzero_coefs,\r\n"]
[1515.506, "o", "        transform_alpha,\r\n"]
[1515.516, "o", "        split_sign,\r\n"]
[1515.526, "o", "        n_jobs,\r\n"]
[1515.536, "o", "        positive_code,\r\n"]
[1515.546, "o", "        transform_max_iter,\r\n"]
[1515.556, "o", "    ):\r\n"]
[1515.566, "o", "        self.transform_algorithm = transform_algorithm\r\n"]
[1515.576, "o", "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n"]
[1515.586, "o", "        self.transform_alpha = transform_alpha\r\n"]
[1515.596, "o", "        self.transform_max_iter = transform_max_iter\r\n"]
[1515.606, "o", "        self.split_sign = split_sign\r\n"]
[1515.616, "o", "        self.n_jobs = n_jobs\r\n"]
[1515.626, "o", "        self.positive_code = positive_code\r\n"]
[1515.636, "o", "\r\n"]
[1515.646, "o", "    def _transform(self, X, dictionary):\r\n"]
[1515.656, "o", "        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n"]
[1515.666, "o", "        SparseCoder.\"\"\"\r\n"]
[1515.676, "o", "        X = self._validate_data(X, reset=False)\r\n"]
[1515.686, "o", "\r\n"]
[1515.696, "o", "        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n"]
[1515.706, "o", "            transform_alpha = self.alpha\r\n"]
[1515.716, "o", "        else:\r\n"]
[1515.726, "o", "            transform_alpha = self.transform_alpha\r\n"]
[1515.736, "o", "\r\n"]
[1515.746, "o", "        code = sparse_encode(\r\n"]
[1515.756, "o", "            X,\r\n"]
[1515.766, "o", "            dictionary,\r\n"]
[1515.776, "o", "            algorithm=self.transform_algorithm,\r\n"]
[1515.786, "o", "            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n"]
[1515.796, "o", "            alpha=transform_alpha,\r\n"]
[1515.806, "o", "            max_iter=self.transform_max_iter,\r\n"]
[1515.816, "o", "            n_jobs=self.n_jobs,\r\n"]
[1515.826, "o", "            positive=self.positive_code,\r\n"]
[1515.836, "o", "        )\r\n"]
[1515.846, "o", "\r\n"]
[1515.856, "o", "        if self.split_sign:\r\n"]
[1515.866, "o", "            # feature vector is split into a positive and negative side\r\n"]
[1515.876, "o", "            n_samples, n_features = code.shape\r\n"]
[1515.886, "o", "            split_code = np.empty((n_samples, 2 * n_features))\r\n"]
[1515.896, "o", "            split_code[:, :n_features] = np.maximum(code, 0)\r\n"]
[1515.906, "o", "            split_code[:, n_features:] = -np.minimum(code, 0)\r\n"]
[1515.916, "o", "            code = split_code\r\n"]
[1515.926, "o", "\r\n"]
[1515.936, "o", "        return code\r\n"]
[1515.946, "o", "\r\n"]
[1515.956, "o", "    def transform(self, X):\r\n"]
[1515.966, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[1515.976, "o", "\r\n"]
[1515.986, "o", "        Coding method is determined by the object parameter\r\n"]
[1515.996, "o", "        `transform_algorithm`.\r\n"]
[1516.006, "o", "\r\n"]
[1516.016, "o", "        Parameters\r\n"]
[1516.026, "o", "        ----------\r\n"]
[1516.036, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[1516.046, "o", "            Test data to be transformed, must have the same number of\r\n"]
[1516.056, "o", "            features as the data used to train the model.\r\n"]
[1516.066, "o", "\r\n"]
[1516.076, "o", "        Returns\r\n"]
[1516.086, "o", "        -------\r\n"]
[1516.096, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[1516.106, "o", "            Transformed data.\r\n"]
[1516.116, "o", "        \"\"\"\r\n"]
[1516.126, "o", "        check_is_fitted(self)\r\n"]
[1516.136, "o", "        return self._transform(X, self.components_)\r\n"]
[1516.146, "o", "\r\n"]
[1516.156, "o", "\r\n"]
[1516.166, "o", "class SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n"]
[1516.176, "o", "    \"\"\"Sparse coding.\r\n"]
[1516.186, "o", "\r\n"]
[1516.196, "o", "    Finds a sparse representation of data against a fixed, precomputed\r\n"]
[1516.206, "o", "    dictionary.\r\n"]
[1516.216, "o", "\r\n"]
[1516.226, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[1516.236, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[1516.246, "o", "\r\n"]
[1516.256, "o", "        X ~= code * dictionary\r\n"]
[1516.266, "o", "\r\n"]
[1516.276, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[1516.286, "o", "\r\n"]
[1516.296, "o", "    Parameters\r\n"]
[1516.306, "o", "    ----------\r\n"]
[1516.316, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1516.326, "o", "        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n"]
[1516.336, "o", "        normalized to unit norm.\r\n"]
[1516.346, "o", "\r\n"]
[1516.356, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[1516.366, "o", "            'threshold'}, default='omp'\r\n"]
[1516.376, "o", "        Algorithm used to transform the data:\r\n"]
[1516.386, "o", "\r\n"]
[1516.396, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[1516.406, "o", "          (`linear_model.lars_path`);\r\n"]
[1516.416, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1516.426, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1516.436, "o", "          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n"]
[1516.446, "o", "          the estimated components are sparse;\r\n"]
[1516.456, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1516.466, "o", "          solution;\r\n"]
[1516.476, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[1516.486, "o", "          the projection ``dictionary * X'``.\r\n"]
[1516.496, "o", "\r\n"]
[1516.506, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[1516.516, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1516.526, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[1516.536, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[1516.546, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1516.556, "o", "\r\n"]
[1516.566, "o", "    transform_alpha : float, default=None\r\n"]
[1516.576, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1516.586, "o", "        penalty applied to the L1 norm.\r\n"]
[1516.596, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1516.606, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1516.616, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[1516.626, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[1516.636, "o", "        `n_nonzero_coefs`.\r\n"]
[1516.646, "o", "        If `None`, default to 1.\r\n"]
[1516.656, "o", "\r\n"]
[1516.666, "o", "    split_sign : bool, default=False\r\n"]
[1516.676, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[1516.686, "o", "        its negative part and its positive part. This can improve the\r\n"]
[1516.696, "o", "        performance of downstream classifiers.\r\n"]
[1516.706, "o", "\r\n"]
[1516.716, "o", "    n_jobs : int, default=None\r\n"]
[1516.726, "o", "        Number of parallel jobs to run.\r\n"]
[1516.736, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1516.746, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1516.756, "o", "        for more details.\r\n"]
[1516.766, "o", "\r\n"]
[1516.776, "o", "    positive_code : bool, default=False\r\n"]
[1516.786, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1516.796, "o", "\r\n"]
[1516.806, "o", "        .. versionadded:: 0.20\r\n"]
[1516.816, "o", "\r\n"]
[1516.826, "o", "    transform_max_iter : int, default=1000\r\n"]
[1516.836, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1516.846, "o", "        `lasso_lars`.\r\n"]
[1516.856, "o", "\r\n"]
[1516.866, "o", "        .. versionadded:: 0.22\r\n"]
[1516.876, "o", "\r\n"]
[1516.886, "o", "    Attributes\r\n"]
[1516.896, "o", "    ----------\r\n"]
[1516.906, "o", "    n_components_ : int\r\n"]
[1516.916, "o", "        Number of atoms.\r\n"]
[1516.926, "o", "\r\n"]
[1516.936, "o", "    n_features_in_ : int\r\n"]
[1516.946, "o", "        Number of features seen during :term:`fit`.\r\n"]
[1516.956, "o", "\r\n"]
[1516.966, "o", "        .. versionadded:: 0.24\r\n"]
[1516.976, "o", "\r\n"]
[1516.986, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[1516.996, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[1517.006, "o", "        has feature names that are all strings.\r\n"]
[1517.016, "o", "\r\n"]
[1517.026, "o", "        .. versionadded:: 1.0\r\n"]
[1517.036, "o", "\r\n"]
[1517.046, "o", "    See Also\r\n"]
[1517.056, "o", "    --------\r\n"]
[1517.066, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1517.076, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n"]
[1517.086, "o", "        dictionary learning algorithm.\r\n"]
[1517.096, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1517.106, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1517.116, "o", "    sparse_encode : Sparse coding where each row of the result is the solution\r\n"]
[1517.126, "o", "        to a sparse coding problem.\r\n"]
[1517.136, "o", "\r\n"]
[1517.146, "o", "    Examples\r\n"]
[1517.156, "o", "    --------\r\n"]
[1517.166, "o", "    >>> import numpy as np\r\n"]
[1517.176, "o", "    >>> from sklearn.decomposition import SparseCoder\r\n"]
[1517.186, "o", "    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n"]
[1517.196, "o", "    >>> dictionary = np.array(\r\n"]
[1517.206, "o", "    ...     [[0, 1, 0],\r\n"]
[1517.216, "o", "    ...      [-1, -1, 2],\r\n"]
[1517.226, "o", "    ...      [1, 1, 1],\r\n"]
[1517.236, "o", "    ...      [0, 1, 1],\r\n"]
[1517.246, "o", "    ...      [0, 2, 1]],\r\n"]
[1517.256, "o", "    ...    dtype=np.float64\r\n"]
[1517.266, "o", "    ... )\r\n"]
[1517.276, "o", "    >>> coder = SparseCoder(\r\n"]
[1517.286, "o", "    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n"]
[1517.296, "o", "    ...     transform_alpha=1e-10,\r\n"]
[1517.306, "o", "    ... )\r\n"]
[1517.316, "o", "    >>> coder.transform(X)\r\n"]
[1517.326, "o", "    array([[ 0.,  0., -1.,  0.,  0.],\r\n"]
[1517.336, "o", "           [ 0.,  1.,  1.,  0.,  0.]])\r\n"]
[1517.346, "o", "    \"\"\"\r\n"]
[1517.356, "o", "\r\n"]
[1517.366, "o", "    _required_parameters = [\"dictionary\"]\r\n"]
[1517.376, "o", "\r\n"]
[1517.386, "o", "    def __init__(\r\n"]
[1517.396, "o", "        self,\r\n"]
[1517.406, "o", "        dictionary,\r\n"]
[1517.416, "o", "        *,\r\n"]
[1517.426, "o", "        transform_algorithm=\"omp\",\r\n"]
[1517.436, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[1517.446, "o", "        transform_alpha=None,\r\n"]
[1517.456, "o", "        split_sign=False,\r\n"]
[1517.466, "o", "        n_jobs=None,\r\n"]
[1517.476, "o", "        positive_code=False,\r\n"]
[1517.486, "o", "        transform_max_iter=1000,\r\n"]
[1517.496, "o", "    ):\r\n"]
[1517.506, "o", "        super().__init__(\r\n"]
[1517.516, "o", "            transform_algorithm,\r\n"]
[1517.526, "o", "            transform_n_nonzero_coefs,\r\n"]
[1517.536, "o", "            transform_alpha,\r\n"]
[1517.546, "o", "            split_sign,\r\n"]
[1517.556, "o", "            n_jobs,\r\n"]
[1517.566, "o", "            positive_code,\r\n"]
[1517.576, "o", "            transform_max_iter,\r\n"]
[1517.586, "o", "        )\r\n"]
[1517.596, "o", "        self.dictionary = dictionary\r\n"]
[1517.606, "o", "\r\n"]
[1517.616, "o", "    def fit(self, X, y=None):\r\n"]
[1517.626, "o", "        \"\"\"Do nothing and return the estimator unchanged.\r\n"]
[1517.636, "o", "\r\n"]
[1517.646, "o", "        This method is just there to implement the usual API and hence\r\n"]
[1517.656, "o", "        work in pipelines.\r\n"]
[1517.666, "o", "\r\n"]
[1517.676, "o", "        Parameters\r\n"]
[1517.686, "o", "        ----------\r\n"]
[1517.696, "o", "        X : Ignored\r\n"]
[1517.706, "o", "            Not used, present for API consistency by convention.\r\n"]
[1517.716, "o", "\r\n"]
[1517.726, "o", "        y : Ignored\r\n"]
[1517.736, "o", "            Not used, present for API consistency by convention.\r\n"]
[1517.746, "o", "\r\n"]
[1517.756, "o", "        Returns\r\n"]
[1517.766, "o", "        -------\r\n"]
[1517.776, "o", "        self : object\r\n"]
[1517.786, "o", "            Returns the instance itself.\r\n"]
[1517.796, "o", "        \"\"\"\r\n"]
[1517.806, "o", "        return self\r\n"]
[1517.816, "o", "\r\n"]
[1517.826, "o", "    def transform(self, X, y=None):\r\n"]
[1517.836, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[1517.846, "o", "\r\n"]
[1517.856, "o", "        Coding method is determined by the object parameter\r\n"]
[1517.866, "o", "        `transform_algorithm`.\r\n"]
[1517.876, "o", "\r\n"]
[1517.886, "o", "        Parameters\r\n"]
[1517.896, "o", "        ----------\r\n"]
[1517.906, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[1517.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1517.926, "o", "            and `n_features` is the number of features.\r\n"]
[1517.936, "o", "\r\n"]
[1517.946, "o", "        y : Ignored\r\n"]
[1517.956, "o", "            Not used, present for API consistency by convention.\r\n"]
[1517.966, "o", "\r\n"]
[1517.976, "o", "        Returns\r\n"]
[1517.986, "o", "        -------\r\n"]
[1517.996, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[1518.006, "o", "            Transformed data.\r\n"]
[1518.016, "o", "        \"\"\"\r\n"]
[1518.026, "o", "        return super()._transform(X, self.dictionary)\r\n"]
[1518.036, "o", "\r\n"]
[1518.046, "o", "    def _more_tags(self):\r\n"]
[1518.056, "o", "        return {\r\n"]
[1518.066, "o", "            \"requires_fit\": False,\r\n"]
[1518.076, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[1518.086, "o", "        }\r\n"]
[1518.096, "o", "\r\n"]
[1518.106, "o", "    @property\r\n"]
[1518.116, "o", "    def n_components_(self):\r\n"]
[1518.126, "o", "        \"\"\"Number of atoms.\"\"\"\r\n"]
[1518.136, "o", "        return self.dictionary.shape[0]\r\n"]
[1518.146, "o", "\r\n"]
[1518.156, "o", "    @property\r\n"]
[1518.166, "o", "    def n_features_in_(self):\r\n"]
[1518.176, "o", "        \"\"\"Number of features seen during `fit`.\"\"\"\r\n"]
[1518.186, "o", "        return self.dictionary.shape[1]\r\n"]
[1518.196, "o", "\r\n"]
[1518.206, "o", "    @property\r\n"]
[1518.216, "o", "    def _n_features_out(self):\r\n"]
[1518.226, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[1518.236, "o", "        return self.n_components_\r\n"]
[1518.246, "o", "\r\n"]
[1518.256, "o", "\r\n"]
[1518.266, "o", "class DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[1518.276, "o", "    \"\"\"Dictionary learning.\r\n"]
[1518.286, "o", "\r\n"]
[1518.296, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[1518.306, "o", "    encoding the fitted data.\r\n"]
[1518.316, "o", "\r\n"]
[1518.326, "o", "    Solves the optimization problem::\r\n"]
[1518.336, "o", "\r\n"]
[1518.346, "o", "        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[1518.356, "o", "                    (U,V)\r\n"]
[1518.366, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[1518.376, "o", "\r\n"]
[1518.386, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[1518.396, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[1518.406, "o", "    of all the entries in the matrix.\r\n"]
[1518.416, "o", "\r\n"]
[1518.426, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[1518.436, "o", "\r\n"]
[1518.446, "o", "    Parameters\r\n"]
[1518.456, "o", "    ----------\r\n"]
[1518.466, "o", "    n_components : int, default=None\r\n"]
[1518.476, "o", "        Number of dictionary elements to extract. If None, then ``n_components``\r\n"]
[1518.486, "o", "        is set to ``n_features``.\r\n"]
[1518.496, "o", "\r\n"]
[1518.506, "o", "    alpha : float, default=1.0\r\n"]
[1518.516, "o", "        Sparsity controlling parameter.\r\n"]
[1518.526, "o", "\r\n"]
[1518.536, "o", "    max_iter : int, default=1000\r\n"]
[1518.546, "o", "        Maximum number of iterations to perform.\r\n"]
[1518.556, "o", "\r\n"]
[1518.566, "o", "    tol : float, default=1e-8\r\n"]
[1518.576, "o", "        Tolerance for numerical error.\r\n"]
[1518.586, "o", "\r\n"]
[1518.596, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[1518.606, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[1518.616, "o", "          problem (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[1518.626, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[1518.636, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n"]
[1518.646, "o", "          faster if the estimated components are sparse.\r\n"]
[1518.656, "o", "\r\n"]
[1518.666, "o", "        .. versionadded:: 0.17\r\n"]
[1518.676, "o", "           *cd* coordinate descent method to improve speed.\r\n"]
[1518.686, "o", "\r\n"]
[1518.696, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[1518.706, "o", "            'threshold'}, default='omp'\r\n"]
[1518.716, "o", "        Algorithm used to transform the data:\r\n"]
[1518.726, "o", "\r\n"]
[1518.736, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[1518.746, "o", "          (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[1518.756, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[1518.766, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1518.776, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n"]
[1518.786, "o", "          will be faster if the estimated components are sparse.\r\n"]
[1518.796, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1518.806, "o", "          solution.\r\n"]
[1518.816, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[1518.826, "o", "          the projection ``dictionary * X'``.\r\n"]
[1518.836, "o", "\r\n"]
[1518.846, "o", "        .. versionadded:: 0.17\r\n"]
[1518.856, "o", "           *lasso_cd* coordinate descent method to improve speed.\r\n"]
[1518.866, "o", "\r\n"]
[1518.876, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[1518.886, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1518.896, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[1518.906, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[1518.916, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1518.926, "o", "\r\n"]
[1518.936, "o", "    transform_alpha : float, default=None\r\n"]
[1518.946, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1518.956, "o", "        penalty applied to the L1 norm.\r\n"]
[1518.966, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1518.976, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1518.986, "o", "        If `None`, defaults to `alpha`.\r\n"]
[1518.996, "o", "\r\n"]
[1519.006, "o", "        .. versionchanged:: 1.2\r\n"]
[1519.016, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[1519.026, "o", "\r\n"]
[1519.036, "o", "    n_jobs : int or None, default=None\r\n"]
[1519.046, "o", "        Number of parallel jobs to run.\r\n"]
[1519.056, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1519.066, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1519.076, "o", "        for more details.\r\n"]
[1519.086, "o", "\r\n"]
[1519.096, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1519.106, "o", "        Initial value for the code, for warm restart. Only used if `code_init`\r\n"]
[1519.116, "o", "        and `dict_init` are not None.\r\n"]
[1519.126, "o", "\r\n"]
[1519.136, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1519.146, "o", "        Initial values for the dictionary, for warm restart. Only used if\r\n"]
[1519.156, "o", "        `code_init` and `dict_init` are not None.\r\n"]
[1519.166, "o", "\r\n"]
[1519.176, "o", "    callback : callable, default=None\r\n"]
[1519.186, "o", "        Callable that gets invoked every five iterations.\r\n"]
[1519.196, "o", "\r\n"]
[1519.206, "o", "        .. versionadded:: 1.3\r\n"]
[1519.216, "o", "\r\n"]
[1519.226, "o", "    verbose : bool, default=False\r\n"]
[1519.236, "o", "        To control the verbosity of the procedure.\r\n"]
[1519.246, "o", "\r\n"]
[1519.256, "o", "    split_sign : bool, default=False\r\n"]
[1519.266, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[1519.276, "o", "        its negative part and its positive part. This can improve the\r\n"]
[1519.286, "o", "        performance of downstream classifiers.\r\n"]
[1519.296, "o", "\r\n"]
[1519.306, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1519.316, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[1519.326, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[1519.336, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[1519.346, "o", "        results across multiple function calls.\r\n"]
[1519.356, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1519.366, "o", "\r\n"]
[1519.376, "o", "    positive_code : bool, default=False\r\n"]
[1519.386, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1519.396, "o", "\r\n"]
[1519.406, "o", "        .. versionadded:: 0.20\r\n"]
[1519.416, "o", "\r\n"]
[1519.426, "o", "    positive_dict : bool, default=False\r\n"]
[1519.436, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1519.446, "o", "\r\n"]
[1519.456, "o", "        .. versionadded:: 0.20\r\n"]
[1519.466, "o", "\r\n"]
[1519.476, "o", "    transform_max_iter : int, default=1000\r\n"]
[1519.486, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1519.496, "o", "        `'lasso_lars'`.\r\n"]
[1519.506, "o", "\r\n"]
[1519.516, "o", "        .. versionadded:: 0.22\r\n"]
[1519.526, "o", "\r\n"]
[1519.536, "o", "    Attributes\r\n"]
[1519.546, "o", "    ----------\r\n"]
[1519.556, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[1519.566, "o", "        dictionary atoms extracted from the data\r\n"]
[1519.576, "o", "\r\n"]
[1519.586, "o", "    error_ : array\r\n"]
[1519.596, "o", "        vector of errors at each iteration\r\n"]
[1519.606, "o", "\r\n"]
[1519.616, "o", "    n_features_in_ : int\r\n"]
[1519.626, "o", "        Number of features seen during :term:`fit`.\r\n"]
[1519.636, "o", "\r\n"]
[1519.646, "o", "        .. versionadded:: 0.24\r\n"]
[1519.656, "o", "\r\n"]
[1519.666, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[1519.676, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[1519.686, "o", "        has feature names that are all strings.\r\n"]
[1519.696, "o", "\r\n"]
[1519.706, "o", "        .. versionadded:: 1.0\r\n"]
[1519.716, "o", "\r\n"]
[1519.726, "o", "    n_iter_ : int\r\n"]
[1519.736, "o", "        Number of iterations run.\r\n"]
[1519.746, "o", "\r\n"]
[1519.756, "o", "    See Also\r\n"]
[1519.766, "o", "    --------\r\n"]
[1519.776, "o", "    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n"]
[1519.786, "o", "        dictionary learning algorithm.\r\n"]
[1519.796, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1519.806, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[1519.816, "o", "        precomputed dictionary.\r\n"]
[1519.826, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1519.836, "o", "\r\n"]
[1519.846, "o", "    References\r\n"]
[1519.856, "o", "    ----------\r\n"]
[1519.866, "o", "\r\n"]
[1519.876, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[1519.886, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[1519.896, "o", "\r\n"]
[1519.906, "o", "    Examples\r\n"]
[1519.916, "o", "    --------\r\n"]
[1519.926, "o", "    >>> import numpy as np\r\n"]
[1519.936, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[1520.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1520.002, "i", "grep -n \"def _minibatch_step\\|_minibatch_step\\(\" sklearn/decomposition/_dict_learning.py\r"]
[1520.004, "o", "grep -n \"def _minibatch_step\\|_minibatch_step\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1522.478, "o", "\u001b[?2004l\r\n"]
[1525.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1525.002, "i", "grep -n \"_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.py\r"]
[1525.004, "o", "grep -n \"_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1527.478, "o", "\u001b[?2004l\r\n"]
[1530.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1530.002, "i", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r"]
[1530.004, "o", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r\n"]
[1530.016, "o", "\u001b[?2004l\r\n"]
[1530.026, "o", "\"\"\" Dictionary learning.\r\n"]
[1530.036, "o", "\"\"\"\r\n"]
[1530.046, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[1530.056, "o", "# License: BSD 3 clause\r\n"]
[1530.066, "o", "\r\n"]
[1530.076, "o", "import itertools\r\n"]
[1530.086, "o", "import sys\r\n"]
[1530.096, "o", "import time\r\n"]
[1530.106, "o", "import warnings\r\n"]
[1530.116, "o", "from math import ceil\r\n"]
[1530.126, "o", "from numbers import Integral, Real\r\n"]
[1530.136, "o", "\r\n"]
[1530.146, "o", "import numpy as np\r\n"]
[1530.156, "o", "from joblib import effective_n_jobs\r\n"]
[1530.166, "o", "from scipy import linalg\r\n"]
[1530.176, "o", "\r\n"]
[1530.186, "o", "from ..base import (\r\n"]
[1530.196, "o", "    BaseEstimator,\r\n"]
[1530.206, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[1530.216, "o", "    TransformerMixin,\r\n"]
[1530.226, "o", "    _fit_context,\r\n"]
[1530.236, "o", ")\r\n"]
[1530.246, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[1530.256, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[1530.266, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[1530.276, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[1530.286, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[1530.296, "o", "from ..utils.validation import check_is_fitted\r\n"]
[1530.306, "o", "\r\n"]
[1530.316, "o", "\r\n"]
[1530.326, "o", "def _check_positive_coding(method, positive):\r\n"]
[1530.336, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[1530.346, "o", "        raise ValueError(\r\n"]
[1530.356, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[1530.366, "o", "        )\r\n"]
[1530.376, "o", "\r\n"]
[1530.386, "o", "\r\n"]
[1530.396, "o", "def _sparse_encode_precomputed(\r\n"]
[1530.406, "o", "    X,\r\n"]
[1530.416, "o", "    dictionary,\r\n"]
[1530.426, "o", "    *,\r\n"]
[1530.436, "o", "    gram=None,\r\n"]
[1530.446, "o", "    cov=None,\r\n"]
[1530.456, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1530.466, "o", "    regularization=None,\r\n"]
[1530.476, "o", "    copy_cov=True,\r\n"]
[1530.486, "o", "    init=None,\r\n"]
[1530.496, "o", "    max_iter=1000,\r\n"]
[1530.506, "o", "    verbose=0,\r\n"]
[1530.516, "o", "    positive=False,\r\n"]
[1530.526, "o", "):\r\n"]
[1530.536, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[1530.546, "o", "\r\n"]
[1530.556, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[1530.566, "o", "\r\n"]
[1530.576, "o", "    Parameters\r\n"]
[1530.586, "o", "    ----------\r\n"]
[1530.596, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[1530.606, "o", "        Data matrix.\r\n"]
[1530.616, "o", "\r\n"]
[1530.626, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1530.636, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1530.646, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[1530.656, "o", "\r\n"]
[1530.666, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[1530.676, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[1530.686, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[1530.696, "o", "\r\n"]
[1530.706, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[1530.716, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[1530.726, "o", "\r\n"]
[1530.736, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1530.746, "o", "            default='lasso_lars'\r\n"]
[1530.756, "o", "        The algorithm used:\r\n"]
[1530.766, "o", "\r\n"]
[1530.776, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[1530.786, "o", "          (`linear_model.lars_path`);\r\n"]
[1530.796, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1530.806, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1530.816, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1530.826, "o", "          the estimated components are sparse;\r\n"]
[1530.836, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1530.846, "o", "          solution;\r\n"]
[1530.856, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1530.866, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[1530.876, "o", "\r\n"]
[1530.886, "o", "    regularization : int or float, default=None\r\n"]
[1530.896, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[1530.906, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[1530.916, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[1530.926, "o", "\r\n"]
[1530.936, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1530.946, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[1530.956, "o", "        `algorithm='lasso_cd'`.\r\n"]
[1530.966, "o", "\r\n"]
[1530.976, "o", "    max_iter : int, default=1000\r\n"]
[1530.986, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1530.996, "o", "        `'lasso_lars'`.\r\n"]
[1531.006, "o", "\r\n"]
[1531.016, "o", "    copy_cov : bool, default=True\r\n"]
[1531.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1531.036, "o", "        be overwritten.\r\n"]
[1531.046, "o", "\r\n"]
[1531.056, "o", "    verbose : int, default=0\r\n"]
[1531.066, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[1531.076, "o", "\r\n"]
[1531.086, "o", "    positive: bool, default=False\r\n"]
[1531.096, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[1531.106, "o", "\r\n"]
[1531.116, "o", "        .. versionadded:: 0.20\r\n"]
[1531.126, "o", "\r\n"]
[1531.136, "o", "    Returns\r\n"]
[1531.146, "o", "    -------\r\n"]
[1531.156, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[1531.166, "o", "        The sparse codes.\r\n"]
[1531.176, "o", "    \"\"\"\r\n"]
[1531.186, "o", "    n_samples, n_features = X.shape\r\n"]
[1531.196, "o", "    n_components = dictionary.shape[0]\r\n"]
[1531.206, "o", "\r\n"]
[1531.216, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[1531.226, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1531.236, "o", "        try:\r\n"]
[1531.246, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1531.256, "o", "\r\n"]
[1531.266, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1531.276, "o", "            # corrects the verbosity level.\r\n"]
[1531.286, "o", "            lasso_lars = LassoLars(\r\n"]
[1531.296, "o", "                alpha=alpha,\r\n"]
[1531.306, "o", "                fit_intercept=False,\r\n"]
[1531.316, "o", "                verbose=verbose,\r\n"]
[1531.326, "o", "                precompute=gram,\r\n"]
[1531.336, "o", "                fit_path=False,\r\n"]
[1531.346, "o", "                positive=positive,\r\n"]
[1531.356, "o", "                max_iter=max_iter,\r\n"]
[1531.366, "o", "            )\r\n"]
[1531.376, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1531.386, "o", "            new_code = lasso_lars.coef_\r\n"]
[1531.396, "o", "        finally:\r\n"]
[1531.406, "o", "            np.seterr(**err_mgt)\r\n"]
[1531.416, "o", "\r\n"]
[1531.426, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[1531.436, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1531.446, "o", "\r\n"]
[1531.456, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[1531.466, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[1531.476, "o", "        # argument that we could pass in from Lasso.\r\n"]
[1531.486, "o", "        clf = Lasso(\r\n"]
[1531.496, "o", "            alpha=alpha,\r\n"]
[1531.506, "o", "            fit_intercept=False,\r\n"]
[1531.516, "o", "            precompute=gram,\r\n"]
[1531.526, "o", "            max_iter=max_iter,\r\n"]
[1531.536, "o", "            warm_start=True,\r\n"]
[1531.546, "o", "            positive=positive,\r\n"]
[1531.556, "o", "        )\r\n"]
[1531.566, "o", "\r\n"]
[1531.576, "o", "        if init is not None:\r\n"]
[1531.586, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[1531.596, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[1531.606, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[1531.616, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[1531.626, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[1531.636, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[1531.646, "o", "                init = np.array(init)\r\n"]
[1531.656, "o", "            clf.coef_ = init\r\n"]
[1531.666, "o", "\r\n"]
[1531.676, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[1531.686, "o", "        new_code = clf.coef_\r\n"]
[1531.696, "o", "\r\n"]
[1531.706, "o", "    elif algorithm == \"lars\":\r\n"]
[1531.716, "o", "        try:\r\n"]
[1531.726, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1531.736, "o", "\r\n"]
[1531.746, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1531.756, "o", "            # corrects the verbosity level.\r\n"]
[1531.766, "o", "            lars = Lars(\r\n"]
[1531.776, "o", "                fit_intercept=False,\r\n"]
[1531.786, "o", "                verbose=verbose,\r\n"]
[1531.796, "o", "                precompute=gram,\r\n"]
[1531.806, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[1531.816, "o", "                fit_path=False,\r\n"]
[1531.826, "o", "            )\r\n"]
[1531.836, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1531.846, "o", "            new_code = lars.coef_\r\n"]
[1531.856, "o", "        finally:\r\n"]
[1531.866, "o", "            np.seterr(**err_mgt)\r\n"]
[1531.876, "o", "\r\n"]
[1531.886, "o", "    elif algorithm == \"threshold\":\r\n"]
[1531.896, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[1531.906, "o", "        if positive:\r\n"]
[1531.916, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[1531.926, "o", "\r\n"]
[1531.936, "o", "    elif algorithm == \"omp\":\r\n"]
[1531.946, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[1531.956, "o", "            Gram=gram,\r\n"]
[1531.966, "o", "            Xy=cov,\r\n"]
[1531.976, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[1531.986, "o", "            tol=None,\r\n"]
[1531.996, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[1532.006, "o", "            copy_Xy=copy_cov,\r\n"]
[1532.016, "o", "        ).T\r\n"]
[1532.026, "o", "\r\n"]
[1532.036, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[1532.046, "o", "\r\n"]
[1532.056, "o", "\r\n"]
[1532.066, "o", "@validate_params(\r\n"]
[1532.076, "o", "    {\r\n"]
[1532.086, "o", "        \"X\": [\"array-like\"],\r\n"]
[1532.096, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[1532.106, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[1532.116, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[1532.126, "o", "        \"algorithm\": [\r\n"]
[1532.136, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[1532.146, "o", "        ],\r\n"]
[1532.156, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1532.166, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[1532.176, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[1532.186, "o", "        \"init\": [\"array-like\", None],\r\n"]
[1532.196, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1532.206, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[1532.216, "o", "        \"check_input\": [\"boolean\"],\r\n"]
[1532.226, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[1532.236, "o", "        \"positive\": [\"boolean\"],\r\n"]
[1532.246, "o", "    },\r\n"]
[1532.256, "o", "    prefer_skip_nested_validation=True,\r\n"]
[1532.266, "o", ")\r\n"]
[1532.276, "o", "# XXX : could be moved to the linear_model module\r\n"]
[1532.286, "o", "def sparse_encode(\r\n"]
[1532.296, "o", "    X,\r\n"]
[1532.306, "o", "    dictionary,\r\n"]
[1532.316, "o", "    *,\r\n"]
[1532.326, "o", "    gram=None,\r\n"]
[1532.336, "o", "    cov=None,\r\n"]
[1532.346, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1532.356, "o", "    n_nonzero_coefs=None,\r\n"]
[1532.366, "o", "    alpha=None,\r\n"]
[1532.376, "o", "    copy_cov=True,\r\n"]
[1532.386, "o", "    init=None,\r\n"]
[1532.396, "o", "    max_iter=1000,\r\n"]
[1532.406, "o", "    n_jobs=None,\r\n"]
[1532.416, "o", "    check_input=True,\r\n"]
[1532.426, "o", "    verbose=0,\r\n"]
[1532.436, "o", "    positive=False,\r\n"]
[1532.446, "o", "):\r\n"]
[1532.456, "o", "    \"\"\"Sparse coding.\r\n"]
[1532.466, "o", "\r\n"]
[1532.476, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[1532.486, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[1532.496, "o", "\r\n"]
[1532.506, "o", "        X ~= code * dictionary\r\n"]
[1532.516, "o", "\r\n"]
[1532.526, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[1532.536, "o", "\r\n"]
[1532.546, "o", "    Parameters\r\n"]
[1532.556, "o", "    ----------\r\n"]
[1532.566, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[1532.576, "o", "        Data matrix.\r\n"]
[1532.586, "o", "\r\n"]
[1532.596, "o", "    dictionary : array-like of shape (n_components, n_features)\r\n"]
[1532.606, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1532.616, "o", "        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[1532.626, "o", "        output.\r\n"]
[1532.636, "o", "\r\n"]
[1532.646, "o", "    gram : array-like of shape (n_components, n_components), default=None\r\n"]
[1532.656, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n"]
[1532.666, "o", "\r\n"]
[1532.676, "o", "    cov : array-like of shape (n_components, n_samples), default=None\r\n"]
[1532.686, "o", "        Precomputed covariance, `dictionary' * X`.\r\n"]
[1532.696, "o", "\r\n"]
[1532.706, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1532.716, "o", "            default='lasso_lars'\r\n"]
[1532.726, "o", "        The algorithm used:\r\n"]
[1532.736, "o", "\r\n"]
[1532.746, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[1532.756, "o", "          (`linear_model.lars_path`);\r\n"]
[1532.766, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1532.776, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1532.786, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1532.796, "o", "          the estimated components are sparse;\r\n"]
[1532.806, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1532.816, "o", "          solution;\r\n"]
[1532.826, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1532.836, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[1532.846, "o", "\r\n"]
[1532.856, "o", "    n_nonzero_coefs : int, default=None\r\n"]
[1532.866, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1532.876, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[1532.886, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[1532.896, "o", "        `n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1532.906, "o", "\r\n"]
[1532.916, "o", "    alpha : float, default=None\r\n"]
[1532.926, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1532.936, "o", "        penalty applied to the L1 norm.\r\n"]
[1532.946, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1532.956, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1532.966, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[1532.976, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[1532.986, "o", "        `n_nonzero_coefs`.\r\n"]
[1532.996, "o", "        If `None`, default to 1.\r\n"]
[1533.006, "o", "\r\n"]
[1533.016, "o", "    copy_cov : bool, default=True\r\n"]
[1533.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1533.036, "o", "        be overwritten.\r\n"]
[1533.046, "o", "\r\n"]
[1533.056, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1533.066, "o", "        Initialization value of the sparse codes. Only used if\r\n"]
[1533.076, "o", "        `algorithm='lasso_cd'`.\r\n"]
[1533.086, "o", "\r\n"]
[1533.096, "o", "    max_iter : int, default=1000\r\n"]
[1533.106, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1533.116, "o", "        `'lasso_lars'`.\r\n"]
[1533.126, "o", "\r\n"]
[1533.136, "o", "    n_jobs : int, default=None\r\n"]
[1533.146, "o", "        Number of parallel jobs to run.\r\n"]
[1533.156, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1533.166, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1533.176, "o", "        for more details.\r\n"]
[1533.186, "o", "\r\n"]
[1533.196, "o", "    check_input : bool, default=True\r\n"]
[1533.206, "o", "        If `False`, the input arrays X and dictionary will not be checked.\r\n"]
[1533.216, "o", "\r\n"]
[1533.226, "o", "    verbose : int, default=0\r\n"]
[1533.236, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[1533.246, "o", "\r\n"]
[1533.256, "o", "    positive : bool, default=False\r\n"]
[1533.266, "o", "        Whether to enforce positivity when finding the encoding.\r\n"]
[1533.276, "o", "\r\n"]
[1533.286, "o", "        .. versionadded:: 0.20\r\n"]
[1533.296, "o", "\r\n"]
[1533.306, "o", "    Returns\r\n"]
[1533.316, "o", "    -------\r\n"]
[1533.326, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1533.336, "o", "        The sparse codes.\r\n"]
[1533.346, "o", "\r\n"]
[1533.356, "o", "    See Also\r\n"]
[1533.366, "o", "    --------\r\n"]
[1533.376, "o", "    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n"]
[1533.386, "o", "        path using LARS algorithm.\r\n"]
[1533.396, "o", "    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n"]
[1533.406, "o", "    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n"]
[1533.416, "o", "    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n"]
[1533.426, "o", "        dictionary.\r\n"]
[1533.436, "o", "    \"\"\"\r\n"]
[1533.446, "o", "    if check_input:\r\n"]
[1533.456, "o", "        if algorithm == \"lasso_cd\":\r\n"]
[1533.466, "o", "            dictionary = check_array(\r\n"]
[1533.476, "o", "                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n"]
[1533.486, "o", "            )\r\n"]
[1533.496, "o", "            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n"]
[1533.506, "o", "        else:\r\n"]
[1533.516, "o", "            dictionary = check_array(dictionary)\r\n"]
[1533.526, "o", "            X = check_array(X)\r\n"]
[1533.536, "o", "\r\n"]
[1533.546, "o", "    if dictionary.shape[1] != X.shape[1]:\r\n"]
[1533.556, "o", "        raise ValueError(\r\n"]
[1533.566, "o", "            \"Dictionary and X have different numbers of features:\"\r\n"]
[1533.576, "o", "            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n"]
[1533.586, "o", "        )\r\n"]
[1533.596, "o", "\r\n"]
[1533.606, "o", "    _check_positive_coding(algorithm, positive)\r\n"]
[1533.616, "o", "\r\n"]
[1533.626, "o", "    return _sparse_encode(\r\n"]
[1533.636, "o", "        X,\r\n"]
[1533.646, "o", "        dictionary,\r\n"]
[1533.656, "o", "        gram=gram,\r\n"]
[1533.666, "o", "        cov=cov,\r\n"]
[1533.676, "o", "        algorithm=algorithm,\r\n"]
[1533.686, "o", "        n_nonzero_coefs=n_nonzero_coefs,\r\n"]
[1533.696, "o", "        alpha=alpha,\r\n"]
[1533.706, "o", "        copy_cov=copy_cov,\r\n"]
[1533.716, "o", "        init=init,\r\n"]
[1533.726, "o", "        max_iter=max_iter,\r\n"]
[1533.736, "o", "        n_jobs=n_jobs,\r\n"]
[1533.746, "o", "        verbose=verbose,\r\n"]
[1533.756, "o", "        positive=positive,\r\n"]
[1533.766, "o", "    )\r\n"]
[1533.776, "o", "\r\n"]
[1533.786, "o", "\r\n"]
[1533.796, "o", "def _sparse_encode(\r\n"]
[1533.806, "o", "    X,\r\n"]
[1533.816, "o", "    dictionary,\r\n"]
[1533.826, "o", "    *,\r\n"]
[1533.836, "o", "    gram=None,\r\n"]
[1533.846, "o", "    cov=None,\r\n"]
[1533.856, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1533.866, "o", "    n_nonzero_coefs=None,\r\n"]
[1533.876, "o", "    alpha=None,\r\n"]
[1533.886, "o", "    copy_cov=True,\r\n"]
[1533.896, "o", "    init=None,\r\n"]
[1533.906, "o", "    max_iter=1000,\r\n"]
[1533.916, "o", "    n_jobs=None,\r\n"]
[1533.926, "o", "    verbose=0,\r\n"]
[1533.936, "o", "    positive=False,\r\n"]
[1533.946, "o", "):\r\n"]
[1533.956, "o", "    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[1533.966, "o", "\r\n"]
[1533.976, "o", "    n_samples, n_features = X.shape\r\n"]
[1533.986, "o", "    n_components = dictionary.shape[0]\r\n"]
[1533.996, "o", "\r\n"]
[1534.006, "o", "    if algorithm in (\"lars\", \"omp\"):\r\n"]
[1534.016, "o", "        regularization = n_nonzero_coefs\r\n"]
[1534.026, "o", "        if regularization is None:\r\n"]
[1534.036, "o", "            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[1534.046, "o", "    else:\r\n"]
[1534.056, "o", "        regularization = alpha\r\n"]
[1534.066, "o", "        if regularization is None:\r\n"]
[1534.076, "o", "            regularization = 1.0\r\n"]
[1534.086, "o", "\r\n"]
[1534.096, "o", "    if gram is None and algorithm != \"threshold\":\r\n"]
[1534.106, "o", "        gram = np.dot(dictionary, dictionary.T)\r\n"]
[1534.116, "o", "\r\n"]
[1534.126, "o", "    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[1534.136, "o", "        copy_cov = False\r\n"]
[1534.146, "o", "        cov = np.dot(dictionary, X.T)\r\n"]
[1534.156, "o", "\r\n"]
[1534.166, "o", "    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[1534.176, "o", "        code = _sparse_encode_precomputed(\r\n"]
[1534.186, "o", "            X,\r\n"]
[1534.196, "o", "            dictionary,\r\n"]
[1534.206, "o", "            gram=gram,\r\n"]
[1534.216, "o", "            cov=cov,\r\n"]
[1534.226, "o", "            algorithm=algorithm,\r\n"]
[1534.236, "o", "            regularization=regularization,\r\n"]
[1534.246, "o", "            copy_cov=copy_cov,\r\n"]
[1534.256, "o", "            init=init,\r\n"]
[1534.266, "o", "            max_iter=max_iter,\r\n"]
[1534.276, "o", "            verbose=verbose,\r\n"]
[1534.286, "o", "            positive=positive,\r\n"]
[1534.296, "o", "        )\r\n"]
[1534.306, "o", "        return code\r\n"]
[1534.316, "o", "\r\n"]
[1534.326, "o", "    # Enter parallel code block\r\n"]
[1534.336, "o", "    n_samples = X.shape[0]\r\n"]
[1534.346, "o", "    n_components = dictionary.shape[0]\r\n"]
[1534.356, "o", "    code = np.empty((n_samples, n_components))\r\n"]
[1534.366, "o", "    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[1534.376, "o", "\r\n"]
[1534.386, "o", "    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[1534.396, "o", "        delayed(_sparse_encode_precomputed)(\r\n"]
[1534.406, "o", "            X[this_slice],\r\n"]
[1534.416, "o", "            dictionary,\r\n"]
[1534.426, "o", "            gram=gram,\r\n"]
[1534.436, "o", "            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[1534.446, "o", "            algorithm=algorithm,\r\n"]
[1534.456, "o", "            regularization=regularization,\r\n"]
[1534.466, "o", "            copy_cov=copy_cov,\r\n"]
[1534.476, "o", "            init=init[this_slice] if init is not None else None,\r\n"]
[1534.486, "o", "            max_iter=max_iter,\r\n"]
[1534.496, "o", "            verbose=verbose,\r\n"]
[1534.506, "o", "            positive=positive,\r\n"]
[1534.516, "o", "        )\r\n"]
[1534.526, "o", "        for this_slice in slices\r\n"]
[1534.536, "o", "    )\r\n"]
[1534.546, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[1534.556, "o", "        code[this_slice] = this_view\r\n"]
[1534.566, "o", "    return code\r\n"]
[1534.576, "o", "\r\n"]
[1534.586, "o", "\r\n"]
[1534.596, "o", "def _update_dict(\r\n"]
[1534.606, "o", "    dictionary,\r\n"]
[1534.616, "o", "    Y,\r\n"]
[1534.626, "o", "    code,\r\n"]
[1534.636, "o", "    A=None,\r\n"]
[1534.646, "o", "    B=None,\r\n"]
[1534.656, "o", "    verbose=False,\r\n"]
[1534.666, "o", "    random_state=None,\r\n"]
[1534.676, "o", "    positive=False,\r\n"]
[1534.686, "o", "):\r\n"]
[1534.696, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[1534.706, "o", "\r\n"]
[1534.716, "o", "    Parameters\r\n"]
[1534.726, "o", "    ----------\r\n"]
[1534.736, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1534.746, "o", "        Value of the dictionary at the previous iteration.\r\n"]
[1534.756, "o", "\r\n"]
[1534.766, "o", "    Y : ndarray of shape (n_samples, n_features)\r\n"]
[1534.776, "o", "        Data matrix.\r\n"]
[1534.786, "o", "\r\n"]
[1534.796, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1534.806, "o", "        Sparse coding of the data against which to optimize the dictionary.\r\n"]
[1534.816, "o", "\r\n"]
[1534.826, "o", "    A : ndarray of shape (n_components, n_components), default=None\r\n"]
[1534.836, "o", "        Together with `B`, sufficient stats of the online model to update the\r\n"]
[1534.846, "o", "        dictionary.\r\n"]
[1534.856, "o", "\r\n"]
[1534.866, "o", "    B : ndarray of shape (n_features, n_components), default=None\r\n"]
[1534.876, "o", "        Together with `A`, sufficient stats of the online model to update the\r\n"]
[1534.886, "o", "        dictionary.\r\n"]
[1534.896, "o", "\r\n"]
[1534.906, "o", "    verbose: bool, default=False\r\n"]
[1534.916, "o", "        Degree of output the procedure will print.\r\n"]
[1534.926, "o", "\r\n"]
[1534.936, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1535.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1535.002, "i", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r"]
[1535.004, "o", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r\n"]
[1535.016, "o", "\u001b[?2004l\r\n"]
[1535.026, "o", "           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[1535.036, "o", "\r\n"]
[1535.046, "o", "    return_n_iter : bool, default=False\r\n"]
[1535.056, "o", "        Whether or not to return the number of iterations.\r\n"]
[1535.066, "o", "\r\n"]
[1535.076, "o", "        .. deprecated:: 1.1\r\n"]
[1535.086, "o", "           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n"]
[1535.096, "o", "\r\n"]
[1535.106, "o", "    positive_dict : bool, default=False\r\n"]
[1535.116, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1535.126, "o", "\r\n"]
[1535.136, "o", "        .. versionadded:: 0.20\r\n"]
[1535.146, "o", "\r\n"]
[1535.156, "o", "    positive_code : bool, default=False\r\n"]
[1535.166, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1535.176, "o", "\r\n"]
[1535.186, "o", "        .. versionadded:: 0.20\r\n"]
[1535.196, "o", "\r\n"]
[1535.206, "o", "    method_max_iter : int, default=1000\r\n"]
[1535.216, "o", "        Maximum number of iterations to perform when solving the lasso problem.\r\n"]
[1535.226, "o", "\r\n"]
[1535.236, "o", "        .. versionadded:: 0.22\r\n"]
[1535.246, "o", "\r\n"]
[1535.256, "o", "    tol : float, default=1e-3\r\n"]
[1535.266, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[1535.276, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[1535.286, "o", "\r\n"]
[1535.296, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[1535.306, "o", "        `tol` to 0.0.\r\n"]
[1535.316, "o", "\r\n"]
[1535.326, "o", "        .. versionadded:: 1.1\r\n"]
[1535.336, "o", "\r\n"]
[1535.346, "o", "    max_no_improvement : int, default=10\r\n"]
[1535.356, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[1535.366, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[1535.376, "o", "        `max_iter` is not None.\r\n"]
[1535.386, "o", "\r\n"]
[1535.396, "o", "        To disable convergence detection based on cost function, set\r\n"]
[1535.406, "o", "        `max_no_improvement` to None.\r\n"]
[1535.416, "o", "\r\n"]
[1535.426, "o", "        .. versionadded:: 1.1\r\n"]
[1535.436, "o", "\r\n"]
[1535.446, "o", "    Returns\r\n"]
[1535.456, "o", "    -------\r\n"]
[1535.466, "o", "    code : ndarray of shape (n_samples, n_components),\r\n"]
[1535.476, "o", "        The sparse code (only returned if `return_code=True`).\r\n"]
[1535.486, "o", "\r\n"]
[1535.496, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[1535.506, "o", "        The solutions to the dictionary learning problem.\r\n"]
[1535.516, "o", "\r\n"]
[1535.526, "o", "    n_iter : int\r\n"]
[1535.536, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[1535.546, "o", "        set to `True`.\r\n"]
[1535.556, "o", "\r\n"]
[1535.566, "o", "    See Also\r\n"]
[1535.576, "o", "    --------\r\n"]
[1535.586, "o", "    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n"]
[1535.596, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1535.606, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n"]
[1535.616, "o", "        learning algorithm.\r\n"]
[1535.626, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1535.636, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1535.646, "o", "    \"\"\"\r\n"]
[1535.656, "o", "    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n"]
[1535.666, "o", "    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n"]
[1535.676, "o", "        raise ValueError(\r\n"]
[1535.686, "o", "            \"The following arguments are incompatible with 'max_iter': \"\r\n"]
[1535.696, "o", "            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n"]
[1535.706, "o", "        )\r\n"]
[1535.716, "o", "\r\n"]
[1535.726, "o", "    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n"]
[1535.736, "o", "    return_inner_stats = _check_warn_deprecated(\r\n"]
[1535.746, "o", "        return_inner_stats,\r\n"]
[1535.756, "o", "        \"return_inner_stats\",\r\n"]
[1535.766, "o", "        default=False,\r\n"]
[1535.776, "o", "        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n"]
[1535.786, "o", "    )\r\n"]
[1535.796, "o", "    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n"]
[1535.806, "o", "    return_n_iter = _check_warn_deprecated(\r\n"]
[1535.816, "o", "        return_n_iter,\r\n"]
[1535.826, "o", "        \"return_n_iter\",\r\n"]
[1535.836, "o", "        default=False,\r\n"]
[1535.846, "o", "        additional_message=(\r\n"]
[1535.856, "o", "            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n"]
[1535.866, "o", "            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n"]
[1535.876, "o", "        ),\r\n"]
[1535.886, "o", "    )\r\n"]
[1535.896, "o", "\r\n"]
[1535.906, "o", "    if max_iter is not None:\r\n"]
[1535.916, "o", "        transform_algorithm = \"lasso_\" + method\r\n"]
[1535.926, "o", "\r\n"]
[1535.936, "o", "        est = MiniBatchDictionaryLearning(\r\n"]
[1535.946, "o", "            n_components=n_components,\r\n"]
[1535.956, "o", "            alpha=alpha,\r\n"]
[1535.966, "o", "            n_iter=n_iter,\r\n"]
[1535.976, "o", "            n_jobs=n_jobs,\r\n"]
[1535.986, "o", "            fit_algorithm=method,\r\n"]
[1535.996, "o", "            batch_size=batch_size,\r\n"]
[1536.006, "o", "            shuffle=shuffle,\r\n"]
[1536.016, "o", "            dict_init=dict_init,\r\n"]
[1536.026, "o", "            random_state=random_state,\r\n"]
[1536.036, "o", "            transform_algorithm=transform_algorithm,\r\n"]
[1536.046, "o", "            transform_alpha=alpha,\r\n"]
[1536.056, "o", "            positive_code=positive_code,\r\n"]
[1536.066, "o", "            positive_dict=positive_dict,\r\n"]
[1536.076, "o", "            transform_max_iter=method_max_iter,\r\n"]
[1536.086, "o", "            verbose=verbose,\r\n"]
[1536.096, "o", "            callback=callback,\r\n"]
[1536.106, "o", "            tol=tol,\r\n"]
[1536.116, "o", "            max_no_improvement=max_no_improvement,\r\n"]
[1536.126, "o", "        ).fit(X)\r\n"]
[1536.136, "o", "\r\n"]
[1536.146, "o", "        if not return_code:\r\n"]
[1536.156, "o", "            return est.components_\r\n"]
[1536.166, "o", "        else:\r\n"]
[1536.176, "o", "            code = est.transform(X)\r\n"]
[1536.186, "o", "            return code, est.components_\r\n"]
[1536.196, "o", "\r\n"]
[1536.206, "o", "    # TODO(1.4) remove the whole old behavior\r\n"]
[1536.216, "o", "    # Fallback to old behavior\r\n"]
[1536.226, "o", "\r\n"]
[1536.236, "o", "    n_iter = _check_warn_deprecated(\r\n"]
[1536.246, "o", "        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n"]
[1536.256, "o", "    )\r\n"]
[1536.266, "o", "\r\n"]
[1536.276, "o", "    if n_components is None:\r\n"]
[1536.286, "o", "        n_components = X.shape[1]\r\n"]
[1536.296, "o", "\r\n"]
[1536.306, "o", "    if method not in (\"lars\", \"cd\"):\r\n"]
[1536.316, "o", "        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n"]
[1536.326, "o", "\r\n"]
[1536.336, "o", "    _check_positive_coding(method, positive_code)\r\n"]
[1536.346, "o", "\r\n"]
[1536.356, "o", "    method = \"lasso_\" + method\r\n"]
[1536.366, "o", "\r\n"]
[1536.376, "o", "    t0 = time.time()\r\n"]
[1536.386, "o", "    n_samples, n_features = X.shape\r\n"]
[1536.396, "o", "    # Avoid integer division problems\r\n"]
[1536.406, "o", "    alpha = float(alpha)\r\n"]
[1536.416, "o", "    random_state = check_random_state(random_state)\r\n"]
[1536.426, "o", "\r\n"]
[1536.436, "o", "    # Init V with SVD of X\r\n"]
[1536.446, "o", "    if dict_init is not None:\r\n"]
[1536.456, "o", "        dictionary = dict_init\r\n"]
[1536.466, "o", "    else:\r\n"]
[1536.476, "o", "        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n"]
[1536.486, "o", "        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[1536.496, "o", "    r = len(dictionary)\r\n"]
[1536.506, "o", "    if n_components <= r:\r\n"]
[1536.516, "o", "        dictionary = dictionary[:n_components, :]\r\n"]
[1536.526, "o", "    else:\r\n"]
[1536.536, "o", "        dictionary = np.r_[\r\n"]
[1536.546, "o", "            dictionary,\r\n"]
[1536.556, "o", "            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n"]
[1536.566, "o", "        ]\r\n"]
[1536.576, "o", "\r\n"]
[1536.586, "o", "    if verbose == 1:\r\n"]
[1536.596, "o", "        print(\"[dict_learning]\", end=\" \")\r\n"]
[1536.606, "o", "\r\n"]
[1536.616, "o", "    if shuffle:\r\n"]
[1536.626, "o", "        X_train = X.copy()\r\n"]
[1536.636, "o", "        random_state.shuffle(X_train)\r\n"]
[1536.646, "o", "    else:\r\n"]
[1536.656, "o", "        X_train = X\r\n"]
[1536.666, "o", "\r\n"]
[1536.676, "o", "    X_train = check_array(\r\n"]
[1536.686, "o", "        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n"]
[1536.696, "o", "    )\r\n"]
[1536.706, "o", "\r\n"]
[1536.716, "o", "    # Fortran-order dict better suited for the sparse coding which is the\r\n"]
[1536.726, "o", "    # bottleneck of this algorithm.\r\n"]
[1536.736, "o", "    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n"]
[1536.746, "o", "    dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[1536.756, "o", "\r\n"]
[1536.766, "o", "    batches = gen_batches(n_samples, batch_size)\r\n"]
[1536.776, "o", "    batches = itertools.cycle(batches)\r\n"]
[1536.786, "o", "\r\n"]
[1536.796, "o", "    # The covariance of the dictionary\r\n"]
[1536.806, "o", "    if inner_stats is None:\r\n"]
[1536.816, "o", "        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n"]
[1536.826, "o", "        # The data approximation\r\n"]
[1536.836, "o", "        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n"]
[1536.846, "o", "    else:\r\n"]
[1536.856, "o", "        A = inner_stats[0].copy()\r\n"]
[1536.866, "o", "        B = inner_stats[1].copy()\r\n"]
[1536.876, "o", "\r\n"]
[1536.886, "o", "    # If n_iter is zero, we need to return zero.\r\n"]
[1536.896, "o", "    ii = iter_offset - 1\r\n"]
[1536.906, "o", "\r\n"]
[1536.916, "o", "    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n"]
[1536.926, "o", "        this_X = X_train[batch]\r\n"]
[1536.936, "o", "        dt = time.time() - t0\r\n"]
[1536.946, "o", "        if verbose == 1:\r\n"]
[1536.956, "o", "            sys.stdout.write(\".\")\r\n"]
[1536.966, "o", "            sys.stdout.flush()\r\n"]
[1536.976, "o", "        elif verbose:\r\n"]
[1536.986, "o", "            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n"]
[1536.996, "o", "                print(\r\n"]
[1537.006, "o", "                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n"]
[1537.016, "o", "                )\r\n"]
[1537.026, "o", "\r\n"]
[1537.036, "o", "        this_code = sparse_encode(\r\n"]
[1537.046, "o", "            this_X,\r\n"]
[1537.056, "o", "            dictionary,\r\n"]
[1537.066, "o", "            algorithm=method,\r\n"]
[1537.076, "o", "            alpha=alpha,\r\n"]
[1537.086, "o", "            n_jobs=n_jobs,\r\n"]
[1537.096, "o", "            check_input=False,\r\n"]
[1537.106, "o", "            positive=positive_code,\r\n"]
[1537.116, "o", "            max_iter=method_max_iter,\r\n"]
[1537.126, "o", "            verbose=verbose,\r\n"]
[1537.136, "o", "        )\r\n"]
[1537.146, "o", "\r\n"]
[1537.156, "o", "        # Update the auxiliary variables\r\n"]
[1537.166, "o", "        if ii < batch_size - 1:\r\n"]
[1537.176, "o", "            theta = float((ii + 1) * batch_size)\r\n"]
[1537.186, "o", "        else:\r\n"]
[1537.196, "o", "            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n"]
[1537.206, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[1537.216, "o", "\r\n"]
[1537.226, "o", "        A *= beta\r\n"]
[1537.236, "o", "        A += np.dot(this_code.T, this_code)\r\n"]
[1537.246, "o", "        B *= beta\r\n"]
[1537.256, "o", "        B += np.dot(this_X.T, this_code)\r\n"]
[1537.266, "o", "\r\n"]
[1537.276, "o", "        # Update dictionary in place\r\n"]
[1537.286, "o", "        _update_dict(\r\n"]
[1537.296, "o", "            dictionary,\r\n"]
[1537.306, "o", "            this_X,\r\n"]
[1537.316, "o", "            this_code,\r\n"]
[1537.326, "o", "            A,\r\n"]
[1537.336, "o", "            B,\r\n"]
[1537.346, "o", "            verbose=verbose,\r\n"]
[1537.356, "o", "            random_state=random_state,\r\n"]
[1537.366, "o", "            positive=positive_dict,\r\n"]
[1537.376, "o", "        )\r\n"]
[1537.386, "o", "\r\n"]
[1537.396, "o", "        # Maybe we need a stopping criteria based on the amount of\r\n"]
[1537.406, "o", "        # modification in the dictionary\r\n"]
[1537.416, "o", "        if callback is not None:\r\n"]
[1537.426, "o", "            callback(locals())\r\n"]
[1537.436, "o", "\r\n"]
[1537.446, "o", "    if return_inner_stats:\r\n"]
[1537.456, "o", "        if return_n_iter:\r\n"]
[1537.466, "o", "            return dictionary, (A, B), ii - iter_offset + 1\r\n"]
[1537.476, "o", "        else:\r\n"]
[1537.486, "o", "            return dictionary, (A, B)\r\n"]
[1537.496, "o", "    if return_code:\r\n"]
[1537.506, "o", "        if verbose > 1:\r\n"]
[1537.516, "o", "            print(\"Learning code...\", end=\" \")\r\n"]
[1537.526, "o", "        elif verbose == 1:\r\n"]
[1537.536, "o", "            print(\"|\", end=\" \")\r\n"]
[1537.546, "o", "        code = sparse_encode(\r\n"]
[1537.556, "o", "            X,\r\n"]
[1537.566, "o", "            dictionary,\r\n"]
[1537.576, "o", "            algorithm=method,\r\n"]
[1537.586, "o", "            alpha=alpha,\r\n"]
[1537.596, "o", "            n_jobs=n_jobs,\r\n"]
[1537.606, "o", "            check_input=False,\r\n"]
[1537.616, "o", "            positive=positive_code,\r\n"]
[1537.626, "o", "            max_iter=method_max_iter,\r\n"]
[1537.636, "o", "            verbose=verbose,\r\n"]
[1537.646, "o", "        )\r\n"]
[1537.656, "o", "        if verbose > 1:\r\n"]
[1537.666, "o", "            dt = time.time() - t0\r\n"]
[1537.676, "o", "            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n"]
[1537.686, "o", "        if return_n_iter:\r\n"]
[1537.696, "o", "            return code, dictionary, ii - iter_offset + 1\r\n"]
[1537.706, "o", "        else:\r\n"]
[1537.716, "o", "            return code, dictionary\r\n"]
[1537.726, "o", "\r\n"]
[1537.736, "o", "    if return_n_iter:\r\n"]
[1537.746, "o", "        return dictionary, ii - iter_offset + 1\r\n"]
[1537.756, "o", "    else:\r\n"]
[1537.766, "o", "        return dictionary\r\n"]
[1537.776, "o", "\r\n"]
[1537.786, "o", "\r\n"]
[1537.796, "o", "@validate_params(\r\n"]
[1537.806, "o", "    {\r\n"]
[1537.816, "o", "        \"X\": [\"array-like\"],\r\n"]
[1537.826, "o", "        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[1537.836, "o", "        \"return_n_iter\": [\"boolean\"],\r\n"]
[1537.846, "o", "        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1537.856, "o", "    },\r\n"]
[1537.866, "o", "    prefer_skip_nested_validation=False,\r\n"]
[1537.876, "o", ")\r\n"]
[1537.886, "o", "def dict_learning(\r\n"]
[1537.896, "o", "    X,\r\n"]
[1537.906, "o", "    n_components,\r\n"]
[1537.916, "o", "    *,\r\n"]
[1537.926, "o", "    alpha,\r\n"]
[1537.936, "o", "    max_iter=100,\r\n"]
[1537.946, "o", "    tol=1e-8,\r\n"]
[1537.956, "o", "    method=\"lars\",\r\n"]
[1537.966, "o", "    n_jobs=None,\r\n"]
[1537.976, "o", "    dict_init=None,\r\n"]
[1537.986, "o", "    code_init=None,\r\n"]
[1537.996, "o", "    callback=None,\r\n"]
[1538.006, "o", "    verbose=False,\r\n"]
[1538.016, "o", "    random_state=None,\r\n"]
[1538.026, "o", "    return_n_iter=False,\r\n"]
[1538.036, "o", "    positive_dict=False,\r\n"]
[1538.046, "o", "    positive_code=False,\r\n"]
[1538.056, "o", "    method_max_iter=1000,\r\n"]
[1538.066, "o", "):\r\n"]
[1538.076, "o", "    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n"]
[1538.086, "o", "\r\n"]
[1538.096, "o", "    Finds the best dictionary and the corresponding sparse code for\r\n"]
[1538.106, "o", "    approximating the data matrix X by solving::\r\n"]
[1538.116, "o", "\r\n"]
[1538.126, "o", "        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[1538.136, "o", "                     (U,V)\r\n"]
[1538.146, "o", "                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n"]
[1538.156, "o", "\r\n"]
[1538.166, "o", "    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n"]
[1538.176, "o", "    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n"]
[1538.186, "o", "    which is the sum of the absolute values of all the entries in the matrix.\r\n"]
[1538.196, "o", "\r\n"]
[1538.206, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[1538.216, "o", "\r\n"]
[1538.226, "o", "    Parameters\r\n"]
[1538.236, "o", "    ----------\r\n"]
[1538.246, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[1538.256, "o", "        Data matrix.\r\n"]
[1538.266, "o", "\r\n"]
[1538.276, "o", "    n_components : int\r\n"]
[1538.286, "o", "        Number of dictionary atoms to extract.\r\n"]
[1538.296, "o", "\r\n"]
[1538.306, "o", "    alpha : int or float\r\n"]
[1538.316, "o", "        Sparsity controlling parameter.\r\n"]
[1538.326, "o", "\r\n"]
[1538.336, "o", "    max_iter : int, default=100\r\n"]
[1538.346, "o", "        Maximum number of iterations to perform.\r\n"]
[1538.356, "o", "\r\n"]
[1538.366, "o", "    tol : float, default=1e-8\r\n"]
[1538.376, "o", "        Tolerance for the stopping condition.\r\n"]
[1538.386, "o", "\r\n"]
[1538.396, "o", "    method : {'lars', 'cd'}, default='lars'\r\n"]
[1538.406, "o", "        The method used:\r\n"]
[1538.416, "o", "\r\n"]
[1538.426, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[1538.436, "o", "           problem (`linear_model.lars_path`);\r\n"]
[1538.446, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[1538.456, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[1538.466, "o", "          the estimated components are sparse.\r\n"]
[1538.476, "o", "\r\n"]
[1538.486, "o", "    n_jobs : int, default=None\r\n"]
[1538.496, "o", "        Number of parallel jobs to run.\r\n"]
[1538.506, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1538.516, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1538.526, "o", "        for more details.\r\n"]
[1538.536, "o", "\r\n"]
[1538.546, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1538.556, "o", "        Initial value for the dictionary for warm restart scenarios. Only used\r\n"]
[1538.566, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[1538.576, "o", "\r\n"]
[1538.586, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1538.596, "o", "        Initial value for the sparse code for warm restart scenarios. Only used\r\n"]
[1538.606, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[1538.616, "o", "\r\n"]
[1538.626, "o", "    callback : callable, default=None\r\n"]
[1538.636, "o", "        Callable that gets invoked every five iterations.\r\n"]
[1538.646, "o", "\r\n"]
[1538.656, "o", "    verbose : bool, default=False\r\n"]
[1538.666, "o", "        To control the verbosity of the procedure.\r\n"]
[1538.676, "o", "\r\n"]
[1538.686, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1538.696, "o", "        Used for randomly initializing the dictionary. Pass an int for\r\n"]
[1538.706, "o", "        reproducible results across multiple function calls.\r\n"]
[1538.716, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1538.726, "o", "\r\n"]
[1538.736, "o", "    return_n_iter : bool, default=False\r\n"]
[1538.746, "o", "        Whether or not to return the number of iterations.\r\n"]
[1538.756, "o", "\r\n"]
[1538.766, "o", "    positive_dict : bool, default=False\r\n"]
[1538.776, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1538.786, "o", "\r\n"]
[1538.796, "o", "        .. versionadded:: 0.20\r\n"]
[1538.806, "o", "\r\n"]
[1538.816, "o", "    positive_code : bool, default=False\r\n"]
[1538.826, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1538.836, "o", "\r\n"]
[1538.846, "o", "        .. versionadded:: 0.20\r\n"]
[1538.856, "o", "\r\n"]
[1538.866, "o", "    method_max_iter : int, default=1000\r\n"]
[1538.876, "o", "        Maximum number of iterations to perform.\r\n"]
[1538.886, "o", "\r\n"]
[1538.896, "o", "        .. versionadded:: 0.22\r\n"]
[1538.906, "o", "\r\n"]
[1538.916, "o", "    Returns\r\n"]
[1538.926, "o", "    -------\r\n"]
[1538.936, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1538.946, "o", "        The sparse code factor in the matrix factorization.\r\n"]
[1538.956, "o", "\r\n"]
[1538.966, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[1538.976, "o", "        The dictionary factor in the matrix factorization.\r\n"]
[1538.986, "o", "\r\n"]
[1538.996, "o", "    errors : array\r\n"]
[1539.006, "o", "        Vector of errors at each iteration.\r\n"]
[1539.016, "o", "\r\n"]
[1539.026, "o", "    n_iter : int\r\n"]
[1539.036, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[1539.046, "o", "        set to True.\r\n"]
[1539.056, "o", "\r\n"]
[1539.066, "o", "    See Also\r\n"]
[1539.076, "o", "    --------\r\n"]
[1539.086, "o", "    dict_learning_online : Solve a dictionary learning matrix factorization\r\n"]
[1539.096, "o", "        problem online.\r\n"]
[1539.106, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1539.116, "o", "    MiniBatchDictionaryLearning : A faster, less accurate version\r\n"]
[1539.126, "o", "        of the dictionary learning algorithm.\r\n"]
[1539.136, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1539.146, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1539.156, "o", "    \"\"\"\r\n"]
[1539.166, "o", "    estimator = DictionaryLearning(\r\n"]
[1539.176, "o", "        n_components=n_components,\r\n"]
[1539.186, "o", "        alpha=alpha,\r\n"]
[1539.196, "o", "        max_iter=max_iter,\r\n"]
[1539.206, "o", "        tol=tol,\r\n"]
[1539.216, "o", "        fit_algorithm=method,\r\n"]
[1539.226, "o", "        n_jobs=n_jobs,\r\n"]
[1539.236, "o", "        dict_init=dict_init,\r\n"]
[1539.246, "o", "        callback=callback,\r\n"]
[1539.256, "o", "        code_init=code_init,\r\n"]
[1539.266, "o", "        verbose=verbose,\r\n"]
[1539.276, "o", "        random_state=random_state,\r\n"]
[1539.286, "o", "        positive_code=positive_code,\r\n"]
[1539.296, "o", "        positive_dict=positive_dict,\r\n"]
[1539.306, "o", "        transform_max_iter=method_max_iter,\r\n"]
[1539.316, "o", "    )\r\n"]
[1539.326, "o", "    code = estimator.fit_transform(X)\r\n"]
[1539.336, "o", "    if return_n_iter:\r\n"]
[1539.346, "o", "        return (\r\n"]
[1539.356, "o", "            code,\r\n"]
[1539.366, "o", "            estimator.components_,\r\n"]
[1539.376, "o", "            estimator.error_,\r\n"]
[1539.386, "o", "            estimator.n_iter_,\r\n"]
[1539.396, "o", "        )\r\n"]
[1539.406, "o", "    return code, estimator.components_, estimator.error_\r\n"]
[1539.416, "o", "\r\n"]
[1539.426, "o", "\r\n"]
[1539.436, "o", "class _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n"]
[1539.446, "o", "    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n"]
[1539.456, "o", "\r\n"]
[1539.466, "o", "    def __init__(\r\n"]
[1539.476, "o", "        self,\r\n"]
[1539.486, "o", "        transform_algorithm,\r\n"]
[1539.496, "o", "        transform_n_nonzero_coefs,\r\n"]
[1539.506, "o", "        transform_alpha,\r\n"]
[1539.516, "o", "        split_sign,\r\n"]
[1539.526, "o", "        n_jobs,\r\n"]
[1539.536, "o", "        positive_code,\r\n"]
[1539.546, "o", "        transform_max_iter,\r\n"]
[1539.556, "o", "    ):\r\n"]
[1539.566, "o", "        self.transform_algorithm = transform_algorithm\r\n"]
[1539.576, "o", "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n"]
[1539.586, "o", "        self.transform_alpha = transform_alpha\r\n"]
[1539.596, "o", "        self.transform_max_iter = transform_max_iter\r\n"]
[1539.606, "o", "        self.split_sign = split_sign\r\n"]
[1539.616, "o", "        self.n_jobs = n_jobs\r\n"]
[1539.626, "o", "        self.positive_code = positive_code\r\n"]
[1539.636, "o", "\r\n"]
[1539.646, "o", "    def _transform(self, X, dictionary):\r\n"]
[1539.656, "o", "        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n"]
[1539.666, "o", "        SparseCoder.\"\"\"\r\n"]
[1539.676, "o", "        X = self._validate_data(X, reset=False)\r\n"]
[1539.686, "o", "\r\n"]
[1539.696, "o", "        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n"]
[1539.706, "o", "            transform_alpha = self.alpha\r\n"]
[1539.716, "o", "        else:\r\n"]
[1539.726, "o", "            transform_alpha = self.transform_alpha\r\n"]
[1539.736, "o", "\r\n"]
[1539.746, "o", "        code = sparse_encode(\r\n"]
[1539.756, "o", "            X,\r\n"]
[1539.766, "o", "            dictionary,\r\n"]
[1539.776, "o", "            algorithm=self.transform_algorithm,\r\n"]
[1539.786, "o", "            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n"]
[1539.796, "o", "            alpha=transform_alpha,\r\n"]
[1539.806, "o", "            max_iter=self.transform_max_iter,\r\n"]
[1539.816, "o", "            n_jobs=self.n_jobs,\r\n"]
[1539.826, "o", "            positive=self.positive_code,\r\n"]
[1539.836, "o", "        )\r\n"]
[1539.846, "o", "\r\n"]
[1539.856, "o", "        if self.split_sign:\r\n"]
[1539.866, "o", "            # feature vector is split into a positive and negative side\r\n"]
[1539.876, "o", "            n_samples, n_features = code.shape\r\n"]
[1539.886, "o", "            split_code = np.empty((n_samples, 2 * n_features))\r\n"]
[1539.896, "o", "            split_code[:, :n_features] = np.maximum(code, 0)\r\n"]
[1539.906, "o", "            split_code[:, n_features:] = -np.minimum(code, 0)\r\n"]
[1539.916, "o", "            code = split_code\r\n"]
[1539.926, "o", "\r\n"]
[1539.936, "o", "        return code\r\n"]
[1540.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1540.002, "i", "sed -n '1600,2600p' sklearn/decomposition/_dict_learning.py\r"]
[1540.004, "o", "sed -n '1600,2600p' sklearn/decomposition/_dict_learning.py\r\n"]
[1540.016, "o", "\u001b[?2004l\r\n"]
[1540.026, "o", "\r\n"]
[1540.036, "o", "    n_jobs : int or None, default=None\r\n"]
[1540.046, "o", "        Number of parallel jobs to run.\r\n"]
[1540.056, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1540.066, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1540.076, "o", "        for more details.\r\n"]
[1540.086, "o", "\r\n"]
[1540.096, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1540.106, "o", "        Initial value for the code, for warm restart. Only used if `code_init`\r\n"]
[1540.116, "o", "        and `dict_init` are not None.\r\n"]
[1540.126, "o", "\r\n"]
[1540.136, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1540.146, "o", "        Initial values for the dictionary, for warm restart. Only used if\r\n"]
[1540.156, "o", "        `code_init` and `dict_init` are not None.\r\n"]
[1540.166, "o", "\r\n"]
[1540.176, "o", "    callback : callable, default=None\r\n"]
[1540.186, "o", "        Callable that gets invoked every five iterations.\r\n"]
[1540.196, "o", "\r\n"]
[1540.206, "o", "        .. versionadded:: 1.3\r\n"]
[1540.216, "o", "\r\n"]
[1540.226, "o", "    verbose : bool, default=False\r\n"]
[1540.236, "o", "        To control the verbosity of the procedure.\r\n"]
[1540.246, "o", "\r\n"]
[1540.256, "o", "    split_sign : bool, default=False\r\n"]
[1540.266, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[1540.276, "o", "        its negative part and its positive part. This can improve the\r\n"]
[1540.286, "o", "        performance of downstream classifiers.\r\n"]
[1540.296, "o", "\r\n"]
[1540.306, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1540.316, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[1540.326, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[1540.336, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[1540.346, "o", "        results across multiple function calls.\r\n"]
[1540.356, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1540.366, "o", "\r\n"]
[1540.376, "o", "    positive_code : bool, default=False\r\n"]
[1540.386, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1540.396, "o", "\r\n"]
[1540.406, "o", "        .. versionadded:: 0.20\r\n"]
[1540.416, "o", "\r\n"]
[1540.426, "o", "    positive_dict : bool, default=False\r\n"]
[1540.436, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1540.446, "o", "\r\n"]
[1540.456, "o", "        .. versionadded:: 0.20\r\n"]
[1540.466, "o", "\r\n"]
[1540.476, "o", "    transform_max_iter : int, default=1000\r\n"]
[1540.486, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1540.496, "o", "        `'lasso_lars'`.\r\n"]
[1540.506, "o", "\r\n"]
[1540.516, "o", "        .. versionadded:: 0.22\r\n"]
[1540.526, "o", "\r\n"]
[1540.536, "o", "    Attributes\r\n"]
[1540.546, "o", "    ----------\r\n"]
[1540.556, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[1540.566, "o", "        dictionary atoms extracted from the data\r\n"]
[1540.576, "o", "\r\n"]
[1540.586, "o", "    error_ : array\r\n"]
[1540.596, "o", "        vector of errors at each iteration\r\n"]
[1540.606, "o", "\r\n"]
[1540.616, "o", "    n_features_in_ : int\r\n"]
[1540.626, "o", "        Number of features seen during :term:`fit`.\r\n"]
[1540.636, "o", "\r\n"]
[1540.646, "o", "        .. versionadded:: 0.24\r\n"]
[1540.656, "o", "\r\n"]
[1540.666, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[1540.676, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[1540.686, "o", "        has feature names that are all strings.\r\n"]
[1540.696, "o", "\r\n"]
[1540.706, "o", "        .. versionadded:: 1.0\r\n"]
[1540.716, "o", "\r\n"]
[1540.726, "o", "    n_iter_ : int\r\n"]
[1540.736, "o", "        Number of iterations run.\r\n"]
[1540.746, "o", "\r\n"]
[1540.756, "o", "    See Also\r\n"]
[1540.766, "o", "    --------\r\n"]
[1540.776, "o", "    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n"]
[1540.786, "o", "        dictionary learning algorithm.\r\n"]
[1540.796, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1540.806, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[1540.816, "o", "        precomputed dictionary.\r\n"]
[1540.826, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1540.836, "o", "\r\n"]
[1540.846, "o", "    References\r\n"]
[1540.856, "o", "    ----------\r\n"]
[1540.866, "o", "\r\n"]
[1540.876, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[1540.886, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[1540.896, "o", "\r\n"]
[1540.906, "o", "    Examples\r\n"]
[1540.916, "o", "    --------\r\n"]
[1540.926, "o", "    >>> import numpy as np\r\n"]
[1540.936, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[1540.946, "o", "    >>> from sklearn.decomposition import DictionaryLearning\r\n"]
[1540.956, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[1540.966, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[1540.976, "o", "    ...     random_state=42,\r\n"]
[1540.986, "o", "    ... )\r\n"]
[1540.996, "o", "    >>> dict_learner = DictionaryLearning(\r\n"]
[1541.006, "o", "    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n"]
[1541.016, "o", "    ...     random_state=42,\r\n"]
[1541.026, "o", "    ... )\r\n"]
[1541.036, "o", "    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n"]
[1541.046, "o", "\r\n"]
[1541.056, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[1541.066, "o", "\r\n"]
[1541.076, "o", "    >>> np.mean(X_transformed == 0)\r\n"]
[1541.086, "o", "    0.41...\r\n"]
[1541.096, "o", "\r\n"]
[1541.106, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[1541.116, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[1541.126, "o", "    the original signal:\r\n"]
[1541.136, "o", "\r\n"]
[1541.146, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[1541.156, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[1541.166, "o", "    0.07...\r\n"]
[1541.176, "o", "    \"\"\"\r\n"]
[1541.186, "o", "\r\n"]
[1541.196, "o", "    _parameter_constraints: dict = {\r\n"]
[1541.206, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1541.216, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1541.226, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1541.236, "o", "        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1541.246, "o", "        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[1541.256, "o", "        \"transform_algorithm\": [\r\n"]
[1541.266, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[1541.276, "o", "        ],\r\n"]
[1541.286, "o", "        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1541.296, "o", "        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[1541.306, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[1541.316, "o", "        \"code_init\": [np.ndarray, None],\r\n"]
[1541.326, "o", "        \"dict_init\": [np.ndarray, None],\r\n"]
[1541.336, "o", "        \"callback\": [callable, None],\r\n"]
[1541.346, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[1541.356, "o", "        \"split_sign\": [\"boolean\"],\r\n"]
[1541.366, "o", "        \"random_state\": [\"random_state\"],\r\n"]
[1541.376, "o", "        \"positive_code\": [\"boolean\"],\r\n"]
[1541.386, "o", "        \"positive_dict\": [\"boolean\"],\r\n"]
[1541.396, "o", "        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1541.406, "o", "    }\r\n"]
[1541.416, "o", "\r\n"]
[1541.426, "o", "    def __init__(\r\n"]
[1541.436, "o", "        self,\r\n"]
[1541.446, "o", "        n_components=None,\r\n"]
[1541.456, "o", "        *,\r\n"]
[1541.466, "o", "        alpha=1,\r\n"]
[1541.476, "o", "        max_iter=1000,\r\n"]
[1541.486, "o", "        tol=1e-8,\r\n"]
[1541.496, "o", "        fit_algorithm=\"lars\",\r\n"]
[1541.506, "o", "        transform_algorithm=\"omp\",\r\n"]
[1541.516, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[1541.526, "o", "        transform_alpha=None,\r\n"]
[1541.536, "o", "        n_jobs=None,\r\n"]
[1541.546, "o", "        code_init=None,\r\n"]
[1541.556, "o", "        dict_init=None,\r\n"]
[1541.566, "o", "        callback=None,\r\n"]
[1541.576, "o", "        verbose=False,\r\n"]
[1541.586, "o", "        split_sign=False,\r\n"]
[1541.596, "o", "        random_state=None,\r\n"]
[1541.606, "o", "        positive_code=False,\r\n"]
[1541.616, "o", "        positive_dict=False,\r\n"]
[1541.626, "o", "        transform_max_iter=1000,\r\n"]
[1541.636, "o", "    ):\r\n"]
[1541.646, "o", "        super().__init__(\r\n"]
[1541.656, "o", "            transform_algorithm,\r\n"]
[1541.666, "o", "            transform_n_nonzero_coefs,\r\n"]
[1541.676, "o", "            transform_alpha,\r\n"]
[1541.686, "o", "            split_sign,\r\n"]
[1541.696, "o", "            n_jobs,\r\n"]
[1541.706, "o", "            positive_code,\r\n"]
[1541.716, "o", "            transform_max_iter,\r\n"]
[1541.726, "o", "        )\r\n"]
[1541.736, "o", "        self.n_components = n_components\r\n"]
[1541.746, "o", "        self.alpha = alpha\r\n"]
[1541.756, "o", "        self.max_iter = max_iter\r\n"]
[1541.766, "o", "        self.tol = tol\r\n"]
[1541.776, "o", "        self.fit_algorithm = fit_algorithm\r\n"]
[1541.786, "o", "        self.code_init = code_init\r\n"]
[1541.796, "o", "        self.dict_init = dict_init\r\n"]
[1541.806, "o", "        self.callback = callback\r\n"]
[1541.816, "o", "        self.verbose = verbose\r\n"]
[1541.826, "o", "        self.random_state = random_state\r\n"]
[1541.836, "o", "        self.positive_dict = positive_dict\r\n"]
[1541.846, "o", "\r\n"]
[1541.856, "o", "    def fit(self, X, y=None):\r\n"]
[1541.866, "o", "        \"\"\"Fit the model from data in X.\r\n"]
[1541.876, "o", "\r\n"]
[1541.886, "o", "        Parameters\r\n"]
[1541.896, "o", "        ----------\r\n"]
[1541.906, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[1541.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1541.926, "o", "            and `n_features` is the number of features.\r\n"]
[1541.936, "o", "\r\n"]
[1541.946, "o", "        y : Ignored\r\n"]
[1541.956, "o", "            Not used, present for API consistency by convention.\r\n"]
[1541.966, "o", "\r\n"]
[1541.976, "o", "        Returns\r\n"]
[1541.986, "o", "        -------\r\n"]
[1541.996, "o", "        self : object\r\n"]
[1542.006, "o", "            Returns the instance itself.\r\n"]
[1542.016, "o", "        \"\"\"\r\n"]
[1542.026, "o", "        self.fit_transform(X)\r\n"]
[1542.036, "o", "        return self\r\n"]
[1542.046, "o", "\r\n"]
[1542.056, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[1542.066, "o", "    def fit_transform(self, X, y=None):\r\n"]
[1542.076, "o", "        \"\"\"Fit the model from data in X and return the transformed data.\r\n"]
[1542.086, "o", "\r\n"]
[1542.096, "o", "        Parameters\r\n"]
[1542.106, "o", "        ----------\r\n"]
[1542.116, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[1542.126, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1542.136, "o", "            and `n_features` is the number of features.\r\n"]
[1542.146, "o", "\r\n"]
[1542.156, "o", "        y : Ignored\r\n"]
[1542.166, "o", "            Not used, present for API consistency by convention.\r\n"]
[1542.176, "o", "\r\n"]
[1542.186, "o", "        Returns\r\n"]
[1542.196, "o", "        -------\r\n"]
[1542.206, "o", "        V : ndarray of shape (n_samples, n_components)\r\n"]
[1542.216, "o", "            Transformed data.\r\n"]
[1542.226, "o", "        \"\"\"\r\n"]
[1542.236, "o", "        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n"]
[1542.246, "o", "\r\n"]
[1542.256, "o", "        method = \"lasso_\" + self.fit_algorithm\r\n"]
[1542.266, "o", "\r\n"]
[1542.276, "o", "        random_state = check_random_state(self.random_state)\r\n"]
[1542.286, "o", "        X = self._validate_data(X)\r\n"]
[1542.296, "o", "\r\n"]
[1542.306, "o", "        if self.n_components is None:\r\n"]
[1542.316, "o", "            n_components = X.shape[1]\r\n"]
[1542.326, "o", "        else:\r\n"]
[1542.336, "o", "            n_components = self.n_components\r\n"]
[1542.346, "o", "\r\n"]
[1542.356, "o", "        V, U, E, self.n_iter_ = _dict_learning(\r\n"]
[1542.366, "o", "            X,\r\n"]
[1542.376, "o", "            n_components,\r\n"]
[1542.386, "o", "            alpha=self.alpha,\r\n"]
[1542.396, "o", "            tol=self.tol,\r\n"]
[1542.406, "o", "            max_iter=self.max_iter,\r\n"]
[1542.416, "o", "            method=method,\r\n"]
[1542.426, "o", "            method_max_iter=self.transform_max_iter,\r\n"]
[1542.436, "o", "            n_jobs=self.n_jobs,\r\n"]
[1542.446, "o", "            code_init=self.code_init,\r\n"]
[1542.456, "o", "            dict_init=self.dict_init,\r\n"]
[1542.466, "o", "            callback=self.callback,\r\n"]
[1542.476, "o", "            verbose=self.verbose,\r\n"]
[1542.486, "o", "            random_state=random_state,\r\n"]
[1542.496, "o", "            return_n_iter=True,\r\n"]
[1542.506, "o", "            positive_dict=self.positive_dict,\r\n"]
[1542.516, "o", "            positive_code=self.positive_code,\r\n"]
[1542.526, "o", "        )\r\n"]
[1542.536, "o", "        self.components_ = U\r\n"]
[1542.546, "o", "        self.error_ = E\r\n"]
[1542.556, "o", "\r\n"]
[1542.566, "o", "        return V\r\n"]
[1542.576, "o", "\r\n"]
[1542.586, "o", "    @property\r\n"]
[1542.596, "o", "    def _n_features_out(self):\r\n"]
[1542.606, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[1542.616, "o", "        return self.components_.shape[0]\r\n"]
[1542.626, "o", "\r\n"]
[1542.636, "o", "    def _more_tags(self):\r\n"]
[1542.646, "o", "        return {\r\n"]
[1542.656, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[1542.666, "o", "        }\r\n"]
[1542.676, "o", "\r\n"]
[1542.686, "o", "\r\n"]
[1542.696, "o", "class MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[1542.706, "o", "    \"\"\"Mini-batch dictionary learning.\r\n"]
[1542.716, "o", "\r\n"]
[1542.726, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[1542.736, "o", "    encoding the fitted data.\r\n"]
[1542.746, "o", "\r\n"]
[1542.756, "o", "    Solves the optimization problem::\r\n"]
[1542.766, "o", "\r\n"]
[1542.776, "o", "       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[1542.786, "o", "                    (U,V)\r\n"]
[1542.796, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[1542.806, "o", "\r\n"]
[1542.816, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[1542.826, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[1542.836, "o", "    of all the entries in the matrix.\r\n"]
[1542.846, "o", "\r\n"]
[1542.856, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[1542.866, "o", "\r\n"]
[1542.876, "o", "    Parameters\r\n"]
[1542.886, "o", "    ----------\r\n"]
[1542.896, "o", "    n_components : int, default=None\r\n"]
[1542.906, "o", "        Number of dictionary elements to extract.\r\n"]
[1542.916, "o", "\r\n"]
[1542.926, "o", "    alpha : float, default=1\r\n"]
[1542.936, "o", "        Sparsity controlling parameter.\r\n"]
[1542.946, "o", "\r\n"]
[1542.956, "o", "    n_iter : int, default=1000\r\n"]
[1542.966, "o", "        Total number of iterations over data batches to perform.\r\n"]
[1542.976, "o", "\r\n"]
[1542.986, "o", "        .. deprecated:: 1.1\r\n"]
[1542.996, "o", "           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[1543.006, "o", "           ``max_iter`` instead.\r\n"]
[1543.016, "o", "\r\n"]
[1543.026, "o", "    max_iter : int, default=None\r\n"]
[1543.036, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[1543.046, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[1543.056, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[1543.066, "o", "\r\n"]
[1543.076, "o", "        .. versionadded:: 1.1\r\n"]
[1543.086, "o", "\r\n"]
[1543.096, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[1543.106, "o", "        The algorithm used:\r\n"]
[1543.116, "o", "\r\n"]
[1543.126, "o", "        - `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[1543.136, "o", "          problem (`linear_model.lars_path`)\r\n"]
[1543.146, "o", "        - `'cd'`: uses the coordinate descent method to compute the\r\n"]
[1543.156, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[1543.166, "o", "          the estimated components are sparse.\r\n"]
[1543.176, "o", "\r\n"]
[1543.186, "o", "    n_jobs : int, default=None\r\n"]
[1543.196, "o", "        Number of parallel jobs to run.\r\n"]
[1543.206, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1543.216, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1543.226, "o", "        for more details.\r\n"]
[1543.236, "o", "\r\n"]
[1543.246, "o", "    batch_size : int, default=256\r\n"]
[1543.256, "o", "        Number of samples in each mini-batch.\r\n"]
[1543.266, "o", "\r\n"]
[1543.276, "o", "        .. versionchanged:: 1.3\r\n"]
[1543.286, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[1543.296, "o", "\r\n"]
[1543.306, "o", "    shuffle : bool, default=True\r\n"]
[1543.316, "o", "        Whether to shuffle the samples before forming batches.\r\n"]
[1543.326, "o", "\r\n"]
[1543.336, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1543.346, "o", "        Initial value of the dictionary for warm restart scenarios.\r\n"]
[1543.356, "o", "\r\n"]
[1543.366, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[1543.376, "o", "            'threshold'}, default='omp'\r\n"]
[1543.386, "o", "        Algorithm used to transform the data:\r\n"]
[1543.396, "o", "\r\n"]
[1543.406, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[1543.416, "o", "          (`linear_model.lars_path`);\r\n"]
[1543.426, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[1543.436, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1543.446, "o", "          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n"]
[1543.456, "o", "          if the estimated components are sparse.\r\n"]
[1543.466, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1543.476, "o", "          solution.\r\n"]
[1543.486, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[1543.496, "o", "          the projection ``dictionary * X'``.\r\n"]
[1543.506, "o", "\r\n"]
[1543.516, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[1543.526, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1543.536, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[1543.546, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[1543.556, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1543.566, "o", "\r\n"]
[1543.576, "o", "    transform_alpha : float, default=None\r\n"]
[1543.586, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1543.596, "o", "        penalty applied to the L1 norm.\r\n"]
[1543.606, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1543.616, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1543.626, "o", "        If `None`, defaults to `alpha`.\r\n"]
[1543.636, "o", "\r\n"]
[1543.646, "o", "        .. versionchanged:: 1.2\r\n"]
[1543.656, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[1543.666, "o", "\r\n"]
[1543.676, "o", "    verbose : bool or int, default=False\r\n"]
[1543.686, "o", "        To control the verbosity of the procedure.\r\n"]
[1543.696, "o", "\r\n"]
[1543.706, "o", "    split_sign : bool, default=False\r\n"]
[1543.716, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[1543.726, "o", "        its negative part and its positive part. This can improve the\r\n"]
[1543.736, "o", "        performance of downstream classifiers.\r\n"]
[1543.746, "o", "\r\n"]
[1543.756, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1543.766, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[1543.776, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[1543.786, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[1543.796, "o", "        results across multiple function calls.\r\n"]
[1543.806, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1543.816, "o", "\r\n"]
[1543.826, "o", "    positive_code : bool, default=False\r\n"]
[1543.836, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1543.846, "o", "\r\n"]
[1543.856, "o", "        .. versionadded:: 0.20\r\n"]
[1543.866, "o", "\r\n"]
[1543.876, "o", "    positive_dict : bool, default=False\r\n"]
[1543.886, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1543.896, "o", "\r\n"]
[1543.906, "o", "        .. versionadded:: 0.20\r\n"]
[1543.916, "o", "\r\n"]
[1543.926, "o", "    transform_max_iter : int, default=1000\r\n"]
[1543.936, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1543.946, "o", "        `'lasso_lars'`.\r\n"]
[1543.956, "o", "\r\n"]
[1543.966, "o", "        .. versionadded:: 0.22\r\n"]
[1543.976, "o", "\r\n"]
[1543.986, "o", "    callback : callable, default=None\r\n"]
[1543.996, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[1544.006, "o", "\r\n"]
[1544.016, "o", "        .. versionadded:: 1.1\r\n"]
[1544.026, "o", "\r\n"]
[1544.036, "o", "    tol : float, default=1e-3\r\n"]
[1544.046, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[1544.056, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[1544.066, "o", "\r\n"]
[1544.076, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[1544.086, "o", "        `tol` to 0.0.\r\n"]
[1544.096, "o", "\r\n"]
[1544.106, "o", "        .. versionadded:: 1.1\r\n"]
[1544.116, "o", "\r\n"]
[1544.126, "o", "    max_no_improvement : int, default=10\r\n"]
[1544.136, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[1544.146, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[1544.156, "o", "        `max_iter` is not None.\r\n"]
[1544.166, "o", "\r\n"]
[1544.176, "o", "        To disable convergence detection based on cost function, set\r\n"]
[1544.186, "o", "        `max_no_improvement` to None.\r\n"]
[1544.196, "o", "\r\n"]
[1544.206, "o", "        .. versionadded:: 1.1\r\n"]
[1544.216, "o", "\r\n"]
[1544.226, "o", "    Attributes\r\n"]
[1544.236, "o", "    ----------\r\n"]
[1544.246, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[1544.256, "o", "        Components extracted from the data.\r\n"]
[1544.266, "o", "\r\n"]
[1544.276, "o", "    n_features_in_ : int\r\n"]
[1544.286, "o", "        Number of features seen during :term:`fit`.\r\n"]
[1544.296, "o", "\r\n"]
[1544.306, "o", "        .. versionadded:: 0.24\r\n"]
[1544.316, "o", "\r\n"]
[1544.326, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[1544.336, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[1544.346, "o", "        has feature names that are all strings.\r\n"]
[1544.356, "o", "\r\n"]
[1544.366, "o", "        .. versionadded:: 1.0\r\n"]
[1544.376, "o", "\r\n"]
[1544.386, "o", "    n_iter_ : int\r\n"]
[1544.396, "o", "        Number of iterations over the full dataset.\r\n"]
[1544.406, "o", "\r\n"]
[1544.416, "o", "    n_steps_ : int\r\n"]
[1544.426, "o", "        Number of mini-batches processed.\r\n"]
[1544.436, "o", "\r\n"]
[1544.446, "o", "        .. versionadded:: 1.1\r\n"]
[1544.456, "o", "\r\n"]
[1544.466, "o", "    See Also\r\n"]
[1544.476, "o", "    --------\r\n"]
[1544.486, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1544.496, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1544.506, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[1544.516, "o", "        precomputed dictionary.\r\n"]
[1544.526, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1544.536, "o", "\r\n"]
[1544.546, "o", "    References\r\n"]
[1544.556, "o", "    ----------\r\n"]
[1544.566, "o", "\r\n"]
[1544.576, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[1544.586, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[1544.596, "o", "\r\n"]
[1544.606, "o", "    Examples\r\n"]
[1544.616, "o", "    --------\r\n"]
[1544.626, "o", "    >>> import numpy as np\r\n"]
[1544.636, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[1544.646, "o", "    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n"]
[1544.656, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[1544.666, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[1544.676, "o", "    ...     random_state=42)\r\n"]
[1544.686, "o", "    >>> dict_learner = MiniBatchDictionaryLearning(\r\n"]
[1544.696, "o", "    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n"]
[1544.706, "o", "    ...     transform_alpha=0.1, random_state=42)\r\n"]
[1544.716, "o", "    >>> X_transformed = dict_learner.fit_transform(X)\r\n"]
[1544.726, "o", "\r\n"]
[1544.736, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[1544.746, "o", "\r\n"]
[1544.756, "o", "    >>> np.mean(X_transformed == 0) < 0.5\r\n"]
[1544.766, "o", "    True\r\n"]
[1544.776, "o", "\r\n"]
[1544.786, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[1544.796, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[1544.806, "o", "    the original signal:\r\n"]
[1544.816, "o", "\r\n"]
[1544.826, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[1544.836, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[1544.846, "o", "    0.057...\r\n"]
[1544.856, "o", "    \"\"\"\r\n"]
[1544.866, "o", "\r\n"]
[1544.876, "o", "    _parameter_constraints: dict = {\r\n"]
[1544.886, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1544.896, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1544.906, "o", "        \"n_iter\": [\r\n"]
[1544.916, "o", "            Interval(Integral, 0, None, closed=\"left\"),\r\n"]
[1544.926, "o", "            Hidden(StrOptions({\"deprecated\"})),\r\n"]
[1544.936, "o", "        ],\r\n"]
[1545.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1545.002, "i", "grep -n \"_minibatch_step\" sklearn/decomposition/_dict_learning.py || true\r"]
[1545.004, "o", "grep -n \"_minibatch_step\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[1545.9948, "o", "\u001b[?2004l\r\n"]
[1546.9836, "o", "\u001b[32m\u001b[K2219\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    def \u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(self, X, dictionary, random_state, step):\r\n"]
[1547.9724, "o", "\u001b[32m\u001b[K2399\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                batch_cost = self.\u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(\r\n"]
[1548.9612, "o", "\u001b[32m\u001b[K2425\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                self.\u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(X_train[batch], dictionary, self._random_state, i)\r\n"]
[1550.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1550.002, "i", "grep -n \"_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.py || true\r"]
[1550.004, "o", "grep -n \"_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[1552.478, "o", "\u001b[?2004l\r\n"]
[1555.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1555.002, "i", "grep -n \"def dict_learning_online\\|def _update_dict\\|def _update_dict\\(\" sklearn/decomposition/_dict_learning.py ||\r"]
[1555.004, "o", "grep -n \"def dict_learning_online\\|def _update_dict\\|def _update_dict\\(\" sklearn/decomposition/_dict_learning.py ||\r\n"]
[1556.654, "o", "| true\r\n"]
[1558.302, "o", "\u001b[?2004l\r\n"]
[1560.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1560.002, "i", "python - << 'PY'\r"]
[1560.004, "o", "python - << 'PY'\r\n"]
[1560.052642, "o", "\u001b[?2004l\r\n"]
[1560.099283, "o", "\u001b[?2004h> import re\r\n"]
[1560.145925, "o", "\u001b[?2004l\r\n"]
[1560.192566, "o", "\u001b[?2004h> p='sklearn/decomposition/_dict_learning.py'\r\n"]
[1560.239208, "o", "\u001b[?2004l\r\n"]
[1560.285849, "o", "\u001b[?2004h> s=open(p,'r',encoding='utf-8').read()\r\n"]
[1560.332491, "o", "\u001b[?2004l\r\n"]
[1560.379132, "o", "\u001b[?2004h> for name in ['_minibatch_step', 'dict_learning_online', '_update_dict']:\r\n"]
[1560.425774, "o", "\u001b[?2004l\r\n"]
[1560.472415, "o", "\u001b[?2004h>     for m in re.finditer(r'\\n\\s*def\\s+'+name+r'\\s*\\(', s):\r\n"]
[1560.519057, "o", "\u001b[?2004l\r\n"]
[1560.565698, "o", "\u001b[?2004h>         start=max(0, s.rfind('\\n', 0, m.start())-200)\r\n"]
[1560.61234, "o", "\u001b[?2004l\r\n"]
[1560.658981, "o", "\u001b[?2004h>         print('\\nFound', name, 'at index', m.start())\r\n"]
[1560.705623, "o", "\u001b[?2004l\r\n"]
[1560.752264, "o", "\u001b[?2004h>         block=s[m.start()-200:m.start()+400]\r\n"]
[1560.798906, "o", "\u001b[?2004l\r\n"]
[1560.845547, "o", "\u001b[?2004h>         print(block)\r\n"]
[1560.892189, "o", "\u001b[?2004l\r\n"]
[1560.93883, "o", "\u001b[?2004h> PY\r\n"]
[1560.985472, "o", "\u001b[?2004l\r\n"]
[1561.032113, "o", "\r\n"]
[1561.078755, "o", "Found _minibatch_step at index 73956\r\n"]
[1561.125396, "o", "tch_size\r\n"]
[1561.172038, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[1561.218679, "o", "\r\n"]
[1561.265321, "o", "        self._A *= beta\r\n"]
[1561.311962, "o", "        self._A += code.T @ code / batch_size\r\n"]
[1561.358604, "o", "        self._B *= beta\r\n"]
[1561.405245, "o", "        self._B += X.T @ code / batch_size\r\n"]
[1561.451887, "o", "\r\n"]
[1561.498528, "o", "    def _minibatch_step(self, X, dictionary, random_state, step):\r\n"]
[1561.54517, "o", "        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n"]
[1561.591811, "o", "        batch_size = X.shape[0]\r\n"]
[1561.638453, "o", "\r\n"]
[1561.685094, "o", "        # Compute code for this batch\r\n"]
[1561.731736, "o", "        code = _sparse_encode(\r\n"]
[1561.778377, "o", "            X,\r\n"]
[1561.825019, "o", "            dictionary,\r\n"]
[1561.87166, "o", "            algorithm=self._fit_algorithm,\r\n"]
[1561.918302, "o", "            alpha=self.alpha,\r\n"]
[1561.964943, "o", "            n_jobs=self.n_jobs,\r\n"]
[1562.011585, "o", "            posi\r\n"]
[1562.058226, "o", "\r\n"]
[1562.104868, "o", "Found dict_learning_online at index 21019\r\n"]
[1562.151509, "o", "ed in version 1.4.\"\r\n"]
[1562.198151, "o", "        )\r\n"]
[1562.244792, "o", "        if additional_message:\r\n"]
[1562.291434, "o", "            msg += f\" {additional_message}\"\r\n"]
[1562.338075, "o", "        warnings.warn(msg, FutureWarning)\r\n"]
[1562.384717, "o", "        return param\r\n"]
[1562.431358, "o", "    else:\r\n"]
[1562.478, "o", "        return default\r\n"]
[1562.524642, "o", "\r\n"]
[1562.571283, "o", "\r\n"]
[1562.617925, "o", "def dict_learning_online(\r\n"]
[1562.664566, "o", "    X,\r\n"]
[1562.711208, "o", "    n_components=2,\r\n"]
[1562.757849, "o", "    *,\r\n"]
[1562.804491, "o", "    alpha=1,\r\n"]
[1562.851132, "o", "    n_iter=\"deprecated\",\r\n"]
[1562.897774, "o", "    max_iter=None,\r\n"]
[1562.944415, "o", "    return_code=True,\r\n"]
[1562.991057, "o", "    dict_init=None,\r\n"]
[1563.037698, "o", "    callback=None,\r\n"]
[1563.08434, "o", "    batch_size=256,\r\n"]
[1563.130981, "o", "    verbose=False,\r\n"]
[1563.177623, "o", "    shuffle=True,\r\n"]
[1563.224264, "o", "    n_jobs=None,\r\n"]
[1563.270906, "o", "    method=\"lars\",\r\n"]
[1563.317547, "o", "    iter_offset=\"deprecated\",\r\n"]
[1563.364189, "o", "    random_state=None,\r\n"]
[1563.41083, "o", "    return_inner_stats=\"deprecated\",\r\n"]
[1563.457472, "o", "    inner_stats=\"deprecated\",\r\n"]
[1563.504113, "o", "    re\r\n"]
[1563.550755, "o", "\r\n"]
[1563.597396, "o", "Found _update_dict at index 14957\r\n"]
[1563.644038, "o", "=verbose,\r\n"]
[1563.690679, "o", "            positive=positive,\r\n"]
[1563.737321, "o", "        )\r\n"]
[1563.783962, "o", "        for this_slice in slices\r\n"]
[1563.830604, "o", "    )\r\n"]
[1563.877245, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[1563.923887, "o", "        code[this_slice] = this_view\r\n"]
[1563.970528, "o", "    return code\r\n"]
[1564.01717, "o", "\r\n"]
[1564.063811, "o", "\r\n"]
[1564.110453, "o", "def _update_dict(\r\n"]
[1564.157094, "o", "    dictionary,\r\n"]
[1564.203736, "o", "    Y,\r\n"]
[1564.250377, "o", "    code,\r\n"]
[1564.297019, "o", "    A=None,\r\n"]
[1564.34366, "o", "    B=None,\r\n"]
[1564.390302, "o", "    verbose=False,\r\n"]
[1564.436943, "o", "    random_state=None,\r\n"]
[1564.483585, "o", "    positive=False,\r\n"]
[1564.530226, "o", "):\r\n"]
[1564.576868, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[1564.623509, "o", "\r\n"]
[1564.670151, "o", "    Parameters\r\n"]
[1564.716792, "o", "    ----------\r\n"]
[1564.763434, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1564.810075, "o", "        Value of the dictionary at the previous iteration.\r\n"]
[1564.856717, "o", "\r\n"]
[1564.903358, "o", "    Y : ndarray of shape (n_samples, n_features)\r\n"]
[1565.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1565.002, "i", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py | sed -n '1,200p'\r"]
[1565.004, "o", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py | sed -n '1,200p'\r\n"]
[1565.030597, "o", "\u001b[?2004l\r\n"]
[1565.055194, "o", "\"\"\" Dictionary learning.\r\n"]
[1565.079791, "o", "\"\"\"\r\n"]
[1565.104388, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[1565.128985, "o", "# License: BSD 3 clause\r\n"]
[1565.153582, "o", "\r\n"]
[1565.178179, "o", "import itertools\r\n"]
[1565.202776, "o", "import sys\r\n"]
[1565.227373, "o", "import time\r\n"]
[1565.25197, "o", "import warnings\r\n"]
[1565.276567, "o", "from math import ceil\r\n"]
[1565.301164, "o", "from numbers import Integral, Real\r\n"]
[1565.325761, "o", "\r\n"]
[1565.350358, "o", "import numpy as np\r\n"]
[1565.374955, "o", "from joblib import effective_n_jobs\r\n"]
[1565.399552, "o", "from scipy import linalg\r\n"]
[1565.424149, "o", "\r\n"]
[1565.448746, "o", "from ..base import (\r\n"]
[1565.473343, "o", "    BaseEstimator,\r\n"]
[1565.49794, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[1565.522537, "o", "    TransformerMixin,\r\n"]
[1565.547134, "o", "    _fit_context,\r\n"]
[1565.571731, "o", ")\r\n"]
[1565.596328, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[1565.620925, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[1565.645522, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[1565.670119, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[1565.694716, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[1565.719313, "o", "from ..utils.validation import check_is_fitted\r\n"]
[1565.74391, "o", "\r\n"]
[1565.768507, "o", "\r\n"]
[1565.793104, "o", "def _check_positive_coding(method, positive):\r\n"]
[1565.817701, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[1565.842299, "o", "        raise ValueError(\r\n"]
[1565.866896, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[1565.891493, "o", "        )\r\n"]
[1565.91609, "o", "\r\n"]
[1565.940687, "o", "\r\n"]
[1565.965284, "o", "def _sparse_encode_precomputed(\r\n"]
[1565.989881, "o", "    X,\r\n"]
[1566.014478, "o", "    dictionary,\r\n"]
[1566.039075, "o", "    *,\r\n"]
[1566.063672, "o", "    gram=None,\r\n"]
[1566.088269, "o", "    cov=None,\r\n"]
[1566.112866, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1566.137463, "o", "    regularization=None,\r\n"]
[1566.16206, "o", "    copy_cov=True,\r\n"]
[1566.186657, "o", "    init=None,\r\n"]
[1566.211254, "o", "    max_iter=1000,\r\n"]
[1566.235851, "o", "    verbose=0,\r\n"]
[1566.260448, "o", "    positive=False,\r\n"]
[1566.285045, "o", "):\r\n"]
[1566.309642, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[1566.334239, "o", "\r\n"]
[1566.358836, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[1566.383433, "o", "\r\n"]
[1566.40803, "o", "    Parameters\r\n"]
[1566.432627, "o", "    ----------\r\n"]
[1566.457224, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[1566.481821, "o", "        Data matrix.\r\n"]
[1566.506418, "o", "\r\n"]
[1566.531015, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1566.555612, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1566.580209, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[1566.604806, "o", "\r\n"]
[1566.629403, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[1566.654, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[1566.678597, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[1566.703194, "o", "\r\n"]
[1566.727791, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[1566.752388, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[1566.776985, "o", "\r\n"]
[1566.801582, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1566.826179, "o", "            default='lasso_lars'\r\n"]
[1566.850776, "o", "        The algorithm used:\r\n"]
[1566.875373, "o", "\r\n"]
[1566.89997, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[1566.924567, "o", "          (`linear_model.lars_path`);\r\n"]
[1566.949164, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1566.973761, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1566.998358, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1567.022955, "o", "          the estimated components are sparse;\r\n"]
[1567.047552, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1567.072149, "o", "          solution;\r\n"]
[1567.096746, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1567.121343, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[1567.14594, "o", "\r\n"]
[1567.170537, "o", "    regularization : int or float, default=None\r\n"]
[1567.195134, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[1567.219731, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[1567.244328, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[1567.268925, "o", "\r\n"]
[1567.293522, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1567.318119, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[1567.342716, "o", "        `algorithm='lasso_cd'`.\r\n"]
[1567.367313, "o", "\r\n"]
[1567.39191, "o", "    max_iter : int, default=1000\r\n"]
[1567.416507, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1567.441104, "o", "        `'lasso_lars'`.\r\n"]
[1567.465701, "o", "\r\n"]
[1567.490299, "o", "    copy_cov : bool, default=True\r\n"]
[1567.514896, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1567.539493, "o", "        be overwritten.\r\n"]
[1567.56409, "o", "\r\n"]
[1567.588687, "o", "    verbose : int, default=0\r\n"]
[1567.613284, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[1567.637881, "o", "\r\n"]
[1567.662478, "o", "    positive: bool, default=False\r\n"]
[1567.687075, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[1567.711672, "o", "\r\n"]
[1567.736269, "o", "        .. versionadded:: 0.20\r\n"]
[1567.760866, "o", "\r\n"]
[1567.785463, "o", "    Returns\r\n"]
[1567.81006, "o", "    -------\r\n"]
[1567.834657, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[1567.859254, "o", "        The sparse codes.\r\n"]
[1567.883851, "o", "    \"\"\"\r\n"]
[1567.908448, "o", "    n_samples, n_features = X.shape\r\n"]
[1567.933045, "o", "    n_components = dictionary.shape[0]\r\n"]
[1567.957642, "o", "\r\n"]
[1567.982239, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[1568.006836, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1568.031433, "o", "        try:\r\n"]
[1568.05603, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1568.080627, "o", "\r\n"]
[1568.105224, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1568.129821, "o", "            # corrects the verbosity level.\r\n"]
[1568.154418, "o", "            lasso_lars = LassoLars(\r\n"]
[1568.179015, "o", "                alpha=alpha,\r\n"]
[1568.203612, "o", "                fit_intercept=False,\r\n"]
[1568.228209, "o", "                verbose=verbose,\r\n"]
[1568.252806, "o", "                precompute=gram,\r\n"]
[1568.277403, "o", "                fit_path=False,\r\n"]
[1568.302, "o", "                positive=positive,\r\n"]
[1568.326597, "o", "                max_iter=max_iter,\r\n"]
[1568.351194, "o", "            )\r\n"]
[1568.375791, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1568.400388, "o", "            new_code = lasso_lars.coef_\r\n"]
[1568.424985, "o", "        finally:\r\n"]
[1568.449582, "o", "            np.seterr(**err_mgt)\r\n"]
[1568.474179, "o", "\r\n"]
[1568.498776, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[1568.523373, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1568.54797, "o", "\r\n"]
[1568.572567, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[1568.597164, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[1568.621761, "o", "        # argument that we could pass in from Lasso.\r\n"]
[1568.646358, "o", "        clf = Lasso(\r\n"]
[1568.670955, "o", "            alpha=alpha,\r\n"]
[1568.695552, "o", "            fit_intercept=False,\r\n"]
[1568.720149, "o", "            precompute=gram,\r\n"]
[1568.744746, "o", "            max_iter=max_iter,\r\n"]
[1568.769343, "o", "            warm_start=True,\r\n"]
[1568.79394, "o", "            positive=positive,\r\n"]
[1568.818537, "o", "        )\r\n"]
[1568.843134, "o", "\r\n"]
[1568.867731, "o", "        if init is not None:\r\n"]
[1568.892328, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[1568.916925, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[1568.941522, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[1568.966119, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[1568.990716, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[1569.015313, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[1569.03991, "o", "                init = np.array(init)\r\n"]
[1569.064507, "o", "            clf.coef_ = init\r\n"]
[1569.089104, "o", "\r\n"]
[1569.113701, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[1569.138299, "o", "        new_code = clf.coef_\r\n"]
[1569.162896, "o", "\r\n"]
[1569.187493, "o", "    elif algorithm == \"lars\":\r\n"]
[1569.21209, "o", "        try:\r\n"]
[1569.236687, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1569.261284, "o", "\r\n"]
[1569.285881, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1569.310478, "o", "            # corrects the verbosity level.\r\n"]
[1569.335075, "o", "            lars = Lars(\r\n"]
[1569.359672, "o", "                fit_intercept=False,\r\n"]
[1569.384269, "o", "                verbose=verbose,\r\n"]
[1569.408866, "o", "                precompute=gram,\r\n"]
[1569.433463, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[1569.45806, "o", "                fit_path=False,\r\n"]
[1569.482657, "o", "            )\r\n"]
[1569.507254, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1569.531851, "o", "            new_code = lars.coef_\r\n"]
[1569.556448, "o", "        finally:\r\n"]
[1569.581045, "o", "            np.seterr(**err_mgt)\r\n"]
[1569.605642, "o", "\r\n"]
[1569.630239, "o", "    elif algorithm == \"threshold\":\r\n"]
[1569.654836, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[1569.679433, "o", "        if positive:\r\n"]
[1569.70403, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[1569.728627, "o", "\r\n"]
[1569.753224, "o", "    elif algorithm == \"omp\":\r\n"]
[1569.777821, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[1569.802418, "o", "            Gram=gram,\r\n"]
[1569.827015, "o", "            Xy=cov,\r\n"]
[1569.851612, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[1569.876209, "o", "            tol=None,\r\n"]
[1569.900806, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[1569.925403, "o", "            copy_Xy=copy_cov,\r\n"]
[1570.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1570.002, "i", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py | sed -n '1,200p'\r"]
[1570.004, "o", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py | sed -n '1,200p'\r\n"]
[1570.030597, "o", "\u001b[?2004l\r\n"]
[1570.055194, "o", "           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[1570.079791, "o", "\r\n"]
[1570.104388, "o", "    return_n_iter : bool, default=False\r\n"]
[1570.128985, "o", "        Whether or not to return the number of iterations.\r\n"]
[1570.153582, "o", "\r\n"]
[1570.178179, "o", "        .. deprecated:: 1.1\r\n"]
[1570.202776, "o", "           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n"]
[1570.227373, "o", "\r\n"]
[1570.25197, "o", "    positive_dict : bool, default=False\r\n"]
[1570.276567, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1570.301164, "o", "\r\n"]
[1570.325761, "o", "        .. versionadded:: 0.20\r\n"]
[1570.350358, "o", "\r\n"]
[1570.374955, "o", "    positive_code : bool, default=False\r\n"]
[1570.399552, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1570.424149, "o", "\r\n"]
[1570.448746, "o", "        .. versionadded:: 0.20\r\n"]
[1570.473343, "o", "\r\n"]
[1570.49794, "o", "    method_max_iter : int, default=1000\r\n"]
[1570.522537, "o", "        Maximum number of iterations to perform when solving the lasso problem.\r\n"]
[1570.547134, "o", "\r\n"]
[1570.571731, "o", "        .. versionadded:: 0.22\r\n"]
[1570.596328, "o", "\r\n"]
[1570.620925, "o", "    tol : float, default=1e-3\r\n"]
[1570.645522, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[1570.670119, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[1570.694716, "o", "\r\n"]
[1570.719313, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[1570.74391, "o", "        `tol` to 0.0.\r\n"]
[1570.768507, "o", "\r\n"]
[1570.793104, "o", "        .. versionadded:: 1.1\r\n"]
[1570.817701, "o", "\r\n"]
[1570.842299, "o", "    max_no_improvement : int, default=10\r\n"]
[1570.866896, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[1570.891493, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[1570.91609, "o", "        `max_iter` is not None.\r\n"]
[1570.940687, "o", "\r\n"]
[1570.965284, "o", "        To disable convergence detection based on cost function, set\r\n"]
[1570.989881, "o", "        `max_no_improvement` to None.\r\n"]
[1571.014478, "o", "\r\n"]
[1571.039075, "o", "        .. versionadded:: 1.1\r\n"]
[1571.063672, "o", "\r\n"]
[1571.088269, "o", "    Returns\r\n"]
[1571.112866, "o", "    -------\r\n"]
[1571.137463, "o", "    code : ndarray of shape (n_samples, n_components),\r\n"]
[1571.16206, "o", "        The sparse code (only returned if `return_code=True`).\r\n"]
[1571.186657, "o", "\r\n"]
[1571.211254, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[1571.235851, "o", "        The solutions to the dictionary learning problem.\r\n"]
[1571.260448, "o", "\r\n"]
[1571.285045, "o", "    n_iter : int\r\n"]
[1571.309642, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[1571.334239, "o", "        set to `True`.\r\n"]
[1571.358836, "o", "\r\n"]
[1571.383433, "o", "    See Also\r\n"]
[1571.40803, "o", "    --------\r\n"]
[1571.432627, "o", "    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n"]
[1571.457224, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1571.481821, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n"]
[1571.506418, "o", "        learning algorithm.\r\n"]
[1571.531015, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1571.555612, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1571.580209, "o", "    \"\"\"\r\n"]
[1571.604806, "o", "    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n"]
[1571.629403, "o", "    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n"]
[1571.654, "o", "        raise ValueError(\r\n"]
[1571.678597, "o", "            \"The following arguments are incompatible with 'max_iter': \"\r\n"]
[1571.703194, "o", "            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n"]
[1571.727791, "o", "        )\r\n"]
[1571.752388, "o", "\r\n"]
[1571.776985, "o", "    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n"]
[1571.801582, "o", "    return_inner_stats = _check_warn_deprecated(\r\n"]
[1571.826179, "o", "        return_inner_stats,\r\n"]
[1571.850776, "o", "        \"return_inner_stats\",\r\n"]
[1571.875373, "o", "        default=False,\r\n"]
[1571.89997, "o", "        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n"]
[1571.924567, "o", "    )\r\n"]
[1571.949164, "o", "    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n"]
[1571.973761, "o", "    return_n_iter = _check_warn_deprecated(\r\n"]
[1571.998358, "o", "        return_n_iter,\r\n"]
[1572.022955, "o", "        \"return_n_iter\",\r\n"]
[1572.047552, "o", "        default=False,\r\n"]
[1572.072149, "o", "        additional_message=(\r\n"]
[1572.096746, "o", "            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n"]
[1572.121343, "o", "            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n"]
[1572.14594, "o", "        ),\r\n"]
[1572.170537, "o", "    )\r\n"]
[1572.195134, "o", "\r\n"]
[1572.219731, "o", "    if max_iter is not None:\r\n"]
[1572.244328, "o", "        transform_algorithm = \"lasso_\" + method\r\n"]
[1572.268925, "o", "\r\n"]
[1572.293522, "o", "        est = MiniBatchDictionaryLearning(\r\n"]
[1572.318119, "o", "            n_components=n_components,\r\n"]
[1572.342716, "o", "            alpha=alpha,\r\n"]
[1572.367313, "o", "            n_iter=n_iter,\r\n"]
[1572.39191, "o", "            n_jobs=n_jobs,\r\n"]
[1572.416507, "o", "            fit_algorithm=method,\r\n"]
[1572.441104, "o", "            batch_size=batch_size,\r\n"]
[1572.465701, "o", "            shuffle=shuffle,\r\n"]
[1572.490299, "o", "            dict_init=dict_init,\r\n"]
[1572.514896, "o", "            random_state=random_state,\r\n"]
[1572.539493, "o", "            transform_algorithm=transform_algorithm,\r\n"]
[1572.56409, "o", "            transform_alpha=alpha,\r\n"]
[1572.588687, "o", "            positive_code=positive_code,\r\n"]
[1572.613284, "o", "            positive_dict=positive_dict,\r\n"]
[1572.637881, "o", "            transform_max_iter=method_max_iter,\r\n"]
[1572.662478, "o", "            verbose=verbose,\r\n"]
[1572.687075, "o", "            callback=callback,\r\n"]
[1572.711672, "o", "            tol=tol,\r\n"]
[1572.736269, "o", "            max_no_improvement=max_no_improvement,\r\n"]
[1572.760866, "o", "        ).fit(X)\r\n"]
[1572.785463, "o", "\r\n"]
[1572.81006, "o", "        if not return_code:\r\n"]
[1572.834657, "o", "            return est.components_\r\n"]
[1572.859254, "o", "        else:\r\n"]
[1572.883851, "o", "            code = est.transform(X)\r\n"]
[1572.908448, "o", "            return code, est.components_\r\n"]
[1572.933045, "o", "\r\n"]
[1572.957642, "o", "    # TODO(1.4) remove the whole old behavior\r\n"]
[1572.982239, "o", "    # Fallback to old behavior\r\n"]
[1573.006836, "o", "\r\n"]
[1573.031433, "o", "    n_iter = _check_warn_deprecated(\r\n"]
[1573.05603, "o", "        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n"]
[1573.080627, "o", "    )\r\n"]
[1573.105224, "o", "\r\n"]
[1573.129821, "o", "    if n_components is None:\r\n"]
[1573.154418, "o", "        n_components = X.shape[1]\r\n"]
[1573.179015, "o", "\r\n"]
[1573.203612, "o", "    if method not in (\"lars\", \"cd\"):\r\n"]
[1573.228209, "o", "        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n"]
[1573.252806, "o", "\r\n"]
[1573.277403, "o", "    _check_positive_coding(method, positive_code)\r\n"]
[1573.302, "o", "\r\n"]
[1573.326597, "o", "    method = \"lasso_\" + method\r\n"]
[1573.351194, "o", "\r\n"]
[1573.375791, "o", "    t0 = time.time()\r\n"]
[1573.400388, "o", "    n_samples, n_features = X.shape\r\n"]
[1573.424985, "o", "    # Avoid integer division problems\r\n"]
[1573.449582, "o", "    alpha = float(alpha)\r\n"]
[1573.474179, "o", "    random_state = check_random_state(random_state)\r\n"]
[1573.498776, "o", "\r\n"]
[1573.523373, "o", "    # Init V with SVD of X\r\n"]
[1573.54797, "o", "    if dict_init is not None:\r\n"]
[1573.572567, "o", "        dictionary = dict_init\r\n"]
[1573.597164, "o", "    else:\r\n"]
[1573.621761, "o", "        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n"]
[1573.646358, "o", "        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[1573.670955, "o", "    r = len(dictionary)\r\n"]
[1573.695552, "o", "    if n_components <= r:\r\n"]
[1573.720149, "o", "        dictionary = dictionary[:n_components, :]\r\n"]
[1573.744746, "o", "    else:\r\n"]
[1573.769343, "o", "        dictionary = np.r_[\r\n"]
[1573.79394, "o", "            dictionary,\r\n"]
[1573.818537, "o", "            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n"]
[1573.843134, "o", "        ]\r\n"]
[1573.867731, "o", "\r\n"]
[1573.892328, "o", "    if verbose == 1:\r\n"]
[1573.916925, "o", "        print(\"[dict_learning]\", end=\" \")\r\n"]
[1573.941522, "o", "\r\n"]
[1573.966119, "o", "    if shuffle:\r\n"]
[1573.990716, "o", "        X_train = X.copy()\r\n"]
[1574.015313, "o", "        random_state.shuffle(X_train)\r\n"]
[1574.03991, "o", "    else:\r\n"]
[1574.064507, "o", "        X_train = X\r\n"]
[1574.089104, "o", "\r\n"]
[1574.113701, "o", "    X_train = check_array(\r\n"]
[1574.138299, "o", "        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n"]
[1574.162896, "o", "    )\r\n"]
[1574.187493, "o", "\r\n"]
[1574.21209, "o", "    # Fortran-order dict better suited for the sparse coding which is the\r\n"]
[1574.236687, "o", "    # bottleneck of this algorithm.\r\n"]
[1574.261284, "o", "    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n"]
[1574.285881, "o", "    dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[1574.310478, "o", "\r\n"]
[1574.335075, "o", "    batches = gen_batches(n_samples, batch_size)\r\n"]
[1574.359672, "o", "    batches = itertools.cycle(batches)\r\n"]
[1574.384269, "o", "\r\n"]
[1574.408866, "o", "    # The covariance of the dictionary\r\n"]
[1574.433463, "o", "    if inner_stats is None:\r\n"]
[1574.45806, "o", "        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n"]
[1574.482657, "o", "        # The data approximation\r\n"]
[1574.507254, "o", "        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n"]
[1574.531851, "o", "    else:\r\n"]
[1574.556448, "o", "        A = inner_stats[0].copy()\r\n"]
[1574.581045, "o", "        B = inner_stats[1].copy()\r\n"]
[1574.605642, "o", "\r\n"]
[1574.630239, "o", "    # If n_iter is zero, we need to return zero.\r\n"]
[1574.654836, "o", "    ii = iter_offset - 1\r\n"]
[1574.679433, "o", "\r\n"]
[1574.70403, "o", "    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n"]
[1574.728627, "o", "        this_X = X_train[batch]\r\n"]
[1574.753224, "o", "        dt = time.time() - t0\r\n"]
[1574.777821, "o", "        if verbose == 1:\r\n"]
[1574.802418, "o", "            sys.stdout.write(\".\")\r\n"]
[1574.827015, "o", "            sys.stdout.flush()\r\n"]
[1574.851612, "o", "        elif verbose:\r\n"]
[1574.876209, "o", "            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n"]
[1574.900806, "o", "                print(\r\n"]
[1574.925403, "o", "                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n"]
[1575.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1575.002, "i", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py | sed -n '1,200p'\r"]
[1575.004, "o", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py | sed -n '1,200p'\r\n"]
[1575.030597, "o", "\u001b[?2004l\r\n"]
[1575.055194, "o", "\r\n"]
[1575.079791, "o", "    n_jobs : int or None, default=None\r\n"]
[1575.104388, "o", "        Number of parallel jobs to run.\r\n"]
[1575.128985, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1575.153582, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1575.178179, "o", "        for more details.\r\n"]
[1575.202776, "o", "\r\n"]
[1575.227373, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1575.25197, "o", "        Initial value for the code, for warm restart. Only used if `code_init`\r\n"]
[1575.276567, "o", "        and `dict_init` are not None.\r\n"]
[1575.301164, "o", "\r\n"]
[1575.325761, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1575.350358, "o", "        Initial values for the dictionary, for warm restart. Only used if\r\n"]
[1575.374955, "o", "        `code_init` and `dict_init` are not None.\r\n"]
[1575.399552, "o", "\r\n"]
[1575.424149, "o", "    callback : callable, default=None\r\n"]
[1575.448746, "o", "        Callable that gets invoked every five iterations.\r\n"]
[1575.473343, "o", "\r\n"]
[1575.49794, "o", "        .. versionadded:: 1.3\r\n"]
[1575.522537, "o", "\r\n"]
[1575.547134, "o", "    verbose : bool, default=False\r\n"]
[1575.571731, "o", "        To control the verbosity of the procedure.\r\n"]
[1575.596328, "o", "\r\n"]
[1575.620925, "o", "    split_sign : bool, default=False\r\n"]
[1575.645522, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[1575.670119, "o", "        its negative part and its positive part. This can improve the\r\n"]
[1575.694716, "o", "        performance of downstream classifiers.\r\n"]
[1575.719313, "o", "\r\n"]
[1575.74391, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1575.768507, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[1575.793104, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[1575.817701, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[1575.842299, "o", "        results across multiple function calls.\r\n"]
[1575.866896, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1575.891493, "o", "\r\n"]
[1575.91609, "o", "    positive_code : bool, default=False\r\n"]
[1575.940687, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1575.965284, "o", "\r\n"]
[1575.989881, "o", "        .. versionadded:: 0.20\r\n"]
[1576.014478, "o", "\r\n"]
[1576.039075, "o", "    positive_dict : bool, default=False\r\n"]
[1576.063672, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1576.088269, "o", "\r\n"]
[1576.112866, "o", "        .. versionadded:: 0.20\r\n"]
[1576.137463, "o", "\r\n"]
[1576.16206, "o", "    transform_max_iter : int, default=1000\r\n"]
[1576.186657, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1576.211254, "o", "        `'lasso_lars'`.\r\n"]
[1576.235851, "o", "\r\n"]
[1576.260448, "o", "        .. versionadded:: 0.22\r\n"]
[1576.285045, "o", "\r\n"]
[1576.309642, "o", "    Attributes\r\n"]
[1576.334239, "o", "    ----------\r\n"]
[1576.358836, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[1576.383433, "o", "        dictionary atoms extracted from the data\r\n"]
[1576.40803, "o", "\r\n"]
[1576.432627, "o", "    error_ : array\r\n"]
[1576.457224, "o", "        vector of errors at each iteration\r\n"]
[1576.481821, "o", "\r\n"]
[1576.506418, "o", "    n_features_in_ : int\r\n"]
[1576.531015, "o", "        Number of features seen during :term:`fit`.\r\n"]
[1576.555612, "o", "\r\n"]
[1576.580209, "o", "        .. versionadded:: 0.24\r\n"]
[1576.604806, "o", "\r\n"]
[1576.629403, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[1576.654, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[1576.678597, "o", "        has feature names that are all strings.\r\n"]
[1576.703194, "o", "\r\n"]
[1576.727791, "o", "        .. versionadded:: 1.0\r\n"]
[1576.752388, "o", "\r\n"]
[1576.776985, "o", "    n_iter_ : int\r\n"]
[1576.801582, "o", "        Number of iterations run.\r\n"]
[1576.826179, "o", "\r\n"]
[1576.850776, "o", "    See Also\r\n"]
[1576.875373, "o", "    --------\r\n"]
[1576.89997, "o", "    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n"]
[1576.924567, "o", "        dictionary learning algorithm.\r\n"]
[1576.949164, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1576.973761, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[1576.998358, "o", "        precomputed dictionary.\r\n"]
[1577.022955, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1577.047552, "o", "\r\n"]
[1577.072149, "o", "    References\r\n"]
[1577.096746, "o", "    ----------\r\n"]
[1577.121343, "o", "\r\n"]
[1577.14594, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[1577.170537, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[1577.195134, "o", "\r\n"]
[1577.219731, "o", "    Examples\r\n"]
[1577.244328, "o", "    --------\r\n"]
[1577.268925, "o", "    >>> import numpy as np\r\n"]
[1577.293522, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[1577.318119, "o", "    >>> from sklearn.decomposition import DictionaryLearning\r\n"]
[1577.342716, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[1577.367313, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[1577.39191, "o", "    ...     random_state=42,\r\n"]
[1577.416507, "o", "    ... )\r\n"]
[1577.441104, "o", "    >>> dict_learner = DictionaryLearning(\r\n"]
[1577.465701, "o", "    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n"]
[1577.490299, "o", "    ...     random_state=42,\r\n"]
[1577.514896, "o", "    ... )\r\n"]
[1577.539493, "o", "    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n"]
[1577.56409, "o", "\r\n"]
[1577.588687, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[1577.613284, "o", "\r\n"]
[1577.637881, "o", "    >>> np.mean(X_transformed == 0)\r\n"]
[1577.662478, "o", "    0.41...\r\n"]
[1577.687075, "o", "\r\n"]
[1577.711672, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[1577.736269, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[1577.760866, "o", "    the original signal:\r\n"]
[1577.785463, "o", "\r\n"]
[1577.81006, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[1577.834657, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[1577.859254, "o", "    0.07...\r\n"]
[1577.883851, "o", "    \"\"\"\r\n"]
[1577.908448, "o", "\r\n"]
[1577.933045, "o", "    _parameter_constraints: dict = {\r\n"]
[1577.957642, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1577.982239, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1578.006836, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1578.031433, "o", "        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1578.05603, "o", "        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[1578.080627, "o", "        \"transform_algorithm\": [\r\n"]
[1578.105224, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[1578.129821, "o", "        ],\r\n"]
[1578.154418, "o", "        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1578.179015, "o", "        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[1578.203612, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[1578.228209, "o", "        \"code_init\": [np.ndarray, None],\r\n"]
[1578.252806, "o", "        \"dict_init\": [np.ndarray, None],\r\n"]
[1578.277403, "o", "        \"callback\": [callable, None],\r\n"]
[1578.302, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[1578.326597, "o", "        \"split_sign\": [\"boolean\"],\r\n"]
[1578.351194, "o", "        \"random_state\": [\"random_state\"],\r\n"]
[1578.375791, "o", "        \"positive_code\": [\"boolean\"],\r\n"]
[1578.400388, "o", "        \"positive_dict\": [\"boolean\"],\r\n"]
[1578.424985, "o", "        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1578.449582, "o", "    }\r\n"]
[1578.474179, "o", "\r\n"]
[1578.498776, "o", "    def __init__(\r\n"]
[1578.523373, "o", "        self,\r\n"]
[1578.54797, "o", "        n_components=None,\r\n"]
[1578.572567, "o", "        *,\r\n"]
[1578.597164, "o", "        alpha=1,\r\n"]
[1578.621761, "o", "        max_iter=1000,\r\n"]
[1578.646358, "o", "        tol=1e-8,\r\n"]
[1578.670955, "o", "        fit_algorithm=\"lars\",\r\n"]
[1578.695552, "o", "        transform_algorithm=\"omp\",\r\n"]
[1578.720149, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[1578.744746, "o", "        transform_alpha=None,\r\n"]
[1578.769343, "o", "        n_jobs=None,\r\n"]
[1578.79394, "o", "        code_init=None,\r\n"]
[1578.818537, "o", "        dict_init=None,\r\n"]
[1578.843134, "o", "        callback=None,\r\n"]
[1578.867731, "o", "        verbose=False,\r\n"]
[1578.892328, "o", "        split_sign=False,\r\n"]
[1578.916925, "o", "        random_state=None,\r\n"]
[1578.941522, "o", "        positive_code=False,\r\n"]
[1578.966119, "o", "        positive_dict=False,\r\n"]
[1578.990716, "o", "        transform_max_iter=1000,\r\n"]
[1579.015313, "o", "    ):\r\n"]
[1579.03991, "o", "        super().__init__(\r\n"]
[1579.064507, "o", "            transform_algorithm,\r\n"]
[1579.089104, "o", "            transform_n_nonzero_coefs,\r\n"]
[1579.113701, "o", "            transform_alpha,\r\n"]
[1579.138299, "o", "            split_sign,\r\n"]
[1579.162896, "o", "            n_jobs,\r\n"]
[1579.187493, "o", "            positive_code,\r\n"]
[1579.21209, "o", "            transform_max_iter,\r\n"]
[1579.236687, "o", "        )\r\n"]
[1579.261284, "o", "        self.n_components = n_components\r\n"]
[1579.285881, "o", "        self.alpha = alpha\r\n"]
[1579.310478, "o", "        self.max_iter = max_iter\r\n"]
[1579.335075, "o", "        self.tol = tol\r\n"]
[1579.359672, "o", "        self.fit_algorithm = fit_algorithm\r\n"]
[1579.384269, "o", "        self.code_init = code_init\r\n"]
[1579.408866, "o", "        self.dict_init = dict_init\r\n"]
[1579.433463, "o", "        self.callback = callback\r\n"]
[1579.45806, "o", "        self.verbose = verbose\r\n"]
[1579.482657, "o", "        self.random_state = random_state\r\n"]
[1579.507254, "o", "        self.positive_dict = positive_dict\r\n"]
[1579.531851, "o", "\r\n"]
[1579.556448, "o", "    def fit(self, X, y=None):\r\n"]
[1579.581045, "o", "        \"\"\"Fit the model from data in X.\r\n"]
[1579.605642, "o", "\r\n"]
[1579.630239, "o", "        Parameters\r\n"]
[1579.654836, "o", "        ----------\r\n"]
[1579.679433, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[1579.70403, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1579.728627, "o", "            and `n_features` is the number of features.\r\n"]
[1579.753224, "o", "\r\n"]
[1579.777821, "o", "        y : Ignored\r\n"]
[1579.802418, "o", "            Not used, present for API consistency by convention.\r\n"]
[1579.827015, "o", "\r\n"]
[1579.851612, "o", "        Returns\r\n"]
[1579.876209, "o", "        -------\r\n"]
[1579.900806, "o", "        self : object\r\n"]
[1579.925403, "o", "            Returns the instance itself.\r\n"]
[1580.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1580.002, "i", "cd /workspace/repo\r"]
[1580.004, "o", "cd /workspace/repo\r\n"]
[1580.006, "o", "\u001b[?2004l\r\n"]
[1585.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1585.002, "i", "grep -R \"def _minibatch_step\\|_minibatch_step\\(\" -n sklearn || true\r"]
[1585.004, "o", "grep -R \"def _minibatch_step\\|_minibatch_step\\(\" -n sklearn || true\r\n"]
[1587.478, "o", "\u001b[?2004l\r\n"]
[1590.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1590.002, "i", "grep -R \"_sparse_encode_precomputed\\s*\\(|sparse_encode\\s*\\(\" -n sklearn/decomposition || true\r"]
[1590.004, "o", "grep -R \"_sparse_encode_precomputed\\s*\\(|sparse_encode\\s*\\(\" -n sklearn/decomposition || true\r\n"]
[1592.478, "o", "\u001b[?2004l\r\n"]
[1595.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1595.002, "i", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r"]
[1595.004, "o", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r\n"]
[1595.016, "o", "\u001b[?2004l\r\n"]
[1595.026, "o", "\"\"\" Dictionary learning.\r\n"]
[1595.036, "o", "\"\"\"\r\n"]
[1595.046, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[1595.056, "o", "# License: BSD 3 clause\r\n"]
[1595.066, "o", "\r\n"]
[1595.076, "o", "import itertools\r\n"]
[1595.086, "o", "import sys\r\n"]
[1595.096, "o", "import time\r\n"]
[1595.106, "o", "import warnings\r\n"]
[1595.116, "o", "from math import ceil\r\n"]
[1595.126, "o", "from numbers import Integral, Real\r\n"]
[1595.136, "o", "\r\n"]
[1595.146, "o", "import numpy as np\r\n"]
[1595.156, "o", "from joblib import effective_n_jobs\r\n"]
[1595.166, "o", "from scipy import linalg\r\n"]
[1595.176, "o", "\r\n"]
[1595.186, "o", "from ..base import (\r\n"]
[1595.196, "o", "    BaseEstimator,\r\n"]
[1595.206, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[1595.216, "o", "    TransformerMixin,\r\n"]
[1595.226, "o", "    _fit_context,\r\n"]
[1595.236, "o", ")\r\n"]
[1595.246, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[1595.256, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[1595.266, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[1595.276, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[1595.286, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[1595.296, "o", "from ..utils.validation import check_is_fitted\r\n"]
[1595.306, "o", "\r\n"]
[1595.316, "o", "\r\n"]
[1595.326, "o", "def _check_positive_coding(method, positive):\r\n"]
[1595.336, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[1595.346, "o", "        raise ValueError(\r\n"]
[1595.356, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[1595.366, "o", "        )\r\n"]
[1595.376, "o", "\r\n"]
[1595.386, "o", "\r\n"]
[1595.396, "o", "def _sparse_encode_precomputed(\r\n"]
[1595.406, "o", "    X,\r\n"]
[1595.416, "o", "    dictionary,\r\n"]
[1595.426, "o", "    *,\r\n"]
[1595.436, "o", "    gram=None,\r\n"]
[1595.446, "o", "    cov=None,\r\n"]
[1595.456, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1595.466, "o", "    regularization=None,\r\n"]
[1595.476, "o", "    copy_cov=True,\r\n"]
[1595.486, "o", "    init=None,\r\n"]
[1595.496, "o", "    max_iter=1000,\r\n"]
[1595.506, "o", "    verbose=0,\r\n"]
[1595.516, "o", "    positive=False,\r\n"]
[1595.526, "o", "):\r\n"]
[1595.536, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[1595.546, "o", "\r\n"]
[1595.556, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[1595.566, "o", "\r\n"]
[1595.576, "o", "    Parameters\r\n"]
[1595.586, "o", "    ----------\r\n"]
[1595.596, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[1595.606, "o", "        Data matrix.\r\n"]
[1595.616, "o", "\r\n"]
[1595.626, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1595.636, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1595.646, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[1595.656, "o", "\r\n"]
[1595.666, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[1595.676, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[1595.686, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[1595.696, "o", "\r\n"]
[1595.706, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[1595.716, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[1595.726, "o", "\r\n"]
[1595.736, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1595.746, "o", "            default='lasso_lars'\r\n"]
[1595.756, "o", "        The algorithm used:\r\n"]
[1595.766, "o", "\r\n"]
[1595.776, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[1595.786, "o", "          (`linear_model.lars_path`);\r\n"]
[1595.796, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1595.806, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1595.816, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1595.826, "o", "          the estimated components are sparse;\r\n"]
[1595.836, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1595.846, "o", "          solution;\r\n"]
[1595.856, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1595.866, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[1595.876, "o", "\r\n"]
[1595.886, "o", "    regularization : int or float, default=None\r\n"]
[1595.896, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[1595.906, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[1595.916, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[1595.926, "o", "\r\n"]
[1595.936, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1595.946, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[1595.956, "o", "        `algorithm='lasso_cd'`.\r\n"]
[1595.966, "o", "\r\n"]
[1595.976, "o", "    max_iter : int, default=1000\r\n"]
[1595.986, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1595.996, "o", "        `'lasso_lars'`.\r\n"]
[1596.006, "o", "\r\n"]
[1596.016, "o", "    copy_cov : bool, default=True\r\n"]
[1596.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1596.036, "o", "        be overwritten.\r\n"]
[1596.046, "o", "\r\n"]
[1596.056, "o", "    verbose : int, default=0\r\n"]
[1596.066, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[1596.076, "o", "\r\n"]
[1596.086, "o", "    positive: bool, default=False\r\n"]
[1596.096, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[1596.106, "o", "\r\n"]
[1596.116, "o", "        .. versionadded:: 0.20\r\n"]
[1596.126, "o", "\r\n"]
[1596.136, "o", "    Returns\r\n"]
[1596.146, "o", "    -------\r\n"]
[1596.156, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[1596.166, "o", "        The sparse codes.\r\n"]
[1596.176, "o", "    \"\"\"\r\n"]
[1596.186, "o", "    n_samples, n_features = X.shape\r\n"]
[1596.196, "o", "    n_components = dictionary.shape[0]\r\n"]
[1596.206, "o", "\r\n"]
[1596.216, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[1596.226, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1596.236, "o", "        try:\r\n"]
[1596.246, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1596.256, "o", "\r\n"]
[1596.266, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1596.276, "o", "            # corrects the verbosity level.\r\n"]
[1596.286, "o", "            lasso_lars = LassoLars(\r\n"]
[1596.296, "o", "                alpha=alpha,\r\n"]
[1596.306, "o", "                fit_intercept=False,\r\n"]
[1596.316, "o", "                verbose=verbose,\r\n"]
[1596.326, "o", "                precompute=gram,\r\n"]
[1596.336, "o", "                fit_path=False,\r\n"]
[1596.346, "o", "                positive=positive,\r\n"]
[1596.356, "o", "                max_iter=max_iter,\r\n"]
[1596.366, "o", "            )\r\n"]
[1596.376, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1596.386, "o", "            new_code = lasso_lars.coef_\r\n"]
[1596.396, "o", "        finally:\r\n"]
[1596.406, "o", "            np.seterr(**err_mgt)\r\n"]
[1596.416, "o", "\r\n"]
[1596.426, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[1596.436, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1596.446, "o", "\r\n"]
[1596.456, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[1596.466, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[1596.476, "o", "        # argument that we could pass in from Lasso.\r\n"]
[1596.486, "o", "        clf = Lasso(\r\n"]
[1596.496, "o", "            alpha=alpha,\r\n"]
[1596.506, "o", "            fit_intercept=False,\r\n"]
[1596.516, "o", "            precompute=gram,\r\n"]
[1596.526, "o", "            max_iter=max_iter,\r\n"]
[1596.536, "o", "            warm_start=True,\r\n"]
[1596.546, "o", "            positive=positive,\r\n"]
[1596.556, "o", "        )\r\n"]
[1596.566, "o", "\r\n"]
[1596.576, "o", "        if init is not None:\r\n"]
[1596.586, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[1596.596, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[1596.606, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[1596.616, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[1596.626, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[1596.636, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[1596.646, "o", "                init = np.array(init)\r\n"]
[1596.656, "o", "            clf.coef_ = init\r\n"]
[1596.666, "o", "\r\n"]
[1596.676, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[1596.686, "o", "        new_code = clf.coef_\r\n"]
[1596.696, "o", "\r\n"]
[1596.706, "o", "    elif algorithm == \"lars\":\r\n"]
[1596.716, "o", "        try:\r\n"]
[1596.726, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1596.736, "o", "\r\n"]
[1596.746, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1596.756, "o", "            # corrects the verbosity level.\r\n"]
[1596.766, "o", "            lars = Lars(\r\n"]
[1596.776, "o", "                fit_intercept=False,\r\n"]
[1596.786, "o", "                verbose=verbose,\r\n"]
[1596.796, "o", "                precompute=gram,\r\n"]
[1596.806, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[1596.816, "o", "                fit_path=False,\r\n"]
[1596.826, "o", "            )\r\n"]
[1596.836, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1596.846, "o", "            new_code = lars.coef_\r\n"]
[1596.856, "o", "        finally:\r\n"]
[1596.866, "o", "            np.seterr(**err_mgt)\r\n"]
[1596.876, "o", "\r\n"]
[1596.886, "o", "    elif algorithm == \"threshold\":\r\n"]
[1596.896, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[1596.906, "o", "        if positive:\r\n"]
[1596.916, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[1596.926, "o", "\r\n"]
[1596.936, "o", "    elif algorithm == \"omp\":\r\n"]
[1596.946, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[1596.956, "o", "            Gram=gram,\r\n"]
[1596.966, "o", "            Xy=cov,\r\n"]
[1596.976, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[1596.986, "o", "            tol=None,\r\n"]
[1596.996, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[1597.006, "o", "            copy_Xy=copy_cov,\r\n"]
[1597.016, "o", "        ).T\r\n"]
[1597.026, "o", "\r\n"]
[1597.036, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[1597.046, "o", "\r\n"]
[1597.056, "o", "\r\n"]
[1597.066, "o", "@validate_params(\r\n"]
[1597.076, "o", "    {\r\n"]
[1597.086, "o", "        \"X\": [\"array-like\"],\r\n"]
[1597.096, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[1597.106, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[1597.116, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[1597.126, "o", "        \"algorithm\": [\r\n"]
[1597.136, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[1597.146, "o", "        ],\r\n"]
[1597.156, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1597.166, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[1597.176, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[1597.186, "o", "        \"init\": [\"array-like\", None],\r\n"]
[1597.196, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1597.206, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[1597.216, "o", "        \"check_input\": [\"boolean\"],\r\n"]
[1597.226, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[1597.236, "o", "        \"positive\": [\"boolean\"],\r\n"]
[1597.246, "o", "    },\r\n"]
[1597.256, "o", "    prefer_skip_nested_validation=True,\r\n"]
[1597.266, "o", ")\r\n"]
[1597.276, "o", "# XXX : could be moved to the linear_model module\r\n"]
[1597.286, "o", "def sparse_encode(\r\n"]
[1597.296, "o", "    X,\r\n"]
[1597.306, "o", "    dictionary,\r\n"]
[1597.316, "o", "    *,\r\n"]
[1597.326, "o", "    gram=None,\r\n"]
[1597.336, "o", "    cov=None,\r\n"]
[1597.346, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1597.356, "o", "    n_nonzero_coefs=None,\r\n"]
[1597.366, "o", "    alpha=None,\r\n"]
[1597.376, "o", "    copy_cov=True,\r\n"]
[1597.386, "o", "    init=None,\r\n"]
[1597.396, "o", "    max_iter=1000,\r\n"]
[1597.406, "o", "    n_jobs=None,\r\n"]
[1597.416, "o", "    check_input=True,\r\n"]
[1597.426, "o", "    verbose=0,\r\n"]
[1597.436, "o", "    positive=False,\r\n"]
[1597.446, "o", "):\r\n"]
[1597.456, "o", "    \"\"\"Sparse coding.\r\n"]
[1597.466, "o", "\r\n"]
[1597.476, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[1597.486, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[1597.496, "o", "\r\n"]
[1597.506, "o", "        X ~= code * dictionary\r\n"]
[1597.516, "o", "\r\n"]
[1597.526, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[1597.536, "o", "\r\n"]
[1597.546, "o", "    Parameters\r\n"]
[1597.556, "o", "    ----------\r\n"]
[1597.566, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[1597.576, "o", "        Data matrix.\r\n"]
[1597.586, "o", "\r\n"]
[1597.596, "o", "    dictionary : array-like of shape (n_components, n_features)\r\n"]
[1597.606, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1597.616, "o", "        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[1597.626, "o", "        output.\r\n"]
[1597.636, "o", "\r\n"]
[1597.646, "o", "    gram : array-like of shape (n_components, n_components), default=None\r\n"]
[1597.656, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n"]
[1597.666, "o", "\r\n"]
[1597.676, "o", "    cov : array-like of shape (n_components, n_samples), default=None\r\n"]
[1597.686, "o", "        Precomputed covariance, `dictionary' * X`.\r\n"]
[1597.696, "o", "\r\n"]
[1597.706, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1597.716, "o", "            default='lasso_lars'\r\n"]
[1597.726, "o", "        The algorithm used:\r\n"]
[1597.736, "o", "\r\n"]
[1597.746, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[1597.756, "o", "          (`linear_model.lars_path`);\r\n"]
[1597.766, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1597.776, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1597.786, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1597.796, "o", "          the estimated components are sparse;\r\n"]
[1597.806, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1597.816, "o", "          solution;\r\n"]
[1597.826, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1597.836, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[1597.846, "o", "\r\n"]
[1597.856, "o", "    n_nonzero_coefs : int, default=None\r\n"]
[1597.866, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1597.876, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[1597.886, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[1597.896, "o", "        `n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1597.906, "o", "\r\n"]
[1597.916, "o", "    alpha : float, default=None\r\n"]
[1597.926, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1597.936, "o", "        penalty applied to the L1 norm.\r\n"]
[1597.946, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1597.956, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1597.966, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[1597.976, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[1597.986, "o", "        `n_nonzero_coefs`.\r\n"]
[1597.996, "o", "        If `None`, default to 1.\r\n"]
[1598.006, "o", "\r\n"]
[1598.016, "o", "    copy_cov : bool, default=True\r\n"]
[1598.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1598.036, "o", "        be overwritten.\r\n"]
[1598.046, "o", "\r\n"]
[1598.056, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1598.066, "o", "        Initialization value of the sparse codes. Only used if\r\n"]
[1598.076, "o", "        `algorithm='lasso_cd'`.\r\n"]
[1598.086, "o", "\r\n"]
[1598.096, "o", "    max_iter : int, default=1000\r\n"]
[1598.106, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1598.116, "o", "        `'lasso_lars'`.\r\n"]
[1598.126, "o", "\r\n"]
[1598.136, "o", "    n_jobs : int, default=None\r\n"]
[1598.146, "o", "        Number of parallel jobs to run.\r\n"]
[1598.156, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1598.166, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1598.176, "o", "        for more details.\r\n"]
[1598.186, "o", "\r\n"]
[1598.196, "o", "    check_input : bool, default=True\r\n"]
[1598.206, "o", "        If `False`, the input arrays X and dictionary will not be checked.\r\n"]
[1598.216, "o", "\r\n"]
[1598.226, "o", "    verbose : int, default=0\r\n"]
[1598.236, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[1598.246, "o", "\r\n"]
[1598.256, "o", "    positive : bool, default=False\r\n"]
[1598.266, "o", "        Whether to enforce positivity when finding the encoding.\r\n"]
[1598.276, "o", "\r\n"]
[1598.286, "o", "        .. versionadded:: 0.20\r\n"]
[1598.296, "o", "\r\n"]
[1598.306, "o", "    Returns\r\n"]
[1598.316, "o", "    -------\r\n"]
[1598.326, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1598.336, "o", "        The sparse codes.\r\n"]
[1598.346, "o", "\r\n"]
[1598.356, "o", "    See Also\r\n"]
[1598.366, "o", "    --------\r\n"]
[1598.376, "o", "    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n"]
[1598.386, "o", "        path using LARS algorithm.\r\n"]
[1598.396, "o", "    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n"]
[1598.406, "o", "    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n"]
[1598.416, "o", "    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n"]
[1598.426, "o", "        dictionary.\r\n"]
[1598.436, "o", "    \"\"\"\r\n"]
[1598.446, "o", "    if check_input:\r\n"]
[1598.456, "o", "        if algorithm == \"lasso_cd\":\r\n"]
[1598.466, "o", "            dictionary = check_array(\r\n"]
[1598.476, "o", "                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n"]
[1598.486, "o", "            )\r\n"]
[1598.496, "o", "            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n"]
[1598.506, "o", "        else:\r\n"]
[1598.516, "o", "            dictionary = check_array(dictionary)\r\n"]
[1598.526, "o", "            X = check_array(X)\r\n"]
[1598.536, "o", "\r\n"]
[1598.546, "o", "    if dictionary.shape[1] != X.shape[1]:\r\n"]
[1598.556, "o", "        raise ValueError(\r\n"]
[1598.566, "o", "            \"Dictionary and X have different numbers of features:\"\r\n"]
[1598.576, "o", "            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n"]
[1598.586, "o", "        )\r\n"]
[1598.596, "o", "\r\n"]
[1598.606, "o", "    _check_positive_coding(algorithm, positive)\r\n"]
[1598.616, "o", "\r\n"]
[1598.626, "o", "    return _sparse_encode(\r\n"]
[1598.636, "o", "        X,\r\n"]
[1598.646, "o", "        dictionary,\r\n"]
[1598.656, "o", "        gram=gram,\r\n"]
[1598.666, "o", "        cov=cov,\r\n"]
[1598.676, "o", "        algorithm=algorithm,\r\n"]
[1598.686, "o", "        n_nonzero_coefs=n_nonzero_coefs,\r\n"]
[1598.696, "o", "        alpha=alpha,\r\n"]
[1598.706, "o", "        copy_cov=copy_cov,\r\n"]
[1598.716, "o", "        init=init,\r\n"]
[1598.726, "o", "        max_iter=max_iter,\r\n"]
[1598.736, "o", "        n_jobs=n_jobs,\r\n"]
[1598.746, "o", "        verbose=verbose,\r\n"]
[1598.756, "o", "        positive=positive,\r\n"]
[1598.766, "o", "    )\r\n"]
[1598.776, "o", "\r\n"]
[1598.786, "o", "\r\n"]
[1598.796, "o", "def _sparse_encode(\r\n"]
[1598.806, "o", "    X,\r\n"]
[1598.816, "o", "    dictionary,\r\n"]
[1598.826, "o", "    *,\r\n"]
[1598.836, "o", "    gram=None,\r\n"]
[1598.846, "o", "    cov=None,\r\n"]
[1598.856, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1598.866, "o", "    n_nonzero_coefs=None,\r\n"]
[1598.876, "o", "    alpha=None,\r\n"]
[1598.886, "o", "    copy_cov=True,\r\n"]
[1598.896, "o", "    init=None,\r\n"]
[1598.906, "o", "    max_iter=1000,\r\n"]
[1598.916, "o", "    n_jobs=None,\r\n"]
[1598.926, "o", "    verbose=0,\r\n"]
[1598.936, "o", "    positive=False,\r\n"]
[1598.946, "o", "):\r\n"]
[1598.956, "o", "    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[1598.966, "o", "\r\n"]
[1598.976, "o", "    n_samples, n_features = X.shape\r\n"]
[1598.986, "o", "    n_components = dictionary.shape[0]\r\n"]
[1598.996, "o", "\r\n"]
[1599.006, "o", "    if algorithm in (\"lars\", \"omp\"):\r\n"]
[1599.016, "o", "        regularization = n_nonzero_coefs\r\n"]
[1599.026, "o", "        if regularization is None:\r\n"]
[1599.036, "o", "            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[1599.046, "o", "    else:\r\n"]
[1599.056, "o", "        regularization = alpha\r\n"]
[1599.066, "o", "        if regularization is None:\r\n"]
[1599.076, "o", "            regularization = 1.0\r\n"]
[1599.086, "o", "\r\n"]
[1599.096, "o", "    if gram is None and algorithm != \"threshold\":\r\n"]
[1599.106, "o", "        gram = np.dot(dictionary, dictionary.T)\r\n"]
[1599.116, "o", "\r\n"]
[1599.126, "o", "    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[1599.136, "o", "        copy_cov = False\r\n"]
[1599.146, "o", "        cov = np.dot(dictionary, X.T)\r\n"]
[1599.156, "o", "\r\n"]
[1599.166, "o", "    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[1599.176, "o", "        code = _sparse_encode_precomputed(\r\n"]
[1599.186, "o", "            X,\r\n"]
[1599.196, "o", "            dictionary,\r\n"]
[1599.206, "o", "            gram=gram,\r\n"]
[1599.216, "o", "            cov=cov,\r\n"]
[1599.226, "o", "            algorithm=algorithm,\r\n"]
[1599.236, "o", "            regularization=regularization,\r\n"]
[1599.246, "o", "            copy_cov=copy_cov,\r\n"]
[1599.256, "o", "            init=init,\r\n"]
[1599.266, "o", "            max_iter=max_iter,\r\n"]
[1599.276, "o", "            verbose=verbose,\r\n"]
[1599.286, "o", "            positive=positive,\r\n"]
[1599.296, "o", "        )\r\n"]
[1599.306, "o", "        return code\r\n"]
[1599.316, "o", "\r\n"]
[1599.326, "o", "    # Enter parallel code block\r\n"]
[1599.336, "o", "    n_samples = X.shape[0]\r\n"]
[1599.346, "o", "    n_components = dictionary.shape[0]\r\n"]
[1599.356, "o", "    code = np.empty((n_samples, n_components))\r\n"]
[1599.366, "o", "    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[1599.376, "o", "\r\n"]
[1599.386, "o", "    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[1599.396, "o", "        delayed(_sparse_encode_precomputed)(\r\n"]
[1599.406, "o", "            X[this_slice],\r\n"]
[1599.416, "o", "            dictionary,\r\n"]
[1599.426, "o", "            gram=gram,\r\n"]
[1599.436, "o", "            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[1599.446, "o", "            algorithm=algorithm,\r\n"]
[1599.456, "o", "            regularization=regularization,\r\n"]
[1599.466, "o", "            copy_cov=copy_cov,\r\n"]
[1599.476, "o", "            init=init[this_slice] if init is not None else None,\r\n"]
[1599.486, "o", "            max_iter=max_iter,\r\n"]
[1599.496, "o", "            verbose=verbose,\r\n"]
[1599.506, "o", "            positive=positive,\r\n"]
[1599.516, "o", "        )\r\n"]
[1599.526, "o", "        for this_slice in slices\r\n"]
[1599.536, "o", "    )\r\n"]
[1599.546, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[1599.556, "o", "        code[this_slice] = this_view\r\n"]
[1599.566, "o", "    return code\r\n"]
[1599.576, "o", "\r\n"]
[1599.586, "o", "\r\n"]
[1599.596, "o", "def _update_dict(\r\n"]
[1599.606, "o", "    dictionary,\r\n"]
[1599.616, "o", "    Y,\r\n"]
[1599.626, "o", "    code,\r\n"]
[1599.636, "o", "    A=None,\r\n"]
[1599.646, "o", "    B=None,\r\n"]
[1599.656, "o", "    verbose=False,\r\n"]
[1599.666, "o", "    random_state=None,\r\n"]
[1599.676, "o", "    positive=False,\r\n"]
[1599.686, "o", "):\r\n"]
[1599.696, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[1599.706, "o", "\r\n"]
[1599.716, "o", "    Parameters\r\n"]
[1599.726, "o", "    ----------\r\n"]
[1599.736, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1599.746, "o", "        Value of the dictionary at the previous iteration.\r\n"]
[1599.756, "o", "\r\n"]
[1599.766, "o", "    Y : ndarray of shape (n_samples, n_features)\r\n"]
[1599.776, "o", "        Data matrix.\r\n"]
[1599.786, "o", "\r\n"]
[1599.796, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1599.806, "o", "        Sparse coding of the data against which to optimize the dictionary.\r\n"]
[1599.816, "o", "\r\n"]
[1599.826, "o", "    A : ndarray of shape (n_components, n_components), default=None\r\n"]
[1599.836, "o", "        Together with `B`, sufficient stats of the online model to update the\r\n"]
[1599.846, "o", "        dictionary.\r\n"]
[1599.856, "o", "\r\n"]
[1599.866, "o", "    B : ndarray of shape (n_features, n_components), default=None\r\n"]
[1599.876, "o", "        Together with `A`, sufficient stats of the online model to update the\r\n"]
[1599.886, "o", "        dictionary.\r\n"]
[1599.896, "o", "\r\n"]
[1599.906, "o", "    verbose: bool, default=False\r\n"]
[1599.916, "o", "        Degree of output the procedure will print.\r\n"]
[1599.926, "o", "\r\n"]
[1599.936, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1600.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1600.002, "i", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r"]
[1600.004, "o", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r\n"]
[1600.016, "o", "\u001b[?2004l\r\n"]
[1600.026, "o", "           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[1600.036, "o", "\r\n"]
[1600.046, "o", "    return_n_iter : bool, default=False\r\n"]
[1600.056, "o", "        Whether or not to return the number of iterations.\r\n"]
[1600.066, "o", "\r\n"]
[1600.076, "o", "        .. deprecated:: 1.1\r\n"]
[1600.086, "o", "           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n"]
[1600.096, "o", "\r\n"]
[1600.106, "o", "    positive_dict : bool, default=False\r\n"]
[1600.116, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1600.126, "o", "\r\n"]
[1600.136, "o", "        .. versionadded:: 0.20\r\n"]
[1600.146, "o", "\r\n"]
[1600.156, "o", "    positive_code : bool, default=False\r\n"]
[1600.166, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1600.176, "o", "\r\n"]
[1600.186, "o", "        .. versionadded:: 0.20\r\n"]
[1600.196, "o", "\r\n"]
[1600.206, "o", "    method_max_iter : int, default=1000\r\n"]
[1600.216, "o", "        Maximum number of iterations to perform when solving the lasso problem.\r\n"]
[1600.226, "o", "\r\n"]
[1600.236, "o", "        .. versionadded:: 0.22\r\n"]
[1600.246, "o", "\r\n"]
[1600.256, "o", "    tol : float, default=1e-3\r\n"]
[1600.266, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[1600.276, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[1600.286, "o", "\r\n"]
[1600.296, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[1600.306, "o", "        `tol` to 0.0.\r\n"]
[1600.316, "o", "\r\n"]
[1600.326, "o", "        .. versionadded:: 1.1\r\n"]
[1600.336, "o", "\r\n"]
[1600.346, "o", "    max_no_improvement : int, default=10\r\n"]
[1600.356, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[1600.366, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[1600.376, "o", "        `max_iter` is not None.\r\n"]
[1600.386, "o", "\r\n"]
[1600.396, "o", "        To disable convergence detection based on cost function, set\r\n"]
[1600.406, "o", "        `max_no_improvement` to None.\r\n"]
[1600.416, "o", "\r\n"]
[1600.426, "o", "        .. versionadded:: 1.1\r\n"]
[1600.436, "o", "\r\n"]
[1600.446, "o", "    Returns\r\n"]
[1600.456, "o", "    -------\r\n"]
[1600.466, "o", "    code : ndarray of shape (n_samples, n_components),\r\n"]
[1600.476, "o", "        The sparse code (only returned if `return_code=True`).\r\n"]
[1600.486, "o", "\r\n"]
[1600.496, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[1600.506, "o", "        The solutions to the dictionary learning problem.\r\n"]
[1600.516, "o", "\r\n"]
[1600.526, "o", "    n_iter : int\r\n"]
[1600.536, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[1600.546, "o", "        set to `True`.\r\n"]
[1600.556, "o", "\r\n"]
[1600.566, "o", "    See Also\r\n"]
[1600.576, "o", "    --------\r\n"]
[1600.586, "o", "    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n"]
[1600.596, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1600.606, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n"]
[1600.616, "o", "        learning algorithm.\r\n"]
[1600.626, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1600.636, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1600.646, "o", "    \"\"\"\r\n"]
[1600.656, "o", "    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n"]
[1600.666, "o", "    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n"]
[1600.676, "o", "        raise ValueError(\r\n"]
[1600.686, "o", "            \"The following arguments are incompatible with 'max_iter': \"\r\n"]
[1600.696, "o", "            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n"]
[1600.706, "o", "        )\r\n"]
[1600.716, "o", "\r\n"]
[1600.726, "o", "    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n"]
[1600.736, "o", "    return_inner_stats = _check_warn_deprecated(\r\n"]
[1600.746, "o", "        return_inner_stats,\r\n"]
[1600.756, "o", "        \"return_inner_stats\",\r\n"]
[1600.766, "o", "        default=False,\r\n"]
[1600.776, "o", "        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n"]
[1600.786, "o", "    )\r\n"]
[1600.796, "o", "    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n"]
[1600.806, "o", "    return_n_iter = _check_warn_deprecated(\r\n"]
[1600.816, "o", "        return_n_iter,\r\n"]
[1600.826, "o", "        \"return_n_iter\",\r\n"]
[1600.836, "o", "        default=False,\r\n"]
[1600.846, "o", "        additional_message=(\r\n"]
[1600.856, "o", "            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n"]
[1600.866, "o", "            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n"]
[1600.876, "o", "        ),\r\n"]
[1600.886, "o", "    )\r\n"]
[1600.896, "o", "\r\n"]
[1600.906, "o", "    if max_iter is not None:\r\n"]
[1600.916, "o", "        transform_algorithm = \"lasso_\" + method\r\n"]
[1600.926, "o", "\r\n"]
[1600.936, "o", "        est = MiniBatchDictionaryLearning(\r\n"]
[1600.946, "o", "            n_components=n_components,\r\n"]
[1600.956, "o", "            alpha=alpha,\r\n"]
[1600.966, "o", "            n_iter=n_iter,\r\n"]
[1600.976, "o", "            n_jobs=n_jobs,\r\n"]
[1600.986, "o", "            fit_algorithm=method,\r\n"]
[1600.996, "o", "            batch_size=batch_size,\r\n"]
[1601.006, "o", "            shuffle=shuffle,\r\n"]
[1601.016, "o", "            dict_init=dict_init,\r\n"]
[1601.026, "o", "            random_state=random_state,\r\n"]
[1601.036, "o", "            transform_algorithm=transform_algorithm,\r\n"]
[1601.046, "o", "            transform_alpha=alpha,\r\n"]
[1601.056, "o", "            positive_code=positive_code,\r\n"]
[1601.066, "o", "            positive_dict=positive_dict,\r\n"]
[1601.076, "o", "            transform_max_iter=method_max_iter,\r\n"]
[1601.086, "o", "            verbose=verbose,\r\n"]
[1601.096, "o", "            callback=callback,\r\n"]
[1601.106, "o", "            tol=tol,\r\n"]
[1601.116, "o", "            max_no_improvement=max_no_improvement,\r\n"]
[1601.126, "o", "        ).fit(X)\r\n"]
[1601.136, "o", "\r\n"]
[1601.146, "o", "        if not return_code:\r\n"]
[1601.156, "o", "            return est.components_\r\n"]
[1601.166, "o", "        else:\r\n"]
[1601.176, "o", "            code = est.transform(X)\r\n"]
[1601.186, "o", "            return code, est.components_\r\n"]
[1601.196, "o", "\r\n"]
[1601.206, "o", "    # TODO(1.4) remove the whole old behavior\r\n"]
[1601.216, "o", "    # Fallback to old behavior\r\n"]
[1601.226, "o", "\r\n"]
[1601.236, "o", "    n_iter = _check_warn_deprecated(\r\n"]
[1601.246, "o", "        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n"]
[1601.256, "o", "    )\r\n"]
[1601.266, "o", "\r\n"]
[1601.276, "o", "    if n_components is None:\r\n"]
[1601.286, "o", "        n_components = X.shape[1]\r\n"]
[1601.296, "o", "\r\n"]
[1601.306, "o", "    if method not in (\"lars\", \"cd\"):\r\n"]
[1601.316, "o", "        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n"]
[1601.326, "o", "\r\n"]
[1601.336, "o", "    _check_positive_coding(method, positive_code)\r\n"]
[1601.346, "o", "\r\n"]
[1601.356, "o", "    method = \"lasso_\" + method\r\n"]
[1601.366, "o", "\r\n"]
[1601.376, "o", "    t0 = time.time()\r\n"]
[1601.386, "o", "    n_samples, n_features = X.shape\r\n"]
[1601.396, "o", "    # Avoid integer division problems\r\n"]
[1601.406, "o", "    alpha = float(alpha)\r\n"]
[1601.416, "o", "    random_state = check_random_state(random_state)\r\n"]
[1601.426, "o", "\r\n"]
[1601.436, "o", "    # Init V with SVD of X\r\n"]
[1601.446, "o", "    if dict_init is not None:\r\n"]
[1601.456, "o", "        dictionary = dict_init\r\n"]
[1601.466, "o", "    else:\r\n"]
[1601.476, "o", "        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n"]
[1601.486, "o", "        dictionary = S[:, np.newaxis] * dictionary\r\n"]
[1601.496, "o", "    r = len(dictionary)\r\n"]
[1601.506, "o", "    if n_components <= r:\r\n"]
[1601.516, "o", "        dictionary = dictionary[:n_components, :]\r\n"]
[1601.526, "o", "    else:\r\n"]
[1601.536, "o", "        dictionary = np.r_[\r\n"]
[1601.546, "o", "            dictionary,\r\n"]
[1601.556, "o", "            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n"]
[1601.566, "o", "        ]\r\n"]
[1601.576, "o", "\r\n"]
[1601.586, "o", "    if verbose == 1:\r\n"]
[1601.596, "o", "        print(\"[dict_learning]\", end=\" \")\r\n"]
[1601.606, "o", "\r\n"]
[1601.616, "o", "    if shuffle:\r\n"]
[1601.626, "o", "        X_train = X.copy()\r\n"]
[1601.636, "o", "        random_state.shuffle(X_train)\r\n"]
[1601.646, "o", "    else:\r\n"]
[1601.656, "o", "        X_train = X\r\n"]
[1601.666, "o", "\r\n"]
[1601.676, "o", "    X_train = check_array(\r\n"]
[1601.686, "o", "        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n"]
[1601.696, "o", "    )\r\n"]
[1601.706, "o", "\r\n"]
[1601.716, "o", "    # Fortran-order dict better suited for the sparse coding which is the\r\n"]
[1601.726, "o", "    # bottleneck of this algorithm.\r\n"]
[1601.736, "o", "    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n"]
[1601.746, "o", "    dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[1601.756, "o", "\r\n"]
[1601.766, "o", "    batches = gen_batches(n_samples, batch_size)\r\n"]
[1601.776, "o", "    batches = itertools.cycle(batches)\r\n"]
[1601.786, "o", "\r\n"]
[1601.796, "o", "    # The covariance of the dictionary\r\n"]
[1601.806, "o", "    if inner_stats is None:\r\n"]
[1601.816, "o", "        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n"]
[1601.826, "o", "        # The data approximation\r\n"]
[1601.836, "o", "        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n"]
[1601.846, "o", "    else:\r\n"]
[1601.856, "o", "        A = inner_stats[0].copy()\r\n"]
[1601.866, "o", "        B = inner_stats[1].copy()\r\n"]
[1601.876, "o", "\r\n"]
[1601.886, "o", "    # If n_iter is zero, we need to return zero.\r\n"]
[1601.896, "o", "    ii = iter_offset - 1\r\n"]
[1601.906, "o", "\r\n"]
[1601.916, "o", "    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n"]
[1601.926, "o", "        this_X = X_train[batch]\r\n"]
[1601.936, "o", "        dt = time.time() - t0\r\n"]
[1601.946, "o", "        if verbose == 1:\r\n"]
[1601.956, "o", "            sys.stdout.write(\".\")\r\n"]
[1601.966, "o", "            sys.stdout.flush()\r\n"]
[1601.976, "o", "        elif verbose:\r\n"]
[1601.986, "o", "            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n"]
[1601.996, "o", "                print(\r\n"]
[1602.006, "o", "                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n"]
[1602.016, "o", "                )\r\n"]
[1602.026, "o", "\r\n"]
[1602.036, "o", "        this_code = sparse_encode(\r\n"]
[1602.046, "o", "            this_X,\r\n"]
[1602.056, "o", "            dictionary,\r\n"]
[1602.066, "o", "            algorithm=method,\r\n"]
[1602.076, "o", "            alpha=alpha,\r\n"]
[1602.086, "o", "            n_jobs=n_jobs,\r\n"]
[1602.096, "o", "            check_input=False,\r\n"]
[1602.106, "o", "            positive=positive_code,\r\n"]
[1602.116, "o", "            max_iter=method_max_iter,\r\n"]
[1602.126, "o", "            verbose=verbose,\r\n"]
[1602.136, "o", "        )\r\n"]
[1602.146, "o", "\r\n"]
[1602.156, "o", "        # Update the auxiliary variables\r\n"]
[1602.166, "o", "        if ii < batch_size - 1:\r\n"]
[1602.176, "o", "            theta = float((ii + 1) * batch_size)\r\n"]
[1602.186, "o", "        else:\r\n"]
[1602.196, "o", "            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n"]
[1602.206, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[1602.216, "o", "\r\n"]
[1602.226, "o", "        A *= beta\r\n"]
[1602.236, "o", "        A += np.dot(this_code.T, this_code)\r\n"]
[1602.246, "o", "        B *= beta\r\n"]
[1602.256, "o", "        B += np.dot(this_X.T, this_code)\r\n"]
[1602.266, "o", "\r\n"]
[1602.276, "o", "        # Update dictionary in place\r\n"]
[1602.286, "o", "        _update_dict(\r\n"]
[1602.296, "o", "            dictionary,\r\n"]
[1602.306, "o", "            this_X,\r\n"]
[1602.316, "o", "            this_code,\r\n"]
[1602.326, "o", "            A,\r\n"]
[1602.336, "o", "            B,\r\n"]
[1602.346, "o", "            verbose=verbose,\r\n"]
[1602.356, "o", "            random_state=random_state,\r\n"]
[1602.366, "o", "            positive=positive_dict,\r\n"]
[1602.376, "o", "        )\r\n"]
[1602.386, "o", "\r\n"]
[1602.396, "o", "        # Maybe we need a stopping criteria based on the amount of\r\n"]
[1602.406, "o", "        # modification in the dictionary\r\n"]
[1602.416, "o", "        if callback is not None:\r\n"]
[1602.426, "o", "            callback(locals())\r\n"]
[1602.436, "o", "\r\n"]
[1602.446, "o", "    if return_inner_stats:\r\n"]
[1602.456, "o", "        if return_n_iter:\r\n"]
[1602.466, "o", "            return dictionary, (A, B), ii - iter_offset + 1\r\n"]
[1602.476, "o", "        else:\r\n"]
[1602.486, "o", "            return dictionary, (A, B)\r\n"]
[1602.496, "o", "    if return_code:\r\n"]
[1602.506, "o", "        if verbose > 1:\r\n"]
[1602.516, "o", "            print(\"Learning code...\", end=\" \")\r\n"]
[1602.526, "o", "        elif verbose == 1:\r\n"]
[1602.536, "o", "            print(\"|\", end=\" \")\r\n"]
[1602.546, "o", "        code = sparse_encode(\r\n"]
[1602.556, "o", "            X,\r\n"]
[1602.566, "o", "            dictionary,\r\n"]
[1602.576, "o", "            algorithm=method,\r\n"]
[1602.586, "o", "            alpha=alpha,\r\n"]
[1602.596, "o", "            n_jobs=n_jobs,\r\n"]
[1602.606, "o", "            check_input=False,\r\n"]
[1602.616, "o", "            positive=positive_code,\r\n"]
[1602.626, "o", "            max_iter=method_max_iter,\r\n"]
[1602.636, "o", "            verbose=verbose,\r\n"]
[1602.646, "o", "        )\r\n"]
[1602.656, "o", "        if verbose > 1:\r\n"]
[1602.666, "o", "            dt = time.time() - t0\r\n"]
[1602.676, "o", "            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n"]
[1602.686, "o", "        if return_n_iter:\r\n"]
[1602.696, "o", "            return code, dictionary, ii - iter_offset + 1\r\n"]
[1602.706, "o", "        else:\r\n"]
[1602.716, "o", "            return code, dictionary\r\n"]
[1602.726, "o", "\r\n"]
[1602.736, "o", "    if return_n_iter:\r\n"]
[1602.746, "o", "        return dictionary, ii - iter_offset + 1\r\n"]
[1602.756, "o", "    else:\r\n"]
[1602.766, "o", "        return dictionary\r\n"]
[1602.776, "o", "\r\n"]
[1602.786, "o", "\r\n"]
[1602.796, "o", "@validate_params(\r\n"]
[1602.806, "o", "    {\r\n"]
[1602.816, "o", "        \"X\": [\"array-like\"],\r\n"]
[1602.826, "o", "        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[1602.836, "o", "        \"return_n_iter\": [\"boolean\"],\r\n"]
[1602.846, "o", "        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1602.856, "o", "    },\r\n"]
[1602.866, "o", "    prefer_skip_nested_validation=False,\r\n"]
[1602.876, "o", ")\r\n"]
[1602.886, "o", "def dict_learning(\r\n"]
[1602.896, "o", "    X,\r\n"]
[1602.906, "o", "    n_components,\r\n"]
[1602.916, "o", "    *,\r\n"]
[1602.926, "o", "    alpha,\r\n"]
[1602.936, "o", "    max_iter=100,\r\n"]
[1602.946, "o", "    tol=1e-8,\r\n"]
[1602.956, "o", "    method=\"lars\",\r\n"]
[1602.966, "o", "    n_jobs=None,\r\n"]
[1602.976, "o", "    dict_init=None,\r\n"]
[1602.986, "o", "    code_init=None,\r\n"]
[1602.996, "o", "    callback=None,\r\n"]
[1603.006, "o", "    verbose=False,\r\n"]
[1603.016, "o", "    random_state=None,\r\n"]
[1603.026, "o", "    return_n_iter=False,\r\n"]
[1603.036, "o", "    positive_dict=False,\r\n"]
[1603.046, "o", "    positive_code=False,\r\n"]
[1603.056, "o", "    method_max_iter=1000,\r\n"]
[1603.066, "o", "):\r\n"]
[1603.076, "o", "    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n"]
[1603.086, "o", "\r\n"]
[1603.096, "o", "    Finds the best dictionary and the corresponding sparse code for\r\n"]
[1603.106, "o", "    approximating the data matrix X by solving::\r\n"]
[1603.116, "o", "\r\n"]
[1603.126, "o", "        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[1603.136, "o", "                     (U,V)\r\n"]
[1603.146, "o", "                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n"]
[1603.156, "o", "\r\n"]
[1603.166, "o", "    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n"]
[1603.176, "o", "    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n"]
[1603.186, "o", "    which is the sum of the absolute values of all the entries in the matrix.\r\n"]
[1603.196, "o", "\r\n"]
[1603.206, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[1603.216, "o", "\r\n"]
[1603.226, "o", "    Parameters\r\n"]
[1603.236, "o", "    ----------\r\n"]
[1603.246, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[1603.256, "o", "        Data matrix.\r\n"]
[1603.266, "o", "\r\n"]
[1603.276, "o", "    n_components : int\r\n"]
[1603.286, "o", "        Number of dictionary atoms to extract.\r\n"]
[1603.296, "o", "\r\n"]
[1603.306, "o", "    alpha : int or float\r\n"]
[1603.316, "o", "        Sparsity controlling parameter.\r\n"]
[1603.326, "o", "\r\n"]
[1603.336, "o", "    max_iter : int, default=100\r\n"]
[1603.346, "o", "        Maximum number of iterations to perform.\r\n"]
[1603.356, "o", "\r\n"]
[1603.366, "o", "    tol : float, default=1e-8\r\n"]
[1603.376, "o", "        Tolerance for the stopping condition.\r\n"]
[1603.386, "o", "\r\n"]
[1603.396, "o", "    method : {'lars', 'cd'}, default='lars'\r\n"]
[1603.406, "o", "        The method used:\r\n"]
[1603.416, "o", "\r\n"]
[1603.426, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[1603.436, "o", "           problem (`linear_model.lars_path`);\r\n"]
[1603.446, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[1603.456, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[1603.466, "o", "          the estimated components are sparse.\r\n"]
[1603.476, "o", "\r\n"]
[1603.486, "o", "    n_jobs : int, default=None\r\n"]
[1603.496, "o", "        Number of parallel jobs to run.\r\n"]
[1603.506, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1603.516, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1603.526, "o", "        for more details.\r\n"]
[1603.536, "o", "\r\n"]
[1603.546, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1603.556, "o", "        Initial value for the dictionary for warm restart scenarios. Only used\r\n"]
[1603.566, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[1603.576, "o", "\r\n"]
[1603.586, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1603.596, "o", "        Initial value for the sparse code for warm restart scenarios. Only used\r\n"]
[1603.606, "o", "        if `code_init` and `dict_init` are not None.\r\n"]
[1603.616, "o", "\r\n"]
[1603.626, "o", "    callback : callable, default=None\r\n"]
[1603.636, "o", "        Callable that gets invoked every five iterations.\r\n"]
[1603.646, "o", "\r\n"]
[1603.656, "o", "    verbose : bool, default=False\r\n"]
[1603.666, "o", "        To control the verbosity of the procedure.\r\n"]
[1603.676, "o", "\r\n"]
[1603.686, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1603.696, "o", "        Used for randomly initializing the dictionary. Pass an int for\r\n"]
[1603.706, "o", "        reproducible results across multiple function calls.\r\n"]
[1603.716, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1603.726, "o", "\r\n"]
[1603.736, "o", "    return_n_iter : bool, default=False\r\n"]
[1603.746, "o", "        Whether or not to return the number of iterations.\r\n"]
[1603.756, "o", "\r\n"]
[1603.766, "o", "    positive_dict : bool, default=False\r\n"]
[1603.776, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1603.786, "o", "\r\n"]
[1603.796, "o", "        .. versionadded:: 0.20\r\n"]
[1603.806, "o", "\r\n"]
[1603.816, "o", "    positive_code : bool, default=False\r\n"]
[1603.826, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1603.836, "o", "\r\n"]
[1603.846, "o", "        .. versionadded:: 0.20\r\n"]
[1603.856, "o", "\r\n"]
[1603.866, "o", "    method_max_iter : int, default=1000\r\n"]
[1603.876, "o", "        Maximum number of iterations to perform.\r\n"]
[1603.886, "o", "\r\n"]
[1603.896, "o", "        .. versionadded:: 0.22\r\n"]
[1603.906, "o", "\r\n"]
[1603.916, "o", "    Returns\r\n"]
[1603.926, "o", "    -------\r\n"]
[1603.936, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1603.946, "o", "        The sparse code factor in the matrix factorization.\r\n"]
[1603.956, "o", "\r\n"]
[1603.966, "o", "    dictionary : ndarray of shape (n_components, n_features),\r\n"]
[1603.976, "o", "        The dictionary factor in the matrix factorization.\r\n"]
[1603.986, "o", "\r\n"]
[1603.996, "o", "    errors : array\r\n"]
[1604.006, "o", "        Vector of errors at each iteration.\r\n"]
[1604.016, "o", "\r\n"]
[1604.026, "o", "    n_iter : int\r\n"]
[1604.036, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[1604.046, "o", "        set to True.\r\n"]
[1604.056, "o", "\r\n"]
[1604.066, "o", "    See Also\r\n"]
[1604.076, "o", "    --------\r\n"]
[1604.086, "o", "    dict_learning_online : Solve a dictionary learning matrix factorization\r\n"]
[1604.096, "o", "        problem online.\r\n"]
[1604.106, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1604.116, "o", "    MiniBatchDictionaryLearning : A faster, less accurate version\r\n"]
[1604.126, "o", "        of the dictionary learning algorithm.\r\n"]
[1604.136, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1604.146, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1604.156, "o", "    \"\"\"\r\n"]
[1604.166, "o", "    estimator = DictionaryLearning(\r\n"]
[1604.176, "o", "        n_components=n_components,\r\n"]
[1604.186, "o", "        alpha=alpha,\r\n"]
[1604.196, "o", "        max_iter=max_iter,\r\n"]
[1604.206, "o", "        tol=tol,\r\n"]
[1604.216, "o", "        fit_algorithm=method,\r\n"]
[1604.226, "o", "        n_jobs=n_jobs,\r\n"]
[1604.236, "o", "        dict_init=dict_init,\r\n"]
[1604.246, "o", "        callback=callback,\r\n"]
[1604.256, "o", "        code_init=code_init,\r\n"]
[1604.266, "o", "        verbose=verbose,\r\n"]
[1604.276, "o", "        random_state=random_state,\r\n"]
[1604.286, "o", "        positive_code=positive_code,\r\n"]
[1604.296, "o", "        positive_dict=positive_dict,\r\n"]
[1604.306, "o", "        transform_max_iter=method_max_iter,\r\n"]
[1604.316, "o", "    )\r\n"]
[1604.326, "o", "    code = estimator.fit_transform(X)\r\n"]
[1604.336, "o", "    if return_n_iter:\r\n"]
[1604.346, "o", "        return (\r\n"]
[1604.356, "o", "            code,\r\n"]
[1604.366, "o", "            estimator.components_,\r\n"]
[1604.376, "o", "            estimator.error_,\r\n"]
[1604.386, "o", "            estimator.n_iter_,\r\n"]
[1604.396, "o", "        )\r\n"]
[1604.406, "o", "    return code, estimator.components_, estimator.error_\r\n"]
[1604.416, "o", "\r\n"]
[1604.426, "o", "\r\n"]
[1604.436, "o", "class _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n"]
[1604.446, "o", "    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n"]
[1604.456, "o", "\r\n"]
[1604.466, "o", "    def __init__(\r\n"]
[1604.476, "o", "        self,\r\n"]
[1604.486, "o", "        transform_algorithm,\r\n"]
[1604.496, "o", "        transform_n_nonzero_coefs,\r\n"]
[1604.506, "o", "        transform_alpha,\r\n"]
[1604.516, "o", "        split_sign,\r\n"]
[1604.526, "o", "        n_jobs,\r\n"]
[1604.536, "o", "        positive_code,\r\n"]
[1604.546, "o", "        transform_max_iter,\r\n"]
[1604.556, "o", "    ):\r\n"]
[1604.566, "o", "        self.transform_algorithm = transform_algorithm\r\n"]
[1604.576, "o", "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n"]
[1604.586, "o", "        self.transform_alpha = transform_alpha\r\n"]
[1604.596, "o", "        self.transform_max_iter = transform_max_iter\r\n"]
[1604.606, "o", "        self.split_sign = split_sign\r\n"]
[1604.616, "o", "        self.n_jobs = n_jobs\r\n"]
[1604.626, "o", "        self.positive_code = positive_code\r\n"]
[1604.636, "o", "\r\n"]
[1604.646, "o", "    def _transform(self, X, dictionary):\r\n"]
[1604.656, "o", "        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n"]
[1604.666, "o", "        SparseCoder.\"\"\"\r\n"]
[1604.676, "o", "        X = self._validate_data(X, reset=False)\r\n"]
[1604.686, "o", "\r\n"]
[1604.696, "o", "        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n"]
[1604.706, "o", "            transform_alpha = self.alpha\r\n"]
[1604.716, "o", "        else:\r\n"]
[1604.726, "o", "            transform_alpha = self.transform_alpha\r\n"]
[1604.736, "o", "\r\n"]
[1604.746, "o", "        code = sparse_encode(\r\n"]
[1604.756, "o", "            X,\r\n"]
[1604.766, "o", "            dictionary,\r\n"]
[1604.776, "o", "            algorithm=self.transform_algorithm,\r\n"]
[1604.786, "o", "            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n"]
[1604.796, "o", "            alpha=transform_alpha,\r\n"]
[1604.806, "o", "            max_iter=self.transform_max_iter,\r\n"]
[1604.816, "o", "            n_jobs=self.n_jobs,\r\n"]
[1604.826, "o", "            positive=self.positive_code,\r\n"]
[1604.836, "o", "        )\r\n"]
[1604.846, "o", "\r\n"]
[1604.856, "o", "        if self.split_sign:\r\n"]
[1604.866, "o", "            # feature vector is split into a positive and negative side\r\n"]
[1604.876, "o", "            n_samples, n_features = code.shape\r\n"]
[1604.886, "o", "            split_code = np.empty((n_samples, 2 * n_features))\r\n"]
[1604.896, "o", "            split_code[:, :n_features] = np.maximum(code, 0)\r\n"]
[1604.906, "o", "            split_code[:, n_features:] = -np.minimum(code, 0)\r\n"]
[1604.916, "o", "            code = split_code\r\n"]
[1604.926, "o", "\r\n"]
[1604.936, "o", "        return code\r\n"]
[1605.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1605.002, "i", "sed -n '1600,2600p' sklearn/decomposition/_dict_learning.py\r"]
[1605.004, "o", "sed -n '1600,2600p' sklearn/decomposition/_dict_learning.py\r\n"]
[1605.016, "o", "\u001b[?2004l\r\n"]
[1605.026, "o", "\r\n"]
[1605.036, "o", "    n_jobs : int or None, default=None\r\n"]
[1605.046, "o", "        Number of parallel jobs to run.\r\n"]
[1605.056, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1605.066, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1605.076, "o", "        for more details.\r\n"]
[1605.086, "o", "\r\n"]
[1605.096, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1605.106, "o", "        Initial value for the code, for warm restart. Only used if `code_init`\r\n"]
[1605.116, "o", "        and `dict_init` are not None.\r\n"]
[1605.126, "o", "\r\n"]
[1605.136, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1605.146, "o", "        Initial values for the dictionary, for warm restart. Only used if\r\n"]
[1605.156, "o", "        `code_init` and `dict_init` are not None.\r\n"]
[1605.166, "o", "\r\n"]
[1605.176, "o", "    callback : callable, default=None\r\n"]
[1605.186, "o", "        Callable that gets invoked every five iterations.\r\n"]
[1605.196, "o", "\r\n"]
[1605.206, "o", "        .. versionadded:: 1.3\r\n"]
[1605.216, "o", "\r\n"]
[1605.226, "o", "    verbose : bool, default=False\r\n"]
[1605.236, "o", "        To control the verbosity of the procedure.\r\n"]
[1605.246, "o", "\r\n"]
[1605.256, "o", "    split_sign : bool, default=False\r\n"]
[1605.266, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[1605.276, "o", "        its negative part and its positive part. This can improve the\r\n"]
[1605.286, "o", "        performance of downstream classifiers.\r\n"]
[1605.296, "o", "\r\n"]
[1605.306, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1605.316, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[1605.326, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[1605.336, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[1605.346, "o", "        results across multiple function calls.\r\n"]
[1605.356, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1605.366, "o", "\r\n"]
[1605.376, "o", "    positive_code : bool, default=False\r\n"]
[1605.386, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1605.396, "o", "\r\n"]
[1605.406, "o", "        .. versionadded:: 0.20\r\n"]
[1605.416, "o", "\r\n"]
[1605.426, "o", "    positive_dict : bool, default=False\r\n"]
[1605.436, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1605.446, "o", "\r\n"]
[1605.456, "o", "        .. versionadded:: 0.20\r\n"]
[1605.466, "o", "\r\n"]
[1605.476, "o", "    transform_max_iter : int, default=1000\r\n"]
[1605.486, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1605.496, "o", "        `'lasso_lars'`.\r\n"]
[1605.506, "o", "\r\n"]
[1605.516, "o", "        .. versionadded:: 0.22\r\n"]
[1605.526, "o", "\r\n"]
[1605.536, "o", "    Attributes\r\n"]
[1605.546, "o", "    ----------\r\n"]
[1605.556, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[1605.566, "o", "        dictionary atoms extracted from the data\r\n"]
[1605.576, "o", "\r\n"]
[1605.586, "o", "    error_ : array\r\n"]
[1605.596, "o", "        vector of errors at each iteration\r\n"]
[1605.606, "o", "\r\n"]
[1605.616, "o", "    n_features_in_ : int\r\n"]
[1605.626, "o", "        Number of features seen during :term:`fit`.\r\n"]
[1605.636, "o", "\r\n"]
[1605.646, "o", "        .. versionadded:: 0.24\r\n"]
[1605.656, "o", "\r\n"]
[1605.666, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[1605.676, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[1605.686, "o", "        has feature names that are all strings.\r\n"]
[1605.696, "o", "\r\n"]
[1605.706, "o", "        .. versionadded:: 1.0\r\n"]
[1605.716, "o", "\r\n"]
[1605.726, "o", "    n_iter_ : int\r\n"]
[1605.736, "o", "        Number of iterations run.\r\n"]
[1605.746, "o", "\r\n"]
[1605.756, "o", "    See Also\r\n"]
[1605.766, "o", "    --------\r\n"]
[1605.776, "o", "    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n"]
[1605.786, "o", "        dictionary learning algorithm.\r\n"]
[1605.796, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1605.806, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[1605.816, "o", "        precomputed dictionary.\r\n"]
[1605.826, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1605.836, "o", "\r\n"]
[1605.846, "o", "    References\r\n"]
[1605.856, "o", "    ----------\r\n"]
[1605.866, "o", "\r\n"]
[1605.876, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[1605.886, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[1605.896, "o", "\r\n"]
[1605.906, "o", "    Examples\r\n"]
[1605.916, "o", "    --------\r\n"]
[1605.926, "o", "    >>> import numpy as np\r\n"]
[1605.936, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[1605.946, "o", "    >>> from sklearn.decomposition import DictionaryLearning\r\n"]
[1605.956, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[1605.966, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[1605.976, "o", "    ...     random_state=42,\r\n"]
[1605.986, "o", "    ... )\r\n"]
[1605.996, "o", "    >>> dict_learner = DictionaryLearning(\r\n"]
[1606.006, "o", "    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n"]
[1606.016, "o", "    ...     random_state=42,\r\n"]
[1606.026, "o", "    ... )\r\n"]
[1606.036, "o", "    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n"]
[1606.046, "o", "\r\n"]
[1606.056, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[1606.066, "o", "\r\n"]
[1606.076, "o", "    >>> np.mean(X_transformed == 0)\r\n"]
[1606.086, "o", "    0.41...\r\n"]
[1606.096, "o", "\r\n"]
[1606.106, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[1606.116, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[1606.126, "o", "    the original signal:\r\n"]
[1606.136, "o", "\r\n"]
[1606.146, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[1606.156, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[1606.166, "o", "    0.07...\r\n"]
[1606.176, "o", "    \"\"\"\r\n"]
[1606.186, "o", "\r\n"]
[1606.196, "o", "    _parameter_constraints: dict = {\r\n"]
[1606.206, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1606.216, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1606.226, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1606.236, "o", "        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1606.246, "o", "        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n"]
[1606.256, "o", "        \"transform_algorithm\": [\r\n"]
[1606.266, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[1606.276, "o", "        ],\r\n"]
[1606.286, "o", "        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1606.296, "o", "        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[1606.306, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[1606.316, "o", "        \"code_init\": [np.ndarray, None],\r\n"]
[1606.326, "o", "        \"dict_init\": [np.ndarray, None],\r\n"]
[1606.336, "o", "        \"callback\": [callable, None],\r\n"]
[1606.346, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[1606.356, "o", "        \"split_sign\": [\"boolean\"],\r\n"]
[1606.366, "o", "        \"random_state\": [\"random_state\"],\r\n"]
[1606.376, "o", "        \"positive_code\": [\"boolean\"],\r\n"]
[1606.386, "o", "        \"positive_dict\": [\"boolean\"],\r\n"]
[1606.396, "o", "        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1606.406, "o", "    }\r\n"]
[1606.416, "o", "\r\n"]
[1606.426, "o", "    def __init__(\r\n"]
[1606.436, "o", "        self,\r\n"]
[1606.446, "o", "        n_components=None,\r\n"]
[1606.456, "o", "        *,\r\n"]
[1606.466, "o", "        alpha=1,\r\n"]
[1606.476, "o", "        max_iter=1000,\r\n"]
[1606.486, "o", "        tol=1e-8,\r\n"]
[1606.496, "o", "        fit_algorithm=\"lars\",\r\n"]
[1606.506, "o", "        transform_algorithm=\"omp\",\r\n"]
[1606.516, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[1606.526, "o", "        transform_alpha=None,\r\n"]
[1606.536, "o", "        n_jobs=None,\r\n"]
[1606.546, "o", "        code_init=None,\r\n"]
[1606.556, "o", "        dict_init=None,\r\n"]
[1606.566, "o", "        callback=None,\r\n"]
[1606.576, "o", "        verbose=False,\r\n"]
[1606.586, "o", "        split_sign=False,\r\n"]
[1606.596, "o", "        random_state=None,\r\n"]
[1606.606, "o", "        positive_code=False,\r\n"]
[1606.616, "o", "        positive_dict=False,\r\n"]
[1606.626, "o", "        transform_max_iter=1000,\r\n"]
[1606.636, "o", "    ):\r\n"]
[1606.646, "o", "        super().__init__(\r\n"]
[1606.656, "o", "            transform_algorithm,\r\n"]
[1606.666, "o", "            transform_n_nonzero_coefs,\r\n"]
[1606.676, "o", "            transform_alpha,\r\n"]
[1606.686, "o", "            split_sign,\r\n"]
[1606.696, "o", "            n_jobs,\r\n"]
[1606.706, "o", "            positive_code,\r\n"]
[1606.716, "o", "            transform_max_iter,\r\n"]
[1606.726, "o", "        )\r\n"]
[1606.736, "o", "        self.n_components = n_components\r\n"]
[1606.746, "o", "        self.alpha = alpha\r\n"]
[1606.756, "o", "        self.max_iter = max_iter\r\n"]
[1606.766, "o", "        self.tol = tol\r\n"]
[1606.776, "o", "        self.fit_algorithm = fit_algorithm\r\n"]
[1606.786, "o", "        self.code_init = code_init\r\n"]
[1606.796, "o", "        self.dict_init = dict_init\r\n"]
[1606.806, "o", "        self.callback = callback\r\n"]
[1606.816, "o", "        self.verbose = verbose\r\n"]
[1606.826, "o", "        self.random_state = random_state\r\n"]
[1606.836, "o", "        self.positive_dict = positive_dict\r\n"]
[1606.846, "o", "\r\n"]
[1606.856, "o", "    def fit(self, X, y=None):\r\n"]
[1606.866, "o", "        \"\"\"Fit the model from data in X.\r\n"]
[1606.876, "o", "\r\n"]
[1606.886, "o", "        Parameters\r\n"]
[1606.896, "o", "        ----------\r\n"]
[1606.906, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[1606.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1606.926, "o", "            and `n_features` is the number of features.\r\n"]
[1606.936, "o", "\r\n"]
[1606.946, "o", "        y : Ignored\r\n"]
[1606.956, "o", "            Not used, present for API consistency by convention.\r\n"]
[1606.966, "o", "\r\n"]
[1606.976, "o", "        Returns\r\n"]
[1606.986, "o", "        -------\r\n"]
[1606.996, "o", "        self : object\r\n"]
[1607.006, "o", "            Returns the instance itself.\r\n"]
[1607.016, "o", "        \"\"\"\r\n"]
[1607.026, "o", "        self.fit_transform(X)\r\n"]
[1607.036, "o", "        return self\r\n"]
[1607.046, "o", "\r\n"]
[1607.056, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[1607.066, "o", "    def fit_transform(self, X, y=None):\r\n"]
[1607.076, "o", "        \"\"\"Fit the model from data in X and return the transformed data.\r\n"]
[1607.086, "o", "\r\n"]
[1607.096, "o", "        Parameters\r\n"]
[1607.106, "o", "        ----------\r\n"]
[1607.116, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[1607.126, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1607.136, "o", "            and `n_features` is the number of features.\r\n"]
[1607.146, "o", "\r\n"]
[1607.156, "o", "        y : Ignored\r\n"]
[1607.166, "o", "            Not used, present for API consistency by convention.\r\n"]
[1607.176, "o", "\r\n"]
[1607.186, "o", "        Returns\r\n"]
[1607.196, "o", "        -------\r\n"]
[1607.206, "o", "        V : ndarray of shape (n_samples, n_components)\r\n"]
[1607.216, "o", "            Transformed data.\r\n"]
[1607.226, "o", "        \"\"\"\r\n"]
[1607.236, "o", "        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n"]
[1607.246, "o", "\r\n"]
[1607.256, "o", "        method = \"lasso_\" + self.fit_algorithm\r\n"]
[1607.266, "o", "\r\n"]
[1607.276, "o", "        random_state = check_random_state(self.random_state)\r\n"]
[1607.286, "o", "        X = self._validate_data(X)\r\n"]
[1607.296, "o", "\r\n"]
[1607.306, "o", "        if self.n_components is None:\r\n"]
[1607.316, "o", "            n_components = X.shape[1]\r\n"]
[1607.326, "o", "        else:\r\n"]
[1607.336, "o", "            n_components = self.n_components\r\n"]
[1607.346, "o", "\r\n"]
[1607.356, "o", "        V, U, E, self.n_iter_ = _dict_learning(\r\n"]
[1607.366, "o", "            X,\r\n"]
[1607.376, "o", "            n_components,\r\n"]
[1607.386, "o", "            alpha=self.alpha,\r\n"]
[1607.396, "o", "            tol=self.tol,\r\n"]
[1607.406, "o", "            max_iter=self.max_iter,\r\n"]
[1607.416, "o", "            method=method,\r\n"]
[1607.426, "o", "            method_max_iter=self.transform_max_iter,\r\n"]
[1607.436, "o", "            n_jobs=self.n_jobs,\r\n"]
[1607.446, "o", "            code_init=self.code_init,\r\n"]
[1607.456, "o", "            dict_init=self.dict_init,\r\n"]
[1607.466, "o", "            callback=self.callback,\r\n"]
[1607.476, "o", "            verbose=self.verbose,\r\n"]
[1607.486, "o", "            random_state=random_state,\r\n"]
[1607.496, "o", "            return_n_iter=True,\r\n"]
[1607.506, "o", "            positive_dict=self.positive_dict,\r\n"]
[1607.516, "o", "            positive_code=self.positive_code,\r\n"]
[1607.526, "o", "        )\r\n"]
[1607.536, "o", "        self.components_ = U\r\n"]
[1607.546, "o", "        self.error_ = E\r\n"]
[1607.556, "o", "\r\n"]
[1607.566, "o", "        return V\r\n"]
[1607.576, "o", "\r\n"]
[1607.586, "o", "    @property\r\n"]
[1607.596, "o", "    def _n_features_out(self):\r\n"]
[1607.606, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[1607.616, "o", "        return self.components_.shape[0]\r\n"]
[1607.626, "o", "\r\n"]
[1607.636, "o", "    def _more_tags(self):\r\n"]
[1607.646, "o", "        return {\r\n"]
[1607.656, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[1607.666, "o", "        }\r\n"]
[1607.676, "o", "\r\n"]
[1607.686, "o", "\r\n"]
[1607.696, "o", "class MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[1607.706, "o", "    \"\"\"Mini-batch dictionary learning.\r\n"]
[1607.716, "o", "\r\n"]
[1607.726, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[1607.736, "o", "    encoding the fitted data.\r\n"]
[1607.746, "o", "\r\n"]
[1607.756, "o", "    Solves the optimization problem::\r\n"]
[1607.766, "o", "\r\n"]
[1607.776, "o", "       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[1607.786, "o", "                    (U,V)\r\n"]
[1607.796, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[1607.806, "o", "\r\n"]
[1607.816, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[1607.826, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[1607.836, "o", "    of all the entries in the matrix.\r\n"]
[1607.846, "o", "\r\n"]
[1607.856, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[1607.866, "o", "\r\n"]
[1607.876, "o", "    Parameters\r\n"]
[1607.886, "o", "    ----------\r\n"]
[1607.896, "o", "    n_components : int, default=None\r\n"]
[1607.906, "o", "        Number of dictionary elements to extract.\r\n"]
[1607.916, "o", "\r\n"]
[1607.926, "o", "    alpha : float, default=1\r\n"]
[1607.936, "o", "        Sparsity controlling parameter.\r\n"]
[1607.946, "o", "\r\n"]
[1607.956, "o", "    n_iter : int, default=1000\r\n"]
[1607.966, "o", "        Total number of iterations over data batches to perform.\r\n"]
[1607.976, "o", "\r\n"]
[1607.986, "o", "        .. deprecated:: 1.1\r\n"]
[1607.996, "o", "           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[1608.006, "o", "           ``max_iter`` instead.\r\n"]
[1608.016, "o", "\r\n"]
[1608.026, "o", "    max_iter : int, default=None\r\n"]
[1608.036, "o", "        Maximum number of iterations over the complete dataset before\r\n"]
[1608.046, "o", "        stopping independently of any early stopping criterion heuristics.\r\n"]
[1608.056, "o", "        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[1608.066, "o", "\r\n"]
[1608.076, "o", "        .. versionadded:: 1.1\r\n"]
[1608.086, "o", "\r\n"]
[1608.096, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[1608.106, "o", "        The algorithm used:\r\n"]
[1608.116, "o", "\r\n"]
[1608.126, "o", "        - `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[1608.136, "o", "          problem (`linear_model.lars_path`)\r\n"]
[1608.146, "o", "        - `'cd'`: uses the coordinate descent method to compute the\r\n"]
[1608.156, "o", "          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[1608.166, "o", "          the estimated components are sparse.\r\n"]
[1608.176, "o", "\r\n"]
[1608.186, "o", "    n_jobs : int, default=None\r\n"]
[1608.196, "o", "        Number of parallel jobs to run.\r\n"]
[1608.206, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1608.216, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1608.226, "o", "        for more details.\r\n"]
[1608.236, "o", "\r\n"]
[1608.246, "o", "    batch_size : int, default=256\r\n"]
[1608.256, "o", "        Number of samples in each mini-batch.\r\n"]
[1608.266, "o", "\r\n"]
[1608.276, "o", "        .. versionchanged:: 1.3\r\n"]
[1608.286, "o", "           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[1608.296, "o", "\r\n"]
[1608.306, "o", "    shuffle : bool, default=True\r\n"]
[1608.316, "o", "        Whether to shuffle the samples before forming batches.\r\n"]
[1608.326, "o", "\r\n"]
[1608.336, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1608.346, "o", "        Initial value of the dictionary for warm restart scenarios.\r\n"]
[1608.356, "o", "\r\n"]
[1608.366, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[1608.376, "o", "            'threshold'}, default='omp'\r\n"]
[1608.386, "o", "        Algorithm used to transform the data:\r\n"]
[1608.396, "o", "\r\n"]
[1608.406, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[1608.416, "o", "          (`linear_model.lars_path`);\r\n"]
[1608.426, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[1608.436, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1608.446, "o", "          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n"]
[1608.456, "o", "          if the estimated components are sparse.\r\n"]
[1608.466, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1608.476, "o", "          solution.\r\n"]
[1608.486, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[1608.496, "o", "          the projection ``dictionary * X'``.\r\n"]
[1608.506, "o", "\r\n"]
[1608.516, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[1608.526, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1608.536, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[1608.546, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[1608.556, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1608.566, "o", "\r\n"]
[1608.576, "o", "    transform_alpha : float, default=None\r\n"]
[1608.586, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1608.596, "o", "        penalty applied to the L1 norm.\r\n"]
[1608.606, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1608.616, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1608.626, "o", "        If `None`, defaults to `alpha`.\r\n"]
[1608.636, "o", "\r\n"]
[1608.646, "o", "        .. versionchanged:: 1.2\r\n"]
[1608.656, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[1608.666, "o", "\r\n"]
[1608.676, "o", "    verbose : bool or int, default=False\r\n"]
[1608.686, "o", "        To control the verbosity of the procedure.\r\n"]
[1608.696, "o", "\r\n"]
[1608.706, "o", "    split_sign : bool, default=False\r\n"]
[1608.716, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[1608.726, "o", "        its negative part and its positive part. This can improve the\r\n"]
[1608.736, "o", "        performance of downstream classifiers.\r\n"]
[1608.746, "o", "\r\n"]
[1608.756, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1608.766, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[1608.776, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[1608.786, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[1608.796, "o", "        results across multiple function calls.\r\n"]
[1608.806, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1608.816, "o", "\r\n"]
[1608.826, "o", "    positive_code : bool, default=False\r\n"]
[1608.836, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1608.846, "o", "\r\n"]
[1608.856, "o", "        .. versionadded:: 0.20\r\n"]
[1608.866, "o", "\r\n"]
[1608.876, "o", "    positive_dict : bool, default=False\r\n"]
[1608.886, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1608.896, "o", "\r\n"]
[1608.906, "o", "        .. versionadded:: 0.20\r\n"]
[1608.916, "o", "\r\n"]
[1608.926, "o", "    transform_max_iter : int, default=1000\r\n"]
[1608.936, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1608.946, "o", "        `'lasso_lars'`.\r\n"]
[1608.956, "o", "\r\n"]
[1608.966, "o", "        .. versionadded:: 0.22\r\n"]
[1608.976, "o", "\r\n"]
[1608.986, "o", "    callback : callable, default=None\r\n"]
[1608.996, "o", "        A callable that gets invoked at the end of each iteration.\r\n"]
[1609.006, "o", "\r\n"]
[1609.016, "o", "        .. versionadded:: 1.1\r\n"]
[1609.026, "o", "\r\n"]
[1609.036, "o", "    tol : float, default=1e-3\r\n"]
[1609.046, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[1609.056, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[1609.066, "o", "\r\n"]
[1609.076, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[1609.086, "o", "        `tol` to 0.0.\r\n"]
[1609.096, "o", "\r\n"]
[1609.106, "o", "        .. versionadded:: 1.1\r\n"]
[1609.116, "o", "\r\n"]
[1609.126, "o", "    max_no_improvement : int, default=10\r\n"]
[1609.136, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[1609.146, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[1609.156, "o", "        `max_iter` is not None.\r\n"]
[1609.166, "o", "\r\n"]
[1609.176, "o", "        To disable convergence detection based on cost function, set\r\n"]
[1609.186, "o", "        `max_no_improvement` to None.\r\n"]
[1609.196, "o", "\r\n"]
[1609.206, "o", "        .. versionadded:: 1.1\r\n"]
[1609.216, "o", "\r\n"]
[1609.226, "o", "    Attributes\r\n"]
[1609.236, "o", "    ----------\r\n"]
[1609.246, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[1609.256, "o", "        Components extracted from the data.\r\n"]
[1609.266, "o", "\r\n"]
[1609.276, "o", "    n_features_in_ : int\r\n"]
[1609.286, "o", "        Number of features seen during :term:`fit`.\r\n"]
[1609.296, "o", "\r\n"]
[1609.306, "o", "        .. versionadded:: 0.24\r\n"]
[1609.316, "o", "\r\n"]
[1609.326, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[1609.336, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[1609.346, "o", "        has feature names that are all strings.\r\n"]
[1609.356, "o", "\r\n"]
[1609.366, "o", "        .. versionadded:: 1.0\r\n"]
[1609.376, "o", "\r\n"]
[1609.386, "o", "    n_iter_ : int\r\n"]
[1609.396, "o", "        Number of iterations over the full dataset.\r\n"]
[1609.406, "o", "\r\n"]
[1609.416, "o", "    n_steps_ : int\r\n"]
[1609.426, "o", "        Number of mini-batches processed.\r\n"]
[1609.436, "o", "\r\n"]
[1609.446, "o", "        .. versionadded:: 1.1\r\n"]
[1609.456, "o", "\r\n"]
[1609.466, "o", "    See Also\r\n"]
[1609.476, "o", "    --------\r\n"]
[1609.486, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1609.496, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1609.506, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[1609.516, "o", "        precomputed dictionary.\r\n"]
[1609.526, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1609.536, "o", "\r\n"]
[1609.546, "o", "    References\r\n"]
[1609.556, "o", "    ----------\r\n"]
[1609.566, "o", "\r\n"]
[1609.576, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[1609.586, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[1609.596, "o", "\r\n"]
[1609.606, "o", "    Examples\r\n"]
[1609.616, "o", "    --------\r\n"]
[1609.626, "o", "    >>> import numpy as np\r\n"]
[1609.636, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[1609.646, "o", "    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n"]
[1609.656, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[1609.666, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[1609.676, "o", "    ...     random_state=42)\r\n"]
[1609.686, "o", "    >>> dict_learner = MiniBatchDictionaryLearning(\r\n"]
[1609.696, "o", "    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n"]
[1609.706, "o", "    ...     transform_alpha=0.1, random_state=42)\r\n"]
[1609.716, "o", "    >>> X_transformed = dict_learner.fit_transform(X)\r\n"]
[1609.726, "o", "\r\n"]
[1609.736, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[1609.746, "o", "\r\n"]
[1609.756, "o", "    >>> np.mean(X_transformed == 0) < 0.5\r\n"]
[1609.766, "o", "    True\r\n"]
[1609.776, "o", "\r\n"]
[1609.786, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[1609.796, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[1609.806, "o", "    the original signal:\r\n"]
[1609.816, "o", "\r\n"]
[1609.826, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[1609.836, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[1609.846, "o", "    0.057...\r\n"]
[1609.856, "o", "    \"\"\"\r\n"]
[1609.866, "o", "\r\n"]
[1609.876, "o", "    _parameter_constraints: dict = {\r\n"]
[1609.886, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1609.896, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1609.906, "o", "        \"n_iter\": [\r\n"]
[1609.916, "o", "            Interval(Integral, 0, None, closed=\"left\"),\r\n"]
[1609.926, "o", "            Hidden(StrOptions({\"deprecated\"})),\r\n"]
[1609.936, "o", "        ],\r\n"]
[1610.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1610.002, "i", "grep -n \"def _minibatch_step\\|_minibatch_step\\(\" sklearn/decomposition/_dict_learning.py\r"]
[1610.004, "o", "grep -n \"def _minibatch_step\\|_minibatch_step\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1612.478, "o", "\u001b[?2004l\r\n"]
[1615.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1615.002, "i", "grep -n \"_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.py\r"]
[1615.004, "o", "grep -n \"_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1617.478, "o", "\u001b[?2004l\r\n"]
[1620.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1620.002, "i", "sed -n '2200,3600p' sklearn/decomposition/_dict_learning.py\r"]
[1620.004, "o", "sed -n '2200,3600p' sklearn/decomposition/_dict_learning.py\r\n"]
[1620.022703, "o", "\u001b[?2004l\r\n"]
[1620.039405, "o", "\r\n"]
[1620.056108, "o", "        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n"]
[1620.072811, "o", "        dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[1620.089514, "o", "\r\n"]
[1620.106216, "o", "        return dictionary\r\n"]
[1620.122919, "o", "\r\n"]
[1620.139622, "o", "    def _update_inner_stats(self, X, code, batch_size, step):\r\n"]
[1620.156324, "o", "        \"\"\"Update the inner stats inplace.\"\"\"\r\n"]
[1620.173027, "o", "        if step < batch_size - 1:\r\n"]
[1620.18973, "o", "            theta = (step + 1) * batch_size\r\n"]
[1620.206432, "o", "        else:\r\n"]
[1620.223135, "o", "            theta = batch_size**2 + step + 1 - batch_size\r\n"]
[1620.239838, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[1620.256541, "o", "\r\n"]
[1620.273243, "o", "        self._A *= beta\r\n"]
[1620.289946, "o", "        self._A += code.T @ code / batch_size\r\n"]
[1620.306649, "o", "        self._B *= beta\r\n"]
[1620.323351, "o", "        self._B += X.T @ code / batch_size\r\n"]
[1620.340054, "o", "\r\n"]
[1620.356757, "o", "    def _minibatch_step(self, X, dictionary, random_state, step):\r\n"]
[1620.373459, "o", "        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n"]
[1620.390162, "o", "        batch_size = X.shape[0]\r\n"]
[1620.406865, "o", "\r\n"]
[1620.423568, "o", "        # Compute code for this batch\r\n"]
[1620.44027, "o", "        code = _sparse_encode(\r\n"]
[1620.456973, "o", "            X,\r\n"]
[1620.473676, "o", "            dictionary,\r\n"]
[1620.490378, "o", "            algorithm=self._fit_algorithm,\r\n"]
[1620.507081, "o", "            alpha=self.alpha,\r\n"]
[1620.523784, "o", "            n_jobs=self.n_jobs,\r\n"]
[1620.540486, "o", "            positive=self.positive_code,\r\n"]
[1620.557189, "o", "            max_iter=self.transform_max_iter,\r\n"]
[1620.573892, "o", "            verbose=self.verbose,\r\n"]
[1620.590595, "o", "        )\r\n"]
[1620.607297, "o", "\r\n"]
[1620.624, "o", "        batch_cost = (\r\n"]
[1620.640703, "o", "            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n"]
[1620.657405, "o", "            + self.alpha * np.sum(np.abs(code))\r\n"]
[1620.674108, "o", "        ) / batch_size\r\n"]
[1620.690811, "o", "\r\n"]
[1620.707514, "o", "        # Update inner stats\r\n"]
[1620.724216, "o", "        self._update_inner_stats(X, code, batch_size, step)\r\n"]
[1620.740919, "o", "\r\n"]
[1620.757622, "o", "        # Update dictionary\r\n"]
[1620.774324, "o", "        _update_dict(\r\n"]
[1620.791027, "o", "            dictionary,\r\n"]
[1620.80773, "o", "            X,\r\n"]
[1620.824432, "o", "            code,\r\n"]
[1620.841135, "o", "            self._A,\r\n"]
[1620.857838, "o", "            self._B,\r\n"]
[1620.874541, "o", "            verbose=self.verbose,\r\n"]
[1620.891243, "o", "            random_state=random_state,\r\n"]
[1620.907946, "o", "            positive=self.positive_dict,\r\n"]
[1620.924649, "o", "        )\r\n"]
[1620.941351, "o", "\r\n"]
[1620.958054, "o", "        return batch_cost\r\n"]
[1620.974757, "o", "\r\n"]
[1620.991459, "o", "    def _check_convergence(\r\n"]
[1621.008162, "o", "        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n"]
[1621.024865, "o", "    ):\r\n"]
[1621.041568, "o", "        \"\"\"Helper function to encapsulate the early stopping logic.\r\n"]
[1621.05827, "o", "\r\n"]
[1621.074973, "o", "        Early stopping is based on two factors:\r\n"]
[1621.091676, "o", "        - A small change of the dictionary between two minibatch updates. This is\r\n"]
[1621.108378, "o", "          controlled by the tol parameter.\r\n"]
[1621.125081, "o", "        - No more improvement on a smoothed estimate of the objective function for a\r\n"]
[1621.141784, "o", "          a certain number of consecutive minibatch updates. This is controlled by\r\n"]
[1621.158486, "o", "          the max_no_improvement parameter.\r\n"]
[1621.175189, "o", "        \"\"\"\r\n"]
[1621.191892, "o", "        batch_size = X.shape[0]\r\n"]
[1621.208595, "o", "\r\n"]
[1621.225297, "o", "        # counts steps starting from 1 for user friendly verbose mode.\r\n"]
[1621.242, "o", "        step = step + 1\r\n"]
[1621.258703, "o", "\r\n"]
[1621.275405, "o", "        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n"]
[1621.292108, "o", "        # too bad value\r\n"]
[1621.308811, "o", "        if step <= min(100, n_samples / batch_size):\r\n"]
[1621.325514, "o", "            if self.verbose:\r\n"]
[1621.342216, "o", "                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n"]
[1621.358919, "o", "            return False\r\n"]
[1621.375622, "o", "\r\n"]
[1621.392324, "o", "        # Compute an Exponentially Weighted Average of the cost function to\r\n"]
[1621.409027, "o", "        # monitor the convergence while discarding minibatch-local stochastic\r\n"]
[1621.42573, "o", "        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n"]
[1621.442432, "o", "        if self._ewa_cost is None:\r\n"]
[1621.459135, "o", "            self._ewa_cost = batch_cost\r\n"]
[1621.475838, "o", "        else:\r\n"]
[1621.492541, "o", "            alpha = batch_size / (n_samples + 1)\r\n"]
[1621.509243, "o", "            alpha = min(alpha, 1)\r\n"]
[1621.525946, "o", "            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n"]
[1621.542649, "o", "\r\n"]
[1621.559351, "o", "        if self.verbose:\r\n"]
[1621.576054, "o", "            print(\r\n"]
[1621.592757, "o", "                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\r\n"]
[1621.609459, "o", "                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\r\n"]
[1621.626162, "o", "            )\r\n"]
[1621.642865, "o", "\r\n"]
[1621.659568, "o", "        # Early stopping based on change of dictionary\r\n"]
[1621.67627, "o", "        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\r\n"]
[1621.692973, "o", "        if self.tol > 0 and dict_diff <= self.tol:\r\n"]
[1621.709676, "o", "            if self.verbose:\r\n"]
[1621.726378, "o", "                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\r\n"]
[1621.743081, "o", "            return True\r\n"]
[1621.759784, "o", "\r\n"]
[1621.776486, "o", "        # Early stopping heuristic due to lack of improvement on smoothed\r\n"]
[1621.793189, "o", "        # cost function\r\n"]
[1621.809892, "o", "        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\r\n"]
[1621.826595, "o", "            self._no_improvement = 0\r\n"]
[1621.843297, "o", "            self._ewa_cost_min = self._ewa_cost\r\n"]
[1621.86, "o", "        else:\r\n"]
[1621.876703, "o", "            self._no_improvement += 1\r\n"]
[1621.893405, "o", "\r\n"]
[1621.910108, "o", "        if (\r\n"]
[1621.926811, "o", "            self.max_no_improvement is not None\r\n"]
[1621.943514, "o", "            and self._no_improvement >= self.max_no_improvement\r\n"]
[1621.960216, "o", "        ):\r\n"]
[1621.976919, "o", "            if self.verbose:\r\n"]
[1621.993622, "o", "                print(\r\n"]
[1622.010324, "o", "                    \"Converged (lack of improvement in objective function) \"\r\n"]
[1622.027027, "o", "                    f\"at step {step}/{n_steps}\"\r\n"]
[1622.04373, "o", "                )\r\n"]
[1622.060432, "o", "            return True\r\n"]
[1622.077135, "o", "\r\n"]
[1622.093838, "o", "        return False\r\n"]
[1622.110541, "o", "\r\n"]
[1622.127243, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[1622.143946, "o", "    def fit(self, X, y=None):\r\n"]
[1622.160649, "o", "        \"\"\"Fit the model from data in X.\r\n"]
[1622.177351, "o", "\r\n"]
[1622.194054, "o", "        Parameters\r\n"]
[1622.210757, "o", "        ----------\r\n"]
[1622.227459, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[1622.244162, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1622.260865, "o", "            and `n_features` is the number of features.\r\n"]
[1622.277568, "o", "\r\n"]
[1622.29427, "o", "        y : Ignored\r\n"]
[1622.310973, "o", "            Not used, present for API consistency by convention.\r\n"]
[1622.327676, "o", "\r\n"]
[1622.344378, "o", "        Returns\r\n"]
[1622.361081, "o", "        -------\r\n"]
[1622.377784, "o", "        self : object\r\n"]
[1622.394486, "o", "            Returns the instance itself.\r\n"]
[1622.411189, "o", "        \"\"\"\r\n"]
[1622.427892, "o", "        X = self._validate_data(\r\n"]
[1622.444595, "o", "            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n"]
[1622.461297, "o", "        )\r\n"]
[1622.478, "o", "\r\n"]
[1622.494703, "o", "        self._check_params(X)\r\n"]
[1622.511405, "o", "\r\n"]
[1622.528108, "o", "        if self.n_iter != \"deprecated\":\r\n"]
[1622.544811, "o", "            warnings.warn(\r\n"]
[1622.561514, "o", "                (\r\n"]
[1622.578216, "o", "                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n"]
[1622.594919, "o", "                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n"]
[1622.611622, "o", "                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n"]
[1622.628324, "o", "                    \"specified.\"\r\n"]
[1622.645027, "o", "                ),\r\n"]
[1622.66173, "o", "                FutureWarning,\r\n"]
[1622.678432, "o", "            )\r\n"]
[1622.695135, "o", "            n_iter = self.n_iter\r\n"]
[1622.711838, "o", "\r\n"]
[1622.728541, "o", "        self._random_state = check_random_state(self.random_state)\r\n"]
[1622.745243, "o", "\r\n"]
[1622.761946, "o", "        dictionary = self._initialize_dict(X, self._random_state)\r\n"]
[1622.778649, "o", "        old_dict = dictionary.copy()\r\n"]
[1622.795351, "o", "\r\n"]
[1622.812054, "o", "        if self.shuffle:\r\n"]
[1622.828757, "o", "            X_train = X.copy()\r\n"]
[1622.845459, "o", "            self._random_state.shuffle(X_train)\r\n"]
[1622.862162, "o", "        else:\r\n"]
[1622.878865, "o", "            X_train = X\r\n"]
[1622.895568, "o", "\r\n"]
[1622.91227, "o", "        n_samples, n_features = X_train.shape\r\n"]
[1622.928973, "o", "\r\n"]
[1622.945676, "o", "        if self.verbose:\r\n"]
[1622.962378, "o", "            print(\"[dict_learning]\")\r\n"]
[1622.979081, "o", "\r\n"]
[1622.995784, "o", "        # Inner stats\r\n"]
[1623.012486, "o", "        self._A = np.zeros(\r\n"]
[1623.029189, "o", "            (self._n_components, self._n_components), dtype=X_train.dtype\r\n"]
[1623.045892, "o", "        )\r\n"]
[1623.062595, "o", "        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n"]
[1623.079297, "o", "\r\n"]
[1623.096, "o", "        if self.max_iter is not None:\r\n"]
[1623.112703, "o", "            # Attributes to monitor the convergence\r\n"]
[1623.129405, "o", "            self._ewa_cost = None\r\n"]
[1623.146108, "o", "            self._ewa_cost_min = None\r\n"]
[1623.162811, "o", "            self._no_improvement = 0\r\n"]
[1623.179514, "o", "\r\n"]
[1623.196216, "o", "            batches = gen_batches(n_samples, self._batch_size)\r\n"]
[1623.212919, "o", "            batches = itertools.cycle(batches)\r\n"]
[1623.229622, "o", "            n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\r\n"]
[1623.246324, "o", "            n_steps = self.max_iter * n_steps_per_iter\r\n"]
[1623.263027, "o", "\r\n"]
[1623.27973, "o", "            i = -1  # to allow max_iter = 0\r\n"]
[1623.296432, "o", "\r\n"]
[1623.313135, "o", "            for i, batch in zip(range(n_steps), batches):\r\n"]
[1623.329838, "o", "                X_batch = X_train[batch]\r\n"]
[1623.346541, "o", "\r\n"]
[1623.363243, "o", "                batch_cost = self._minibatch_step(\r\n"]
[1623.379946, "o", "                    X_batch, dictionary, self._random_state, i\r\n"]
[1623.396649, "o", "                )\r\n"]
[1623.413351, "o", "\r\n"]
[1623.430054, "o", "                if self._check_convergence(\r\n"]
[1623.446757, "o", "                    X_batch, batch_cost, dictionary, old_dict, n_samples, i, n_steps\r\n"]
[1623.463459, "o", "                ):\r\n"]
[1623.480162, "o", "                    break\r\n"]
[1623.496865, "o", "\r\n"]
[1623.513568, "o", "                # XXX callback param added for backward compat in #18975 but a common\r\n"]
[1623.53027, "o", "                # unified callback API should be preferred\r\n"]
[1623.546973, "o", "                if self.callback is not None:\r\n"]
[1623.563676, "o", "                    self.callback(locals())\r\n"]
[1623.580378, "o", "\r\n"]
[1623.597081, "o", "                old_dict[:] = dictionary\r\n"]
[1623.613784, "o", "\r\n"]
[1623.630486, "o", "            self.n_steps_ = i + 1\r\n"]
[1623.647189, "o", "            self.n_iter_ = np.ceil(self.n_steps_ / n_steps_per_iter)\r\n"]
[1623.663892, "o", "        else:\r\n"]
[1623.680595, "o", "            # TODO remove this branch in 1.4\r\n"]
[1623.697297, "o", "            n_iter = 1000 if self.n_iter == \"deprecated\" else self.n_iter\r\n"]
[1623.714, "o", "\r\n"]
[1623.730703, "o", "            batches = gen_batches(n_samples, self._batch_size)\r\n"]
[1623.747405, "o", "            batches = itertools.cycle(batches)\r\n"]
[1623.764108, "o", "\r\n"]
[1623.780811, "o", "            for i, batch in zip(range(n_iter), batches):\r\n"]
[1623.797514, "o", "                self._minibatch_step(X_train[batch], dictionary, self._random_state, i)\r\n"]
[1623.814216, "o", "\r\n"]
[1623.830919, "o", "                trigger_verbose = self.verbose and i % ceil(100.0 / self.verbose) == 0\r\n"]
[1623.847622, "o", "                if self.verbose > 10 or trigger_verbose:\r\n"]
[1623.864324, "o", "                    print(f\"{i} batches processed.\")\r\n"]
[1623.881027, "o", "\r\n"]
[1623.89773, "o", "                if self.callback is not None:\r\n"]
[1623.914432, "o", "                    self.callback(locals())\r\n"]
[1623.931135, "o", "\r\n"]
[1623.947838, "o", "            self.n_steps_ = n_iter\r\n"]
[1623.964541, "o", "            self.n_iter_ = np.ceil(n_iter / int(np.ceil(n_samples / self._batch_size)))\r\n"]
[1623.981243, "o", "\r\n"]
[1623.997946, "o", "        self.components_ = dictionary\r\n"]
[1624.014649, "o", "\r\n"]
[1624.031351, "o", "        return self\r\n"]
[1624.048054, "o", "\r\n"]
[1624.064757, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[1624.081459, "o", "    def partial_fit(self, X, y=None):\r\n"]
[1624.098162, "o", "        \"\"\"Update the model using the data in X as a mini-batch.\r\n"]
[1624.114865, "o", "\r\n"]
[1624.131568, "o", "        Parameters\r\n"]
[1624.14827, "o", "        ----------\r\n"]
[1624.164973, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[1624.181676, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1624.198378, "o", "            and `n_features` is the number of features.\r\n"]
[1624.215081, "o", "\r\n"]
[1624.231784, "o", "        y : Ignored\r\n"]
[1624.248486, "o", "            Not used, present for API consistency by convention.\r\n"]
[1624.265189, "o", "\r\n"]
[1624.281892, "o", "        Returns\r\n"]
[1624.298595, "o", "        -------\r\n"]
[1624.315297, "o", "        self : object\r\n"]
[1624.332, "o", "            Return the instance itself.\r\n"]
[1624.348703, "o", "        \"\"\"\r\n"]
[1624.365405, "o", "        has_components = hasattr(self, \"components_\")\r\n"]
[1624.382108, "o", "\r\n"]
[1624.398811, "o", "        X = self._validate_data(\r\n"]
[1624.415514, "o", "            X, dtype=[np.float64, np.float32], order=\"C\", reset=not has_components\r\n"]
[1624.432216, "o", "        )\r\n"]
[1624.448919, "o", "\r\n"]
[1624.465622, "o", "        if not has_components:\r\n"]
[1624.482324, "o", "            # This instance has not been fitted yet (fit or partial_fit)\r\n"]
[1624.499027, "o", "            self._check_params(X)\r\n"]
[1624.51573, "o", "            self._random_state = check_random_state(self.random_state)\r\n"]
[1624.532432, "o", "\r\n"]
[1624.549135, "o", "            dictionary = self._initialize_dict(X, self._random_state)\r\n"]
[1624.565838, "o", "\r\n"]
[1624.582541, "o", "            self.n_steps_ = 0\r\n"]
[1624.599243, "o", "\r\n"]
[1624.615946, "o", "            self._A = np.zeros((self._n_components, self._n_components), dtype=X.dtype)\r\n"]
[1624.632649, "o", "            self._B = np.zeros((X.shape[1], self._n_components), dtype=X.dtype)\r\n"]
[1624.649351, "o", "        else:\r\n"]
[1624.666054, "o", "            dictionary = self.components_\r\n"]
[1624.682757, "o", "\r\n"]
[1624.699459, "o", "        self._minibatch_step(X, dictionary, self._random_state, self.n_steps_)\r\n"]
[1624.716162, "o", "\r\n"]
[1624.732865, "o", "        self.components_ = dictionary\r\n"]
[1624.749568, "o", "        self.n_steps_ += 1\r\n"]
[1624.76627, "o", "\r\n"]
[1624.782973, "o", "        return self\r\n"]
[1624.799676, "o", "\r\n"]
[1624.816378, "o", "    @property\r\n"]
[1624.833081, "o", "    def _n_features_out(self):\r\n"]
[1624.849784, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[1624.866486, "o", "        return self.components_.shape[0]\r\n"]
[1624.883189, "o", "\r\n"]
[1624.899892, "o", "    def _more_tags(self):\r\n"]
[1624.916595, "o", "        return {\r\n"]
[1624.933297, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[1625.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1625.002, "i", "sed -n '3600,5200p' sklearn/decomposition/_dict_learning.py\r"]
[1625.004, "o", "sed -n '3600,5200p' sklearn/decomposition/_dict_learning.py\r\n"]
[1625.006, "o", "\u001b[?2004l\r\n"]
[1630.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1630.002, "i", "grep -R \"def _minibatch_step\\|_minibatch_step\\(\" -n sklearn || true\r"]
[1630.004, "o", "grep -R \"def _minibatch_step\\|_minibatch_step\\(\" -n sklearn || true\r\n"]
[1632.478, "o", "\u001b[?2004l\r\n"]
[1635.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1635.002, "i", "grep -n \"class _BaseSparseCoding\\|def transform\\(|def partial_fit\\(\" sklearn/decomposition/_dict_learning.py\r"]
[1635.004, "o", "grep -n \"class _BaseSparseCoding\\|def transform\\(|def partial_fit\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1637.478, "o", "\u001b[?2004l\r\n"]
[1640.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1640.002, "i", "grep -R \"_sparse_encode_precomputed\\s*\\(|sparse_encode\\s*\\(\" -n sklearn/decomposition || true\r"]
[1640.004, "o", "grep -R \"_sparse_encode_precomputed\\s*\\(|sparse_encode\\s*\\(\" -n sklearn/decomposition || true\r\n"]
[1642.478, "o", "\u001b[?2004l\r\n"]
[1645.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1645.002, "i", "sed -n '1,1200p' sklearn/decomposition/_dict_learning.py\r"]
[1645.004, "o", "sed -n '1,1200p' sklearn/decomposition/_dict_learning.py\r\n"]
[1645.016, "o", "\u001b[?2004l\r\n"]
[1645.026, "o", "\"\"\" Dictionary learning.\r\n"]
[1645.036, "o", "\"\"\"\r\n"]
[1645.046, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[1645.056, "o", "# License: BSD 3 clause\r\n"]
[1645.066, "o", "\r\n"]
[1645.076, "o", "import itertools\r\n"]
[1645.086, "o", "import sys\r\n"]
[1645.096, "o", "import time\r\n"]
[1645.106, "o", "import warnings\r\n"]
[1645.116, "o", "from math import ceil\r\n"]
[1645.126, "o", "from numbers import Integral, Real\r\n"]
[1645.136, "o", "\r\n"]
[1645.146, "o", "import numpy as np\r\n"]
[1645.156, "o", "from joblib import effective_n_jobs\r\n"]
[1645.166, "o", "from scipy import linalg\r\n"]
[1645.176, "o", "\r\n"]
[1645.186, "o", "from ..base import (\r\n"]
[1645.196, "o", "    BaseEstimator,\r\n"]
[1645.206, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[1645.216, "o", "    TransformerMixin,\r\n"]
[1645.226, "o", "    _fit_context,\r\n"]
[1645.236, "o", ")\r\n"]
[1645.246, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[1645.256, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[1645.266, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[1645.276, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[1645.286, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[1645.296, "o", "from ..utils.validation import check_is_fitted\r\n"]
[1645.306, "o", "\r\n"]
[1645.316, "o", "\r\n"]
[1645.326, "o", "def _check_positive_coding(method, positive):\r\n"]
[1645.336, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[1645.346, "o", "        raise ValueError(\r\n"]
[1645.356, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[1645.366, "o", "        )\r\n"]
[1645.376, "o", "\r\n"]
[1645.386, "o", "\r\n"]
[1645.396, "o", "def _sparse_encode_precomputed(\r\n"]
[1645.406, "o", "    X,\r\n"]
[1645.416, "o", "    dictionary,\r\n"]
[1645.426, "o", "    *,\r\n"]
[1645.436, "o", "    gram=None,\r\n"]
[1645.446, "o", "    cov=None,\r\n"]
[1645.456, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1645.466, "o", "    regularization=None,\r\n"]
[1645.476, "o", "    copy_cov=True,\r\n"]
[1645.486, "o", "    init=None,\r\n"]
[1645.496, "o", "    max_iter=1000,\r\n"]
[1645.506, "o", "    verbose=0,\r\n"]
[1645.516, "o", "    positive=False,\r\n"]
[1645.526, "o", "):\r\n"]
[1645.536, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[1645.546, "o", "\r\n"]
[1645.556, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[1645.566, "o", "\r\n"]
[1645.576, "o", "    Parameters\r\n"]
[1645.586, "o", "    ----------\r\n"]
[1645.596, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[1645.606, "o", "        Data matrix.\r\n"]
[1645.616, "o", "\r\n"]
[1645.626, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1645.636, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1645.646, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[1645.656, "o", "\r\n"]
[1645.666, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[1645.676, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[1645.686, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[1645.696, "o", "\r\n"]
[1645.706, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[1645.716, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[1645.726, "o", "\r\n"]
[1645.736, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1645.746, "o", "            default='lasso_lars'\r\n"]
[1645.756, "o", "        The algorithm used:\r\n"]
[1645.766, "o", "\r\n"]
[1645.776, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[1645.786, "o", "          (`linear_model.lars_path`);\r\n"]
[1645.796, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1645.806, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1645.816, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1645.826, "o", "          the estimated components are sparse;\r\n"]
[1645.836, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1645.846, "o", "          solution;\r\n"]
[1645.856, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1645.866, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[1645.876, "o", "\r\n"]
[1645.886, "o", "    regularization : int or float, default=None\r\n"]
[1645.896, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[1645.906, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[1645.916, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[1645.926, "o", "\r\n"]
[1645.936, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1645.946, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[1645.956, "o", "        `algorithm='lasso_cd'`.\r\n"]
[1645.966, "o", "\r\n"]
[1645.976, "o", "    max_iter : int, default=1000\r\n"]
[1645.986, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1645.996, "o", "        `'lasso_lars'`.\r\n"]
[1646.006, "o", "\r\n"]
[1646.016, "o", "    copy_cov : bool, default=True\r\n"]
[1646.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1646.036, "o", "        be overwritten.\r\n"]
[1646.046, "o", "\r\n"]
[1646.056, "o", "    verbose : int, default=0\r\n"]
[1646.066, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[1646.076, "o", "\r\n"]
[1646.086, "o", "    positive: bool, default=False\r\n"]
[1646.096, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[1646.106, "o", "\r\n"]
[1646.116, "o", "        .. versionadded:: 0.20\r\n"]
[1646.126, "o", "\r\n"]
[1646.136, "o", "    Returns\r\n"]
[1646.146, "o", "    -------\r\n"]
[1646.156, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[1646.166, "o", "        The sparse codes.\r\n"]
[1646.176, "o", "    \"\"\"\r\n"]
[1646.186, "o", "    n_samples, n_features = X.shape\r\n"]
[1646.196, "o", "    n_components = dictionary.shape[0]\r\n"]
[1646.206, "o", "\r\n"]
[1646.216, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[1646.226, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1646.236, "o", "        try:\r\n"]
[1646.246, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1646.256, "o", "\r\n"]
[1646.266, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1646.276, "o", "            # corrects the verbosity level.\r\n"]
[1646.286, "o", "            lasso_lars = LassoLars(\r\n"]
[1646.296, "o", "                alpha=alpha,\r\n"]
[1646.306, "o", "                fit_intercept=False,\r\n"]
[1646.316, "o", "                verbose=verbose,\r\n"]
[1646.326, "o", "                precompute=gram,\r\n"]
[1646.336, "o", "                fit_path=False,\r\n"]
[1646.346, "o", "                positive=positive,\r\n"]
[1646.356, "o", "                max_iter=max_iter,\r\n"]
[1646.366, "o", "            )\r\n"]
[1646.376, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1646.386, "o", "            new_code = lasso_lars.coef_\r\n"]
[1646.396, "o", "        finally:\r\n"]
[1646.406, "o", "            np.seterr(**err_mgt)\r\n"]
[1646.416, "o", "\r\n"]
[1646.426, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[1646.436, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1646.446, "o", "\r\n"]
[1646.456, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[1646.466, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[1646.476, "o", "        # argument that we could pass in from Lasso.\r\n"]
[1646.486, "o", "        clf = Lasso(\r\n"]
[1646.496, "o", "            alpha=alpha,\r\n"]
[1646.506, "o", "            fit_intercept=False,\r\n"]
[1646.516, "o", "            precompute=gram,\r\n"]
[1646.526, "o", "            max_iter=max_iter,\r\n"]
[1646.536, "o", "            warm_start=True,\r\n"]
[1646.546, "o", "            positive=positive,\r\n"]
[1646.556, "o", "        )\r\n"]
[1646.566, "o", "\r\n"]
[1646.576, "o", "        if init is not None:\r\n"]
[1646.586, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[1646.596, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[1646.606, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[1646.616, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[1646.626, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[1646.636, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[1646.646, "o", "                init = np.array(init)\r\n"]
[1646.656, "o", "            clf.coef_ = init\r\n"]
[1646.666, "o", "\r\n"]
[1646.676, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[1646.686, "o", "        new_code = clf.coef_\r\n"]
[1646.696, "o", "\r\n"]
[1646.706, "o", "    elif algorithm == \"lars\":\r\n"]
[1646.716, "o", "        try:\r\n"]
[1646.726, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1646.736, "o", "\r\n"]
[1646.746, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1646.756, "o", "            # corrects the verbosity level.\r\n"]
[1646.766, "o", "            lars = Lars(\r\n"]
[1646.776, "o", "                fit_intercept=False,\r\n"]
[1646.786, "o", "                verbose=verbose,\r\n"]
[1646.796, "o", "                precompute=gram,\r\n"]
[1646.806, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[1646.816, "o", "                fit_path=False,\r\n"]
[1646.826, "o", "            )\r\n"]
[1646.836, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1646.846, "o", "            new_code = lars.coef_\r\n"]
[1646.856, "o", "        finally:\r\n"]
[1646.866, "o", "            np.seterr(**err_mgt)\r\n"]
[1646.876, "o", "\r\n"]
[1646.886, "o", "    elif algorithm == \"threshold\":\r\n"]
[1646.896, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[1646.906, "o", "        if positive:\r\n"]
[1646.916, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[1646.926, "o", "\r\n"]
[1646.936, "o", "    elif algorithm == \"omp\":\r\n"]
[1646.946, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[1646.956, "o", "            Gram=gram,\r\n"]
[1646.966, "o", "            Xy=cov,\r\n"]
[1646.976, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[1646.986, "o", "            tol=None,\r\n"]
[1646.996, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[1647.006, "o", "            copy_Xy=copy_cov,\r\n"]
[1647.016, "o", "        ).T\r\n"]
[1647.026, "o", "\r\n"]
[1647.036, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[1647.046, "o", "\r\n"]
[1647.056, "o", "\r\n"]
[1647.066, "o", "@validate_params(\r\n"]
[1647.076, "o", "    {\r\n"]
[1647.086, "o", "        \"X\": [\"array-like\"],\r\n"]
[1647.096, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[1647.106, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[1647.116, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[1647.126, "o", "        \"algorithm\": [\r\n"]
[1647.136, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[1647.146, "o", "        ],\r\n"]
[1647.156, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1647.166, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[1647.176, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[1647.186, "o", "        \"init\": [\"array-like\", None],\r\n"]
[1647.196, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1647.206, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[1647.216, "o", "        \"check_input\": [\"boolean\"],\r\n"]
[1647.226, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[1647.236, "o", "        \"positive\": [\"boolean\"],\r\n"]
[1647.246, "o", "    },\r\n"]
[1647.256, "o", "    prefer_skip_nested_validation=True,\r\n"]
[1647.266, "o", ")\r\n"]
[1647.276, "o", "# XXX : could be moved to the linear_model module\r\n"]
[1647.286, "o", "def sparse_encode(\r\n"]
[1647.296, "o", "    X,\r\n"]
[1647.306, "o", "    dictionary,\r\n"]
[1647.316, "o", "    *,\r\n"]
[1647.326, "o", "    gram=None,\r\n"]
[1647.336, "o", "    cov=None,\r\n"]
[1647.346, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1647.356, "o", "    n_nonzero_coefs=None,\r\n"]
[1647.366, "o", "    alpha=None,\r\n"]
[1647.376, "o", "    copy_cov=True,\r\n"]
[1647.386, "o", "    init=None,\r\n"]
[1647.396, "o", "    max_iter=1000,\r\n"]
[1647.406, "o", "    n_jobs=None,\r\n"]
[1647.416, "o", "    check_input=True,\r\n"]
[1647.426, "o", "    verbose=0,\r\n"]
[1647.436, "o", "    positive=False,\r\n"]
[1647.446, "o", "):\r\n"]
[1647.456, "o", "    \"\"\"Sparse coding.\r\n"]
[1647.466, "o", "\r\n"]
[1647.476, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[1647.486, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[1647.496, "o", "\r\n"]
[1647.506, "o", "        X ~= code * dictionary\r\n"]
[1647.516, "o", "\r\n"]
[1647.526, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[1647.536, "o", "\r\n"]
[1647.546, "o", "    Parameters\r\n"]
[1647.556, "o", "    ----------\r\n"]
[1647.566, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[1647.576, "o", "        Data matrix.\r\n"]
[1647.586, "o", "\r\n"]
[1647.596, "o", "    dictionary : array-like of shape (n_components, n_features)\r\n"]
[1647.606, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1647.616, "o", "        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[1647.626, "o", "        output.\r\n"]
[1647.636, "o", "\r\n"]
[1647.646, "o", "    gram : array-like of shape (n_components, n_components), default=None\r\n"]
[1647.656, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n"]
[1647.666, "o", "\r\n"]
[1647.676, "o", "    cov : array-like of shape (n_components, n_samples), default=None\r\n"]
[1647.686, "o", "        Precomputed covariance, `dictionary' * X`.\r\n"]
[1647.696, "o", "\r\n"]
[1647.706, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1647.716, "o", "            default='lasso_lars'\r\n"]
[1647.726, "o", "        The algorithm used:\r\n"]
[1647.736, "o", "\r\n"]
[1647.746, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[1647.756, "o", "          (`linear_model.lars_path`);\r\n"]
[1647.766, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1647.776, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1647.786, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1647.796, "o", "          the estimated components are sparse;\r\n"]
[1647.806, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1647.816, "o", "          solution;\r\n"]
[1647.826, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1647.836, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[1647.846, "o", "\r\n"]
[1647.856, "o", "    n_nonzero_coefs : int, default=None\r\n"]
[1647.866, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1647.876, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[1647.886, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[1647.896, "o", "        `n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1647.906, "o", "\r\n"]
[1647.916, "o", "    alpha : float, default=None\r\n"]
[1647.926, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1647.936, "o", "        penalty applied to the L1 norm.\r\n"]
[1647.946, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1647.956, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1647.966, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[1647.976, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[1647.986, "o", "        `n_nonzero_coefs`.\r\n"]
[1647.996, "o", "        If `None`, default to 1.\r\n"]
[1648.006, "o", "\r\n"]
[1648.016, "o", "    copy_cov : bool, default=True\r\n"]
[1648.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1648.036, "o", "        be overwritten.\r\n"]
[1648.046, "o", "\r\n"]
[1648.056, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1648.066, "o", "        Initialization value of the sparse codes. Only used if\r\n"]
[1648.076, "o", "        `algorithm='lasso_cd'`.\r\n"]
[1648.086, "o", "\r\n"]
[1648.096, "o", "    max_iter : int, default=1000\r\n"]
[1648.106, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1648.116, "o", "        `'lasso_lars'`.\r\n"]
[1648.126, "o", "\r\n"]
[1648.136, "o", "    n_jobs : int, default=None\r\n"]
[1648.146, "o", "        Number of parallel jobs to run.\r\n"]
[1648.156, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1648.166, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1648.176, "o", "        for more details.\r\n"]
[1648.186, "o", "\r\n"]
[1648.196, "o", "    check_input : bool, default=True\r\n"]
[1648.206, "o", "        If `False`, the input arrays X and dictionary will not be checked.\r\n"]
[1648.216, "o", "\r\n"]
[1648.226, "o", "    verbose : int, default=0\r\n"]
[1648.236, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[1648.246, "o", "\r\n"]
[1648.256, "o", "    positive : bool, default=False\r\n"]
[1648.266, "o", "        Whether to enforce positivity when finding the encoding.\r\n"]
[1648.276, "o", "\r\n"]
[1648.286, "o", "        .. versionadded:: 0.20\r\n"]
[1648.296, "o", "\r\n"]
[1648.306, "o", "    Returns\r\n"]
[1648.316, "o", "    -------\r\n"]
[1648.326, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1648.336, "o", "        The sparse codes.\r\n"]
[1648.346, "o", "\r\n"]
[1648.356, "o", "    See Also\r\n"]
[1648.366, "o", "    --------\r\n"]
[1648.376, "o", "    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n"]
[1648.386, "o", "        path using LARS algorithm.\r\n"]
[1648.396, "o", "    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n"]
[1648.406, "o", "    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n"]
[1648.416, "o", "    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n"]
[1648.426, "o", "        dictionary.\r\n"]
[1648.436, "o", "    \"\"\"\r\n"]
[1648.446, "o", "    if check_input:\r\n"]
[1648.456, "o", "        if algorithm == \"lasso_cd\":\r\n"]
[1648.466, "o", "            dictionary = check_array(\r\n"]
[1648.476, "o", "                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n"]
[1648.486, "o", "            )\r\n"]
[1648.496, "o", "            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n"]
[1648.506, "o", "        else:\r\n"]
[1648.516, "o", "            dictionary = check_array(dictionary)\r\n"]
[1648.526, "o", "            X = check_array(X)\r\n"]
[1648.536, "o", "\r\n"]
[1648.546, "o", "    if dictionary.shape[1] != X.shape[1]:\r\n"]
[1648.556, "o", "        raise ValueError(\r\n"]
[1648.566, "o", "            \"Dictionary and X have different numbers of features:\"\r\n"]
[1648.576, "o", "            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n"]
[1648.586, "o", "        )\r\n"]
[1648.596, "o", "\r\n"]
[1648.606, "o", "    _check_positive_coding(algorithm, positive)\r\n"]
[1648.616, "o", "\r\n"]
[1648.626, "o", "    return _sparse_encode(\r\n"]
[1648.636, "o", "        X,\r\n"]
[1648.646, "o", "        dictionary,\r\n"]
[1648.656, "o", "        gram=gram,\r\n"]
[1648.666, "o", "        cov=cov,\r\n"]
[1648.676, "o", "        algorithm=algorithm,\r\n"]
[1648.686, "o", "        n_nonzero_coefs=n_nonzero_coefs,\r\n"]
[1648.696, "o", "        alpha=alpha,\r\n"]
[1648.706, "o", "        copy_cov=copy_cov,\r\n"]
[1648.716, "o", "        init=init,\r\n"]
[1648.726, "o", "        max_iter=max_iter,\r\n"]
[1648.736, "o", "        n_jobs=n_jobs,\r\n"]
[1648.746, "o", "        verbose=verbose,\r\n"]
[1648.756, "o", "        positive=positive,\r\n"]
[1648.766, "o", "    )\r\n"]
[1648.776, "o", "\r\n"]
[1648.786, "o", "\r\n"]
[1648.796, "o", "def _sparse_encode(\r\n"]
[1648.806, "o", "    X,\r\n"]
[1648.816, "o", "    dictionary,\r\n"]
[1648.826, "o", "    *,\r\n"]
[1648.836, "o", "    gram=None,\r\n"]
[1648.846, "o", "    cov=None,\r\n"]
[1648.856, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1648.866, "o", "    n_nonzero_coefs=None,\r\n"]
[1648.876, "o", "    alpha=None,\r\n"]
[1648.886, "o", "    copy_cov=True,\r\n"]
[1648.896, "o", "    init=None,\r\n"]
[1648.906, "o", "    max_iter=1000,\r\n"]
[1648.916, "o", "    n_jobs=None,\r\n"]
[1648.926, "o", "    verbose=0,\r\n"]
[1648.936, "o", "    positive=False,\r\n"]
[1648.946, "o", "):\r\n"]
[1648.956, "o", "    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[1648.966, "o", "\r\n"]
[1648.976, "o", "    n_samples, n_features = X.shape\r\n"]
[1648.986, "o", "    n_components = dictionary.shape[0]\r\n"]
[1648.996, "o", "\r\n"]
[1649.006, "o", "    if algorithm in (\"lars\", \"omp\"):\r\n"]
[1649.016, "o", "        regularization = n_nonzero_coefs\r\n"]
[1649.026, "o", "        if regularization is None:\r\n"]
[1649.036, "o", "            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[1649.046, "o", "    else:\r\n"]
[1649.056, "o", "        regularization = alpha\r\n"]
[1649.066, "o", "        if regularization is None:\r\n"]
[1649.076, "o", "            regularization = 1.0\r\n"]
[1649.086, "o", "\r\n"]
[1649.096, "o", "    if gram is None and algorithm != \"threshold\":\r\n"]
[1649.106, "o", "        gram = np.dot(dictionary, dictionary.T)\r\n"]
[1649.116, "o", "\r\n"]
[1649.126, "o", "    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[1649.136, "o", "        copy_cov = False\r\n"]
[1649.146, "o", "        cov = np.dot(dictionary, X.T)\r\n"]
[1649.156, "o", "\r\n"]
[1649.166, "o", "    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[1649.176, "o", "        code = _sparse_encode_precomputed(\r\n"]
[1649.186, "o", "            X,\r\n"]
[1649.196, "o", "            dictionary,\r\n"]
[1649.206, "o", "            gram=gram,\r\n"]
[1649.216, "o", "            cov=cov,\r\n"]
[1649.226, "o", "            algorithm=algorithm,\r\n"]
[1649.236, "o", "            regularization=regularization,\r\n"]
[1649.246, "o", "            copy_cov=copy_cov,\r\n"]
[1649.256, "o", "            init=init,\r\n"]
[1649.266, "o", "            max_iter=max_iter,\r\n"]
[1649.276, "o", "            verbose=verbose,\r\n"]
[1649.286, "o", "            positive=positive,\r\n"]
[1649.296, "o", "        )\r\n"]
[1649.306, "o", "        return code\r\n"]
[1649.316, "o", "\r\n"]
[1649.326, "o", "    # Enter parallel code block\r\n"]
[1649.336, "o", "    n_samples = X.shape[0]\r\n"]
[1649.346, "o", "    n_components = dictionary.shape[0]\r\n"]
[1649.356, "o", "    code = np.empty((n_samples, n_components))\r\n"]
[1649.366, "o", "    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[1649.376, "o", "\r\n"]
[1649.386, "o", "    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[1649.396, "o", "        delayed(_sparse_encode_precomputed)(\r\n"]
[1649.406, "o", "            X[this_slice],\r\n"]
[1649.416, "o", "            dictionary,\r\n"]
[1649.426, "o", "            gram=gram,\r\n"]
[1649.436, "o", "            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[1649.446, "o", "            algorithm=algorithm,\r\n"]
[1649.456, "o", "            regularization=regularization,\r\n"]
[1649.466, "o", "            copy_cov=copy_cov,\r\n"]
[1649.476, "o", "            init=init[this_slice] if init is not None else None,\r\n"]
[1649.486, "o", "            max_iter=max_iter,\r\n"]
[1649.496, "o", "            verbose=verbose,\r\n"]
[1649.506, "o", "            positive=positive,\r\n"]
[1649.516, "o", "        )\r\n"]
[1649.526, "o", "        for this_slice in slices\r\n"]
[1649.536, "o", "    )\r\n"]
[1649.546, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[1649.556, "o", "        code[this_slice] = this_view\r\n"]
[1649.566, "o", "    return code\r\n"]
[1649.576, "o", "\r\n"]
[1649.586, "o", "\r\n"]
[1649.596, "o", "def _update_dict(\r\n"]
[1649.606, "o", "    dictionary,\r\n"]
[1649.616, "o", "    Y,\r\n"]
[1649.626, "o", "    code,\r\n"]
[1649.636, "o", "    A=None,\r\n"]
[1649.646, "o", "    B=None,\r\n"]
[1649.656, "o", "    verbose=False,\r\n"]
[1649.666, "o", "    random_state=None,\r\n"]
[1649.676, "o", "    positive=False,\r\n"]
[1649.686, "o", "):\r\n"]
[1649.696, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[1649.706, "o", "\r\n"]
[1649.716, "o", "    Parameters\r\n"]
[1649.726, "o", "    ----------\r\n"]
[1649.736, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1649.746, "o", "        Value of the dictionary at the previous iteration.\r\n"]
[1649.756, "o", "\r\n"]
[1649.766, "o", "    Y : ndarray of shape (n_samples, n_features)\r\n"]
[1649.776, "o", "        Data matrix.\r\n"]
[1649.786, "o", "\r\n"]
[1649.796, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1649.806, "o", "        Sparse coding of the data against which to optimize the dictionary.\r\n"]
[1649.816, "o", "\r\n"]
[1649.826, "o", "    A : ndarray of shape (n_components, n_components), default=None\r\n"]
[1649.836, "o", "        Together with `B`, sufficient stats of the online model to update the\r\n"]
[1649.846, "o", "        dictionary.\r\n"]
[1649.856, "o", "\r\n"]
[1649.866, "o", "    B : ndarray of shape (n_features, n_components), default=None\r\n"]
[1649.876, "o", "        Together with `A`, sufficient stats of the online model to update the\r\n"]
[1649.886, "o", "        dictionary.\r\n"]
[1649.896, "o", "\r\n"]
[1649.906, "o", "    verbose: bool, default=False\r\n"]
[1649.916, "o", "        Degree of output the procedure will print.\r\n"]
[1649.926, "o", "\r\n"]
[1649.936, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1650.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1650.002, "i", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r"]
[1650.004, "o", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r\n"]
[1650.016, "o", "\u001b[?2004l\r\n"]
[1650.026, "o", "    n_iter : int\r\n"]
[1650.036, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[1650.046, "o", "        set to True.\r\n"]
[1650.056, "o", "\r\n"]
[1650.066, "o", "    See Also\r\n"]
[1650.076, "o", "    --------\r\n"]
[1650.086, "o", "    dict_learning_online : Solve a dictionary learning matrix factorization\r\n"]
[1650.096, "o", "        problem online.\r\n"]
[1650.106, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1650.116, "o", "    MiniBatchDictionaryLearning : A faster, less accurate version\r\n"]
[1650.126, "o", "        of the dictionary learning algorithm.\r\n"]
[1650.136, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1650.146, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1650.156, "o", "    \"\"\"\r\n"]
[1650.166, "o", "    estimator = DictionaryLearning(\r\n"]
[1650.176, "o", "        n_components=n_components,\r\n"]
[1650.186, "o", "        alpha=alpha,\r\n"]
[1650.196, "o", "        max_iter=max_iter,\r\n"]
[1650.206, "o", "        tol=tol,\r\n"]
[1650.216, "o", "        fit_algorithm=method,\r\n"]
[1650.226, "o", "        n_jobs=n_jobs,\r\n"]
[1650.236, "o", "        dict_init=dict_init,\r\n"]
[1650.246, "o", "        callback=callback,\r\n"]
[1650.256, "o", "        code_init=code_init,\r\n"]
[1650.266, "o", "        verbose=verbose,\r\n"]
[1650.276, "o", "        random_state=random_state,\r\n"]
[1650.286, "o", "        positive_code=positive_code,\r\n"]
[1650.296, "o", "        positive_dict=positive_dict,\r\n"]
[1650.306, "o", "        transform_max_iter=method_max_iter,\r\n"]
[1650.316, "o", "    )\r\n"]
[1650.326, "o", "    code = estimator.fit_transform(X)\r\n"]
[1650.336, "o", "    if return_n_iter:\r\n"]
[1650.346, "o", "        return (\r\n"]
[1650.356, "o", "            code,\r\n"]
[1650.366, "o", "            estimator.components_,\r\n"]
[1650.376, "o", "            estimator.error_,\r\n"]
[1650.386, "o", "            estimator.n_iter_,\r\n"]
[1650.396, "o", "        )\r\n"]
[1650.406, "o", "    return code, estimator.components_, estimator.error_\r\n"]
[1650.416, "o", "\r\n"]
[1650.426, "o", "\r\n"]
[1650.436, "o", "class _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n"]
[1650.446, "o", "    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n"]
[1650.456, "o", "\r\n"]
[1650.466, "o", "    def __init__(\r\n"]
[1650.476, "o", "        self,\r\n"]
[1650.486, "o", "        transform_algorithm,\r\n"]
[1650.496, "o", "        transform_n_nonzero_coefs,\r\n"]
[1650.506, "o", "        transform_alpha,\r\n"]
[1650.516, "o", "        split_sign,\r\n"]
[1650.526, "o", "        n_jobs,\r\n"]
[1650.536, "o", "        positive_code,\r\n"]
[1650.546, "o", "        transform_max_iter,\r\n"]
[1650.556, "o", "    ):\r\n"]
[1650.566, "o", "        self.transform_algorithm = transform_algorithm\r\n"]
[1650.576, "o", "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n"]
[1650.586, "o", "        self.transform_alpha = transform_alpha\r\n"]
[1650.596, "o", "        self.transform_max_iter = transform_max_iter\r\n"]
[1650.606, "o", "        self.split_sign = split_sign\r\n"]
[1650.616, "o", "        self.n_jobs = n_jobs\r\n"]
[1650.626, "o", "        self.positive_code = positive_code\r\n"]
[1650.636, "o", "\r\n"]
[1650.646, "o", "    def _transform(self, X, dictionary):\r\n"]
[1650.656, "o", "        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n"]
[1650.666, "o", "        SparseCoder.\"\"\"\r\n"]
[1650.676, "o", "        X = self._validate_data(X, reset=False)\r\n"]
[1650.686, "o", "\r\n"]
[1650.696, "o", "        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n"]
[1650.706, "o", "            transform_alpha = self.alpha\r\n"]
[1650.716, "o", "        else:\r\n"]
[1650.726, "o", "            transform_alpha = self.transform_alpha\r\n"]
[1650.736, "o", "\r\n"]
[1650.746, "o", "        code = sparse_encode(\r\n"]
[1650.756, "o", "            X,\r\n"]
[1650.766, "o", "            dictionary,\r\n"]
[1650.776, "o", "            algorithm=self.transform_algorithm,\r\n"]
[1650.786, "o", "            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n"]
[1650.796, "o", "            alpha=transform_alpha,\r\n"]
[1650.806, "o", "            max_iter=self.transform_max_iter,\r\n"]
[1650.816, "o", "            n_jobs=self.n_jobs,\r\n"]
[1650.826, "o", "            positive=self.positive_code,\r\n"]
[1650.836, "o", "        )\r\n"]
[1650.846, "o", "\r\n"]
[1650.856, "o", "        if self.split_sign:\r\n"]
[1650.866, "o", "            # feature vector is split into a positive and negative side\r\n"]
[1650.876, "o", "            n_samples, n_features = code.shape\r\n"]
[1650.886, "o", "            split_code = np.empty((n_samples, 2 * n_features))\r\n"]
[1650.896, "o", "            split_code[:, :n_features] = np.maximum(code, 0)\r\n"]
[1650.906, "o", "            split_code[:, n_features:] = -np.minimum(code, 0)\r\n"]
[1650.916, "o", "            code = split_code\r\n"]
[1650.926, "o", "\r\n"]
[1650.936, "o", "        return code\r\n"]
[1650.946, "o", "\r\n"]
[1650.956, "o", "    def transform(self, X):\r\n"]
[1650.966, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[1650.976, "o", "\r\n"]
[1650.986, "o", "        Coding method is determined by the object parameter\r\n"]
[1650.996, "o", "        `transform_algorithm`.\r\n"]
[1651.006, "o", "\r\n"]
[1651.016, "o", "        Parameters\r\n"]
[1651.026, "o", "        ----------\r\n"]
[1651.036, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[1651.046, "o", "            Test data to be transformed, must have the same number of\r\n"]
[1651.056, "o", "            features as the data used to train the model.\r\n"]
[1651.066, "o", "\r\n"]
[1651.076, "o", "        Returns\r\n"]
[1651.086, "o", "        -------\r\n"]
[1651.096, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[1651.106, "o", "            Transformed data.\r\n"]
[1651.116, "o", "        \"\"\"\r\n"]
[1651.126, "o", "        check_is_fitted(self)\r\n"]
[1651.136, "o", "        return self._transform(X, self.components_)\r\n"]
[1651.146, "o", "\r\n"]
[1651.156, "o", "\r\n"]
[1651.166, "o", "class SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n"]
[1651.176, "o", "    \"\"\"Sparse coding.\r\n"]
[1651.186, "o", "\r\n"]
[1651.196, "o", "    Finds a sparse representation of data against a fixed, precomputed\r\n"]
[1651.206, "o", "    dictionary.\r\n"]
[1651.216, "o", "\r\n"]
[1651.226, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[1651.236, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[1651.246, "o", "\r\n"]
[1651.256, "o", "        X ~= code * dictionary\r\n"]
[1651.266, "o", "\r\n"]
[1651.276, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[1651.286, "o", "\r\n"]
[1651.296, "o", "    Parameters\r\n"]
[1651.306, "o", "    ----------\r\n"]
[1651.316, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1651.326, "o", "        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n"]
[1651.336, "o", "        normalized to unit norm.\r\n"]
[1651.346, "o", "\r\n"]
[1651.356, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[1651.366, "o", "            'threshold'}, default='omp'\r\n"]
[1651.376, "o", "        Algorithm used to transform the data:\r\n"]
[1651.386, "o", "\r\n"]
[1651.396, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[1651.406, "o", "          (`linear_model.lars_path`);\r\n"]
[1651.416, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1651.426, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1651.436, "o", "          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n"]
[1651.446, "o", "          the estimated components are sparse;\r\n"]
[1651.456, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1651.466, "o", "          solution;\r\n"]
[1651.476, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[1651.486, "o", "          the projection ``dictionary * X'``.\r\n"]
[1651.496, "o", "\r\n"]
[1651.506, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[1651.516, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1651.526, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[1651.536, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[1651.546, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1651.556, "o", "\r\n"]
[1651.566, "o", "    transform_alpha : float, default=None\r\n"]
[1651.576, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1651.586, "o", "        penalty applied to the L1 norm.\r\n"]
[1651.596, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1651.606, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1651.616, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[1651.626, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[1651.636, "o", "        `n_nonzero_coefs`.\r\n"]
[1651.646, "o", "        If `None`, default to 1.\r\n"]
[1651.656, "o", "\r\n"]
[1651.666, "o", "    split_sign : bool, default=False\r\n"]
[1651.676, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[1651.686, "o", "        its negative part and its positive part. This can improve the\r\n"]
[1651.696, "o", "        performance of downstream classifiers.\r\n"]
[1651.706, "o", "\r\n"]
[1651.716, "o", "    n_jobs : int, default=None\r\n"]
[1651.726, "o", "        Number of parallel jobs to run.\r\n"]
[1651.736, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1651.746, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1651.756, "o", "        for more details.\r\n"]
[1651.766, "o", "\r\n"]
[1651.776, "o", "    positive_code : bool, default=False\r\n"]
[1651.786, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1651.796, "o", "\r\n"]
[1651.806, "o", "        .. versionadded:: 0.20\r\n"]
[1651.816, "o", "\r\n"]
[1651.826, "o", "    transform_max_iter : int, default=1000\r\n"]
[1651.836, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1651.846, "o", "        `lasso_lars`.\r\n"]
[1651.856, "o", "\r\n"]
[1651.866, "o", "        .. versionadded:: 0.22\r\n"]
[1651.876, "o", "\r\n"]
[1651.886, "o", "    Attributes\r\n"]
[1651.896, "o", "    ----------\r\n"]
[1651.906, "o", "    n_components_ : int\r\n"]
[1651.916, "o", "        Number of atoms.\r\n"]
[1651.926, "o", "\r\n"]
[1651.936, "o", "    n_features_in_ : int\r\n"]
[1651.946, "o", "        Number of features seen during :term:`fit`.\r\n"]
[1651.956, "o", "\r\n"]
[1651.966, "o", "        .. versionadded:: 0.24\r\n"]
[1651.976, "o", "\r\n"]
[1651.986, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[1651.996, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[1652.006, "o", "        has feature names that are all strings.\r\n"]
[1652.016, "o", "\r\n"]
[1652.026, "o", "        .. versionadded:: 1.0\r\n"]
[1652.036, "o", "\r\n"]
[1652.046, "o", "    See Also\r\n"]
[1652.056, "o", "    --------\r\n"]
[1652.066, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1652.076, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n"]
[1652.086, "o", "        dictionary learning algorithm.\r\n"]
[1652.096, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1652.106, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1652.116, "o", "    sparse_encode : Sparse coding where each row of the result is the solution\r\n"]
[1652.126, "o", "        to a sparse coding problem.\r\n"]
[1652.136, "o", "\r\n"]
[1652.146, "o", "    Examples\r\n"]
[1652.156, "o", "    --------\r\n"]
[1652.166, "o", "    >>> import numpy as np\r\n"]
[1652.176, "o", "    >>> from sklearn.decomposition import SparseCoder\r\n"]
[1652.186, "o", "    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n"]
[1652.196, "o", "    >>> dictionary = np.array(\r\n"]
[1652.206, "o", "    ...     [[0, 1, 0],\r\n"]
[1652.216, "o", "    ...      [-1, -1, 2],\r\n"]
[1652.226, "o", "    ...      [1, 1, 1],\r\n"]
[1652.236, "o", "    ...      [0, 1, 1],\r\n"]
[1652.246, "o", "    ...      [0, 2, 1]],\r\n"]
[1652.256, "o", "    ...    dtype=np.float64\r\n"]
[1652.266, "o", "    ... )\r\n"]
[1652.276, "o", "    >>> coder = SparseCoder(\r\n"]
[1652.286, "o", "    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n"]
[1652.296, "o", "    ...     transform_alpha=1e-10,\r\n"]
[1652.306, "o", "    ... )\r\n"]
[1652.316, "o", "    >>> coder.transform(X)\r\n"]
[1652.326, "o", "    array([[ 0.,  0., -1.,  0.,  0.],\r\n"]
[1652.336, "o", "           [ 0.,  1.,  1.,  0.,  0.]])\r\n"]
[1652.346, "o", "    \"\"\"\r\n"]
[1652.356, "o", "\r\n"]
[1652.366, "o", "    _required_parameters = [\"dictionary\"]\r\n"]
[1652.376, "o", "\r\n"]
[1652.386, "o", "    def __init__(\r\n"]
[1652.396, "o", "        self,\r\n"]
[1652.406, "o", "        dictionary,\r\n"]
[1652.416, "o", "        *,\r\n"]
[1652.426, "o", "        transform_algorithm=\"omp\",\r\n"]
[1652.436, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[1652.446, "o", "        transform_alpha=None,\r\n"]
[1652.456, "o", "        split_sign=False,\r\n"]
[1652.466, "o", "        n_jobs=None,\r\n"]
[1652.476, "o", "        positive_code=False,\r\n"]
[1652.486, "o", "        transform_max_iter=1000,\r\n"]
[1652.496, "o", "    ):\r\n"]
[1652.506, "o", "        super().__init__(\r\n"]
[1652.516, "o", "            transform_algorithm,\r\n"]
[1652.526, "o", "            transform_n_nonzero_coefs,\r\n"]
[1652.536, "o", "            transform_alpha,\r\n"]
[1652.546, "o", "            split_sign,\r\n"]
[1652.556, "o", "            n_jobs,\r\n"]
[1652.566, "o", "            positive_code,\r\n"]
[1652.576, "o", "            transform_max_iter,\r\n"]
[1652.586, "o", "        )\r\n"]
[1652.596, "o", "        self.dictionary = dictionary\r\n"]
[1652.606, "o", "\r\n"]
[1652.616, "o", "    def fit(self, X, y=None):\r\n"]
[1652.626, "o", "        \"\"\"Do nothing and return the estimator unchanged.\r\n"]
[1652.636, "o", "\r\n"]
[1652.646, "o", "        This method is just there to implement the usual API and hence\r\n"]
[1652.656, "o", "        work in pipelines.\r\n"]
[1652.666, "o", "\r\n"]
[1652.676, "o", "        Parameters\r\n"]
[1652.686, "o", "        ----------\r\n"]
[1652.696, "o", "        X : Ignored\r\n"]
[1652.706, "o", "            Not used, present for API consistency by convention.\r\n"]
[1652.716, "o", "\r\n"]
[1652.726, "o", "        y : Ignored\r\n"]
[1652.736, "o", "            Not used, present for API consistency by convention.\r\n"]
[1652.746, "o", "\r\n"]
[1652.756, "o", "        Returns\r\n"]
[1652.766, "o", "        -------\r\n"]
[1652.776, "o", "        self : object\r\n"]
[1652.786, "o", "            Returns the instance itself.\r\n"]
[1652.796, "o", "        \"\"\"\r\n"]
[1652.806, "o", "        return self\r\n"]
[1652.816, "o", "\r\n"]
[1652.826, "o", "    def transform(self, X, y=None):\r\n"]
[1652.836, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[1652.846, "o", "\r\n"]
[1652.856, "o", "        Coding method is determined by the object parameter\r\n"]
[1652.866, "o", "        `transform_algorithm`.\r\n"]
[1652.876, "o", "\r\n"]
[1652.886, "o", "        Parameters\r\n"]
[1652.896, "o", "        ----------\r\n"]
[1652.906, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[1652.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1652.926, "o", "            and `n_features` is the number of features.\r\n"]
[1652.936, "o", "\r\n"]
[1652.946, "o", "        y : Ignored\r\n"]
[1652.956, "o", "            Not used, present for API consistency by convention.\r\n"]
[1652.966, "o", "\r\n"]
[1652.976, "o", "        Returns\r\n"]
[1652.986, "o", "        -------\r\n"]
[1652.996, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[1653.006, "o", "            Transformed data.\r\n"]
[1653.016, "o", "        \"\"\"\r\n"]
[1653.026, "o", "        return super()._transform(X, self.dictionary)\r\n"]
[1653.036, "o", "\r\n"]
[1653.046, "o", "    def _more_tags(self):\r\n"]
[1653.056, "o", "        return {\r\n"]
[1653.066, "o", "            \"requires_fit\": False,\r\n"]
[1653.076, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[1653.086, "o", "        }\r\n"]
[1653.096, "o", "\r\n"]
[1653.106, "o", "    @property\r\n"]
[1653.116, "o", "    def n_components_(self):\r\n"]
[1653.126, "o", "        \"\"\"Number of atoms.\"\"\"\r\n"]
[1653.136, "o", "        return self.dictionary.shape[0]\r\n"]
[1653.146, "o", "\r\n"]
[1653.156, "o", "    @property\r\n"]
[1653.166, "o", "    def n_features_in_(self):\r\n"]
[1653.176, "o", "        \"\"\"Number of features seen during `fit`.\"\"\"\r\n"]
[1653.186, "o", "        return self.dictionary.shape[1]\r\n"]
[1653.196, "o", "\r\n"]
[1653.206, "o", "    @property\r\n"]
[1653.216, "o", "    def _n_features_out(self):\r\n"]
[1653.226, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[1653.236, "o", "        return self.n_components_\r\n"]
[1653.246, "o", "\r\n"]
[1653.256, "o", "\r\n"]
[1653.266, "o", "class DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[1653.276, "o", "    \"\"\"Dictionary learning.\r\n"]
[1653.286, "o", "\r\n"]
[1653.296, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[1653.306, "o", "    encoding the fitted data.\r\n"]
[1653.316, "o", "\r\n"]
[1653.326, "o", "    Solves the optimization problem::\r\n"]
[1653.336, "o", "\r\n"]
[1653.346, "o", "        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[1653.356, "o", "                    (U,V)\r\n"]
[1653.366, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[1653.376, "o", "\r\n"]
[1653.386, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[1653.396, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[1653.406, "o", "    of all the entries in the matrix.\r\n"]
[1653.416, "o", "\r\n"]
[1653.426, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[1653.436, "o", "\r\n"]
[1653.446, "o", "    Parameters\r\n"]
[1653.456, "o", "    ----------\r\n"]
[1653.466, "o", "    n_components : int, default=None\r\n"]
[1653.476, "o", "        Number of dictionary elements to extract. If None, then ``n_components``\r\n"]
[1653.486, "o", "        is set to ``n_features``.\r\n"]
[1653.496, "o", "\r\n"]
[1653.506, "o", "    alpha : float, default=1.0\r\n"]
[1653.516, "o", "        Sparsity controlling parameter.\r\n"]
[1653.526, "o", "\r\n"]
[1653.536, "o", "    max_iter : int, default=1000\r\n"]
[1653.546, "o", "        Maximum number of iterations to perform.\r\n"]
[1653.556, "o", "\r\n"]
[1653.566, "o", "    tol : float, default=1e-8\r\n"]
[1653.576, "o", "        Tolerance for numerical error.\r\n"]
[1653.586, "o", "\r\n"]
[1653.596, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[1653.606, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[1653.616, "o", "          problem (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[1653.626, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[1653.636, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n"]
[1653.646, "o", "          faster if the estimated components are sparse.\r\n"]
[1653.656, "o", "\r\n"]
[1653.666, "o", "        .. versionadded:: 0.17\r\n"]
[1653.676, "o", "           *cd* coordinate descent method to improve speed.\r\n"]
[1653.686, "o", "\r\n"]
[1653.696, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[1653.706, "o", "            'threshold'}, default='omp'\r\n"]
[1653.716, "o", "        Algorithm used to transform the data:\r\n"]
[1653.726, "o", "\r\n"]
[1653.736, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[1653.746, "o", "          (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[1653.756, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[1653.766, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1653.776, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n"]
[1653.786, "o", "          will be faster if the estimated components are sparse.\r\n"]
[1653.796, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1653.806, "o", "          solution.\r\n"]
[1653.816, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[1653.826, "o", "          the projection ``dictionary * X'``.\r\n"]
[1653.836, "o", "\r\n"]
[1653.846, "o", "        .. versionadded:: 0.17\r\n"]
[1653.856, "o", "           *lasso_cd* coordinate descent method to improve speed.\r\n"]
[1653.866, "o", "\r\n"]
[1653.876, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[1653.886, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1653.896, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[1653.906, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[1653.916, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1653.926, "o", "\r\n"]
[1653.936, "o", "    transform_alpha : float, default=None\r\n"]
[1653.946, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1653.956, "o", "        penalty applied to the L1 norm.\r\n"]
[1653.966, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1653.976, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1653.986, "o", "        If `None`, defaults to `alpha`.\r\n"]
[1653.996, "o", "\r\n"]
[1654.006, "o", "        .. versionchanged:: 1.2\r\n"]
[1654.016, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[1654.026, "o", "\r\n"]
[1654.036, "o", "    n_jobs : int or None, default=None\r\n"]
[1654.046, "o", "        Number of parallel jobs to run.\r\n"]
[1654.056, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1654.066, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1654.076, "o", "        for more details.\r\n"]
[1654.086, "o", "\r\n"]
[1654.096, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1654.106, "o", "        Initial value for the code, for warm restart. Only used if `code_init`\r\n"]
[1654.116, "o", "        and `dict_init` are not None.\r\n"]
[1654.126, "o", "\r\n"]
[1654.136, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1654.146, "o", "        Initial values for the dictionary, for warm restart. Only used if\r\n"]
[1654.156, "o", "        `code_init` and `dict_init` are not None.\r\n"]
[1654.166, "o", "\r\n"]
[1654.176, "o", "    callback : callable, default=None\r\n"]
[1654.186, "o", "        Callable that gets invoked every five iterations.\r\n"]
[1654.196, "o", "\r\n"]
[1654.206, "o", "        .. versionadded:: 1.3\r\n"]
[1654.216, "o", "\r\n"]
[1654.226, "o", "    verbose : bool, default=False\r\n"]
[1654.236, "o", "        To control the verbosity of the procedure.\r\n"]
[1654.246, "o", "\r\n"]
[1654.256, "o", "    split_sign : bool, default=False\r\n"]
[1654.266, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[1654.276, "o", "        its negative part and its positive part. This can improve the\r\n"]
[1654.286, "o", "        performance of downstream classifiers.\r\n"]
[1654.296, "o", "\r\n"]
[1654.306, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1654.316, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[1654.326, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[1654.336, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[1654.346, "o", "        results across multiple function calls.\r\n"]
[1654.356, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1654.366, "o", "\r\n"]
[1654.376, "o", "    positive_code : bool, default=False\r\n"]
[1654.386, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1654.396, "o", "\r\n"]
[1654.406, "o", "        .. versionadded:: 0.20\r\n"]
[1654.416, "o", "\r\n"]
[1654.426, "o", "    positive_dict : bool, default=False\r\n"]
[1654.436, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1654.446, "o", "\r\n"]
[1654.456, "o", "        .. versionadded:: 0.20\r\n"]
[1654.466, "o", "\r\n"]
[1654.476, "o", "    transform_max_iter : int, default=1000\r\n"]
[1654.486, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1654.496, "o", "        `'lasso_lars'`.\r\n"]
[1654.506, "o", "\r\n"]
[1654.516, "o", "        .. versionadded:: 0.22\r\n"]
[1654.526, "o", "\r\n"]
[1654.536, "o", "    Attributes\r\n"]
[1654.546, "o", "    ----------\r\n"]
[1654.556, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[1654.566, "o", "        dictionary atoms extracted from the data\r\n"]
[1654.576, "o", "\r\n"]
[1654.586, "o", "    error_ : array\r\n"]
[1654.596, "o", "        vector of errors at each iteration\r\n"]
[1654.606, "o", "\r\n"]
[1654.616, "o", "    n_features_in_ : int\r\n"]
[1654.626, "o", "        Number of features seen during :term:`fit`.\r\n"]
[1654.636, "o", "\r\n"]
[1654.646, "o", "        .. versionadded:: 0.24\r\n"]
[1654.656, "o", "\r\n"]
[1654.666, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[1654.676, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[1654.686, "o", "        has feature names that are all strings.\r\n"]
[1654.696, "o", "\r\n"]
[1654.706, "o", "        .. versionadded:: 1.0\r\n"]
[1654.716, "o", "\r\n"]
[1654.726, "o", "    n_iter_ : int\r\n"]
[1654.736, "o", "        Number of iterations run.\r\n"]
[1654.746, "o", "\r\n"]
[1654.756, "o", "    See Also\r\n"]
[1654.766, "o", "    --------\r\n"]
[1654.776, "o", "    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n"]
[1654.786, "o", "        dictionary learning algorithm.\r\n"]
[1654.796, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1654.806, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[1654.816, "o", "        precomputed dictionary.\r\n"]
[1654.826, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1654.836, "o", "\r\n"]
[1654.846, "o", "    References\r\n"]
[1654.856, "o", "    ----------\r\n"]
[1654.866, "o", "\r\n"]
[1654.876, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[1654.886, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[1654.896, "o", "\r\n"]
[1654.906, "o", "    Examples\r\n"]
[1654.916, "o", "    --------\r\n"]
[1654.926, "o", "    >>> import numpy as np\r\n"]
[1654.936, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[1655.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1655.002, "i", "sed -n '2000,2800p' sklearn/decomposition/_dict_learning.py\r"]
[1655.004, "o", "sed -n '2000,2800p' sklearn/decomposition/_dict_learning.py\r\n"]
[1655.016, "o", "\u001b[?2004l\r\n"]
[1655.026, "o", "\r\n"]
[1655.036, "o", "    tol : float, default=1e-3\r\n"]
[1655.046, "o", "        Control early stopping based on the norm of the differences in the\r\n"]
[1655.056, "o", "        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n"]
[1655.066, "o", "\r\n"]
[1655.076, "o", "        To disable early stopping based on changes in the dictionary, set\r\n"]
[1655.086, "o", "        `tol` to 0.0.\r\n"]
[1655.096, "o", "\r\n"]
[1655.106, "o", "        .. versionadded:: 1.1\r\n"]
[1655.116, "o", "\r\n"]
[1655.126, "o", "    max_no_improvement : int, default=10\r\n"]
[1655.136, "o", "        Control early stopping based on the consecutive number of mini batches\r\n"]
[1655.146, "o", "        that does not yield an improvement on the smoothed cost function. Used only if\r\n"]
[1655.156, "o", "        `max_iter` is not None.\r\n"]
[1655.166, "o", "\r\n"]
[1655.176, "o", "        To disable convergence detection based on cost function, set\r\n"]
[1655.186, "o", "        `max_no_improvement` to None.\r\n"]
[1655.196, "o", "\r\n"]
[1655.206, "o", "        .. versionadded:: 1.1\r\n"]
[1655.216, "o", "\r\n"]
[1655.226, "o", "    Attributes\r\n"]
[1655.236, "o", "    ----------\r\n"]
[1655.246, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[1655.256, "o", "        Components extracted from the data.\r\n"]
[1655.266, "o", "\r\n"]
[1655.276, "o", "    n_features_in_ : int\r\n"]
[1655.286, "o", "        Number of features seen during :term:`fit`.\r\n"]
[1655.296, "o", "\r\n"]
[1655.306, "o", "        .. versionadded:: 0.24\r\n"]
[1655.316, "o", "\r\n"]
[1655.326, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[1655.336, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[1655.346, "o", "        has feature names that are all strings.\r\n"]
[1655.356, "o", "\r\n"]
[1655.366, "o", "        .. versionadded:: 1.0\r\n"]
[1655.376, "o", "\r\n"]
[1655.386, "o", "    n_iter_ : int\r\n"]
[1655.396, "o", "        Number of iterations over the full dataset.\r\n"]
[1655.406, "o", "\r\n"]
[1655.416, "o", "    n_steps_ : int\r\n"]
[1655.426, "o", "        Number of mini-batches processed.\r\n"]
[1655.436, "o", "\r\n"]
[1655.446, "o", "        .. versionadded:: 1.1\r\n"]
[1655.456, "o", "\r\n"]
[1655.466, "o", "    See Also\r\n"]
[1655.476, "o", "    --------\r\n"]
[1655.486, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1655.496, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1655.506, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[1655.516, "o", "        precomputed dictionary.\r\n"]
[1655.526, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1655.536, "o", "\r\n"]
[1655.546, "o", "    References\r\n"]
[1655.556, "o", "    ----------\r\n"]
[1655.566, "o", "\r\n"]
[1655.576, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[1655.586, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[1655.596, "o", "\r\n"]
[1655.606, "o", "    Examples\r\n"]
[1655.616, "o", "    --------\r\n"]
[1655.626, "o", "    >>> import numpy as np\r\n"]
[1655.636, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[1655.646, "o", "    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n"]
[1655.656, "o", "    >>> X, dictionary, code = make_sparse_coded_signal(\r\n"]
[1655.666, "o", "    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n"]
[1655.676, "o", "    ...     random_state=42)\r\n"]
[1655.686, "o", "    >>> dict_learner = MiniBatchDictionaryLearning(\r\n"]
[1655.696, "o", "    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n"]
[1655.706, "o", "    ...     transform_alpha=0.1, random_state=42)\r\n"]
[1655.716, "o", "    >>> X_transformed = dict_learner.fit_transform(X)\r\n"]
[1655.726, "o", "\r\n"]
[1655.736, "o", "    We can check the level of sparsity of `X_transformed`:\r\n"]
[1655.746, "o", "\r\n"]
[1655.756, "o", "    >>> np.mean(X_transformed == 0) < 0.5\r\n"]
[1655.766, "o", "    True\r\n"]
[1655.776, "o", "\r\n"]
[1655.786, "o", "    We can compare the average squared euclidean norm of the reconstruction\r\n"]
[1655.796, "o", "    error of the sparse coded signal relative to the squared euclidean norm of\r\n"]
[1655.806, "o", "    the original signal:\r\n"]
[1655.816, "o", "\r\n"]
[1655.826, "o", "    >>> X_hat = X_transformed @ dict_learner.components_\r\n"]
[1655.836, "o", "    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n"]
[1655.846, "o", "    0.057...\r\n"]
[1655.856, "o", "    \"\"\"\r\n"]
[1655.866, "o", "\r\n"]
[1655.876, "o", "    _parameter_constraints: dict = {\r\n"]
[1655.886, "o", "        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1655.896, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1655.906, "o", "        \"n_iter\": [\r\n"]
[1655.916, "o", "            Interval(Integral, 0, None, closed=\"left\"),\r\n"]
[1655.926, "o", "            Hidden(StrOptions({\"deprecated\"})),\r\n"]
[1655.936, "o", "        ],\r\n"]
[1655.946, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n"]
[1655.956, "o", "        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n"]
[1655.966, "o", "        \"n_jobs\": [None, Integral],\r\n"]
[1655.976, "o", "        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n"]
[1655.986, "o", "        \"shuffle\": [\"boolean\"],\r\n"]
[1655.996, "o", "        \"dict_init\": [None, np.ndarray],\r\n"]
[1656.006, "o", "        \"transform_algorithm\": [\r\n"]
[1656.016, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[1656.026, "o", "        ],\r\n"]
[1656.036, "o", "        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1656.046, "o", "        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[1656.056, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[1656.066, "o", "        \"split_sign\": [\"boolean\"],\r\n"]
[1656.076, "o", "        \"random_state\": [\"random_state\"],\r\n"]
[1656.086, "o", "        \"positive_code\": [\"boolean\"],\r\n"]
[1656.096, "o", "        \"positive_dict\": [\"boolean\"],\r\n"]
[1656.106, "o", "        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1656.116, "o", "        \"callback\": [None, callable],\r\n"]
[1656.126, "o", "        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n"]
[1656.136, "o", "        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n"]
[1656.146, "o", "    }\r\n"]
[1656.156, "o", "\r\n"]
[1656.166, "o", "    def __init__(\r\n"]
[1656.176, "o", "        self,\r\n"]
[1656.186, "o", "        n_components=None,\r\n"]
[1656.196, "o", "        *,\r\n"]
[1656.206, "o", "        alpha=1,\r\n"]
[1656.216, "o", "        n_iter=\"deprecated\",\r\n"]
[1656.226, "o", "        max_iter=None,\r\n"]
[1656.236, "o", "        fit_algorithm=\"lars\",\r\n"]
[1656.246, "o", "        n_jobs=None,\r\n"]
[1656.256, "o", "        batch_size=256,\r\n"]
[1656.266, "o", "        shuffle=True,\r\n"]
[1656.276, "o", "        dict_init=None,\r\n"]
[1656.286, "o", "        transform_algorithm=\"omp\",\r\n"]
[1656.296, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[1656.306, "o", "        transform_alpha=None,\r\n"]
[1656.316, "o", "        verbose=False,\r\n"]
[1656.326, "o", "        split_sign=False,\r\n"]
[1656.336, "o", "        random_state=None,\r\n"]
[1656.346, "o", "        positive_code=False,\r\n"]
[1656.356, "o", "        positive_dict=False,\r\n"]
[1656.366, "o", "        transform_max_iter=1000,\r\n"]
[1656.376, "o", "        callback=None,\r\n"]
[1656.386, "o", "        tol=1e-3,\r\n"]
[1656.396, "o", "        max_no_improvement=10,\r\n"]
[1656.406, "o", "    ):\r\n"]
[1656.416, "o", "        super().__init__(\r\n"]
[1656.426, "o", "            transform_algorithm,\r\n"]
[1656.436, "o", "            transform_n_nonzero_coefs,\r\n"]
[1656.446, "o", "            transform_alpha,\r\n"]
[1656.456, "o", "            split_sign,\r\n"]
[1656.466, "o", "            n_jobs,\r\n"]
[1656.476, "o", "            positive_code,\r\n"]
[1656.486, "o", "            transform_max_iter,\r\n"]
[1656.496, "o", "        )\r\n"]
[1656.506, "o", "        self.n_components = n_components\r\n"]
[1656.516, "o", "        self.alpha = alpha\r\n"]
[1656.526, "o", "        self.n_iter = n_iter\r\n"]
[1656.536, "o", "        self.max_iter = max_iter\r\n"]
[1656.546, "o", "        self.fit_algorithm = fit_algorithm\r\n"]
[1656.556, "o", "        self.dict_init = dict_init\r\n"]
[1656.566, "o", "        self.verbose = verbose\r\n"]
[1656.576, "o", "        self.shuffle = shuffle\r\n"]
[1656.586, "o", "        self.batch_size = batch_size\r\n"]
[1656.596, "o", "        self.split_sign = split_sign\r\n"]
[1656.606, "o", "        self.random_state = random_state\r\n"]
[1656.616, "o", "        self.positive_dict = positive_dict\r\n"]
[1656.626, "o", "        self.callback = callback\r\n"]
[1656.636, "o", "        self.max_no_improvement = max_no_improvement\r\n"]
[1656.646, "o", "        self.tol = tol\r\n"]
[1656.656, "o", "\r\n"]
[1656.666, "o", "    def _check_params(self, X):\r\n"]
[1656.676, "o", "        # n_components\r\n"]
[1656.686, "o", "        self._n_components = self.n_components\r\n"]
[1656.696, "o", "        if self._n_components is None:\r\n"]
[1656.706, "o", "            self._n_components = X.shape[1]\r\n"]
[1656.716, "o", "\r\n"]
[1656.726, "o", "        # fit_algorithm\r\n"]
[1656.736, "o", "        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n"]
[1656.746, "o", "        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n"]
[1656.756, "o", "\r\n"]
[1656.766, "o", "        # batch_size\r\n"]
[1656.776, "o", "        self._batch_size = min(self.batch_size, X.shape[0])\r\n"]
[1656.786, "o", "\r\n"]
[1656.796, "o", "    def _initialize_dict(self, X, random_state):\r\n"]
[1656.806, "o", "        \"\"\"Initialization of the dictionary.\"\"\"\r\n"]
[1656.816, "o", "        if self.dict_init is not None:\r\n"]
[1656.826, "o", "            dictionary = self.dict_init\r\n"]
[1656.836, "o", "        else:\r\n"]
[1656.846, "o", "            # Init V with SVD of X\r\n"]
[1656.856, "o", "            _, S, dictionary = randomized_svd(\r\n"]
[1656.866, "o", "                X, self._n_components, random_state=random_state\r\n"]
[1656.876, "o", "            )\r\n"]
[1656.886, "o", "            dictionary = S[:, np.newaxis] * dictionary\r\n"]
[1656.896, "o", "\r\n"]
[1656.906, "o", "        if self._n_components <= len(dictionary):\r\n"]
[1656.916, "o", "            dictionary = dictionary[: self._n_components, :]\r\n"]
[1656.926, "o", "        else:\r\n"]
[1656.936, "o", "            dictionary = np.concatenate(\r\n"]
[1656.946, "o", "                (\r\n"]
[1656.956, "o", "                    dictionary,\r\n"]
[1656.966, "o", "                    np.zeros(\r\n"]
[1656.976, "o", "                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n"]
[1656.986, "o", "                        dtype=dictionary.dtype,\r\n"]
[1656.996, "o", "                    ),\r\n"]
[1657.006, "o", "                )\r\n"]
[1657.016, "o", "            )\r\n"]
[1657.026, "o", "\r\n"]
[1657.036, "o", "        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n"]
[1657.046, "o", "        dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[1657.056, "o", "\r\n"]
[1657.066, "o", "        return dictionary\r\n"]
[1657.076, "o", "\r\n"]
[1657.086, "o", "    def _update_inner_stats(self, X, code, batch_size, step):\r\n"]
[1657.096, "o", "        \"\"\"Update the inner stats inplace.\"\"\"\r\n"]
[1657.106, "o", "        if step < batch_size - 1:\r\n"]
[1657.116, "o", "            theta = (step + 1) * batch_size\r\n"]
[1657.126, "o", "        else:\r\n"]
[1657.136, "o", "            theta = batch_size**2 + step + 1 - batch_size\r\n"]
[1657.146, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[1657.156, "o", "\r\n"]
[1657.166, "o", "        self._A *= beta\r\n"]
[1657.176, "o", "        self._A += code.T @ code / batch_size\r\n"]
[1657.186, "o", "        self._B *= beta\r\n"]
[1657.196, "o", "        self._B += X.T @ code / batch_size\r\n"]
[1657.206, "o", "\r\n"]
[1657.216, "o", "    def _minibatch_step(self, X, dictionary, random_state, step):\r\n"]
[1657.226, "o", "        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n"]
[1657.236, "o", "        batch_size = X.shape[0]\r\n"]
[1657.246, "o", "\r\n"]
[1657.256, "o", "        # Compute code for this batch\r\n"]
[1657.266, "o", "        code = _sparse_encode(\r\n"]
[1657.276, "o", "            X,\r\n"]
[1657.286, "o", "            dictionary,\r\n"]
[1657.296, "o", "            algorithm=self._fit_algorithm,\r\n"]
[1657.306, "o", "            alpha=self.alpha,\r\n"]
[1657.316, "o", "            n_jobs=self.n_jobs,\r\n"]
[1657.326, "o", "            positive=self.positive_code,\r\n"]
[1657.336, "o", "            max_iter=self.transform_max_iter,\r\n"]
[1657.346, "o", "            verbose=self.verbose,\r\n"]
[1657.356, "o", "        )\r\n"]
[1657.366, "o", "\r\n"]
[1657.376, "o", "        batch_cost = (\r\n"]
[1657.386, "o", "            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n"]
[1657.396, "o", "            + self.alpha * np.sum(np.abs(code))\r\n"]
[1657.406, "o", "        ) / batch_size\r\n"]
[1657.416, "o", "\r\n"]
[1657.426, "o", "        # Update inner stats\r\n"]
[1657.436, "o", "        self._update_inner_stats(X, code, batch_size, step)\r\n"]
[1657.446, "o", "\r\n"]
[1657.456, "o", "        # Update dictionary\r\n"]
[1657.466, "o", "        _update_dict(\r\n"]
[1657.476, "o", "            dictionary,\r\n"]
[1657.486, "o", "            X,\r\n"]
[1657.496, "o", "            code,\r\n"]
[1657.506, "o", "            self._A,\r\n"]
[1657.516, "o", "            self._B,\r\n"]
[1657.526, "o", "            verbose=self.verbose,\r\n"]
[1657.536, "o", "            random_state=random_state,\r\n"]
[1657.546, "o", "            positive=self.positive_dict,\r\n"]
[1657.556, "o", "        )\r\n"]
[1657.566, "o", "\r\n"]
[1657.576, "o", "        return batch_cost\r\n"]
[1657.586, "o", "\r\n"]
[1657.596, "o", "    def _check_convergence(\r\n"]
[1657.606, "o", "        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n"]
[1657.616, "o", "    ):\r\n"]
[1657.626, "o", "        \"\"\"Helper function to encapsulate the early stopping logic.\r\n"]
[1657.636, "o", "\r\n"]
[1657.646, "o", "        Early stopping is based on two factors:\r\n"]
[1657.656, "o", "        - A small change of the dictionary between two minibatch updates. This is\r\n"]
[1657.666, "o", "          controlled by the tol parameter.\r\n"]
[1657.676, "o", "        - No more improvement on a smoothed estimate of the objective function for a\r\n"]
[1657.686, "o", "          a certain number of consecutive minibatch updates. This is controlled by\r\n"]
[1657.696, "o", "          the max_no_improvement parameter.\r\n"]
[1657.706, "o", "        \"\"\"\r\n"]
[1657.716, "o", "        batch_size = X.shape[0]\r\n"]
[1657.726, "o", "\r\n"]
[1657.736, "o", "        # counts steps starting from 1 for user friendly verbose mode.\r\n"]
[1657.746, "o", "        step = step + 1\r\n"]
[1657.756, "o", "\r\n"]
[1657.766, "o", "        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n"]
[1657.776, "o", "        # too bad value\r\n"]
[1657.786, "o", "        if step <= min(100, n_samples / batch_size):\r\n"]
[1657.796, "o", "            if self.verbose:\r\n"]
[1657.806, "o", "                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n"]
[1657.816, "o", "            return False\r\n"]
[1657.826, "o", "\r\n"]
[1657.836, "o", "        # Compute an Exponentially Weighted Average of the cost function to\r\n"]
[1657.846, "o", "        # monitor the convergence while discarding minibatch-local stochastic\r\n"]
[1657.856, "o", "        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n"]
[1657.866, "o", "        if self._ewa_cost is None:\r\n"]
[1657.876, "o", "            self._ewa_cost = batch_cost\r\n"]
[1657.886, "o", "        else:\r\n"]
[1657.896, "o", "            alpha = batch_size / (n_samples + 1)\r\n"]
[1657.906, "o", "            alpha = min(alpha, 1)\r\n"]
[1657.916, "o", "            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n"]
[1657.926, "o", "\r\n"]
[1657.936, "o", "        if self.verbose:\r\n"]
[1657.946, "o", "            print(\r\n"]
[1657.956, "o", "                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\r\n"]
[1657.966, "o", "                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\r\n"]
[1657.976, "o", "            )\r\n"]
[1657.986, "o", "\r\n"]
[1657.996, "o", "        # Early stopping based on change of dictionary\r\n"]
[1658.006, "o", "        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\r\n"]
[1658.016, "o", "        if self.tol > 0 and dict_diff <= self.tol:\r\n"]
[1658.026, "o", "            if self.verbose:\r\n"]
[1658.036, "o", "                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\r\n"]
[1658.046, "o", "            return True\r\n"]
[1658.056, "o", "\r\n"]
[1658.066, "o", "        # Early stopping heuristic due to lack of improvement on smoothed\r\n"]
[1658.076, "o", "        # cost function\r\n"]
[1658.086, "o", "        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\r\n"]
[1658.096, "o", "            self._no_improvement = 0\r\n"]
[1658.106, "o", "            self._ewa_cost_min = self._ewa_cost\r\n"]
[1658.116, "o", "        else:\r\n"]
[1658.126, "o", "            self._no_improvement += 1\r\n"]
[1658.136, "o", "\r\n"]
[1658.146, "o", "        if (\r\n"]
[1658.156, "o", "            self.max_no_improvement is not None\r\n"]
[1658.166, "o", "            and self._no_improvement >= self.max_no_improvement\r\n"]
[1658.176, "o", "        ):\r\n"]
[1658.186, "o", "            if self.verbose:\r\n"]
[1658.196, "o", "                print(\r\n"]
[1658.206, "o", "                    \"Converged (lack of improvement in objective function) \"\r\n"]
[1658.216, "o", "                    f\"at step {step}/{n_steps}\"\r\n"]
[1658.226, "o", "                )\r\n"]
[1658.236, "o", "            return True\r\n"]
[1658.246, "o", "\r\n"]
[1658.256, "o", "        return False\r\n"]
[1658.266, "o", "\r\n"]
[1658.276, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[1658.286, "o", "    def fit(self, X, y=None):\r\n"]
[1658.296, "o", "        \"\"\"Fit the model from data in X.\r\n"]
[1658.306, "o", "\r\n"]
[1658.316, "o", "        Parameters\r\n"]
[1658.326, "o", "        ----------\r\n"]
[1658.336, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[1658.346, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1658.356, "o", "            and `n_features` is the number of features.\r\n"]
[1658.366, "o", "\r\n"]
[1658.376, "o", "        y : Ignored\r\n"]
[1658.386, "o", "            Not used, present for API consistency by convention.\r\n"]
[1658.396, "o", "\r\n"]
[1658.406, "o", "        Returns\r\n"]
[1658.416, "o", "        -------\r\n"]
[1658.426, "o", "        self : object\r\n"]
[1658.436, "o", "            Returns the instance itself.\r\n"]
[1658.446, "o", "        \"\"\"\r\n"]
[1658.456, "o", "        X = self._validate_data(\r\n"]
[1658.466, "o", "            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n"]
[1658.476, "o", "        )\r\n"]
[1658.486, "o", "\r\n"]
[1658.496, "o", "        self._check_params(X)\r\n"]
[1658.506, "o", "\r\n"]
[1658.516, "o", "        if self.n_iter != \"deprecated\":\r\n"]
[1658.526, "o", "            warnings.warn(\r\n"]
[1658.536, "o", "                (\r\n"]
[1658.546, "o", "                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n"]
[1658.556, "o", "                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n"]
[1658.566, "o", "                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n"]
[1658.576, "o", "                    \"specified.\"\r\n"]
[1658.586, "o", "                ),\r\n"]
[1658.596, "o", "                FutureWarning,\r\n"]
[1658.606, "o", "            )\r\n"]
[1658.616, "o", "            n_iter = self.n_iter\r\n"]
[1658.626, "o", "\r\n"]
[1658.636, "o", "        self._random_state = check_random_state(self.random_state)\r\n"]
[1658.646, "o", "\r\n"]
[1658.656, "o", "        dictionary = self._initialize_dict(X, self._random_state)\r\n"]
[1658.666, "o", "        old_dict = dictionary.copy()\r\n"]
[1658.676, "o", "\r\n"]
[1658.686, "o", "        if self.shuffle:\r\n"]
[1658.696, "o", "            X_train = X.copy()\r\n"]
[1658.706, "o", "            self._random_state.shuffle(X_train)\r\n"]
[1658.716, "o", "        else:\r\n"]
[1658.726, "o", "            X_train = X\r\n"]
[1658.736, "o", "\r\n"]
[1658.746, "o", "        n_samples, n_features = X_train.shape\r\n"]
[1658.756, "o", "\r\n"]
[1658.766, "o", "        if self.verbose:\r\n"]
[1658.776, "o", "            print(\"[dict_learning]\")\r\n"]
[1658.786, "o", "\r\n"]
[1658.796, "o", "        # Inner stats\r\n"]
[1658.806, "o", "        self._A = np.zeros(\r\n"]
[1658.816, "o", "            (self._n_components, self._n_components), dtype=X_train.dtype\r\n"]
[1658.826, "o", "        )\r\n"]
[1658.836, "o", "        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n"]
[1658.846, "o", "\r\n"]
[1658.856, "o", "        if self.max_iter is not None:\r\n"]
[1658.866, "o", "            # Attributes to monitor the convergence\r\n"]
[1658.876, "o", "            self._ewa_cost = None\r\n"]
[1658.886, "o", "            self._ewa_cost_min = None\r\n"]
[1658.896, "o", "            self._no_improvement = 0\r\n"]
[1658.906, "o", "\r\n"]
[1658.916, "o", "            batches = gen_batches(n_samples, self._batch_size)\r\n"]
[1658.926, "o", "            batches = itertools.cycle(batches)\r\n"]
[1658.936, "o", "            n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\r\n"]
[1658.946, "o", "            n_steps = self.max_iter * n_steps_per_iter\r\n"]
[1658.956, "o", "\r\n"]
[1658.966, "o", "            i = -1  # to allow max_iter = 0\r\n"]
[1658.976, "o", "\r\n"]
[1658.986, "o", "            for i, batch in zip(range(n_steps), batches):\r\n"]
[1658.996, "o", "                X_batch = X_train[batch]\r\n"]
[1659.006, "o", "\r\n"]
[1659.016, "o", "                batch_cost = self._minibatch_step(\r\n"]
[1659.026, "o", "                    X_batch, dictionary, self._random_state, i\r\n"]
[1659.036, "o", "                )\r\n"]
[1659.046, "o", "\r\n"]
[1659.056, "o", "                if self._check_convergence(\r\n"]
[1659.066, "o", "                    X_batch, batch_cost, dictionary, old_dict, n_samples, i, n_steps\r\n"]
[1659.076, "o", "                ):\r\n"]
[1659.086, "o", "                    break\r\n"]
[1659.096, "o", "\r\n"]
[1659.106, "o", "                # XXX callback param added for backward compat in #18975 but a common\r\n"]
[1659.116, "o", "                # unified callback API should be preferred\r\n"]
[1659.126, "o", "                if self.callback is not None:\r\n"]
[1659.136, "o", "                    self.callback(locals())\r\n"]
[1659.146, "o", "\r\n"]
[1659.156, "o", "                old_dict[:] = dictionary\r\n"]
[1659.166, "o", "\r\n"]
[1659.176, "o", "            self.n_steps_ = i + 1\r\n"]
[1659.186, "o", "            self.n_iter_ = np.ceil(self.n_steps_ / n_steps_per_iter)\r\n"]
[1659.196, "o", "        else:\r\n"]
[1659.206, "o", "            # TODO remove this branch in 1.4\r\n"]
[1659.216, "o", "            n_iter = 1000 if self.n_iter == \"deprecated\" else self.n_iter\r\n"]
[1659.226, "o", "\r\n"]
[1659.236, "o", "            batches = gen_batches(n_samples, self._batch_size)\r\n"]
[1659.246, "o", "            batches = itertools.cycle(batches)\r\n"]
[1659.256, "o", "\r\n"]
[1659.266, "o", "            for i, batch in zip(range(n_iter), batches):\r\n"]
[1659.276, "o", "                self._minibatch_step(X_train[batch], dictionary, self._random_state, i)\r\n"]
[1659.286, "o", "\r\n"]
[1659.296, "o", "                trigger_verbose = self.verbose and i % ceil(100.0 / self.verbose) == 0\r\n"]
[1659.306, "o", "                if self.verbose > 10 or trigger_verbose:\r\n"]
[1659.316, "o", "                    print(f\"{i} batches processed.\")\r\n"]
[1659.326, "o", "\r\n"]
[1659.336, "o", "                if self.callback is not None:\r\n"]
[1659.346, "o", "                    self.callback(locals())\r\n"]
[1659.356, "o", "\r\n"]
[1659.366, "o", "            self.n_steps_ = n_iter\r\n"]
[1659.376, "o", "            self.n_iter_ = np.ceil(n_iter / int(np.ceil(n_samples / self._batch_size)))\r\n"]
[1659.386, "o", "\r\n"]
[1659.396, "o", "        self.components_ = dictionary\r\n"]
[1659.406, "o", "\r\n"]
[1659.416, "o", "        return self\r\n"]
[1659.426, "o", "\r\n"]
[1659.436, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[1659.446, "o", "    def partial_fit(self, X, y=None):\r\n"]
[1659.456, "o", "        \"\"\"Update the model using the data in X as a mini-batch.\r\n"]
[1659.466, "o", "\r\n"]
[1659.476, "o", "        Parameters\r\n"]
[1659.486, "o", "        ----------\r\n"]
[1659.496, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[1659.506, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1659.516, "o", "            and `n_features` is the number of features.\r\n"]
[1659.526, "o", "\r\n"]
[1659.536, "o", "        y : Ignored\r\n"]
[1659.546, "o", "            Not used, present for API consistency by convention.\r\n"]
[1659.556, "o", "\r\n"]
[1659.566, "o", "        Returns\r\n"]
[1659.576, "o", "        -------\r\n"]
[1659.586, "o", "        self : object\r\n"]
[1659.596, "o", "            Return the instance itself.\r\n"]
[1659.606, "o", "        \"\"\"\r\n"]
[1659.616, "o", "        has_components = hasattr(self, \"components_\")\r\n"]
[1659.626, "o", "\r\n"]
[1659.636, "o", "        X = self._validate_data(\r\n"]
[1659.646, "o", "            X, dtype=[np.float64, np.float32], order=\"C\", reset=not has_components\r\n"]
[1659.656, "o", "        )\r\n"]
[1659.666, "o", "\r\n"]
[1659.676, "o", "        if not has_components:\r\n"]
[1659.686, "o", "            # This instance has not been fitted yet (fit or partial_fit)\r\n"]
[1659.696, "o", "            self._check_params(X)\r\n"]
[1659.706, "o", "            self._random_state = check_random_state(self.random_state)\r\n"]
[1659.716, "o", "\r\n"]
[1659.726, "o", "            dictionary = self._initialize_dict(X, self._random_state)\r\n"]
[1659.736, "o", "\r\n"]
[1659.746, "o", "            self.n_steps_ = 0\r\n"]
[1659.756, "o", "\r\n"]
[1659.766, "o", "            self._A = np.zeros((self._n_components, self._n_components), dtype=X.dtype)\r\n"]
[1659.776, "o", "            self._B = np.zeros((X.shape[1], self._n_components), dtype=X.dtype)\r\n"]
[1659.786, "o", "        else:\r\n"]
[1659.796, "o", "            dictionary = self.components_\r\n"]
[1659.806, "o", "\r\n"]
[1659.816, "o", "        self._minibatch_step(X, dictionary, self._random_state, self.n_steps_)\r\n"]
[1659.826, "o", "\r\n"]
[1659.836, "o", "        self.components_ = dictionary\r\n"]
[1659.846, "o", "        self.n_steps_ += 1\r\n"]
[1659.856, "o", "\r\n"]
[1659.866, "o", "        return self\r\n"]
[1659.876, "o", "\r\n"]
[1659.886, "o", "    @property\r\n"]
[1659.896, "o", "    def _n_features_out(self):\r\n"]
[1659.906, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[1659.916, "o", "        return self.components_.shape[0]\r\n"]
[1659.926, "o", "\r\n"]
[1659.936, "o", "    def _more_tags(self):\r\n"]
[1660.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1660.002, "i", "grep -R \"def _minibatch_step\\|_minibatch_step\\(\" -n sklearn || true\r"]
[1660.004, "o", "grep -R \"def _minibatch_step\\|_minibatch_step\\(\" -n sklearn || true\r\n"]
[1662.478, "o", "\u001b[?2004l\r\n"]
[1665.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1665.002, "i", "grep -R \"self\\._A\\|self\\._B\\|_ewa_cost\\|_no_improvement\" -n sklearn/decomposition/_dict_learning.py || true\r"]
[1665.004, "o", "grep -R \"self\\._A\\|self\\._B\\|_ewa_cost\\|_no_improvement\" -n sklearn/decomposition/_dict_learning.py || true\r\n"]
[1665.151412, "o", "\u001b[?2004l\r\n"]
[1665.296824, "o", "\u001b[32m\u001b[K688\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    max\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K=10,\r\n"]
[1665.442235, "o", "\u001b[32m\u001b[K832\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    max\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K : int, default=10\r\n"]
[1665.587647, "o", "\u001b[32m\u001b[K838\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        `max\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K` to None.\r\n"]
[1665.733059, "o", "\u001b[32m\u001b[K909\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            max\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K=max\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K,\r\n"]
[1665.878471, "o", "\u001b[32m\u001b[K2010\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    max\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K : int, default=10\r\n"]
[1666.023882, "o", "\u001b[32m\u001b[K2016\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        `max\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K` to None.\r\n"]
[1666.169294, "o", "\u001b[32m\u001b[K2111\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \"max\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n"]
[1666.314706, "o", "\u001b[32m\u001b[K2137\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        max\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K=10,\r\n"]
[1666.460118, "o", "\u001b[32m\u001b[K2161\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        self.max\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K = max\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K\r\n"]
[1666.605529, "o", "\u001b[32m\u001b[K2214\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kself._A\u001b[m\u001b[K *= beta\r\n"]
[1666.750941, "o", "\u001b[32m\u001b[K2215\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kself._A\u001b[m\u001b[K += code.T @ code / batch_size\r\n"]
[1666.896353, "o", "\u001b[32m\u001b[K2216\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kself._B\u001b[m\u001b[K *= beta\r\n"]
[1667.041765, "o", "\u001b[32m\u001b[K2217\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kself._B\u001b[m\u001b[K += X.T @ code / batch_size\r\n"]
[1667.187176, "o", "\u001b[32m\u001b[K2248\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            \u001b[01;31m\u001b[Kself._A\u001b[m\u001b[K,\r\n"]
[1667.332588, "o", "\u001b[32m\u001b[K2249\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            \u001b[01;31m\u001b[Kself._B\u001b[m\u001b[K,\r\n"]
[1667.478, "o", "\u001b[32m\u001b[K2267\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K          the max\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K parameter.\r\n"]
[1667.623412, "o", "\u001b[32m\u001b[K2284\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        if self.\u001b[01;31m\u001b[K_ewa_cost\u001b[m\u001b[K is None:\r\n"]
[1667.768824, "o", "\u001b[32m\u001b[K2285\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            self.\u001b[01;31m\u001b[K_ewa_cost\u001b[m\u001b[K = batch_cost\r\n"]
[1667.914235, "o", "\u001b[32m\u001b[K2289\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            self.\u001b[01;31m\u001b[K_ewa_cost\u001b[m\u001b[K = self.\u001b[01;31m\u001b[K_ewa_cost\u001b[m\u001b[K * (1 - alpha) + batch_cost * alpha\r\n"]
[1668.059647, "o", "\u001b[32m\u001b[K2294\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                f\"{batch_cost}, ewa cost: {self.\u001b[01;31m\u001b[K_ewa_cost\u001b[m\u001b[K}\"\r\n"]
[1668.205059, "o", "\u001b[32m\u001b[K2306\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        if self.\u001b[01;31m\u001b[K_ewa_cost\u001b[m\u001b[K_min is None or self.\u001b[01;31m\u001b[K_ewa_cost\u001b[m\u001b[K < self.\u001b[01;31m\u001b[K_ewa_cost\u001b[m\u001b[K_min:\r\n"]
[1668.350471, "o", "\u001b[32m\u001b[K2307\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            self.\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K = 0\r\n"]
[1668.495882, "o", "\u001b[32m\u001b[K2308\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            self.\u001b[01;31m\u001b[K_ewa_cost\u001b[m\u001b[K_min = self.\u001b[01;31m\u001b[K_ewa_cost\u001b[m\u001b[K\r\n"]
[1668.641294, "o", "\u001b[32m\u001b[K2310\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            self.\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K += 1\r\n"]
[1668.786706, "o", "\u001b[32m\u001b[K2313\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            self.max\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K is not None\r\n"]
[1668.932118, "o", "\u001b[32m\u001b[K2314\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            and self.\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K >= self.max\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K\r\n"]
[1669.077529, "o", "\u001b[32m\u001b[K2378\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kself._A\u001b[m\u001b[K = np.zeros(\r\n"]
[1669.222941, "o", "\u001b[32m\u001b[K2381\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kself._B\u001b[m\u001b[K = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n"]
[1669.368353, "o", "\u001b[32m\u001b[K2385\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            self.\u001b[01;31m\u001b[K_ewa_cost\u001b[m\u001b[K = None\r\n"]
[1669.513765, "o", "\u001b[32m\u001b[K2386\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            self.\u001b[01;31m\u001b[K_ewa_cost\u001b[m\u001b[K_min = None\r\n"]
[1669.659176, "o", "\u001b[32m\u001b[K2387\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            self.\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K = 0\r\n"]
[1669.804588, "o", "\u001b[32m\u001b[K2474\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            \u001b[01;31m\u001b[Kself._A\u001b[m\u001b[K = np.zeros((self._n_components, self._n_components), dtype=X.dtype)\r\n"]
[1670.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1670.002, "i", "grep -R \"_sparse_encode_precomputed\\s*\\(|sparse_encode\\s*\\(\" -n sklearn/decomposition || true\r"]
[1670.004, "o", "grep -R \"_sparse_encode_precomputed\\s*\\(|sparse_encode\\s*\\(\" -n sklearn/decomposition || true\r\n"]
[1672.478, "o", "\u001b[?2004l\r\n"]
[1675.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1675.002, "i", "sed -n '1,1200p' sklearn/decomposition/_dict_learning.py\r"]
[1675.004, "o", "sed -n '1,1200p' sklearn/decomposition/_dict_learning.py\r\n"]
[1675.016, "o", "\u001b[?2004l\r\n"]
[1675.026, "o", "\"\"\" Dictionary learning.\r\n"]
[1675.036, "o", "\"\"\"\r\n"]
[1675.046, "o", "# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n"]
[1675.056, "o", "# License: BSD 3 clause\r\n"]
[1675.066, "o", "\r\n"]
[1675.076, "o", "import itertools\r\n"]
[1675.086, "o", "import sys\r\n"]
[1675.096, "o", "import time\r\n"]
[1675.106, "o", "import warnings\r\n"]
[1675.116, "o", "from math import ceil\r\n"]
[1675.126, "o", "from numbers import Integral, Real\r\n"]
[1675.136, "o", "\r\n"]
[1675.146, "o", "import numpy as np\r\n"]
[1675.156, "o", "from joblib import effective_n_jobs\r\n"]
[1675.166, "o", "from scipy import linalg\r\n"]
[1675.176, "o", "\r\n"]
[1675.186, "o", "from ..base import (\r\n"]
[1675.196, "o", "    BaseEstimator,\r\n"]
[1675.206, "o", "    ClassNamePrefixFeaturesOutMixin,\r\n"]
[1675.216, "o", "    TransformerMixin,\r\n"]
[1675.226, "o", "    _fit_context,\r\n"]
[1675.236, "o", ")\r\n"]
[1675.246, "o", "from ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n"]
[1675.256, "o", "from ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n"]
[1675.266, "o", "from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n"]
[1675.276, "o", "from ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n"]
[1675.286, "o", "from ..utils.parallel import Parallel, delayed\r\n"]
[1675.296, "o", "from ..utils.validation import check_is_fitted\r\n"]
[1675.306, "o", "\r\n"]
[1675.316, "o", "\r\n"]
[1675.326, "o", "def _check_positive_coding(method, positive):\r\n"]
[1675.336, "o", "    if positive and method in [\"omp\", \"lars\"]:\r\n"]
[1675.346, "o", "        raise ValueError(\r\n"]
[1675.356, "o", "            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n"]
[1675.366, "o", "        )\r\n"]
[1675.376, "o", "\r\n"]
[1675.386, "o", "\r\n"]
[1675.396, "o", "def _sparse_encode_precomputed(\r\n"]
[1675.406, "o", "    X,\r\n"]
[1675.416, "o", "    dictionary,\r\n"]
[1675.426, "o", "    *,\r\n"]
[1675.436, "o", "    gram=None,\r\n"]
[1675.446, "o", "    cov=None,\r\n"]
[1675.456, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1675.466, "o", "    regularization=None,\r\n"]
[1675.476, "o", "    copy_cov=True,\r\n"]
[1675.486, "o", "    init=None,\r\n"]
[1675.496, "o", "    max_iter=1000,\r\n"]
[1675.506, "o", "    verbose=0,\r\n"]
[1675.516, "o", "    positive=False,\r\n"]
[1675.526, "o", "):\r\n"]
[1675.536, "o", "    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n"]
[1675.546, "o", "\r\n"]
[1675.556, "o", "    Each row of the result is the solution to a Lasso problem.\r\n"]
[1675.566, "o", "\r\n"]
[1675.576, "o", "    Parameters\r\n"]
[1675.586, "o", "    ----------\r\n"]
[1675.596, "o", "    X : ndarray of shape (n_samples, n_features)\r\n"]
[1675.606, "o", "        Data matrix.\r\n"]
[1675.616, "o", "\r\n"]
[1675.626, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1675.636, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1675.646, "o", "        the data. Some of the algorithms assume normalized rows.\r\n"]
[1675.656, "o", "\r\n"]
[1675.666, "o", "    gram : ndarray of shape (n_components, n_components), default=None\r\n"]
[1675.676, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`\r\n"]
[1675.686, "o", "        gram can be `None` if method is 'threshold'.\r\n"]
[1675.696, "o", "\r\n"]
[1675.706, "o", "    cov : ndarray of shape (n_components, n_samples), default=None\r\n"]
[1675.716, "o", "        Precomputed covariance, `dictionary * X'`.\r\n"]
[1675.726, "o", "\r\n"]
[1675.736, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1675.746, "o", "            default='lasso_lars'\r\n"]
[1675.756, "o", "        The algorithm used:\r\n"]
[1675.766, "o", "\r\n"]
[1675.776, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[1675.786, "o", "          (`linear_model.lars_path`);\r\n"]
[1675.796, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1675.806, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1675.816, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1675.826, "o", "          the estimated components are sparse;\r\n"]
[1675.836, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1675.846, "o", "          solution;\r\n"]
[1675.856, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1675.866, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[1675.876, "o", "\r\n"]
[1675.886, "o", "    regularization : int or float, default=None\r\n"]
[1675.896, "o", "        The regularization parameter. It corresponds to alpha when\r\n"]
[1675.906, "o", "        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n"]
[1675.916, "o", "        Otherwise it corresponds to `n_nonzero_coefs`.\r\n"]
[1675.926, "o", "\r\n"]
[1675.936, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1675.946, "o", "        Initialization value of the sparse code. Only used if\r\n"]
[1675.956, "o", "        `algorithm='lasso_cd'`.\r\n"]
[1675.966, "o", "\r\n"]
[1675.976, "o", "    max_iter : int, default=1000\r\n"]
[1675.986, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1675.996, "o", "        `'lasso_lars'`.\r\n"]
[1676.006, "o", "\r\n"]
[1676.016, "o", "    copy_cov : bool, default=True\r\n"]
[1676.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1676.036, "o", "        be overwritten.\r\n"]
[1676.046, "o", "\r\n"]
[1676.056, "o", "    verbose : int, default=0\r\n"]
[1676.066, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[1676.076, "o", "\r\n"]
[1676.086, "o", "    positive: bool, default=False\r\n"]
[1676.096, "o", "        Whether to enforce a positivity constraint on the sparse code.\r\n"]
[1676.106, "o", "\r\n"]
[1676.116, "o", "        .. versionadded:: 0.20\r\n"]
[1676.126, "o", "\r\n"]
[1676.136, "o", "    Returns\r\n"]
[1676.146, "o", "    -------\r\n"]
[1676.156, "o", "    code : ndarray of shape (n_components, n_features)\r\n"]
[1676.166, "o", "        The sparse codes.\r\n"]
[1676.176, "o", "    \"\"\"\r\n"]
[1676.186, "o", "    n_samples, n_features = X.shape\r\n"]
[1676.196, "o", "    n_components = dictionary.shape[0]\r\n"]
[1676.206, "o", "\r\n"]
[1676.216, "o", "    if algorithm == \"lasso_lars\":\r\n"]
[1676.226, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1676.236, "o", "        try:\r\n"]
[1676.246, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1676.256, "o", "\r\n"]
[1676.266, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1676.276, "o", "            # corrects the verbosity level.\r\n"]
[1676.286, "o", "            lasso_lars = LassoLars(\r\n"]
[1676.296, "o", "                alpha=alpha,\r\n"]
[1676.306, "o", "                fit_intercept=False,\r\n"]
[1676.316, "o", "                verbose=verbose,\r\n"]
[1676.326, "o", "                precompute=gram,\r\n"]
[1676.336, "o", "                fit_path=False,\r\n"]
[1676.346, "o", "                positive=positive,\r\n"]
[1676.356, "o", "                max_iter=max_iter,\r\n"]
[1676.366, "o", "            )\r\n"]
[1676.376, "o", "            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1676.386, "o", "            new_code = lasso_lars.coef_\r\n"]
[1676.396, "o", "        finally:\r\n"]
[1676.406, "o", "            np.seterr(**err_mgt)\r\n"]
[1676.416, "o", "\r\n"]
[1676.426, "o", "    elif algorithm == \"lasso_cd\":\r\n"]
[1676.436, "o", "        alpha = float(regularization) / n_features  # account for scaling\r\n"]
[1676.446, "o", "\r\n"]
[1676.456, "o", "        # TODO: Make verbosity argument for Lasso?\r\n"]
[1676.466, "o", "        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n"]
[1676.476, "o", "        # argument that we could pass in from Lasso.\r\n"]
[1676.486, "o", "        clf = Lasso(\r\n"]
[1676.496, "o", "            alpha=alpha,\r\n"]
[1676.506, "o", "            fit_intercept=False,\r\n"]
[1676.516, "o", "            precompute=gram,\r\n"]
[1676.526, "o", "            max_iter=max_iter,\r\n"]
[1676.536, "o", "            warm_start=True,\r\n"]
[1676.546, "o", "            positive=positive,\r\n"]
[1676.556, "o", "        )\r\n"]
[1676.566, "o", "\r\n"]
[1676.576, "o", "        if init is not None:\r\n"]
[1676.586, "o", "            # In some workflows using coordinate descent algorithms:\r\n"]
[1676.596, "o", "            #  - users might provide NumPy arrays with read-only buffers\r\n"]
[1676.606, "o", "            #  - `joblib` might memmap arrays making their buffer read-only\r\n"]
[1676.616, "o", "            # TODO: move this handling (which is currently too broad)\r\n"]
[1676.626, "o", "            # closer to the actual private function which need buffers to be writable.\r\n"]
[1676.636, "o", "            if not init.flags[\"WRITEABLE\"]:\r\n"]
[1676.646, "o", "                init = np.array(init)\r\n"]
[1676.656, "o", "            clf.coef_ = init\r\n"]
[1676.666, "o", "\r\n"]
[1676.676, "o", "        clf.fit(dictionary.T, X.T, check_input=False)\r\n"]
[1676.686, "o", "        new_code = clf.coef_\r\n"]
[1676.696, "o", "\r\n"]
[1676.706, "o", "    elif algorithm == \"lars\":\r\n"]
[1676.716, "o", "        try:\r\n"]
[1676.726, "o", "            err_mgt = np.seterr(all=\"ignore\")\r\n"]
[1676.736, "o", "\r\n"]
[1676.746, "o", "            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n"]
[1676.756, "o", "            # corrects the verbosity level.\r\n"]
[1676.766, "o", "            lars = Lars(\r\n"]
[1676.776, "o", "                fit_intercept=False,\r\n"]
[1676.786, "o", "                verbose=verbose,\r\n"]
[1676.796, "o", "                precompute=gram,\r\n"]
[1676.806, "o", "                n_nonzero_coefs=int(regularization),\r\n"]
[1676.816, "o", "                fit_path=False,\r\n"]
[1676.826, "o", "            )\r\n"]
[1676.836, "o", "            lars.fit(dictionary.T, X.T, Xy=cov)\r\n"]
[1676.846, "o", "            new_code = lars.coef_\r\n"]
[1676.856, "o", "        finally:\r\n"]
[1676.866, "o", "            np.seterr(**err_mgt)\r\n"]
[1676.876, "o", "\r\n"]
[1676.886, "o", "    elif algorithm == \"threshold\":\r\n"]
[1676.896, "o", "        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n"]
[1676.906, "o", "        if positive:\r\n"]
[1676.916, "o", "            np.clip(new_code, 0, None, out=new_code)\r\n"]
[1676.926, "o", "\r\n"]
[1676.936, "o", "    elif algorithm == \"omp\":\r\n"]
[1676.946, "o", "        new_code = orthogonal_mp_gram(\r\n"]
[1676.956, "o", "            Gram=gram,\r\n"]
[1676.966, "o", "            Xy=cov,\r\n"]
[1676.976, "o", "            n_nonzero_coefs=int(regularization),\r\n"]
[1676.986, "o", "            tol=None,\r\n"]
[1676.996, "o", "            norms_squared=row_norms(X, squared=True),\r\n"]
[1677.006, "o", "            copy_Xy=copy_cov,\r\n"]
[1677.016, "o", "        ).T\r\n"]
[1677.026, "o", "\r\n"]
[1677.036, "o", "    return new_code.reshape(n_samples, n_components)\r\n"]
[1677.046, "o", "\r\n"]
[1677.056, "o", "\r\n"]
[1677.066, "o", "@validate_params(\r\n"]
[1677.076, "o", "    {\r\n"]
[1677.086, "o", "        \"X\": [\"array-like\"],\r\n"]
[1677.096, "o", "        \"dictionary\": [\"array-like\"],\r\n"]
[1677.106, "o", "        \"gram\": [\"array-like\", None],\r\n"]
[1677.116, "o", "        \"cov\": [\"array-like\", None],\r\n"]
[1677.126, "o", "        \"algorithm\": [\r\n"]
[1677.136, "o", "            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n"]
[1677.146, "o", "        ],\r\n"]
[1677.156, "o", "        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n"]
[1677.166, "o", "        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n"]
[1677.176, "o", "        \"copy_cov\": [\"boolean\"],\r\n"]
[1677.186, "o", "        \"init\": [\"array-like\", None],\r\n"]
[1677.196, "o", "        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n"]
[1677.206, "o", "        \"n_jobs\": [Integral, None],\r\n"]
[1677.216, "o", "        \"check_input\": [\"boolean\"],\r\n"]
[1677.226, "o", "        \"verbose\": [\"verbose\"],\r\n"]
[1677.236, "o", "        \"positive\": [\"boolean\"],\r\n"]
[1677.246, "o", "    },\r\n"]
[1677.256, "o", "    prefer_skip_nested_validation=True,\r\n"]
[1677.266, "o", ")\r\n"]
[1677.276, "o", "# XXX : could be moved to the linear_model module\r\n"]
[1677.286, "o", "def sparse_encode(\r\n"]
[1677.296, "o", "    X,\r\n"]
[1677.306, "o", "    dictionary,\r\n"]
[1677.316, "o", "    *,\r\n"]
[1677.326, "o", "    gram=None,\r\n"]
[1677.336, "o", "    cov=None,\r\n"]
[1677.346, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1677.356, "o", "    n_nonzero_coefs=None,\r\n"]
[1677.366, "o", "    alpha=None,\r\n"]
[1677.376, "o", "    copy_cov=True,\r\n"]
[1677.386, "o", "    init=None,\r\n"]
[1677.396, "o", "    max_iter=1000,\r\n"]
[1677.406, "o", "    n_jobs=None,\r\n"]
[1677.416, "o", "    check_input=True,\r\n"]
[1677.426, "o", "    verbose=0,\r\n"]
[1677.436, "o", "    positive=False,\r\n"]
[1677.446, "o", "):\r\n"]
[1677.456, "o", "    \"\"\"Sparse coding.\r\n"]
[1677.466, "o", "\r\n"]
[1677.476, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[1677.486, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[1677.496, "o", "\r\n"]
[1677.506, "o", "        X ~= code * dictionary\r\n"]
[1677.516, "o", "\r\n"]
[1677.526, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[1677.536, "o", "\r\n"]
[1677.546, "o", "    Parameters\r\n"]
[1677.556, "o", "    ----------\r\n"]
[1677.566, "o", "    X : array-like of shape (n_samples, n_features)\r\n"]
[1677.576, "o", "        Data matrix.\r\n"]
[1677.586, "o", "\r\n"]
[1677.596, "o", "    dictionary : array-like of shape (n_components, n_features)\r\n"]
[1677.606, "o", "        The dictionary matrix against which to solve the sparse coding of\r\n"]
[1677.616, "o", "        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[1677.626, "o", "        output.\r\n"]
[1677.636, "o", "\r\n"]
[1677.646, "o", "    gram : array-like of shape (n_components, n_components), default=None\r\n"]
[1677.656, "o", "        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n"]
[1677.666, "o", "\r\n"]
[1677.676, "o", "    cov : array-like of shape (n_components, n_samples), default=None\r\n"]
[1677.686, "o", "        Precomputed covariance, `dictionary' * X`.\r\n"]
[1677.696, "o", "\r\n"]
[1677.706, "o", "    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n"]
[1677.716, "o", "            default='lasso_lars'\r\n"]
[1677.726, "o", "        The algorithm used:\r\n"]
[1677.736, "o", "\r\n"]
[1677.746, "o", "        * `'lars'`: uses the least angle regression method\r\n"]
[1677.756, "o", "          (`linear_model.lars_path`);\r\n"]
[1677.766, "o", "        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1677.776, "o", "        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1677.786, "o", "          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n"]
[1677.796, "o", "          the estimated components are sparse;\r\n"]
[1677.806, "o", "        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1677.816, "o", "          solution;\r\n"]
[1677.826, "o", "        * `'threshold'`: squashes to zero all coefficients less than\r\n"]
[1677.836, "o", "          regularization from the projection `dictionary * data'`.\r\n"]
[1677.846, "o", "\r\n"]
[1677.856, "o", "    n_nonzero_coefs : int, default=None\r\n"]
[1677.866, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1677.876, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[1677.886, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[1677.896, "o", "        `n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1677.906, "o", "\r\n"]
[1677.916, "o", "    alpha : float, default=None\r\n"]
[1677.926, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1677.936, "o", "        penalty applied to the L1 norm.\r\n"]
[1677.946, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1677.956, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1677.966, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[1677.976, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[1677.986, "o", "        `n_nonzero_coefs`.\r\n"]
[1677.996, "o", "        If `None`, default to 1.\r\n"]
[1678.006, "o", "\r\n"]
[1678.016, "o", "    copy_cov : bool, default=True\r\n"]
[1678.026, "o", "        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n"]
[1678.036, "o", "        be overwritten.\r\n"]
[1678.046, "o", "\r\n"]
[1678.056, "o", "    init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1678.066, "o", "        Initialization value of the sparse codes. Only used if\r\n"]
[1678.076, "o", "        `algorithm='lasso_cd'`.\r\n"]
[1678.086, "o", "\r\n"]
[1678.096, "o", "    max_iter : int, default=1000\r\n"]
[1678.106, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1678.116, "o", "        `'lasso_lars'`.\r\n"]
[1678.126, "o", "\r\n"]
[1678.136, "o", "    n_jobs : int, default=None\r\n"]
[1678.146, "o", "        Number of parallel jobs to run.\r\n"]
[1678.156, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1678.166, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1678.176, "o", "        for more details.\r\n"]
[1678.186, "o", "\r\n"]
[1678.196, "o", "    check_input : bool, default=True\r\n"]
[1678.206, "o", "        If `False`, the input arrays X and dictionary will not be checked.\r\n"]
[1678.216, "o", "\r\n"]
[1678.226, "o", "    verbose : int, default=0\r\n"]
[1678.236, "o", "        Controls the verbosity; the higher, the more messages.\r\n"]
[1678.246, "o", "\r\n"]
[1678.256, "o", "    positive : bool, default=False\r\n"]
[1678.266, "o", "        Whether to enforce positivity when finding the encoding.\r\n"]
[1678.276, "o", "\r\n"]
[1678.286, "o", "        .. versionadded:: 0.20\r\n"]
[1678.296, "o", "\r\n"]
[1678.306, "o", "    Returns\r\n"]
[1678.316, "o", "    -------\r\n"]
[1678.326, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1678.336, "o", "        The sparse codes.\r\n"]
[1678.346, "o", "\r\n"]
[1678.356, "o", "    See Also\r\n"]
[1678.366, "o", "    --------\r\n"]
[1678.376, "o", "    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n"]
[1678.386, "o", "        path using LARS algorithm.\r\n"]
[1678.396, "o", "    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n"]
[1678.406, "o", "    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n"]
[1678.416, "o", "    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n"]
[1678.426, "o", "        dictionary.\r\n"]
[1678.436, "o", "    \"\"\"\r\n"]
[1678.446, "o", "    if check_input:\r\n"]
[1678.456, "o", "        if algorithm == \"lasso_cd\":\r\n"]
[1678.466, "o", "            dictionary = check_array(\r\n"]
[1678.476, "o", "                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n"]
[1678.486, "o", "            )\r\n"]
[1678.496, "o", "            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n"]
[1678.506, "o", "        else:\r\n"]
[1678.516, "o", "            dictionary = check_array(dictionary)\r\n"]
[1678.526, "o", "            X = check_array(X)\r\n"]
[1678.536, "o", "\r\n"]
[1678.546, "o", "    if dictionary.shape[1] != X.shape[1]:\r\n"]
[1678.556, "o", "        raise ValueError(\r\n"]
[1678.566, "o", "            \"Dictionary and X have different numbers of features:\"\r\n"]
[1678.576, "o", "            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n"]
[1678.586, "o", "        )\r\n"]
[1678.596, "o", "\r\n"]
[1678.606, "o", "    _check_positive_coding(algorithm, positive)\r\n"]
[1678.616, "o", "\r\n"]
[1678.626, "o", "    return _sparse_encode(\r\n"]
[1678.636, "o", "        X,\r\n"]
[1678.646, "o", "        dictionary,\r\n"]
[1678.656, "o", "        gram=gram,\r\n"]
[1678.666, "o", "        cov=cov,\r\n"]
[1678.676, "o", "        algorithm=algorithm,\r\n"]
[1678.686, "o", "        n_nonzero_coefs=n_nonzero_coefs,\r\n"]
[1678.696, "o", "        alpha=alpha,\r\n"]
[1678.706, "o", "        copy_cov=copy_cov,\r\n"]
[1678.716, "o", "        init=init,\r\n"]
[1678.726, "o", "        max_iter=max_iter,\r\n"]
[1678.736, "o", "        n_jobs=n_jobs,\r\n"]
[1678.746, "o", "        verbose=verbose,\r\n"]
[1678.756, "o", "        positive=positive,\r\n"]
[1678.766, "o", "    )\r\n"]
[1678.776, "o", "\r\n"]
[1678.786, "o", "\r\n"]
[1678.796, "o", "def _sparse_encode(\r\n"]
[1678.806, "o", "    X,\r\n"]
[1678.816, "o", "    dictionary,\r\n"]
[1678.826, "o", "    *,\r\n"]
[1678.836, "o", "    gram=None,\r\n"]
[1678.846, "o", "    cov=None,\r\n"]
[1678.856, "o", "    algorithm=\"lasso_lars\",\r\n"]
[1678.866, "o", "    n_nonzero_coefs=None,\r\n"]
[1678.876, "o", "    alpha=None,\r\n"]
[1678.886, "o", "    copy_cov=True,\r\n"]
[1678.896, "o", "    init=None,\r\n"]
[1678.906, "o", "    max_iter=1000,\r\n"]
[1678.916, "o", "    n_jobs=None,\r\n"]
[1678.926, "o", "    verbose=0,\r\n"]
[1678.936, "o", "    positive=False,\r\n"]
[1678.946, "o", "):\r\n"]
[1678.956, "o", "    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n"]
[1678.966, "o", "\r\n"]
[1678.976, "o", "    n_samples, n_features = X.shape\r\n"]
[1678.986, "o", "    n_components = dictionary.shape[0]\r\n"]
[1678.996, "o", "\r\n"]
[1679.006, "o", "    if algorithm in (\"lars\", \"omp\"):\r\n"]
[1679.016, "o", "        regularization = n_nonzero_coefs\r\n"]
[1679.026, "o", "        if regularization is None:\r\n"]
[1679.036, "o", "            regularization = min(max(n_features / 10, 1), n_components)\r\n"]
[1679.046, "o", "    else:\r\n"]
[1679.056, "o", "        regularization = alpha\r\n"]
[1679.066, "o", "        if regularization is None:\r\n"]
[1679.076, "o", "            regularization = 1.0\r\n"]
[1679.086, "o", "\r\n"]
[1679.096, "o", "    if gram is None and algorithm != \"threshold\":\r\n"]
[1679.106, "o", "        gram = np.dot(dictionary, dictionary.T)\r\n"]
[1679.116, "o", "\r\n"]
[1679.126, "o", "    if cov is None and algorithm != \"lasso_cd\":\r\n"]
[1679.136, "o", "        copy_cov = False\r\n"]
[1679.146, "o", "        cov = np.dot(dictionary, X.T)\r\n"]
[1679.156, "o", "\r\n"]
[1679.166, "o", "    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n"]
[1679.176, "o", "        code = _sparse_encode_precomputed(\r\n"]
[1679.186, "o", "            X,\r\n"]
[1679.196, "o", "            dictionary,\r\n"]
[1679.206, "o", "            gram=gram,\r\n"]
[1679.216, "o", "            cov=cov,\r\n"]
[1679.226, "o", "            algorithm=algorithm,\r\n"]
[1679.236, "o", "            regularization=regularization,\r\n"]
[1679.246, "o", "            copy_cov=copy_cov,\r\n"]
[1679.256, "o", "            init=init,\r\n"]
[1679.266, "o", "            max_iter=max_iter,\r\n"]
[1679.276, "o", "            verbose=verbose,\r\n"]
[1679.286, "o", "            positive=positive,\r\n"]
[1679.296, "o", "        )\r\n"]
[1679.306, "o", "        return code\r\n"]
[1679.316, "o", "\r\n"]
[1679.326, "o", "    # Enter parallel code block\r\n"]
[1679.336, "o", "    n_samples = X.shape[0]\r\n"]
[1679.346, "o", "    n_components = dictionary.shape[0]\r\n"]
[1679.356, "o", "    code = np.empty((n_samples, n_components))\r\n"]
[1679.366, "o", "    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n"]
[1679.376, "o", "\r\n"]
[1679.386, "o", "    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n"]
[1679.396, "o", "        delayed(_sparse_encode_precomputed)(\r\n"]
[1679.406, "o", "            X[this_slice],\r\n"]
[1679.416, "o", "            dictionary,\r\n"]
[1679.426, "o", "            gram=gram,\r\n"]
[1679.436, "o", "            cov=cov[:, this_slice] if cov is not None else None,\r\n"]
[1679.446, "o", "            algorithm=algorithm,\r\n"]
[1679.456, "o", "            regularization=regularization,\r\n"]
[1679.466, "o", "            copy_cov=copy_cov,\r\n"]
[1679.476, "o", "            init=init[this_slice] if init is not None else None,\r\n"]
[1679.486, "o", "            max_iter=max_iter,\r\n"]
[1679.496, "o", "            verbose=verbose,\r\n"]
[1679.506, "o", "            positive=positive,\r\n"]
[1679.516, "o", "        )\r\n"]
[1679.526, "o", "        for this_slice in slices\r\n"]
[1679.536, "o", "    )\r\n"]
[1679.546, "o", "    for this_slice, this_view in zip(slices, code_views):\r\n"]
[1679.556, "o", "        code[this_slice] = this_view\r\n"]
[1679.566, "o", "    return code\r\n"]
[1679.576, "o", "\r\n"]
[1679.586, "o", "\r\n"]
[1679.596, "o", "def _update_dict(\r\n"]
[1679.606, "o", "    dictionary,\r\n"]
[1679.616, "o", "    Y,\r\n"]
[1679.626, "o", "    code,\r\n"]
[1679.636, "o", "    A=None,\r\n"]
[1679.646, "o", "    B=None,\r\n"]
[1679.656, "o", "    verbose=False,\r\n"]
[1679.666, "o", "    random_state=None,\r\n"]
[1679.676, "o", "    positive=False,\r\n"]
[1679.686, "o", "):\r\n"]
[1679.696, "o", "    \"\"\"Update the dense dictionary factor in place.\r\n"]
[1679.706, "o", "\r\n"]
[1679.716, "o", "    Parameters\r\n"]
[1679.726, "o", "    ----------\r\n"]
[1679.736, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1679.746, "o", "        Value of the dictionary at the previous iteration.\r\n"]
[1679.756, "o", "\r\n"]
[1679.766, "o", "    Y : ndarray of shape (n_samples, n_features)\r\n"]
[1679.776, "o", "        Data matrix.\r\n"]
[1679.786, "o", "\r\n"]
[1679.796, "o", "    code : ndarray of shape (n_samples, n_components)\r\n"]
[1679.806, "o", "        Sparse coding of the data against which to optimize the dictionary.\r\n"]
[1679.816, "o", "\r\n"]
[1679.826, "o", "    A : ndarray of shape (n_components, n_components), default=None\r\n"]
[1679.836, "o", "        Together with `B`, sufficient stats of the online model to update the\r\n"]
[1679.846, "o", "        dictionary.\r\n"]
[1679.856, "o", "\r\n"]
[1679.866, "o", "    B : ndarray of shape (n_features, n_components), default=None\r\n"]
[1679.876, "o", "        Together with `A`, sufficient stats of the online model to update the\r\n"]
[1679.886, "o", "        dictionary.\r\n"]
[1679.896, "o", "\r\n"]
[1679.906, "o", "    verbose: bool, default=False\r\n"]
[1679.916, "o", "        Degree of output the procedure will print.\r\n"]
[1679.926, "o", "\r\n"]
[1679.936, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1680.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1680.002, "i", "sed -n '1200,2200p' sklearn/decomposition/_dict_learning.py\r"]
[1680.004, "o", "sed -n '1200,2200p' sklearn/decomposition/_dict_learning.py\r\n"]
[1680.016, "o", "\u001b[?2004l\r\n"]
[1680.026, "o", "    n_iter : int\r\n"]
[1680.036, "o", "        Number of iterations run. Returned only if `return_n_iter` is\r\n"]
[1680.046, "o", "        set to True.\r\n"]
[1680.056, "o", "\r\n"]
[1680.066, "o", "    See Also\r\n"]
[1680.076, "o", "    --------\r\n"]
[1680.086, "o", "    dict_learning_online : Solve a dictionary learning matrix factorization\r\n"]
[1680.096, "o", "        problem online.\r\n"]
[1680.106, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1680.116, "o", "    MiniBatchDictionaryLearning : A faster, less accurate version\r\n"]
[1680.126, "o", "        of the dictionary learning algorithm.\r\n"]
[1680.136, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1680.146, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1680.156, "o", "    \"\"\"\r\n"]
[1680.166, "o", "    estimator = DictionaryLearning(\r\n"]
[1680.176, "o", "        n_components=n_components,\r\n"]
[1680.186, "o", "        alpha=alpha,\r\n"]
[1680.196, "o", "        max_iter=max_iter,\r\n"]
[1680.206, "o", "        tol=tol,\r\n"]
[1680.216, "o", "        fit_algorithm=method,\r\n"]
[1680.226, "o", "        n_jobs=n_jobs,\r\n"]
[1680.236, "o", "        dict_init=dict_init,\r\n"]
[1680.246, "o", "        callback=callback,\r\n"]
[1680.256, "o", "        code_init=code_init,\r\n"]
[1680.266, "o", "        verbose=verbose,\r\n"]
[1680.276, "o", "        random_state=random_state,\r\n"]
[1680.286, "o", "        positive_code=positive_code,\r\n"]
[1680.296, "o", "        positive_dict=positive_dict,\r\n"]
[1680.306, "o", "        transform_max_iter=method_max_iter,\r\n"]
[1680.316, "o", "    )\r\n"]
[1680.326, "o", "    code = estimator.fit_transform(X)\r\n"]
[1680.336, "o", "    if return_n_iter:\r\n"]
[1680.346, "o", "        return (\r\n"]
[1680.356, "o", "            code,\r\n"]
[1680.366, "o", "            estimator.components_,\r\n"]
[1680.376, "o", "            estimator.error_,\r\n"]
[1680.386, "o", "            estimator.n_iter_,\r\n"]
[1680.396, "o", "        )\r\n"]
[1680.406, "o", "    return code, estimator.components_, estimator.error_\r\n"]
[1680.416, "o", "\r\n"]
[1680.426, "o", "\r\n"]
[1680.436, "o", "class _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n"]
[1680.446, "o", "    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n"]
[1680.456, "o", "\r\n"]
[1680.466, "o", "    def __init__(\r\n"]
[1680.476, "o", "        self,\r\n"]
[1680.486, "o", "        transform_algorithm,\r\n"]
[1680.496, "o", "        transform_n_nonzero_coefs,\r\n"]
[1680.506, "o", "        transform_alpha,\r\n"]
[1680.516, "o", "        split_sign,\r\n"]
[1680.526, "o", "        n_jobs,\r\n"]
[1680.536, "o", "        positive_code,\r\n"]
[1680.546, "o", "        transform_max_iter,\r\n"]
[1680.556, "o", "    ):\r\n"]
[1680.566, "o", "        self.transform_algorithm = transform_algorithm\r\n"]
[1680.576, "o", "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n"]
[1680.586, "o", "        self.transform_alpha = transform_alpha\r\n"]
[1680.596, "o", "        self.transform_max_iter = transform_max_iter\r\n"]
[1680.606, "o", "        self.split_sign = split_sign\r\n"]
[1680.616, "o", "        self.n_jobs = n_jobs\r\n"]
[1680.626, "o", "        self.positive_code = positive_code\r\n"]
[1680.636, "o", "\r\n"]
[1680.646, "o", "    def _transform(self, X, dictionary):\r\n"]
[1680.656, "o", "        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n"]
[1680.666, "o", "        SparseCoder.\"\"\"\r\n"]
[1680.676, "o", "        X = self._validate_data(X, reset=False)\r\n"]
[1680.686, "o", "\r\n"]
[1680.696, "o", "        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n"]
[1680.706, "o", "            transform_alpha = self.alpha\r\n"]
[1680.716, "o", "        else:\r\n"]
[1680.726, "o", "            transform_alpha = self.transform_alpha\r\n"]
[1680.736, "o", "\r\n"]
[1680.746, "o", "        code = sparse_encode(\r\n"]
[1680.756, "o", "            X,\r\n"]
[1680.766, "o", "            dictionary,\r\n"]
[1680.776, "o", "            algorithm=self.transform_algorithm,\r\n"]
[1680.786, "o", "            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n"]
[1680.796, "o", "            alpha=transform_alpha,\r\n"]
[1680.806, "o", "            max_iter=self.transform_max_iter,\r\n"]
[1680.816, "o", "            n_jobs=self.n_jobs,\r\n"]
[1680.826, "o", "            positive=self.positive_code,\r\n"]
[1680.836, "o", "        )\r\n"]
[1680.846, "o", "\r\n"]
[1680.856, "o", "        if self.split_sign:\r\n"]
[1680.866, "o", "            # feature vector is split into a positive and negative side\r\n"]
[1680.876, "o", "            n_samples, n_features = code.shape\r\n"]
[1680.886, "o", "            split_code = np.empty((n_samples, 2 * n_features))\r\n"]
[1680.896, "o", "            split_code[:, :n_features] = np.maximum(code, 0)\r\n"]
[1680.906, "o", "            split_code[:, n_features:] = -np.minimum(code, 0)\r\n"]
[1680.916, "o", "            code = split_code\r\n"]
[1680.926, "o", "\r\n"]
[1680.936, "o", "        return code\r\n"]
[1680.946, "o", "\r\n"]
[1680.956, "o", "    def transform(self, X):\r\n"]
[1680.966, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[1680.976, "o", "\r\n"]
[1680.986, "o", "        Coding method is determined by the object parameter\r\n"]
[1680.996, "o", "        `transform_algorithm`.\r\n"]
[1681.006, "o", "\r\n"]
[1681.016, "o", "        Parameters\r\n"]
[1681.026, "o", "        ----------\r\n"]
[1681.036, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[1681.046, "o", "            Test data to be transformed, must have the same number of\r\n"]
[1681.056, "o", "            features as the data used to train the model.\r\n"]
[1681.066, "o", "\r\n"]
[1681.076, "o", "        Returns\r\n"]
[1681.086, "o", "        -------\r\n"]
[1681.096, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[1681.106, "o", "            Transformed data.\r\n"]
[1681.116, "o", "        \"\"\"\r\n"]
[1681.126, "o", "        check_is_fitted(self)\r\n"]
[1681.136, "o", "        return self._transform(X, self.components_)\r\n"]
[1681.146, "o", "\r\n"]
[1681.156, "o", "\r\n"]
[1681.166, "o", "class SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n"]
[1681.176, "o", "    \"\"\"Sparse coding.\r\n"]
[1681.186, "o", "\r\n"]
[1681.196, "o", "    Finds a sparse representation of data against a fixed, precomputed\r\n"]
[1681.206, "o", "    dictionary.\r\n"]
[1681.216, "o", "\r\n"]
[1681.226, "o", "    Each row of the result is the solution to a sparse coding problem.\r\n"]
[1681.236, "o", "    The goal is to find a sparse array `code` such that::\r\n"]
[1681.246, "o", "\r\n"]
[1681.256, "o", "        X ~= code * dictionary\r\n"]
[1681.266, "o", "\r\n"]
[1681.276, "o", "    Read more in the :ref:`User Guide <SparseCoder>`.\r\n"]
[1681.286, "o", "\r\n"]
[1681.296, "o", "    Parameters\r\n"]
[1681.306, "o", "    ----------\r\n"]
[1681.316, "o", "    dictionary : ndarray of shape (n_components, n_features)\r\n"]
[1681.326, "o", "        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n"]
[1681.336, "o", "        normalized to unit norm.\r\n"]
[1681.346, "o", "\r\n"]
[1681.356, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[1681.366, "o", "            'threshold'}, default='omp'\r\n"]
[1681.376, "o", "        Algorithm used to transform the data:\r\n"]
[1681.386, "o", "\r\n"]
[1681.396, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[1681.406, "o", "          (`linear_model.lars_path`);\r\n"]
[1681.416, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n"]
[1681.426, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1681.436, "o", "          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n"]
[1681.446, "o", "          the estimated components are sparse;\r\n"]
[1681.456, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1681.466, "o", "          solution;\r\n"]
[1681.476, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[1681.486, "o", "          the projection ``dictionary * X'``.\r\n"]
[1681.496, "o", "\r\n"]
[1681.506, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[1681.516, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1681.526, "o", "        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n"]
[1681.536, "o", "        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n"]
[1681.546, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1681.556, "o", "\r\n"]
[1681.566, "o", "    transform_alpha : float, default=None\r\n"]
[1681.576, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1681.586, "o", "        penalty applied to the L1 norm.\r\n"]
[1681.596, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1681.606, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1681.616, "o", "        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n"]
[1681.626, "o", "        the reconstruction error targeted. In this case, it overrides\r\n"]
[1681.636, "o", "        `n_nonzero_coefs`.\r\n"]
[1681.646, "o", "        If `None`, default to 1.\r\n"]
[1681.656, "o", "\r\n"]
[1681.666, "o", "    split_sign : bool, default=False\r\n"]
[1681.676, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[1681.686, "o", "        its negative part and its positive part. This can improve the\r\n"]
[1681.696, "o", "        performance of downstream classifiers.\r\n"]
[1681.706, "o", "\r\n"]
[1681.716, "o", "    n_jobs : int, default=None\r\n"]
[1681.726, "o", "        Number of parallel jobs to run.\r\n"]
[1681.736, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1681.746, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1681.756, "o", "        for more details.\r\n"]
[1681.766, "o", "\r\n"]
[1681.776, "o", "    positive_code : bool, default=False\r\n"]
[1681.786, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1681.796, "o", "\r\n"]
[1681.806, "o", "        .. versionadded:: 0.20\r\n"]
[1681.816, "o", "\r\n"]
[1681.826, "o", "    transform_max_iter : int, default=1000\r\n"]
[1681.836, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1681.846, "o", "        `lasso_lars`.\r\n"]
[1681.856, "o", "\r\n"]
[1681.866, "o", "        .. versionadded:: 0.22\r\n"]
[1681.876, "o", "\r\n"]
[1681.886, "o", "    Attributes\r\n"]
[1681.896, "o", "    ----------\r\n"]
[1681.906, "o", "    n_components_ : int\r\n"]
[1681.916, "o", "        Number of atoms.\r\n"]
[1681.926, "o", "\r\n"]
[1681.936, "o", "    n_features_in_ : int\r\n"]
[1681.946, "o", "        Number of features seen during :term:`fit`.\r\n"]
[1681.956, "o", "\r\n"]
[1681.966, "o", "        .. versionadded:: 0.24\r\n"]
[1681.976, "o", "\r\n"]
[1681.986, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[1681.996, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[1682.006, "o", "        has feature names that are all strings.\r\n"]
[1682.016, "o", "\r\n"]
[1682.026, "o", "        .. versionadded:: 1.0\r\n"]
[1682.036, "o", "\r\n"]
[1682.046, "o", "    See Also\r\n"]
[1682.056, "o", "    --------\r\n"]
[1682.066, "o", "    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n"]
[1682.076, "o", "    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n"]
[1682.086, "o", "        dictionary learning algorithm.\r\n"]
[1682.096, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1682.106, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1682.116, "o", "    sparse_encode : Sparse coding where each row of the result is the solution\r\n"]
[1682.126, "o", "        to a sparse coding problem.\r\n"]
[1682.136, "o", "\r\n"]
[1682.146, "o", "    Examples\r\n"]
[1682.156, "o", "    --------\r\n"]
[1682.166, "o", "    >>> import numpy as np\r\n"]
[1682.176, "o", "    >>> from sklearn.decomposition import SparseCoder\r\n"]
[1682.186, "o", "    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n"]
[1682.196, "o", "    >>> dictionary = np.array(\r\n"]
[1682.206, "o", "    ...     [[0, 1, 0],\r\n"]
[1682.216, "o", "    ...      [-1, -1, 2],\r\n"]
[1682.226, "o", "    ...      [1, 1, 1],\r\n"]
[1682.236, "o", "    ...      [0, 1, 1],\r\n"]
[1682.246, "o", "    ...      [0, 2, 1]],\r\n"]
[1682.256, "o", "    ...    dtype=np.float64\r\n"]
[1682.266, "o", "    ... )\r\n"]
[1682.276, "o", "    >>> coder = SparseCoder(\r\n"]
[1682.286, "o", "    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n"]
[1682.296, "o", "    ...     transform_alpha=1e-10,\r\n"]
[1682.306, "o", "    ... )\r\n"]
[1682.316, "o", "    >>> coder.transform(X)\r\n"]
[1682.326, "o", "    array([[ 0.,  0., -1.,  0.,  0.],\r\n"]
[1682.336, "o", "           [ 0.,  1.,  1.,  0.,  0.]])\r\n"]
[1682.346, "o", "    \"\"\"\r\n"]
[1682.356, "o", "\r\n"]
[1682.366, "o", "    _required_parameters = [\"dictionary\"]\r\n"]
[1682.376, "o", "\r\n"]
[1682.386, "o", "    def __init__(\r\n"]
[1682.396, "o", "        self,\r\n"]
[1682.406, "o", "        dictionary,\r\n"]
[1682.416, "o", "        *,\r\n"]
[1682.426, "o", "        transform_algorithm=\"omp\",\r\n"]
[1682.436, "o", "        transform_n_nonzero_coefs=None,\r\n"]
[1682.446, "o", "        transform_alpha=None,\r\n"]
[1682.456, "o", "        split_sign=False,\r\n"]
[1682.466, "o", "        n_jobs=None,\r\n"]
[1682.476, "o", "        positive_code=False,\r\n"]
[1682.486, "o", "        transform_max_iter=1000,\r\n"]
[1682.496, "o", "    ):\r\n"]
[1682.506, "o", "        super().__init__(\r\n"]
[1682.516, "o", "            transform_algorithm,\r\n"]
[1682.526, "o", "            transform_n_nonzero_coefs,\r\n"]
[1682.536, "o", "            transform_alpha,\r\n"]
[1682.546, "o", "            split_sign,\r\n"]
[1682.556, "o", "            n_jobs,\r\n"]
[1682.566, "o", "            positive_code,\r\n"]
[1682.576, "o", "            transform_max_iter,\r\n"]
[1682.586, "o", "        )\r\n"]
[1682.596, "o", "        self.dictionary = dictionary\r\n"]
[1682.606, "o", "\r\n"]
[1682.616, "o", "    def fit(self, X, y=None):\r\n"]
[1682.626, "o", "        \"\"\"Do nothing and return the estimator unchanged.\r\n"]
[1682.636, "o", "\r\n"]
[1682.646, "o", "        This method is just there to implement the usual API and hence\r\n"]
[1682.656, "o", "        work in pipelines.\r\n"]
[1682.666, "o", "\r\n"]
[1682.676, "o", "        Parameters\r\n"]
[1682.686, "o", "        ----------\r\n"]
[1682.696, "o", "        X : Ignored\r\n"]
[1682.706, "o", "            Not used, present for API consistency by convention.\r\n"]
[1682.716, "o", "\r\n"]
[1682.726, "o", "        y : Ignored\r\n"]
[1682.736, "o", "            Not used, present for API consistency by convention.\r\n"]
[1682.746, "o", "\r\n"]
[1682.756, "o", "        Returns\r\n"]
[1682.766, "o", "        -------\r\n"]
[1682.776, "o", "        self : object\r\n"]
[1682.786, "o", "            Returns the instance itself.\r\n"]
[1682.796, "o", "        \"\"\"\r\n"]
[1682.806, "o", "        return self\r\n"]
[1682.816, "o", "\r\n"]
[1682.826, "o", "    def transform(self, X, y=None):\r\n"]
[1682.836, "o", "        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n"]
[1682.846, "o", "\r\n"]
[1682.856, "o", "        Coding method is determined by the object parameter\r\n"]
[1682.866, "o", "        `transform_algorithm`.\r\n"]
[1682.876, "o", "\r\n"]
[1682.886, "o", "        Parameters\r\n"]
[1682.896, "o", "        ----------\r\n"]
[1682.906, "o", "        X : ndarray of shape (n_samples, n_features)\r\n"]
[1682.916, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1682.926, "o", "            and `n_features` is the number of features.\r\n"]
[1682.936, "o", "\r\n"]
[1682.946, "o", "        y : Ignored\r\n"]
[1682.956, "o", "            Not used, present for API consistency by convention.\r\n"]
[1682.966, "o", "\r\n"]
[1682.976, "o", "        Returns\r\n"]
[1682.986, "o", "        -------\r\n"]
[1682.996, "o", "        X_new : ndarray of shape (n_samples, n_components)\r\n"]
[1683.006, "o", "            Transformed data.\r\n"]
[1683.016, "o", "        \"\"\"\r\n"]
[1683.026, "o", "        return super()._transform(X, self.dictionary)\r\n"]
[1683.036, "o", "\r\n"]
[1683.046, "o", "    def _more_tags(self):\r\n"]
[1683.056, "o", "        return {\r\n"]
[1683.066, "o", "            \"requires_fit\": False,\r\n"]
[1683.076, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[1683.086, "o", "        }\r\n"]
[1683.096, "o", "\r\n"]
[1683.106, "o", "    @property\r\n"]
[1683.116, "o", "    def n_components_(self):\r\n"]
[1683.126, "o", "        \"\"\"Number of atoms.\"\"\"\r\n"]
[1683.136, "o", "        return self.dictionary.shape[0]\r\n"]
[1683.146, "o", "\r\n"]
[1683.156, "o", "    @property\r\n"]
[1683.166, "o", "    def n_features_in_(self):\r\n"]
[1683.176, "o", "        \"\"\"Number of features seen during `fit`.\"\"\"\r\n"]
[1683.186, "o", "        return self.dictionary.shape[1]\r\n"]
[1683.196, "o", "\r\n"]
[1683.206, "o", "    @property\r\n"]
[1683.216, "o", "    def _n_features_out(self):\r\n"]
[1683.226, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[1683.236, "o", "        return self.n_components_\r\n"]
[1683.246, "o", "\r\n"]
[1683.256, "o", "\r\n"]
[1683.266, "o", "class DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[1683.276, "o", "    \"\"\"Dictionary learning.\r\n"]
[1683.286, "o", "\r\n"]
[1683.296, "o", "    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[1683.306, "o", "    encoding the fitted data.\r\n"]
[1683.316, "o", "\r\n"]
[1683.326, "o", "    Solves the optimization problem::\r\n"]
[1683.336, "o", "\r\n"]
[1683.346, "o", "        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[1683.356, "o", "                    (U,V)\r\n"]
[1683.366, "o", "                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[1683.376, "o", "\r\n"]
[1683.386, "o", "    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[1683.396, "o", "    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[1683.406, "o", "    of all the entries in the matrix.\r\n"]
[1683.416, "o", "\r\n"]
[1683.426, "o", "    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[1683.436, "o", "\r\n"]
[1683.446, "o", "    Parameters\r\n"]
[1683.456, "o", "    ----------\r\n"]
[1683.466, "o", "    n_components : int, default=None\r\n"]
[1683.476, "o", "        Number of dictionary elements to extract. If None, then ``n_components``\r\n"]
[1683.486, "o", "        is set to ``n_features``.\r\n"]
[1683.496, "o", "\r\n"]
[1683.506, "o", "    alpha : float, default=1.0\r\n"]
[1683.516, "o", "        Sparsity controlling parameter.\r\n"]
[1683.526, "o", "\r\n"]
[1683.536, "o", "    max_iter : int, default=1000\r\n"]
[1683.546, "o", "        Maximum number of iterations to perform.\r\n"]
[1683.556, "o", "\r\n"]
[1683.566, "o", "    tol : float, default=1e-8\r\n"]
[1683.576, "o", "        Tolerance for numerical error.\r\n"]
[1683.586, "o", "\r\n"]
[1683.596, "o", "    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[1683.606, "o", "        * `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[1683.616, "o", "          problem (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[1683.626, "o", "        * `'cd'`: uses the coordinate descent method to compute the\r\n"]
[1683.636, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n"]
[1683.646, "o", "          faster if the estimated components are sparse.\r\n"]
[1683.656, "o", "\r\n"]
[1683.666, "o", "        .. versionadded:: 0.17\r\n"]
[1683.676, "o", "           *cd* coordinate descent method to improve speed.\r\n"]
[1683.686, "o", "\r\n"]
[1683.696, "o", "    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[1683.706, "o", "            'threshold'}, default='omp'\r\n"]
[1683.716, "o", "        Algorithm used to transform the data:\r\n"]
[1683.726, "o", "\r\n"]
[1683.736, "o", "        - `'lars'`: uses the least angle regression method\r\n"]
[1683.746, "o", "          (:func:`~sklearn.linear_model.lars_path`);\r\n"]
[1683.756, "o", "        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[1683.766, "o", "        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1683.776, "o", "          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n"]
[1683.786, "o", "          will be faster if the estimated components are sparse.\r\n"]
[1683.796, "o", "        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1683.806, "o", "          solution.\r\n"]
[1683.816, "o", "        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[1683.826, "o", "          the projection ``dictionary * X'``.\r\n"]
[1683.836, "o", "\r\n"]
[1683.846, "o", "        .. versionadded:: 0.17\r\n"]
[1683.856, "o", "           *lasso_cd* coordinate descent method to improve speed.\r\n"]
[1683.866, "o", "\r\n"]
[1683.876, "o", "    transform_n_nonzero_coefs : int, default=None\r\n"]
[1683.886, "o", "        Number of nonzero coefficients to target in each column of the\r\n"]
[1683.896, "o", "        solution. This is only used by `algorithm='lars'` and\r\n"]
[1683.906, "o", "        `algorithm='omp'`. If `None`, then\r\n"]
[1683.916, "o", "        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n"]
[1683.926, "o", "\r\n"]
[1683.936, "o", "    transform_alpha : float, default=None\r\n"]
[1683.946, "o", "        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n"]
[1683.956, "o", "        penalty applied to the L1 norm.\r\n"]
[1683.966, "o", "        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n"]
[1683.976, "o", "        threshold below which coefficients will be squashed to zero.\r\n"]
[1683.986, "o", "        If `None`, defaults to `alpha`.\r\n"]
[1683.996, "o", "\r\n"]
[1684.006, "o", "        .. versionchanged:: 1.2\r\n"]
[1684.016, "o", "            When None, default value changed from 1.0 to `alpha`.\r\n"]
[1684.026, "o", "\r\n"]
[1684.036, "o", "    n_jobs : int or None, default=None\r\n"]
[1684.046, "o", "        Number of parallel jobs to run.\r\n"]
[1684.056, "o", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1684.066, "o", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1684.076, "o", "        for more details.\r\n"]
[1684.086, "o", "\r\n"]
[1684.096, "o", "    code_init : ndarray of shape (n_samples, n_components), default=None\r\n"]
[1684.106, "o", "        Initial value for the code, for warm restart. Only used if `code_init`\r\n"]
[1684.116, "o", "        and `dict_init` are not None.\r\n"]
[1684.126, "o", "\r\n"]
[1684.136, "o", "    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1684.146, "o", "        Initial values for the dictionary, for warm restart. Only used if\r\n"]
[1684.156, "o", "        `code_init` and `dict_init` are not None.\r\n"]
[1684.166, "o", "\r\n"]
[1684.176, "o", "    callback : callable, default=None\r\n"]
[1684.186, "o", "        Callable that gets invoked every five iterations.\r\n"]
[1684.196, "o", "\r\n"]
[1684.206, "o", "        .. versionadded:: 1.3\r\n"]
[1684.216, "o", "\r\n"]
[1684.226, "o", "    verbose : bool, default=False\r\n"]
[1684.236, "o", "        To control the verbosity of the procedure.\r\n"]
[1684.246, "o", "\r\n"]
[1684.256, "o", "    split_sign : bool, default=False\r\n"]
[1684.266, "o", "        Whether to split the sparse feature vector into the concatenation of\r\n"]
[1684.276, "o", "        its negative part and its positive part. This can improve the\r\n"]
[1684.286, "o", "        performance of downstream classifiers.\r\n"]
[1684.296, "o", "\r\n"]
[1684.306, "o", "    random_state : int, RandomState instance or None, default=None\r\n"]
[1684.316, "o", "        Used for initializing the dictionary when ``dict_init`` is not\r\n"]
[1684.326, "o", "        specified, randomly shuffling the data when ``shuffle`` is set to\r\n"]
[1684.336, "o", "        ``True``, and updating the dictionary. Pass an int for reproducible\r\n"]
[1684.346, "o", "        results across multiple function calls.\r\n"]
[1684.356, "o", "        See :term:`Glossary <random_state>`.\r\n"]
[1684.366, "o", "\r\n"]
[1684.376, "o", "    positive_code : bool, default=False\r\n"]
[1684.386, "o", "        Whether to enforce positivity when finding the code.\r\n"]
[1684.396, "o", "\r\n"]
[1684.406, "o", "        .. versionadded:: 0.20\r\n"]
[1684.416, "o", "\r\n"]
[1684.426, "o", "    positive_dict : bool, default=False\r\n"]
[1684.436, "o", "        Whether to enforce positivity when finding the dictionary.\r\n"]
[1684.446, "o", "\r\n"]
[1684.456, "o", "        .. versionadded:: 0.20\r\n"]
[1684.466, "o", "\r\n"]
[1684.476, "o", "    transform_max_iter : int, default=1000\r\n"]
[1684.486, "o", "        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n"]
[1684.496, "o", "        `'lasso_lars'`.\r\n"]
[1684.506, "o", "\r\n"]
[1684.516, "o", "        .. versionadded:: 0.22\r\n"]
[1684.526, "o", "\r\n"]
[1684.536, "o", "    Attributes\r\n"]
[1684.546, "o", "    ----------\r\n"]
[1684.556, "o", "    components_ : ndarray of shape (n_components, n_features)\r\n"]
[1684.566, "o", "        dictionary atoms extracted from the data\r\n"]
[1684.576, "o", "\r\n"]
[1684.586, "o", "    error_ : array\r\n"]
[1684.596, "o", "        vector of errors at each iteration\r\n"]
[1684.606, "o", "\r\n"]
[1684.616, "o", "    n_features_in_ : int\r\n"]
[1684.626, "o", "        Number of features seen during :term:`fit`.\r\n"]
[1684.636, "o", "\r\n"]
[1684.646, "o", "        .. versionadded:: 0.24\r\n"]
[1684.656, "o", "\r\n"]
[1684.666, "o", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n"]
[1684.676, "o", "        Names of features seen during :term:`fit`. Defined only when `X`\r\n"]
[1684.686, "o", "        has feature names that are all strings.\r\n"]
[1684.696, "o", "\r\n"]
[1684.706, "o", "        .. versionadded:: 1.0\r\n"]
[1684.716, "o", "\r\n"]
[1684.726, "o", "    n_iter_ : int\r\n"]
[1684.736, "o", "        Number of iterations run.\r\n"]
[1684.746, "o", "\r\n"]
[1684.756, "o", "    See Also\r\n"]
[1684.766, "o", "    --------\r\n"]
[1684.776, "o", "    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n"]
[1684.786, "o", "        dictionary learning algorithm.\r\n"]
[1684.796, "o", "    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n"]
[1684.806, "o", "    SparseCoder : Find a sparse representation of data from a fixed,\r\n"]
[1684.816, "o", "        precomputed dictionary.\r\n"]
[1684.826, "o", "    SparsePCA : Sparse Principal Components Analysis.\r\n"]
[1684.836, "o", "\r\n"]
[1684.846, "o", "    References\r\n"]
[1684.856, "o", "    ----------\r\n"]
[1684.866, "o", "\r\n"]
[1684.876, "o", "    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n"]
[1684.886, "o", "    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n"]
[1684.896, "o", "\r\n"]
[1684.906, "o", "    Examples\r\n"]
[1684.916, "o", "    --------\r\n"]
[1684.926, "o", "    >>> import numpy as np\r\n"]
[1684.936, "o", "    >>> from sklearn.datasets import make_sparse_coded_signal\r\n"]
[1685.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1685.002, "i", "sed -n '2200,3400p' sklearn/decomposition/_dict_learning.py\r"]
[1685.004, "o", "sed -n '2200,3400p' sklearn/decomposition/_dict_learning.py\r\n"]
[1685.022703, "o", "\u001b[?2004l\r\n"]
[1685.039405, "o", "\r\n"]
[1685.056108, "o", "        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n"]
[1685.072811, "o", "        dictionary = np.require(dictionary, requirements=\"W\")\r\n"]
[1685.089514, "o", "\r\n"]
[1685.106216, "o", "        return dictionary\r\n"]
[1685.122919, "o", "\r\n"]
[1685.139622, "o", "    def _update_inner_stats(self, X, code, batch_size, step):\r\n"]
[1685.156324, "o", "        \"\"\"Update the inner stats inplace.\"\"\"\r\n"]
[1685.173027, "o", "        if step < batch_size - 1:\r\n"]
[1685.18973, "o", "            theta = (step + 1) * batch_size\r\n"]
[1685.206432, "o", "        else:\r\n"]
[1685.223135, "o", "            theta = batch_size**2 + step + 1 - batch_size\r\n"]
[1685.239838, "o", "        beta = (theta + 1 - batch_size) / (theta + 1)\r\n"]
[1685.256541, "o", "\r\n"]
[1685.273243, "o", "        self._A *= beta\r\n"]
[1685.289946, "o", "        self._A += code.T @ code / batch_size\r\n"]
[1685.306649, "o", "        self._B *= beta\r\n"]
[1685.323351, "o", "        self._B += X.T @ code / batch_size\r\n"]
[1685.340054, "o", "\r\n"]
[1685.356757, "o", "    def _minibatch_step(self, X, dictionary, random_state, step):\r\n"]
[1685.373459, "o", "        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n"]
[1685.390162, "o", "        batch_size = X.shape[0]\r\n"]
[1685.406865, "o", "\r\n"]
[1685.423568, "o", "        # Compute code for this batch\r\n"]
[1685.44027, "o", "        code = _sparse_encode(\r\n"]
[1685.456973, "o", "            X,\r\n"]
[1685.473676, "o", "            dictionary,\r\n"]
[1685.490378, "o", "            algorithm=self._fit_algorithm,\r\n"]
[1685.507081, "o", "            alpha=self.alpha,\r\n"]
[1685.523784, "o", "            n_jobs=self.n_jobs,\r\n"]
[1685.540486, "o", "            positive=self.positive_code,\r\n"]
[1685.557189, "o", "            max_iter=self.transform_max_iter,\r\n"]
[1685.573892, "o", "            verbose=self.verbose,\r\n"]
[1685.590595, "o", "        )\r\n"]
[1685.607297, "o", "\r\n"]
[1685.624, "o", "        batch_cost = (\r\n"]
[1685.640703, "o", "            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n"]
[1685.657405, "o", "            + self.alpha * np.sum(np.abs(code))\r\n"]
[1685.674108, "o", "        ) / batch_size\r\n"]
[1685.690811, "o", "\r\n"]
[1685.707514, "o", "        # Update inner stats\r\n"]
[1685.724216, "o", "        self._update_inner_stats(X, code, batch_size, step)\r\n"]
[1685.740919, "o", "\r\n"]
[1685.757622, "o", "        # Update dictionary\r\n"]
[1685.774324, "o", "        _update_dict(\r\n"]
[1685.791027, "o", "            dictionary,\r\n"]
[1685.80773, "o", "            X,\r\n"]
[1685.824432, "o", "            code,\r\n"]
[1685.841135, "o", "            self._A,\r\n"]
[1685.857838, "o", "            self._B,\r\n"]
[1685.874541, "o", "            verbose=self.verbose,\r\n"]
[1685.891243, "o", "            random_state=random_state,\r\n"]
[1685.907946, "o", "            positive=self.positive_dict,\r\n"]
[1685.924649, "o", "        )\r\n"]
[1685.941351, "o", "\r\n"]
[1685.958054, "o", "        return batch_cost\r\n"]
[1685.974757, "o", "\r\n"]
[1685.991459, "o", "    def _check_convergence(\r\n"]
[1686.008162, "o", "        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n"]
[1686.024865, "o", "    ):\r\n"]
[1686.041568, "o", "        \"\"\"Helper function to encapsulate the early stopping logic.\r\n"]
[1686.05827, "o", "\r\n"]
[1686.074973, "o", "        Early stopping is based on two factors:\r\n"]
[1686.091676, "o", "        - A small change of the dictionary between two minibatch updates. This is\r\n"]
[1686.108378, "o", "          controlled by the tol parameter.\r\n"]
[1686.125081, "o", "        - No more improvement on a smoothed estimate of the objective function for a\r\n"]
[1686.141784, "o", "          a certain number of consecutive minibatch updates. This is controlled by\r\n"]
[1686.158486, "o", "          the max_no_improvement parameter.\r\n"]
[1686.175189, "o", "        \"\"\"\r\n"]
[1686.191892, "o", "        batch_size = X.shape[0]\r\n"]
[1686.208595, "o", "\r\n"]
[1686.225297, "o", "        # counts steps starting from 1 for user friendly verbose mode.\r\n"]
[1686.242, "o", "        step = step + 1\r\n"]
[1686.258703, "o", "\r\n"]
[1686.275405, "o", "        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n"]
[1686.292108, "o", "        # too bad value\r\n"]
[1686.308811, "o", "        if step <= min(100, n_samples / batch_size):\r\n"]
[1686.325514, "o", "            if self.verbose:\r\n"]
[1686.342216, "o", "                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n"]
[1686.358919, "o", "            return False\r\n"]
[1686.375622, "o", "\r\n"]
[1686.392324, "o", "        # Compute an Exponentially Weighted Average of the cost function to\r\n"]
[1686.409027, "o", "        # monitor the convergence while discarding minibatch-local stochastic\r\n"]
[1686.42573, "o", "        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n"]
[1686.442432, "o", "        if self._ewa_cost is None:\r\n"]
[1686.459135, "o", "            self._ewa_cost = batch_cost\r\n"]
[1686.475838, "o", "        else:\r\n"]
[1686.492541, "o", "            alpha = batch_size / (n_samples + 1)\r\n"]
[1686.509243, "o", "            alpha = min(alpha, 1)\r\n"]
[1686.525946, "o", "            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n"]
[1686.542649, "o", "\r\n"]
[1686.559351, "o", "        if self.verbose:\r\n"]
[1686.576054, "o", "            print(\r\n"]
[1686.592757, "o", "                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\r\n"]
[1686.609459, "o", "                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\r\n"]
[1686.626162, "o", "            )\r\n"]
[1686.642865, "o", "\r\n"]
[1686.659568, "o", "        # Early stopping based on change of dictionary\r\n"]
[1686.67627, "o", "        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\r\n"]
[1686.692973, "o", "        if self.tol > 0 and dict_diff <= self.tol:\r\n"]
[1686.709676, "o", "            if self.verbose:\r\n"]
[1686.726378, "o", "                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\r\n"]
[1686.743081, "o", "            return True\r\n"]
[1686.759784, "o", "\r\n"]
[1686.776486, "o", "        # Early stopping heuristic due to lack of improvement on smoothed\r\n"]
[1686.793189, "o", "        # cost function\r\n"]
[1686.809892, "o", "        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\r\n"]
[1686.826595, "o", "            self._no_improvement = 0\r\n"]
[1686.843297, "o", "            self._ewa_cost_min = self._ewa_cost\r\n"]
[1686.86, "o", "        else:\r\n"]
[1686.876703, "o", "            self._no_improvement += 1\r\n"]
[1686.893405, "o", "\r\n"]
[1686.910108, "o", "        if (\r\n"]
[1686.926811, "o", "            self.max_no_improvement is not None\r\n"]
[1686.943514, "o", "            and self._no_improvement >= self.max_no_improvement\r\n"]
[1686.960216, "o", "        ):\r\n"]
[1686.976919, "o", "            if self.verbose:\r\n"]
[1686.993622, "o", "                print(\r\n"]
[1687.010324, "o", "                    \"Converged (lack of improvement in objective function) \"\r\n"]
[1687.027027, "o", "                    f\"at step {step}/{n_steps}\"\r\n"]
[1687.04373, "o", "                )\r\n"]
[1687.060432, "o", "            return True\r\n"]
[1687.077135, "o", "\r\n"]
[1687.093838, "o", "        return False\r\n"]
[1687.110541, "o", "\r\n"]
[1687.127243, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[1687.143946, "o", "    def fit(self, X, y=None):\r\n"]
[1687.160649, "o", "        \"\"\"Fit the model from data in X.\r\n"]
[1687.177351, "o", "\r\n"]
[1687.194054, "o", "        Parameters\r\n"]
[1687.210757, "o", "        ----------\r\n"]
[1687.227459, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[1687.244162, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1687.260865, "o", "            and `n_features` is the number of features.\r\n"]
[1687.277568, "o", "\r\n"]
[1687.29427, "o", "        y : Ignored\r\n"]
[1687.310973, "o", "            Not used, present for API consistency by convention.\r\n"]
[1687.327676, "o", "\r\n"]
[1687.344378, "o", "        Returns\r\n"]
[1687.361081, "o", "        -------\r\n"]
[1687.377784, "o", "        self : object\r\n"]
[1687.394486, "o", "            Returns the instance itself.\r\n"]
[1687.411189, "o", "        \"\"\"\r\n"]
[1687.427892, "o", "        X = self._validate_data(\r\n"]
[1687.444595, "o", "            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n"]
[1687.461297, "o", "        )\r\n"]
[1687.478, "o", "\r\n"]
[1687.494703, "o", "        self._check_params(X)\r\n"]
[1687.511405, "o", "\r\n"]
[1687.528108, "o", "        if self.n_iter != \"deprecated\":\r\n"]
[1687.544811, "o", "            warnings.warn(\r\n"]
[1687.561514, "o", "                (\r\n"]
[1687.578216, "o", "                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n"]
[1687.594919, "o", "                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n"]
[1687.611622, "o", "                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n"]
[1687.628324, "o", "                    \"specified.\"\r\n"]
[1687.645027, "o", "                ),\r\n"]
[1687.66173, "o", "                FutureWarning,\r\n"]
[1687.678432, "o", "            )\r\n"]
[1687.695135, "o", "            n_iter = self.n_iter\r\n"]
[1687.711838, "o", "\r\n"]
[1687.728541, "o", "        self._random_state = check_random_state(self.random_state)\r\n"]
[1687.745243, "o", "\r\n"]
[1687.761946, "o", "        dictionary = self._initialize_dict(X, self._random_state)\r\n"]
[1687.778649, "o", "        old_dict = dictionary.copy()\r\n"]
[1687.795351, "o", "\r\n"]
[1687.812054, "o", "        if self.shuffle:\r\n"]
[1687.828757, "o", "            X_train = X.copy()\r\n"]
[1687.845459, "o", "            self._random_state.shuffle(X_train)\r\n"]
[1687.862162, "o", "        else:\r\n"]
[1687.878865, "o", "            X_train = X\r\n"]
[1687.895568, "o", "\r\n"]
[1687.91227, "o", "        n_samples, n_features = X_train.shape\r\n"]
[1687.928973, "o", "\r\n"]
[1687.945676, "o", "        if self.verbose:\r\n"]
[1687.962378, "o", "            print(\"[dict_learning]\")\r\n"]
[1687.979081, "o", "\r\n"]
[1687.995784, "o", "        # Inner stats\r\n"]
[1688.012486, "o", "        self._A = np.zeros(\r\n"]
[1688.029189, "o", "            (self._n_components, self._n_components), dtype=X_train.dtype\r\n"]
[1688.045892, "o", "        )\r\n"]
[1688.062595, "o", "        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n"]
[1688.079297, "o", "\r\n"]
[1688.096, "o", "        if self.max_iter is not None:\r\n"]
[1688.112703, "o", "            # Attributes to monitor the convergence\r\n"]
[1688.129405, "o", "            self._ewa_cost = None\r\n"]
[1688.146108, "o", "            self._ewa_cost_min = None\r\n"]
[1688.162811, "o", "            self._no_improvement = 0\r\n"]
[1688.179514, "o", "\r\n"]
[1688.196216, "o", "            batches = gen_batches(n_samples, self._batch_size)\r\n"]
[1688.212919, "o", "            batches = itertools.cycle(batches)\r\n"]
[1688.229622, "o", "            n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\r\n"]
[1688.246324, "o", "            n_steps = self.max_iter * n_steps_per_iter\r\n"]
[1688.263027, "o", "\r\n"]
[1688.27973, "o", "            i = -1  # to allow max_iter = 0\r\n"]
[1688.296432, "o", "\r\n"]
[1688.313135, "o", "            for i, batch in zip(range(n_steps), batches):\r\n"]
[1688.329838, "o", "                X_batch = X_train[batch]\r\n"]
[1688.346541, "o", "\r\n"]
[1688.363243, "o", "                batch_cost = self._minibatch_step(\r\n"]
[1688.379946, "o", "                    X_batch, dictionary, self._random_state, i\r\n"]
[1688.396649, "o", "                )\r\n"]
[1688.413351, "o", "\r\n"]
[1688.430054, "o", "                if self._check_convergence(\r\n"]
[1688.446757, "o", "                    X_batch, batch_cost, dictionary, old_dict, n_samples, i, n_steps\r\n"]
[1688.463459, "o", "                ):\r\n"]
[1688.480162, "o", "                    break\r\n"]
[1688.496865, "o", "\r\n"]
[1688.513568, "o", "                # XXX callback param added for backward compat in #18975 but a common\r\n"]
[1688.53027, "o", "                # unified callback API should be preferred\r\n"]
[1688.546973, "o", "                if self.callback is not None:\r\n"]
[1688.563676, "o", "                    self.callback(locals())\r\n"]
[1688.580378, "o", "\r\n"]
[1688.597081, "o", "                old_dict[:] = dictionary\r\n"]
[1688.613784, "o", "\r\n"]
[1688.630486, "o", "            self.n_steps_ = i + 1\r\n"]
[1688.647189, "o", "            self.n_iter_ = np.ceil(self.n_steps_ / n_steps_per_iter)\r\n"]
[1688.663892, "o", "        else:\r\n"]
[1688.680595, "o", "            # TODO remove this branch in 1.4\r\n"]
[1688.697297, "o", "            n_iter = 1000 if self.n_iter == \"deprecated\" else self.n_iter\r\n"]
[1688.714, "o", "\r\n"]
[1688.730703, "o", "            batches = gen_batches(n_samples, self._batch_size)\r\n"]
[1688.747405, "o", "            batches = itertools.cycle(batches)\r\n"]
[1688.764108, "o", "\r\n"]
[1688.780811, "o", "            for i, batch in zip(range(n_iter), batches):\r\n"]
[1688.797514, "o", "                self._minibatch_step(X_train[batch], dictionary, self._random_state, i)\r\n"]
[1688.814216, "o", "\r\n"]
[1688.830919, "o", "                trigger_verbose = self.verbose and i % ceil(100.0 / self.verbose) == 0\r\n"]
[1688.847622, "o", "                if self.verbose > 10 or trigger_verbose:\r\n"]
[1688.864324, "o", "                    print(f\"{i} batches processed.\")\r\n"]
[1688.881027, "o", "\r\n"]
[1688.89773, "o", "                if self.callback is not None:\r\n"]
[1688.914432, "o", "                    self.callback(locals())\r\n"]
[1688.931135, "o", "\r\n"]
[1688.947838, "o", "            self.n_steps_ = n_iter\r\n"]
[1688.964541, "o", "            self.n_iter_ = np.ceil(n_iter / int(np.ceil(n_samples / self._batch_size)))\r\n"]
[1688.981243, "o", "\r\n"]
[1688.997946, "o", "        self.components_ = dictionary\r\n"]
[1689.014649, "o", "\r\n"]
[1689.031351, "o", "        return self\r\n"]
[1689.048054, "o", "\r\n"]
[1689.064757, "o", "    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[1689.081459, "o", "    def partial_fit(self, X, y=None):\r\n"]
[1689.098162, "o", "        \"\"\"Update the model using the data in X as a mini-batch.\r\n"]
[1689.114865, "o", "\r\n"]
[1689.131568, "o", "        Parameters\r\n"]
[1689.14827, "o", "        ----------\r\n"]
[1689.164973, "o", "        X : array-like of shape (n_samples, n_features)\r\n"]
[1689.181676, "o", "            Training vector, where `n_samples` is the number of samples\r\n"]
[1689.198378, "o", "            and `n_features` is the number of features.\r\n"]
[1689.215081, "o", "\r\n"]
[1689.231784, "o", "        y : Ignored\r\n"]
[1689.248486, "o", "            Not used, present for API consistency by convention.\r\n"]
[1689.265189, "o", "\r\n"]
[1689.281892, "o", "        Returns\r\n"]
[1689.298595, "o", "        -------\r\n"]
[1689.315297, "o", "        self : object\r\n"]
[1689.332, "o", "            Return the instance itself.\r\n"]
[1689.348703, "o", "        \"\"\"\r\n"]
[1689.365405, "o", "        has_components = hasattr(self, \"components_\")\r\n"]
[1689.382108, "o", "\r\n"]
[1689.398811, "o", "        X = self._validate_data(\r\n"]
[1689.415514, "o", "            X, dtype=[np.float64, np.float32], order=\"C\", reset=not has_components\r\n"]
[1689.432216, "o", "        )\r\n"]
[1689.448919, "o", "\r\n"]
[1689.465622, "o", "        if not has_components:\r\n"]
[1689.482324, "o", "            # This instance has not been fitted yet (fit or partial_fit)\r\n"]
[1689.499027, "o", "            self._check_params(X)\r\n"]
[1689.51573, "o", "            self._random_state = check_random_state(self.random_state)\r\n"]
[1689.532432, "o", "\r\n"]
[1689.549135, "o", "            dictionary = self._initialize_dict(X, self._random_state)\r\n"]
[1689.565838, "o", "\r\n"]
[1689.582541, "o", "            self.n_steps_ = 0\r\n"]
[1689.599243, "o", "\r\n"]
[1689.615946, "o", "            self._A = np.zeros((self._n_components, self._n_components), dtype=X.dtype)\r\n"]
[1689.632649, "o", "            self._B = np.zeros((X.shape[1], self._n_components), dtype=X.dtype)\r\n"]
[1689.649351, "o", "        else:\r\n"]
[1689.666054, "o", "            dictionary = self.components_\r\n"]
[1689.682757, "o", "\r\n"]
[1689.699459, "o", "        self._minibatch_step(X, dictionary, self._random_state, self.n_steps_)\r\n"]
[1689.716162, "o", "\r\n"]
[1689.732865, "o", "        self.components_ = dictionary\r\n"]
[1689.749568, "o", "        self.n_steps_ += 1\r\n"]
[1689.76627, "o", "\r\n"]
[1689.782973, "o", "        return self\r\n"]
[1689.799676, "o", "\r\n"]
[1689.816378, "o", "    @property\r\n"]
[1689.833081, "o", "    def _n_features_out(self):\r\n"]
[1689.849784, "o", "        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[1689.866486, "o", "        return self.components_.shape[0]\r\n"]
[1689.883189, "o", "\r\n"]
[1689.899892, "o", "    def _more_tags(self):\r\n"]
[1689.916595, "o", "        return {\r\n"]
[1689.933297, "o", "            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[1690.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1690.002, "i", "grep -n \"old_dict\" sklearn/decomposition/_dict_learning.py\r"]
[1690.004, "o", "grep -n \"old_dict\" sklearn/decomposition/_dict_learning.py\r\n"]
[1690.83, "o", "\u001b[?2004l\r\n"]
[1691.654, "o", "\u001b[32m\u001b[K2258\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        self, X, batch_cost, new_dict, \u001b[01;31m\u001b[Kold_dict\u001b[m\u001b[K, n_samples, step, n_steps\r\n"]
[1692.478, "o", "\u001b[32m\u001b[K2298\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        dict_diff = linalg.norm(new_dict - \u001b[01;31m\u001b[Kold_dict\u001b[m\u001b[K) / self._n_components\r\n"]
[1693.302, "o", "\u001b[32m\u001b[K2364\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kold_dict\u001b[m\u001b[K = dictionary.copy()\r\n"]
[1694.126, "o", "\u001b[32m\u001b[K2404\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                    X_batch, batch_cost, dictionary, \u001b[01;31m\u001b[Kold_dict\u001b[m\u001b[K, n_samples, i, n_steps\r\n"]
[1695.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1695.002, "i", "nl -ba sklearn/decomposition/_dict_learning.py | sed -n '1,2600p' | sed -n '1750,1950p'\r"]
[1695.004, "o", "nl -ba sklearn/decomposition/_dict_learning.py | sed -n '1,2600p' | sed -n '1750,1950p'\r\n"]
[1695.030475, "o", "\u001b[?2004l\r\n"]
[1695.05495, "o", "  1750\t        transform_alpha=None,\r\n"]
[1695.079426, "o", "  1751\t        n_jobs=None,\r\n"]
[1695.103901, "o", "  1752\t        code_init=None,\r\n"]
[1695.128376, "o", "  1753\t        dict_init=None,\r\n"]
[1695.152851, "o", "  1754\t        callback=None,\r\n"]
[1695.177327, "o", "  1755\t        verbose=False,\r\n"]
[1695.201802, "o", "  1756\t        split_sign=False,\r\n"]
[1695.226277, "o", "  1757\t        random_state=None,\r\n"]
[1695.250752, "o", "  1758\t        positive_code=False,\r\n"]
[1695.275228, "o", "  1759\t        positive_dict=False,\r\n"]
[1695.299703, "o", "  1760\t        transform_max_iter=1000,\r\n"]
[1695.324178, "o", "  1761\t    ):\r\n"]
[1695.348653, "o", "  1762\t        super().__init__(\r\n"]
[1695.373129, "o", "  1763\t            transform_algorithm,\r\n"]
[1695.397604, "o", "  1764\t            transform_n_nonzero_coefs,\r\n"]
[1695.422079, "o", "  1765\t            transform_alpha,\r\n"]
[1695.446554, "o", "  1766\t            split_sign,\r\n"]
[1695.47103, "o", "  1767\t            n_jobs,\r\n"]
[1695.495505, "o", "  1768\t            positive_code,\r\n"]
[1695.51998, "o", "  1769\t            transform_max_iter,\r\n"]
[1695.544455, "o", "  1770\t        )\r\n"]
[1695.568931, "o", "  1771\t        self.n_components = n_components\r\n"]
[1695.593406, "o", "  1772\t        self.alpha = alpha\r\n"]
[1695.617881, "o", "  1773\t        self.max_iter = max_iter\r\n"]
[1695.642356, "o", "  1774\t        self.tol = tol\r\n"]
[1695.666832, "o", "  1775\t        self.fit_algorithm = fit_algorithm\r\n"]
[1695.691307, "o", "  1776\t        self.code_init = code_init\r\n"]
[1695.715782, "o", "  1777\t        self.dict_init = dict_init\r\n"]
[1695.740257, "o", "  1778\t        self.callback = callback\r\n"]
[1695.764733, "o", "  1779\t        self.verbose = verbose\r\n"]
[1695.789208, "o", "  1780\t        self.random_state = random_state\r\n"]
[1695.813683, "o", "  1781\t        self.positive_dict = positive_dict\r\n"]
[1695.838158, "o", "  1782\t\r\n"]
[1695.862634, "o", "  1783\t    def fit(self, X, y=None):\r\n"]
[1695.887109, "o", "  1784\t        \"\"\"Fit the model from data in X.\r\n"]
[1695.911584, "o", "  1785\t\r\n"]
[1695.936059, "o", "  1786\t        Parameters\r\n"]
[1695.960535, "o", "  1787\t        ----------\r\n"]
[1695.98501, "o", "  1788\t        X : array-like of shape (n_samples, n_features)\r\n"]
[1696.009485, "o", "  1789\t            Training vector, where `n_samples` is the number of samples\r\n"]
[1696.03396, "o", "  1790\t            and `n_features` is the number of features.\r\n"]
[1696.058436, "o", "  1791\t\r\n"]
[1696.082911, "o", "  1792\t        y : Ignored\r\n"]
[1696.107386, "o", "  1793\t            Not used, present for API consistency by convention.\r\n"]
[1696.131861, "o", "  1794\t\r\n"]
[1696.156337, "o", "  1795\t        Returns\r\n"]
[1696.180812, "o", "  1796\t        -------\r\n"]
[1696.205287, "o", "  1797\t        self : object\r\n"]
[1696.229762, "o", "  1798\t            Returns the instance itself.\r\n"]
[1696.254238, "o", "  1799\t        \"\"\"\r\n"]
[1696.278713, "o", "  1800\t        self.fit_transform(X)\r\n"]
[1696.303188, "o", "  1801\t        return self\r\n"]
[1696.327663, "o", "  1802\t\r\n"]
[1696.352139, "o", "  1803\t    @_fit_context(prefer_skip_nested_validation=True)\r\n"]
[1696.376614, "o", "  1804\t    def fit_transform(self, X, y=None):\r\n"]
[1696.401089, "o", "  1805\t        \"\"\"Fit the model from data in X and return the transformed data.\r\n"]
[1696.425564, "o", "  1806\t\r\n"]
[1696.45004, "o", "  1807\t        Parameters\r\n"]
[1696.474515, "o", "  1808\t        ----------\r\n"]
[1696.49899, "o", "  1809\t        X : array-like of shape (n_samples, n_features)\r\n"]
[1696.523465, "o", "  1810\t            Training vector, where `n_samples` is the number of samples\r\n"]
[1696.547941, "o", "  1811\t            and `n_features` is the number of features.\r\n"]
[1696.572416, "o", "  1812\t\r\n"]
[1696.596891, "o", "  1813\t        y : Ignored\r\n"]
[1696.621366, "o", "  1814\t            Not used, present for API consistency by convention.\r\n"]
[1696.645842, "o", "  1815\t\r\n"]
[1696.670317, "o", "  1816\t        Returns\r\n"]
[1696.694792, "o", "  1817\t        -------\r\n"]
[1696.719267, "o", "  1818\t        V : ndarray of shape (n_samples, n_components)\r\n"]
[1696.743743, "o", "  1819\t            Transformed data.\r\n"]
[1696.768218, "o", "  1820\t        \"\"\"\r\n"]
[1696.792693, "o", "  1821\t        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n"]
[1696.817168, "o", "  1822\t\r\n"]
[1696.841644, "o", "  1823\t        method = \"lasso_\" + self.fit_algorithm\r\n"]
[1696.866119, "o", "  1824\t\r\n"]
[1696.890594, "o", "  1825\t        random_state = check_random_state(self.random_state)\r\n"]
[1696.915069, "o", "  1826\t        X = self._validate_data(X)\r\n"]
[1696.939545, "o", "  1827\t\r\n"]
[1696.96402, "o", "  1828\t        if self.n_components is None:\r\n"]
[1696.988495, "o", "  1829\t            n_components = X.shape[1]\r\n"]
[1697.01297, "o", "  1830\t        else:\r\n"]
[1697.037446, "o", "  1831\t            n_components = self.n_components\r\n"]
[1697.061921, "o", "  1832\t\r\n"]
[1697.086396, "o", "  1833\t        V, U, E, self.n_iter_ = _dict_learning(\r\n"]
[1697.110871, "o", "  1834\t            X,\r\n"]
[1697.135347, "o", "  1835\t            n_components,\r\n"]
[1697.159822, "o", "  1836\t            alpha=self.alpha,\r\n"]
[1697.184297, "o", "  1837\t            tol=self.tol,\r\n"]
[1697.208772, "o", "  1838\t            max_iter=self.max_iter,\r\n"]
[1697.233248, "o", "  1839\t            method=method,\r\n"]
[1697.257723, "o", "  1840\t            method_max_iter=self.transform_max_iter,\r\n"]
[1697.282198, "o", "  1841\t            n_jobs=self.n_jobs,\r\n"]
[1697.306673, "o", "  1842\t            code_init=self.code_init,\r\n"]
[1697.331149, "o", "  1843\t            dict_init=self.dict_init,\r\n"]
[1697.355624, "o", "  1844\t            callback=self.callback,\r\n"]
[1697.380099, "o", "  1845\t            verbose=self.verbose,\r\n"]
[1697.404574, "o", "  1846\t            random_state=random_state,\r\n"]
[1697.42905, "o", "  1847\t            return_n_iter=True,\r\n"]
[1697.453525, "o", "  1848\t            positive_dict=self.positive_dict,\r\n"]
[1697.478, "o", "  1849\t            positive_code=self.positive_code,\r\n"]
[1697.502475, "o", "  1850\t        )\r\n"]
[1697.52695, "o", "  1851\t        self.components_ = U\r\n"]
[1697.551426, "o", "  1852\t        self.error_ = E\r\n"]
[1697.575901, "o", "  1853\t\r\n"]
[1697.600376, "o", "  1854\t        return V\r\n"]
[1697.624851, "o", "  1855\t\r\n"]
[1697.649327, "o", "  1856\t    @property\r\n"]
[1697.673802, "o", "  1857\t    def _n_features_out(self):\r\n"]
[1697.698277, "o", "  1858\t        \"\"\"Number of transformed output features.\"\"\"\r\n"]
[1697.722752, "o", "  1859\t        return self.components_.shape[0]\r\n"]
[1697.747228, "o", "  1860\t\r\n"]
[1697.771703, "o", "  1861\t    def _more_tags(self):\r\n"]
[1697.796178, "o", "  1862\t        return {\r\n"]
[1697.820653, "o", "  1863\t            \"preserves_dtype\": [np.float64, np.float32],\r\n"]
[1697.845129, "o", "  1864\t        }\r\n"]
[1697.869604, "o", "  1865\t\r\n"]
[1697.894079, "o", "  1866\t\r\n"]
[1697.918554, "o", "  1867\tclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[1697.94303, "o", "  1868\t    \"\"\"Mini-batch dictionary learning.\r\n"]
[1697.967505, "o", "  1869\t\r\n"]
[1697.99198, "o", "  1870\t    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n"]
[1698.016455, "o", "  1871\t    encoding the fitted data.\r\n"]
[1698.040931, "o", "  1872\t\r\n"]
[1698.065406, "o", "  1873\t    Solves the optimization problem::\r\n"]
[1698.089881, "o", "  1874\t\r\n"]
[1698.114356, "o", "  1875\t       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n"]
[1698.138832, "o", "  1876\t                    (U,V)\r\n"]
[1698.163307, "o", "  1877\t                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n"]
[1698.187782, "o", "  1878\t\r\n"]
[1698.212257, "o", "  1879\t    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n"]
[1698.236733, "o", "  1880\t    the entry-wise matrix norm which is the sum of the absolute values\r\n"]
[1698.261208, "o", "  1881\t    of all the entries in the matrix.\r\n"]
[1698.285683, "o", "  1882\t\r\n"]
[1698.310158, "o", "  1883\t    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n"]
[1698.334634, "o", "  1884\t\r\n"]
[1698.359109, "o", "  1885\t    Parameters\r\n"]
[1698.383584, "o", "  1886\t    ----------\r\n"]
[1698.408059, "o", "  1887\t    n_components : int, default=None\r\n"]
[1698.432535, "o", "  1888\t        Number of dictionary elements to extract.\r\n"]
[1698.45701, "o", "  1889\t\r\n"]
[1698.481485, "o", "  1890\t    alpha : float, default=1\r\n"]
[1698.50596, "o", "  1891\t        Sparsity controlling parameter.\r\n"]
[1698.530436, "o", "  1892\t\r\n"]
[1698.554911, "o", "  1893\t    n_iter : int, default=1000\r\n"]
[1698.579386, "o", "  1894\t        Total number of iterations over data batches to perform.\r\n"]
[1698.603861, "o", "  1895\t\r\n"]
[1698.628337, "o", "  1896\t        .. deprecated:: 1.1\r\n"]
[1698.652812, "o", "  1897\t           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n"]
[1698.677287, "o", "  1898\t           ``max_iter`` instead.\r\n"]
[1698.701762, "o", "  1899\t\r\n"]
[1698.726238, "o", "  1900\t    max_iter : int, default=None\r\n"]
[1698.750713, "o", "  1901\t        Maximum number of iterations over the complete dataset before\r\n"]
[1698.775188, "o", "  1902\t        stopping independently of any early stopping criterion heuristics.\r\n"]
[1698.799663, "o", "  1903\t        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n"]
[1698.824139, "o", "  1904\t\r\n"]
[1698.848614, "o", "  1905\t        .. versionadded:: 1.1\r\n"]
[1698.873089, "o", "  1906\t\r\n"]
[1698.897564, "o", "  1907\t    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n"]
[1698.92204, "o", "  1908\t        The algorithm used:\r\n"]
[1698.946515, "o", "  1909\t\r\n"]
[1698.97099, "o", "  1910\t        - `'lars'`: uses the least angle regression method to solve the lasso\r\n"]
[1698.995465, "o", "  1911\t          problem (`linear_model.lars_path`)\r\n"]
[1699.019941, "o", "  1912\t        - `'cd'`: uses the coordinate descent method to compute the\r\n"]
[1699.044416, "o", "  1913\t          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n"]
[1699.068891, "o", "  1914\t          the estimated components are sparse.\r\n"]
[1699.093366, "o", "  1915\t\r\n"]
[1699.117842, "o", "  1916\t    n_jobs : int, default=None\r\n"]
[1699.142317, "o", "  1917\t        Number of parallel jobs to run.\r\n"]
[1699.166792, "o", "  1918\t        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n"]
[1699.191267, "o", "  1919\t        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n"]
[1699.215743, "o", "  1920\t        for more details.\r\n"]
[1699.240218, "o", "  1921\t\r\n"]
[1699.264693, "o", "  1922\t    batch_size : int, default=256\r\n"]
[1699.289168, "o", "  1923\t        Number of samples in each mini-batch.\r\n"]
[1699.313644, "o", "  1924\t\r\n"]
[1699.338119, "o", "  1925\t        .. versionchanged:: 1.3\r\n"]
[1699.362594, "o", "  1926\t           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n"]
[1699.387069, "o", "  1927\t\r\n"]
[1699.411545, "o", "  1928\t    shuffle : bool, default=True\r\n"]
[1699.43602, "o", "  1929\t        Whether to shuffle the samples before forming batches.\r\n"]
[1699.460495, "o", "  1930\t\r\n"]
[1699.48497, "o", "  1931\t    dict_init : ndarray of shape (n_components, n_features), default=None\r\n"]
[1699.509446, "o", "  1932\t        Initial value of the dictionary for warm restart scenarios.\r\n"]
[1699.533921, "o", "  1933\t\r\n"]
[1699.558396, "o", "  1934\t    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n"]
[1699.582871, "o", "  1935\t            'threshold'}, default='omp'\r\n"]
[1699.607347, "o", "  1936\t        Algorithm used to transform the data:\r\n"]
[1699.631822, "o", "  1937\t\r\n"]
[1699.656297, "o", "  1938\t        - `'lars'`: uses the least angle regression method\r\n"]
[1699.680772, "o", "  1939\t          (`linear_model.lars_path`);\r\n"]
[1699.705248, "o", "  1940\t        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n"]
[1699.729723, "o", "  1941\t        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n"]
[1699.754198, "o", "  1942\t          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n"]
[1699.778673, "o", "  1943\t          if the estimated components are sparse.\r\n"]
[1699.803149, "o", "  1944\t        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n"]
[1699.827624, "o", "  1945\t          solution.\r\n"]
[1699.852099, "o", "  1946\t        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n"]
[1699.876574, "o", "  1947\t          the projection ``dictionary * X'``.\r\n"]
[1699.90105, "o", "  1948\t\r\n"]
[1699.925525, "o", "  1949\t    transform_n_nonzero_coefs : int, default=None\r\n"]
[1700.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1700.002, "i", "python - << 'PY'\r"]
[1700.004, "o", "python - << 'PY'\r\n"]
[1700.1296, "o", "\u001b[?2004l\r\n"]
[1700.2532, "o", "\u001b[?2004h> from pathlib import Path\r\n"]
[1700.3768, "o", "\u001b[?2004l\r\n"]
[1700.5004, "o", "\u001b[?2004h> p=Path('sklearn/decomposition/_dict_learning.py')\r\n"]
[1700.624, "o", "\u001b[?2004l\r\n"]
[1700.7476, "o", "\u001b[?2004h> s=p.read_text()\r\n"]
[1700.8712, "o", "\u001b[?2004l\r\n"]
[1700.9948, "o", "\u001b[?2004h> # Move unconditional old_dict copy into the max_iter branch to avoid unnecessary work when max_iter is None\r\n"]
[1701.1184, "o", "\u001b[?2004l\r\n"]
[1701.242, "o", "\u001b[?2004h> old_line = '        old_dict = dictionary.copy()\\n'\r\n"]
[1701.3656, "o", "\u001b[?2004l\r\n"]
[1701.4892, "o", "\u001b[?2004h> if old_line in s:\r\n"]
[1701.6128, "o", "\u001b[?2004l\r\n"]
[1701.7364, "o", "\u001b[?2004h>     s = s.replace(old_line, '')\r\n"]
[1701.86, "o", "\u001b[?2004l\r\n"]
[1701.9836, "o", "\u001b[?2004h>     # Insert a guarded copy inside the max_iter branch after setting monitoring attrs\r\n"]
[1702.1072, "o", "\u001b[?2004l\r\n"]
[1702.2308, "o", "\u001b[?2004h>     anchor = '            self._no_improvement = 0\\n\\n            batches = gen_batches'\r\n"]
[1702.3544, "o", "\u001b[?2004l\r\n"]
[1702.478, "o", "\u001b[?2004h>     insert = '            old_dict = dictionary.copy()\\n\\n            batches = gen_batches'\r\n"]
[1702.6016, "o", "\u001b[?2004l\r\n"]
[1702.7252, "o", "\u001b[?2004h>     if anchor in s:\r\n"]
[1702.8488, "o", "\u001b[?2004l\r\n"]
[1702.9724, "o", "\u001b[?2004h>         s = s.replace(anchor, insert)\r\n"]
[1703.096, "o", "\u001b[?2004l\r\n"]
[1703.2196, "o", "\u001b[?2004h>         p.write_text(s)\r\n"]
[1703.3432, "o", "\u001b[?2004l\r\n"]
[1703.4668, "o", "\u001b[?2004h>         print('Patched: moved old_dict copy inside max_iter branch')\r\n"]
[1703.5904, "o", "\u001b[?2004l\r\n"]
[1703.714, "o", "\u001b[?2004h>     else:\r\n"]
[1703.8376, "o", "\u001b[?2004l\r\n"]
[1703.9612, "o", "\u001b[?2004h>         print('Anchor not found; no patch applied')\r\n"]
[1704.0848, "o", "\u001b[?2004l\r\n"]
[1704.2084, "o", "\u001b[?2004h> else:\r\n"]
[1704.332, "o", "\u001b[?2004l\r\n"]
[1704.4556, "o", "\u001b[?2004h>     print('old_dict copy line not found or previously patched')\r\n"]
[1704.5792, "o", "\u001b[?2004l\r\n"]
[1704.7028, "o", "\u001b[?2004h> PY\r\n"]
[1704.8264, "o", "\u001b[?2004l\r\n"]
[1705.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1705.002, "i", "git --no-pager diff -- sklearn/decomposition/_dict_learning.py | sed -n '1,200p'\r"]
[1705.004, "o", "git --no-pager diff -- sklearn/decomposition/_dict_learning.py | sed -n '1,200p'\r\n"]
[1705.230727, "o", "\u001b[?2004l\r\n"]
[1705.455455, "o", "diff --git a/sklearn/decomposition/_dict_learning.py b/sklearn/decomposition/_dict_learning.py\r\n"]
[1705.680182, "o", "index 48beac450..ddfda0392 100644\r\n"]
[1705.904909, "o", "--- a/sklearn/decomposition/_dict_learning.py\r\n"]
[1706.129636, "o", "+++ b/sklearn/decomposition/_dict_learning.py\r\n"]
[1706.354364, "o", "@@ -2361,7 +2361,6 @@ class MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[1706.579091, "o", "         self._random_state = check_random_state(self.random_state)\r\n"]
[1706.803818, "o", " \r\n"]
[1707.028545, "o", "         dictionary = self._initialize_dict(X, self._random_state)\r\n"]
[1707.253273, "o", "-        old_dict = dictionary.copy()\r\n"]
[1707.478, "o", " \r\n"]
[1707.702727, "o", "         if self.shuffle:\r\n"]
[1707.927455, "o", "             X_train = X.copy()\r\n"]
[1708.152182, "o", "@@ -2384,7 +2383,7 @@ class MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n"]
[1708.376909, "o", "             # Attributes to monitor the convergence\r\n"]
[1708.601636, "o", "             self._ewa_cost = None\r\n"]
[1708.826364, "o", "             self._ewa_cost_min = None\r\n"]
[1709.051091, "o", "-            self._no_improvement = 0\r\n"]
[1709.275818, "o", "+            old_dict = dictionary.copy()\r\n"]
[1709.500545, "o", " \r\n"]
[1709.725273, "o", "             batches = gen_batches(n_samples, self._batch_size)\r\n"]
[1710.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1710.002, "i", "pytest -q sklearn/decomposition/tests/test_dict_learning.py -k minibatch -q\r"]
[1710.004, "o", "pytest -q sklearn/decomposition/tests/test_dict_learning.py -k minibatch -q\r\n"]
[1712.478, "o", "\u001b[?2004l\r\n"]
[1715.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1715.002, "i", "cd asv_benchmarks\r"]
[1715.004, "o", "cd asv_benchmarks\r\n"]
[1715.006, "o", "\u001b[?2004l\r\n"]
[1720.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1720.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r"]
[1720.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r\n"]
[1720.315, "o", "stderr\r\n"]
[1720.624, "o", "\u001b[?2004l\r\n"]
[1720.933, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[1721.242, "o", "\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n"]
[1721.551, "o", "[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n"]
[1721.86, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n"]
[1722.169, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0m                                                                                     ok\r\n"]
[1722.478, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ============\u001b[0m\r\n"]
[1722.787, "o", "              --                 n_jobs   \r\n"]
[1723.096, "o", "              --------------- ------------\r\n"]
[1723.405, "o", "               fit_algorithm       1      \r\n"]
[1723.714, "o", "              =============== ============\r\n"]
[1724.023, "o", "                    lars       6.70\u00b10.02s \r\n"]
[1724.332, "o", "                     cd        1.55\u00b10.04s \r\n"]
[1724.641, "o", "              =============== ============\r\n"]
[1725.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1725.002, "i", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r"]
[1725.004, "o", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r\n"]
[1726.654, "o", "\u001b[?2004l\r\n"]
[1728.302, "o", "Commits with results:\r\n"]
[1730.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1730.002, "i", "cd ..\r"]
[1730.004, "o", "cd ..\r\n"]
[1730.006, "o", "\u001b[?2004l\r\n"]
[1735.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1735.002, "i", "git add sklearn/decomposition/_dict_learning.py\r"]
[1735.004, "o", "git add sklearn/decomposition/_dict_learning.py\r\n"]
[1735.006, "o", "\u001b[?2004l\r\n"]
[1740.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1740.002, "i", "git commit -m \"Perf: avoid unnecessary dictionary copy in MiniBatchDictionaryLearning when max_iter is None. Move o\r"]
[1740.004, "o", "git commit -m \"Perf: avoid unnecessary dictionary copy in MiniBatchDictionaryLearning when max_iter is None. Move o\r\n"]
[1740.3356, "o", "old_dict copy inside max_iter branch to reduce allocations in common ASV path. All minibatch dict learning tests pass.\"\r\n"]
[1740.6652, "o", "\u001b[?2004l\r\n"]
[1740.9948, "o", "Author identity unknown\r\n"]
[1741.3244, "o", "\r\n"]
[1741.654, "o", "*** Please tell me who you are.\r\n"]
[1741.9836, "o", "\r\n"]
[1742.3132, "o", "Run\r\n"]
[1742.6428, "o", "\r\n"]
[1742.9724, "o", "  git config --global user.email \"you@example.com\"\r\n"]
[1743.302, "o", "  git config --global user.name \"Your Name\"\r\n"]
[1743.6316, "o", "\r\n"]
[1743.9612, "o", "to set your account's default identity.\r\n"]
[1744.2908, "o", "Omit --global to set the identity only in this repository.\r\n"]
[1744.6204, "o", "\r\n"]
[1745.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1745.002, "i", "cd asv_benchmarks\r"]
[1745.004, "o", "cd asv_benchmarks\r\n"]
[1745.006, "o", "\u001b[?2004l\r\n"]
[1750.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1750.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r"]
[1750.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r\n"]
[1750.296824, "o", "stderr\r\n"]
[1750.587647, "o", "\u001b[?2004l\r\n"]
[1750.878471, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[1751.169294, "o", "\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n"]
[1751.460118, "o", "[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n"]
[1751.750941, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n"]
[1752.041765, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0masv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r\n"]
[1752.332588, "o", "                                                                                     ok\r\n"]
[1752.623412, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ============\u001b[0m\r\n"]
[1752.914235, "o", "              --                 n_jobs   \r\n"]
[1753.205059, "o", "              --------------- ------------\r\n"]
[1753.495882, "o", "               fit_algorithm       1      \r\n"]
[1753.786706, "o", "              =============== ============\r\n"]
[1754.077529, "o", "                    lars       6.80\u00b10.02s \r\n"]
[1754.368353, "o", "                     cd        1.59\u00b10.02s \r\n"]
[1754.659176, "o", "              =============== ============\r\n"]
[1755.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1755.002, "i", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r"]
[1755.004, "o", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r\n"]
[1756.654, "o", "\u001b[?2004l\r\n"]
[1758.302, "o", "Commits with results:\r\n"]
[1760.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1760.002, "i", "cd /workspace/repo\r"]
[1760.004, "o", "cd /workspace/repo\r\n"]
[1760.006, "o", "\u001b[?2004l\r\n"]
[1765.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1765.002, "i", "git config user.email \"devnull@example.com\"\r"]
[1765.004, "o", "git config user.email \"devnull@example.com\"\r\n"]
[1765.006, "o", "\u001b[?2004l\r\n"]
[1770.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1770.002, "i", "git config user.name \"ASV Optimizer\"\r"]
[1770.004, "o", "git config user.name \"ASV Optimizer\"\r\n"]
[1770.006, "o", "\u001b[?2004l\r\n"]
[1775.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1775.002, "i", "git add sklearn/decomposition/_dict_learning.py\r"]
[1775.004, "o", "git add sklearn/decomposition/_dict_learning.py\r\n"]
[1775.006, "o", "\u001b[?2004l\r\n"]
[1780.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1780.002, "i", "git commit -m \"Perf: avoid unnecessary dictionary copy in MiniBatchDictionaryLearning when max_iter is None by movi\r"]
[1780.004, "o", "git commit -m \"Perf: avoid unnecessary dictionary copy in MiniBatchDictionaryLearning when max_iter is None by movi\r\n"]
[1781.242, "o", "ing old_dict copy inside the max_iter branch. Reduces allocations in common path; tests pass.\"\r\n"]
[1782.478, "o", "\u001b[?2004l\r\n"]
[1783.714, "o", "[main 8250665ac] Perf: avoid unnecessary dictionary copy in MiniBatchDictionaryLearning when max_iter is None by moving old_dict copy inside the max_iter branch. Reduces allocations in common path; tests pass.\r\n"]
[1785.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1785.002, "i", "cd asv_benchmarks\r"]
[1785.004, "o", "cd asv_benchmarks\r\n"]
[1785.006, "o", "\u001b[?2004l\r\n"]
[1790.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1790.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r"]
[1790.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r\n"]
[1790.296824, "o", "stderr\r\n"]
[1790.587647, "o", "\u001b[?2004l\r\n"]
[1790.878471, "o", "\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n"]
[1791.169294, "o", "\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n"]
[1791.460118, "o", "[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n"]
[1791.750941, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n"]
[1792.041765, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0masv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r\n"]
[1792.332588, "o", "                                                                                     ok\r\n"]
[1792.623412, "o", "[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ============\u001b[0m\r\n"]
[1792.914235, "o", "              --                 n_jobs   \r\n"]
[1793.205059, "o", "              --------------- ------------\r\n"]
[1793.495882, "o", "               fit_algorithm       1      \r\n"]
[1793.786706, "o", "              =============== ============\r\n"]
[1794.077529, "o", "                    lars       6.72\u00b10.03s \r\n"]
[1794.368353, "o", "                     cd        1.58\u00b10.02s \r\n"]
[1794.659176, "o", "              =============== ============\r\n"]
[1795.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1795.002, "i", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r"]
[1795.004, "o", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r\n"]
[1796.654, "o", "\u001b[?2004l\r\n"]
[1798.302, "o", "Commits with results:\r\n"]
[1800.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1800.002, "i", "grep -n \"self\\._A\\|self\\._B\" sklearn/decomposition/_dict_learning.py\r"]
[1800.004, "o", "grep -n \"self\\._A\\|self\\._B\" sklearn/decomposition/_dict_learning.py\r\n"]
[1802.478, "o", "\u001b[?2004l\r\n"]
[1805.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1805.002, "i", "sed -n '1,2600p' sklearn/decomposition/_dict_learning.py | nl -ba | sed -n '2200,2600p'\r"]
[1805.004, "o", "sed -n '1,2600p' sklearn/decomposition/_dict_learning.py | nl -ba | sed -n '2200,2600p'\r\n"]
[1807.478, "o", "\u001b[?2004l\r\n"]
[1810.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1810.002, "i", "cd /workspace/repo\r"]
[1810.004, "o", "cd /workspace/repo\r\n"]
[1810.006, "o", "\u001b[?2004l\r\n"]
[1815.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1815.002, "i", "git show --stat -1\r"]
[1815.004, "o", "git show --stat -1\r\n"]
[1816.654, "o", "\u001b[?2004l\r\n"]
[1818.302, "o", "error: cannot run less: No such file or directory\r\n"]
[1820.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1820.002, "i", "cd asv_benchmarks\r"]
[1820.004, "o", "cd asv_benchmarks\r\n"]
[1820.006, "o", "\u001b[?2004l\r\n"]
[1825.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1825.002, "i", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r"]
[1825.004, "o", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r\n"]
[1826.654, "o", "\u001b[?2004l\r\n"]
[1828.302, "o", "Commits with results:\r\n"]
[1830.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1830.002, "i", "\u001b[?2004l\r"]
[1830.004, "o", "\u001b[?2004l\r\n"]
[1832.478, "o", "\r\n"]
