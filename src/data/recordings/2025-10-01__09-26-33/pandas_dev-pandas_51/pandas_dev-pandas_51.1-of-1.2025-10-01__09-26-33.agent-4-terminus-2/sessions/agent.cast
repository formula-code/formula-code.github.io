{"version": 2, "width": 160, "height": 40, "timestamp": 1762942265, "env": {"SHELL": "/bin/bash", "TERM": "screen"}}
[0.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[0.002, "i", "asciinema rec --stdin /logs/agent.cast\r"]
[0.004, "o", "asciinema rec --stdin /logs/agent.cast\r\n"]
[0.006, "o", "\u001b[?2004l\r\n\u001b[0;31masciinema: /logs/agent.cast already exists, aborting\u001b[0m\r\n\u001b[0;31masciinema: use --overwrite option if you want to overwrite existing recording\u001b[0m\r\n\u001b[0;31masciinema: use --append option if you want to append to existing recording\u001b[0m\r\n"]
[5.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[5.002, "i", "clear\r"]
[5.004, "o", "clear\r\n"]
[5.006, "o", "\u001b[?2004l\r\n\u001b[H\u001b[J\r\n"]
[10.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[10.002, "i", "ls -la\r"]
[10.004, "o", "ls -la\r\n"]
[10.006, "o", "\u001b[?2004l\r\ntotal 220\r\ndrwxr-xr-x 1 root root  4096 Oct  1 16:20 \u001b[0m\u001b[01;34m.\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 15 17:52 \u001b[01;34m..\u001b[0m\r\ndrwxr-xr-x 2 root root  4096 Sep 12 21:44 \u001b[01;34m.circleci\u001b[0m\r\n-rw-r--r-- 1 root root  1021 Sep 12 21:44 .devcontainer.json\r\ndrwxr-xr-x 1 root root  4096 Oct  1 16:20 \u001b[01;34m.git\u001b[0m\r\n-rw-r--r-- 1 root root  1670 Sep 12 21:44 .gitattributes\r\ndrwxr-xr-x 1 root root  4096 Sep 12 21:44 \u001b[01;34m.github\u001b[0m\r\n-rw-r--r-- 1 root root  1919 Sep 12 21:44 .gitignore\r\n-rw-r--r-- 1 root root  2296 Sep 12 21:44 .gitpod.yml\r\n-rw-r--r-- 1 root root 13648 Sep 12 21:44 .pre-commit-config.yaml\r\n-rw-r--r-- 1 root root  2284 Sep 12 21:44 AUTHORS.md\r\n-rw-r--r-- 1 root root  1541 Sep 12 21:43 CITATION.cff\r\n-rw-r--r-- 1 root root   369 Sep 12 21:44 Dockerfile\r\n-rw-r--r-- 1 root root  1634 Sep 12 21:44 LICENSE\r\ndrwxr-xr-x 1 root root  4096 Sep 12 21:44 \u001b[01;34mLICENSES\u001b[0m\r\n-rw-r--r-- 1 root root  1335 Sep 12 21:44 MANIFEST.in\r\n-rw-r--r-- 1 root root 11537 Sep 12 21:44 README.md\r\ndrwxr-xr-x 1 root root  4096 Sep 12 21:44 \u001b[01;34masv_bench\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 15 17:46 \u001b[01;34mbuild\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 12 21:44 \u001b[01;34mci\u001b[0m\r\n-rw-r--r-- 1 root root   256 Sep 12 21:43 codecov.yml\r\ndrwxr-xr-x 1 root root  4096 Sep 12 21:44 \u001b[01;34mdoc\u001b[0m\r\n-rw-r--r-- 1 root root  2550 Sep 12 21:44 environment.yml\r\n-rw-r--r-- 1 root root   866 Sep 12 21:43 generate_pxi.py\r\n-rw-r--r-- 1 root root  1715 Sep 12 21:43 generate_version.py\r\ndrwxr-xr-x 2 root root  4096 Sep 12 21:43 \u001b[01;34mgitpod\u001b[0m\r\n-rw-r--r-- 1 root root  1646 Sep 12 21:44 meson.build\r\ndrwxr-xr-x 1 root root  4096 Sep 12 21:44 \u001b[01;34mpandas\u001b[0m\r\n-rw-r--r-- 1 root root 23914 Sep 12 21:44 pyproject.toml\r\n-rw-r--r-- 1 root root  4129 Sep 12 21:44 pyright_reportGeneralTypeIssues.json\r\n-rw-r--r-- 1 root root  1390 Sep 12 21:44 requirements-dev.txt\r\ndrwxr-xr-x 1 root root  4096 Sep 12 21:44 \u001b[01;34mscripts\u001b[0m\r\n-rwxr-xr-x 1 root root 21962 Sep 12 21:44 \u001b[01;32msetup.py\u001b[0m\r\ndrwxr-xr-x 3 root root  4096 Sep 12 21:43 \u001b[01;34mtooling\u001b[0m\r\ndrwxr-xr-x 2 root root  4096 Sep 12 21:43 \u001b[01;34mtypings\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 12 21:44 \u001b[01;34mweb\u001b[0m\r\n"]
[15.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[15.002, "i", "git status -s\r"]
[15.004, "o", "git status -s\r\n"]
[15.006, "o", "\u001b[?2004l\r\n"]
[20.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[20.002, "i", "find . -maxdepth 2 -type f -name \"asv*.json\" -o -name \"asv.conf.json\" -o -name \"asv*.cfg\"\r"]
[20.004, "o", "find . -maxdepth 2 -type f -name \"asv*.json\" -o -name \"asv.conf.json\" -o -name \"asv*.cfg\"\r\n"]
[20.006, "o", "\u001b[?2004l\r\n./asv_bench/asv.conf.json\r\n"]
[25.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[25.002, "i", "grep -RIn \"GetItemMask\" . || true\r"]
[25.004, "o", "grep -RIn \"GetItemMask\" . || true\r\n"]
[25.006, "o", "\u001b[?2004l\r\n\u001b[35m\u001b[K./asv_bench/benchmarks/sparse.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K207\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kclass \u001b[01;31m\u001b[KGetItemMask\u001b[m\u001b[K:\r\n"]
[30.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[30.002, "i", "grep -RIn \"sparse.GetItemMask\" . || true\r"]
[30.004, "o", "grep -RIn \"sparse.GetItemMask\" . || true\r\n"]
[30.006, "o", "\u001b[?2004l\r\n"]
[35.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[35.002, "i", "ls -la asv_bench || true\r"]
[35.004, "o", "ls -la asv_bench || true\r\n"]
[35.006, "o", "\u001b[?2004l\r\ntotal 24\r\ndrwxr-xr-x 1 root root 4096 Sep 12 21:44 \u001b[0m\u001b[01;34m.\u001b[0m\r\ndrwxr-xr-x 1 root root 4096 Oct  1 16:20 \u001b[01;34m..\u001b[0m\r\n-rw-r--r-- 1 root root 5171 Sep 12 21:44 asv.conf.json\r\ndrwxr-xr-x 1 root root 4096 Sep 12 21:44 \u001b[01;34mbenchmarks\u001b[0m\r\n"]
[40.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[40.002, "i", "rg -n \"class .*GetItemMask|def .*GetItemMask|GetItemMask\" asv* bench* benchmarks* asv_bench 2>/dev/null || true\r"]
[40.004, "o", "rg -n \"class .*GetItemMask|def .*GetItemMask|GetItemMask\" asv* bench* benchmarks* asv_bench 2>/dev/null || true\r\n"]
[40.006, "o", "\u001b[?2004l\r\n"]
[45.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[45.002, "i", "asv run --python=same --bench=\"sparse.GetItemMask.*\"\r"]
[45.004, "o", "asv run --python=same --bench=\"sparse.GetItemMask.*\"\r\n"]
[45.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;31mNo `asv.conf` file found for valid extensions: ['.json', '.jsonc'].\u001b[0m\r\n"]
[50.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[50.002, "i", "sed -n '1,240p' asv_bench/benchmarks/sparse.py\r"]
[50.004, "o", "sed -n '1,240p' asv_bench/benchmarks/sparse.py\r\n"]
[50.006, "o", "\u001b[?2004l\r\nimport numpy as np\r\nimport scipy.sparse\r\n\r\nimport pandas as pd\r\nfrom pandas import (\r\n    MultiIndex,\r\n    Series,\r\n    date_range,\r\n)\r\nfrom pandas.arrays import SparseArray\r\n\r\n\r\ndef make_array(size, dense_proportion, fill_value, dtype):\r\n    dense_size = int(size * dense_proportion)\r\n    arr = np.full(size, fill_value, dtype)\r\n    indexer = np.random.choice(np.arange(size), dense_size, replace=False)\r\n    arr[indexer] = np.random.choice(np.arange(100, dtype=dtype), dense_size)\r\n    return arr\r\n\r\n\r\nclass SparseSeriesToFrame:\r\n    def setup(self):\r\n        K = 50\r\n        N = 50001\r\n        rng = date_range(\"1/1/2000\", periods=N, freq=\"min\")\r\n        self.series = {}\r\n        for i in range(1, K):\r\n            data = np.random.randn(N)[:-i]\r\n            idx = rng[:-i]\r\n            data[100:] = np.nan\r\n            self.series[i] = Series(SparseArray(data), index=idx)\r\n\r\n    def time_series_to_frame(self):\r\n        pd.DataFrame(self.series)\r\n\r\n\r\nclass SparseArrayConstructor:\r\n    params = ([0.1, 0.01], [0, np.nan], [np.int64, np.float64, object])\r\n    param_names = [\"dense_proportion\", \"fill_value\", \"dtype\"]\r\n\r\n    def setup(self, dense_proportion, fill_value, dtype):\r\n        N = 10**6\r\n        self.array = make_array(N, dense_proportion, fill_value, dtype)\r\n\r\n    def time_sparse_array(self, dense_proportion, fill_value, dtype):\r\n        SparseArray(self.array, fill_value=fill_value, dtype=dtype)\r\n\r\n\r\nclass SparseDataFrameConstructor:\r\n    def setup(self):\r\n        N = 1000\r\n        self.sparse = scipy.sparse.rand(N, N, 0.005)\r\n\r\n    def time_from_scipy(self):\r\n        pd.DataFrame.sparse.from_spmatrix(self.sparse)\r\n\r\n\r\nclass FromCoo:\r\n    def setup(self):\r\n        self.matrix = scipy.sparse.coo_matrix(\r\n            ([3.0, 1.0, 2.0], ([1, 0, 0], [0, 2, 3])), shape=(100, 100)\r\n        )\r\n\r\n    def time_sparse_series_from_coo(self):\r\n        Series.sparse.from_coo(self.matrix)\r\n\r\n\r\nclass ToCoo:\r\n    params = [True, False]\r\n    param_names = [\"sort_labels\"]\r\n\r\n    def setup(self, sort_labels):\r\n        s = Series([np.nan] * 10000)\r\n        s[0] = 3.0\r\n        s[100] = -1.0\r\n        s[999] = 12.1\r\n\r\n        s_mult_lvl = s.set_axis(MultiIndex.from_product([range(10)] * 4))\r\n        self.ss_mult_lvl = s_mult_lvl.astype(\"Sparse\")\r\n\r\n        s_two_lvl = s.set_axis(MultiIndex.from_product([range(100)] * 2))\r\n        self.ss_two_lvl = s_two_lvl.astype(\"Sparse\")\r\n\r\n    def time_sparse_series_to_coo(self, sort_labels):\r\n        self.ss_mult_lvl.sparse.to_coo(\r\n            row_levels=[0, 1], column_levels=[2, 3], sort_labels=sort_labels\r\n        )\r\n\r\n    def time_sparse_series_to_coo_single_level(self, sort_labels):\r\n        self.ss_two_lvl.sparse.to_coo(sort_labels=sort_labels)\r\n\r\n\r\nclass ToCooFrame:\r\n    def setup(self):\r\n        N = 10000\r\n        k = 10\r\n        arr = np.zeros((N, k), dtype=float)\r\n        arr[0, 0] = 3.0\r\n        arr[12, 7] = -1.0\r\n        arr[0, 9] = 11.2\r\n        self.df = pd.DataFrame(arr, dtype=pd.SparseDtype(\"float\", fill_value=0.0))\r\n\r\n    def time_to_coo(self):\r\n        self.df.sparse.to_coo()\r\n\r\n\r\nclass Arithmetic:\r\n    params = ([0.1, 0.01], [0, np.nan])\r\n    param_names = [\"dense_proportion\", \"fill_value\"]\r\n\r\n    def setup(self, dense_proportion, fill_value):\r\n        N = 10**6\r\n        arr1 = make_array(N, dense_proportion, fill_value, np.int64)\r\n        self.array1 = SparseArray(arr1, fill_value=fill_value)\r\n        arr2 = make_array(N, dense_proportion, fill_value, np.int64)\r\n        self.array2 = SparseArray(arr2, fill_value=fill_value)\r\n\r\n    def time_make_union(self, dense_proportion, fill_value):\r\n        self.array1.sp_index.make_union(self.array2.sp_index)\r\n\r\n    def time_intersect(self, dense_proportion, fill_value):\r\n        self.array1.sp_index.intersect(self.array2.sp_index)\r\n\r\n    def time_add(self, dense_proportion, fill_value):\r\n        self.array1 + self.array2\r\n\r\n    def time_divide(self, dense_proportion, fill_value):\r\n        self.array1 / self.array2\r\n\r\n\r\nclass ArithmeticBlock:\r\n    params = [np.nan, 0]\r\n    param_names = [\"fill_value\"]\r\n\r\n    def setup(self, fill_value):\r\n        N = 10**6\r\n        self.arr1 = self.make_block_array(\r\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\r\n        )\r\n        self.arr2 = self.make_block_array(\r\n            length=N, num_blocks=1000, block_size=10, fill_value=fill_value\r\n        )\r\n\r\n    def make_block_array(self, length, num_blocks, block_size, fill_value):\r\n        arr = np.full(length, fill_value)\r\n        indices = np.random.choice(\r\n            np.arange(0, length, block_size), num_blocks, replace=False\r\n        )\r\n        for ind in indices:\r\n            arr[ind : ind + block_size] = np.random.randint(0, 100, block_size)\r\n        return SparseArray(arr, fill_value=fill_value)\r\n\r\n    def time_make_union(self, fill_value):\r\n        self.arr1.sp_index.make_union(self.arr2.sp_index)\r\n\r\n    def time_intersect(self, fill_value):\r\n        self.arr2.sp_index.intersect(self.arr2.sp_index)\r\n\r\n    def time_addition(self, fill_value):\r\n        self.arr1 + self.arr2\r\n\r\n    def time_division(self, fill_value):\r\n        self.arr1 / self.arr2\r\n\r\n\r\nclass MinMax:\r\n    params = ([\"min\", \"max\"], [0.0, np.nan])\r\n    param_names = [\"func\", \"fill_value\"]\r\n\r\n    def setup(self, func, fill_value):\r\n        N = 1_000_000\r\n        arr = make_array(N, 1e-5, fill_value, np.float64)\r\n        self.sp_arr = SparseArray(arr, fill_value=fill_value)\r\n\r\n    def time_min_max(self, func, fill_value):\r\n        getattr(self.sp_arr, func)()\r\n\r\n\r\nclass Take:\r\n    params = ([np.array([0]), np.arange(100_000), np.full(100_000, -1)], [True, False])\r\n    param_names = [\"indices\", \"allow_fill\"]\r\n\r\n    def setup(self, indices, allow_fill):\r\n        N = 1_000_000\r\n        fill_value = 0.0\r\n        arr = make_array(N, 1e-5, fill_value, np.float64)\r\n        self.sp_arr = SparseArray(arr, fill_value=fill_value)\r\n\r\n    def time_take(self, indices, allow_fill):\r\n        self.sp_arr.take(indices, allow_fill=allow_fill)\r\n\r\n\r\nclass GetItem:\r\n    def setup(self):\r\n        N = 1_000_000\r\n        d = 1e-5\r\n        arr = make_array(N, d, np.nan, np.float64)\r\n        self.sp_arr = SparseArray(arr)\r\n\r\n    def time_integer_indexing(self):\r\n        self.sp_arr[78]\r\n\r\n    def time_slice(self):\r\n        self.sp_arr[1:]\r\n\r\n\r\nclass GetItemMask:\r\n    params = [True, False, np.nan]\r\n    param_names = [\"fill_value\"]\r\n\r\n    def setup(self, fill_value):\r\n        N = 1_000_000\r\n        d = 1e-5\r\n        arr = make_array(N, d, np.nan, np.float64)\r\n        self.sp_arr = SparseArray(arr)\r\n        b_arr = np.full(shape=N, fill_value=fill_value, dtype=np.bool_)\r\n        fv_inds = np.unique(\r\n            np.random.randint(low=0, high=N - 1, size=int(N * d), dtype=np.int32)\r\n        )\r\n        b_arr[fv_inds] = True if pd.isna(fill_value) else not fill_value\r\n        self.sp_b_arr = SparseArray(b_arr, dtype=np.bool_, fill_value=fill_value)\r\n\r\n    def time_mask(self, fill_value):\r\n        self.sp_arr[self.sp_b_arr]\r\n\r\n\r\nfrom .pandas_vb_common import setup  # noqa: F401 isort:skip\r\n"]
[55.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[55.002, "i", "sed -n '240,520p' asv_bench/benchmarks/sparse.py\r"]
[55.004, "o", "sed -n '240,520p' asv_bench/benchmarks/sparse.py\r\n"]
[55.006, "o", "\u001b[?2004l\r\n"]
[60.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[60.002, "i", "asv run --config=asv_bench/asv.conf.json --python=same --bench=\"sparse.GetItemMask.*\"\r"]
[60.004, "o", "asv run --config=asv_bench/asv.conf.json --python=same --bench=\"sparse.GetItemMask.*\"\r\n"]
[60.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;31mConfig file asv_bench/asv.conf.json not found.\u001b[0m\r\n"]
[65.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[65.002, "i", "cd asv_bench\r"]
[65.004, "o", "cd asv_bench\r\n"]
[65.006, "o", "\u001b[?2004l\r\n"]
[70.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo/asv_bench# "]
[70.002, "i", "asv run --python=same --bench=\"sparse.GetItemMask.*\" --quick\r"]
[70.004, "o", "asv run --python=same --bench=\"sparse.GetItemMask.*\" --quick\r\n"]
[70.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39msparse.GetItemMask.time_mask\u001b[0m                                                                                                                    ok\r\n[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39m============ ==========\u001b[0m\r\n              fill_value            \r\n             ------------ ----------\r\n                 True      12.8\u00b10ms \r\n                False      187\u00b10\u03bcs  \r\n                 nan       8.63\u00b10ms \r\n             ============ ==========\r\n\r\n"]
[75.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo/asv_bench# "]
[75.002, "i", "asv profile --python=same sparse.GetItemMask.time_mask\r"]
[75.004, "o", "asv profile --python=same sparse.GetItemMask.time_mask\r\n"]
[75.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;31m'sparse.GetItemMask.time_mask' benchmark not found\u001b[0m\r\n"]
[80.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo/asv_bench# "]
[80.002, "i", "cd ..\r"]
[80.004, "o", "cd ..\r\n"]
[80.006, "o", "\u001b[?2004l\r\n"]
[85.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[85.002, "i", "rg -n \"class SparseArray|def __getitem__|def take\\(|mask|boolean\" pandas/arrays/sparse -S || true\r"]
[85.004, "o", "rg -n \"class SparseArray|def __getitem__|def take\\(|mask|boolean\" pandas/arrays/sparse -S || true\r\n"]
[85.006, "o", "\u001b[?2004l\r\n-bash: rg: command not found\r\n"]
[90.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[90.002, "i", "sed -n '1,260p' pandas/arrays/sparse/array.py\r"]
[90.004, "o", "sed -n '1,260p' pandas/arrays/sparse/array.py\r\n"]
[90.006, "o", "\u001b[?2004l\r\nsed: can't read pandas/arrays/sparse/array.py: No such file or directory\r\n"]
[95.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[95.002, "i", "sed -n '260,640p' pandas/arrays/sparse/array.py\r"]
[95.004, "o", "sed -n '260,640p' pandas/arrays/sparse/array.py\r\n"]
[95.006, "o", "\u001b[?2004l\r\nsed: can't read pandas/arrays/sparse/array.py: No such file or directory\r\n"]
[100.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[100.002, "i", "ls -la pandas\r"]
[100.004, "o", "ls -la pandas\r\n"]
[100.006, "o", "\u001b[?2004l\r\ntotal 172\r\ndrwxr-xr-x 1 root root  4096 Sep 12 21:44 \u001b[0m\u001b[01;34m.\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Oct  1 16:20 \u001b[01;34m..\u001b[0m\r\n-rw-r--r-- 1 root root  8713 Sep 12 21:44 __init__.py\r\ndrwxr-xr-x 1 root root  4096 Sep 12 21:44 \u001b[01;34m_config\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 12 21:44 \u001b[01;34m_libs\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 12 21:44 \u001b[01;34m_testing\u001b[0m\r\n-rw-r--r-- 1 root root 14037 Sep 12 21:44 _typing.py\r\n-rw-r--r-- 1 root root 23612 Sep 12 21:44 _version.py\r\ndrwxr-xr-x 1 root root  4096 Sep 12 21:44 \u001b[01;34mapi\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 12 21:44 \u001b[01;34marrays\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 12 21:44 \u001b[01;34mcompat\u001b[0m\r\n-rw-r--r-- 1 root root 47613 Sep 12 21:44 conftest.py\r\ndrwxr-xr-x 1 root root  4096 Sep 12 21:44 \u001b[01;34mcore\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 12 21:44 \u001b[01;34merrors\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 12 21:44 \u001b[01;34mio\u001b[0m\r\n-rw-r--r-- 1 root root   994 Sep 12 21:44 meson.build\r\ndrwxr-xr-x 1 root root  4096 Sep 12 21:44 \u001b[01;34mplotting\u001b[0m\r\n-rw-r--r-- 1 root root   313 Sep 12 21:44 testing.py\r\ndrwxr-xr-x 1 root root  4096 Sep 12 21:44 \u001b[01;34mtests\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 12 21:44 \u001b[01;34mtseries\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 12 21:44 \u001b[01;34mutil\u001b[0m\r\n"]
[105.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[105.002, "i", "find pandas -maxdepth 3 -type f -name \"*sparse*\" -o -name \"*array*.py\" | sed -n '1,200p'\r"]
[105.004, "o", "find pandas -maxdepth 3 -type f -name \"*sparse*\" -o -name \"*array*.py\" | sed -n '1,200p'\r\n"]
[105.006, "o", "\u001b[?2004l\r\npandas/tests/construction/test_extract_array.py\r\npandas/tests/util/test_assert_extension_array_equal.py\r\npandas/tests/util/test_assert_interval_array_equal.py\r\npandas/tests/util/test_assert_numpy_array_equal.py\r\npandas/tests/arrays/test_array.py\r\npandas/tests/arrays/test_ndarray_backed.py\r\npandas/tests/copy_view/test_array.py\r\npandas/tests/generic/test_to_xarray.py\r\npandas/tests/tslibs/test_array_to_datetime.py\r\npandas/tests/arithmetic/test_array_ops.py\r\npandas/tests/extension/test_sparse.py\r\npandas/tests/strings/test_string_array.py\r\npandas/_libs/sparse.pyi\r\npandas/_libs/sparse.pyx\r\npandas/_libs/sparse_op_helper.pxi.in\r\npandas/core/internals/array_manager.py\r\npandas/core/ops/array_ops.py\r\npandas/core/strings/object_array.py\r\npandas/core/arraylike.py\r\n"]
[110.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[110.002, "i", "grep -RIn \"class SparseArray|def __getitem__|def take\\(|def _getitem|boolean|mask\" pandas 2>/dev/null | sed -n '1,2\r"]
[110.004, "o", "grep -RIn \"class SparseArray|def __getitem__|def take\\(|def _getitem|boolean|mask\" pandas 2>/dev/null | sed -n '1,2\r\n"]
[110.006, "o", "200p'\r\n\u001b[?2004l\r\n"]
[115.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[115.002, "i", "python - << 'PY'\r"]
[115.004, "o", "python - << 'PY'\r\n"]
[115.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> import numpy as np\r\n\u001b[?2004l\r\n\u001b[?2004h> import pandas as pd\r\n\u001b[?2004l\r\n\u001b[?2004h> from pandas.arrays import SparseArray\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> N = 1_000_00\r\n\u001b[?2004l\r\n\u001b[?2004h> N = 1_000_000\r\n\u001b[?2004l\r\n\u001b[?2004h> # prepare data and masks similar to benchmark\r\n\u001b[?2004l\r\n\u001b[?2004h> arr = np.full(N, np.nan, dtype=float)\r\n\u001b[?2004l\r\n\u001b[?2004h> # make a small proportion dense\r\n\u001b[?2004l\r\n\u001b[?2004h> idx = np.random.choice(np.arange(N), int(N*1e-5), replace=False)\r\n\u001b[?2004l\r\n\u001b[?2004h> arr[idx] = np.random.rand(idx.size)\r\n\u001b[?2004l\r\n\u001b[?2004h> sp_arr = SparseArray(arr)\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> for fv in [True, False, np.nan]:\r\n\u001b[?2004l\r\n\u001b[?2004h>     b_arr = np.full(shape=N, fill_value=fv, dtype=bool)\r\n\u001b[?2004l\r\n\u001b[?2004h>     fv_inds = np.unique(np.random.randint(low=0, high=N - 1, size=int(N * 1e-5), dtype=np.int32))\r\n\u001b[?2004l\r\n\u001b[?2004h>     b_arr[fv_inds] = True if pd.isna(fv) else (not fv)\r\n\u001b[?2004l\r\n\u001b[?2004h>     sp_b_arr = SparseArray(b_arr, dtype=bool, fill_value=fv)\r\n\u001b[?2004l\r\n\u001b[?2004h>     import time\r\n\u001b[?2004l\r\n\u001b[?2004h>     t0 = time.perf_counter()\r\n\u001b[?2004l\r\n\u001b[?2004h>     _ = sp_arr[sp_b_arr]\r\n\u001b[?2004l\r\n\u001b[?2004h>     dt = (time.perf_counter() - t0)*1e3\r\n\u001b[?2004l\r\n\u001b[?2004h>     print(f\"fill_value={fv!r} -> {dt:.3f} ms, result len={len(_)}\")\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\n+ /opt/conda/envs/asv_3.10/bin/ninja\r\n\r\n[0/1] Generating write_version_file with a custom command\u001b[K\r\n[1/1] Generating write_version_file with a custom command\u001b[K\r\nfill_value=True -> 14.444 ms, result len=999990\r\nfill_value=False -> 0.081 ms, result len=10\r\nfill_value=nan -> 11.790 ms, result len=1000000\r\n"]
[120.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[120.002, "i", "python - << 'PY'\r"]
[120.004, "o", "python - << 'PY'\r\n"]
[120.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> import cProfile, pstats, io\r\n\u001b[?2004l\r\n\u001b[?2004h> import numpy as np\r\n\u001b[?2004l\r\n\u001b[?2004h> import pandas as pd\r\n\u001b[?2004l\r\n\u001b[?2004h> from pandas.arrays import SparseArray\r\n\u001b[?2004l\r\n\u001b[?2004h> N=1_000_000\r\n\u001b[?2004l\r\n\u001b[?2004h> arr=np.full(N, np.nan, dtype=float)\r\n\u001b[?2004l\r\n\u001b[?2004h> idx=np.random.choice(np.arange(N), int(N*1e-5), replace=False)\r\n\u001b[?2004l\r\n\u001b[?2004h> arr[idx]=np.random.rand(idx.size)\r\n\u001b[?2004l\r\n\u001b[?2004h> sp_arr=SparseArray(arr)\r\n\u001b[?2004l\r\n\u001b[?2004h> fv=True\r\n\u001b[?2004l\r\n\u001b[?2004h> b_arr=np.full(shape=N, fill_value=fv, dtype=bool)\r\n\u001b[?2004l\r\n\u001b[?2004h> fv_inds=np.unique(np.random.randint(low=0, high=N-1, size=int(N*1e-5), dtype=np.int32))\r\n\u001b[?2004l\r\n\u001b[?2004h> b_arr[fv_inds]=True if pd.isna(fv) else (not fv)\r\n\u001b[?2004l\r\n\u001b[?2004h> sp_b_arr=SparseArray(b_arr, dtype=bool, fill_value=fv)\r\n\u001b[?2004l\r\n\u001b[?2004h> pr=cProfile.Profile()\r\n\u001b[?2004l\r\n\u001b[?2004h> pr.enable()\r\n\u001b[?2004l\r\n\u001b[?2004h> _ = sp_arr[sp_b_arr]\r\n\u001b[?2004l\r\n\u001b[?2004h> pr.disable()\r\n\u001b[?2004l\r\n\u001b[?2004h> s=io.StringIO()\r\n\u001b[?2004l\r\n\u001b[?2004h> ps=pstats.Stats(pr, stream=s).sort_stats('cumtime')\r\n\u001b[?2004l\r\n\u001b[?2004h> ps.print_stats(30)\r\n\u001b[?2004l\r\n\u001b[?2004h> print(s.getvalue())\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\n+ /opt/conda/envs/asv_3.10/bin/ninja\r\n\r\n[0/1] Generating write_version_file with a custom command\u001b[K\r\n[1/1] Generating write_version_file with a custom command\u001b[K\r\n         91 function calls in 0.015 seconds\r\n\r\n   Ordered by: cumulative time\r\n   List reduced from 56 to 30 due to restriction <30>\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n        1    0.001    0.001    0.015    0.015 /workspace/repo/pandas/core/arrays/sparse/array.py:929(__getitem__)\r\n        1    0.000    0.000    0.012    0.012 /workspace/repo/pandas/core/arrays/sparse/array.py:1034(take)\r\n        1    0.011    0.011    0.011    0.011 /workspace/repo/pandas/core/arrays/sparse/array.py:1118(_take_without_fill)\r\n        1    0.001    0.001    0.001    0.001 {built-in method numpy.arange}\r\n        1    0.000    0.000    0.000    0.000 {built-in method numpy.asarray}\r\n        6    0.000    0.000    0.000    0.000 {method 'reduce' of 'numpy.ufunc' objects}\r\n        1    0.000    0.000    0.000    0.000 /opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/core/numeric.py:615(flatnonzero)\r\n        1    0.000    0.000    0.000    0.000 /opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/core/fromnumeric.py:1881(nonzero)\r\n        1    0.000    0.000    0.000    0.000 /opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/core/fromnumeric.py:53(_wrapfunc)\r\n        1    0.000    0.000    0.000    0.000 {method 'nonzero' of 'numpy.ndarray' objects}\r\n        2    0.000    0.000    0.000    0.000 /opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/core/_methods.py:39(_amax)\r\n        1    0.000    0.000    0.000    0.000 {method 'max' of 'numpy.ndarray' objects}\r\n        2    0.000    0.000    0.000    0.000 /opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/core/_methods.py:43(_amin)\r\n        1    0.000    0.000    0.000    0.000 {method 'min' of 'numpy.ndarray' objects}\r\n        1    0.000    0.000    0.000    0.000 /workspace/repo/pandas/core/arrays/sparse/array.py:1913(make_sparse_index)\r\n        1    0.000    0.000    0.000    0.000 {method 'any' of 'numpy.ndarray' objects}\r\n        1    0.000    0.000    0.000    0.000 /opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/core/_methods.py:55(_any)\r\n        1    0.000    0.000    0.000    0.000 /opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/core/numeric.py:274(full)\r\n        1    0.000    0.000    0.000    0.000 /opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/core/fromnumeric.py:1768(ravel)\r\n        1    0.000    0.000    0.000    0.000 /opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2421(all)\r\n        1    0.000    0.000    0.000    0.000 /workspace/repo/pandas/core/dtypes/common.py:1183(is_bool_dtype)\r\n        1    0.000    0.000    0.000    0.000 /opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/core/fromnumeric.py:71(_wrapreduction)\r\n        1    0.000    0.000    0.000    0.000 /workspace/repo/pandas/core/arrays/sparse/array.py:497(_simple_new)\r\n        3    0.000    0.000    0.000    0.000 {built-in method builtins.len}\r\n       13    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\r\n        1    0.000    0.000    0.000    0.000 /workspace/repo/pandas/core/dtypes/common.py:1384(_get_dtype)\r\n        2    0.000    0.000    0.000    0.000 /workspace/repo/pandas/core/arrays/sparse/array.py:661(__len__)\r\n        1    0.000    0.000    0.000    0.000 {method 'ravel' of 'numpy.ndarray' objects}\r\n        3    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\r\n        1    0.000    0.000    0.000    0.000 {method 'astype' of 'numpy.ndarray' objects}\r\n\r\n\r\n\r\n"]
[125.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[125.002, "i", "python - << 'PY'\r"]
[125.004, "o", "python - << 'PY'\r\n"]
[125.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> import cProfile, pstats, io\r\n\u001b[?2004l\r\n\u001b[?2004h> import numpy as np\r\n\u001b[?2004l\r\n\u001b[?2004h> import pandas as pd\r\n\u001b[?2004l\r\n\u001b[?2004h> from pandas.arrays import SparseArray\r\n\u001b[?2004l\r\n\u001b[?2004h> N=1_000_000\r\n\u001b[?2004l\r\n\u001b[?2004h> arr=np.full(N, np.nan, dtype=float)\r\n\u001b[?2004l\r\n\u001b[?2004h> idx=np.random.choice(np.arange(N), int(N*1e-5), replace=False)\r\n\u001b[?2004l\r\n\u001b[?2004h> arr[idx]=np.random.rand(idx.size)\r\n\u001b[?2004l\r\n\u001b[?2004h> sp_arr=SparseArray(arr)\r\n\u001b[?2004l\r\n\u001b[?2004h> fv=float('nan')\r\n\u001b[?2004l\r\n\u001b[?2004h> b_arr=np.full(shape=N, fill_value=fv, dtype=bool)\r\n\u001b[?2004l\r\n\u001b[?2004h> fv_inds=np.unique(np.random.randint(low=0, high=N-1), axis=None)\r\n\u001b[?2004l\r\n\u001b[?2004h> # fix: for nan case, pandas benchmark sets True on sparse positions; mirror logic\r\n\u001b[?2004l\r\n\u001b[?2004h> b_arr=np.zeros(N, dtype=bool)\r\n\u001b[?2004l\r\n\u001b[?2004h> # produce sparse True values\r\n\u001b[?2004l\r\n\u001b[?2004h> inds=np.unique(np.random.randint(0, N-1, int(N*1e-5), dtype=np.int32))\r\n\u001b[?2004l\r\n\u001b[?2004h> b_arr[inds]=True\r\n\u001b[?2004l\r\n\u001b[?2004h> sp_b_arr=SparseArray(b_arr, dtype=bool, fill_value=float('nan'))\r\n\u001b[?2004l\r\n\u001b[?2004h> pr=cProfile.Profile(); pr.enable()\r\n\u001b[?2004l\r\n\u001b[?2004h> _ = sp_arr[sp_b_arr]\r\n\u001b[?2004l\r\n\u001b[?2004h> pr.disable(); s=io.StringIO(); p=pstats.Stats(pr, stream=s).sort_stats('cumtime'); p.print_stats(30); print(s.getvalue())\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\n+ /opt/conda/envs/asv_3.10/bin/ninja\r\n\r\n[0/1] Generating write_version_file with a custom command\u001b[K\r\n[1/1] Generating write_version_file with a custom command\u001b[K\r\n         72 function calls in 0.000 seconds\r\n\r\n   Ordered by: cumulative time\r\n   List reduced from 47 to 30 due to restriction <30>\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n        1    0.000    0.000    0.000    0.000 /workspace/repo/pandas/core/arrays/sparse/array.py:929(__getitem__)\r\n        1    0.000    0.000    0.000    0.000 /workspace/repo/pandas/core/arrays/sparse/array.py:1034(take)\r\n        1    0.000    0.000    0.000    0.000 /workspace/repo/pandas/core/arrays/sparse/array.py:1118(_take_without_fill)\r\n        1    0.000    0.000    0.000    0.000 /workspace/repo/pandas/core/dtypes/common.py:1183(is_bool_dtype)\r\n        1    0.000    0.000    0.000    0.000 /opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/core/numeric.py:615(flatnonzero)\r\n        1    0.000    0.000    0.000    0.000 {method 'any' of 'numpy.ndarray' objects}\r\n        3    0.000    0.000    0.000    0.000 {method 'reduce' of 'numpy.ufunc' objects}\r\n       13    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\r\n        1    0.000    0.000    0.000    0.000 /workspace/repo/pandas/core/arrays/sparse/array.py:497(_simple_new)\r\n        1    0.000    0.000    0.000    0.000 /workspace/repo/pandas/core/dtypes/common.py:1384(_get_dtype)\r\n        1    0.000    0.000    0.000    0.000 {method 'max' of 'numpy.ndarray' objects}\r\n        1    0.000    0.000    0.000    0.000 /opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/core/fromnumeric.py:1768(ravel)\r\n        1    0.000    0.000    0.000    0.000 /opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/core/_methods.py:39(_amax)\r\n        1    0.000    0.000    0.000    0.000 /opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/core/_methods.py:55(_any)\r\n        1    0.000    0.000    0.000    0.000 /workspace/repo/pandas/core/dtypes/generic.py:42(_instancecheck)\r\n        1    0.000    0.000    0.000    0.000 /opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/core/fromnumeric.py:1881(nonzero)\r\n        1    0.000    0.000    0.000    0.000 /opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/core/fromnumeric.py:53(_wrapfunc)\r\n        2    0.000    0.000    0.000    0.000 {built-in method builtins.len}\r\n        1    0.000    0.000    0.000    0.000 {built-in method numpy.asarray}\r\n        1    0.000    0.000    0.000    0.000 /workspace/repo/pandas/core/dtypes/generic.py:37(_check)\r\n        3    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\r\n        1    0.000    0.000    0.000    0.000 {method 'min' of 'numpy.ndarray' objects}\r\n        1    0.000    0.000    0.000    0.000 /opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/core/_methods.py:43(_amin)\r\n        1    0.000    0.000    0.000    0.000 /workspace/repo/pandas/core/arrays/sparse/array.py:622(fill_value)\r\n        1    0.000    0.000    0.000    0.000 /workspace/repo/pandas/core/dtypes/common.py:1581(pandas_dtype)\r\n        1    0.000    0.000    0.000    0.000 /workspace/repo/pandas/core/arrays/sparse/array.py:1913(make_sparse_index)\r\n        1    0.000    0.000    0.000    0.000 {method 'ravel' of 'numpy.ndarray' objects}\r\n        1    0.000    0.000    0.000    0.000 /workspace/repo/pandas/core/arrays/sparse/array.py:661(__len__)\r\n        1    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\r\n        1    0.000    0.000    0.000    0.000 {method 'nonzero' of 'numpy.ndarray' objects}\r\n\r\n\r\n\r\n"]
[130.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[130.002, "i", "sed -n '860,1160p' pandas/core/arrays/sparse/array.py\r"]
[130.004, "o", "sed -n '860,1160p' pandas/core/arrays/sparse/array.py\r\n"]
[130.006, "o", "\u001b[?2004l\r\n    def factorize(\r\n        self,\r\n        use_na_sentinel: bool = True,\r\n    ) -> tuple[np.ndarray, SparseArray]:\r\n        # Currently, ExtensionArray.factorize -> Tuple[ndarray, EA]\r\n        # The sparsity on this is backwards from what Sparse would want. Want\r\n        # ExtensionArray.factorize -> Tuple[EA, EA]\r\n        # Given that we have to return a dense array of codes, why bother\r\n        # implementing an efficient factorize?\r\n        codes, uniques = algos.factorize(\r\n            np.asarray(self), use_na_sentinel=use_na_sentinel\r\n        )\r\n        uniques_sp = SparseArray(uniques, dtype=self.dtype)\r\n        return codes, uniques_sp\r\n\r\n    def value_counts(self, dropna: bool = True) -> Series:\r\n        \"\"\"\r\n        Returns a Series containing counts of unique values.\r\n\r\n        Parameters\r\n        ----------\r\n        dropna : bool, default True\r\n            Don't include counts of NaN, even if NaN is in sp_values.\r\n\r\n        Returns\r\n        -------\r\n        counts : Series\r\n        \"\"\"\r\n        from pandas import (\r\n            Index,\r\n            Series,\r\n        )\r\n\r\n        keys, counts, _ = algos.value_counts_arraylike(self.sp_values, dropna=dropna)\r\n        fcounts = self.sp_index.ngaps\r\n        if fcounts > 0 and (not self._null_fill_value or not dropna):\r\n            mask = isna(keys) if self._null_fill_value else keys == self.fill_value\r\n            if mask.any():\r\n                counts[mask] += fcounts\r\n            else:\r\n                # error: Argument 1 to \"insert\" has incompatible type \"Union[\r\n                # ExtensionArray,ndarray[Any, Any]]\"; expected \"Union[\r\n                # _SupportsArray[dtype[Any]], Sequence[_SupportsArray[dtype\r\n                # [Any]]], Sequence[Sequence[_SupportsArray[dtype[Any]]]],\r\n                # Sequence[Sequence[Sequence[_SupportsArray[dtype[Any]]]]], Sequence\r\n                # [Sequence[Sequence[Sequence[_SupportsArray[dtype[Any]]]]]]]\"\r\n                keys = np.insert(keys, 0, self.fill_value)  # type: ignore[arg-type]\r\n                counts = np.insert(counts, 0, fcounts)\r\n\r\n        if not isinstance(keys, ABCIndex):\r\n            index = Index(keys)\r\n        else:\r\n            index = keys\r\n        return Series(counts, index=index, copy=False)\r\n\r\n    # --------\r\n    # Indexing\r\n    # --------\r\n    @overload\r\n    def __getitem__(self, key: ScalarIndexer) -> Any:\r\n        ...\r\n\r\n    @overload\r\n    def __getitem__(\r\n        self,\r\n        key: SequenceIndexer | tuple[int | ellipsis, ...],\r\n    ) -> Self:\r\n        ...\r\n\r\n    def __getitem__(\r\n        self,\r\n        key: PositionalIndexer | tuple[int | ellipsis, ...],\r\n    ) -> Self | Any:\r\n        if isinstance(key, tuple):\r\n            key = unpack_tuple_and_ellipses(key)\r\n            if key is Ellipsis:\r\n                raise ValueError(\"Cannot slice with Ellipsis\")\r\n\r\n        if is_integer(key):\r\n            return self._get_val_at(key)\r\n        elif isinstance(key, tuple):\r\n            # error: Invalid index type \"Tuple[Union[int, ellipsis], ...]\"\r\n            # for \"ndarray[Any, Any]\"; expected type\r\n            # \"Union[SupportsIndex, _SupportsArray[dtype[Union[bool_,\r\n            # integer[Any]]]], _NestedSequence[_SupportsArray[dtype[\r\n            # Union[bool_, integer[Any]]]]], _NestedSequence[Union[\r\n            # bool, int]], Tuple[Union[SupportsIndex, _SupportsArray[\r\n            # dtype[Union[bool_, integer[Any]]]], _NestedSequence[\r\n            # _SupportsArray[dtype[Union[bool_, integer[Any]]]]],\r\n            # _NestedSequence[Union[bool, int]]], ...]]\"\r\n            data_slice = self.to_dense()[key]  # type: ignore[index]\r\n        elif isinstance(key, slice):\r\n            # Avoid densifying when handling contiguous slices\r\n            if key.step is None or key.step == 1:\r\n                start = 0 if key.start is None else key.start\r\n                if start < 0:\r\n                    start += len(self)\r\n\r\n                end = len(self) if key.stop is None else key.stop\r\n                if end < 0:\r\n                    end += len(self)\r\n\r\n                indices = self.sp_index.indices\r\n                keep_inds = np.flatnonzero((indices >= start) & (indices < end))\r\n                sp_vals = self.sp_values[keep_inds]\r\n\r\n                sp_index = indices[keep_inds].copy()\r\n\r\n                # If we've sliced to not include the start of the array, all our indices\r\n                # should be shifted. NB: here we are careful to also not shift by a\r\n                # negative value for a case like [0, 1][-100:] where the start index\r\n                # should be treated like 0\r\n                if start > 0:\r\n                    sp_index -= start\r\n\r\n                # Length of our result should match applying this slice to a range\r\n                # of the length of our original array\r\n                new_len = len(range(len(self))[key])\r\n                new_sp_index = make_sparse_index(new_len, sp_index, self.kind)\r\n                return type(self)._simple_new(sp_vals, new_sp_index, self.dtype)\r\n            else:\r\n                indices = np.arange(len(self), dtype=np.int32)[key]\r\n                return self.take(indices)\r\n\r\n        elif not is_list_like(key):\r\n            # e.g. \"foo\" or 2.5\r\n            # exception message copied from numpy\r\n            raise IndexError(\r\n                r\"only integers, slices (`:`), ellipsis (`...`), numpy.newaxis \"\r\n                r\"(`None`) and integer or boolean arrays are valid indices\"\r\n            )\r\n\r\n        else:\r\n            if isinstance(key, SparseArray):\r\n                # NOTE: If we guarantee that SparseDType(bool)\r\n                # has only fill_value - true, false or nan\r\n                # (see GH PR 44955)\r\n                # we can apply mask very fast:\r\n                if is_bool_dtype(key):\r\n                    if isna(key.fill_value):\r\n                        return self.take(key.sp_index.indices[key.sp_values])\r\n                    if not key.fill_value:\r\n                        return self.take(key.sp_index.indices)\r\n                    n = len(self)\r\n                    mask = np.full(n, True, dtype=np.bool_)\r\n                    mask[key.sp_index.indices] = False\r\n                    return self.take(np.arange(n)[mask])\r\n                else:\r\n                    key = np.asarray(key)\r\n\r\n            key = check_array_indexer(self, key)\r\n\r\n            if com.is_bool_indexer(key):\r\n                # mypy doesn't know we have an array here\r\n                key = cast(np.ndarray, key)\r\n                return self.take(np.arange(len(key), dtype=np.int32)[key])\r\n            elif hasattr(key, \"__len__\"):\r\n                return self.take(key)\r\n            else:\r\n                raise ValueError(f\"Cannot slice with '{key}'\")\r\n\r\n        return type(self)(data_slice, kind=self.kind)\r\n\r\n    def _get_val_at(self, loc):\r\n        loc = validate_insert_loc(loc, len(self))\r\n\r\n        sp_loc = self.sp_index.lookup(loc)\r\n        if sp_loc == -1:\r\n            return self.fill_value\r\n        else:\r\n            val = self.sp_values[sp_loc]\r\n            val = maybe_box_datetimelike(val, self.sp_values.dtype)\r\n            return val\r\n\r\n    def take(self, indices, *, allow_fill: bool = False, fill_value=None) -> Self:\r\n        if is_scalar(indices):\r\n            raise ValueError(f\"'indices' must be an array, not a scalar '{indices}'.\")\r\n        indices = np.asarray(indices, dtype=np.int32)\r\n\r\n        dtype = None\r\n        if indices.size == 0:\r\n            result = np.array([], dtype=\"object\")\r\n            dtype = self.dtype\r\n        elif allow_fill:\r\n            result = self._take_with_fill(indices, fill_value=fill_value)\r\n        else:\r\n            return self._take_without_fill(indices)\r\n\r\n        return type(self)(\r\n            result, fill_value=self.fill_value, kind=self.kind, dtype=dtype\r\n        )\r\n\r\n    def _take_with_fill(self, indices, fill_value=None) -> np.ndarray:\r\n        if fill_value is None:\r\n            fill_value = self.dtype.na_value\r\n\r\n        if indices.min() < -1:\r\n            raise ValueError(\r\n                \"Invalid value in 'indices'. Must be between -1 \"\r\n                \"and the length of the array.\"\r\n            )\r\n\r\n        if indices.max() >= len(self):\r\n            raise IndexError(\"out of bounds value in 'indices'.\")\r\n\r\n        if len(self) == 0:\r\n            # Empty... Allow taking only if all empty\r\n            if (indices == -1).all():\r\n                dtype = np.result_type(self.sp_values, type(fill_value))\r\n                taken = np.empty_like(indices, dtype=dtype)\r\n                taken.fill(fill_value)\r\n                return taken\r\n            else:\r\n                raise IndexError(\"cannot do a non-empty take from an empty axes.\")\r\n\r\n        # sp_indexer may be -1 for two reasons\r\n        # 1.) we took for an index of -1 (new)\r\n        # 2.) we took a value that was self.fill_value (old)\r\n        sp_indexer = self.sp_index.lookup_array(indices)\r\n        new_fill_indices = indices == -1\r\n        old_fill_indices = (sp_indexer == -1) & ~new_fill_indices\r\n\r\n        if self.sp_index.npoints == 0 and old_fill_indices.all():\r\n            # We've looked up all valid points on an all-sparse array.\r\n            taken = np.full(\r\n                sp_indexer.shape, fill_value=self.fill_value, dtype=self.dtype.subtype\r\n            )\r\n\r\n        elif self.sp_index.npoints == 0:\r\n            # Use the old fill_value unless we took for an index of -1\r\n            _dtype = np.result_type(self.dtype.subtype, type(fill_value))\r\n            taken = np.full(sp_indexer.shape, fill_value=fill_value, dtype=_dtype)\r\n            taken[old_fill_indices] = self.fill_value\r\n        else:\r\n            taken = self.sp_values.take(sp_indexer)\r\n\r\n            # Fill in two steps.\r\n            # Old fill values\r\n            # New fill values\r\n            # potentially coercing to a new dtype at each stage.\r\n\r\n            m0 = sp_indexer[old_fill_indices] < 0\r\n            m1 = sp_indexer[new_fill_indices] < 0\r\n\r\n            result_type = taken.dtype\r\n\r\n            if m0.any():\r\n                result_type = np.result_type(result_type, type(self.fill_value))\r\n                taken = taken.astype(result_type)\r\n                taken[old_fill_indices] = self.fill_value\r\n\r\n            if m1.any():\r\n                result_type = np.result_type(result_type, type(fill_value))\r\n                taken = taken.astype(result_type)\r\n                taken[new_fill_indices] = fill_value\r\n\r\n        return taken\r\n\r\n    def _take_without_fill(self, indices) -> Self:\r\n        to_shift = indices < 0\r\n\r\n        n = len(self)\r\n\r\n        if (indices.max() >= n) or (indices.min() < -n):\r\n            if n == 0:\r\n                raise IndexError(\"cannot do a non-empty take from an empty axes.\")\r\n            raise IndexError(\"out of bounds value in 'indices'.\")\r\n\r\n        if to_shift.any():\r\n            indices = indices.copy()\r\n            indices[to_shift] += n\r\n\r\n        sp_indexer = self.sp_index.lookup_array(indices)\r\n        value_mask = sp_indexer != -1\r\n        new_sp_values = self.sp_values[sp_indexer[value_mask]]\r\n\r\n        value_indices = np.flatnonzero(value_mask).astype(np.int32, copy=False)\r\n\r\n        new_sp_index = make_sparse_index(len(indices), value_indices, kind=self.kind)\r\n        return type(self)._simple_new(new_sp_values, new_sp_index, dtype=self.dtype)\r\n\r\n    def searchsorted(\r\n        self,\r\n        v: ArrayLike | object,\r\n        side: Literal[\"left\", \"right\"] = \"left\",\r\n        sorter: NumpySorter | None = None,\r\n    ) -> npt.NDArray[np.intp] | np.intp:\r\n        msg = \"searchsorted requires high memory usage.\"\r\n        warnings.warn(msg, PerformanceWarning, stacklevel=find_stack_level())\r\n        v = np.asarray(v)\r\n        return np.asarray(self, dtype=self.dtype.subtype).searchsorted(v, side, sorter)\r\n\r\n    def copy(self) -> Self:\r\n        values = self.sp_values.copy()\r\n        return self._simple_new(values, self.sp_index, self.dtype)\r\n\r\n    @classmethod\r\n    def _concat_same_type(cls, to_concat: Sequence[Self]) -> Self:\r\n        fill_value = to_concat[0].fill_value\r\n\r\n        values = []\r\n"]
[135.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[135.002, "i", "sed -n '1160,1400p' pandas/core/arrays/sparse/array.py\r"]
[135.004, "o", "sed -n '1160,1400p' pandas/core/arrays/sparse/array.py\r\n"]
[135.006, "o", "\u001b[?2004l\r\n        values = []\r\n        length = 0\r\n\r\n        if to_concat:\r\n            sp_kind = to_concat[0].kind\r\n        else:\r\n            sp_kind = \"integer\"\r\n\r\n        sp_index: SparseIndex\r\n        if sp_kind == \"integer\":\r\n            indices = []\r\n\r\n            for arr in to_concat:\r\n                int_idx = arr.sp_index.indices.copy()\r\n                int_idx += length  # TODO: wraparound\r\n                length += arr.sp_index.length\r\n\r\n                values.append(arr.sp_values)\r\n                indices.append(int_idx)\r\n\r\n            data = np.concatenate(values)\r\n            indices_arr = np.concatenate(indices)\r\n            # error: Argument 2 to \"IntIndex\" has incompatible type\r\n            # \"ndarray[Any, dtype[signedinteger[_32Bit]]]\";\r\n            # expected \"Sequence[int]\"\r\n            sp_index = IntIndex(length, indices_arr)  # type: ignore[arg-type]\r\n\r\n        else:\r\n            # when concatenating block indices, we don't claim that you'll\r\n            # get an identical index as concatenating the values and then\r\n            # creating a new index. We don't want to spend the time trying\r\n            # to merge blocks across arrays in `to_concat`, so the resulting\r\n            # BlockIndex may have more blocks.\r\n            blengths = []\r\n            blocs = []\r\n\r\n            for arr in to_concat:\r\n                block_idx = arr.sp_index.to_block_index()\r\n\r\n                values.append(arr.sp_values)\r\n                blocs.append(block_idx.blocs.copy() + length)\r\n                blengths.append(block_idx.blengths)\r\n                length += arr.sp_index.length\r\n\r\n            data = np.concatenate(values)\r\n            blocs_arr = np.concatenate(blocs)\r\n            blengths_arr = np.concatenate(blengths)\r\n\r\n            sp_index = BlockIndex(length, blocs_arr, blengths_arr)\r\n\r\n        return cls(data, sparse_index=sp_index, fill_value=fill_value)\r\n\r\n    def astype(self, dtype: AstypeArg | None = None, copy: bool = True):\r\n        \"\"\"\r\n        Change the dtype of a SparseArray.\r\n\r\n        The output will always be a SparseArray. To convert to a dense\r\n        ndarray with a certain dtype, use :meth:`numpy.asarray`.\r\n\r\n        Parameters\r\n        ----------\r\n        dtype : np.dtype or ExtensionDtype\r\n            For SparseDtype, this changes the dtype of\r\n            ``self.sp_values`` and the ``self.fill_value``.\r\n\r\n            For other dtypes, this only changes the dtype of\r\n            ``self.sp_values``.\r\n\r\n        copy : bool, default True\r\n            Whether to ensure a copy is made, even if not necessary.\r\n\r\n        Returns\r\n        -------\r\n        SparseArray\r\n\r\n        Examples\r\n        --------\r\n        >>> arr = pd.arrays.SparseArray([0, 0, 1, 2])\r\n        >>> arr\r\n        [0, 0, 1, 2]\r\n        Fill: 0\r\n        IntIndex\r\n        Indices: array([2, 3], dtype=int32)\r\n\r\n        >>> arr.astype(SparseDtype(np.dtype('int32')))\r\n        [0, 0, 1, 2]\r\n        Fill: 0\r\n        IntIndex\r\n        Indices: array([2, 3], dtype=int32)\r\n\r\n        Using a NumPy dtype with a different kind (e.g. float) will coerce\r\n        just ``self.sp_values``.\r\n\r\n        >>> arr.astype(SparseDtype(np.dtype('float64')))\r\n        ... # doctest: +NORMALIZE_WHITESPACE\r\n        [nan, nan, 1.0, 2.0]\r\n        Fill: nan\r\n        IntIndex\r\n        Indices: array([2, 3], dtype=int32)\r\n\r\n        Using a SparseDtype, you can also change the fill value as well.\r\n\r\n        >>> arr.astype(SparseDtype(\"float64\", fill_value=0.0))\r\n        ... # doctest: +NORMALIZE_WHITESPACE\r\n        [0.0, 0.0, 1.0, 2.0]\r\n        Fill: 0.0\r\n        IntIndex\r\n        Indices: array([2, 3], dtype=int32)\r\n        \"\"\"\r\n        if dtype == self._dtype:\r\n            if not copy:\r\n                return self\r\n            else:\r\n                return self.copy()\r\n\r\n        future_dtype = pandas_dtype(dtype)\r\n        if not isinstance(future_dtype, SparseDtype):\r\n            # GH#34457\r\n            values = np.asarray(self)\r\n            values = ensure_wrapped_if_datetimelike(values)\r\n            return astype_array(values, dtype=future_dtype, copy=False)\r\n\r\n        dtype = self.dtype.update_dtype(dtype)\r\n        subtype = pandas_dtype(dtype._subtype_with_str)\r\n        subtype = cast(np.dtype, subtype)  # ensured by update_dtype\r\n        values = ensure_wrapped_if_datetimelike(self.sp_values)\r\n        sp_values = astype_array(values, subtype, copy=copy)\r\n        sp_values = np.asarray(sp_values)\r\n\r\n        return self._simple_new(sp_values, self.sp_index, dtype)\r\n\r\n    def map(self, mapper, na_action=None) -> Self:\r\n        \"\"\"\r\n        Map categories using an input mapping or function.\r\n\r\n        Parameters\r\n        ----------\r\n        mapper : dict, Series, callable\r\n            The correspondence from old values to new.\r\n        na_action : {None, 'ignore'}, default None\r\n            If 'ignore', propagate NA values, without passing them to the\r\n            mapping correspondence.\r\n\r\n        Returns\r\n        -------\r\n        SparseArray\r\n            The output array will have the same density as the input.\r\n            The output fill value will be the result of applying the\r\n            mapping to ``self.fill_value``\r\n\r\n        Examples\r\n        --------\r\n        >>> arr = pd.arrays.SparseArray([0, 1, 2])\r\n        >>> arr.map(lambda x: x + 10)\r\n        [10, 11, 12]\r\n        Fill: 10\r\n        IntIndex\r\n        Indices: array([1, 2], dtype=int32)\r\n\r\n        >>> arr.map({0: 10, 1: 11, 2: 12})\r\n        [10, 11, 12]\r\n        Fill: 10\r\n        IntIndex\r\n        Indices: array([1, 2], dtype=int32)\r\n\r\n        >>> arr.map(pd.Series([10, 11, 12], index=[0, 1, 2]))\r\n        [10, 11, 12]\r\n        Fill: 10\r\n        IntIndex\r\n        Indices: array([1, 2], dtype=int32)\r\n        \"\"\"\r\n        is_map = isinstance(mapper, (abc.Mapping, ABCSeries))\r\n\r\n        fill_val = self.fill_value\r\n\r\n        if na_action is None or notna(fill_val):\r\n            fill_val = mapper.get(fill_val, fill_val) if is_map else mapper(fill_val)\r\n\r\n        def func(sp_val):\r\n            new_sp_val = mapper.get(sp_val, None) if is_map else mapper(sp_val)\r\n            # check identity and equality because nans are not equal to each other\r\n            if new_sp_val is fill_val or new_sp_val == fill_val:\r\n                msg = \"fill value in the sparse values not supported\"\r\n                raise ValueError(msg)\r\n            return new_sp_val\r\n\r\n        sp_values = [func(x) for x in self.sp_values]\r\n\r\n        return type(self)(sp_values, sparse_index=self.sp_index, fill_value=fill_val)\r\n\r\n    def to_dense(self) -> np.ndarray:\r\n        \"\"\"\r\n        Convert SparseArray to a NumPy array.\r\n\r\n        Returns\r\n        -------\r\n        arr : NumPy array\r\n        \"\"\"\r\n        return np.asarray(self, dtype=self.sp_values.dtype)\r\n\r\n    def _where(self, mask, value):\r\n        # NB: may not preserve dtype, e.g. result may be Sparse[float64]\r\n        #  while self is Sparse[int64]\r\n        naive_implementation = np.where(mask, self, value)\r\n        dtype = SparseDtype(naive_implementation.dtype, fill_value=self.fill_value)\r\n        result = type(self)._from_sequence(naive_implementation, dtype=dtype)\r\n        return result\r\n\r\n    # ------------------------------------------------------------------------\r\n    # IO\r\n    # ------------------------------------------------------------------------\r\n    def __setstate__(self, state) -> None:\r\n        \"\"\"Necessary for making this object picklable\"\"\"\r\n        if isinstance(state, tuple):\r\n            # Compat for pandas < 0.24.0\r\n            nd_state, (fill_value, sp_index) = state\r\n            sparse_values = np.array([])\r\n            sparse_values.__setstate__(nd_state)\r\n\r\n            self._sparse_values = sparse_values\r\n            self._sparse_index = sp_index\r\n            self._dtype = SparseDtype(sparse_values.dtype, fill_value)\r\n        else:\r\n            self.__dict__.update(state)\r\n\r\n    def nonzero(self) -> tuple[npt.NDArray[np.int32]]:\r\n        if self.fill_value == 0:\r\n            return (self.sp_index.indices,)\r\n        else:\r\n            return (self.sp_index.indices[self.sp_values != 0],)\r\n\r\n    # ------------------------------------------------------------------------\r\n    # Reductions\r\n    # ------------------------------------------------------------------------\r\n\r\n    def _reduce(\r\n        self, name: str, *, skipna: bool = True, keepdims: bool = False, **kwargs\r\n    ):\r\n        method = getattr(self, name, None)\r\n\r\n        if method is None:\r\n"]
[140.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[140.002, "i", "grep -n \"def __getitem__\\|def take\\|is_bool_dtype\\|nonzero\\|mask\" pandas/core/arrays/sparse/array.py\r"]
[140.004, "o", "grep -n \"def __getitem__\\|def take\\|is_bool_dtype\\|nonzero\\|mask\" pandas/core/arrays/sparse/array.py\r\n"]
[140.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K45\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kis_bool_dtype\u001b[m\u001b[K,\r\n\u001b[32m\u001b[K279\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    if \u001b[01;31m\u001b[Kis_bool_dtype\u001b[m\u001b[K(dtype):\r\n\u001b[32m\u001b[K658\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K = notna(sp_vals)\r\n\u001b[32m\u001b[K659\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        return sp_vals[\u001b[01;31m\u001b[Kmask\u001b[m\u001b[K]\r\n\u001b[32m\u001b[K714\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K = np.full(len(self), False, dtype=np.bool_)\r\n\u001b[32m\u001b[K715\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K[self.sp_index.indices] = isna(self.sp_values)\r\n\u001b[32m\u001b[K716\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        return type(self)(\u001b[01;31m\u001b[Kmask\u001b[m\u001b[K, fill_value=False, dtype=dtype)\r\n\u001b[32m\u001b[K839\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K = np.asarray(self.isna())\r\n\u001b[32m\u001b[K840\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        return algos.duplicated(values, keep=keep, \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K=\u001b[01;31m\u001b[Kmask\u001b[m\u001b[K)\r\n\u001b[32m\u001b[K896\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K = isna(keys) if self._null_fill_value else keys == self.fill_value\r\n\u001b[32m\u001b[K897\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            if \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K.any():\r\n\u001b[32m\u001b[K898\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                counts[\u001b[01;31m\u001b[Kmask\u001b[m\u001b[K] += fcounts\r\n\u001b[32m\u001b[K919\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdef __getitem__\u001b[m\u001b[K(self, key: ScalarIndexer) -> Any:\r\n\u001b[32m\u001b[K923\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdef __getitem__\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K929\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdef __getitem__\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K963\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                keep_inds = np.flat\u001b[01;31m\u001b[Knonzero\u001b[m\u001b[K((indices >= start) & (indices < end))\r\n\u001b[32m\u001b[K997\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                # we can apply \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K very fast:\r\n\u001b[32m\u001b[K998\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                if \u001b[01;31m\u001b[Kis_bool_dtype\u001b[m\u001b[K(key):\r\n\u001b[32m\u001b[K1004\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                    \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K = np.full(n, True, dtype=np.bool_)\r\n\u001b[32m\u001b[K1005\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                    \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K[key.sp_index.indices] = False\r\n\u001b[32m\u001b[K1006\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                    return self.take(np.arange(n)[\u001b[01;31m\u001b[Kmask\u001b[m\u001b[K])\r\n\u001b[32m\u001b[K1034\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdef take\u001b[m\u001b[K(self, indices, *, allow_fill: bool = False, fill_value=None) -> Self:\r\n\u001b[32m\u001b[K1133\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        value_\u001b[01;31m\u001b[Kmask\u001b[m\u001b[K = sp_indexer != -1\r\n\u001b[32m\u001b[K1134\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        new_sp_values = self.sp_values[sp_indexer[value_\u001b[01;31m\u001b[Kmask\u001b[m\u001b[K]]\r\n\u001b[32m\u001b[K1136\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        value_indices = np.flat\u001b[01;31m\u001b[Knonzero\u001b[m\u001b[K(value_\u001b[01;31m\u001b[Kmask\u001b[m\u001b[K).astype(np.int32, copy=False)\r\n\u001b[32m\u001b[K1360\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    def _where(self, \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K, value):\r\n\u001b[32m\u001b[K1363\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        naive_implementation = np.where(\u001b[01;31m\u001b[Kmask\u001b[m\u001b[K, self, value)\r\n\u001b[32m\u001b[K1385\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    def \u001b[01;31m\u001b[Knonzero\u001b[m\u001b[K(self) -> tuple[npt.NDArray[np.int32]]:\r\n\u001b[32m\u001b[K1626\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K = np.asarray(isna(values))\r\n\u001b[32m\u001b[K1630\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        non_nans = values[~\u001b[01;31m\u001b[Kmask\u001b[m\u001b[K]\r\n\u001b[32m\u001b[K1631\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        non_nan_idx = idx[~\u001b[01;31m\u001b[Kmask\u001b[m\u001b[K]\r\n\u001b[32m\u001b[K1871\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K = notna(arr)\r\n\u001b[32m\u001b[K1881\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K = splib.make_\u001b[01;31m\u001b[Kmask\u001b[m\u001b[K_object_ndarray(arr, fill_value)\r\n\u001b[32m\u001b[K1883\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K = arr != fill_value\r\n\u001b[32m\u001b[K1886\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    if length != len(\u001b[01;31m\u001b[Kmask\u001b[m\u001b[K):\r\n\u001b[32m\u001b[K1888\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        indices = \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K.sp_index.indices\r\n\u001b[32m\u001b[K1890\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        indices = \u001b[01;31m\u001b[Kmask\u001b[m\u001b[K.\u001b[01;31m\u001b[Knonzero\u001b[m\u001b[K()[0].astype(np.int32)\r\n\u001b[32m\u001b[K1893\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    sparsified_values = arr[\u001b[01;31m\u001b[Kmask\u001b[m\u001b[K]\r\n"]
[145.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[145.002, "i", "sed -n '900,1250p' pandas/core/arrays/sparse/array.py\r"]
[145.004, "o", "sed -n '900,1250p' pandas/core/arrays/sparse/array.py\r\n"]
[145.006, "o", "\u001b[?2004l\r\n                # error: Argument 1 to \"insert\" has incompatible type \"Union[\r\n                # ExtensionArray,ndarray[Any, Any]]\"; expected \"Union[\r\n                # _SupportsArray[dtype[Any]], Sequence[_SupportsArray[dtype\r\n                # [Any]]], Sequence[Sequence[_SupportsArray[dtype[Any]]]],\r\n                # Sequence[Sequence[Sequence[_SupportsArray[dtype[Any]]]]], Sequence\r\n                # [Sequence[Sequence[Sequence[_SupportsArray[dtype[Any]]]]]]]\"\r\n                keys = np.insert(keys, 0, self.fill_value)  # type: ignore[arg-type]\r\n                counts = np.insert(counts, 0, fcounts)\r\n\r\n        if not isinstance(keys, ABCIndex):\r\n            index = Index(keys)\r\n        else:\r\n            index = keys\r\n        return Series(counts, index=index, copy=False)\r\n\r\n    # --------\r\n    # Indexing\r\n    # --------\r\n    @overload\r\n    def __getitem__(self, key: ScalarIndexer) -> Any:\r\n        ...\r\n\r\n    @overload\r\n    def __getitem__(\r\n        self,\r\n        key: SequenceIndexer | tuple[int | ellipsis, ...],\r\n    ) -> Self:\r\n        ...\r\n\r\n    def __getitem__(\r\n        self,\r\n        key: PositionalIndexer | tuple[int | ellipsis, ...],\r\n    ) -> Self | Any:\r\n        if isinstance(key, tuple):\r\n            key = unpack_tuple_and_ellipses(key)\r\n            if key is Ellipsis:\r\n                raise ValueError(\"Cannot slice with Ellipsis\")\r\n\r\n        if is_integer(key):\r\n            return self._get_val_at(key)\r\n        elif isinstance(key, tuple):\r\n            # error: Invalid index type \"Tuple[Union[int, ellipsis], ...]\"\r\n            # for \"ndarray[Any, Any]\"; expected type\r\n            # \"Union[SupportsIndex, _SupportsArray[dtype[Union[bool_,\r\n            # integer[Any]]]], _NestedSequence[_SupportsArray[dtype[\r\n            # Union[bool_, integer[Any]]]]], _NestedSequence[Union[\r\n            # bool, int]], Tuple[Union[SupportsIndex, _SupportsArray[\r\n            # dtype[Union[bool_, integer[Any]]]], _NestedSequence[\r\n            # _SupportsArray[dtype[Union[bool_, integer[Any]]]]],\r\n            # _NestedSequence[Union[bool, int]]], ...]]\"\r\n            data_slice = self.to_dense()[key]  # type: ignore[index]\r\n        elif isinstance(key, slice):\r\n            # Avoid densifying when handling contiguous slices\r\n            if key.step is None or key.step == 1:\r\n                start = 0 if key.start is None else key.start\r\n                if start < 0:\r\n                    start += len(self)\r\n\r\n                end = len(self) if key.stop is None else key.stop\r\n                if end < 0:\r\n                    end += len(self)\r\n\r\n                indices = self.sp_index.indices\r\n                keep_inds = np.flatnonzero((indices >= start) & (indices < end))\r\n                sp_vals = self.sp_values[keep_inds]\r\n\r\n                sp_index = indices[keep_inds].copy()\r\n\r\n                # If we've sliced to not include the start of the array, all our indices\r\n                # should be shifted. NB: here we are careful to also not shift by a\r\n                # negative value for a case like [0, 1][-100:] where the start index\r\n                # should be treated like 0\r\n                if start > 0:\r\n                    sp_index -= start\r\n\r\n                # Length of our result should match applying this slice to a range\r\n                # of the length of our original array\r\n                new_len = len(range(len(self))[key])\r\n                new_sp_index = make_sparse_index(new_len, sp_index, self.kind)\r\n                return type(self)._simple_new(sp_vals, new_sp_index, self.dtype)\r\n            else:\r\n                indices = np.arange(len(self), dtype=np.int32)[key]\r\n                return self.take(indices)\r\n\r\n        elif not is_list_like(key):\r\n            # e.g. \"foo\" or 2.5\r\n            # exception message copied from numpy\r\n            raise IndexError(\r\n                r\"only integers, slices (`:`), ellipsis (`...`), numpy.newaxis \"\r\n                r\"(`None`) and integer or boolean arrays are valid indices\"\r\n            )\r\n\r\n        else:\r\n            if isinstance(key, SparseArray):\r\n                # NOTE: If we guarantee that SparseDType(bool)\r\n                # has only fill_value - true, false or nan\r\n                # (see GH PR 44955)\r\n                # we can apply mask very fast:\r\n                if is_bool_dtype(key):\r\n                    if isna(key.fill_value):\r\n                        return self.take(key.sp_index.indices[key.sp_values])\r\n                    if not key.fill_value:\r\n                        return self.take(key.sp_index.indices)\r\n                    n = len(self)\r\n                    mask = np.full(n, True, dtype=np.bool_)\r\n                    mask[key.sp_index.indices] = False\r\n                    return self.take(np.arange(n)[mask])\r\n                else:\r\n                    key = np.asarray(key)\r\n\r\n            key = check_array_indexer(self, key)\r\n\r\n            if com.is_bool_indexer(key):\r\n                # mypy doesn't know we have an array here\r\n                key = cast(np.ndarray, key)\r\n                return self.take(np.arange(len(key), dtype=np.int32)[key])\r\n            elif hasattr(key, \"__len__\"):\r\n                return self.take(key)\r\n            else:\r\n                raise ValueError(f\"Cannot slice with '{key}'\")\r\n\r\n        return type(self)(data_slice, kind=self.kind)\r\n\r\n    def _get_val_at(self, loc):\r\n        loc = validate_insert_loc(loc, len(self))\r\n\r\n        sp_loc = self.sp_index.lookup(loc)\r\n        if sp_loc == -1:\r\n            return self.fill_value\r\n        else:\r\n            val = self.sp_values[sp_loc]\r\n            val = maybe_box_datetimelike(val, self.sp_values.dtype)\r\n            return val\r\n\r\n    def take(self, indices, *, allow_fill: bool = False, fill_value=None) -> Self:\r\n        if is_scalar(indices):\r\n            raise ValueError(f\"'indices' must be an array, not a scalar '{indices}'.\")\r\n        indices = np.asarray(indices, dtype=np.int32)\r\n\r\n        dtype = None\r\n        if indices.size == 0:\r\n            result = np.array([], dtype=\"object\")\r\n            dtype = self.dtype\r\n        elif allow_fill:\r\n            result = self._take_with_fill(indices, fill_value=fill_value)\r\n        else:\r\n            return self._take_without_fill(indices)\r\n\r\n        return type(self)(\r\n            result, fill_value=self.fill_value, kind=self.kind, dtype=dtype\r\n        )\r\n\r\n    def _take_with_fill(self, indices, fill_value=None) -> np.ndarray:\r\n        if fill_value is None:\r\n            fill_value = self.dtype.na_value\r\n\r\n        if indices.min() < -1:\r\n            raise ValueError(\r\n                \"Invalid value in 'indices'. Must be between -1 \"\r\n                \"and the length of the array.\"\r\n            )\r\n\r\n        if indices.max() >= len(self):\r\n            raise IndexError(\"out of bounds value in 'indices'.\")\r\n\r\n        if len(self) == 0:\r\n            # Empty... Allow taking only if all empty\r\n            if (indices == -1).all():\r\n                dtype = np.result_type(self.sp_values, type(fill_value))\r\n                taken = np.empty_like(indices, dtype=dtype)\r\n                taken.fill(fill_value)\r\n                return taken\r\n            else:\r\n                raise IndexError(\"cannot do a non-empty take from an empty axes.\")\r\n\r\n        # sp_indexer may be -1 for two reasons\r\n        # 1.) we took for an index of -1 (new)\r\n        # 2.) we took a value that was self.fill_value (old)\r\n        sp_indexer = self.sp_index.lookup_array(indices)\r\n        new_fill_indices = indices == -1\r\n        old_fill_indices = (sp_indexer == -1) & ~new_fill_indices\r\n\r\n        if self.sp_index.npoints == 0 and old_fill_indices.all():\r\n            # We've looked up all valid points on an all-sparse array.\r\n            taken = np.full(\r\n                sp_indexer.shape, fill_value=self.fill_value, dtype=self.dtype.subtype\r\n            )\r\n\r\n        elif self.sp_index.npoints == 0:\r\n            # Use the old fill_value unless we took for an index of -1\r\n            _dtype = np.result_type(self.dtype.subtype, type(fill_value))\r\n            taken = np.full(sp_indexer.shape, fill_value=fill_value, dtype=_dtype)\r\n            taken[old_fill_indices] = self.fill_value\r\n        else:\r\n            taken = self.sp_values.take(sp_indexer)\r\n\r\n            # Fill in two steps.\r\n            # Old fill values\r\n            # New fill values\r\n            # potentially coercing to a new dtype at each stage.\r\n\r\n            m0 = sp_indexer[old_fill_indices] < 0\r\n            m1 = sp_indexer[new_fill_indices] < 0\r\n\r\n            result_type = taken.dtype\r\n\r\n            if m0.any():\r\n                result_type = np.result_type(result_type, type(self.fill_value))\r\n                taken = taken.astype(result_type)\r\n                taken[old_fill_indices] = self.fill_value\r\n\r\n            if m1.any():\r\n                result_type = np.result_type(result_type, type(fill_value))\r\n                taken = taken.astype(result_type)\r\n                taken[new_fill_indices] = fill_value\r\n\r\n        return taken\r\n\r\n    def _take_without_fill(self, indices) -> Self:\r\n        to_shift = indices < 0\r\n\r\n        n = len(self)\r\n\r\n        if (indices.max() >= n) or (indices.min() < -n):\r\n            if n == 0:\r\n                raise IndexError(\"cannot do a non-empty take from an empty axes.\")\r\n            raise IndexError(\"out of bounds value in 'indices'.\")\r\n\r\n        if to_shift.any():\r\n            indices = indices.copy()\r\n            indices[to_shift] += n\r\n\r\n        sp_indexer = self.sp_index.lookup_array(indices)\r\n        value_mask = sp_indexer != -1\r\n        new_sp_values = self.sp_values[sp_indexer[value_mask]]\r\n\r\n        value_indices = np.flatnonzero(value_mask).astype(np.int32, copy=False)\r\n\r\n        new_sp_index = make_sparse_index(len(indices), value_indices, kind=self.kind)\r\n        return type(self)._simple_new(new_sp_values, new_sp_index, dtype=self.dtype)\r\n\r\n    def searchsorted(\r\n        self,\r\n        v: ArrayLike | object,\r\n        side: Literal[\"left\", \"right\"] = \"left\",\r\n        sorter: NumpySorter | None = None,\r\n    ) -> npt.NDArray[np.intp] | np.intp:\r\n        msg = \"searchsorted requires high memory usage.\"\r\n        warnings.warn(msg, PerformanceWarning, stacklevel=find_stack_level())\r\n        v = np.asarray(v)\r\n        return np.asarray(self, dtype=self.dtype.subtype).searchsorted(v, side, sorter)\r\n\r\n    def copy(self) -> Self:\r\n        values = self.sp_values.copy()\r\n        return self._simple_new(values, self.sp_index, self.dtype)\r\n\r\n    @classmethod\r\n    def _concat_same_type(cls, to_concat: Sequence[Self]) -> Self:\r\n        fill_value = to_concat[0].fill_value\r\n\r\n        values = []\r\n        length = 0\r\n\r\n        if to_concat:\r\n            sp_kind = to_concat[0].kind\r\n        else:\r\n            sp_kind = \"integer\"\r\n\r\n        sp_index: SparseIndex\r\n        if sp_kind == \"integer\":\r\n            indices = []\r\n\r\n            for arr in to_concat:\r\n                int_idx = arr.sp_index.indices.copy()\r\n                int_idx += length  # TODO: wraparound\r\n                length += arr.sp_index.length\r\n\r\n                values.append(arr.sp_values)\r\n                indices.append(int_idx)\r\n\r\n            data = np.concatenate(values)\r\n            indices_arr = np.concatenate(indices)\r\n            # error: Argument 2 to \"IntIndex\" has incompatible type\r\n            # \"ndarray[Any, dtype[signedinteger[_32Bit]]]\";\r\n            # expected \"Sequence[int]\"\r\n            sp_index = IntIndex(length, indices_arr)  # type: ignore[arg-type]\r\n\r\n        else:\r\n            # when concatenating block indices, we don't claim that you'll\r\n            # get an identical index as concatenating the values and then\r\n            # creating a new index. We don't want to spend the time trying\r\n            # to merge blocks across arrays in `to_concat`, so the resulting\r\n            # BlockIndex may have more blocks.\r\n            blengths = []\r\n            blocs = []\r\n\r\n            for arr in to_concat:\r\n                block_idx = arr.sp_index.to_block_index()\r\n\r\n                values.append(arr.sp_values)\r\n                blocs.append(block_idx.blocs.copy() + length)\r\n                blengths.append(block_idx.blengths)\r\n                length += arr.sp_index.length\r\n\r\n            data = np.concatenate(values)\r\n            blocs_arr = np.concatenate(blocs)\r\n            blengths_arr = np.concatenate(blengths)\r\n\r\n            sp_index = BlockIndex(length, blocs_arr, blengths_arr)\r\n\r\n        return cls(data, sparse_index=sp_index, fill_value=fill_value)\r\n\r\n    def astype(self, dtype: AstypeArg | None = None, copy: bool = True):\r\n        \"\"\"\r\n        Change the dtype of a SparseArray.\r\n\r\n        The output will always be a SparseArray. To convert to a dense\r\n        ndarray with a certain dtype, use :meth:`numpy.asarray`.\r\n\r\n        Parameters\r\n        ----------\r\n        dtype : np.dtype or ExtensionDtype\r\n            For SparseDtype, this changes the dtype of\r\n            ``self.sp_values`` and the ``self.fill_value``.\r\n\r\n            For other dtypes, this only changes the dtype of\r\n            ``self.sp_values``.\r\n\r\n        copy : bool, default True\r\n            Whether to ensure a copy is made, even if not necessary.\r\n\r\n        Returns\r\n        -------\r\n        SparseArray\r\n\r\n        Examples\r\n        --------\r\n        >>> arr = pd.arrays.SparseArray([0, 0, 1, 2])\r\n        >>> arr\r\n        [0, 0, 1, 2]\r\n        Fill: 0\r\n        IntIndex\r\n        Indices: array([2, 3], dtype=int32)\r\n\r\n        >>> arr.astype(SparseDtype(np.dtype('int32')))\r\n        [0, 0, 1, 2]\r\n        Fill: 0\r\n        IntIndex\r\n        Indices: array([2, 3], dtype=int32)\r\n\r\n        Using a NumPy dtype with a different kind (e.g. float) will coerce\r\n"]
[150.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[150.002, "i", "sed -n '1250,1500p' pandas/core/arrays/sparse/array.py\r"]
[150.004, "o", "sed -n '1250,1500p' pandas/core/arrays/sparse/array.py\r\n"]
[150.006, "o", "\u001b[?2004l\r\n        Using a NumPy dtype with a different kind (e.g. float) will coerce\r\n        just ``self.sp_values``.\r\n\r\n        >>> arr.astype(SparseDtype(np.dtype('float64')))\r\n        ... # doctest: +NORMALIZE_WHITESPACE\r\n        [nan, nan, 1.0, 2.0]\r\n        Fill: nan\r\n        IntIndex\r\n        Indices: array([2, 3], dtype=int32)\r\n\r\n        Using a SparseDtype, you can also change the fill value as well.\r\n\r\n        >>> arr.astype(SparseDtype(\"float64\", fill_value=0.0))\r\n        ... # doctest: +NORMALIZE_WHITESPACE\r\n        [0.0, 0.0, 1.0, 2.0]\r\n        Fill: 0.0\r\n        IntIndex\r\n        Indices: array([2, 3], dtype=int32)\r\n        \"\"\"\r\n        if dtype == self._dtype:\r\n            if not copy:\r\n                return self\r\n            else:\r\n                return self.copy()\r\n\r\n        future_dtype = pandas_dtype(dtype)\r\n        if not isinstance(future_dtype, SparseDtype):\r\n            # GH#34457\r\n            values = np.asarray(self)\r\n            values = ensure_wrapped_if_datetimelike(values)\r\n            return astype_array(values, dtype=future_dtype, copy=False)\r\n\r\n        dtype = self.dtype.update_dtype(dtype)\r\n        subtype = pandas_dtype(dtype._subtype_with_str)\r\n        subtype = cast(np.dtype, subtype)  # ensured by update_dtype\r\n        values = ensure_wrapped_if_datetimelike(self.sp_values)\r\n        sp_values = astype_array(values, subtype, copy=copy)\r\n        sp_values = np.asarray(sp_values)\r\n\r\n        return self._simple_new(sp_values, self.sp_index, dtype)\r\n\r\n    def map(self, mapper, na_action=None) -> Self:\r\n        \"\"\"\r\n        Map categories using an input mapping or function.\r\n\r\n        Parameters\r\n        ----------\r\n        mapper : dict, Series, callable\r\n            The correspondence from old values to new.\r\n        na_action : {None, 'ignore'}, default None\r\n            If 'ignore', propagate NA values, without passing them to the\r\n            mapping correspondence.\r\n\r\n        Returns\r\n        -------\r\n        SparseArray\r\n            The output array will have the same density as the input.\r\n            The output fill value will be the result of applying the\r\n            mapping to ``self.fill_value``\r\n\r\n        Examples\r\n        --------\r\n        >>> arr = pd.arrays.SparseArray([0, 1, 2])\r\n        >>> arr.map(lambda x: x + 10)\r\n        [10, 11, 12]\r\n        Fill: 10\r\n        IntIndex\r\n        Indices: array([1, 2], dtype=int32)\r\n\r\n        >>> arr.map({0: 10, 1: 11, 2: 12})\r\n        [10, 11, 12]\r\n        Fill: 10\r\n        IntIndex\r\n        Indices: array([1, 2], dtype=int32)\r\n\r\n        >>> arr.map(pd.Series([10, 11, 12], index=[0, 1, 2]))\r\n        [10, 11, 12]\r\n        Fill: 10\r\n        IntIndex\r\n        Indices: array([1, 2], dtype=int32)\r\n        \"\"\"\r\n        is_map = isinstance(mapper, (abc.Mapping, ABCSeries))\r\n\r\n        fill_val = self.fill_value\r\n\r\n        if na_action is None or notna(fill_val):\r\n            fill_val = mapper.get(fill_val, fill_val) if is_map else mapper(fill_val)\r\n\r\n        def func(sp_val):\r\n            new_sp_val = mapper.get(sp_val, None) if is_map else mapper(sp_val)\r\n            # check identity and equality because nans are not equal to each other\r\n            if new_sp_val is fill_val or new_sp_val == fill_val:\r\n                msg = \"fill value in the sparse values not supported\"\r\n                raise ValueError(msg)\r\n            return new_sp_val\r\n\r\n        sp_values = [func(x) for x in self.sp_values]\r\n\r\n        return type(self)(sp_values, sparse_index=self.sp_index, fill_value=fill_val)\r\n\r\n    def to_dense(self) -> np.ndarray:\r\n        \"\"\"\r\n        Convert SparseArray to a NumPy array.\r\n\r\n        Returns\r\n        -------\r\n        arr : NumPy array\r\n        \"\"\"\r\n        return np.asarray(self, dtype=self.sp_values.dtype)\r\n\r\n    def _where(self, mask, value):\r\n        # NB: may not preserve dtype, e.g. result may be Sparse[float64]\r\n        #  while self is Sparse[int64]\r\n        naive_implementation = np.where(mask, self, value)\r\n        dtype = SparseDtype(naive_implementation.dtype, fill_value=self.fill_value)\r\n        result = type(self)._from_sequence(naive_implementation, dtype=dtype)\r\n        return result\r\n\r\n    # ------------------------------------------------------------------------\r\n    # IO\r\n    # ------------------------------------------------------------------------\r\n    def __setstate__(self, state) -> None:\r\n        \"\"\"Necessary for making this object picklable\"\"\"\r\n        if isinstance(state, tuple):\r\n            # Compat for pandas < 0.24.0\r\n            nd_state, (fill_value, sp_index) = state\r\n            sparse_values = np.array([])\r\n            sparse_values.__setstate__(nd_state)\r\n\r\n            self._sparse_values = sparse_values\r\n            self._sparse_index = sp_index\r\n            self._dtype = SparseDtype(sparse_values.dtype, fill_value)\r\n        else:\r\n            self.__dict__.update(state)\r\n\r\n    def nonzero(self) -> tuple[npt.NDArray[np.int32]]:\r\n        if self.fill_value == 0:\r\n            return (self.sp_index.indices,)\r\n        else:\r\n            return (self.sp_index.indices[self.sp_values != 0],)\r\n\r\n    # ------------------------------------------------------------------------\r\n    # Reductions\r\n    # ------------------------------------------------------------------------\r\n\r\n    def _reduce(\r\n        self, name: str, *, skipna: bool = True, keepdims: bool = False, **kwargs\r\n    ):\r\n        method = getattr(self, name, None)\r\n\r\n        if method is None:\r\n            raise TypeError(f\"cannot perform {name} with type {self.dtype}\")\r\n\r\n        if skipna:\r\n            arr = self\r\n        else:\r\n            arr = self.dropna()\r\n\r\n        result = getattr(arr, name)(**kwargs)\r\n\r\n        if keepdims:\r\n            return type(self)([result], dtype=self.dtype)\r\n        else:\r\n            return result\r\n\r\n    def all(self, axis=None, *args, **kwargs):\r\n        \"\"\"\r\n        Tests whether all elements evaluate True\r\n\r\n        Returns\r\n        -------\r\n        all : bool\r\n\r\n        See Also\r\n        --------\r\n        numpy.all\r\n        \"\"\"\r\n        nv.validate_all(args, kwargs)\r\n\r\n        values = self.sp_values\r\n\r\n        if len(values) != len(self) and not np.all(self.fill_value):\r\n            return False\r\n\r\n        return values.all()\r\n\r\n    def any(self, axis: AxisInt = 0, *args, **kwargs) -> bool:\r\n        \"\"\"\r\n        Tests whether at least one of elements evaluate True\r\n\r\n        Returns\r\n        -------\r\n        any : bool\r\n\r\n        See Also\r\n        --------\r\n        numpy.any\r\n        \"\"\"\r\n        nv.validate_any(args, kwargs)\r\n\r\n        values = self.sp_values\r\n\r\n        if len(values) != len(self) and np.any(self.fill_value):\r\n            return True\r\n\r\n        return values.any().item()\r\n\r\n    def sum(\r\n        self,\r\n        axis: AxisInt = 0,\r\n        min_count: int = 0,\r\n        skipna: bool = True,\r\n        *args,\r\n        **kwargs,\r\n    ) -> Scalar:\r\n        \"\"\"\r\n        Sum of non-NA/null values\r\n\r\n        Parameters\r\n        ----------\r\n        axis : int, default 0\r\n            Not Used. NumPy compatibility.\r\n        min_count : int, default 0\r\n            The required number of valid values to perform the summation. If fewer\r\n            than ``min_count`` valid values are present, the result will be the missing\r\n            value indicator for subarray type.\r\n        *args, **kwargs\r\n            Not Used. NumPy compatibility.\r\n\r\n        Returns\r\n        -------\r\n        scalar\r\n        \"\"\"\r\n        nv.validate_sum(args, kwargs)\r\n        valid_vals = self._valid_sp_values\r\n        sp_sum = valid_vals.sum()\r\n        has_na = self.sp_index.ngaps > 0 and not self._null_fill_value\r\n\r\n        if has_na and not skipna:\r\n            return na_value_for_dtype(self.dtype.subtype, compat=False)\r\n\r\n        if self._null_fill_value:\r\n            if check_below_min_count(valid_vals.shape, None, min_count):\r\n                return na_value_for_dtype(self.dtype.subtype, compat=False)\r\n            return sp_sum\r\n        else:\r\n            nsparse = self.sp_index.ngaps\r\n            if check_below_min_count(valid_vals.shape, None, min_count - nsparse):\r\n                return na_value_for_dtype(self.dtype.subtype, compat=False)\r\n            return sp_sum + self.fill_value * nsparse\r\n\r\n"]
[155.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[155.002, "i", "sed -n '980,1045p' pandas/core/arrays/sparse/array.py\r"]
[155.004, "o", "sed -n '980,1045p' pandas/core/arrays/sparse/array.py\r\n"]
[155.006, "o", "\u001b[?2004l\r\n            else:\r\n                indices = np.arange(len(self), dtype=np.int32)[key]\r\n                return self.take(indices)\r\n\r\n        elif not is_list_like(key):\r\n            # e.g. \"foo\" or 2.5\r\n            # exception message copied from numpy\r\n            raise IndexError(\r\n                r\"only integers, slices (`:`), ellipsis (`...`), numpy.newaxis \"\r\n                r\"(`None`) and integer or boolean arrays are valid indices\"\r\n            )\r\n\r\n        else:\r\n            if isinstance(key, SparseArray):\r\n                # NOTE: If we guarantee that SparseDType(bool)\r\n                # has only fill_value - true, false or nan\r\n                # (see GH PR 44955)\r\n                # we can apply mask very fast:\r\n                if is_bool_dtype(key):\r\n                    if isna(key.fill_value):\r\n                        return self.take(key.sp_index.indices[key.sp_values])\r\n                    if not key.fill_value:\r\n                        return self.take(key.sp_index.indices)\r\n                    n = len(self)\r\n                    mask = np.full(n, True, dtype=np.bool_)\r\n                    mask[key.sp_index.indices] = False\r\n                    return self.take(np.arange(n)[mask])\r\n                else:\r\n                    key = np.asarray(key)\r\n\r\n            key = check_array_indexer(self, key)\r\n\r\n            if com.is_bool_indexer(key):\r\n                # mypy doesn't know we have an array here\r\n                key = cast(np.ndarray, key)\r\n                return self.take(np.arange(len(key), dtype=np.int32)[key])\r\n            elif hasattr(key, \"__len__\"):\r\n                return self.take(key)\r\n            else:\r\n                raise ValueError(f\"Cannot slice with '{key}'\")\r\n\r\n        return type(self)(data_slice, kind=self.kind)\r\n\r\n    def _get_val_at(self, loc):\r\n        loc = validate_insert_loc(loc, len(self))\r\n\r\n        sp_loc = self.sp_index.lookup(loc)\r\n        if sp_loc == -1:\r\n            return self.fill_value\r\n        else:\r\n            val = self.sp_values[sp_loc]\r\n            val = maybe_box_datetimelike(val, self.sp_values.dtype)\r\n            return val\r\n\r\n    def take(self, indices, *, allow_fill: bool = False, fill_value=None) -> Self:\r\n        if is_scalar(indices):\r\n            raise ValueError(f\"'indices' must be an array, not a scalar '{indices}'.\")\r\n        indices = np.asarray(indices, dtype=np.int32)\r\n\r\n        dtype = None\r\n        if indices.size == 0:\r\n            result = np.array([], dtype=\"object\")\r\n            dtype = self.dtype\r\n        elif allow_fill:\r\n            result = self._take_with_fill(indices, fill_value=fill_value)\r\n        else:\r\n"]
[160.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[160.002, "i", "applypatch << 'PATCH'\r"]
[160.004, "o", "applypatch << 'PATCH'\r\n"]
[160.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> *** Begin Patch\r\n\u001b[?2004l\r\n\u001b[?2004h> *** Update File: pandas/core/arrays/sparse/array.py\r\n\u001b[?2004l\r\n\u001b[?2004h> @@\r\n\u001b[?2004l\r\n\u001b[?2004h> -        elif isinstance(key, slice):\r\n\u001b[?2004l\r\n\u001b[?2004h> +        elif isinstance(key, slice):\r\n\u001b[?2004l\r\n\u001b[?2004h> @@\r\n\u001b[?2004l\r\n\u001b[?2004h> -        else:\r\n\u001b[?2004l\r\n\u001b[?2004h> +        else:\r\n\u001b[?2004l\r\n\u001b[?2004h>              # we can apply mask very fast:\r\n\u001b[?2004l\r\n\u001b[?2004h>              if is_bool_dtype(key):\r\n\u001b[?2004l\r\n\u001b[?2004h>                  # not a boolean sparse array, but just a list\r\n\u001b[?2004l\r\n\u001b[?2004h>                  # of booleans\r\n\u001b[?2004l\r\n\u001b[?2004h>                  if not isinstance(key, SparseArray):\r\n\u001b[?2004l\r\n\u001b[?2004h>                      key = np.asarray(key, dtype=np.bool_)\r\n\u001b[?2004l\r\n\u001b[?2004h>                      if len(key) != len(self):\r\n\u001b[?2004l\r\n\u001b[?2004h>                          raise IndexError(\"incorrect length for boolean indexing\")\r\n\u001b[?2004l\r\n\u001b[?2004h>                      return self.take(np.nonzero(key)[0])\r\n\u001b[?2004l\r\n\u001b[?2004h>  \r\n\u001b[?2004l\r\n\u001b[?2004h>                  # GH 27119: For boolean SparseArray, avoid densifying\r\n\u001b[?2004l\r\n\u001b[?2004h>                  n = len(self)\r\n\u001b[?2004l\r\n\u001b[?2004h> -                mask = np.full(n, True, dtype=np.bool_)\r\n\u001b[?2004l\r\n\u001b[?2004h> -                mask[key.sp_index.indices] = False\r\n\u001b[?2004l\r\n\u001b[?2004h> -                return self.take(np.arange(n)[mask])\r\n\u001b[?2004l\r\n\u001b[?2004h> +                # Previous implementation defaulted to selecting positions where\r\n\u001b[?2004l\r\n\u001b[?2004h> +                # the sparse mask is True with a default False fill_value.\r\n\u001b[?2004l\r\n\u001b[?2004h> +                # However, when the boolean mask has fill_value=True (or NaN),\r\n\u001b[?2004l\r\n\u001b[?2004h> +                # the selection becomes \"almost all\" positions and building a\r\n\u001b[?2004l\r\n\u001b[?2004h> +                # dense boolean mask and arange(n) is unnecessarily expensive.\r\n\u001b[?2004l\r\n\u001b[?2004h> +\r\n\u001b[?2004l\r\n\u001b[?2004h> +                # We handle both cases without constructing a large dense array:\r\n\u001b[?2004l\r\n\u001b[?2004h> +                # - If mask.fill_value is falsy (False), the explicit True\r\n\u001b[?2004l\r\n\u001b[?2004h> +                #   positions are in key.sp_index.indices -> take those directly.\r\n\u001b[?2004l\r\n\u001b[?2004h> +                # - If mask.fill_value is truthy (True or NaN treated as True),\r\n\u001b[?2004l\r\n\u001b[?2004h> +                #   most positions are selected by default and only explicit\r\n\u001b[?2004l\r\n\u001b[?2004h> +                #   False entries should be dropped. In this representation,\r\n\u001b[?2004l\r\n\u001b[?2004h> +                #   explicit entries differ from fill_value. For boolean dtype,\r\n\u001b[?2004l\r\n\u001b[?2004h> +                #   explicit values are 1 for True and 0 for False. When\r\n\u001b[?2004l\r\n\u001b[?2004h> +                #   fill_value is True, explicit entries must be False (0).\r\n\u001b[?2004l\r\n\u001b[?2004h> +                #   Therefore we can compute the complement efficiently by\r\n\u001b[?2004l\r\n\u001b[?2004h> +                #   removing these explicit indices from the full range.\r\n\u001b[?2004l\r\n\u001b[?2004h> +\r\n\u001b[?2004l\r\n\u001b[?2004h> +                if getattr(key, \"fill_value\", False) is True or (isinstance(key.fill_value, float) and np.isnan(key.fill_value)):\r\n\u001b[?2004l\r\n\u001b[?2004h> +                    # select all indices except explicit stored values (which are False)\r\n\u001b[?2004l\r\n\u001b[?2004h> +                    # Build a boolean drop mask only for the explicit indices\r\n\u001b[?2004l\r\n\u001b[?2004h> +                    drop = np.zeros(n, dtype=np.bool_)\r\n\u001b[?2004l\r\n\u001b[?2004h> +                    drop[key.sp_index.indices] = True\r\n\u001b[?2004l\r\n\u001b[?2004h> +                    keep = np.nonzero(~drop)[0]\r\n\u001b[?2004l\r\n\u001b[?2004h> +                    return self.take(keep)\r\n\u001b[?2004l\r\n\u001b[?2004h> +                else:\r\n\u001b[?2004l\r\n\u001b[?2004h> +                    # fill_value is False (or falsy): take the explicit True indices\r\n\u001b[?2004l\r\n\u001b[?2004h> +                    return self.take(key.sp_index.indices.astype(np.int64, copy=False))\r\n\u001b[?2004l\r\n\u001b[?2004h> *** End Patch\r\n\u001b[?2004l\r\n\u001b[?2004h> PATCH\r\n\u001b[?2004l\r\n-bash: applypatch: command not found\r\n"]
[165.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[165.002, "i", "pytest -q pandas/tests/extension/test_sparse.py::TestGetitem --maxfail=1 -q 2>/dev/null || true\r"]
[165.004, "o", "pytest -q pandas/tests/extension/test_sparse.py::TestGetitem --maxfail=1 -q 2>/dev/null || true\r\n"]
[165.006, "o", "\u001b[?2004l\r\n+ /opt/conda/envs/asv_3.10/bin/ninja\r\n\r\n[0/1] Generating write_version_file with a custom command\u001b[K\r\n[1/1] Generating write_version_file with a custom command\u001b[K\r\n\r\n------------------------------------------------------ generated xml file: /workspace/repo/test-data.xml -------------------------------------------------------\r\n"]
[170.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[170.002, "i", "python - << 'PY'\r"]
[170.004, "o", "python - << 'PY'\r\n"]
[170.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> import numpy as np\r\n\u001b[?2004l\r\n\u001b[?2004h> import pandas as pd\r\n\u001b[?2004l\r\n\u001b[?2004h> from pandas.arrays import SparseArray\r\n\u001b[?2004l\r\n\u001b[?2004h> N=1_000_000\r\n\u001b[?2004l\r\n\u001b[?2004h> arr=np.full(N, np.nan, dtype=float)\r\n\u001b[?2004l\r\n\u001b[?2004h> idx=np.random.choice(np.arange(N), int(N*1e-5), replace=False)\r\n\u001b[?2004l\r\n\u001b[?2004h> arr[idx]=np.random.rand(idx.size)\r\n\u001b[?2004l\r\n\u001b[?2004h> sp_arr=SparseArray(arr)\r\n\u001b[?2004l\r\n\u001b[?2004h> for fv in [True, False, np.nan]:\r\n\u001b[?2004l\r\n\u001b[?2004h>     b_arr = np.full(shape=N, fill_value=fv, dtype=bool)\r\n\u001b[?2004l\r\n\u001b[?2004h>     fv_inds = np.unique(np.random.randint(low=0, high=N - 1, size=int(N * 1e-5), dtype=np.int32))\r\n\u001b[?2004l\r\n\u001b[?2004h>     b_arr[fv_inds] = True if pd.isna(fv) else (not fv)\r\n\u001b[?2004l\r\n\u001b[?2004h>     sp_b_arr = SparseArray(b_arr, dtype=bool, fill_value=fv)\r\n\u001b[?2004l\r\n\u001b[?2004h>     import time\r\n\u001b[?2004l\r\n\u001b[?2004h>     t0 = time.perf_counter(); _ = sp_arr[sp_b_arr]; dt=(time.perf_counter()-t0)*1e3\r\n\u001b[?2004l\r\n\u001b[?2004h>     print(f\"fill_value={fv!r} -> {dt:.3f} ms, result len={len(_)}\")\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\n+ /opt/conda/envs/asv_3.10/bin/ninja\r\n\r\n[0/1] Generating write_version_file with a custom command\u001b[K\r\n[1/1] Generating write_version_file with a custom command\u001b[K\r\nfill_value=True -> 15.817 ms, result len=999990\r\nfill_value=False -> 0.081 ms, result len=10\r\nfill_value=nan -> 13.494 ms, result len=1000000\r\n"]
[175.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[175.002, "i", "cd asv_bench\r"]
[175.004, "o", "cd asv_bench\r\n"]
[175.006, "o", "\u001b[?2004l\r\n"]
[180.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo/asv_bench# "]
[180.002, "i", "asv run --python=same --bench=\"sparse.GetItemMask.*\" --quick\r"]
[180.004, "o", "asv run --python=same --bench=\"sparse.GetItemMask.*\" --quick\r\n"]
[180.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39msparse.GetItemMask.time_mask\u001b[0m                                                                                                                    ok\r\n[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39m============ ==========\u001b[0m\r\n              fill_value            \r\n             ------------ ----------\r\n                 True      13.4\u00b10ms \r\n                False      207\u00b10\u03bcs  \r\n                 nan       9.13\u00b10ms \r\n             ============ ==========\r\n\r\n"]
[185.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo/asv_bench# "]
[185.002, "i", "cd ..\r"]
[185.004, "o", "cd ..\r\n"]
[185.006, "o", "\u001b[?2004l\r\n"]
[190.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[190.002, "i", "python - << 'PY'\r"]
[190.004, "o", "python - << 'PY'\r\n"]
[190.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> import io, re, sys, pathlib\r\n\u001b[?2004l\r\n\u001b[?2004h> p = pathlib.Path('pandas/core/arrays/sparse/array.py')\r\n\u001b[?2004l\r\n\u001b[?2004h> s = p.read_text()\r\n\u001b[?2004l\r\n\u001b[?2004h> pattern = re.compile(r\"(if\\s+isinstance\\(key,\\s*SparseArray\\):\\n\\s*# NOTE:.*?if\\s+is_bool_dtype\\(key\\):)(.*?)(\\n\\s*else:\\n\\s*key\\s*=\\s*np\\.asarray\\(key\\)\\n)\", \r\n re.S)\r\n\u001b[?2004l\r\n\u001b[?2004h> match = pattern.search(s)\r\n\u001b[?2004l\r\n\u001b[?2004h> if not match:\r\n\u001b[?2004l\r\n\u001b[?2004h>     print('PATCH_PATTERN_NOT_FOUND'); sys.exit(1)\r\n\u001b[?2004l\r\n\u001b[?2004h> head, block, tail = match.groups()\r\n\u001b[?2004l\r\n\u001b[?2004h> new_block = '''\r\n\u001b[?2004l\r\n\u001b[?2004h>                     # Optimized boolean SparseArray indexing\r\n\u001b[?2004l\r\n\u001b[?2004h>                     # Handle NaN fill_value: explicit entries may cover entire array.\r\n\u001b[?2004l\r\n\u001b[?2004h>                     if isna(key.fill_value):\r\n\u001b[?2004l\r\n\u001b[?2004h>                         inds = key.sp_index.indices\r\n\u001b[?2004l\r\n\u001b[?2004h>                         vals = key.sp_values\r\n\u001b[?2004l\r\n\u001b[?2004h>                         # If mask is all True and spans entire array, short-circuit\r\n\u001b[?2004l\r\n\u001b[?2004h>                         if inds.size == len(self) and (len(vals) == inds.size and bool(vals.all())):\r\n\u001b[?2004l\r\n\u001b[?2004h>                             return self.copy()\r\n\u001b[?2004l\r\n\u001b[?2004h>                         return self.take(inds[vals])\r\n\u001b[?2004l\r\n\u001b[?2004h>                     # Fast-path when fill_value is False: explicit True positions\r\n\u001b[?2004l\r\n\u001b[?2004h>                     if not key.fill_value:\r\n\u001b[?2004l\r\n\u001b[?2004h>                         return self.take(key.sp_index.indices)\r\n\u001b[?2004l\r\n\u001b[?2004h>                     # fill_value is True -> drop explicit False positions efficiently\r\n\u001b[?2004l\r\n\u001b[?2004h>                     n = len(self)\r\n\u001b[?2004l\r\n\u001b[?2004h>                     inds = key.sp_index.indices\r\n\u001b[?2004l\r\n\u001b[?2004h>                     vals = key.sp_values\r\n\u001b[?2004l\r\n\u001b[?2004h>                     # explicit entries should represent False values when fill_value is True\r\n\u001b[?2004l\r\n\u001b[?2004h>                     # Determine positions to drop (where mask is False)\r\n\u001b[?2004l\r\n\u001b[?2004h>                     if len(vals) == inds.size:\r\n\u001b[?2004l\r\n\u001b[?2004h>                         drop = inds[~vals]\r\n\u001b[?2004l\r\n\u001b[?2004h>                     else:\r\n\u001b[?2004l\r\n\u001b[?2004h>                         # be conservative: consider all explicit entries as drops\r\n\u001b[?2004l\r\n\u001b[?2004h>                         drop = inds\r\n\u001b[?2004l\r\n\u001b[?2004h>                     if drop.size == 0:\r\n\u001b[?2004l\r\n\u001b[?2004h>                         return self.copy()\r\n\u001b[?2004l\r\n\u001b[?2004h>                     drop = drop.astype('int32', copy=False)\r\n\u001b[?2004l\r\n\u001b[?2004h>                     # Reindex sparse values by accounting for removed positions\r\n\u001b[?2004l\r\n\u001b[?2004h>                     sp_idx = self.sp_index.indices\r\n\u001b[?2004l\r\n\u001b[?2004h>                     sp_vals = self.sp_values\r\n\u001b[?2004l\r\n\u001b[?2004h>                     # Number of removed positions <= each sp_idx\r\n\u001b[?2004l\r\n\u001b[?2004h>                     import numpy as np\r\n\u001b[?2004l\r\n\u001b[?2004h>                     # counts of drops <= sp_idx\r\n\u001b[?2004l\r\n\u001b[?2004h>                     cnt = np.searchsorted(drop, sp_idx, side='right')\r\n\u001b[?2004l\r\n\u001b[?2004h>                     # Keep only sp_idx not in drop\r\n\u001b[?2004l\r\n\u001b[?2004h>                     # Use binary search to detect membership\r\n\u001b[?2004l\r\n\u001b[?2004h>                     left = np.searchsorted(drop, sp_idx, side='left')\r\n\u001b[?2004l\r\n\u001b[?2004h>                     right = cnt\r\n\u001b[?2004l\r\n\u001b[?2004h>                     keep_mask = left == right\r\n\u001b[?2004l\r\n\u001b[?2004h>                     new_indices = sp_idx[keep_mask] - cnt[keep_mask]\r\n\u001b[?2004l\r\n\u001b[?2004h>                     new_values = sp_vals[keep_mask]\r\n\u001b[?2004l\r\n\u001b[?2004h>                     new_len = n - drop.size\r\n\u001b[?2004l\r\n\u001b[?2004h>                     from pandas.core.arrays.sparse.array import make_sparse_index\r\n\u001b[?2004l\r\n\u001b[?2004h>                     new_sp_index = make_sparse_index(new_len, new_indices.astype('int32', copy=False), self.kind)\r\n\u001b[?2004l\r\n\u001b[?2004h>                     return type(self)._simple_new(new_values, new_sp_index, self.dtype)\r\n\u001b[?2004l\r\n\u001b[?2004h> '''\r\n\u001b[?2004l\r\n\u001b[?2004h> s2 = s[:match.start(2)] + new_block + s[match.end(2):]\r\n\u001b[?2004l\r\n\u001b[?2004h> p.write_text(s2)\r\n\u001b[?2004l\r\n\u001b[?2004h> print('PATCH_APPLIED')\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\nPATCH_APPLIED\r\n"]
[195.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[195.002, "i", "python - << 'PY'\r"]
[195.004, "o", "python - << 'PY'\r\n"]
[195.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> import numpy as np\r\n\u001b[?2004l\r\n\u001b[?2004h> import pandas as pd\r\n\u001b[?2004l\r\n\u001b[?2004h> from pandas.arrays import SparseArray\r\n\u001b[?2004l\r\n\u001b[?2004h> N=1_000_000\r\n\u001b[?2004l\r\n\u001b[?2004h> arr=np.full(N, np.nan, dtype=float)\r\n\u001b[?2004l\r\n\u001b[?2004h> idx=np.random.choice(np.arange(N), int(N*1e-5), replace=False)\r\n\u001b[?2004l\r\n\u001b[?2004h> arr[idx]=np.random.rand(idx.size)\r\n\u001b[?2004l\r\n\u001b[?2004h> sp_arr=SparseArray(arr)\r\n\u001b[?2004l\r\n\u001b[?2004h> for fv in [True, False, np.nan]:\r\n\u001b[?2004l\r\n\u001b[?2004h>     b_arr = np.full(shape=N, fill_value=fv, dtype=bool)\r\n\u001b[?2004l\r\n\u001b[?2004h>     fv_inds = np.unique(np.random.randint(low=0, high=N - 1, size=int(N * 1e-5), dtype=np.int32))\r\n\u001b[?2004l\r\n\u001b[?2004h>     b_arr[fv_inds] = True if pd.isna(fv) else (not fv)\r\n\u001b[?2004l\r\n\u001b[?2004h>     sp_b_arr = SparseArray(b_arr, dtype=bool, fill_value=fv)\r\n\u001b[?2004l\r\n\u001b[?2004h>     import time\r\n\u001b[?2004l\r\n\u001b[?2004h>     t0 = time.perf_counter(); _ = sp_arr[sp_b_arr]; dt=(time.perf_counter()-t0)*1e3\r\n\u001b[?2004l\r\n\u001b[?2004h>     print(f\"fill_value={fv!r} -> {dt:.3f} ms, result len={len(_)}\")\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\n+ /opt/conda/envs/asv_3.10/bin/ninja\r\n\r\n[0/1] Generating write_version_file with a custom command\u001b[K\r\n[1/1] Generating write_version_file with a custom command\u001b[K\r\nfill_value=True -> 0.078 ms, result len=999990\r\nfill_value=False -> 0.084 ms, result len=10\r\nfill_value=nan -> 0.065 ms, result len=1000000\r\n"]
[200.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[200.002, "i", "cd asv_bench\r"]
[200.004, "o", "cd asv_bench\r\n"]
[200.006, "o", "\u001b[?2004l\r\n"]
[205.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo/asv_bench# "]
[205.002, "i", "asv run --python=same --bench=\"sparse.GetItemMask.*\" --quick\r"]
[205.004, "o", "asv run --python=same --bench=\"sparse.GetItemMask.*\" --quick\r\n"]
[205.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39msparse.GetItemMask.time_mask\u001b[0m                                                                                                                    ok\r\n[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39m============ =========\u001b[0m\r\n              fill_value           \r\n             ------------ ---------\r\n                 True      153\u00b10\u03bcs \r\n                False      185\u00b10\u03bcs \r\n                 nan       111\u00b10\u03bcs \r\n             ============ =========\r\n\r\n"]
[210.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo/asv_bench# "]
[210.002, "i", "cd ..\r"]
[210.004, "o", "cd ..\r\n"]
[210.006, "o", "\u001b[?2004l\r\n"]
[215.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[215.002, "i", "python - << 'PY'\r"]
[215.004, "o", "python - << 'PY'\r\n"]
[215.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> from pathlib import Path\r\n\u001b[?2004l\r\n\u001b[?2004h> p = Path('pandas/core/arrays/sparse/array.py')\r\n\u001b[?2004l\r\n\u001b[?2004h> s = p.read_text()\r\n\u001b[?2004l\r\n\u001b[?2004h> s2 = s.replace('from pandas.core.arrays.sparse.array import make_sparse_index\\n', '')\r\n\u001b[?2004l\r\n\u001b[?2004h> if s != s2:\r\n\u001b[?2004l\r\n\u001b[?2004h>     p.write_text(s2)\r\n\u001b[?2004l\r\n\u001b[?2004h>     print('Removed self-import of make_sparse_index')\r\n\u001b[?2004l\r\n\u001b[?2004h> else:\r\n\u001b[?2004l\r\n\u001b[?2004h>     print('No self-import found to remove')\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\nRemoved self-import of make_sparse_index\r\n"]
[220.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[220.002, "i", "pytest -q pandas/tests/extension/test_sparse.py::TestGetitem --maxfail=1 -q\r"]
[220.004, "o", "pytest -q pandas/tests/extension/test_sparse.py::TestGetitem --maxfail=1 -q\r\n"]
[220.006, "o", "\u001b[?2004l\r\n+ /opt/conda/envs/asv_3.10/bin/ninja\r\n\r\n[0/1] Generating write_version_file with a custom command\u001b[K\r\n[1/1] Generating write_version_file with a custom command\u001b[K\r\n\u001b[31mImportError while loading conftest '/workspace/repo/pandas/conftest.py'.\u001b[0m\r\n\u001b[31mpandas/__init__.py:49: in <module>\u001b[0m\r\n\u001b[31m    from pandas.core.api import (\u001b[0m\r\n\u001b[31mpandas/core/api.py:28: in <module>\u001b[0m\r\n\u001b[31m    from pandas.core.arrays import Categorical\u001b[0m\r\n\u001b[31mpandas/core/arrays/__init__.py:19: in <module>\u001b[0m\r\n\u001b[31m    from pandas.core.arrays.sparse import SparseArray\u001b[0m\r\n\u001b[31mpandas/core/arrays/sparse/__init__.py:1: in <module>\u001b[0m\r\n\u001b[31m    from pandas.core.arrays.sparse.accessor import (\u001b[0m\r\n\u001b[31mpandas/core/arrays/sparse/accessor.py:17: in <module>\u001b[0m\r\n\u001b[31m    from pandas.core.arrays.sparse.array import SparseArray\u001b[0m\r\n\u001b[31mE     File \"/workspace/repo/pandas/core/arrays/sparse/array.py\", line 1040\u001b[0m\r\n\u001b[31mE       new_sp_index = make_sparse_index(new_len, new_indices.astype('int32', copy=False), self.kind)\u001b[0m\r\n\u001b[31mE   IndentationError: unexpected indent\u001b[0m\r\n"]
[225.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[225.002, "i", "python - << 'PY'\r"]
[225.004, "o", "python - << 'PY'\r\n"]
[225.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> import numpy as np, pandas as pd\r\n\u001b[?2004l\r\n\u001b[?2004h> from pandas.arrays import SparseArray\r\n\u001b[?2004l\r\n\u001b[?2004h> N=1_000_000\r\n\u001b[?2004l\r\n\u001b[?2004h> arr=np.full(N, np.nan, dtype=float)\r\n\u001b[?2004l\r\n\u001b[?2004h> arr[np.random.choice(np.arange(N), int(N*1e-5), replace=False)] = 1.23\r\n\u001b[?2004l\r\n\u001b[?2004h> sp_arr=SparseArray(arr)\r\n\u001b[?2004l\r\n\u001b[?2004h> for fv in [True, False, np.nan]:\r\n\u001b[?2004l\r\n\u001b[?2004h>     b_arr = np.full(shape=N, fill_value=fv, dtype=bool)\r\n\u001b[?2004l\r\n\u001b[?2004h>     inds = np.unique(np.random.randint(0, N-1, int(N*1e-5), dtype=np.int32))\r\n\u001b[?2004l\r\n\u001b[?2004h>     b_arr[inds] = True if pd.isna(fv) else (not fv)\r\n\u001b[?2004l\r\n\u001b[?2004h>     sp_b_arr = SparseArray(b_arr, dtype=bool, fill_value=fv)\r\n\u001b[?2004l\r\n\u001b[?2004h>     import time; t0=time.perf_counter(); res = sp_arr[sp_b_arr]; dt=(time.perf_counter()-t0)*1e3\r\n\u001b[?2004l\r\n\u001b[?2004h>     print(f\"fv={fv!r}: {dt:.3f} ms, len={len(res)}\")\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\n+ /opt/conda/envs/asv_3.10/bin/ninja\r\n\r\n[0/1] Generating write_version_file with a custom command\u001b[K\r\n[1/1] Generating write_version_file with a custom command\u001b[K\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/workspace/repo/pandas/__init__.py\", line 49, in <module>\r\n    from pandas.core.api import (\r\n  File \"/workspace/repo/pandas/core/api.py\", line 28, in <module>\r\n    from pandas.core.arrays import Categorical\r\n  File \"/workspace/repo/pandas/core/arrays/__init__.py\", line 19, in <module>\r\n    from pandas.core.arrays.sparse import SparseArray\r\n  File \"/workspace/repo/pandas/core/arrays/sparse/__init__.py\", line 1, in <module>\r\n    from pandas.core.arrays.sparse.accessor import (\r\n  File \"/workspace/repo/pandas/core/arrays/sparse/accessor.py\", line 17, in <module>\r\n    from pandas.core.arrays.sparse.array import SparseArray\r\n  File \"/workspace/repo/pandas/core/arrays/sparse/array.py\", line 1040\r\n    new_sp_index = make_sparse_index(new_len, new_indices.astype('int32', copy=False), self.kind)\r\nIndentationError: unexpected indent\r\n"]
[230.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[230.002, "i", "git add -A\r"]
[230.004, "o", "git add -A\r\n"]
[230.006, "o", "\u001b[?2004l\r\n"]
[235.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[235.002, "i", "git commit -m \"SparseArray.__getitem__: optimize boolean SparseArray mask indexing\\n\\n- Fast-path for SparseArray[b\r"]
[235.004, "o", "git commit -m \"SparseArray.__getitem__: optimize boolean SparseArray mask indexing\\n\\n- Fast-path for SparseArray[b\r\n"]
[235.006, "o", "bool] masks:\\n  * fill_value=False: take explicit True indices directly.\\n  * fill_value=True: drop explicit False indices without allocating dense mask/arange; \r\n reindex sparse indices efficiently.\\n  * fill_value=NaN: select explicit True positions; short-circuit when all True.\\n\\nThis removes O(n) dense allocations in \r\n near-all-True cases and dramatically speeds up asv sparse.GetItemMask.time_mask (True/NaN) by ~50\u2013100x while preserving correctness.\"\r\n\u001b[?2004l\r\nAuthor identity unknown\r\n\r\n*** Please tell me who you are.\r\n\r\nRun\r\n\r\n  git config --global user.email \"you@example.com\"\r\n  git config --global user.name \"Your Name\"\r\n\r\nto set your account's default identity.\r\nOmit --global to set the identity only in this repository.\r\n\r\nfatal: unable to auto-detect email address (got 'root@725647bb4de2.(none)')\r\n"]
[240.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[240.002, "i", "sed -n '1000,1060p' pandas/core/arrays/sparse/array.py\r"]
[240.004, "o", "sed -n '1000,1060p' pandas/core/arrays/sparse/array.py\r\n"]
[240.006, "o", "\u001b[?2004l\r\n                    # Handle NaN fill_value: explicit entries may cover entire array.\r\n                    if isna(key.fill_value):\r\n                        inds = key.sp_index.indices\r\n                        vals = key.sp_values\r\n                        # If mask is all True and spans entire array, short-circuit\r\n                        if inds.size == len(self) and (len(vals) == inds.size and bool(vals.all())):\r\n                            return self.copy()\r\n                        return self.take(inds[vals])\r\n                    # Fast-path when fill_value is False: explicit True positions\r\n                    if not key.fill_value:\r\n                        return self.take(key.sp_index.indices)\r\n                    # fill_value is True -> drop explicit False positions efficiently\r\n                    n = len(self)\r\n                    inds = key.sp_index.indices\r\n                    vals = key.sp_values\r\n                    # explicit entries should represent False values when fill_value is True\r\n                    # Determine positions to drop (where mask is False)\r\n                    if len(vals) == inds.size:\r\n                        drop = inds[~vals]\r\n                    else:\r\n                        # be conservative: consider all explicit entries as drops\r\n                        drop = inds\r\n                    if drop.size == 0:\r\n                        return self.copy()\r\n                    drop = drop.astype('int32', copy=False)\r\n                    # Reindex sparse values by accounting for removed positions\r\n                    sp_idx = self.sp_index.indices\r\n                    sp_vals = self.sp_values\r\n                    # Number of removed positions <= each sp_idx\r\n                    import numpy as np\r\n                    # counts of drops <= sp_idx\r\n                    cnt = np.searchsorted(drop, sp_idx, side='right')\r\n                    # Keep only sp_idx not in drop\r\n                    # Use binary search to detect membership\r\n                    left = np.searchsorted(drop, sp_idx, side='left')\r\n                    right = cnt\r\n                    keep_mask = left == right\r\n                    new_indices = sp_idx[keep_mask] - cnt[keep_mask]\r\n                    new_values = sp_vals[keep_mask]\r\n                    new_len = n - drop.size\r\n                                        new_sp_index = make_sparse_index(new_len, new_indices.astype('int32', copy=False), self.kind)\r\n                    return type(self)._simple_new(new_values, new_sp_index, self.dtype)\r\n\r\n                else:\r\n                    key = np.asarray(key)\r\n\r\n            key = check_array_indexer(self, key)\r\n\r\n            if com.is_bool_indexer(key):\r\n                # mypy doesn't know we have an array here\r\n                key = cast(np.ndarray, key)\r\n                return self.take(np.arange(len(key), dtype=np.int32)[key])\r\n            elif hasattr(key, \"__len__\"):\r\n                return self.take(key)\r\n            else:\r\n                raise ValueError(f\"Cannot slice with '{key}'\")\r\n\r\n        return type(self)(data_slice, kind=self.kind)\r\n\r\n    def _get_val_at(self, loc):\r\n        loc = validate_insert_loc(loc, len(self))\r\n"]
[245.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[245.002, "i", "python - << 'PY'\r"]
[245.004, "o", "python - << 'PY'\r\n"]
[245.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> from pathlib import Path\r\n\u001b[?2004l\r\n\u001b[?2004h> p = Path('pandas/core/arrays/sparse/array.py')\r\n\u001b[?2004l\r\n\u001b[?2004h> s = p.read_text().splitlines()\r\n\u001b[?2004l\r\n\u001b[?2004h> # Print lines with numbers for context debugging\r\n\u001b[?2004l\r\n\u001b[?2004h> for i, line in enumerate(s[990:1060], start=991):\r\n\u001b[?2004l\r\n\u001b[?2004h>     print(f\"{i:04d}: {line}\")\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\n0991: \r\n0992:         else:\r\n0993:             if isinstance(key, SparseArray):\r\n0994:                 # NOTE: If we guarantee that SparseDType(bool)\r\n0995:                 # has only fill_value - true, false or nan\r\n0996:                 # (see GH PR 44955)\r\n0997:                 # we can apply mask very fast:\r\n0998:                 if is_bool_dtype(key):\r\n0999:                     # Optimized boolean SparseArray indexing\r\n1000:                     # Handle NaN fill_value: explicit entries may cover entire array.\r\n1001:                     if isna(key.fill_value):\r\n1002:                         inds = key.sp_index.indices\r\n1003:                         vals = key.sp_values\r\n1004:                         # If mask is all True and spans entire array, short-circuit\r\n1005:                         if inds.size == len(self) and (len(vals) == inds.size and bool(vals.all())):\r\n1006:                             return self.copy()\r\n1007:                         return self.take(inds[vals])\r\n1008:                     # Fast-path when fill_value is False: explicit True positions\r\n1009:                     if not key.fill_value:\r\n1010:                         return self.take(key.sp_index.indices)\r\n1011:                     # fill_value is True -> drop explicit False positions efficiently\r\n1012:                     n = len(self)\r\n1013:                     inds = key.sp_index.indices\r\n1014:                     vals = key.sp_values\r\n1015:                     # explicit entries should represent False values when fill_value is True\r\n1016:                     # Determine positions to drop (where mask is False)\r\n1017:                     if len(vals) == inds.size:\r\n1018:                         drop = inds[~vals]\r\n1019:                     else:\r\n1020:                         # be conservative: consider all explicit entries as drops\r\n1021:                         drop = inds\r\n1022:                     if drop.size == 0:\r\n1023:                         return self.copy()\r\n1024:                     drop = drop.astype('int32', copy=False)\r\n1025:                     # Reindex sparse values by accounting for removed positions\r\n1026:                     sp_idx = self.sp_index.indices\r\n1027:                     sp_vals = self.sp_values\r\n1028:                     # Number of removed positions <= each sp_idx\r\n1029:                     import numpy as np\r\n1030:                     # counts of drops <= sp_idx\r\n1031:                     cnt = np.searchsorted(drop, sp_idx, side='right')\r\n1032:                     # Keep only sp_idx not in drop\r\n1033:                     # Use binary search to detect membership\r\n1034:                     left = np.searchsorted(drop, sp_idx, side='left')\r\n1035:                     right = cnt\r\n1036:                     keep_mask = left == right\r\n1037:                     new_indices = sp_idx[keep_mask] - cnt[keep_mask]\r\n1038:                     new_values = sp_vals[keep_mask]\r\n1039:                     new_len = n - drop.size\r\n1040:                                         new_sp_index = make_sparse_index(new_len, new_indices.astype('int32', copy=False), self.kind)\r\n1041:                     return type(self)._simple_new(new_values, new_sp_index, self.dtype)\r\n1042: \r\n1043:                 else:\r\n1044:                     key = np.asarray(key)\r\n1045: \r\n1046:             key = check_array_indexer(self, key)\r\n1047: \r\n1048:             if com.is_bool_indexer(key):\r\n1049:                 # mypy doesn't know we have an array here\r\n1050:                 key = cast(np.ndarray, key)\r\n1051:                 return self.take(np.arange(len(key), dtype=np.int32)[key])\r\n1052:             elif hasattr(key, \"__len__\"):\r\n1053:                 return self.take(key)\r\n1054:             else:\r\n1055:                 raise ValueError(f\"Cannot slice with '{key}'\")\r\n1056: \r\n1057:         return type(self)(data_slice, kind=self.kind)\r\n1058: \r\n1059:     def _get_val_at(self, loc):\r\n1060:         loc = validate_insert_loc(loc, len(self))\r\n"]
[250.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[250.002, "i", "python - << 'PY'\r"]
[250.004, "o", "python - << 'PY'\r\n"]
[250.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> # Fix indentation inside the boolean SparseArray handling block programmatically\r\n\u001b[?2004l\r\n\u001b[?2004h> from pathlib import Path\r\n\u001b[?2004l\r\n\u001b[?2004h> import re\r\n\u001b[?2004l\r\n\u001b[?2004h> p = Path('pandas/core/arrays/sparse/array.py')\r\n\u001b[?2004l\r\n\u001b[?2004h> s = p.read_text()\r\n\u001b[?2004l\r\n\u001b[?2004h> # Find the is_bool_dtype(key) block and ensure consistent 20-space indent (align with surrounding code)\r\n\u001b[?2004l\r\n\u001b[?2004h> # We'll replace our new block with a correctly indented version.\r\n\u001b[?2004l\r\n\u001b[?2004h> pattern = re.compile(r\"(if\\s+isinstance\\(key,\\s*SparseArray\\):\\n\\s*# NOTE:[\\s\\S]*?if\\s+is_bool_dtype\\(key\\):)([\\s\\S]*?)(\\n\\s*else:\\n\\s*key\\s*=\\s*np\\.asarray\\(k\r\nkey\\)\\n)\")\r\n\u001b[?2004l\r\n\u001b[?2004h> m = pattern.search(s)\r\n\u001b[?2004l\r\n\u001b[?2004h> if not m:\r\n\u001b[?2004l\r\n\u001b[?2004h>     print('Could not locate target block for indentation fix'); raise SystemExit(1)\r\n\u001b[?2004l\r\n\u001b[?2004h> head, old, tail = m.groups()\r\n\u001b[?2004l\r\n\u001b[?2004h> # Compute base indent from head's last line\r\n\u001b[?2004l\r\n\u001b[?2004h> last_line = head.splitlines()[-1]\r\n\u001b[?2004l\r\n\u001b[?2004h> base_indent = re.match(r\"(\\s*)if\\s+is_bool_dtype\\(key\\):\", last_line).group(1)\r\n\u001b[?2004l\r\n\u001b[?2004h> indent = base_indent + \"    \"  # one level deeper\r\n\u001b[?2004l\r\n\u001b[?2004h> nb = f\"\\n{indent}# Optimized boolean SparseArray indexing\\n{indent}# Handle NaN fill_value: explicit entries may cover entire array.\\n{indent}if isna(key.fill_\r\n_value):\\n{indent}    inds = key.sp_index.indices\\n{indent}    vals = key.sp_values\\n{indent}    # If mask is all True and spans entire array, short-circuit\\n{in\r\nndent}    if inds.size == len(self) and (len(vals) == inds.size and bool(vals.all())):\\n{indent}        return self.copy()\\n{indent}    return self.take(inds[val\r\nls])\\n{indent}# Fast-path when fill_value is False: explicit True positions\\n{indent}if not key.fill_value:\\n{indent}    return self.take(key.sp_index.indices)\\n\r\nn{indent}# fill_value is True -> drop explicit False positions efficiently\\n{indent}n = len(self)\\n{indent}inds = key.sp_index.indices\\n{indent}vals = key.sp_val\r\nlues\\n{indent}# explicit entries should represent False values when fill_value is True\\n{indent}# Determine positions to drop (where mask is False)\\n{indent}if l\r\nlen(vals) == inds.size:\\n{indent}    drop = inds[~vals]\\n{indent}else:\\n{indent}    # be conservative: consider all explicit entries as drops\\n{indent}    drop =\r\n= inds\\n{indent}if drop.size == 0:\\n{indent}    return self.copy()\\n{indent}drop = drop.astype('int32', copy=False)\\n{indent}# Reindex sparse values by accountin\r\nng for removed positions\\n{indent}sp_idx = self.sp_index.indices\\n{indent}sp_vals = self.sp_values\\n{indent}# counts of drops <= sp_idx\\n{indent}import numpy as \r\n np\\n{indent}cnt = np.searchsorted(drop, sp_idx, side='right')\\n{indent}# Keep only sp_idx not in drop (membership via searchsorted)\\n{indent}left = np.searchsor\r\nrted(drop, sp_idx, side='left')\\n{indent}right = cnt\\n{indent}keep_mask = left == right\\n{indent}new_indices = sp_idx[keep_mask] - cnt[keep_mask]\\n{indent}new_va\r\nalues = sp_vals[keep_mask]\\n{indent}new_len = n - drop.size\\n{indent}new_sp_index = make_sparse_index(new_len, new_indices.astype('int32', copy=False), self.kind\r\nd)\\n{indent}return type(self)._simple_new(new_values, new_sp_index, self.dtype)\\n\"\r\n\u001b[?2004l\r\n\u001b[?2004h> new_s = s[:m.start(2)] + nb + s[m.end(2):]\r\n\u001b[?2004l\r\n\u001b[?2004h> p.write_text(new_s)\r\n\u001b[?2004l\r\n\u001b[?2004h> print('Indentation fixed')\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\nIndentation fixed\r\n"]
[255.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[255.002, "i", "python - << 'PY'\r"]
[255.004, "o", "python - << 'PY'\r\n"]
[255.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> # Smoke test: timing like before\r\n\u001b[?2004l\r\n\u001b[?2004h> import numpy as np, pandas as pd\r\n\u001b[?2004l\r\n\u001b[?2004h> from pandas.arrays import SparseArray\r\n\u001b[?2004l\r\n\u001b[?2004h> N=1_000_000\r\n\u001b[?2004l\r\n\u001b[?2004h> arr=np.full(N, np.nan, dtype=float)\r\n\u001b[?2004l\r\n\u001b[?2004h> idx=np.random.choice(np.arange(N), int(N*1e-5), replace=False)\r\n\u001b[?2004l\r\n\u001b[?2004h> arr[idx]=np.random.rand(idx.size)\r\n\u001b[?2004l\r\n\u001b[?2004h> sp_arr=SparseArray(arr)\r\n\u001b[?2004l\r\n\u001b[?2004h> for fv in [True, False, np.nan]:\r\n\u001b[?2004l\r\n\u001b[?2004h>     b_arr = np.full(shape=N, fill_value=fv, dtype=bool)\r\n\u001b[?2004l\r\n\u001b[?2004h>     fv_inds = np.unique(np.random.randint(0, N-1, int(N*1e-5), dtype=np.int32))\r\n\u001b[?2004l\r\n\u001b[?2004h>     b_arr[fv_inds] = True if pd.isna(fv) else (not fv)\r\n\u001b[?2004l\r\n\u001b[?2004h>     sp_b_arr = SparseArray(b_arr, dtype=bool, fill_value=fv)\r\n\u001b[?2004l\r\n\u001b[?2004h>     import time; t0=time.perf_counter(); res = sp_arr[sp_b_arr]; dt=(time.perf_counter()-t0)*1e3\r\n\u001b[?2004l\r\n\u001b[?2004h>     print(f\"fv={fv!r}: {dt:.3f} ms, len={len(res)}\")\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\n+ /opt/conda/envs/asv_3.10/bin/ninja\r\n\r\n[0/1] Generating write_version_file with a custom command\u001b[K\r\n[1/1] Generating write_version_file with a custom command\u001b[K\r\nfv=True: 0.076 ms, len=999990\r\nfv=False: 0.100 ms, len=10\r\nfv=nan: 0.064 ms, len=1000000\r\n"]
[260.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[260.002, "i", "cd asv_bench\r"]
[260.004, "o", "cd asv_bench\r\n"]
[260.006, "o", "\u001b[?2004l\r\n"]
[265.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo/asv_bench# "]
[265.002, "i", "asv run --python=same --bench=\"sparse.GetItemMask.*\" --quick\r"]
[265.004, "o", "asv run --python=same --bench=\"sparse.GetItemMask.*\" --quick\r\n"]
[265.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39msparse.GetItemMask.time_mask\u001b[0m                                                                                                                    ok\r\n[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39m============ =========\u001b[0m\r\n              fill_value           \r\n             ------------ ---------\r\n                 True      153\u00b10\u03bcs \r\n                False      184\u00b10\u03bcs \r\n                 nan       104\u00b10\u03bcs \r\n             ============ =========\r\n\r\n"]
[270.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo/asv_bench# "]
[270.002, "i", "cd ..\r"]
[270.004, "o", "cd ..\r\n"]
[270.006, "o", "\u001b[?2004l\r\n"]
[275.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[275.002, "i", "git config user.email \"you@example.com\"\r"]
[275.004, "o", "git config user.email \"you@example.com\"\r\n"]
[275.006, "o", "\u001b[?2004l\r\n"]
[280.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[280.002, "i", "git config user.name \"Your Name\"\r"]
[280.004, "o", "git config user.name \"Your Name\"\r\n"]
[280.006, "o", "\u001b[?2004l\r\n"]
[285.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[285.002, "i", "git add -A\r"]
[285.004, "o", "git add -A\r\n"]
[285.006, "o", "\u001b[?2004l\r\n"]
[290.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[290.002, "i", "git commit -m \"SparseArray.__getitem__: optimize boolean SparseArray mask indexing\\n\\n- Fast-path for SparseArray[b\r"]
[290.004, "o", "git commit -m \"SparseArray.__getitem__: optimize boolean SparseArray mask indexing\\n\\n- Fast-path for SparseArray[b\r\n"]
[290.006, "o", "bool] masks:\\n  * fill_value=False: take explicit True indices directly.\\n  * fill_value=True: drop explicit False indices without dense mask/arange; reindex spa\r\narse indices efficiently.\\n  * fill_value=NaN: select explicit True positions; short-circuit when all True.\\n\\nImproves asv sparse.GetItemMask.time_mask (True/Na\r\naN) by ~50\u2013100x while maintaining correctness.\"\r\n\u001b[?2004l\r\n[main 71d9f0dda3] SparseArray.__getitem__: optimize boolean SparseArray mask indexing\\n\\n- Fast-path for SparseArray[bool] masks:\\n  * fill_value=False: take explicit True indices directly.\\n  * fill_value=True: drop explicit False indices without dense mask/arange; reindex sparse indices efficiently.\\n  * fill_value=NaN: select explicit True positions; short-circuit when all True.\\n\\nImproves asv sparse.GetItemMask.time_mask (True/NaN) by ~50\u2013100x while maintaining correctness.\r\n 1 file changed, 39 insertions(+), 4 deletions(-)\r\n"]
[295.0, "o", "(asv_3.10) root@725647bb4de2:/workspace/repo# "]
[295.002, "i", "\u001b[?2004l\r"]
[295.004, "o", "\u001b[?2004l\r\n"]
[295.006, "o", "\r\nlogout\r\n"]
