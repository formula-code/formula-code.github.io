{"paperHeader":{"title":"FormulaCode: Evaluating Agentic Optimization on Large Codebases","authors":[{"name":"Atharva Sehgal","url":"https://atharvas.net/","superscript":"1,*"},{"name":"James Hou","url":"https://jamesahou.github.io/","superscript":"2,*"},{"name":"Akanksha Sarkar","url":"https://milstein-program.as.cornell.edu/akanksha-sarkar/","superscript":"3"},{"name":"Ishaan Mantripragada","url":"https://www.linkedin.com/in/ishaanmantri/","superscript":"2"},{"name":"Swarat Chaudhuri","url":"https://www.cs.utexas.edu/~swarat/","superscript":"1"},{"name":"Jennifer J. Sun","url":"https://jenjsun.com/","superscript":"3"},{"name":"Yisong Yue","url":"https://www.yisongyue.com/","superscript":"2"}],"affiliations":[{"superscript":"1","label":"UT Austin"},{"superscript":"2","label":"Caltech"},{"superscript":"3","label":"Cornell"}],"equalContributionNote":"* Equal contribution","actions":[{"label":"Arxiv","icon":"file-text","href":"https://example.com"},{"label":"Code","icon":"github","href":"https://github.com/formula-code/"},{"label":"Huggingface","icon":"database","href":"https://example.com"}],"abstract":{"title":"Abstract","paragraphs":["Rapid advances in LLM agents have demonstrated the ability to optimize code at the repository level, leading to an urgent need for benchmarks that measure this ability to drive impact in real-world use cases. Existing code benchmarks, either relying on synthetic/LLM-generated tasks, single objective workloads, or binary pass/fail outcomes, offer a constrained evaluation landscape compared to these emerging capabilities.","To bridge this gap, we introduce FormulaCode, a novel benchmark designed for evaluating agentic optimization on large codebases, with a focus on real-world multi-objective performance optimization.","FormulaCode is a <em>live</em> benchmark consisting of 961 real world performance bottleneck tasks mined from scientific GitHub repositories, with an average of 1532 workloads per task. As such, FormulaCode represents the first large-scale analysis of the <em>holistic ability of LLM agents to optimize codebases</em>.","We find that FormulaCode proves to be a challenging dataset for frontier LLMs and agent frameworks, with unrestricted repository exploration emerging as the primary component for finding performance inefficiencies. FormulaCode is also robust to data-leakage; simply copying the online solution yields no leaderboard improvement."]},"leaderboard":{"title":"FormulaCode’s Leaderboard (Tentative)","description":"Snapshot of latest results on FormulaCode. Updated Monthly!"},"hero":{"eyebrow":"Don’t see your model on the leaderboard?","instructions":"To evaluate an agent on FormulaCode, Follow the <a href=https://github.com/formula-code/terminal-bench/>Installation instructions</a> and run:","command":"harbor run -d formulacode@0.1.0.post20251025 -a oracle","body":"The next sections dive into FormulaCode’s analysis with interactive visualizations on a representative subset of FormulaCode. For up-to-date results and insights, please read the paper!","cta":{"label":"Read the paper","href":"https://arxiv.org/abs/2409.XXXXX"}}},"paperFooter":{"citation":{"show":"true","title":"Citation","bibtex":"@misc{sehgal2025formulacode,\r\n        title={FormulaCode: Evaluating Agentic Optimization on Large Codebases}, \r\n        author={Atharva Sehgal and James Hou and Akanksha Sarkar and Ishaan Mantripragada and Swarat Chaudhuri and Jennifer J. Sun and Yisong Yue},\r\n        year={2025},\r\n        eprint={2409.XXXXX},\r\n        archivePrefix={arXiv},\r\n        primaryClass={cs.LG},\r\n        url={https://arxiv.org/abs/2409.XXXXX}, \r\n}"},"funding":{"title":"Acknowledgements","description":"This project was made possible by a Slingshot Award from the <a href=https://www.laude.org/>Laude Institute</a> and an <a href=https://www.neurosymbolic.org/>NSF Expeditions</a> grant."},"relatedWork":{"show":"true","title":"Related Work\r\npaperFooter.relatedWork.text This project would not be possible without the excellent work of the community. These are some relevant papers to better understand the premise of our work:\r\n<ul>\r\n<li><a href=\"https://www.nature.com/articles/s41586-023-06924-6\">FunSearch: Making new\r\n    discoveries in mathematical sciences using Large Language Models</a> </li>\r\n<li><a href=\"https://arxiv.org/abs/2305.01582\">Interpretable Machine Learning for Science\r\n    with PySR and SymbolicRegression.jl</a> </li>\r\n<li><a href=\"https://arxiv.org/abs/2310.19791\">LILO: Learning Interpretable Libraries by\r\n    Compressing and Documenting Code</a> </li>\r\n<li><a href=\"https://arxiv.org/abs/1911.12247 \">LLM-SR: Scientific Equation Discovery via\r\n    Programming with Large Language Models</a> </li>\r\n<li><a href=\"https://arxiv.org/abs/2210.05050 \">Neurosymbolic Programming for Science</a>\r\n</li>\r\n</ul>"},"acknowledgements":{"show":"true","title":"Acknowledgements","text":"The website design heavily uses the template developed by <a href=https://pudding.cool/author/fox-meyer/>Fox Meyer</a> and <a href=https://pudding.cool/author/jan-diehm/>Jan Diehm</a> for their interactive article in <a href='https://pudding.cool/'>Pudding.cool</a> on <a href='https://github.com/the-pudding/wine-animals'>The Pour-ing of species</a> that is distributed under an MIT license. The code itself is based on <a href='https://github.com/the-pudding/svelte-starter' target='_blank'>The Pudding's SvelteKit starter template</a>. The Visualizations use <a href='https://layercake.graphics/' target='_blank'>LayerCake</a> and <a href='https://d3js.org/' target='_blank'>D3.js</a>. The source code for this website is available <a href='https://github.com/formula-code/formula-code.github.io/'>here</a>."}},"opening":[{"text":"Your codebase isn’t as fast as it used to be and you want to use an agent to optimize the code. You’ve got no preference for a model or agent framework, but you want it to work without any intervention. Which agent model pair do you choose?","instructions":"<span class=tap-click>Tap on a Model/Agent to select.</span> <span class=random-click>Just pick a random one for me.</span>","gpt5":"You picked <span class=bold>Terminus 2 + GPT-5</span>. A conservative choice! GPT-5 often overlooks small optimizations in favor of large ones. It is best when you want to produce <em>module-level</em> optimizations. <span class=instructions>How do we know? Keep scrolling.</span>","claude":"You picked <span class=bold>Terminus 2 + Claude Sonnet 4.0</span>. A reliable choice! Claude Sonnet 4.0 performs the best in our benchmarks at finding  function-level and class-level optimizations, but fails on <em>module-level</em> optimizations. So you might need to give it a hand for large scale tasks. <span class=instructions>How do we know? Keep scrolling.</span>","oracle":"You picked a <span class=bold>Human</span>. At all levels, expert solutions consistently and repeatedly perform well; forming the basis of our comparative study.","gpt5Advantage":"<span class=bold>GPT-5</span> has slightly outperformed humans on <em>module</em> level performance, with an aggregate advantage of 1.04.","claudeAdvantage":"<span class=bold>Claude</span> has outperformed humans on <em>parameter</em> level performance, with an aggregate advantage of 1.04. However, on <em>module</em> level performance, its advantage reverses to -0.04 against the human expert.","oracleAdvantage":"<span class=bold>Humans</span> get an advantage score of 0 by default. This is to prevent other models from cheating.","gpt5Quad":"The <span class=selected-agent-circle-span>Terminus 2/GPT-5 pair you picked</span> falls into the superoptimization quadrant.","claudeQuad":"The <span class=selected-agent-circle-span>Terminus 2/Claude Sonnet 4.0 pair you picked</span> falls into the superoptimization quadrant.","oracleQuad":"When we compare against <span class=selected-agent-circle-span>Experts</span> on the other axis, the distribution follows the line of equal advantage and hence is in the no-optimization (over experts) zone."}],"steps":[{"type":"text","value":"Your codebase isn’t as fast as it used to be and you want to use an agent to optimize the code. You’ve got no preference for a model or agent framework, but you want it to work without any intervention. Which agent model pair do you choose?"},{"type":"text","value":"Couldn’t decide? Maybe this info will help: <span class=bold>Terminus 2 + GPT-5</span> has the highest advantage at producing <em>module-level</em> optimizations, but it often overlooks small optimizations, <span class=bold>Terminus 2 + Claude Sonnet 4.0</span> finds <em>function-level</em> optimizations pretty well, but it might not be the best for deep optimizations. <span class=instructions>How do we know? Keep scrolling.</span>"},{"type":"text","value":"We scraped 110+ GitHub repositories with crowdsourced performance workloads and identified all pull requests that <em>intended</em> to improve the performance of a specific piece of code.  Then, we measured the runtime of the repository before and after to see if the PR’s performance improvement was statistically significant."},{"type":"text","value":"After analyzing 1M+ PRs, we were able to identify 961 performance-improving tasks with over 1,472,080 total performance workloads across all tasks. For each of these problems, we asked a frontier LLM agent to optimize the code, given the same tools available to the human developers, and then measured the performance after rejecting optimizations that broke the code. <span class=instructions>Read more in the methodology.</span>"},{"type":"text","value":"Here’s a cumulative distribution function of the <em>speedup ratio</em> for each of our models. <span class=instructions>Hover over a model to see more details!</span> A CDF is essentially an integration over the histogram; the <em>slower</em> the CDF line rises, the more benchmarks live in the faster region, and the better the model."},{"type":"text","value":"On first glance, it looks like our agents are doing pretty well! For <span class=bold>GPT-5</span> and <span class=bold>Claude Sonnet 4.0</span>, there are a lot of jagged bumps, and about 3-5% of all benchmarks are outliers, where both models show extreme code optimizations. However, 75 to 80% of all benchmarks are modest improvements, with a speedup of less than 10%."},{"type":"text","value":"However, with a median of 81 benchmarks per task, good performance on a lone workload doesn’t tell us much about the <em>holistic</em> performance of such agents. What we really care about is whether models have a <em>consistent advantage</em> at optimizing code."}],"postIntro":[{"type":"text","value":"What emerges from the above analysis is that speedup alone doesn’t capture the full picture."},{"type":"text","value":"<em>Performance optimizations rarely have isolated effects</em>; an optimization in one part of the code could significantly slow down or speed up another part of the code."},{"type":"text","value":"Instead, we hypothesize that good performance optimizations produce an <span class=yellow-bold>aggregate advantage</span>. This requires reasoning about multiple workloads across multiple functionalities and target resources, and ensuring we <em>consistently</em> produce speedups."},{"type":"text","value":"To understand more, let’s dive deeper into the data."}],"chartScroll":[{"block":[{"type":"text","value":"Instead of looking at the expert-produced speedup and the model-produced speedup separately, let’s look at them together on a scatterplot."},{"type":"text","value":"The <span class=bold>Human Speedup</span> is on the y-axis here, so the better the human speedup, the closer it is to the top. And the <span class=bold>Model Speedup</span> is on the x-axis."}]},{"block":[{"type":"text","value":"Each data point represents a statistically significant workload captured in our benchmark."},{"type":"text","value":"The <span class=yellow-bold>highlighted workload</span> lies at position x=1.11 and y=1.38. That is, the human engineer optimized this workload to be 38% faster than the baseline while the agent’s optimization was only 11% faster."},{"type":"text","value":"The agent’s achievements are much less impressive now because the agent demonstrates no <strong>Advantage</strong> over the oracle."}]},{"block":[{"type":"text","value":"So, where do the most impressive speedups lie? Let’s load the entire dataset and demarcate some regions of interest."},{"type":"text","value":"The identify function line depicts <strong>Equal advantage</strong>. For any workload on this line, an agent-written patch is as good as a human-written patch."}]},{"block":[{"type":"text","value":"Workloads that cause <em>slowdowns</em> will have a speedup less than 1.00x."},{"type":"text","value":"The  <strong>No oracle speedup</strong> line and a <strong>No agent speedup</strong> line centered at 1.00 help visualize this."},{"type":"text","value":"Now, we have 4 regions of interest."}]},{"block":[{"type":"text","value":"The <span class=regression-span>Bottom Left</span> region characterizes Regressions; these are all the workloads where the agent and the oracle both caused a <strong>Performance Regression</strong>."},{"type":"text","value":"This could be an intentional tradeoff, or just a tricky workload  for both agents and humans."}]},{"block":[{"type":"text","value":"The <span class=sub-optimization-span>Top left</span> region shows sub-optimal benchmarks – the benchmarks where the oracle achieved a speedup but the agent caused a regression."},{"type":"text","value":"This is the worst region for an agent."}]},{"block":[{"type":"text","value":"The <span class=under-optimization-span>Top right</span> region shows under-optimized benchmarks – the agent still achieves some speedup but the expert-provided solution was much better."},{"type":"text","value":"Any workload here is a worthwhile tradeoff depending on resource prioritization."}]},{"block":[{"type":"text","value":"What we are really interested in are <span class=sweet-rect-span>Super optimizations</span> – these are the workflows where the agent produced optimizations that were better than the oracle optimizations and better than the baseline."}]},{"block":[{"type":"text","value":"This allows us to define a notion of <strong>agent advantage</strong>. Mathematically, given two dimensionless vectors depicting the oracle speedups and the agent speedups:"},{"type":"math","value":"\\text{oracle-speedup} = \\mathbf{o}_{1:N}"},{"type":"math","value":"\\text{agent-speedup} = \\mathbf{a}_{1:N}"},{"type":"text","value":"We can define a metric for the overall performance by calculating the average distance from the equal advantage line:"},{"type":"math","value":"\\text{advantage} = \\frac{1}{N} \\sum_{i=1}^{N} o_i - a_i"},{"type":"text","value":"Intuitively, the closer a point is to the equal advantage line, the lower its score."}]},{"block":[{"type":"text","value":"What if an agent tries to <span class=bold>minimic the Human</span>’s steps?"},{"type":"text","value":"Unsurprisingly, all the points lie on the equal advantage line. This means that any simply replicating a memorized solution would get an advantage of 0.0."}]},{"block":[{"type":"text","value":"Here’s the <span class=bold>Human v/s Claude</span> plot."},{"type":"text","value":"Most benchmarks are either super optimal or under optimal!"},{"type":"text","value":"Claude’s advantage score here is 0.0749, which means Claude does <em>slightly</em> better than the expert on these problems."}]},{"block":[{"type":"text","value":"The <span class=bold>Human v/s GPT-5</span> comparison, is similar."},{"type":"text","value":"We see a few superoptimizations but mostly suboptimizations."},{"type":"text","value":"GPT-5’s advantage score is -0.02. So, it’s slightly worse off than humans."}]},{"block":[{"type":"text","value":"This is surprising. Is Claude truly better than GPT-5 and humans?"},{"type":"text","value":"This is a good time to talk about our <span class=yellow-bold>grouping scheme</span>."},{"type":"text","value":"In the <span class=”highlight-grouping”>bottom left corner</span>, notice that the current data points aren’t being aggregated. So, we’re still looking at <em>singular</em> workloads."},{"type":"text","value":"To investigate the <em>holistic</em> optimization abilities, we can group workloads together based on their prefix strings (e.g: Aggregate all workloads under <code>pandas.algorithm.*</code>)."}]},{"block":[{"type":"text","value":"This is the same <span class=bold>Human v/s Claude</span> plot but aggregated on <span class=bold>Modules</span>."},{"type":"text","value":"The oracle’s performance increases significantly and most of Claude’s optimizations disappear! The new advantage score is now <span class=yellow-bold>-0.0002</span>."},{"type":"text","value":"So, Claude’s aggregate performance optimization capabilities are much weaker than its individual performance optimization capabilities."},{"type":"text","value":"With the same aggregation, GPT’s advantage score is 0.0034. <em>Their advantage flipped</em>."}]},{"block":[{"type":"text","value":"But all this is conditioned on our definition of what counts as equal advantage. What if the minimum acceptable speedup is different?"},{"type":"text","value":"<span class=instructions><span class=slider-span>Use the sliders</span> to set your own criteria for equal advantage, and keep scrolling to see a model-by-model breakdown based on your selection.</span>"}]}],"postScatter":[{"type":"text","value":"Use the nav boxes to navigate through all the model groups."}]}