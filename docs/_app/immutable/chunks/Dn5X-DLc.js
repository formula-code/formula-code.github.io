const e={title:"FormulaCode: Evaluating Agentic Optimization on Large Codebases",authors:[{name:"Atharva Sehgal",url:"https://atharvas.net/",superscript:"1,*"},{name:"James Hou",url:"https://jamesahou.github.io/",superscript:"2,*"},{name:"Akanksha Sarkar",url:"https://milstein-program.as.cornell.edu/akanksha-sarkar/",superscript:"3"},{name:"Ishaan Mantripragada",url:"https://www.linkedin.com/in/ishaanmantri/",superscript:"2"},{name:"Swarat Chaudhuri",url:"https://www.cs.utexas.edu/~swarat/",superscript:"1"},{name:"Jennifer J. Sun",url:"https://jenjsun.com/",superscript:"3"},{name:"Yisong Yue",url:"https://www.yisongyue.com/",superscript:"2"}],affiliations:[{superscript:"1",label:"UT Austin"},{superscript:"2",label:"Caltech"},{superscript:"3",label:"Cornell"}],equalContributionNote:"* Equal contribution; Mail to atharvas@utexas.edu",actions:[{label:"Arxiv",icon:"file-text",href:"https://example.com"},{label:"Code",icon:"github",href:"https://example.com"},{label:"Huggingface",icon:"database",href:"https://example.com"}],abstract:{title:"Abstract",paragraphs:["Rapid advances in LLM agents have demonstrated the ability to optimize code at the repository level, leading to an urgent need for benchmarks that measure this ability to drive impact in real-world use cases. Existing code benchmarks, either relying on synthetic/LLM-generated tasks, single objective workloads, or binary pass/fail outcomes, offer a constrained evaluation landscape compared to these emerging capabilities.","To bridge this gap, we introduce FormulaCode, a novel benchmark designed for evaluating agentic optimization on large codebases, with a focus on real-world multi-objective performance optimization.","FormulaCode is a <em>live</em> benchmark consisting of 961 real world performance bottleneck tasks mined from scientific GitHub repositories, with an average of 1532 workloads per task. As such, FormulaCode represents the first large-scale analysis of the <em>holistic ability of LLM agents to optimize codebases</em>.","We find that FormulaCode proves to be a challenging dataset for frontier LLMs and agent frameworks, with unrestricted repository exploration emerging as the primary component for finding performance inefficiencies. FormulaCode is also robust to data-leakage; simply copying the online solution yields no leaderboard improvement."]},leaderboard:{title:"FormulaCode’s Leaderboard",description:"Snapshot of latest results on FormulaCode. Updated daily!",columns:[{label:"Agent",key:"Agent"},{label:"Overall",key:"all-level"},{label:"Parameter-level",key:"param-level"},{label:"Function-level",key:"func-level"},{label:"Class-level",key:"class-level"},{label:"Module-level",key:"module-level"}],rows:[{Agent:"Terminus-2 (Claude Sonnet 4.0)","all-level":"0.017661","param-level":"0.018890","func-level":"0.014764","class-level":"0.019422","module-level":"0.018581"},{Agent:"Terminus-2 (GPT-5)","all-level":"0.010994","param-level":"0.011201","func-level":"0.011158","class-level":"0.010659","module-level":"0.010499"},{Agent:"Expert","all-level":"0.000000","param-level":"0.000000","func-level":"0.000000","class-level":"0.000000","module-level":"0.000000"}]},hero:{eyebrow:"Want to learn more?",title:"Interactive Visualizations",body:"The next sections feature interactive visualization on a representative subset of FormulaCode. For up-to-date results and insights, please read the paper!",cta:{label:"Read the paper",href:"https://arxiv.org/abs/2409.XXXXX"}}},t={citation:{show:"true",title:"Citation",bibtex:`@misc{sehgal2025formulacode,\r
        title={FormulaCode: Evaluating Agentic Optimization on Large Codebases}, \r
        author={Atharva Sehgal and James Hou and Akanksha Sarkar and Swarat Chaudhuri and Jennifer J. Sun and Yisong Yue},\r
        year={2025},\r
        eprint={2409.XXXXX},\r
        archivePrefix={arXiv},\r
        primaryClass={cs.LG},\r
        url={https://arxiv.org/abs/2409.XXXXX}, \r
}`},relatedWork:{show:"true",title:`Related Work\r
paperFooter.relatedWork.text This project would not be possible without the excellent work of the community. These are some relevant papers to better understand the premise of our work:\r
<ul>\r
<li><a href="https://www.nature.com/articles/s41586-023-06924-6">FunSearch: Making new\r
    discoveries in mathematical sciences using Large Language Models</a> </li>\r
<li><a href="https://arxiv.org/abs/2305.01582">Interpretable Machine Learning for Science\r
    with PySR and SymbolicRegression.jl</a> </li>\r
<li><a href="https://arxiv.org/abs/2310.19791">LILO: Learning Interpretable Libraries by\r
    Compressing and Documenting Code</a> </li>\r
<li><a href="https://arxiv.org/abs/1911.12247 ">LLM-SR: Scientific Equation Discovery via\r
    Programming with Large Language Models</a> </li>\r
<li><a href="https://arxiv.org/abs/2210.05050 ">Neurosymbolic Programming for Science</a>\r
</li>\r
</ul>`},acknowledgements:{show:"true",title:"Acknowledgements",text:"The website design heavily uses the template developed by <a href=https://pudding.cool/author/fox-meyer/>Fox Meyer</a> and <a href=https://pudding.cool/author/jan-diehm/>Jan Diehm</a> for their interactive article in <a href='https://pudding.cool/'>Pudding.cool</a> on <a href='https://github.com/the-pudding/wine-animals'>The Pour-ing of species</a> that is distributed under an MIT license. The code itself is based on <a href='https://github.com/the-pudding/svelte-starter' target='_blank'>The Pudding's SvelteKit starter template</a>. The Visualizations use <a href='https://layercake.graphics/' target='_blank'>LayerCake</a> and <a href='https://d3js.org/' target='_blank'>D3.js</a>. The source code for this website is available <a href='https://github.com/formula-code/formula-code.github.io/'>here</a>."}},a=[{text:"Your codebase isn’t as fast as it used to be and you want to use an agent to optimize the code. You’ve got no preference for a model or agent framework, but you want it to work without any intervention. Which agent model pair do you choose?",instructions:"<span class=tap-click>Tap on a Model/Agent to select.</span> <span class=random-click>Just pick a random one for me.</span>",gpt5:"You picked <span class=bold>Terminus 2 + GPT-5</span>. A conservative choice! GPT-5 often overlooks small optimizations in favor of large ones. It is best when you want to produce <em>module-level</em> optimizations. <span class=instructions>How do we know? Keep scrolling.</span>",claude:"You picked <span class=bold>Terminus 2 + Claude Sonnet 4.0</span>. A reliable choice! Claude Sonnet 4.0 performs the best in our benchmarks at finding  function-level and class-level optimizations, but fails on <em>module-level</em> optimizations. So you might need to give it a hand for large scale tasks. <span class=instructions>How do we know? Keep scrolling.</span>",oracle:"You picked a <span class=bold>Human</span>. At all levels, expert solutions consistently and repeatedly perform well; forming the basis of our comparative study.",gpt5Advantage:"<span class=bold>GPT-5</span> has slightly outperformed humans on <em>module</em> level performance, with an aggregate advantage of 1.04.",claudeAdvantage:"<span class=bold>Claude</span> has outperformed humans on <em>parameter</em> level performance, with an aggregate advantage of 1.04. However, on <em>module</em> level performance, its advantage reverses to -0.04 against the human expert.",oracleAdvantage:"<span class=bold>Humans</span> get an advantage score of 0 by default. This is to prevent other models from cheating.",gpt5Quad:"The <span class=selected-agent-circle-span>Terminus 2/GPT-5 pair you picked</span> falls into the superoptimization quadrant.",claudeQuad:"The <span class=selected-agent-circle-span>Terminus 2/Claude Sonnet 4.0 pair you picked</span> falls into the superoptimization quadrant.",oracleQuad:"When we compare against <span class=selected-agent-circle-span>Experts</span> on the other axis, the distribution follows the line of equal advantage and hence is in the no-optimization (over experts) zone."}],o=[{type:"text",value:"Your codebase isn’t as fast as it used to be and you want to use an agent to optimize the code. You’ve got no preference for a model or agent framework, but you want it to work without any intervention. Which agent model pair do you choose?"},{type:"text",value:"Couldn’t decide? Maybe this info will help: <span class=bold>Terminus 2 + GPT-5</span> has the highest advantage at producing <em>module-level</em> optimizations, but it often overlooks small optimizations, <span class=bold>Terminus 2 + Claude Sonnet 4.0</span> finds <em>function-level</em> optimizations pretty well, but it might not be the best for deep optimizations. <span class=instructions>How do we know? Keep scrolling.</span>"},{type:"text",value:"We scraped 110+ GitHub repositories with crowdsourced performance workloads and identified all pull requests that talked about performance optimization.  Then, we measured the runtime of the code before and after to see if the PR’s performance improvement was statistically significant."},{type:"text",value:"After analyzing 1M+ PRs, we were able to identify 961 performance-improving tasks with over 1,472,080 total performance workloads across all tasks. For each of these problems, we asked a frontier LLM agent to optimize the code, given the same tools available to the human developers, and then measured the performance after rejecting optimizations that broke the code. <span class=instructions>Read more in the methodology.</span>"},{type:"text",value:"Here’s a cumulative distribution function of the <em>speedup ratio</em> for each of our models. <span class=instructions>Hover over a model to see more details!</span> A CDF is essentially an integration over the histogram; the <em>slower</em> the CDF line rises, the more benchmarks live in the faster region, and the better the model."},{type:"text",value:"On first glance, it looks like our agents are doing pretty well! For <span class=bold>GPT-5</span> and <span class=bold>Claude Sonnet 4.0</span>, there are a lot of jagged bumps, and about 3-5% of all benchmarks are outliers, where both models show extreme code optimizations. However, 75 to 80% of all benchmarks are modest improvements, with a speedup of less than 10%."},{type:"text",value:"However, with a median of 81 benchmarks per task, good performance on a lone workload doesn’t tell us much about the <em>holistic</em> performance of such agents. What we really care about is whether models have a <em>consistent advantage</em> at optimizing code."}],s=[{type:"text",value:"What emerges from the above analysis is that speedup alone doesn’t capture the full picture."},{type:"text",value:"<em>Performance optimizations rarely have isolated effects</em>; an optimization in one part of the code could significantly slow down or speed up another part of the code."},{type:"text",value:"Instead, we hypothesize that good performance optimizations produce an <span class=yellow-bold>aggregate advantage</span>. This requires reasoning about multiple workloads across multiple functionalities and target resources, and ensuring we <em>consistently</em> produce speedups."},{type:"text",value:"To understand more, let’s dive deeper into the data."}],n=[{block:[{type:"text",value:"Instead of looking at the expert-produced speedup and the model-produced speedup separately, let’s look at them together on a scatterplot."},{type:"text",value:"The <span class=bold>Human Speedup</span> is on the y-axis here, so the better the human speedup, the closer it is to the top. And the <span class=bold>Model Speedup</span> is on the x-axis."}]},{block:[{type:"text",value:"Each data point represents a statistically significant workload captured in our benchmark."},{type:"text",value:"The <span class=yellow-bold>highlighted workload</span> lies at position x=1.11 and y=1.38. That is, the human engineer optimized this workload to be 38% faster than the baseline while the agent’s optimization was only 11% faster."},{type:"text",value:"The agent’s achievements are much less impressive now because the agent demonstrates no <strong>Advantage</strong> over the oracle."}]},{block:[{type:"text",value:"So, where do the most impressive speedups lie? Let’s demarcate some regions on this plot.."},{type:"text",value:"The identify function line depicts <strong>Equal advantage</strong>. For any workload on this line, an agent-written patch is as good as a human-written patch."}]},{block:[{type:"text",value:"We can also define a  <strong>No oracle speedup</strong> line and a <strong>No agent speedup</strong> line centered at 1.00x."},{type:"text",value:"Now, we have 4 regions of interest."}]},{block:[{type:"text",value:"The <span class=regression-span>Bottom Left</span> region characterizes Regressions; these are all the workloads where the agent and the oracle both caused a <strong>Performance Regression</strong>."},{type:"text",value:"This could be an intentional tradeoff, or just a tricky workload  for both agents and humans."}]},{block:[{type:"text",value:"The <span class=sub-optimization-span>Top left</span> region shows sub-optimal benchmarks – the benchmarks where the oracle achieved a speedup but the agent caused a regression."},{type:"text",value:"This is the worst region for an agent."}]},{block:[{type:"text",value:"The <span class=under-optimization-span>Top right</span> region shows under-optimized benchmarks – the agent still achieves some speedup but the expert-provided solution was much better."},{type:"text",value:"Any workload here is a worthwhile tradeoff depending on resource prioritization."}]},{block:[{type:"text",value:"What we really want to look out for are models in the <span class=sweet-rect-span>shaded region</span> — these are the workflows where the agent produced optimizations that were better than the oracle optimizations."}]},{block:[{type:"text",value:"This allows us to define a notion of <strong>agent advantage</strong>. Mathematically, given two dimensionless vectors depicting the oracle speedups and the agent speedups:"},{type:"math",value:"\\text{oracle-speedup} = \\mathbf{o}_{1:N}"},{type:"math",value:"\\text{agent-speedup} = \\mathbf{a}_{1:N}"},{type:"text",value:"We can define a metric for the overall performance by calculating the average distance from the equal advantage line by calculating:"},{type:"math",value:"\\text{advantage} = \\frac{1}{N} \\sum_{i=1}^{N} o_i - a_i"},{type:"text",value:"Intuitively, the closer a point is to the equal advantage line, the lower its score."}]},{block:[{type:"text",value:"What if an agent tries to <span class=bold>minimic the Human</span>’s steps?"},{type:"text",value:"Unsurprisingly, all the points lie on the equal advantage line. This means that any simply replicating a memorized solution would get an advantage of 0.0."}]},{block:[{type:"text",value:"Here’s the <span class=bold>Human v/s Claude</span> plot."},{type:"text",value:"There are a lot of sub-optimal workflows but a few superoptimizatons!"},{type:"text",value:"The advantage score here is 0.21, which means Claude does slightly better than the expert on these problems."}]},{block:[{type:"text",value:"The <span class=bold>Human v/s GPT-5</span> comparison, is similar."},{type:"text",value:"We see a few superoptimizations but mostly suboptimizations."},{type:"text",value:"GPT-5’s advantage score is -0.02. So, it’s slightly worse off than humans."}]},{block:[{type:"text",value:"This is surprising. Is Claude truly better than GPT-5 and humans?"},{type:"text",value:"This is a good time to talk about our <span class=bold>grouping scheme</span>."},{type:"text",value:"In the <span class=”highlight-grouping”>bottom left corner</span>, notice that the current data points have not been aggregated based on their level. So, our results only apply if we want to optimize <em>singular</em> workloads."},{type:"text",value:"To investigate the <em>holistic</em> optimization abilities, we can group workloads together based on their prefix strings (i.e: A module aggregation groups all <code>astropy.coordinates.*</code>  workloads together)."}]},{block:[{type:"text",value:"This is the same <span class=bold>Human v/s Claude</span> plot but aggregated on <span class=bold>Modules</span>."},{type:"text",value:"Most of our optimizations disappear! The new advantage score is now -0.01. With the same aggregation, GPT"},{type:"text",value:"So, Claude’s aggregate performance at optimizing a series of workloads that depict a class is much weaker than its ability to optimize individual workloads."}]},{block:[{type:"text",value:"But all this is conditioned on our rather strict definition of <span class=bold>equal advantage</span>. What happens when we change the balance in the model’s favor?"},{type:"text",value:"<span class=insructions><span class=slider-span>Use the sliders</span> and filters to set your own criteria for equal advantage, and keep scrolling to see a model-by-model breakdown based on your selection.</span>"}]}],i=[{type:"text",value:"Use the <span class=l-r-arrows>left and right arrows</span> to navigate through all the model groups."}],r=[{type:"text",value:"To get our initial list of wines, we queried a bunch of different filter combinations on the Vivino app, including details like wine types (Red, White, Rosé, etc.), price range ($0–$10,000+), and average rating (0–5 stars), and removed duplicates. For each wine, we collected the following: Vivino ID, name, year, winery, country, region, type, rating, number of ratings, price, currency, and a URL with the image of the label. All details were collected in March 2024 and may not reflect current price or rating, especially if those have changed dramatically."},{type:"text",value:"To avoid over-prioritizing wines whose brands included animals, we <a href=https://github.com/the-pudding/wine-animals/blob/main/tasks/limit-wine.js>limited each brand to one wine per type</a> (Dessert, Fortified, Red, Rosé, Sparkling, White), choosing the wine with the most ratings as a proxy for which wine would be more well known."},{type:"text",value:"We then <a href=https://github.com/the-pudding/wine-animals/blob/main/tasks/identify-wine.js>fed the label images</a> into the OpenAI API (gpt-4-vision-preview model). We tested several prompts including open-ended questions like <span class=code>What’s on this image?</span>, but ultimately the prompt that worked the best was <span class=code>This image is a wine label. Do you see any animals or humans on it? On a scale of 0-1, with 0 being 'not certain at all' and 1 being 'very certain', how sure are you?</span>"},{type:"text",value:"This provided us with responses that looked something like this: <span class=code>I do not see any animals or humans on the wine label. The label contains text and a small emblem at the top, but there are no discernible figures of humans or animals. I am very certain about this assessment; so on the scale from 0 to 1, my confidence level is 1.</span> Occasionally, Chat-GPT would respond with something like <span class=code>I'm sorry, I can't assist with these requests.</span> In those cases we reran the prompt or manually reviewed the label. We ended up abandoning Chat-GPT’s certainty prediction because it wildly swung toward the poles: only 0.6% of the wines had confidence scores of something other than 0 or 1."},{type:"text",value:"We then manually reviewed a subset of wines to check Chat-GPT’s accuracy. Chat-GPT was particularly bad with pachyderms. It also struggled when animals weren’t the focal point and with what counted as an “animal.” Sometimes anthropomorphic or mythical creatures and insects were included, sometimes not. Knowing these shortcomings, we paid more attention to manually reviewing these types of labels. There still may be a few labels that slipped through the cracks, but overall this should be a solid sample."},{type:"text",value:"We ended up identifying 1,488 animal wines (the animal counts may not equal the total number of wines because multiple animals could appear on each label). We then manually grouped them into larger categories. Some animals rolled up, others did not. For example:"}],l=[{type:"text",value:"This opened up a lot of existential questions about what constitutes an animal, like “Is a duck a bird?” or “Is a zebra a horse?” We tried to strike a balance between common perception and scientific naming with binomial nomenclature (genus and species), often going back to the question “What would a kid call this animal?”"},{type:"text",value:"We limited our analysis to animal groupings with at least 20 wines, knocking out things like bats, monkeys, rodents, and marsupials. Our apologies to <a href=https://www.yellowtailwine.com/en-us/>Yellow Tail</a>, the lone kangaroo label in our dataset."},{type:"text",value:"To calculate the percentage of wines in each animal group that were good deals, we found the percentage of wines at or below the overall median price ($29.99) and at or above the median rating (4 stars) for each animal group."},{type:"text",value:"For the trend line, we calculated the best-fit regression (logarithmic) and used <a href=https://harryjstevens.com/>Harry Stevens’</a> <a href=https://github.com/HarryStevens/d3-regression>d3-regression package</a> to draw the line."},{type:"text",value:"We also bucketed the wine by price, rating, type, and country to compare each animal group’s distribution to all wines. We then calculated a Z-score for each bucket to find animal wines with statistically more or statistically fewer wines per bucket than all wines."},{type:"text",value:"To draw the 3D wine bottles we used Adobe Illustrator and Adobe Dimension, roughly following <a href=https://www.youtube.com/watch?v=X0qPDl3kMK4>this tutorial</a> from <a href=https://www.silvermoondesignschool.com/>Silver Moon Design School</a>. We exported 8 different views of each bottle from Dimension and combined them in an image sprite for the 360° rotation."}],p={paperHeader:e,paperFooter:t,opening:a,steps:o,postIntro:s,chartScroll:n,postScatter:i,methodsA:r,methodsB:l};export{p as c};
